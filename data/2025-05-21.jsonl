{"title": "A Weight Function Lemma Heuristic for Graph Pebbling", "abstract": "Graph pebbling is a problem in which pebbles are distributed across the\nvertices of a graph and moved according to a specific rule: two pebbles are\nremoved from a vertex to place one on an adjacent vertex. The goal is to\ndetermine the minimum number of pebbles required to ensure that any target\nvertex can be reached, known as the pebbling number. Computing the pebbling\nnumber lies beyond NP in the polynomial hierarchy, leading to bounding methods.\nOne of the most prominent techniques for upper bounds is the Weight Function\nLemma (WFL), which relies on costly integer linear optimization. To mitigate\nthis cost, an alternative approach is to consider the dual formulation of the\nproblem, which allows solutions to be constructed by hand through the selection\nof strategies given by subtrees with associated weight functions. To improve\nthe bounds, the weights should be distributed as uniformly as possible among\nthe vertices, balancing their individual contribution. However, despite its\nsimplicity, this approach lacks a formal framework. To fill this gap, we\nintroduce a novel heuristic method that refines the selection of balanced\nstrategies. The method is motivated by our theoretical analysis of the\nlimitations of the dual approach, in which we prove lower bounds on the best\nbounds achievable. Our theoretical analysis shows that the bottleneck lies in\nthe farthest vertices from the target, forcing surplus weight onto the closer\nneighborhoods. To minimize surplus weight beyond the theoretical minimum, our\nproposed heuristic prioritizes weight assignment to the farthest vertices,\nbuilding the subtrees starting from the shortest paths to them and then filling\nin the weights for the remaining vertices. Applying our heuristic to Flower\nsnarks and Blanu\\v{s}a snarks, we improve the best-known upper bounds,\ndemonstrating the effectiveness of a structured strategy selection when using\nthe WFL.", "published": "2025-05-21 22:01:49", "link": "http://arxiv.org/abs/2505.16050v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Families of tractable problems with respect to vertex-interval-membership width and its generalisations", "abstract": "Temporal graphs are graphs whose edges are labelled with times at which they\nare active. Their time-sensitivity provides a useful model of real networks,\nbut renders many problems studied on temporal graphs more computationally\ncomplex than their static counterparts. To contend with this, there has been\nrecent work devising parameters for which temporal problems become tractable.\nOne such parameter is vertex-interval-membership width. Broadly, this gives a\nbound on the number of vertices we need to keep track of at any time in order\nto solve any of a family of problems. Our contributions are two-fold. Firstly,\nwe introduce a new parameter, tree-interval-membership-width, that generalises\nboth vertex-interval-membership-width and several existing generalisations.\nSecondly, we provide meta-algorithms for both parameters which can be used to\nprove fixed-parameter-tractability for large families of problems, bypassing\nthe need to give involved dynamic programming arguments for every problem. We\napply these algorithms to temporal versions of Hamiltonian path, matching, edge\ndeletion to limit maximum reachability, and firefighting.", "published": "2025-05-21 16:12:39", "link": "http://arxiv.org/abs/2505.15699v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "First-order transducibility among classes of sparse graphs", "abstract": "We prove several negative results about first-order transducibility for\nclasses of sparse graphs:\n  - for every $t \\in \\mathbb{N}$, the class of graphs of treewidth at most\n$t+1$ is not transducible from the class of graphs of treewidth at most $t$;\n  - for every $t \\in \\mathbb{N}$, the class of graphs with Hadwiger number at\nmost $t+2$ is not transducible from the class of graphs with Hadwiger number at\nmost $t$; and\n  - the class of graphs of treewidth at most $4$ is not transducible from the\nclass of planar graphs.\n  These results are obtained by combining the known upper and lower bounds on\nthe weak coloring numbers of the considered graph classes with the following\ntwo new observations:\n  - If a weakly sparse graph class $\\mathscr D$ is transducible from a class\n$\\mathscr C$ of bounded expansion, then for some $k \\in \\mathbb{N}$, every\ngraph $G \\in \\mathscr D$ is a $k$-congested depth-$k$ minor of a graph\n$H^\\circ$ obtained from some $H\\in \\mathscr C$ by adding a universal vertex.\n  - The operations of adding a universal vertex and of taking $k$-congested\ndepth-$k$ minors, for a fixed $k$, preserve the degree of the distance-$d$ weak\ncoloring number of a graph class, understood as a polynomial in $d$.", "published": "2025-05-21 15:31:33", "link": "http://arxiv.org/abs/2505.15655v1", "categories": ["cs.LO", "cs.DM", "math.CO"], "primary_category": "cs.LO"}
{"title": "Creation of fixed points in block-parallel Boolean automata networks", "abstract": "In the context of discrete dynamical systems and their applications, fixed\npoints often have a clear interpretation. This is indeed a central topic of\ngene regulatory mechanisms modeled by Boolean automata networks (BANs), where a\nxollection of Boolean entities (the automata) update their state depending on\nthe states of others. Fixed points represent phenotypes such as differentiated\ncell types. The interaction graph of a BAN captures the architecture of\ndependencies among its automata. A first seminal result is that cycles of\ninteractions (so called feedbacks) are the engines of dynamical complexity. A\nsecond seminal result is that fixed points are invariant under block-sequential\nupdate schedules, which update the automata following an ordered partition of\nthe set of automata. In this article we study the ability of block-parallel\nupdate schedules (dual to the latter) to break this fixed point invariance\nproperty, with a focus on the simplest feedback mechanism: the canonical\npositive cycle. We quantify numerically the creation of new fixed points, and\nprovide families of block-parallel update schedules generating exponentially\nmany fixed points on this elementary structure of interaction.", "published": "2025-05-21 13:22:25", "link": "http://arxiv.org/abs/2505.15499v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "$4K_1$-free graph with the cop number $3$", "abstract": "The game of cops and robber is a two-player turn-based game played on a graph\nwhere the cops try to capture the robber. The cop number of a graph $G$,\ndenoted by $c(G)$ is the minimum number of cops required to capture the robber.\nFor a given class of graphs ${\\cal F}$, let $c({\\cal F}):=\\sup\\{c(F)|F\\in {\\cal\nF}\\}$, and let Forb$({\\cal F})$ denote the class of ${\\cal F}$-free graphs. We\nshow that the complement of the Shrikhande graph is $(4K_1,C_{\\ell}$)-free for\nany $\\ell \\geq 6$ and has the cop number~$3$. This provides a counterexample\nfor the conjecture proposed by Sivaraman (arxiv, 2019) which states that if $G$\nis $C_{\\ell}$-free for all $\\ell\\ge 6$, then $c(G)\\le 2$. This also gives a\nnegative answer to the question posed by Turcotte (Discrete Math. 345:112660\n(2022)) 112660. to check whether $c($Forb$(pK_1))=p-2$. Turcotte also posed the\nquestion to check whether $c($Forb$(pK_1+K_2))\\leq p+1$, for $p\\geq 3$. We\nprove that this result indeed holds. We also generalize this result for\nForb$(pK_1+qK_2)$. Motivated by the results of Baird et al. (Contrib. Discrete\nMath. 9:70--84 (2014)) and Turcotte and Yvon (Discrete Appl. Math. 301:74--98\n(2021)), we define the upper threshold degree and lower threshold degree for a\nparticular class of graphs and show some computational advantage to find the\ncop number using these.", "published": "2025-05-21 12:00:32", "link": "http://arxiv.org/abs/2505.15416v1", "categories": ["cs.DM", "math.CO", "05C57"], "primary_category": "cs.DM"}
{"title": "Minimum blocking sets for families of partitions", "abstract": "A $3$-partition of an $n$-element set $V$ is a triple of pairwise disjoint\nnonempty subsets $X,Y,Z$ such that $V=X\\cup Y\\cup Z$. We determine the minimum\nsize $\\varphi_3(n)$ of a set $\\mathcal{E}$ of triples such that for every\n3-partition $X,Y,Z$ of the set $\\{1,\\dots,n\\}$, there is some $\\{x,y,z\\}\\in\n\\mathcal{E}$ with $x\\in X$, $y\\in Y$, and $z\\in Z$. In particular,\n$$\\varphi_3(n)=\\left\\lceil{\\frac{n(n-2)}{3}}\\right\\rceil.$$ For $d>3$, one may\ndefine an analogous number $\\varphi_d(n)$. We determine the order of magnitude\nof $\\varphi_d(n)$, and prove the following upper and lower bounds, for $d>3$:\n$$\\frac{2 n^{d-1}}{d!} -o(n^{d-1}) \\leq \\varphi_d(n) \\leq\n\\frac{0.86}{(d-1)!}n^{d-1}+o(n^{d-1}).$$", "published": "2025-05-21 10:47:46", "link": "http://arxiv.org/abs/2505.15362v1", "categories": ["math.CO", "cs.DM", "05D15"], "primary_category": "math.CO"}
{"title": "Strong odd colorings in graph classes of bounded expansion", "abstract": "We prove that for every $d\\in \\mathbb{N}$ and a graph class of bounded\nexpansion $\\mathscr{C}$, there exists some $c\\in \\mathbb{N}$ so that every\ngraph from $\\mathscr{C}$ admits a proper coloring with at most $c$ colors\nsatisfying the following condition: in every ball of radius $d$, every color\nappears either zero times or an odd number of times. For $d=1$, this provides a\npositive answer to a question raised by Goetze, Klute, Knauer, Parada, Pe\\~na,\nand Ueckerdt [ArXiv 2505.02736] about the boundedness of the strong odd\nchromatic number in graph classes of bounded expansion. The key technical\ningredient towards the result is a proof that the strong odd coloring number of\na sets system can be bounded in terms of its semi-ladder index, 2VC dimension,\nand the maximum subchromatic number among induced subsystems.", "published": "2025-05-21 09:17:10", "link": "http://arxiv.org/abs/2505.15288v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation", "abstract": "Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.", "published": "2025-05-21 22:33:40", "link": "http://arxiv.org/abs/2505.16065v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "abstract": "Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.", "published": "2025-05-21 17:59:01", "link": "http://arxiv.org/abs/2505.15807v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning", "abstract": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.", "published": "2025-05-21 17:27:42", "link": "http://arxiv.org/abs/2505.15776v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Text-to-Pipeline: Bridging Natural Language and Data Preparation Pipelines", "abstract": "Data preparation (DP) transforms raw data into a form suitable for downstream\napplications, typically by composing operations into executable pipelines.\nBuilding such pipelines is time-consuming and requires sophisticated\nprogramming skills. If we can build the pipelines with natural language (NL),\nthe technical barrier of DP will be significantly reduced. However,\nconstructing DP pipelines from NL instructions remains underexplored. To fill\nthe gap, we introduce Text-to-Pipeline, a new task that translates NL data\npreparation instructions into DP pipelines. Furthermore, we develop a benchmark\nnamed PARROT to support systematic evaluation. To simulate realistic DP\nscenarios, we mined transformation patterns from production pipelines and\ninstantiated them on 23,009 real-world tables collected from six public\nsources. The resulting benchmark comprises ~18,000 pipelines covering 16 core\nDP operators. We evaluated cutting-edge large language models on PARROTand\nobserved that they only solved 72.86% of the cases, revealing notable\nlimitations in instruction understanding and multi-step reasoning. To address\nthis, we propose Pipeline-Agent, a stronger baseline that iteratively predicts\nand executes operations with intermediate table feedback, achieving the best\nperformance of 76.17%. Despite this improvement, there remains substantial room\nfor progress on Text-to-Pipeline. Our data, codes, and evaluation tools are\navailable at https://anonymous.4open.science/r/Text-to-Pipeline.", "published": "2025-05-21 15:40:53", "link": "http://arxiv.org/abs/2505.15874v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search", "abstract": "Nearest neighbor search is central in machine learning, information\nretrieval, and databases. For high-dimensional datasets, graph-based methods\nsuch as HNSW, DiskANN, and NSG have become popular thanks to their empirical\naccuracy and efficiency. These methods construct a directed graph over the\ndataset and perform beam search on the graph to find nodes close to a given\nquery. While significant work has focused on practical refinements and\ntheoretical understanding of graph-based methods, many questions remain. We\npropose a new distance-based termination condition for beam search to replace\nthe commonly used condition based on beam width. We prove that, as long as the\nsearch graph is navigable, our resulting Adaptive Beam Search method is\nguaranteed to approximately solve the nearest-neighbor problem, establishing a\nconnection between navigability and the performance of graph-based search. We\nalso provide extensive experiments on our new termination condition for both\nnavigable graphs and approximately navigable graphs used in practice, such as\nHNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard\nbeam search over a range of recall values, data sets, graph constructions, and\ntarget number of nearest neighbors. It thus provides a simple and practical way\nto improve the performance of popular methods.", "published": "2025-05-21 15:18:53", "link": "http://arxiv.org/abs/2505.15636v1", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.LG"], "primary_category": "cs.IR"}
{"title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research.", "published": "2025-05-21 14:44:40", "link": "http://arxiv.org/abs/2505.15872v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MIRB: Mathematical Information Retrieval Benchmark", "abstract": "Mathematical Information Retrieval (MIR) is the task of retrieving\ninformation from mathematical documents and plays a key role in various\napplications, including theorem search in mathematical libraries, answer\nretrieval on math forums, and premise selection in automated theorem proving.\nHowever, a unified benchmark for evaluating these diverse retrieval tasks has\nbeen lacking. In this paper, we introduce MIRB (Mathematical Information\nRetrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\nincludes four tasks: semantic statement retrieval, question-answer retrieval,\npremise retrieval, and formula retrieval, spanning a total of 12 datasets. We\nevaluate 13 retrieval models on this benchmark and analyze the challenges\ninherent to MIR. We hope that MIRB provides a comprehensive framework for\nevaluating MIR systems and helps advance the development of more effective\nretrieval models tailored to the mathematical domain.", "published": "2025-05-21 14:40:27", "link": "http://arxiv.org/abs/2505.15585v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Do RAG Systems Suffer From Positional Bias?", "abstract": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.", "published": "2025-05-21 14:18:01", "link": "http://arxiv.org/abs/2505.15561v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning", "abstract": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.", "published": "2025-05-21 13:27:14", "link": "http://arxiv.org/abs/2505.15507v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "20-XX, 08A02", "F.4.1; I.2"], "primary_category": "cs.LG"}
{"title": "Reranking with Compressed Document Representation", "abstract": "Reranking, the process of refining the output of a first-stage retriever, is\noften considered computationally expensive, especially with Large Language\nModels. Borrowing from recent advances in document compression for RAG, we\nreduce the input size by compressing documents into fixed-size embedding\nrepresentations. We then teach a reranker to use compressed inputs by\ndistillation. Although based on a billion-size model, our trained reranker\nusing this compressed input can challenge smaller rerankers in terms of both\neffectiveness and efficiency, especially for long documents. Given that text\ncompressors are still in their early development stages, we view this approach\nas promising.", "published": "2025-05-21 11:35:11", "link": "http://arxiv.org/abs/2505.15394v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "abstract": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.", "published": "2025-05-21 07:38:45", "link": "http://arxiv.org/abs/2505.15210v1", "categories": ["cs.CL", "cs.IR", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Robust Relevance Feedback for Interactive Known-Item Video Search", "abstract": "Known-item search (KIS) involves only a single search target, making\nrelevance feedback-typically a powerful technique for efficiently identifying\nmultiple positive examples to infer user intent-inapplicable. PicHunter\naddresses this issue by asking users to select the top-k most similar examples\nto the unique search target from a displayed set. Under ideal conditions, when\nthe user's perception aligns closely with the machine's perception of\nsimilarity, consistent and precise judgments can elevate the target to the top\nposition within a few iterations. However, in practical scenarios, expecting\nusers to provide consistent judgments is often unrealistic, especially when the\nunderlying embedding features used for similarity measurements lack\ninterpretability. To enhance robustness, we first introduce a pairwise relative\njudgment feedback that improves the stability of top-k selections by mitigating\nthe impact of misaligned feedback. Then, we decompose user perception into\nmultiple sub-perceptions, each represented as an independent embedding space.\nThis approach assumes that users may not consistently align with a single\nrepresentation but are more likely to align with one or several among multiple\nrepresentations. We develop a predictive user model that estimates the\ncombination of sub-perceptions based on each user feedback instance. The\npredictive user model is then trained to filter out the misaligned\nsub-perceptions. Experimental evaluations on the large-scale open-domain\ndataset V3C indicate that the proposed model can optimize over 60% search\ntargets to the top rank when their initial ranks at the search depth between 10\nand 50. Even for targets initially ranked between 1,000 and 5,000, the model\nachieves a success rate exceeding 40% in optimizing ranks to the top,\ndemonstrating the enhanced robustness of relevance feedback in KIS despite\ninconsistent feedback.", "published": "2025-05-21 05:31:49", "link": "http://arxiv.org/abs/2505.15128v1", "categories": ["cs.IR", "cs.MM"], "primary_category": "cs.IR"}
{"title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.", "published": "2025-05-21 05:09:43", "link": "http://arxiv.org/abs/2505.15117v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization", "abstract": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our\nimplementation is publicly available at\nhttps://github.com/zxh20001117/StepSearch.", "published": "2025-05-21 05:01:31", "link": "http://arxiv.org/abs/2505.15107v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AutoData: A Multi-Agent System for Open Web Data Collection", "abstract": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.", "published": "2025-05-21 04:32:35", "link": "http://arxiv.org/abs/2505.15859v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "ThinkRec: Thinking-based recommendation via LLM", "abstract": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.", "published": "2025-05-21 04:25:18", "link": "http://arxiv.org/abs/2505.15091v2", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "MoTime: A Dataset Suite for Multimodal Time Series Forecasting", "abstract": "While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research.", "published": "2025-05-21 03:39:42", "link": "http://arxiv.org/abs/2505.15072v1", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.LG"}
{"title": "An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc", "abstract": "Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,\nwhich need to be sparse to leverage inverted index structures during retrieval.\nSPLADE, the most popular LSR model, uses FLOPS regularization to encourage\nvector sparsity during training. However, FLOPS regularization does not ensure\nsparsity among terms - only within a given query or document. Terms with very\nhigh Document Frequencies (DFs) substantially increase latency in production\nretrieval engines, such as Apache Solr, due to their lengthy posting lists. To\naddress the issue of high DFs, we present a new variant of FLOPS\nregularization: DF-FLOPS. This new regularization technique penalizes the usage\nof high-DF terms, thereby shortening posting lists and reducing retrieval\nlatency. Unlike other inference-time sparsification methods, such as stopword\nremoval, DF-FLOPS regularization allows for the selective inclusion of\nhigh-frequency terms in cases where the terms are truly salient. We find that\nDF-FLOPS successfully reduces the prevalence of high-DF terms and lowers\nretrieval latency (around 10x faster) in a production-grade engine while\nmaintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and\ncross-domain (improved performance in 12 out of 13 tasks on which we tested).\nWith retrieval latencies on par with BM25, this work provides an important step\ntowards making LSR practical for deployment in production-grade search engines.", "published": "2025-05-21 03:35:51", "link": "http://arxiv.org/abs/2505.15070v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "GitHub Repository Complexity Leads to Diminished Web Archive Availability", "abstract": "Software is often developed using versioned controlled software, such as Git,\nand hosted on centralized Web hosts, such as GitHub and GitLab. These Web\nhosted software repositories are made available to users in the form of\ntraditional HTML Web pages for each source file and directory, as well as a\npresentational home page and various descriptive pages. We examined more than\n12,000 Web hosted Git repository project home pages, primarily from GitHub, to\nmeasure how well their presentational components are preserved in the Internet\nArchive, as well as the source trees of the collected GitHub repositories to\nassess the extent to which their source code has been preserved. We found that\nmore than 31% of the archived repository home pages examined exhibited some\nform of minor page damage and 1.6% exhibited major page damage. We also found\nthat of the source trees analyzed, less than 5% of their source files were\narchived, on average, with the majority of repositories not having source files\nsaved in the Internet Archive at all. The highest concentration of archived\nsource files available were those linked directly from repositories' home pages\nat a rate of 14.89% across all available repositories and sharply dropping off\nat deeper levels of a repository's directory tree.", "published": "2025-05-21 02:51:30", "link": "http://arxiv.org/abs/2505.15042v1", "categories": ["cs.DL", "cs.IR", "cs.SE"], "primary_category": "cs.DL"}
{"title": "Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI", "abstract": "Peer review is vital in academia for evaluating research quality. Top AI\nconferences use reviewer confidence scores to ensure review reliability, but\nexisting studies lack fine-grained analysis of text-score consistency,\npotentially missing key details. This work assesses consistency at word,\nsentence, and aspect levels using deep learning and NLP conference review data.\nWe employ deep learning to detect hedge sentences and aspects, then analyze\nreport length, hedge word/sentence frequency, aspect mentions, and sentiment to\nevaluate text-score alignment. Correlation, significance, and regression tests\nexamine confidence scores' impact on paper outcomes. Results show high\ntext-score consistency across all levels, with regression revealing higher\nconfidence scores correlate with paper rejection, validating expert assessments\nand peer review fairness.", "published": "2025-05-21 02:26:47", "link": "http://arxiv.org/abs/2505.15031v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CRAFT: Training-Free Cascaded Retrieval for Tabular QA", "abstract": "Table Question Answering (TQA) involves retrieving relevant tables from a\nlarge corpus to answer natural language queries. Traditional dense retrieval\nmodels, such as DTR and ColBERT, not only incur high computational costs for\nlarge-scale retrieval tasks but also require retraining or fine-tuning on new\ndatasets, limiting their adaptability to evolving domains and knowledge. In\nthis work, we propose $\\textbf{CRAFT}$, a cascaded retrieval approach that\nfirst uses a sparse retrieval model to filter a subset of candidate tables\nbefore applying more computationally expensive dense models and neural\nre-rankers. Our approach achieves better retrieval performance than\nstate-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further\nenhance table representations by generating table descriptions and titles using\nGemini Flash 1.5. End-to-end TQA results using various Large Language Models\n(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate\n$\\textbf{CRAFT}$ effectiveness.", "published": "2025-05-21 00:09:34", "link": "http://arxiv.org/abs/2505.14984v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Age-Energy Analysis in Multi-Source Systems with Wake-up Control and Packet Management", "abstract": "In recent years, there has been an increasing focus on real-time mobile\napplications, such as news updates and weather forecast. In these applications,\ndata freshness is of significant importance, which can be measured by\nage-of-synchronization (AoS). At the same time, the reduction of carbon\nemission is increasingly required by the communication operators. Thus, how to\nreduce energy consumption while keeping the data fresh becomes a matter of\nconcern. In this paper, we study the age-energy trade-off in a multi-source\nsingle-server system, where the server can turn to sleep mode to save energy.\nWe adopt the stochastic hybrid system (SHS) method to analyze the average AoS\nand power consumption with three wake-up policies including N-policy,\nsingle-sleep policy and multi-sleep policy, and three packet preemption\nstrategies, including Last-Come-First-Serve with preemption-in-Service\n(LCFS-S), LCFS with preemption-only-in-Waiting (LCFS-W), and LCFS with\npreemption-and-Queueing (LCFS-Q). The trade-off performance is analyzed via\nboth closed-form expressions and numerical simulations. It is found that\nN-policy attains the best trade-off performance among all three sleep policies.\nAmong packet management strategies, LCFS-S is suitable for scenarios with high\nrequirements on energy saving and small arrival rate difference between\nsources. LCFS-Q is suitable for scenarios with high requirements on information\nfreshness and large arrival rate difference between sources.", "published": "2025-05-21 23:17:57", "link": "http://arxiv.org/abs/2505.16073v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Directional Sparsity Based Statistical Channel Estimation for 6D Movable Antenna Communications", "abstract": "Six-dimensional movable antenna (6DMA) is an innovative and transformative\ntechnology to improve wireless network capacity by adjusting the 3D positions\nand 3D rotations of antennas/surfaces (sub-arrays) based on the channel spatial\ndistribution. For optimization of the antenna positions and rotations, the\nacquisition of statistical channel state information (CSI) is essential for\n6DMA systems. In this paper, we unveil for the first time a new\n\\textbf{\\textit{directional sparsity}} property of the 6DMA channels between\nthe base station (BS) and the distributed users, where each user has\nsignificant channel gains only with a (small) subset of 6DMA position-rotation\npairs, which can receive direct/reflected signals from the user. By exploiting\nthis property, a covariance-based algorithm is proposed for estimating the\nstatistical CSI in terms of the average channel power at a small number of 6DMA\npositions and rotations. Based on such limited channel power estimation, the\naverage channel powers for all possible 6DMA positions and rotations in the BS\nmovement region are reconstructed by further estimating the multi-path average\npower and direction-of-arrival (DOA) vectors of all users. Simulation results\nshow that the proposed directional sparsity-based algorithm can achieve higher\nchannel power estimation accuracy than existing benchmark schemes, while\nrequiring a lower pilot overhead.", "published": "2025-05-21 19:05:01", "link": "http://arxiv.org/abs/2505.15947v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Linearized Polynomial Chinese remainder codes", "abstract": "In this paper, we introduce a new family of codes relevent for rank and\nsum-rank metrics. These codes are based on an effective Chinese remainders\ntheorem for linearized polynomials over finite fields. We propose a decoding\nalgorithm for some instances of these codes.", "published": "2025-05-21 16:29:11", "link": "http://arxiv.org/abs/2505.15720v1", "categories": ["math.RA", "cs.IT", "cs.SC", "math.IT", "94B05, 94B35, 94B70, 11T71, 11T55", "G.2; H.1.1; E.4"], "primary_category": "math.RA"}
{"title": "Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima", "abstract": "We study the problem of best-arm identification in stochastic multi-armed\nbandits under the fixed-confidence setting, with a particular focus on\ninstances that admit multiple optimal arms. While the Track-and-Stop algorithm\nof Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,\nits performance in the presence of multiple optima has remained insufficiently\nunderstood. In this work, we revisit the Track-and-Stop strategy and propose a\nmodified stopping rule that ensures instance-optimality even when the set of\noptimal arms is not a singleton. Our analysis introduces a new\ninformation-theoretic lower bound that explicitly accounts for multiple optimal\narms, and we demonstrate that our stopping rule tightly matches this bound.", "published": "2025-05-21 15:22:37", "link": "http://arxiv.org/abs/2505.15643v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Evaluation of Mobile Environment for Vehicular Visible Light Communication Using Multiple LEDs and Event Cameras", "abstract": "In the fields of Advanced Driver Assistance Systems (ADAS) and Autonomous\nDriving (AD), sensors that serve as the ``eyes'' for sensing the vehicle's\nsurrounding environment are essential. Traditionally, image sensors and LiDAR\nhave played this role. However, a new type of vision sensor, event cameras, has\nrecently attracted attention. Event cameras respond to changes in the\nsurrounding environment (e.g., motion), exhibit strong robustness against\nmotion blur, and perform well in high dynamic range environments, which are\ndesirable in robotics applications. Furthermore, the asynchronous and\nlow-latency principles of data acquisition make event cameras suitable for\noptical communication. By adding communication functionality to event cameras,\nit becomes possible to utilize I2V communication to immediately share\ninformation about forward collisions, sudden braking, and road conditions,\nthereby contributing to hazard avoidance. Additionally, receiving information\nsuch as signal timing and traffic volume enables speed adjustment and optimal\nroute selection, facilitating more efficient driving. In this study, we\nconstruct a vehicle visible light communication system where event cameras are\nreceivers, and multiple LEDs are transmitters. In driving scenes, the system\ntracks the transmitter positions and separates densely packed LED light sources\nusing pilot sequences based on Walsh-Hadamard codes. As a result, outdoor\nvehicle experiments demonstrate error-free communication under conditions where\nthe transmitter-receiver distance was within 40 meters and the vehicle's\ndriving speed was 30 km/h (8.3 m/s).", "published": "2025-05-21 11:54:56", "link": "http://arxiv.org/abs/2505.15412v1", "categories": ["cs.RO", "cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.RO"}
{"title": "Phasebook: A Survey of Selected Open Problems in Phase Retrieval", "abstract": "Phase retrieval is an inverse problem that, on one hand, is crucial in many\napplications across imaging and physics, and, on the other hand, leads to deep\nresearch questions in theoretical signal processing and applied harmonic\nanalysis. This survey paper is an outcome of the recent workshop Phase\nRetrieval in Mathematics and Applications (PRiMA) (held on August 5--9 2024 at\nthe Lorentz Center in Leiden, The Netherlands) that brought together experts\nworking on theoretical and practical aspects of the phase retrieval problem\nwith the purpose to formulate and explore essential open problems in the field.", "published": "2025-05-21 10:22:55", "link": "http://arxiv.org/abs/2505.15351v1", "categories": ["cs.IT", "math.IT", "math.OC"], "primary_category": "cs.IT"}
{"title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens", "abstract": "Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models.", "published": "2025-05-21 09:34:27", "link": "http://arxiv.org/abs/2505.15303v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "A Unified Approach to Quantum Contraction and Correlation Coefficients", "abstract": "In classical information theory, the maximal correlation coefficient is used\nto establish strong limits on distributed processing. Through its relation to\nthe $\\chi^{2}$-contraction coefficient, it also establishes fundamental bounds\non sequential processing. Two distinct quantum extensions of the maximal\ncorrelation coefficient have been introduced to recover these two scenarios,\nbut they do not recover the entire classical framework. We introduce a family\nof non-commutative $L^{2}(p)$ spaces induced by operator monotone functions\nfrom which families of quantum maximal correlation coefficients and the quantum\n$\\chi^{2}$-divergences can be identified. Through this framework, we lift the\nclassical results to the quantum setting. For distributed processing, using our\nquantum maximal correlation coefficients, we establish strong limits on\nconverting quantum states under local operations. For sequential processing, we\nclarify the relation between the data processing inequality of quantum maximal\ncorrelation coefficients, $\\chi^{2}$-contraction coefficients, and\n$f$-divergences. Moreover, we establish the quantum maximal correlation\ncoefficients and $\\chi^{2}$-contraction coefficients are often computable via\nlinear algebraic methods, which in particular implies a method for obtaining\nrigorous, computable upper bounds for time-homogeneous quantum Markov chains\nwith a unique, full rank fixed point.", "published": "2025-05-21 08:58:45", "link": "http://arxiv.org/abs/2505.15281v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Fiber Nonlinearity Mitigation in Coherent Optical Systems", "abstract": "Fiber nonlinearity represents a critical challenge to the capacity\nenhancement of modern optical communication systems. In recent years,\nsignificant research efforts have focused on mitigating its impact through two\ncomplementary approaches. On the one hand, researchers have investigated\npractical digital signal processing (DSP) techniques to mitigate or compensate\nfor nonlinear impairments, such as reversing fiber propagation effects through\ndigital backpropagation (DBP). However, the high computational complexity of\nthese techniques often discourages their practical implementation. On the other\nhand, information-theoretic studies have sought to establish the capacity\nlimits of the nonlinear optical fiber channel, providing a framework for\nevaluating the ultimate performance of existing optical networks and guiding\nthe design of next-generation systems. This work reviews recent advances and\nproposes future directions for nonlinearity compensation and mitigation,\nincluding constellation shaping techniques and low-complexity DBP. Furthermore,\nit highlights the potential of these innovations both in advancing the\ntheoretical understanding of fiber capacity limits and in enabling practical\nDSP implementations.", "published": "2025-05-21 08:47:02", "link": "http://arxiv.org/abs/2505.15268v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Experimental Evaluation of Multiple Active RISs for 5G MIMO Commercial Networks", "abstract": "While numerous experimental studies have demonstrated the feasibility of\nreconfigurable intelligent surface (RIS) technology, most have primarily\nfocused on extending coverage. In contrast, this paper presents an experimental\nevaluation of multiple active RISs deployed in a 5G multiple-input\nmultiple-output (MIMO) commercial network, emphasizing enhancements in channel\nrank and throughput. We propose a low-complexity, codebook-based beamforming\nalgorithm specifically tailored for multi-RIS configurations, which diversifies\ndirectional channels and reduces reliance on explicit channel state\ninformation. Field tests using a commercial base station and user equipment\nreveal that the multi-RIS system can improve channel rank and throughput by up\nto 14% compared to single-RIS deployments, while maintaining low computational\ncomplexity. These findings underscore the practical benefits of active\nmulti-RIS systems for next-generation networks.", "published": "2025-05-21 08:23:30", "link": "http://arxiv.org/abs/2505.15247v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Performance Analysis of Fluid Antenna System under Spatially-Correlated Rician Fading Channels", "abstract": "Fluid antenna systems (FAS) are among the most promising technologies for the\nsixth generation (6G) mobile communication networks. Unlike traditional\nfixed-position multiple-input multiple-output (MIMO) systems, a FAS possesses\nposition reconfigurability to switch on-demand among $N$ predefined ports over\na prescribed space. This paper explores the performance of a single-input\nsingle-output (SISO) model with a fixed-position antenna transmitter and a\nsingle-antenna FAS receiver, referred to as the Rx-SISO-FAS model, under\nspatially-correlated Rician fading channels. Our contributions include exact\nexpressions and closed-form bounds for the outage probability of the\nRx-SISO-FAS model, as well as exact and closed-form lower bounds for the\nergodic rate. Importantly, we also analyze the performance considering both\nuniform linear array (ULA) and uniform planar array (UPA) configurations for\nthe ports of the FAS. To gain insights, we evaluate the diversity order of the\nproposed model and our analytical results indicate that with a fixed overall\nsystem size, increasing the number of ports, $N$, significantly decreases the\noutage performance of FAS under different Rician fading factors. Our numerical\nresults further demonstrate that: $i)$ the Rx-SISO-FAS model can enhance\nperformance under spatially-correlated Rician fading channels over the\nfixed-position antenna counterpart; $ii)$ the Rician factor negatively impacts\nperformance in the low signal-to-noise ratio (SNR) regime; $iii$) FAS can\noutperform an $L$ branches maximum ratio combining (MRC) system under Rician\nfading channels; and $iv)$ when the number of ports is identical, UPA\noutperforms ULA.", "published": "2025-05-21 07:26:08", "link": "http://arxiv.org/abs/2505.15200v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Cost-aware LLM-based Online Dataset Annotation", "abstract": "Recent advances in large language models (LLMs) have enabled automated\ndataset labeling with minimal human supervision. While majority voting across\nmultiple LLMs can improve label reliability by mitigating individual model\nbiases, it incurs high computational costs due to repeated querying. In this\nwork, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),\nfor efficient and accurate LLM-based dataset annotation. CaMVo adaptively\nselects a subset of LLMs for each data instance based on contextual embeddings,\nbalancing confidence and cost without requiring pre-training or ground-truth\nlabels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator\nover confidence scores, CaMVo estimates a lower bound on labeling accuracy for\neach LLM and aggregates responses through weighted majority voting. Our\nempirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates\nthat CaMVo achieves comparable or superior accuracy to full majority voting\nwhile significantly reducing labeling costs. This establishes CaMVo as a\npractical and robust solution for cost-efficient annotation in dynamic labeling\nenvironments.", "published": "2025-05-21 04:49:44", "link": "http://arxiv.org/abs/2505.15101v1", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications", "abstract": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs.", "published": "2025-05-21 16:48:28", "link": "http://arxiv.org/abs/2505.15741v1", "categories": ["cs.NE", "cs.CL", "cs.MA", "I.2.7; I.2.11"], "primary_category": "cs.NE"}
{"title": "SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer", "abstract": "Swarm robotic trajectory planning faces challenges in computational\nefficiency, scalability, and safety, particularly in complex, obstacle-dense\nenvironments. To address these issues, we propose SwarmDiff, a hierarchical and\nscalable generative framework for swarm robots. We model the swarm's\nmacroscopic state using Probability Density Functions (PDFs) and leverage\nconditional diffusion models to generate risk-aware macroscopic trajectory\ndistributions, which then guide the generation of individual robot trajectories\nat the microscopic level. To ensure a balance between the swarm's optimal\ntransportation and risk awareness, we integrate Wasserstein metrics and\nConditional Value at Risk (CVaR). Additionally, we introduce a Diffusion\nTransformer (DiT) to improve sampling efficiency and generation quality by\ncapturing long-range dependencies. Extensive simulations and real-world\nexperiments demonstrate that SwarmDiff outperforms existing methods in\ncomputational efficiency, trajectory validity, and scalability, making it a\nreliable solution for swarm robotic trajectory planning.", "published": "2025-05-21 15:56:55", "link": "http://arxiv.org/abs/2505.15679v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Object-centric Processes with Structured Data and Exact Synchronization (Extended Version)", "abstract": "Real-world processes often involve interdependent objects that also carry\ndata values, such as integers, reals, or strings. However, existing process\nformalisms fall short to combine key modeling features, such as tracking object\nidentities, supporting complex datatypes, handling dependencies among them, and\nobject-aware synchronization. Object-centric Petri nets with identifiers\n(OPIDs) partially address these needs but treat objects as unstructured\nidentifiers (e.g., order and item IDs), overlooking the rich semantics of\ncomplex data values (e.g., item prices or other attributes). To overcome these\nlimitations, we introduce data-aware OPIDs (DOPIDs), a framework that strictly\nextends OPIDs by incorporating structured data manipulation capabilities, and\nfull synchronization mechanisms. In spite of the expressiveness of the model,\nwe show that it can be made operational: Specifically, we define a novel\nconformance checking approach leveraging satisfiability modulo theories (SMT)\nto compute data-aware object-centric alignments.", "published": "2025-05-21 11:52:11", "link": "http://arxiv.org/abs/2505.15409v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments", "abstract": "As robots are increasingly deployed to collaborate on tasks within shared\nworkspaces and resources, the failure of an individual robot can critically\naffect the group's performance. This issue is particularly challenging when\nrobots lack global information or direct communication, relying instead on\nsocial interaction for coordination and to complete their tasks. In this study,\nwe propose a novel fault-tolerance technique leveraging physical contact\ninteractions in multi-robot systems, specifically under conditions of limited\nsensing and spatial confinement. We introduce the \"Active Contact Response\"\n(ACR) method, where each robot modulates its behavior based on the likelihood\nof encountering an inoperative (faulty) robot. Active robots are capable of\ncollectively repositioning stationary and faulty peers to reduce obstructions\nand maintain optimal group functionality. We implement our algorithm in a team\nof autonomous robots, equipped with contact-sensing and collision-tolerance\ncapabilities, tasked with collectively excavating cohesive model pellets.\nExperimental results indicate that the ACR method significantly improves the\nsystem's recovery time from robot failures, enabling continued collective\nexcavation with minimal performance degradation. Thus, this work demonstrates\nthe potential of leveraging local, social, and physical interactions to enhance\nfault tolerance and coordination in multi-robot systems operating in\nconstrained and extreme environments.", "published": "2025-05-21 02:43:36", "link": "http://arxiv.org/abs/2505.15036v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions", "abstract": "Social organisms which construct nests consisting of tunnels and chambers\nnecessarily navigate confined and crowded conditions. Unlike low-density\ncollectives like bird flocks and insect swarms, in which hydrodynamic and\nstatistical phenomena dominate, the physics of glasses and supercooled fluids\nis important to understand clogging behaviors in high-density collectives. Our\nprevious work revealed that fire ants flowing in confined tunnels utilize\ndiverse behaviors like unequal workload distributions, spontaneous direction\nreversals, and limited interaction times to mitigate clogging and jamming and\nthus maintain functional flow; implementation of similar rules in a small\nrobophysical swarm led to high performance through spontaneous dissolution of\nclogs and clusters. However, how the insects learn such behaviors, and how we\ncan develop \"task capable\" active matter in such regimes, remains a challenge\nin part because interaction dynamics are dominated by local, time-consuming\ncollisions and no single agent can guide the entire collective. Here, we\nhypothesized that effective flow and clog mitigation could emerge purely\nthrough local learning. We tasked small groups of robots with pellet excavation\nin a narrow tunnel, allowing them to modify reversal probabilities over time.\nInitially, robots had equal probabilities and clogs were common. Reversals\nimproved flow. When reversal probabilities adapted via collisions and noisy\ntunnel length estimates, workload inequality and performance improved. Our\nrobophysical study of an excavating swarm shows that, despite the seeming\ncomplexity and difficulty of the task, simple learning rules can mitigate or\nleverage unavoidable features in task-capable dense active matter, leading to\nhypotheses for dense biological and robotic swarms.", "published": "2025-05-21 02:42:32", "link": "http://arxiv.org/abs/2505.15033v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Regularizing Ill-Posed Inverse Problems: Deblurring Barcodes", "abstract": "This manuscript is designed to introduce students in applied mathematics and\ndata science to the concept of regularization for ill-posed inverse problems.\nConstruct a mathematical model that describes how an image gets blurred.\nConvert a calculus problem into a linear algebra problem by discretization.\nInverting the blurring process should sharpen up an image; this requires the\nsolution of a system of linear algebraic equations. Solving this linear system\nof equations turns out to be delicate, as deblurring is an example of an\nill-posed inverse problem. To address this challenge, recast the system as a\nregularized least squares problem (also known as ridge regression).", "published": "2025-05-21 21:55:40", "link": "http://arxiv.org/abs/2505.16045v1", "categories": ["math.NA", "cs.NA", "65F22, 45Q05"], "primary_category": "math.NA"}
{"title": "CUR Matrix Approximation through Convex Optimization for Feature Selection", "abstract": "The singular value decomposition (SVD) is commonly used in applications\nrequiring a low rank matrix approximation. However, the singular vectors cannot\nbe interpreted in terms of the original data. For applications requiring this\ntype of interpretation, e.g., selection of important data matrix columns or\nrows, the approximate CUR matrix factorization can be used. Work on the CUR\nmatrix approximation has generally focused on algorithm development,\ntheoretical guarantees, and applications. In this work, we present a novel\ndeterministic CUR formulation and algorithm with theoretical convergence\nguarantees. The algorithm utilizes convex optimization, finds important columns\nand rows separately, and allows the user to control the number of important\ncolumns and rows selected from the original data matrix. We present numerical\nresults and demonstrate the effectiveness of our CUR algorithm as a feature\nselection method on gene expression data. These results are compared to those\nusing the SVD and other CUR algorithms as the feature selection method. Lastly,\nwe present a novel application of CUR as a feature selection method to\ndetermine discriminant proteins when clustering protein expression data in a\nself-organizing map (SOM), and compare the performance of multiple CUR\nalgorithms in this application.", "published": "2025-05-21 21:24:36", "link": "http://arxiv.org/abs/2505.16032v1", "categories": ["math.NA", "cs.NA", "65F55, 65K05"], "primary_category": "math.NA"}
{"title": "Locally Subspace-Informed Neural Operators for Efficient Multiscale PDE Solving", "abstract": "Neural operators (NOs) struggle with high-contrast multiscale partial\ndifferential equations (PDEs), where fine-scale heterogeneities cause large\nerrors. To address this, we use the Generalized Multiscale Finite Element\nMethod (GMsFEM) that constructs localized spectral basis functions on coarse\ngrids. This approach efficiently captures dominant multiscale features while\nsolving heterogeneous PDEs accurately at reduced computational cost. However,\ncomputing these basis functions is computationally expensive. This gap\nmotivates our core idea: to use a NO to learn the subspace itself - rather than\nindividual basis functions - by employing a subspace-informed loss. On standard\nmultiscale benchmarks - namely a linear elliptic diffusion problem and the\nnonlinear, steady-state Richards equation - our hybrid method cuts solution\nerror by approximately $60\\%$ compared with standalone NOs and reduces\nbasis-construction time by about $60$ times relative to classical GMsFEM, while\nremaining independent of forcing terms and boundary conditions. The result\nfuses multiscale finite-element robustness with NO speed, yielding a practical\nsolver for heterogeneous PDEs.", "published": "2025-05-21 21:18:28", "link": "http://arxiv.org/abs/2505.16030v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A broken-FEEC framework for structure-preserving discretizations of polar domains with tensor-product splines", "abstract": "We propose a novel projection-based approach to derive structure-preserving\nFinite Element Exterior Calculus (FEEC) discretizations using standard\ntensor-product splines on domains with a polar singularity. This approach\nfollows the main lines of broken-FEEC schemes which define stable and\nstructure-preserving operators in non-conforming discretizations of the de Rham\nsequence. Here, we devise a polar broken-FEEC framework that enables the use of\nstandard tensor-product spline spaces while ensuring stability and smoothness\nfor the solutions, as well as the preservation of the de Rham structure: A\nbenefit of this approach is the ability to reuse codes that implement standard\nsplines on smooth parametric domains, and efficient solvers such as\nKronecker-product spline interpolation. Our construction is based on two\npillars: the first one is an explicit characterization of smooth polar spline\nspaces within the tensor-product splines ones, which are either discontinuous\nor non square-integrable as a result of the singular polar pushforward\noperators. The second pillar consists of local, explicit and matrix-free\nconforming projection operators that map general tensor-product splines onto\nsmooth polar splines, and that commute with the differential operators of the\nde Rham sequence.", "published": "2025-05-21 20:25:41", "link": "http://arxiv.org/abs/2505.15996v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "Fast-wave slow-wave spectral deferred correction methods applied to the compressible Euler equations", "abstract": "This paper investigates the application of a fast-wave slow-wave spectral\ndeferred correction time-stepping method (FWSW-SDC) to the compressible Euler\nequations. The resulting model achieves arbitrary order accuracy in time,\ndemonstrating robust performance in standard benchmark idealised test cases for\ndynamical cores used for numerical weather prediction. The model uses a\ncompatible finite element spatial discretisation, achieving good linear wave\ndispersion properties without spurious computational modes. A convergence test\nconfirms the model's high temporal accuracy. Arbitrarily high spatial-temporal\nconvergence is demonstrated using a gravity wave test case. The model is\nfurther extended to include the parametrisation of a simple physics process by\nadding two phases of moisture and its validity is demonstrated for a rising\nthermal problem. Finally, a baroclinic wave in simulated in a Cartesian domain.", "published": "2025-05-21 20:07:04", "link": "http://arxiv.org/abs/2505.15985v1", "categories": ["math.NA", "cs.NA", "physics.ao-ph"], "primary_category": "math.NA"}
{"title": "Improving the Predictability of the Madden-Julian Oscillation at Subseasonal Scales with Gaussian Process Models", "abstract": "The Madden--Julian Oscillation (MJO) is an influential climate phenomenon\nthat plays a vital role in modulating global weather patterns. In spite of the\nimprovement in MJO predictions made by machine learning algorithms, such as\nneural networks, most of them cannot provide the uncertainty levels in the MJO\nforecasts directly. To address this problem, we develop a nonparametric\nstrategy based on Gaussian process (GP) models. We calibrate GPs using\nempirical correlations and we propose a posteriori covariance correction.\nNumerical experiments demonstrate that our model has better prediction skills\nthan the ANN models for the first five lead days. Additionally, our posteriori\ncovariance correction extends the probabilistic coverage by more than three\nweeks.", "published": "2025-05-21 18:40:40", "link": "http://arxiv.org/abs/2505.15934v1", "categories": ["math.NA", "cs.NA", "physics.ao-ph", "stat.ML"], "primary_category": "math.NA"}
{"title": "Elasto-acoustic wave propagation in geophysical media using hybrid high-order methods on general meshes", "abstract": "Hybrid high-order (HHO) methods are numerical methods characterized by\nseveral interesting properties such as local conservativity, geometric\nflexibility and high-order accuracy. Here, HHO schemes are studied for the\nspace semi-discretization of coupled elasto-acoustic waves in the time domain\nusing a first-order formulation. Explicit and singly diagonal implicit\nRunge--Kutta (ERK & SDIRK) schemes are used for the time discretization. We\nshow that an efficient implementation of explicit (resp. implicit) time schemes\ncalls for a static condensation of the face (resp. cell) unknowns. Crucially,\nboth static condensation procedures only involve block-diagonal matrices. Then,\nwe provide numerical estimates for the CFL stability limit of ERK schemes and\npresent a comparative study on the efficiency of explicit versus implicit\nschemes. Our findings indicate that implicit time schemes remain competitive in\nmany situations. Finally, simulations in a 2D realistic geophysical\nconfiguration are performed, illustrating the geometrical flexibility of the\nHHO method: both hybrid (triangular and quadrangular) and nonconforming (with\nhanging nodes) meshes are easily handled, delivering results of comparable\naccuracy to a reference spectral element software based on tensorized elements.", "published": "2025-05-21 17:22:45", "link": "http://arxiv.org/abs/2505.15771v1", "categories": ["math.NA", "cs.CE", "cs.NA"], "primary_category": "math.NA"}
{"title": "Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms", "abstract": "Gradient-based learning imposes (deep) neural networks to be differentiable\nat all steps. This includes model-based architectures constructed by unrolling\niterations of an iterative algorithm onto layers of a neural network, known as\nalgorithm unrolling. However, greedy sparse recovery algorithms depend on the\nnon-differentiable argsort operator, which hinders their integration into\nneural networks. In this paper, we address this challenge in Orthogonal\nMatching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular\nrepresentative algorithms in this class. We propose permutation-based variants\nof these algorithms and approximate permutation matrices using \"soft\"\npermutation matrices derived from softsort, a continuous relaxation of argsort.\nWe demonstrate -- both theoretically and numerically -- that Soft-OMP and\nSoft-IHT, as differentiable counterparts of OMP and IHT and fully compatible\nwith neural network training, effectively approximate these algorithms with a\ncontrollable degree of accuracy. This leads to the development of OMP- and\nIHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,\nrespectively. Finally, by choosing weights as \"structure-aware\" trainable\nparameters, we connect our approach to structured sparse recovery and\ndemonstrate its ability to extract latent sparsity patterns from data.", "published": "2025-05-21 15:36:38", "link": "http://arxiv.org/abs/2505.15661v1", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA"], "primary_category": "cs.LG"}
{"title": "Damping optimization of discrete mechanical systems -- rod/string model", "abstract": "This paper investigates two optimization criteria for damping optimization in\na multi-body oscillator system with arbitrary degrees of freedom ($n$),\nresembling string/rod free vibrations. The total average energy over all\npossible initial data and the total average displacement over all possible\ninitial data. Our first result shows that both criteria are equivalent to the\ntrace minimization of the solution of the Lyapunov equation with different\nright-hand sides. As the second result, we prove that in the case of damping\nwith one damper, for the discrete system, the minimal trace for each criterion\ncan be expressed as a linear or cubic function of the dimension $n$.\nConsequently, the optimal damping position is determined solely by the number\nof dominant eigenfrequencies and the optimal viscosity, independent of the\ndimension $n$, offering efficient damping optimization in discrete systems. The\npaper concludes with numerical examples illustrating the presented theoretical\nframework and results.", "published": "2025-05-21 15:19:59", "link": "http://arxiv.org/abs/2505.15640v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Improved power methods for computing eigenvalues of dual quaternion Hermitian matrices", "abstract": "This paper investigates the eigenvalue computation problem of the dual\nquaternion Hermitian matrix closely related to multi-agent group control.\nRecently, power method was proposed by Cui and Qi in Journal of Scientific\nComputing, 100 (2024) to solve such problem. Recognizing that the convergence\nrate of power method is slow due to its dependence on the eigenvalue\ndistribution, we propose two improved versions of power method based on dual\ncomplex adjoint matrices and Aitken extrapolation, named DCAM-PM and ADCAM-PM.\nThey achieve notable efficiency improvements and demonstrate significantly\nfaster convergence. However, power method may be invalid for dual quaternion\nHermitian matrices with eigenvalues having identical standard parts but\ndistinct dual parts. To overcome this disadvantage, utilizing the\neigen-decomposition properties of dual complex adjoint matrix, we propose a\nnovel algorithm EDDCAM-EA which surpasses the power method in both accuracy and\nspeed. Application to eigenvalue computations of dual quaternion Hermitian\nmatrices in multi-agent formation control and numerical experiments highlight\nthe remarkable accuracy and speed of our proposed algorithms.", "published": "2025-05-21 14:40:16", "link": "http://arxiv.org/abs/2505.15584v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Machine learning-based parameter optimization for M\u00fcntz spectral methods", "abstract": "Spectral methods employing non-standard polynomial bases, such as M\\\"untz\npolynomials, have proven effective for accurately solving problems with\nsolutions exhibiting low regularity, notably including sub-diffusion equations.\nHowever, due to the absence of theoretical guidance, the key parameters\ncontrolling the exponents of M\\\"untz polynomials are usually determined\nempirically through extensive numerical experiments, leading to a\ntime-consuming tuning process. To address this issue, we propose a novel\nmachine learning-based optimization framework for the M\\\"untz spectral method.\nAs an illustrative example, we optimize the parameter selection for solving\ntime-fractional partial differential equations (PDEs). Specifically, an\nartificial neural network (ANN) is employed to predict optimal parameter values\nbased solely on the time-fractional order as input. The ANN is trained by\nminimizing solution errors on a one-dimensional time-fractional\nconvection-diffusion equation featuring manufactured exact solutions that\nmanifest singularities of varying intensity, covering a comprehensive range of\nsampled fractional orders. Numerical results for time-fractional PDEs in both\none and two dimensions demonstrate that the ANN-based parameter prediction\nsignificantly improves the accuracy of the M\\\"untz spectral method. Moreover,\nthe trained ANN generalizes effectively from one-dimensional to two-dimensional\ncases, highlighting its robustness across spatial dimensions. Additionally, we\nverify that the ANN substantially outperforms traditional function\napproximators, such as spline interpolation, in both prediction accuracy and\ntraining efficiency. The proposed optimization framework can be extended beyond\nfractional PDEs, offering a versatile and powerful approach for spectral\nmethods applied to various low-regularity problems.", "published": "2025-05-21 13:58:24", "link": "http://arxiv.org/abs/2505.15538v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code Generation with No Performance Overhead", "abstract": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.", "published": "2025-05-21 13:56:16", "link": "http://arxiv.org/abs/2505.15535v1", "categories": ["math.NA", "cs.NA", "65M60, 74B20, 74S05"], "primary_category": "math.NA"}
{"title": "Efficient Differentiable Approximation of Generalized Low-rank Regularization", "abstract": "Low-rank regularization (LRR) has been widely applied in various machine\nlearning tasks, but the associated optimization is challenging. Directly\noptimizing the rank function under constraints is NP-hard in general. To\novercome this difficulty, various relaxations of the rank function were\nstudied. However, optimization of these relaxed LRRs typically depends on\nsingular value decomposition, which is a time-consuming and nondifferentiable\noperator that cannot be optimized with gradient-based techniques. To address\nthese challenges, in this paper we propose an efficient differentiable\napproximation of the generalized LRR. The considered LRR form subsumes many\npopular choices like the nuclear norm, the Schatten-$p$ norm, and various\nnonconvex relaxations. Our method enables LRR terms to be appended to loss\nfunctions in a plug-and-play fashion, and the GPU-friendly operations enable\nefficient and convenient implementation. Furthermore, convergence analysis is\npresented, which rigorously shows that both the bias and the variance of our\nrank estimator rapidly reduce with increased sample size and iteration steps.\nIn the experimental study, the proposed method is applied to various tasks,\nwhich demonstrates its versatility and efficiency. Code is available at\nhttps://github.com/naiqili/EDLRR.", "published": "2025-05-21 11:49:17", "link": "http://arxiv.org/abs/2505.15407v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Quantization of Probability Distributions via Divide-and-Conquer: Convergence and Error Propagation under Distributional Arithmetic Operations", "abstract": "This article studies a general divide-and-conquer algorithm for approximating\ncontinuous one-dimensional probability distributions with finite mean. The\narticle presents a numerical study that compares pre-existing approximation\nschemes with a special focus on the stability of the discrete approximations\nwhen they undergo arithmetic operations. The main results are a simple upper\nbound of the approximation error in terms of the Wasserstein-1 distance that is\nvalid for all continuous distributions with finite mean. In many use-cases, the\nstudied method achieve optimal rate of convergence, and numerical experiments\nshow that the algorithm is more stable than pre-existing approximation schemes\nin the context of arithmetic operations.", "published": "2025-05-21 09:03:14", "link": "http://arxiv.org/abs/2505.15283v1", "categories": ["math.PR", "cs.CE", "cs.NA", "math.NA"], "primary_category": "math.PR"}
{"title": "A coupled HDG discretization for the interaction between acoustic and elastic waves", "abstract": "We propose and analyze an HDG scheme for the Laplace-domain interaction\nbetween a transient acoustic wave and a bounded elastic solid embedded in an\nunbounded fluid medium. Two mixed variables (the stress tensor and the velocity\nof the acoustic wave) are included while the symmetry of the stress tensor is\nimposed weakly by considering the antisymmetric part of the strain tensor (the\nspin or vorticity tensor) as an additional unknown. The optimal convergence of\nthe method is demonstrated theoretically and numerical results confirming the\ntheoretical prediction are presented.", "published": "2025-05-21 04:59:43", "link": "http://arxiv.org/abs/2505.15106v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "74J05, 65M60, 65M15, 65M12"], "primary_category": "math.NA"}
{"title": "Runge-Kutta Methods and Stiff Order Conditions for Semilinear ODEs", "abstract": "Classical convergence theory of Runge-Kutta methods assumes that the time\nstep is small relative to the Lipschitz constant of the ordinary differential\nequation (ODE). For stiff problems, that assumption is often violated, and a\nproblematic degradation in accuracy, known as order reduction, can arise. High\nstage order methods can avoid order reduction, but they must be fully implicit.\nFor linear problems, weaker stiff order conditions exist and are compatible\nwith computationally efficient methods, i.e., explicit or diagonally implicit.\nThis work develops a new theory of stiff order conditions and convergence for\nsemilinear ODEs, consisting of a stiff linear term and a non-stiff nonlinear\nterm. New semilinear order conditions are formulated in terms of orthogonality\nrelations enumerated by rooted trees. Novel, optimized diagonally implicit\nmethods are constructed that satisfy these semilinear conditions. Numerical\nresults demonstrate that for a broad class of relevant nonlinear test problems,\nthese new methods successfully mitigate order reduction and yield highly\naccurate numerical approximations.", "published": "2025-05-21 04:40:21", "link": "http://arxiv.org/abs/2505.15099v1", "categories": ["math.NA", "cs.NA", "65L05, 65L06, 65L20, 65L70, 65M20"], "primary_category": "math.NA"}
{"title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "abstract": "Financial markets pose fundamental challenges for asset return prediction due\nto their high dimensionality, non-stationarity, and persistent volatility.\nDespite advances in large language models and multi-agent systems, current\nquantitative research pipelines suffer from limited automation, weak\ninterpretability, and fragmented coordination across key components such as\nfactor mining and model innovation. In this paper, we propose R&D-Agent for\nQuantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent\nframework designed to automate the full-stack research and development of\nquantitative strategies via coordinated factor-model co-optimization.\nRD-Agent(Q) decomposes the quant process into two iterative stages: a Research\nstage that dynamically sets goal-aligned prompts, formulates hypotheses based\non domain priors, and maps them to concrete tasks, and a Development stage that\nemploys a code-generation agent, Co-STEER, to implement task-specific code,\nwhich is then executed in real-market backtests. The two stages are connected\nthrough a feedback stage that thoroughly evaluates experimental outcomes and\ninforms subsequent iterations, with a multi-armed bandit scheduler for adaptive\ndirection selection. Empirically, RD-Agent(Q) achieves up to 2X higher\nannualized returns than classical factor libraries using 70% fewer factors, and\noutperforms state-of-the-art deep time-series models on real markets. Its joint\nfactor-model optimization delivers a strong balance between predictive accuracy\nand strategy robustness. Our code is available at:\nhttps://github.com/microsoft/RD-Agent.", "published": "2025-05-21 06:20:56", "link": "http://arxiv.org/abs/2505.15155v1", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Shortermism and excessive risk taking in optimal execution with a target performance", "abstract": "We deal with the optimal execution problem when the broker's goal is to reach\na performance barrier avoiding a downside barrier. The performance is provided\nby the wealth accumulated by trading in the market, the shares detained by the\nbroker evaluated at the market price plus a slippage cost yielding a quadratic\ninventory cost. Over a short horizon, this type of remuneration leads, at the\nsame time, to a more aggressive and less risky strategy compared to the\nclassical one, and over a long horizon the performance turns to be poorer and\nmore dispersed.", "published": "2025-05-21 15:02:07", "link": "http://arxiv.org/abs/2505.15611v1", "categories": ["q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Liquidity provision with $\u03c4$-reset strategies: a dynamic historical liquidity approach", "abstract": "Since the launch of Uniswap and other AMM protocols, the DeFi industry has\nevolved from simple constant product functions with uniform liquidity\ndistribution across the entire price axis to more advanced mechanisms that\nallow Liquidity Providers (LPs) to concentrate capital within selected price\nranges. This evolution has introduced new research challenges focused on\noptimizing capital allocation in Decentralized Exchanges (DEXs) under dynamic\nmarket conditions. In this paper, we present a methodology for finding optimal\nliquidity provision strategies in DEXs within a specific family of $\\tau$-reset\nstrategies. The approach is detailed step by step and includes an original\nmethod for approximating historical liquidity within active pool ranges using a\nparametric model that does not rely on historical liquidity data. We find\noptimal LP strategies using a machine learning approach, evaluate performance\nover an out-of-time period, and compare the resulting strategies against a\nuniform benchmark. All experiments were conducted using a custom backtesting\nframework specifically developed for Concentrated Liquidity Market Makers\n(CLMMs). The effectiveness and flexibility of the proposed methodology are\ndemonstrated across various Uniswap v3 trading pairs, and also benchmarked\nagainst an alternative backtesting and strategy development tool.", "published": "2025-05-21 10:09:29", "link": "http://arxiv.org/abs/2505.15338v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Quantile Predictions for Equity Premium using Penalized Quantile Regression with Consistent Variable Selection across Multiple Quantiles", "abstract": "This paper considers equity premium prediction, for which mean regression can\nbe problematic due to heteroscedasticity and heavy-tails of the error. We show\nadvantages of quantile predictions using a novel penalized quantile regression\nthat offers a model for a full spectrum analysis on the equity premium\ndistribution. To enhance model interpretability and address the well-known\nissue of crossing quantile predictions in quantile regression, we propose a\nmodel that enforces the selection of a common set of variables across all\nquantiles. Such a selection consistency is achieved by simultaneously\nestimating all quantiles with a group penalty that ensures sparsity pattern is\nthe same for all quantiles. Consistency results are provided that allow the\nnumber of predictors to increase with the sample size. A Huberized quantile\nloss function and an augmented data approach are implemented for computational\nefficiency. Simulation studies show the effectiveness of the proposed approach.\nEmpirical results show that the proposed method outperforms several benchmark\nmethods. Moreover, we find some important predictors reverse their relationship\nto the excess return from lower to upper quantiles, potentially offering\ninteresting insights to the domain experts. Our proposed method can be applied\nto other fields.", "published": "2025-05-21 21:05:08", "link": "http://arxiv.org/abs/2505.16019v1", "categories": ["stat.ME", "q-fin.ST", "stat.AP"], "primary_category": "stat.ME"}
{"title": "Agent-based Liquidity Risk Modelling for Financial Markets", "abstract": "In this paper, we describe a novel agent-based approach for modelling the\ntransaction cost of buying or selling an asset in financial markets, e.g., to\nliquidate a large position as a result of a margin call to meet financial\nobligations. The simple act of buying or selling in the market causes a price\nimpact and there is a cost described as liquidity risk. For example, when\nselling a large order, there is market slippage -- each successive trade will\nexecute at the same or worse price. When the market adjusts to the new\ninformation revealed by the execution of such a large order, we observe in the\ndata a permanent price impact that can be attributed to the change in the\nfundamental value as market participants reassess the value of the asset. In\nour ABM model, we introduce a novel mechanism where traders assume orderflow is\ninformed and each trade reveals some information about the value of the asset,\nand traders update their belief of the fundamental value for every trade. The\nresult is emergent, realistic price impact without oversimplifying the problem\nas most stylised models do, but within a realistic framework that models the\nexchange with its protocols, its limit orderbook and its auction mechanism and\nthat can calculate the transaction cost of any execution strategy without\nlimitation. Our stochastic ABM model calculates the costs and uncertainties of\nbuying and selling in a market by running Monte-Carlo simulations, for a better\nunderstanding of liquidity risk and can be used to optimise for optimal\nexecution under liquidity risk. We demonstrate its practical application in the\nreal world by calculating the liquidity risk for the Hang-Seng Futures Index.", "published": "2025-05-21 09:25:32", "link": "http://arxiv.org/abs/2505.15296v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schr\u00f6dinger Bridge's End", "abstract": "Scientists often want to make predictions beyond the observed time horizon of\n\"snapshot\" data following latent stochastic dynamics. For example, in time\ncourse single-cell mRNA profiling, scientists have access to cellular\ntranscriptional state measurements (snapshots) from different biological\nreplicates at different time points, but they cannot access the trajectory of\nany one cell because measurement destroys the cell. Researchers want to\nforecast (e.g.) differentiation outcomes from early state measurements of stem\ncells. Recent Schr\\\"odinger-bridge (SB) methods are natural for interpolating\nbetween snapshots. But past SB papers have not addressed forecasting -- likely\nsince existing methods either (1) reduce to following pre-set reference\ndynamics (chosen before seeing data) or (2) require the user to choose a fixed,\nstate-independent volatility since they minimize a Kullback-Leibler divergence.\nEither case can lead to poor forecasting quality. In the present work, we\npropose a new framework, SnapMMD, that learns dynamics by directly fitting the\njoint distribution of both state measurements and observation time with a\nmaximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to\ninfer unknown and state-dependent volatilities from the observed data. We show\nin a variety of real and synthetic experiments that our method delivers\naccurate forecasts. Moreover, our approach allows us to learn in the presence\nof incomplete state measurements and yields an $R^2$-style statistic that\ndiagnoses fit. We also find that our method's performance at interpolation (and\ngeneral velocity-field reconstruction) is at least as good as (and often better\nthan) state-of-the-art in almost all of our experiments.", "published": "2025-05-21 23:52:57", "link": "http://arxiv.org/abs/2505.16082v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Bidirectional Variational Autoencoders", "abstract": "We present the new bidirectional variational autoencoder (BVAE) network\narchitecture. The BVAE uses a single neural network both to encode and decode\ninstead of an encoder-decoder network pair. The network encodes in the forward\ndirection and decodes in the backward direction through the same synaptic web.\nSimulations compared BVAEs and ordinary VAEs on the four image tasks of image\nreconstruction, classification, interpolation, and generation. The image\ndatasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and\nCelebA-64 face images. The bidirectional structure of BVAEs cut the parameter\ncount by almost 50% and still slightly outperformed the unidirectional VAEs.", "published": "2025-05-21 23:19:43", "link": "http://arxiv.org/abs/2505.16074v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals", "abstract": "We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for\ncausal inference that jointly models potential outcomes and counterfactuals.\nTrained via flow matching, PO-Flow provides a unified framework for\nindividualized potential outcome prediction, counterfactual predictions, and\nuncertainty-aware density learning. Among generative models, it is the first to\nenable density learning of potential outcomes without requiring explicit\ndistributional assumptions (e.g., Gaussian mixtures), while also supporting\ncounterfactual prediction conditioned on factual outcomes in general\nobservational datasets. On benchmarks such as ACIC, IHDP, and IBM, it\nconsistently outperforms prior methods across a range of causal inference\ntasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including\ncounterfactual image generation, demonstrating its broad applicability.", "published": "2025-05-21 22:02:48", "link": "http://arxiv.org/abs/2505.16051v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "abstract": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models.", "published": "2025-05-21 21:34:18", "link": "http://arxiv.org/abs/2505.16037v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision", "abstract": "Learning complex functions that involve multi-step reasoning poses a\nsignificant challenge for standard supervised learning from input-output\nexamples. Chain-of-thought (CoT) supervision, which provides intermediate\nreasoning steps together with the final output, has emerged as a powerful\nempirical technique, underpinning much of the recent progress in the reasoning\ncapabilities of large language models. This paper develops a statistical theory\nof learning under CoT supervision. A key characteristic of the CoT setting, in\ncontrast to standard supervision, is the mismatch between the training\nobjective (CoT risk) and the test objective (end-to-end risk). A central part\nof our analysis, distinguished from prior work, is explicitly linking those two\ntypes of risk to achieve sharper sample complexity bounds. This is achieved via\nthe *CoT information measure* $\\mathcal{I}_{\\mathcal{D},\nh_\\star}^{\\mathrm{CoT}}(\\epsilon; \\calH)$, which quantifies the additional\ndiscriminative power gained from observing the reasoning process. The main\ntheoretical results demonstrate how CoT supervision can yield significantly\nfaster learning rates compared to standard E2E supervision. Specifically, it is\nshown that the sample complexity required to achieve a target E2E error\n$\\epsilon$ scales as $d/\\mathcal{I}_{\\mathcal{D},\nh_\\star}^{\\mathrm{CoT}}(\\epsilon; \\calH)$, where $d$ is a measure of hypothesis\nclass complexity, which can be much faster than standard $d/\\epsilon$ rates.\nInformation-theoretic lower bounds in terms of the CoT information are also\nobtained. Together, these results suggest that CoT information is a fundamental\nmeasure of statistical complexity for learning under chain-of-thought\nsupervision.", "published": "2025-05-21 18:28:54", "link": "http://arxiv.org/abs/2505.15927v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Last Layer Empirical Bayes", "abstract": "The task of quantifying the inherent uncertainty associated with neural\nnetwork predictions is a key challenge in artificial intelligence. Bayesian\nneural networks (BNNs) and deep ensembles are among the most prominent\napproaches to tackle this task. Both approaches produce predictions by\ncomputing an expectation of neural network outputs over some distribution on\nthe corresponding weights; this distribution is given by the posterior in the\ncase of BNNs, and by a mixture of point masses for ensembles. Inspired by\nrecent work showing that the distribution used by ensembles can be understood\nas a posterior corresponding to a learned data-dependent prior, we propose last\nlayer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a\nnormalizing flow, which is then trained to maximize the evidence lower bound;\nto retain tractability we use the flow only on the last layer. We show why LLEB\nis well motivated, and how it interpolates between standard BNNs and ensembles\nin terms of the strength of the prior that they use. LLEB performs on par with\nexisting approaches, highlighting that empirical Bayes is a promising direction\nfor future research in uncertainty quantification.", "published": "2025-05-21 18:00:00", "link": "http://arxiv.org/abs/2505.15888v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Conditional Transport Maps", "abstract": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability.", "published": "2025-05-21 17:59:02", "link": "http://arxiv.org/abs/2505.15808v1", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.AP", "stat.ML", "49Q22 (Primary) 68T07 (Secondary)", "I.5.1; I.2.0; G.3"], "primary_category": "cs.LG"}
{"title": "Are machine learning interpretations reliable? A stability study on global interpretations", "abstract": "As machine learning systems are increasingly used in high-stakes domains,\nthere is a growing emphasis placed on making them interpretable to improve\ntrust in these systems. In response, a range of interpretable machine learning\n(IML) methods have been developed to generate human-understandable insights\ninto otherwise black box models. With these methods, a fundamental question\narises: Are these interpretations reliable? Unlike with prediction accuracy or\nother evaluation metrics for supervised models, the proximity to the true\ninterpretation is difficult to define. Instead, we ask a closely related\nquestion that we argue is a prerequisite for reliability: Are these\ninterpretations stable? We define stability as findings that are consistent or\nreliable under small random perturbations to the data or algorithms. In this\nstudy, we conduct the first systematic, large-scale empirical stability study\non popular machine learning global interpretations for both supervised and\nunsupervised tasks on tabular data. Our findings reveal that popular\ninterpretation methods are frequently unstable, notably less stable than the\npredictions themselves, and that there is no association between the accuracy\nof machine learning predictions and the stability of their associated\ninterpretations. Moreover, we show that no single method consistently provides\nthe most stable interpretations across a range of benchmark datasets. Overall,\nthese results suggest that interpretability alone does not warrant trust, and\nunderscores the need for rigorous evaluation of interpretation stability in\nfuture work. To support these principles, we have developed and released an\nopen source IML dashboard and Python package to enable researchers to assess\nthe stability and reliability of their own data-driven interpretations and\ndiscoveries.", "published": "2025-05-21 16:34:11", "link": "http://arxiv.org/abs/2505.15728v1", "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "Privacy-Preserving Conformal Prediction Under Local Differential Privacy", "abstract": "Conformal prediction (CP) provides sets of candidate classes with a\nguaranteed probability of containing the true class. However, it typically\nrelies on a calibration set with clean labels. We address privacy-sensitive\nscenarios where the aggregator is untrusted and can only access a perturbed\nversion of the true labels. We propose two complementary approaches under local\ndifferential privacy (LDP). In the first approach, users do not access the\nmodel but instead provide their input features and a perturbed label using a\nk-ary randomized response. In the second approach, which enforces stricter\nprivacy constraints, users add noise to their conformity score by binary search\nresponse. This method requires access to the classification model but preserves\nboth data and label privacy. Both approaches compute the conformal threshold\ndirectly from noisy data without accessing the true labels. We prove\nfinite-sample coverage guarantees and demonstrate robust coverage even under\nsevere randomization. This approach unifies strong local privacy with\npredictive uncertainty control, making it well-suited for sensitive\napplications such as medical imaging or large language model queries,\nregardless of whether users can (or are willing to) compute their own scores.", "published": "2025-05-21 16:29:44", "link": "http://arxiv.org/abs/2505.15721v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes", "abstract": "We revisit the classical problem of Bayesian ensembles and address the\nchallenge of learning optimal combinations of Bayesian models in an online,\ncontinual learning setting. To this end, we reinterpret existing approaches\nsuch as Bayesian model averaging (BMA) and Bayesian stacking through a novel\nempirical Bayes lens, shedding new light on the limitations and pathologies of\nBMA. Further motivated by insights from online optimization, we propose Online\nBayesian Stacking (OBS), a method that optimizes the log-score over predictive\ndistributions to adaptively combine Bayesian models. A key contribution of our\nwork is establishing a novel connection between OBS and portfolio selection,\nbridging Bayesian ensemble learning with a rich, well-studied theoretical\nframework that offers efficient algorithms and extensive regret analysis. We\nfurther clarify the relationship between OBS and online BMA, showing that they\noptimize related but distinct cost functions. Through theoretical analysis and\nempirical evaluation, we identify scenarios where OBS outperforms online BMA\nand provide principled guidance on when practitioners should prefer one\napproach over the other.", "published": "2025-05-21 15:19:08", "link": "http://arxiv.org/abs/2505.15638v1", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Aligning Explanations with Human Communication", "abstract": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility.", "published": "2025-05-21 15:14:05", "link": "http://arxiv.org/abs/2505.15626v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions", "abstract": "Personalized federated learning has emerged as a popular approach to training\non devices holding statistically heterogeneous data, known as clients. However,\nmost existing approaches require a client to have labeled data for training or\nfinetuning in order to obtain their own personalized model. In this paper we\naddress this by proposing FLowDUP, a novel method that is able to generate a\npersonalized model using only a forward pass with unlabeled data. The generated\nmodel parameters reside in a low-dimensional subspace, enabling efficient\ncommunication and computation. FLowDUP's learning objective is theoretically\nmotivated by our new transductive multi-task PAC-Bayesian generalization bound,\nthat provides performance guarantees for unlabeled clients. The objective is\nstructured in such a way that it allows both clients with labeled data and\nclients with only unlabeled data to contribute to the training process. To\nsupplement our theoretical results we carry out a thorough experimental\nevaluation of FLowDUP, demonstrating strong empirical performance on a range of\ndatasets with differing sorts of statistically heterogeneous clients. Through\nnumerous ablation studies, we test the efficacy of the individual components of\nthe method.", "published": "2025-05-21 14:30:59", "link": "http://arxiv.org/abs/2505.15579v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes", "abstract": "We present new fast-rate generalization bounds for multi-task and\nmeta-learning in the unbalanced setting, i.e. when the tasks have training sets\nof different sizes, as is typically the case in real-world scenarios.\nPreviously, only standard-rate bounds were known for this situation, while\nfast-rate bounds were limited to the setting where all training sets are of\nequal size. Our new bounds are numerically computable as well as interpretable,\nand we demonstrate their flexibility in handling a number of cases where they\ngive stronger guarantees than previous bounds. Besides the bounds themselves,\nwe also make conceptual contributions: we demonstrate that the unbalanced\nmulti-task setting has different statistical properties than the balanced\nsituation, specifically that proofs from the balanced situation do not carry\nover to the unbalanced setting. Additionally, we shed light on the fact that\nthe unbalanced situation allows two meaningful definitions of multi-task risk,\ndepending on whether if all tasks should be considered equally important or if\nsample-rich tasks should receive more weight than sample-poor ones.", "published": "2025-05-21 13:22:07", "link": "http://arxiv.org/abs/2505.15496v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs", "abstract": "Uncertainty estimation remains a critical challenge in adapting pre-trained\nlanguage models to classification tasks, particularly under parameter-efficient\nfine-tuning approaches such as adapters. We introduce AdUE1, an efficient\npost-hoc uncertainty estimation (UE) method, to enhance softmax-based\nestimates. Our approach (1) uses a differentiable approximation of the maximum\nfunction and (2) applies additional regularization through L2-SP, anchoring the\nfine-tuned head weights and regularizing the model. Evaluations on five NLP\nclassification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,\nQwen) demonstrate that our method consistently outperforms established\nbaselines such as Mahalanobis distance and softmax response. Our approach is\nlightweight (no base-model changes) and produces better-calibrated confidence.", "published": "2025-05-21 12:23:40", "link": "http://arxiv.org/abs/2505.15443v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adaptive Temperature Scaling with Conformal Prediction", "abstract": "Conformal prediction enables the construction of high-coverage prediction\nsets for any pre-trained model, guaranteeing that the true label lies within\nthe set with a specified probability. However, these sets do not provide\nprobability estimates for individual labels, limiting their practical use. In\nthis paper, we propose, to the best of our knowledge, the first method for\nassigning calibrated probabilities to elements of a conformal prediction set.\nOur approach frames this as an adaptive calibration problem, selecting an\ninput-specific temperature parameter to match the desired coverage level.\nExperiments on several challenging image classification datasets demonstrate\nthat our method maintains coverage guarantees while significantly reducing\nexpected calibration error.", "published": "2025-05-21 12:18:15", "link": "http://arxiv.org/abs/2505.15437v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Uncertainty Quantification in SVM prediction", "abstract": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments.", "published": "2025-05-21 12:11:07", "link": "http://arxiv.org/abs/2505.15429v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding", "abstract": "Capturing nonlinear relationships without sacrificing interpretability\nremains a persistent challenge in regression modeling. We introduce SplitWise,\na novel framework that enhances stepwise regression. It adaptively transforms\nnumeric predictors into threshold-based binary features using shallow decision\ntrees, but only when such transformations improve model fit, as assessed by the\nAkaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\nThis approach preserves the transparency of linear models while flexibly\ncapturing nonlinear effects. Implemented as a user-friendly R package,\nSplitWise is evaluated on both synthetic and real-world datasets. The results\nshow that it consistently produces more parsimonious and generalizable models\nthan traditional stepwise and penalized regression techniques.", "published": "2025-05-21 12:06:43", "link": "http://arxiv.org/abs/2505.15423v1", "categories": ["cs.LG", "econ.EM", "stat.AP", "stat.ME", "stat.ML", "62H20, 62J05, 68T05", "G.3; I.2.6; I.5.1; I.5.2"], "primary_category": "cs.LG"}
{"title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion", "abstract": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference.", "published": "2025-05-21 12:00:37", "link": "http://arxiv.org/abs/2505.15417v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Human in the Loop Adaptive Optimization for Improved Time Series Forecasting", "abstract": "Time series forecasting models often produce systematic, predictable errors\neven in critical domains such as energy, finance, and healthcare. We introduce\na novel post training adaptive optimization framework that improves forecast\naccuracy without retraining or architectural changes. Our method automatically\napplies expressive transformations optimized via reinforcement learning,\ncontextual bandits, or genetic algorithms to correct model outputs in a\nlightweight and model agnostic way. Theoretically, we prove that affine\ncorrections always reduce the mean squared error; practically, we extend this\nidea with dynamic action based optimization. The framework also supports an\noptional human in the loop component: domain experts can guide corrections\nusing natural language, which is parsed into actions by a language model.\nAcross multiple benchmarks (e.g., electricity, weather, traffic), we observe\nconsistent accuracy gains with minimal computational overhead. Our interactive\ndemo shows the framework's real time usability. By combining automated post hoc\nrefinement with interpretable and extensible mechanisms, our approach offers a\npowerful new direction for practical forecasting systems.", "published": "2025-05-21 10:30:02", "link": "http://arxiv.org/abs/2505.15354v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Policy Testing in Markov Decision Processes", "abstract": "We study the policy testing problem in discounted Markov decision processes\n(MDPs) under the fixed-confidence setting. The goal is to determine whether the\nvalue of a given policy exceeds a specified threshold while minimizing the\nnumber of observations. We begin by deriving an instance-specific lower bound\nthat any algorithm must satisfy. This lower bound is characterized as the\nsolution to an optimization problem with non-convex constraints. We propose a\npolicy testing algorithm inspired by this optimization problem--a common\napproach in pure exploration problems such as best-arm identification, where\nasymptotically optimal algorithms often stem from such optimization-based\ncharacterizations. As for other pure exploration tasks in MDPs, however, the\nnon-convex constraints in the lower-bound problem present significant\nchallenges, raising doubts about whether statistically optimal and\ncomputationally tractable algorithms can be designed. To address this, we\nreformulate the lower-bound problem by interchanging the roles of the objective\nand the constraints, yielding an alternative problem with a non-convex\nobjective but convex constraints. Strikingly, this reformulated problem admits\nan interpretation as a policy optimization task in a newly constructed reversed\nMDP. Leveraging recent advances in policy gradient methods, we efficiently\nsolve this problem and use it to design a policy testing algorithm that is\nstatistically optimal--matching the instance-specific lower bound on sample\ncomplexity--while remaining computationally tractable. We validate our approach\nwith numerical experiments.", "published": "2025-05-21 10:13:54", "link": "http://arxiv.org/abs/2505.15342v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge", "abstract": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty.", "published": "2025-05-21 08:16:18", "link": "http://arxiv.org/abs/2505.15240v1", "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "abstract": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent.", "published": "2025-05-21 08:16:03", "link": "http://arxiv.org/abs/2505.15239v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Estimation methods of Matrix-valued AR model", "abstract": "This article proposes novel estimation methods for the Matrix Autoregressive\n(MAR) model, specifically adaptations of the Yule-Walker equations and Burg's\nmethod, addressing limitations in existing techniques. The MAR model, by\nmaintaining a matrix structure and requiring significantly fewer parameters\nthan vector autoregressive (VAR) models, offers a parsimonious, yet effective,\nalternative for high-dimensional time series. Empirical results demonstrate\nthat MAR models estimated via the proposed methods achieve a comparable fit to\nVAR models across metrics such as MAE and RMSE. These findings underscore the\nutility of Yule-Walker and Burg-type estimators in constructing efficient and\ninterpretable models for complex temporal data.", "published": "2025-05-21 07:47:05", "link": "http://arxiv.org/abs/2505.15220v1", "categories": ["math.ST", "stat.ML", "stat.TH", "62M10"], "primary_category": "math.ST"}
{"title": "Clustering and Pruning in Causal Data Fusion", "abstract": "Data fusion, the process of combining observational and experimental data,\ncan enable the identification of causal effects that would otherwise remain\nnon-identifiable. Although identification algorithms have been developed for\nspecific scenarios, do-calculus remains the only general-purpose tool for\ncausal data fusion, particularly when variables are present in some data\nsources but not others. However, approaches based on do-calculus may encounter\ncomputational challenges as the number of variables increases and the causal\ngraph grows in complexity. Consequently, there exists a need to reduce the size\nof such models while preserving the essential features. For this purpose, we\npropose pruning (removing unnecessary variables) and clustering (combining\nvariables) as preprocessing operations for causal data fusion. We generalize\nearlier results on a single data source and derive conditions for applying\npruning and clustering in the case of multiple data sources. We give sufficient\nconditions for inferring the identifiability or non-identifiability of a causal\neffect in a larger graph based on a smaller graph and show how to obtain the\ncorresponding identifying functional for identifiable causal effects. Examples\nfrom epidemiology and social science demonstrate the use of the results.", "published": "2025-05-21 07:44:39", "link": "http://arxiv.org/abs/2505.15215v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Reconstruction of Graph Signals on Complex Manifolds with Kernel Methods", "abstract": "Graph signals are widely used to describe vertex attributes or features in\ngraph-structured data, with applications spanning the internet, social media,\ntransportation, sensor networks, and biomedicine. Graph signal processing (GSP)\nhas emerged to facilitate the analysis, processing, and sampling of such\nsignals. While kernel methods have been extensively studied for estimating\ngraph signals from samples provided on a subset of vertices, their application\nto complex-valued graph signals remains largely unexplored. This paper\nintroduces a novel framework for reconstructing graph signals using kernel\nmethods on complex manifolds. By embedding graph vertices into a\nhigher-dimensional complex ambient space that approximates a lower-dimensional\nmanifold, the framework extends the reproducing kernel Hilbert space to complex\nmanifolds. It leverages Hermitian metrics and geometric measures to\ncharacterize kernels and graph signals. Additionally, several traditional\nkernels and graph topology-driven kernels are proposed for reconstructing\ncomplex graph signals. Finally, experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness of this framework in\naccurately reconstructing complex graph signals, outperforming conventional\nkernel-based approaches. This work lays a foundational basis for integrating\ncomplex geometry and kernel methods in GSP.", "published": "2025-05-21 07:27:38", "link": "http://arxiv.org/abs/2505.15202v1", "categories": ["eess.SP", "stat.ML"], "primary_category": "eess.SP"}
{"title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "abstract": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.", "published": "2025-05-21 07:26:36", "link": "http://arxiv.org/abs/2505.15201v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing", "abstract": "Retraining a model using its own predictions together with the original,\npotentially noisy labels is a well-known strategy for improving the model\nperformance. While prior works have demonstrated the benefits of specific\nheuristic retraining schemes, the question of how to optimally combine the\nmodel's predictions and the provided labels remains largely open. This paper\naddresses this fundamental question for binary classification tasks. We develop\na principled framework based on approximate message passing (AMP) to analyze\niterative retraining procedures for two ground truth settings: Gaussian mixture\nmodel (GMM) and generalized linear model (GLM). Our main contribution is the\nderivation of the Bayes optimal aggregator function to combine the current\nmodel's predictions and the given labels, which when used to retrain the same\nmodel, minimizes its prediction error. We also quantify the performance of this\noptimal retraining strategy over multiple rounds. We complement our theoretical\nresults by proposing a practically usable version of the theoretically-optimal\naggregator function for linear probing with the cross-entropy loss, and\ndemonstrate its superiority over baseline methods in the high label noise\nregime.", "published": "2025-05-21 07:16:44", "link": "http://arxiv.org/abs/2505.15195v1", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "A Linear Approach to Data Poisoning", "abstract": "We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training.", "published": "2025-05-21 06:45:06", "link": "http://arxiv.org/abs/2505.15175v1", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms", "abstract": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts.", "published": "2025-05-21 05:56:31", "link": "http://arxiv.org/abs/2505.15141v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "abstract": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models.", "published": "2025-05-21 03:32:30", "link": "http://arxiv.org/abs/2505.15064v1", "categories": ["cs.LG", "math.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Restricted Spectral Gap Decomposition for Simulated Tempering Targeting Mixture Distributions", "abstract": "Simulated tempering is a widely used strategy for sampling from multimodal\ndistributions. In this paper, we consider simulated tempering combined with an\narbitrary local Markov chain Monte Carlo sampler and present a new\ndecomposition theorem that provides a lower bound on the restricted spectral\ngap of the algorithm for sampling from mixture distributions. By working with\nthe restricted spectral gap, the applicability of our results is extended to\nbroader settings such as when the usual spectral gap is difficult to bound or\nbecomes degenerate. We demonstrate the application of our theoretical results\nby analyzing simulated tempering combined with random walk Metropolis--Hastings\nfor sampling from mixtures of Gaussian distributions. We show that in\nfixed-dimensional settings, the algorithm's complexity scales polynomially with\nthe separation between modes and logarithmically with $1/\\varepsilon$, where\n$\\varepsilon$ is the target accuracy in total variation distance.", "published": "2025-05-21 03:28:55", "link": "http://arxiv.org/abs/2505.15059v1", "categories": ["math.ST", "math.PR", "stat.CO", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Infinite hierarchical contrastive clustering for personal digital envirotyping", "abstract": "Daily environments have profound influence on our health and behavior. Recent\nwork has shown that digital envirotyping, where computer vision is applied to\nimages of daily environments taken during ecological momentary assessment\n(EMA), can be used to identify meaningful relationships between environmental\nfeatures and health outcomes of interest. To systematically study such effects\non an individual level, it is helpful to group images into distinct\nenvironments encountered in an individual's daily life; these may then be\nanalyzed, further grouped into related environments with similar features, and\nlinked to health outcomes. Here we introduce infinite hierarchical contrastive\nclustering to address this challenge. Building on the established contrastive\nclustering framework, our method a) allows an arbitrary number of clusters\nwithout requiring the full Dirichlet Process machinery by placing a\nstick-breaking prior on predicted cluster probabilities; and b) encourages\ndistinct environments to form well-defined sub-clusters within each cluster of\nrelated environments by incorporating a participant-specific prediction loss.\nOur experiments show that our model effectively identifies distinct personal\nenvironments and groups these environments into meaningful environment types.\nWe then illustrate how the resulting clusters can be linked to various health\noutcomes, highlighting the potential of our approach to advance the\nenvirotyping paradigm.", "published": "2025-05-21 02:11:23", "link": "http://arxiv.org/abs/2505.15022v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds", "abstract": "First-order adaptive optimization methods like Adam are the default choices\nfor training modern deep neural networks. Despite their empirical success, the\ntheoretical understanding of these methods in non-smooth settings, particularly\nin Deep ReLU networks, remains limited. ReLU activations create exponentially\nmany region boundaries where standard smoothness assumptions break down.\n\\textbf{We derive the first\n\\(\\tilde{O}\\!\\bigl(\\sqrt{d_{\\mathrm{eff}}/n}\\bigr)\\) generalization bound for\nAdam in Deep ReLU networks and the first global-optimal convergence for Adam in\nthe non smooth, non convex relu landscape without a global PL or convexity\nassumption.} Our analysis is based on stratified Morse theory and novel results\nin Kakeya sets. We develop a multi-layer refinement framework that\nprogressively tightens bounds on region crossings. We prove that the number of\nregion crossings collapses from exponential to near-linear in the effective\ndimension. Using a Kakeya based method, we give a tighter generalization bound\nthan PAC-Bayes approaches and showcase convergence using a mild uniform low\nbarrier assumption.", "published": "2025-05-21 01:34:16", "link": "http://arxiv.org/abs/2505.15013v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "abstract": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios.", "published": "2025-05-21 01:26:21", "link": "http://arxiv.org/abs/2505.15008v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process.", "published": "2025-05-21 01:06:29", "link": "http://arxiv.org/abs/2505.14999v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Pre-validation Revisited", "abstract": "Pre-validation is a way to build prediction model with two datasets of\nsignificantly different feature dimensions. Previous work showed that the\nasymptotic distribution of the resulting test statistic for the pre-validated\npredictor deviates from a standard Normal, hence leads to issues in hypothesis\ntesting. In this paper, we revisit the pre-validation procedure and extend the\nproblem formulation without any independence assumption on the two feature\nsets. We propose not only an analytical distribution of the test statistic for\nthe pre-validated predictor under certain models, but also a generic bootstrap\nprocedure to conduct inference. We show properties and benefits of\npre-validation in prediction, inference and error estimation by simulations and\napplications, including analysis of a breast cancer study and a synthetic GWAS\nexample.", "published": "2025-05-21 00:20:14", "link": "http://arxiv.org/abs/2505.14985v2", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "AudioMorphix: Training-free audio editing with diffusion probabilistic models", "abstract": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.", "published": "2025-05-21 23:23:37", "link": "http://arxiv.org/abs/2505.16076v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation", "abstract": "Studies on schizophrenia assessments using deep learning typically treat it\nas a classification task to detect the presence or absence of the disorder,\noversimplifying the condition and reducing its clinical applicability. This\ntraditional approach overlooks the complexity of schizophrenia, limiting its\npractical value in healthcare settings. This study shifts the focus to\nindividual symptom severity estimation using a multimodal approach that\nintegrates speech, video, and text inputs. We develop unimodal models for each\nmodality and a multimodal framework to improve accuracy and robustness. By\ncapturing a more detailed symptom profile, this approach can help in enhancing\ndiagnostic precision and support personalized treatment, offering a scalable\nand objective tool for mental health assessment.", "published": "2025-05-21 21:55:35", "link": "http://arxiv.org/abs/2505.16044v1", "categories": ["eess.AS", "cs.LG", "eess.IV", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Analyzing the Impact of Accent on English Speech: Acoustic and Articulatory Perspectives", "abstract": "Advancements in AI-driven speech-based applications have transformed diverse\nindustries ranging from healthcare to customer service. However, the increasing\nprevalence of non-native accented speech in global interactions poses\nsignificant challenges for speech-processing systems, which are often trained\non datasets dominated by native speech. This study investigates accented\nEnglish speech through articulatory and acoustic analysis, identifying simpler\ncoordination patterns and higher average pitch than native speech. Using\neigenspectra and Vocal Tract Variable-based coordination features, we establish\nan efficient method for quantifying accent strength without relying on\nresource-intensive phonetic transcriptions. Our findings provide a new avenue\nfor research on the impacts of accents on speech intelligibility and offer\ninsights for developing inclusive, robust speech processing systems that\naccommodate diverse linguistic communities.", "published": "2025-05-21 19:31:40", "link": "http://arxiv.org/abs/2505.15965v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "abstract": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.", "published": "2025-05-21 19:17:29", "link": "http://arxiv.org/abs/2505.15957v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Deep Learning Framework for Efficient Multichannel Acoustic Feedback Control", "abstract": "This study presents a deep-learning framework for controlling multichannel\nacoustic feedback in audio devices. Traditional digital signal processing\nmethods struggle with convergence when dealing with highly correlated noise\nsuch as feedback. We introduce a Convolutional Recurrent Network that\nefficiently combines spatial and temporal processing, significantly enhancing\nspeech enhancement capabilities with lower computational demands. Our approach\nutilizes three training methods: In-a-Loop Training, Teacher Forcing, and a\nHybrid strategy with a Multichannel Wiener Filter, optimizing performance in\ncomplex acoustic environments. This scalable framework offers a robust solution\nfor real-world applications, making significant advances in Acoustic Feedback\nControl technology.", "published": "2025-05-21 18:07:48", "link": "http://arxiv.org/abs/2505.15914v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASVspoof2019 vs. ASVspoof5: Assessment and Comparison", "abstract": "ASVspoof challenges are designed to advance the understanding of spoofing\nspeech attacks and encourage the development of robust countermeasure systems.\nThese challenges provide a standardized database for assessing and comparing\nspoofing-robust automatic speaker verification solutions. The ASVspoof5\nchallenge introduces a shift in database conditions compared to ASVspoof2019.\nWhile ASVspoof2019 has mismatched conditions only in spoofing attacks in the\nevaluation set, ASVspoof5 incorporates mismatches in both bona fide and spoofed\nspeech statistics. This paper examines the impact of these mismatches,\npresenting qualitative and quantitative comparisons within and between the two\ndatabases. We show the increased difficulty for genuine and spoofed speech and\ndemonstrate that in ASVspoof5, not only are the attacks more challenging, but\nthe genuine speech also shifts toward spoofed speech compared to ASVspoof2019.", "published": "2025-05-21 18:04:44", "link": "http://arxiv.org/abs/2505.15911v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality", "abstract": "Despite extensive research on toxic speech detection in text, a critical gap\nremains in handling spoken Mandarin audio. The lack of annotated datasets that\ncapture the unique prosodic cues and culturally specific expressions in\nMandarin leaves spoken toxicity underexplored. To address this, we introduce\nToxicTone -- the largest public dataset of its kind -- featuring detailed\nannotations that distinguish both forms of toxicity (e.g., profanity, bullying)\nand sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,\nsourced from diverse real-world audio and organized into 13 topical categories,\nmirrors authentic communication scenarios. We also propose a multimodal\ndetection framework that integrates acoustic, linguistic, and emotional\nfeatures using state-of-the-art speech and emotion encoders. Extensive\nexperiments show our approach outperforms text-only and baseline models,\nunderscoring the essential role of speech-specific cues in revealing hidden\ntoxic expressions.", "published": "2025-05-21 17:25:27", "link": "http://arxiv.org/abs/2505.15773v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling", "abstract": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.", "published": "2025-05-21 17:23:12", "link": "http://arxiv.org/abs/2505.15772v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "abstract": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "published": "2025-05-21 16:13:57", "link": "http://arxiv.org/abs/2505.15700v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model", "abstract": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.", "published": "2025-05-21 15:48:30", "link": "http://arxiv.org/abs/2505.15670v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information", "abstract": "Quantization in SSL speech models (e.g., HuBERT) improves compression and\nperformance in tasks like language modeling, resynthesis, and text-to-speech\nbut often discards prosodic and paralinguistic information (e.g., emotion,\nprominence). While increasing codebook size mitigates some loss, it\ninefficiently raises bitrates. We propose Segmentation-Variant Codebooks\n(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,\nutterance), factorizing it into multiple streams of segment-specific discrete\nfeatures. Our results show that SVCs are significantly more effective at\npreserving prosodic and paralinguistic information across probing tasks.\nAdditionally, we find that pooling before rather than after discretization\nbetter retains segment-level information. Resynthesis experiments further\nconfirm improved style realization and slightly improved quality while\npreserving intelligibility.", "published": "2025-05-21 15:44:32", "link": "http://arxiv.org/abs/2505.15667v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word Level Timestamp Generation for Automatic Speech Recognition and Translation", "abstract": "We introduce a data-driven approach for enabling word-level timestamp\nprediction in the Canary model. Accurate timestamp information is crucial for a\nvariety of downstream tasks such as speech content retrieval and timed\nsubtitles. While traditional hybrid systems and end-to-end (E2E) models may\nemploy external modules for timestamp prediction, our approach eliminates the\nneed for separate alignment mechanisms. By leveraging the NeMo Forced Aligner\n(NFA) as a teacher model, we generate word-level timestamps and train the\nCanary model to predict timestamps directly. We introduce a new <|timestamp|>\ntoken, enabling the Canary model to predict start and end timestamps for each\nword. Our method demonstrates precision and recall rates between 80% and 90%,\nwith timestamp prediction errors ranging from 20 to 120 ms across four\nlanguages, with minimal WER degradation. Additionally, we extend our system to\nautomatic speech translation (AST) tasks, achieving timestamp prediction errors\naround 200 milliseconds.", "published": "2025-05-21 15:24:29", "link": "http://arxiv.org/abs/2505.15646v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes", "abstract": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.", "published": "2025-05-21 14:17:25", "link": "http://arxiv.org/abs/2505.15559v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models", "abstract": "The rise of Large Audio Language Models (LAMs) brings both potential and\nrisks, as their audio outputs may contain harmful or unethical content.\nHowever, current research lacks a systematic, quantitative evaluation of LAM\nsafety especially against jailbreak attacks, which are challenging due to the\ntemporal and semantic nature of speech. To bridge this gap, we introduce\nAJailBench, the first benchmark specifically designed to evaluate jailbreak\nvulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of\n1,495 adversarial audio prompts spanning 10 policy-violating categories,\nconverted from textual jailbreak attacks using realistic text to speech\nsynthesis. Using this dataset, we evaluate several state-of-the-art LAMs and\nreveal that none exhibit consistent robustness across attacks. To further\nstrengthen jailbreak testing and simulate more realistic attack conditions, we\npropose a method to generate dynamic adversarial variants. Our Audio\nPerturbation Toolkit (APT) applies targeted distortions across time, frequency,\nand amplitude domains. To preserve the original jailbreak intent, we enforce a\nsemantic consistency constraint and employ Bayesian optimization to efficiently\nsearch for perturbations that are both subtle and highly effective. This\nresults in AJailBench-APT, an extended dataset of optimized adversarial audio\nsamples. Our findings demonstrate that even small, semantically preserved\nperturbations can significantly reduce the safety performance of leading LAMs,\nunderscoring the need for more robust and semantically aware defense\nmechanisms.", "published": "2025-05-21 11:47:47", "link": "http://arxiv.org/abs/2505.15406v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning", "abstract": "Recent advances in discrete audio codecs have significantly improved speech\nrepresentation modeling, while codec language models have enabled in-context\nlearning for zero-shot speech synthesis. Inspired by this, we propose a voice\nconversion (VC) model within the VALLE-X framework, leveraging its strong\nin-context learning capabilities for speaker adaptation. To enhance prosody\ncontrol, we introduce a prosody-aware audio codec encoder (PACE) module, which\nisolates and refines prosody from other sources, improving expressiveness and\ncontrol. By integrating PACE into our VC model, we achieve greater flexibility\nin prosody manipulation while preserving speaker timbre. Experimental\nevaluation results demonstrate that our approach outperforms baseline VC\nsystems in prosody preservation, timbre consistency, and overall naturalness,\nsurpassing baseline VC systems.", "published": "2025-05-21 11:42:49", "link": "http://arxiv.org/abs/2505.15402v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding", "abstract": "Modern autoregressive speech synthesis models leveraging language models have\ndemonstrated remarkable performance. However, the sequential nature of next\ntoken prediction in these models leads to significant latency, hindering their\ndeployment in scenarios where inference speed is critical. In this work, we\npropose Speech Speculative Decoding (SSD), a novel framework for autoregressive\nspeech synthesis acceleration. Specifically, our method employs a lightweight\ndraft model to generate candidate token sequences, which are subsequently\nverified in parallel by the target model using the proposed SSD framework.\nExperimental results demonstrate that SSD achieves a significant speedup of\n1.4x compared with conventional autoregressive decoding, while maintaining high\nfidelity and naturalness. Subjective evaluations further validate the\neffectiveness of SSD in preserving the perceptual quality of the target model\nwhile accelerating inference.", "published": "2025-05-21 11:17:04", "link": "http://arxiv.org/abs/2505.15380v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Relevance of Clinical Assessment Tasks for the Automatic Detection of Parkinson's Disease Medication State from Speech", "abstract": "The automatic identification of medication states of Parkinson's disease (PD)\npatients can assist clinicians in monitoring and scheduling personalized\ntreatments, as well as studying the effects of medication in alleviating the\nmotor symptoms that characterize the disease. This paper explores speech as a\nnon-invasive and accessible biomarker for identifying PD medication states,\nintroducing a novel approach that addresses this task from a\nspeaker-independent perspective. While traditional machine learning models\nachieve competitive results, self-supervised speech representations prove\nessential for optimal performance, significantly surpassing knowledge-based\nacoustic descriptors. Experiments across diverse speech assessment tasks\nhighlight the relevance of prosody and continuous speech in distinguishing\nmedication states, reaching an F1-score of 88.2%. These findings may streamline\nclinicians' work and reduce patient effort in voice recordings.", "published": "2025-05-21 11:15:26", "link": "http://arxiv.org/abs/2505.15378v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN", "abstract": "Pitch manipulation is the process of producers adjusting the pitch of an\naudio segment to a specific key and intonation, which is essential in music\nproduction. Neural-network-based pitch-manipulation systems have been popular\nin recent years due to their superior synthesis quality compared to classical\nDSP methods. However, their performance is still limited due to their\ninaccurate feature disentanglement using source-filter models and the lack of\npaired in- and out-of-tune training data. This work proposes Neurodyne to\naddress these issues. Specifically, Neurodyne uses adversarial representation\nlearning to learn a pitch-independent latent representation to avoid inaccurate\ndisentanglement and cycle-consistency training to create paired training data\nimplicitly. Experimental results on global-key and template-based pitch\nmanipulation demonstrate the effectiveness of the proposed system, marking\nimproved synthesis quality while maintaining the original singer identity.", "published": "2025-05-21 10:58:10", "link": "http://arxiv.org/abs/2505.15368v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MHANet: Multi-scale Hybrid Attention Network for Auditory Attention Detection", "abstract": "Auditory attention detection (AAD) aims to detect the target speaker in a\nmulti-talker environment from brain signals, such as electroencephalography\n(EEG), which has made great progress. However, most AAD methods solely utilize\nattention mechanisms sequentially and overlook valuable multi-scale contextual\ninformation within EEG signals, limiting their ability to capture long-short\nrange spatiotemporal dependencies simultaneously. To address these issues, this\npaper proposes a multi-scale hybrid attention network (MHANet) for AAD, which\nconsists of the multi-scale hybrid attention (MHA) module and the\nspatiotemporal convolution (STC) module. Specifically, MHA combines channel\nattention and multi-scale temporal and global attention mechanisms. This\neffectively extracts multi-scale temporal patterns within EEG signals and\ncaptures long-short range spatiotemporal dependencies simultaneously. To\nfurther improve the performance of AAD, STC utilizes temporal and spatial\nconvolutions to aggregate expressive spatiotemporal representations.\nExperimental results show that the proposed MHANet achieves state-of-the-art\nperformance with fewer trainable parameters across three datasets, 3 times\nlower than that of the most advanced model. Code is available at:\nhttps://github.com/fchest/MHANet.", "published": "2025-05-21 10:55:56", "link": "http://arxiv.org/abs/2505.15364v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Decoding Phone Pairs from MEG Signals Across Speech Modalities", "abstract": "Understanding the neural mechanisms underlying speech production is essential\nfor both advancing cognitive neuroscience theory and developing practical\ncommunication technologies. In this study, we investigated\nmagnetoencephalography signals to decode phones from brain activity during\nspeech production and perception (passive listening and voice playback) tasks.\nUsing a dataset comprising 17 participants, we performed pairwise phone\nclassification, extending our analysis to 15 phonetic pairs. Multiple machine\nlearning approaches, including regularized linear models and neural network\narchitectures, were compared to determine their effectiveness in decoding\nphonetic information. Our results demonstrate significantly higher decoding\naccuracy during speech production (76.6%) compared to passive listening and\nplayback modalities (~51%), emphasizing the richer neural information available\nduring overt speech. Among the models, the Elastic Net classifier consistently\noutperformed more complex neural networks, highlighting the effectiveness of\ntraditional regularization techniques when applied to limited and\nhigh-dimensional MEG datasets. Besides, analysis of specific brain frequency\nbands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)\nand Theta (4-7 Hz), contributed the most substantially to decoding accuracy,\nsuggesting that these bands encode critical speech production-related neural\nprocesses. Despite using advanced denoising methods, it remains unclear whether\ndecoding solely reflects neural activity or if residual muscular or movement\nartifacts also contributed, indicating the need for further methodological\nrefinement. Overall, our findings underline the critical importance of\nexamining overt speech production paradigms, which, despite their complexity,\noffer opportunities to improve brain-computer interfaces to help individuals\nwith severe speech impairments.", "published": "2025-05-21 10:31:34", "link": "http://arxiv.org/abs/2505.15355v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD", "eess.AS", "I.2.6; I.5.1"], "primary_category": "cs.CL"}
{"title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation", "abstract": "The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.", "published": "2025-05-21 10:05:25", "link": "http://arxiv.org/abs/2505.15333v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Analysis of ABC Frontend Audio Systems for the NIST-SRE24", "abstract": "We present a comprehensive analysis of the embedding extractors (frontends)\ndeveloped by the ABC team for the audio track of NIST SRE 2024. We follow the\ntwo scenarios imposed by NIST: using only a provided set of telephone\nrecordings for training (fixed) or adding publicly available data (open\ncondition). Under these constraints, we develop the best possible speaker\nembedding extractors for the pre-dominant conversational telephone speech (CTS)\ndomain. We explored architectures based on ResNet with different pooling\nmechanisms, recently introduced ReDimNet architecture, as well as a system\nbased on the XLS-R model, which represents the family of large pre-trained\nself-supervised models. In open condition, we train on VoxBlink2 dataset,\ncontaining 110 thousand speakers across multiple languages. We observed a good\nperformance and robustness of VoxBlink-trained models, and our experiments show\npractical recipes for developing state-of-the-art frontends for speaker\nrecognition.", "published": "2025-05-21 09:52:27", "link": "http://arxiv.org/abs/2505.15320v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Pre-training an Effective Respiratory Audio Foundation Model", "abstract": "Recent advancements in foundation models have sparked interest in respiratory\naudio foundation models. However, the effectiveness of applying conventional\npre-training schemes to datasets that are small-sized and lack diversity has\nnot been sufficiently verified. This study aims to explore better pre-training\npractices for respiratory sounds by comparing numerous pre-trained audio\nmodels. Our investigation reveals that models pre-trained on AudioSet, a\ngeneral audio dataset, are more effective than the models specifically\npre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory\nsound datasets for further pre-training enhances performance, and preserving\nthe frequency-wise information when aggregating features is vital. Along with\nmore insights found in the experiments, we establish a new state-of-the-art for\nthe OPERA benchmark, contributing to advancing respiratory audio foundation\nmodels. Our code is available online at\nhttps://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.", "published": "2025-05-21 09:36:11", "link": "http://arxiv.org/abs/2505.15307v1", "categories": ["eess.AS", "cs.SD", "68T07", "J.3"], "primary_category": "eess.AS"}
{"title": "Voice-ENHANCE: Speech Restoration using a Diffusion-based Voice Conversion Framework", "abstract": "We propose a speech enhancement system that combines speaker-agnostic speech\nrestoration with voice conversion (VC) to obtain a studio-level quality speech\nsignal. While voice conversion models are typically used to change speaker\ncharacteristics, they can also serve as a means of speech restoration when the\ntarget speaker is the same as the source speaker. However, since VC models are\nvulnerable to noisy conditions, we have included a generative speech\nrestoration (GSR) model at the front end of our proposed system. The GSR model\nperforms noise suppression and restores speech damage incurred during that\nprocess without knowledge about the target speaker. The VC stage then uses\nguidance from clean speaker embeddings to further restore the output speech. By\nemploying this two-stage approach, we have achieved speech quality objective\nmetric scores comparable to state-of-the-art (SOTA) methods across multiple\ndatasets.", "published": "2025-05-21 08:33:47", "link": "http://arxiv.org/abs/2505.15254v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Audio Detection Using Fine-Tuned Audio Spectrogram Transformers: A Dataset-Driven Evaluation of Mixed AI-Human Speech", "abstract": "The rapid advancement of artificial intelligence (AI) has enabled\nsophisticated audio generation and voice cloning technologies, posing\nsignificant security risks for applications reliant on voice authentication.\nWhile existing datasets and models primarily focus on distinguishing between\nhuman and fully synthetic speech, real-world attacks often involve audio that\ncombines both genuine and cloned segments. To address this gap, we construct a\nnovel hybrid audio dataset incorporating human, AI-generated, cloned, and mixed\naudio samples. We further propose fine-tuned Audio Spectrogram Transformer\n(AST)-based models tailored for detecting these complex acoustic patterns.\nExtensive experiments demonstrate that our approach significantly outperforms\nexisting baselines in mixed-audio detection, achieving 97\\% classification\naccuracy. Our findings highlight the importance of hybrid datasets and tailored\nmodels in advancing the robustness of speech-based authentication systems.", "published": "2025-05-21 05:43:41", "link": "http://arxiv.org/abs/2505.15136v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SHEET: A Multi-purpose Open-source Speech Human Evaluation Estimation Toolkit", "abstract": "We introduce SHEET, a multi-purpose open-source toolkit designed to\naccelerate subjective speech quality assessment (SSQA) research. SHEET stands\nfor the Speech Human Evaluation Estimation Toolkit, which focuses on\ndata-driven deep neural network-based models trained to predict human-labeled\nquality scores of speech samples. SHEET provides comprehensive training and\nevaluation scripts, multi-dataset and multi-model support, as well as\npre-trained models accessible via Torch Hub and HuggingFace Spaces. To\ndemonstrate its capabilities, we re-evaluated SSL-MOS, a speech self-supervised\nlearning (SSL)-based SSQA model widely used in recent scientific papers, on an\nextensive list of speech SSL models. Experiments were conducted on two\nrepresentative SSQA datasets named BVCC and NISQA, and we identified the\noptimal speech SSL model, whose performance surpassed the original SSL-MOS\nimplementation and was comparable to state-of-the-art methods.", "published": "2025-05-21 03:30:23", "link": "http://arxiv.org/abs/2505.15061v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars", "abstract": "Whole-body audio-driven avatar pose and expression generation is a critical\ntask for creating lifelike digital humans and enhancing the capabilities of\ninteractive virtual agents, with wide-ranging applications in virtual reality,\ndigital entertainment, and remote communication. Existing approaches often\ngenerate audio-driven facial expressions and gestures independently, which\nintroduces a significant limitation: the lack of seamless coordination between\nfacial and gestural elements, resulting in less natural and cohesive\nanimations. To address this limitation, we propose AsynFusion, a novel\nframework that leverages diffusion transformers to achieve harmonious\nexpression and gesture synthesis. The proposed method is built upon a\ndual-branch DiT architecture, which enables the parallel generation of facial\nexpressions and gestures. Within the model, we introduce a Cooperative\nSynchronization Module to facilitate bidirectional feature interaction between\nthe two modalities, and an Asynchronous LCM Sampling strategy to reduce\ncomputational overhead while maintaining high-quality outputs. Extensive\nexperiments demonstrate that AsynFusion achieves state-of-the-art performance\nin generating real-time, synchronized whole-body animations, consistently\noutperforming existing methods in both quantitative and qualitative\nevaluations.", "published": "2025-05-21 03:28:53", "link": "http://arxiv.org/abs/2505.15058v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.GR", "eess.AS", "68T10"], "primary_category": "cs.SD"}
{"title": "EASY: Emotion-aware Speaker Anonymization via Factorized Distillation", "abstract": "Emotion plays a significant role in speech interaction, conveyed through\ntone, pitch, and rhythm, enabling the expression of feelings and intentions\nbeyond words to create a more personalized experience. However, most existing\nspeaker anonymization systems employ parallel disentanglement methods, which\nonly separate speech into linguistic content and speaker identity, often\nneglecting the preservation of the original emotional state. In this study, we\nintroduce EASY, an emotion-aware speaker anonymization framework. EASY employs\na novel sequential disentanglement process to disentangle speaker identity,\nlinguistic content, and emotional representation, modeling each speech\nattribute in distinct subspaces through a factorized distillation approach. By\nindependently constraining speaker identity and emotional representation, EASY\nminimizes information leakage, enhancing privacy protection while preserving\noriginal linguistic content and emotional state. Experimental results on the\nVoicePrivacy Challenge official datasets demonstrate that our proposed approach\noutperforms all baseline systems, effectively protecting speaker privacy while\nmaintaining linguistic content and emotional state.", "published": "2025-05-21 01:17:09", "link": "http://arxiv.org/abs/2505.15004v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discrete Audio Representations for Automated Audio Captioning", "abstract": "Discrete audio representations, termed audio tokens, are broadly categorized\ninto semantic and acoustic tokens, typically generated through unsupervised\ntokenization of continuous audio representations. However, their applicability\nto automated audio captioning (AAC) remains underexplored. This paper\nsystematically investigates the viability of audio token-driven models for AAC\nthrough comparative analyses of various tokenization methods. Our findings\nreveal that audio tokenization leads to performance degradation in AAC models\ncompared to those that directly utilize continuous audio representations. To\naddress this issue, we introduce a supervised audio tokenizer trained with an\naudio tagging objective. Unlike unsupervised tokenizers, which lack explicit\nsemantic understanding, the proposed tokenizer effectively captures audio event\ninformation. Experiments conducted on the Clotho dataset demonstrate that the\nproposed audio tokens outperform conventional audio tokens in the AAC task.", "published": "2025-05-21 00:27:38", "link": "http://arxiv.org/abs/2505.14989v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Application of Quaternions to Obtain Analytic Solutions to Systems of Polarization Components", "abstract": "The usual way to describe mathematically a beam of coherent light passing\nthrough a system of waveplates is via the Jones vector and Jones matrix. This\npaper will show that a quaternion can be used to represent both the optical\nsignal and the waveplate component it passes through, replacing the Jones\nvector and the Jones matrix. The quaternion description is easier to manipulate\nthan the matrix-vector description; for example it can be inverted. As well as\nthe Jones vector, the state of polarization (SOP) of an optical signal is often\ndescribed as a three-dimensional vector on the Poincar\\'e sphere, or as a\npolarization ellipse, and it will be shown how these three forms are closely\nrelated to the quaternion representation. Similarly, the action of a waveplate\nmay be represented as a rotation about an axis on the Poincar\\'e sphere, and\nthat rotation is shown to have a logarithm-exponential relationship to the\nwaveplate's quaternion. The paper presents rules to decide if two optical\nsignals are aligned or orthogonal in phase or in polarization from their\nquaternions, and presents the quaternion operations to change the phase or\nchange the SOP. Light passing through a system of waveplates is written as a\nproduct of quaternions, and it can be hard to simplify or manipulate that\nexpression because quaternion multiplication does not commute. The paper brings\ntogether several mathematical tools that allow such a quaternion product to be\nrearranged, including the new idea of partial conjugation. Finally, a worked\nexample is included of the quaternion mathematics applied to a waveplate\nproblem that has not been solved before. It is shown that an endless optical\nphase shifter can be built using three rotatable waveplates, and equations for\nthe angles of rotation are derived to produce the desired phase shift for given\ninput and output SOPs.", "published": "2025-05-21 20:29:50", "link": "http://arxiv.org/abs/2505.15999v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "AI-Assisted NLOS Sensing for RIS-Based Indoor Localization in Smart Factories", "abstract": "In the era of Industry 4.0, precise indoor localization is vital for\nautomation and efficiency in smart factories. Reconfigurable Intelligent\nSurfaces (RIS) are emerging as key enablers in 6G networks for joint sensing\nand communication. However, RIS faces significant challenges in\nNon-Line-of-Sight (NLOS) and multipath propagation, particularly in\nlocalization scenarios, where detecting NLOS conditions is crucial for ensuring\nnot only reliable results and increased connectivity but also the safety of\nsmart factory personnel. This study introduces an AI-assisted framework\nemploying a Convolutional Neural Network (CNN) customized for accurate\nLine-of-Sight (LOS) and Non-Line-of-Sight (NLOS) classification to enhance\nRIS-based localization using measured, synthetic, mixed-measured, and\nmixed-synthetic experimental data, that is, original, augmented, slightly\nnoisy, and highly noisy data, respectively. Validated through such data from\nthree different environments, the proposed customized-CNN (cCNN) model achieves\n{95.0\\%-99.0\\%} accuracy, outperforming standard pre-trained models like Visual\nGeometry Group 16 (VGG-16) with an accuracy of {85.5\\%-88.0\\%}. By addressing\nRIS limitations in NLOS scenarios, this framework offers scalable and\nhigh-precision localization solutions for 6G-enabled smart factories.", "published": "2025-05-21 20:12:30", "link": "http://arxiv.org/abs/2505.15989v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Hierarchical Optimization Framework Using Deep Reinforcement Learning for Task-Driven Bandwidth Allocation in 5G Teleoperation", "abstract": "The evolution of 5G wireless technology has revolutionized connectivity,\nenabling a diverse range of applications. Among these are critical use cases\nsuch as real time teleoperation, which demands ultra reliable low latency\ncommunications (URLLC) to ensure precise and uninterrupted control, and\nenhanced mobile broadband (eMBB) services, which cater to data-intensive\napplications requiring high throughput and bandwidth. In our scenario, there\nare two queues, one for eMBB users and one for URLLC users. In teleoperation\ntasks, control commands are received in the URLLC queue, where communication\ndelays occur. The dynamic index (DI) controls the service rate, affecting the\ntelerobotic (URLLC) queue. A separate queue models eMBB data traffic. Both\nqueues are managed through network slicing and application delay constraints,\nleading to a unified Lagrangian-based Lyapunov optimization for efficient\nresource allocation. We propose a DRL based hierarchical optimization framework\nthat consists of two levels. At the first level, network optimization\ndynamically allocates resources for eMBB and URLLC users using a Lagrangian\nfunctional and an actor critic network to balance competing objectives. At the\nsecond level, control optimization finetunes the best gains for robots,\nensuring stability and responsiveness in network conditions. This hierarchical\napproach enhances both communication and control processes, ensuring efficient\nresource utilization and optimized performance across the network.", "published": "2025-05-21 19:53:20", "link": "http://arxiv.org/abs/2505.15977v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation", "abstract": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods.", "published": "2025-05-21 17:56:02", "link": "http://arxiv.org/abs/2505.15802v1", "categories": ["cs.LG", "eess.SP", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A Generative AI Framework with Multi-Agent Learning", "abstract": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.", "published": "2025-05-21 14:25:31", "link": "http://arxiv.org/abs/2505.15571v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers", "abstract": "Concept discovery in neural networks often targets individual neurons or\nhuman-interpretable features, overlooking distributed layer-wide patterns. We\nstudy the Neural Activation Pattern (NAP) methodology, which clusters\nfull-layer activation distributions to identify such layer-level concepts.\nApplied to visual object recognition and radio receiver models, we propose\nimproved normalization, distribution estimation, distance metrics, and varied\ncluster selection. In the radio receiver model, distinct concepts did not\nemerge; instead, a continuous activation manifold shaped by Signal-to-Noise\nRatio (SNR) was observed -- highlighting SNR as a key learned factor,\nconsistent with classical receiver behavior and supporting physical\nplausibility. Our enhancements to NAP improved in-distribution vs.\nout-of-distribution separation, suggesting better generalization and indirectly\nvalidating clustering quality. These results underscore the importance of\nclustering design and activation manifolds in interpreting and troubleshooting\nneural network behavior.", "published": "2025-05-21 14:23:38", "link": "http://arxiv.org/abs/2505.15570v1", "categories": ["cs.LG", "eess.SP", "68T07 (Primary) 62H30, 94A05 (Secondary)", "I.2.6; I.5.3; C.2.1"], "primary_category": "cs.LG"}
{"title": "Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection", "abstract": "Germany's transition to a renewable energy-based power system is reshaping\ngrid operations, requiring advanced monitoring and control to manage\ndecentralized generation. Machine learning (ML) has emerged as a powerful tool\nfor power system protection, particularly for fault detection (FD) and fault\nline identification (FLI) in transmission grids. However, ML model reliability\ndepends on data quality and availability. Data sparsity resulting from sensor\nfailures, communication disruptions, or reduced sampling rates poses a\nchallenge to ML-based FD and FLI. Yet, its impact has not been systematically\nvalidated prior to this work. In response, we propose a framework to assess the\nimpact of data sparsity on ML-based FD and FLI performance. We simulate\nrealistic data sparsity scenarios, evaluate their impact, derive quantitative\ninsights, and demonstrate the effectiveness of this evaluation strategy by\napplying it to an existing ML-based framework. Results show the ML model\nremains robust for FD, maintaining an F1-score of 0.999 $\\pm$ 0.000 even after\na 50x data reduction. In contrast, FLI is more sensitive, with performance\ndecreasing by 55.61% for missing voltage measurements and 9.73% due to\ncommunication failures at critical network points. These findings offer\nactionable insights for optimizing ML models for real-world grid protection.\nThis enables more efficient FD and supports targeted improvements in FLI.", "published": "2025-05-21 14:17:58", "link": "http://arxiv.org/abs/2505.15560v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Robust Activity Detection for Massive Random Access", "abstract": "Massive machine-type communications (mMTC) are fundamental to the Internet of\nThings (IoT) framework in future wireless networks, involving the connection of\na vast number of devices with sporadic transmission patterns. Traditional\ndevice activity detection (AD) methods are typically developed for Gaussian\nnoise, but their performance may deteriorate when these conditions are not met,\nparticularly in the presence of heavy-tailed impulsive noise. In this paper, we\npropose robust statistical techniques for AD that do not rely on the Gaussian\nassumption and replace the Gaussian loss function with robust loss functions\nthat can effectively mitigate the impact of heavy-tailed noise and outliers.\nFirst, we prove that the coordinate-wise (conditional) objective function is\ngeodesically convex and derive a fixed-point (FP) algorithm for minimizing it,\nalong with convergence guarantees. Building on the FP algorithm, we propose two\nrobust algorithms for solving the full (unconditional) objective function: a\ncoordinate-wise optimization algorithm (RCWO) and a greedy covariance\nlearning-based matching pursuit algorithm (RCL-MP). Numerical experiments\ndemonstrate that the proposed methods significantly outperform existing\nalgorithms in scenarios with non-Gaussian noise, achieving higher detection\naccuracy and robustness.", "published": "2025-05-21 14:15:51", "link": "http://arxiv.org/abs/2505.15555v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection", "abstract": "The Line-of-Sight (LoS) identification is crucial to ensure reliable\nhigh-frequency communication links, especially those vulnerable to blockages.\nNetwork Digital Twins and Artificial Intelligence are key technologies enabling\nblockage detection (LoS identification) for high-frequency wireless systems,\ne.g., 6>GHz. In this work, we enhance Network Digital Twins by incorporating\nAge of Information (AoI) metrics, a quantification of status update freshness,\nenabling reliable real-time blockage detection (LoS identification) in dynamic\nwireless environments. By integrating raytracing techniques, we automate\nlarge-scale collection and labeling of channel data, specifically tailored to\nthe evolving conditions of the environment. The introduced AoI is integrated\nwith the loss function to prioritize more recent information to fine-tune deep\nlearning models in case of performance degradation (model drift). The\neffectiveness of the proposed solution is demonstrated in realistic urban\nsimulations, highlighting the trade-off between input resolution, computational\ncost, and model performance. A resolution reduction of 4x8 from an original\nchannel sample size of (32, 1024) along the angle and subcarrier dimension\nresults in a computational speedup of 32 times. The proposed fine-tuning\nsuccessfully mitigates performance degradation while requiring only 1% of the\navailable data samples, enabling automated and fast mitigation of model drifts.", "published": "2025-05-21 13:43:47", "link": "http://arxiv.org/abs/2505.15519v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SINR Maximizing Distributionally Robust Adaptive Beamforming", "abstract": "This paper addresses the robust adaptive beamforming (RAB) problem via the\nworst-case signal-to-interference-plus-noise ratio (SINR) maximization over\ndistributional uncertainty sets for the random interference-plus-noise\ncovariance (INC) matrix and desired signal steering vector. Our study explores\ntwo distinct uncertainty sets for the INC matrix and three for the steering\nvector. The uncertainty sets of the INC matrix account for the support and the\npositive semidefinite (PSD) mean of the distribution, as well as a similarity\nconstraint on the mean. The uncertainty sets for the steering vector consist of\nthe constraints on the first- and second-order moments of its associated\nprobability distribution. The RAB problem is formulated as the minimization of\nthe worst-case expected value of the SINR denominator over any distribution\nwithin the uncertainty set of the INC matrix, subject to the condition that the\nexpected value of the numerator is greater than or equal to one for every\ndistribution within the uncertainty set of the steering vector. By leveraging\nthe strong duality of linear conic programming, this RAB problem is\nreformulated as a quadratic matrix inequality problem. Subsequently, it is\naddressed by iteratively solving a sequence of linear matrix inequality\nrelaxation problems, incorporating a penalty term for the rank-one PSD matrix\nconstraint. We further analyze the convergence of the iterative algorithm. The\nproposed robust beamforming approach is validated through simulation examples,\nwhich illustrate improved performance in terms of the array output SINR.", "published": "2025-05-21 13:20:58", "link": "http://arxiv.org/abs/2505.15493v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "AI-empowered Real-Time Line-of-Sight Identification via Network Digital Twins", "abstract": "The identification of Line-of-Sight (LoS) conditions is critical for ensuring\nreliable high-frequency communication links, which are particularly vulnerable\nto blockages and rapid channel variations. Network Digital Twins (NDTs) and\nRay-Tracing (RT) techniques can significantly automate the large-scale\ncollection and labeling of channel data, tailored to specific wireless\nenvironments. This paper examines the quality of Artificial Intelligence (AI)\nmodels trained on data generated by Network Digital Twins. We propose and\nevaluate training strategies for a general-purpose Deep Learning model,\ndemonstrating superior performance compared to the current state-of-the-art. In\nterms of classification accuracy, our approach outperforms the state-of-the-art\nDeep Learning model by 5% in very low SNR conditions and by approximately 10%\nin medium-to-high SNR scenarios. Additionally, the proposed strategies\neffectively reduce the input size to the Deep Learning model while preserving\nits performance. The computational cost, measured in floating-point operations\nper second (FLOPs) during inference, is reduced by 98.55% relative to\nstate-of-the-art solutions, making it ideal for real-time applications.", "published": "2025-05-21 12:54:00", "link": "http://arxiv.org/abs/2505.15478v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On Optimizing Time-, Space- and Power-Domain Energy-Saving Techniques for Sub-6 GHz Base Stations", "abstract": "What is the optimal base station (BS) resource allocation strategy given a\nmeasurement-based power consumption model and a fixed target user rate?\nRush-to-sleep in time, rush-to-mute in space, awake-but-whisper in power, or a\ncombination of them? We propose in this paper an efficient solution to the\nproblem of finding the optimal number of active time slots, active antennas,\nand transmit power at active antennas in a multiple-input multiple-output\n(MIMO) orthogonal frequency-division multiplexing (OFDM) system under per-user\nrate and per-antenna transmit power constraints. The use of a parametric power\nconsumption model validated on operator measurements of 4G and 5G BSs enhances\nthe interpretation of the results. We discuss the optimal energy-saving\nstrategy at different network loads for three BS configurations. Using as few\nBS antennas as possible is close to optimal in BSs not implementing time-domain\npower savings such as micro-discontinuous transmission ({\\mu}DTX).\nEnergy-saving schemes that jointly operate in the three domains are instead\noptimal when the BS hardware can enter time-domain power-saving modes, with a\ntendency for rush-to-mute in massive MIMO and for rush-to-sleep in BS with\nfewer antennas. Median energy savings up to $30\\%$ are achieved at low network\nloads.", "published": "2025-05-21 12:25:26", "link": "http://arxiv.org/abs/2505.15445v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Wireless Sensing via Pinching-Antenna Systems", "abstract": "A wireless sensing architecture via pinching antenna systems is proposed.\nCompared to conventional wireless systems, PASS offers flexible antenna\ndeployment and improved probing performance for wireless sensing by leveraging\ndielectric waveguides and pinching antennas (PAs). To enhance signal reception,\nleaky coaxial (LCX) cables are used to uniformly collect echo signals over a\nwide area. The Cram\\'er-Rao bound (CRB) for multi-target sensing is derived and\nthen minimized through the joint optimization of the transmit waveform and the\npositions of PAs. To solve the resulting highly coupled, non-convex problem, a\ntwo-stage particle swarm optimization (PSO)-based algorithm is proposed.\nNumerical results demonstrate significant gains in sensing accuracy and\nrobustness over conventional sensing systems, highlighting the benefits of\nintegrating LCX-based reception with optimized PASS configurations.", "published": "2025-05-21 12:11:17", "link": "http://arxiv.org/abs/2505.15430v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RIS Beam Calibration for ISAC Systems: Modeling and Performance Analysis", "abstract": "High-accuracy localization is a key enabler for integrated sensing and\ncommunication (ISAC), playing an essential role in various applications such as\nautonomous driving. Antenna arrays and reconfigurable intelligent surface (RIS)\nare incorporated into these systems to achieve high angular resolution,\nassisting in the localization process. However, array and RIS beam patterns in\npractice often deviate from the idealized models used for algorithm design,\nleading to significant degradation in positioning accuracy. This mismatch\nhighlights the need for beam calibration to bridge the gap between theoretical\nmodels and real-world hardware behavior. In this paper, we present and analyze\nthree beam models considering several key non-idealities such as mutual\ncoupling, non-ideal codebook, and measurement uncertainties. Based on the\nmodels, we then develop calibration algorithms to estimate the model parameters\nthat can be used for future localization tasks. This work evaluates the\neffectiveness of the beam models and the calibration algorithms using both\ntheoretical bounds and real-world beam pattern data from an RIS prototype. The\nsimulation results show that the model incorporating combined impacts can\naccurately reconstruct measured beam patterns. This highlights the necessity of\nrealistic beam modeling and calibration to achieve high-accuracy localization.", "published": "2025-05-21 11:45:22", "link": "http://arxiv.org/abs/2505.15403v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference", "abstract": "In electromyogram (EMG)-based motion recognition, a subject-specific\nclassifier is typically trained with sufficient labeled data. However, this\nprocess demands extensive data collection over extended periods, burdening the\nsubject. To address this, utilizing information from pre-training on multiple\nsubjects for the training of the target subject could be beneficial. This paper\nproposes an inter-subject variance transfer learning method based on a Bayesian\napproach. This method is founded on the simple hypothesis that while the means\nof EMG features vary greatly across subjects, their variances may exhibit\nsimilar patterns. Our approach transfers variance information, acquired through\npre-training on multiple source subjects, to a target subject within a Bayesian\nupdating framework, thereby allowing accurate classification using limited\ntarget calibration data. A coefficient was also introduced to adjust the amount\nof information transferred for efficient transfer learning. Experimental\nevaluations using two EMG datasets demonstrated the effectiveness of our\nvariance transfer strategy and its superiority compared to existing methods.", "published": "2025-05-21 11:18:39", "link": "http://arxiv.org/abs/2505.15381v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Robust Secure Communications in Near-Field ISCAP Systems with Extremely Large-Scale Antenna Array", "abstract": "This paper investigates robust secure communications in a near-field\nintegrated sensing, communication, and powering (ISCAP) system, in which the\nbase station (BS) is equipped with an extremely large-scale antenna array\n(ELAA). In this system, the BS transmits confidential messages to a single\nlegitimate communication user (CU), simultaneously providing wireless power\ntransfer to multiple energy receivers (ERs) and performing point target\nsensing. We consider a scenario in which both the ERs and the sensing target\nmay act as potential eavesdroppers attempting to intercept the confidential\nmessages. To safeguard secure communication, the BS employs a joint beamforming\ndesign by transmitting information beams combined with dedicated triple-purpose\nbeams serving as energy and sensing signals, as well as artificial noise (AN)\nfor effectively jamming potential eavesdroppers. It is assumed that only coarse\nlocation information of the ERs and sensing targets or eavesdroppers is\navailable at the BS, leading to imperfect channel state information (CSI).\nUnder this setup, we formulate a robust beamforming optimization problem with\nthe objective of maximizing the secrecy rate for the CU, while ensuring\nworst-case performance requirements on both target sensing and wireless energy\nharvesting at the ERs. To address the non-convex robust joint beamforming\nproblem and facilitate the deployment of a low-complexity algorithm, we employ\nthe S-procedure alongside an eavesdropping CSI error-bound determination method\nto acquire a high-quality solution.", "published": "2025-05-21 08:58:28", "link": "http://arxiv.org/abs/2505.15279v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control", "abstract": "Electromyogram (EMG) signals recorded from the skin surface enable intuitive\ncontrol of assistive devices such as prosthetic limbs. However, in EMG-based\nmotion recognition, collecting comprehensive training data for all target\nmotions remains challenging, particularly for complex combined motions. This\npaper proposes a method to efficiently recognize combined motions using\nsynthetic EMG data generated through convex combinations of basic motion\npatterns. Instead of measuring all possible combined motions, the proposed\nmethod utilizes measured basic motion data along with synthetically combined\nmotion data for training. This approach expands the range of recognizable\ncombined motions while minimizing the required training data collection. We\nevaluated the effectiveness of the proposed method through an upper limb motion\nclassification experiment with eight subjects. The experimental results\ndemonstrated that the proposed method improved the classification accuracy for\nunseen combined motions by approximately 17%.", "published": "2025-05-21 07:46:26", "link": "http://arxiv.org/abs/2505.15218v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network", "abstract": "Automated epileptic seizure detection from electroencephalogram (EEG) remains\nchallenging due to significant individual differences in EEG patterns across\npatients. While existing studies achieve high accuracy with patient-specific\napproaches, they face difficulties in generalizing to new patients. To address\nthis, we propose a detection framework combining domain adversarial training\nwith a convolutional neural network (CNN) and a bidirectional long short-term\nmemory (BiLSTM). First, the CNN extracts local patient-invariant features\nthrough domain adversarial training, which optimizes seizure detection accuracy\nwhile minimizing patient-specific characteristics. Then, the BiLSTM captures\ntemporal dependencies in the extracted features to model seizure evolution\npatterns. Evaluation using EEG recordings from 20 patients with focal epilepsy\ndemonstrated superior performance over non-adversarial methods, achieving high\ndetection accuracy across different patients. The integration of adversarial\ntraining with temporal modeling enables robust cross-patient seizure detection.", "published": "2025-05-21 07:27:55", "link": "http://arxiv.org/abs/2505.15203v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Study of Brain Connectivity by Multichannel EEG Quaternion Principal Component Analysis for Alzheimer Disease Classification", "abstract": "The early detection of Alzheimer's disease (AD) through widespread screening\nhas emerged as a primary strategy to mitigate the significant global impact of\nAD. EEG measurements offer a promising solution for extensive AD detection.\nHowever, the intricate and nonlinear dynamics of multichannel EEG signals pose\na considerable challenge for real-time AD diagnosis. This paper introduces a\nnovel algorithm, which is based on Quaternion Principal Component Analysis\n(QPCA) of multichannel EEG signals, for AD classification. The algorithm\nextracts high dimensional correlations among different channels to generate\nfeatures that are maximally representative with minimal information redundancy.\nThis provides a multidimensional and precise measure of brain connectivity in\ndisease assessment. Simulations have been conducted to evaluate the performance\nand to identify the most critical EEG channels or brain regions for AD\nclassification. The results reveal a significant drop of connectivity measure\nin the alpha bands. The average AD classification accuracy for all 4-channel\ncombinations reached 95%, while some particular permutations of channels\nachieved 100% accuracy rate. Furthermore, the temporal lobe emerges as one of\nthe most important regions in AD classification given that the EEG signals are\nrecorded during the presentation of an auditory stimulant. The selection of key\nparameters of the QPCA algorithm have been evaluated and some recommendations\nare proposed for further performance enhancement. This paper marks the first\napplication of the QPCA algorithm for AD classification and brain connectivity\nanalysis using multichannel EEG signals.", "published": "2025-05-21 03:19:25", "link": "http://arxiv.org/abs/2505.15052v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Rate-Distortion Optimization with Non-Reference Metrics for UGC Compression", "abstract": "Service providers must encode a large volume of noisy videos to meet the\ndemand for user-generated content (UGC) in online video-sharing platforms.\nHowever, low-quality UGC challenges conventional codecs based on\nrate-distortion optimization (RDO) with full-reference metrics (FRMs). While\neffective for pristine videos, FRMs drive codecs to preserve artifacts when the\ninput is degraded, resulting in suboptimal compression. A more suitable\napproach used to assess UGC quality is based on non-reference metrics (NRMs).\nHowever, RDO with NRMs as a measure of distortion requires an iterative\nworkflow of encoding, decoding, and metric evaluation, which is computationally\nimpractical. This paper overcomes this limitation by linearizing the NRM around\nthe uncompressed video. The resulting cost function enables block-wise bit\nallocation in the transform domain by estimating the alignment of the\nquantization error with the gradient of the NRM. To avoid large deviations from\nthe input, we add sum of squared errors (SSE) regularization. We derive\nexpressions for both the SSE regularization parameter and the Lagrangian, akin\nto the relationship used for SSE-RDO. Experiments with images and videos show\nbitrate savings of more than 30\\% over SSE-RDO using the target NRM, with no\ndecoder complexity overhead and minimal encoder complexity increase.", "published": "2025-05-21 01:15:32", "link": "http://arxiv.org/abs/2505.15003v1", "categories": ["eess.IV", "eess.SP"], "primary_category": "eess.IV"}
{"title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research.", "published": "2025-05-21 14:44:40", "link": "http://arxiv.org/abs/2505.15872v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Linear Approach to Data Poisoning", "abstract": "We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training.", "published": "2025-05-21 06:45:06", "link": "http://arxiv.org/abs/2505.15175v2", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "abstract": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.", "published": "2025-05-21 19:17:29", "link": "http://arxiv.org/abs/2505.15957v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction", "abstract": "Effective evaluation of web data record extraction methods is crucial, yet\nhampered by static, domain-specific benchmarks and opaque scoring practices.\nThis makes fair comparison between traditional algorithmic techniques, which\nrely on structural heuristics, and Large Language Model (LLM)-based approaches,\noffering zero-shot extraction across diverse layouts, particularly challenging.\nTo overcome these limitations, we introduce a concrete evaluation framework.\nOur framework systematically generates evaluation datasets from arbitrary MHTML\nsnapshots, annotates XPath-based supervision labels, and employs\nstructure-aware metrics for consistent scoring, specifically preventing text\nhallucination and allowing only for the assessment of positional hallucination.\nIt also incorporates preprocessing strategies to optimize input for LLMs while\npreserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON.\nAdditionally, we created a publicly available synthetic dataset by transforming\nDOM structures and modifying content. We benchmark deterministic heuristic\nalgorithms and off-the-shelf LLMs across these multiple input formats. Our\nbenchmarking shows that Flat JSON input enables LLMs to achieve superior\nextraction accuracy (F1 score of 0.9567) and minimal hallucination compared to\nother input formats like Slimmed HTML and Hierarchical JSON. We establish a\nstandardized foundation for rigorous benchmarking, paving the way for the next\nprincipled advancements in web data record extraction.", "published": "2025-05-21 21:03:37", "link": "http://arxiv.org/abs/2505.17125v1", "categories": ["cs.DB", "cs.AI", "cs.IR"], "primary_category": "cs.DB"}
{"title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "abstract": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "published": "2025-05-21 16:29:00", "link": "http://arxiv.org/abs/2505.17117v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization", "abstract": "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and\nMulti-Agent Debate (MAD), have been proposed to further enrich Large Language\nModels' (LLMs) complex problem-solving capacities in reasoning scenarios.\nHowever, these methods may fail to solve complex problems due to the lack of\nability to find optimal solutions. Swarm Intelligence has been serving as a\npowerful tool for finding optima in the field of traditional optimization\nproblems. To this end, we propose integrating swarm intelligence into the\nreasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)\nparadigm. In this paradigm, we formulate LLM reasoning as an optimization\nproblem and use a swarm intelligence scheme to guide a group of LLM-based\nagents in collaboratively searching for optimal solutions. To avoid swarm\nintelligence getting trapped in local optima, we further develop a Swarm\nIntelligence Enhancing Reasoning (SIER) framework, which develops a\ndensity-driven strategy to enhance the reasoning ability. To be specific, we\npropose to perform kernel density estimation and non-dominated sorting to\noptimize both solution quality and diversity simultaneously. In this case, SIER\nefficiently enhances solution space exploration through expanding the diversity\nof the reasoning path. Besides, a step-level quality evaluation is used to help\nagents improve solution quality by correcting low-quality intermediate steps.\nThen, we use quality thresholds to dynamically control the termination of\nexploration and the selection of candidate steps, enabling a more flexible and\nefficient reasoning process. Extensive experiments are ...", "published": "2025-05-21 15:48:13", "link": "http://arxiv.org/abs/2505.17115v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution", "abstract": "Large Language Model (LLM) agents can automate cybersecurity tasks and can\nadapt to the evolving cybersecurity landscape without re-engineering. While LLM\nagents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF)\ncompetitions, they have two key limitations: accessing latest cybersecurity\nexpertise beyond training data, and integrating new knowledge into complex task\nplanning. Knowledge-based approaches that incorporate technical understanding\ninto the task-solving automation can tackle these limitations. We present\nCRAKEN, a knowledge-based LLM agent framework that improves cybersecurity\ncapability through three core mechanisms: contextual decomposition of\ntask-critical information, iterative self-reflected knowledge retrieval, and\nknowledge-hint injection that transforms insights into adaptive attack\nstrategies. Comprehensive evaluations with different configurations show\nCRAKEN's effectiveness in multi-stage vulnerability detection and exploitation\ncompared to previous approaches. Our extensible architecture establishes new\nmethodologies for embedding new security knowledge into LLM-driven\ncybersecurity agentic systems. With a knowledge database of CTF writeups,\nCRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works\nby 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK\ntechniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating\nimproved cybersecurity capabilities via knowledge-based execution. We make our\nframework open source to public\nhttps://github.com/NYU-LLM-CTF/nyuctf_agents_craken.", "published": "2025-05-21 11:01:11", "link": "http://arxiv.org/abs/2505.17107v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Conformal Language Model Reasoning with Coherent Factuality", "abstract": "Language models are increasingly being used in important decision pipelines,\nso ensuring the correctness of their outputs is crucial. Recent work has\nproposed evaluating the \"factuality\" of claims decomposed from a language model\ngeneration and applying conformal prediction techniques to filter out those\nclaims that are not factual. This can be effective for tasks such as\ninformation retrieval, where constituent claims may be evaluated in isolation\nfor factuality, but is not appropriate for reasoning tasks, as steps of a\nlogical argument can be evaluated for correctness only within the context of\nthe claims that precede them. To capture this, we define \"coherent factuality\"\nand develop a conformal-prediction-based method to guarantee coherent\nfactuality for language model outputs. Our approach applies split conformal\nprediction to subgraphs within a \"deducibility\" graph\" that represents the\nsteps of a reasoning problem. We evaluate our method on mathematical reasoning\nproblems from the MATH and FELM datasets and find that our algorithm\nconsistently produces correct and substantiated orderings of claims, achieving\ncoherent factuality across target coverage levels. Moreover, we achieve 90%\nfactuality on our stricter definition while retaining 80% or more of the\noriginal claims, highlighting the utility of our deducibility-graph-guided\napproach.", "published": "2025-05-21 22:40:51", "link": "http://arxiv.org/abs/2505.17126v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech", "abstract": "In this paper, we propose a novel framework to control voice style in\nprompt-based, controllable text-to-speech systems by leveraging textual\npersonas as voice style prompts. We present two persona rewriting strategies to\ntransform generic persona descriptions into speech-oriented prompts, enabling\nfine-grained manipulation of prosodic attributes such as pitch, emotion, and\nspeaking rate. Experimental results demonstrate that our methods enhance the\nnaturalness, clarity, and consistency of synthesized speech. Finally, we\nanalyze implicit social biases introduced by LLM-based rewriting, with a focus\non gender. We underscore voice style as a crucial factor for persona-driven AI\ndialogue systems.", "published": "2025-05-21 01:28:56", "link": "http://arxiv.org/abs/2505.17093v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization", "abstract": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our code\nwill be released on https://github.com/Zillwang/StepSearch.", "published": "2025-05-21 05:01:31", "link": "http://arxiv.org/abs/2505.15107v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ThinkRec: Thinking-based recommendation via LLM", "abstract": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.", "published": "2025-05-21 04:25:18", "link": "http://arxiv.org/abs/2505.15091v3", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "ASVspoof2019 vs. ASVspoof5: Assessment and Comparison", "abstract": "ASVspoof challenges are designed to advance the understanding of spoofing\nspeech attacks and encourage the development of robust countermeasure systems.\nThese challenges provide a standardized database for assessing and comparing\nspoofing-robust automatic speaker verification solutions. The ASVspoof5\nchallenge introduces a shift in database conditions compared to ASVspoof2019.\nWhile ASVspoof2019 has mismatched conditions only in spoofing attacks in the\nevaluation set, ASVspoof5 incorporates mismatches in both bona fide and spoofed\nspeech statistics. This paper examines the impact of these mismatches,\npresenting qualitative and quantitative comparisons within and between the two\ndatabases. We show the increased difficulty for genuine and spoofed speech and\ndemonstrate that in ASVspoof5, not only are the attacks more challenging, but\nthe genuine speech also shifts toward spoofed speech compared to ASVspoof2019.", "published": "2025-05-21 18:04:44", "link": "http://arxiv.org/abs/2505.15911v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "abstract": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "published": "2025-05-21 16:13:57", "link": "http://arxiv.org/abs/2505.15700v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Relevance of Clinical Assessment Tasks for the Automatic Detection of Parkinson's Disease Medication State from Speech", "abstract": "The automatic identification of medication states of Parkinson's disease (PD)\npatients can assist clinicians in monitoring and scheduling personalized\ntreatments, as well as studying the effects of medication in alleviating the\nmotor symptoms that characterize the disease. This paper explores speech as a\nnon-invasive and accessible biomarker for identifying PD medication states,\nintroducing a novel approach that addresses this task from a\nspeaker-independent perspective. While traditional machine learning models\nachieve competitive results, self-supervised speech representations prove\nessential for optimal performance, significantly surpassing knowledge-based\nacoustic descriptors. Experiments across diverse speech assessment tasks\nhighlight the relevance of prosody and continuous speech in distinguishing\nmedication states, reaching an F1-score of 88.2%. These findings may streamline\nclinicians' work and reduce patient effort in voice recordings.", "published": "2025-05-21 11:15:26", "link": "http://arxiv.org/abs/2505.15378v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting", "abstract": "Radio Frequency (RF) fingerprinting offers a promising approach for drone\nidentification and security, although it suffers from significant performance\ndegradation when operating on different transmission channels. This paper\npresents CrossRF, a domain-invariant deep learning approach that addresses the\nproblem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)\nidentification. Our approach aims to minimize the domain gap between different\nRF channels by using adversarial learning to train a more robust model that\nmaintains consistent identification performance despite channel variations. We\nvalidate our approach using the UAVSig dataset, comprising real-world\nover-the-air RF signals from identical drone models operating across several\nfrequency channels, ensuring that the findings correspond to real-world\nscenarios. The experimental results show CrossRF's efficiency, achieving up to\n99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only\n26.39% using conventional methods. The model maintains robust performance in\nmore difficult multi-channel scenarios (87.57% accuracy adapting from Channels\n1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller\nclassification. These results confirm CrossRF's ability to significantly reduce\nperformance degradation due to cross-channel variations while maintaining high\nidentification accuracy with minimal training data requirements, making it\nparticularly suitable for practical drone security applications.", "published": "2025-05-21 12:20:10", "link": "http://arxiv.org/abs/2505.18200v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "abstract": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "published": "2025-05-21 16:29:00", "link": "http://arxiv.org/abs/2505.17117v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization", "abstract": "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and\nMulti-Agent Debate (MAD), have been proposed to further enrich Large Language\nModels' (LLMs) complex problem-solving capacities in reasoning scenarios.\nHowever, these methods may fail to solve complex problems due to the lack of\nability to find optimal solutions. Swarm Intelligence has been serving as a\npowerful tool for finding optima in the field of traditional optimization\nproblems. To this end, we propose integrating swarm intelligence into the\nreasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)\nparadigm. In this paradigm, we formulate LLM reasoning as an optimization\nproblem and use a swarm intelligence scheme to guide a group of LLM-based\nagents in collaboratively searching for optimal solutions. To avoid swarm\nintelligence getting trapped in local optima, we further develop a Swarm\nIntelligence Enhancing Reasoning (SIER) framework, which develops a\ndensity-driven strategy to enhance the reasoning ability. To be specific, we\npropose to perform kernel density estimation and non-dominated sorting to\noptimize both solution quality and diversity simultaneously. In this case, SIER\nefficiently enhances solution space exploration through expanding the diversity\nof the reasoning path. Besides, a step-level quality evaluation is used to help\nagents improve solution quality by correcting low-quality intermediate steps.\nThen, we use quality thresholds to dynamically control the termination of\nexploration and the selection of candidate steps, enabling a more flexible and\nefficient reasoning process. Extensive experiments are ...", "published": "2025-05-21 15:48:13", "link": "http://arxiv.org/abs/2505.17115v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
