{"title": "FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis", "abstract": "This paper presents a novel approach for explainability in financial analysis\nby deriving financially-explainable statistical relationships through\naspect-based sentiment analysis, Pearson correlation, Granger causality &\nuncertainty coefficient. The proposed methodology involves constructing an\naspect list from financial literature and applying aspect-based sentiment\nanalysis on social media text to compute sentiment scores for each aspect.\nPearson correlation is then applied to uncover financially explainable\nrelationships between aspect sentiment scores and stock prices. Findings for\nderived relationships are made robust by applying Granger causality to\ndetermine the forecasting ability of each aspect sentiment score for stock\nprices. Finally, an added layer of interpretability is added by evaluating\nuncertainty coefficient scores between aspect sentiment scores and stock\nprices. This allows us to determine the aspects whose sentiment scores are most\nstatistically significant for stock prices. Relative to other methods, our\napproach provides a more informative and accurate understanding of the\nrelationship between sentiment analysis and stock prices. Specifically, this\nmethodology enables an interpretation of the statistical relationship between\naspect-based sentiment scores and stock prices, which offers explainability to\nAI-driven financial decision-making.", "published": "2023-03-05 03:18:56", "link": "http://arxiv.org/abs/2303.02563v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effectiveness of Data Augmentation for Parameter Efficient Tuning with\n  Limited Data", "abstract": "Recent work has demonstrated that using parameter efficient tuning techniques\nsuch as prefix tuning (or P-tuning) on pretrained language models can yield\nperformance that is comparable or superior to fine-tuning while dramatically\nreducing trainable parameters. Nevertheless, the effectiveness of such methods\nunder the context of data augmentation, a common strategy to improve learning\nunder low data regimes, has not been fully explored. In this paper, we examine\nthe effectiveness of several popular task-agnostic data augmentation\ntechniques, i.e., EDA, Back Translation, and Mixup, when using two general\nparameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity.\nWe show that data augmentation can be used to boost the performance of P-tuning\nand LoRA models, but the effectiveness of each technique varies and certain\nmethods can lead to a notable degradation in performance, particularly when\nusing larger models and on harder tasks. We further analyze the sentence\nrepresentations of P-tuning compared to fine-tuning to help understand the\nabove behaviour, and reveal how P-tuning generally presents a more limited\nability to separate the sentence embeddings from different classes of augmented\ndata. In addition, it displays poorer performance on heavily altered data.\nHowever, we demonstrate that by adding a simple contrastive loss function it\ncan help mitigate such issues for prefix tuning, resulting in sizable\nimprovements to augmented data performance.", "published": "2023-03-05 04:12:17", "link": "http://arxiv.org/abs/2303.02577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Based Counterfactual Queries for Visual Question Answering", "abstract": "Visual Question Answering (VQA) has been a popular task that combines vision\nand language, with numerous relevant implementations in literature. Even though\nthere are some attempts that approach explainability and robustness issues in\nVQA models, very few of them employ counterfactuals as a means of probing such\nchallenges in a model-agnostic way. In this work, we propose a systematic\nmethod for explaining the behavior and investigating the robustness of VQA\nmodels through counterfactual perturbations. For this reason, we exploit\nstructured knowledge bases to perform deterministic, optimal and controllable\nword-level replacements targeting the linguistic modality, and we then evaluate\nthe model's response against such counterfactual inputs. Finally, we\nqualitatively extract local and global explanations based on counterfactual\nresponses, which are ultimately proven insightful towards interpreting VQA\nmodel behaviors. By performing a variety of perturbation types, targeting\ndifferent parts of speech of the input question, we gain insights to the\nreasoning of the model, through the comparison of its responses in different\nadversarial circumstances. Overall, we reveal possible biases in the\ndecision-making process of the model, as well as expected and unexpected\npatterns, which impact its performance quantitatively and qualitatively, as\nindicated by our analysis.", "published": "2023-03-05 08:00:30", "link": "http://arxiv.org/abs/2303.02601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining both Commonality and Specificity from Multiple Documents for\n  Multi-Document Summarization", "abstract": "The multi-document summarization task requires the designed summarizer to\ngenerate a short text that covers the important information of original\ndocuments and satisfies content diversity. This paper proposes a multi-document\nsummarization approach based on hierarchical clustering of documents. It\nutilizes the constructed class tree of documents to extract both the sentences\nreflecting the commonality of all documents and the sentences reflecting the\nspecificity of some subclasses of these documents for generating a summary, so\nas to satisfy the coverage and diversity requirements of multi-document\nsummarization. Comparative experiments with different variant approaches on\nDUC'2002-2004 datasets prove the effectiveness of mining both the commonality\nand specificity of documents for multi-document summarization. Experiments on\nDUC'2004 and Multi-News datasets show that our approach achieves competitive\nperformance compared to the state-of-the-art unsupervised and supervised\napproaches.", "published": "2023-03-05 14:25:05", "link": "http://arxiv.org/abs/2303.02677v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Industry Risk Assessment via Hierarchical Financial Data Using Stock\n  Market Sentiment Indicators", "abstract": "Risk assessment across industries is paramount for ensuring a robust and\nsustainable economy. While previous studies have relied heavily on official\nstatistics for their accuracy, they often lag behind real-time developments.\nAddressing this gap, our research endeavors to integrate market microstructure\ntheory with AI technologies to refine industry risk predictions. This paper\npresents an approach to analyzing industry trends leveraging real-time stock\nmarket data and generative small language models (SLMs). By enhancing the\ntimeliness of risk assessments and delving into the influence of\nnon-traditional factors such as market sentiment and investor behavior, we\nstrive to develop a more holistic and dynamic risk assessment model. One of the\nkey challenges lies in the inherent noise in raw data, which can compromise the\nprecision of statistical analyses. Moreover, textual data about industry\nanalysis necessitates a deeper understanding facilitated by pre-trained\nlanguage models. To tackle these issues, we propose a dual-pronged approach to\nindustry trend analysis: explicit and implicit analysis. For explicit analysis,\nwe employ a hierarchical data analysis methodology that spans the industry and\nindividual listed company levels. This strategic breakdown helps mitigate the\nimpact of data noise, ensuring a more accurate portrayal of industry dynamics.\nIn parallel, we introduce implicit analysis, where we pre-train an SML to\ninterpret industry trends within the context of current news events. This\napproach leverages the extensive knowledge embedded in the pre-training corpus,\nenabling a nuanced understanding of industry trends and their underlying\ndrivers. Experimental results based on our proposed methodology demonstrate its\neffectiveness in delivering robust industry trend analyses, underscoring its\npotential to revolutionize risk assessment practices across industries.", "published": "2023-03-05 16:17:56", "link": "http://arxiv.org/abs/2303.02707v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "WADER at SemEval-2023 Task 9: A Weak-labelling framework for Data\n  augmentation in tExt Regression Tasks", "abstract": "Intimacy is an essential element of human relationships and language is a\ncrucial means of conveying it. Textual intimacy analysis can reveal social\nnorms in different contexts and serve as a benchmark for testing computational\nmodels' ability to understand social information. In this paper, we propose a\nnovel weak-labeling strategy for data augmentation in text regression tasks\ncalled WADER. WADER uses data augmentation to address the problems of data\nimbalance and data scarcity and provides a method for data augmentation in\ncross-lingual, zero-shot tasks. We benchmark the performance of\nState-of-the-Art pre-trained multilingual language models using WADER and\nanalyze the use of sampling techniques to mitigate bias in data and optimally\nselect augmentation candidates. Our results show that WADER outperforms the\nbaseline model and provides a direction for mitigating data imbalance and\nscarcity in text regression tasks.", "published": "2023-03-05 19:45:42", "link": "http://arxiv.org/abs/2303.02758v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-Based Learning for Thread Structure Prediction in Cybersecurity\n  Forums", "abstract": "With recent trends indicating cyber crimes increasing in both frequency and\ncost, it is imperative to develop new methods that leverage data-rich hacker\nforums to assist in combating ever evolving cyber threats. Defining\ninteractions within these forums is critical as it facilitates identifying\nhighly skilled users, which can improve prediction of novel threats and future\ncyber attacks. We propose a method called Next Paragraph Prediction with\nInstructional Prompting (NPP-IP) to predict thread structures while grounded on\nthe context around posts. This is the first time to apply an instructional\nprompting approach to the cybersecurity domain. We evaluate our NPP-IP with the\nReddit dataset and Hacker Forums dataset that has posts and thread structures\nof real hacker forums' threads, and compare our method's performance with\nexisting methods. The experimental evaluation shows that our proposed method\ncan predict the thread structure significantly better than existing methods\nallowing for better social network prediction based on forum interactions.", "published": "2023-03-05 04:26:17", "link": "http://arxiv.org/abs/2303.05400v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Hybrid Y-Net Architecture for Singing Voice Separation", "abstract": "This research paper presents a novel deep learning-based neural network\narchitecture, named Y-Net, for achieving music source separation. The proposed\narchitecture performs end-to-end hybrid source separation by extracting\nfeatures from both spectrogram and waveform domains. Inspired by the U-Net\narchitecture, Y-Net predicts a spectrogram mask to separate vocal sources from\na mixture signal. Our results demonstrate the effectiveness of the proposed\narchitecture for music source separation with fewer parameters. Overall, our\nwork presents a promising approach for improving the accuracy and efficiency of\nmusic source separation.", "published": "2023-03-05 07:54:49", "link": "http://arxiv.org/abs/2303.02599v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Heterogeneous Graph Learning for Acoustic Event Classification", "abstract": "Heterogeneous graphs provide a compact, efficient, and scalable way to model\ndata involving multiple disparate modalities. This makes modeling audiovisual\ndata using heterogeneous graphs an attractive option. However, graph structure\ndoes not appear naturally in audiovisual data. Graphs for audiovisual data are\nconstructed manually which is both difficult and sub-optimal. In this work, we\naddress this problem by (i) proposing a parametric graph construction strategy\nfor the intra-modal edges, and (ii) learning the crossmodal edges. To this end,\nwe develop a new model, heterogeneous graph crossmodal network (HGCN) that\nlearns the crossmodal edges. Our proposed model can adapt to various spatial\nand temporal scales owing to its parametric construction, while the learnable\ncrossmodal edges effectively connect the relevant nodes across modalities.\nExperiments on a large benchmark dataset (AudioSet) show that our model is\nstate-of-the-art (0.53 mean average precision), outperforming transformer-based\nmodels and other graph-based models.", "published": "2023-03-05 13:06:53", "link": "http://arxiv.org/abs/2303.02665v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-frequency Network for Robust Speaker Recognition", "abstract": "The wide deployment of speech-based biometric systems usually demands\nhigh-performance speaker recognition algorithms. However, most of the prior\nworks for speaker recognition either process the speech in the frequency domain\nor time domain, which may produce suboptimal results because both time and\nfrequency domains are important for speaker recognition. In this paper, we\nattempt to analyze the speech signal in both time and frequency domains and\npropose the time-frequency network~(TFN) for speaker recognition by extracting\nand fusing the features in the two domains. Based on the recent advance of deep\nneural networks, we propose a convolution neural network to encode the raw\nspeech waveform and the frequency spectrum into domain-specific features, which\nare then fused and transformed into a classification feature space for speaker\nrecognition. Experimental results on the publicly available datasets TIMIT and\nLibriSpeech show that our framework is effective to combine the information in\nthe two domains and performs better than the state-of-the-art methods for\nspeaker recognition.", "published": "2023-03-05 13:48:47", "link": "http://arxiv.org/abs/2303.02673v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparative Study of Self-Supervised Speech Representations in Read\n  and Spontaneous TTS", "abstract": "Recent work has explored using self-supervised learning (SSL) speech\nrepresentations such as wav2vec2.0 as the representation medium in standard\ntwo-stage TTS, in place of conventionally used mel-spectrograms. It is however\nunclear which speech SSL is the better fit for TTS, and whether or not the\nperformance differs between read and spontaneous TTS, the later of which is\narguably more challenging. This study aims at addressing these questions by\ntesting several speech SSLs, including different layers of the same SSL, in\ntwo-stage TTS on both read and spontaneous corpora, while maintaining constant\nTTS model architecture and training settings. Results from listening tests show\nthat the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other\ntested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work\nsheds light on both how speech SSL can readily improve current TTS systems, and\nhow SSLs compare in the challenging generative task of TTS. Audio examples can\nbe found at https://www.speech.kth.se/tts-demos/ssr_tts", "published": "2023-03-05 17:20:10", "link": "http://arxiv.org/abs/2303.02719v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T05", "I.2.7; I.2.6; H.5.5"], "primary_category": "eess.AS"}
