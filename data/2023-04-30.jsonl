{"title": "Building a Non-native Speech Corpus Featuring Chinese-English Bilingual\n  Children: Compilation and Rationale", "abstract": "This paper introduces a non-native speech corpus consisting of narratives\nfrom fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5\nhours of children taking a narrative comprehension test in English (L2) are\npresented, along with human-rated scores and annotations of grammatical and\npronunciation errors. The children also completed the parallel MAIN tests in\nChinese (L1) for reference purposes. For all tests we recorded audio and video\nwith our innovative self-developed remote collection methods. The video\nrecordings serve to mitigate the challenge of low intelligibility in L2\nnarratives produced by young children during the transcription process. This\ncorpus offers valuable resources for second language teaching and has the\npotential to enhance the overall performance of automatic speech recognition\n(ASR).", "published": "2023-04-30 10:41:43", "link": "http://arxiv.org/abs/2305.00446v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to enumerate trees from a context-free grammar", "abstract": "I present a simple algorithm for enumerating the trees generated by a Context\nFree Grammar (CFG). The algorithm uses a pairing function to form a bijection\nbetween CFG derivations and natural numbers, so that trees can be uniquely\ndecoded from counting. This provides a general way to number expressions in\nnatural logical languages, and potentially can be extended to other\ncombinatorial problems. I also show how this algorithm may be generalized to\nmore general forms of derivation, including analogs of Lempel-Ziv coding on\ntrees.", "published": "2023-04-30 16:40:54", "link": "http://arxiv.org/abs/2305.00522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via\n  ChatGPT for Mental Health Support", "abstract": "Developing specialized dialogue systems for mental health support requires\nmulti-turn conversation data, which has recently garnered increasing attention.\nHowever, gathering and releasing large-scale, real-life multi-turn\nconversations that could facilitate advancements in mental health support\npresents challenges in data privacy protection and the time and cost involved\nin crowdsourcing. To address these challenges, we introduce SMILE, a\nsingle-turn to multi-turn inclusive language expansion technique that prompts\nChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work\nbegins by analyzing language transformation and validating the feasibility of\nour proposed method. We conduct a study on dialogue diversity, including\nlexical features, semantic features, and dialogue topics, demonstrating the\neffectiveness of our method. Further, we employ our method to generate a\nlarge-scale, lifelike, and diverse dialogue dataset named SMILECHAT, consisting\nof 55k dialogues. Finally, we utilize the collected corpus to develop a mental\nhealth chatbot, MeChat. To better assess the quality of SMILECHAT, we collect a\nsmall-scale real-life counseling dataset conducted by data anonymization. Both\nautomatic and human evaluations demonstrate significant improvements in our\ndialogue system and confirm that SMILECHAT is high-quality. Code, data, and\nmodel are publicly available at https://github.com/qiuhuachuan/smile.", "published": "2023-04-30 11:26:10", "link": "http://arxiv.org/abs/2305.00450v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reliable Gradient-free and Likelihood-free Prompt Tuning", "abstract": "Due to privacy or commercial constraints, large pre-trained language models\n(PLMs) are often offered as black-box APIs. Fine-tuning such models to\ndownstream tasks is challenging because one can neither access the model's\ninternal representations nor propagate gradients through it. This paper\naddresses these challenges by developing techniques for adapting PLMs with only\nAPI access. Building on recent work on soft prompt tuning, we develop methods\nto tune the soft prompts without requiring gradient computation. Further, we\ndevelop extensions that in addition to not requiring gradients also do not need\nto access any internal representation of the PLM beyond the input embeddings.\nMoreover, instead of learning a single prompt, our methods learn a distribution\nover prompts allowing us to quantify predictive uncertainty. Ours is the first\nwork to consider uncertainty in prompts when only having API access to the PLM.\nFinally, through extensive experiments, we carefully vet the proposed methods\nand find them competitive with (and sometimes even improving on) gradient-based\napproaches with full access to the PLM.", "published": "2023-04-30 22:33:08", "link": "http://arxiv.org/abs/2305.00593v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "S2abEL: A Dataset for Entity Linking from Scientific Tables", "abstract": "Entity linking (EL) is the task of linking a textual mention to its\ncorresponding entry in a knowledge base, and is critical for many\nknowledge-intensive NLP applications. When applied to tables in scientific\npapers, EL is a step toward large-scale scientific knowledge bases that could\nenable advanced scientific question answering and analytics. We present the\nfirst dataset for EL in scientific tables. EL for scientific tables is\nespecially challenging because scientific knowledge bases can be very\nincomplete, and disambiguating table mentions typically requires understanding\nthe papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL\nin machine learning results tables and includes hand-labeled cell types,\nattributed sources, and entity links from the PaperswithCode taxonomy for 8,429\ncells from 732 tables. We introduce a neural baseline method designed for EL on\nscientific tables containing many out-of-knowledge-base mentions, and show that\nit significantly outperforms a state-of-the-art generic table EL method. The\nbest baselines fall below human performance, and our analysis highlights\navenues for improvement.", "published": "2023-04-30 02:07:22", "link": "http://arxiv.org/abs/2305.00366v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Constructing a Knowledge Graph from Textual Descriptions of Software\n  Vulnerabilities in the National Vulnerability Database", "abstract": "Knowledge graphs have shown promise for several cybersecurity tasks, such as\nvulnerability assessment and threat analysis. In this work, we present a new\nmethod for constructing a vulnerability knowledge graph from information in the\nNational Vulnerability Database (NVD). Our approach combines named entity\nrecognition (NER), relation extraction (RE), and entity prediction using a\ncombination of neural models, heuristic rules, and knowledge graph embeddings.\nWe demonstrate how our method helps to fix missing entities in knowledge graphs\nused for cybersecurity and evaluate the performance.", "published": "2023-04-30 04:23:40", "link": "http://arxiv.org/abs/2305.00382v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.CR"}
{"title": "Multimodal Graph Transformer for Multimodal Question Answering", "abstract": "Despite the success of Transformer models in vision and language tasks, they\noften learn knowledge from enormous data implicitly and cannot utilize\nstructured input data directly. On the other hand, structured learning\napproaches such as graph neural networks (GNNs) that integrate prior\ninformation can barely compete with Transformer models. In this work, we aim to\nbenefit from both worlds and propose a novel Multimodal Graph Transformer for\nquestion answering tasks that requires performing reasoning across multiple\nmodalities. We introduce a graph-involved plug-and-play quasi-attention\nmechanism to incorporate multimodal graph information, acquired from text and\nvisual data, to the vanilla self-attention as effective prior. In particular,\nwe construct the text graph, dense region graph, and semantic graph to generate\nadjacency matrices, and then compose them with input vision and language\nfeatures to perform downstream reasoning. Such a way of regularizing\nself-attention with graph information significantly improves the inferring\nability and helps align features from different modalities. We validate the\neffectiveness of Multimodal Graph Transformer over its Transformer baselines on\nGQA, VQAv2, and MultiModalQA datasets.", "published": "2023-04-30 21:22:35", "link": "http://arxiv.org/abs/2305.00581v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How does GPT-2 compute greater-than?: Interpreting mathematical\n  abilities in a pre-trained language model", "abstract": "Pre-trained language models can be surprisingly adept at tasks they were not\nexplicitly trained on, but how they implement these capabilities is poorly\nunderstood. In this paper, we investigate the basic mathematical abilities\noften acquired by pre-trained language models. Concretely, we use mechanistic\ninterpretability techniques to explain the (limited) mathematical abilities of\nGPT-2 small. As a case study, we examine its ability to take in sentences such\nas \"The war lasted from the year 1732 to the year 17\", and predict valid\ntwo-digit end years (years > 32). We first identify a circuit, a small subset\nof GPT-2 small's computational graph that computes this task's output. Then, we\nexplain the role of each circuit component, showing that GPT-2 small's final\nmulti-layer perceptrons boost the probability of end years greater than the\nstart year. Finally, we find related tasks that activate our circuit. Our\nresults suggest that GPT-2 small computes greater-than using a complex but\ngeneral mechanism that activates across diverse contexts.", "published": "2023-04-30 21:44:21", "link": "http://arxiv.org/abs/2305.00586v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NewsPanda: Media Monitoring for Timely Conservation Action", "abstract": "Non-governmental organizations for environmental conservation have a\nsignificant interest in monitoring conservation-related media and getting\ntimely updates about infrastructure construction projects as they may cause\nmassive impact to key conservation areas. Such monitoring, however, is\ndifficult and time-consuming. We introduce NewsPanda, a toolkit which\nautomatically detects and analyzes online articles related to environmental\nconservation and infrastructure construction. We fine-tune a BERT-based model\nusing active learning methods and noise correction algorithms to identify\narticles that are relevant to conservation and infrastructure construction. For\nthe identified articles, we perform further analysis, extracting keywords and\nfinding potentially related sources. NewsPanda has been successfully deployed\nby the World Wide Fund for Nature teams in the UK, India, and Nepal since\nFebruary 2022. It currently monitors over 80,000 websites and 1,074\nconservation sites across India and Nepal, saving more than 30 hours of human\nefforts weekly. We have now scaled it up to cover 60,000 conservation sites\nglobally.", "published": "2023-04-30 07:15:29", "link": "http://arxiv.org/abs/2305.01503v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Beyond Classification: Financial Reasoning in State-of-the-Art Language\n  Models", "abstract": "Large Language Models (LLMs), consisting of 100 billion or more parameters,\nhave demonstrated remarkable ability in complex multi-step reasoning tasks.\nHowever, the application of such generic advancements has been limited to a few\nfields, such as clinical or legal, with the field of financial reasoning\nremaining largely unexplored. To the best of our knowledge, the ability of LLMs\nto solve financial reasoning problems has never been dealt with, and whether it\ncan be performed at any scale remains unknown. To address this knowledge gap,\nthis research presents a comprehensive investigation into the potential\napplication of LLMs in the financial domain. The investigation includes a\ndetailed exploration of a range of subjects, including task formulation,\nsynthetic data generation, prompting methods, and evaluation capability.\nFurthermore, the study benchmarks various GPT variants with parameter scales\nranging from 2.8B to 13B, with and without instruction tuning, on diverse\ndataset sizes. By analyzing the results, we reveal that the ability to generate\ncoherent financial reasoning first emerges at 6B parameters, and continues to\nimprove with better instruction-tuning or larger datasets. Additionally, the\nstudy provides a publicly accessible dataset named sFIOG (Synthetic-Financial\nInvestment Opinion Generation), consisting of 11,802 synthetic investment\nthesis samples, to support further research in the field of financial\nreasoning. Overall, this research seeks to contribute to the understanding of\nthe efficacy of language models in the field of finance, with a particular\nemphasis on their ability to engage in sophisticated reasoning and analysis\nwithin the context of investment decision-making.", "published": "2023-04-30 04:36:05", "link": "http://arxiv.org/abs/2305.01505v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Sensitive Data Detection with High-Throughput Machine Learning Models in\n  Electrical Health Records", "abstract": "In the era of big data, there is an increasing need for healthcare providers,\ncommunities, and researchers to share data and collaborate to improve health\noutcomes, generate valuable insights, and advance research. The Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) is a federal law\ndesigned to protect sensitive health information by defining regulations for\nprotected health information (PHI). However, it does not provide efficient\ntools for detecting or removing PHI before data sharing. One of the challenges\nin this area of research is the heterogeneous nature of PHI fields in data\nacross different parties. This variability makes rule-based sensitive variable\nidentification systems that work on one database fail on another. To address\nthis issue, our paper explores the use of machine learning algorithms to\nidentify sensitive variables in structured data, thus facilitating the\nde-identification process. We made a key observation that the distributions of\nmetadata of PHI fields and non-PHI fields are very different. Based on this\nnovel finding, we engineered over 30 features from the metadata of the original\nfeatures and used machine learning to build classification models to\nautomatically identify PHI fields in structured Electronic Health Record (EHR)\ndata. We trained the model on a variety of large EHR databases from different\ndata sources and found that our algorithm achieves 99% accuracy when detecting\nPHI-related fields for unseen datasets. The implications of our study are\nsignificant and can benefit industries that handle sensitive data.", "published": "2023-04-30 16:14:23", "link": "http://arxiv.org/abs/2305.03169v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Working Memory Capacity of ChatGPT: An Empirical Study", "abstract": "Working memory is a critical aspect of both human intelligence and artificial\nintelligence, serving as a workspace for the temporary storage and manipulation\nof information. In this paper, we systematically assess the working memory\ncapacity of ChatGPT, a large language model developed by OpenAI, by examining\nits performance in verbal and spatial n-back tasks under various conditions.\nOur experiments reveal that ChatGPT has a working memory capacity limit\nstrikingly similar to that of humans. Furthermore, we investigate the impact of\ndifferent instruction strategies on ChatGPT's performance and observe that the\nfundamental patterns of a capacity limit persist. From our empirical findings,\nwe propose that n-back tasks may serve as tools for benchmarking the working\nmemory capacity of large language models and hold potential for informing\nfuture efforts aimed at enhancing AI working memory.", "published": "2023-04-30 11:54:40", "link": "http://arxiv.org/abs/2305.03731v4", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "A Review of Deep Learning Techniques for Speech Processing", "abstract": "The field of speech processing has undergone a transformative shift with the\nadvent of deep learning. The use of multiple processing layers has enabled the\ncreation of models capable of extracting intricate features from speech data.\nThis development has paved the way for unparalleled advancements in speech\nrecognition, text-to-speech synthesis, automatic speech recognition, and\nemotion recognition, propelling the performance of these tasks to unprecedented\nheights. The power of deep learning techniques has opened up new avenues for\nresearch and innovation in the field of speech processing, with far-reaching\nimplications for a range of industries and applications. This review paper\nprovides a comprehensive overview of the key deep learning models and their\napplications in speech-processing tasks. We begin by tracing the evolution of\nspeech processing research, from early approaches, such as MFCC and HMM, to\nmore recent advances in deep learning architectures, such as CNNs, RNNs,\ntransformers, conformers, and diffusion models. We categorize the approaches\nand compare their strengths and weaknesses for solving speech-processing tasks.\nFurthermore, we extensively cover various speech-processing tasks, datasets,\nand benchmarks used in the literature and describe how different deep-learning\nnetworks have been utilized to tackle these tasks. Additionally, we discuss the\nchallenges and future directions of deep learning in speech processing,\nincluding the need for more parameter-efficient, interpretable models and the\npotential of deep learning for multimodal speech processing. By examining the\nfield's evolution, comparing and contrasting different approaches, and\nhighlighting future directions and challenges, we hope to inspire further\nresearch in this exciting and rapidly advancing field.", "published": "2023-04-30 00:17:42", "link": "http://arxiv.org/abs/2305.00359v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Transformer-based Sequence Labeling for Audio Classification based on\n  MFCCs", "abstract": "Audio classification is vital in areas such as speech and music recognition.\nFeature extraction from the audio signal, such as Mel-Spectrograms and MFCCs,\nis a critical step in audio classification. These features are transformed into\nspectrograms for classification. Researchers have explored various techniques,\nincluding traditional machine and deep learning methods to classify\nspectrograms, but these can be computationally expensive. To simplify this\nprocess, a more straightforward approach inspired by sequence classification in\nNLP can be used. This paper proposes a Transformer-encoder-based model for\naudio classification using MFCCs. The model was benchmarked against the ESC-50,\nSpeech Commands v0.02 and UrbanSound8k datasets and has shown strong\nperformance, with the highest accuracy of 95.2% obtained upon training the\nmodel on the UrbanSound8k dataset. The model consisted of a mere 127,544 total\nparameters, making it light-weight yet highly efficient at the audio\nclassification task.", "published": "2023-04-30 07:25:43", "link": "http://arxiv.org/abs/2305.00417v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer of knowledge among instruments in automatic music transcription", "abstract": "Automatic music transcription (AMT) is one of the most challenging tasks in\nthe music information retrieval domain. It is the process of converting an\naudio recording of music into a symbolic representation containing information\nabout the notes, chords, and rhythm. Current research in this domain focuses on\ndeveloping new models based on transformer architecture or using methods to\nperform semi-supervised training, which gives outstanding results, but the\ncomputational cost of training such models is enormous.\n  This work shows how to employ easily generated synthesized audio data\nproduced by software synthesizers to train a universal model. It is a good base\nfor further transfer learning to quickly adapt transcription model for other\ninstruments. Achieved results prove that using synthesized data for training\nmay be a good base for pretraining general-purpose models, where the task of\ntranscription is not focused on one instrument.", "published": "2023-04-30 08:37:41", "link": "http://arxiv.org/abs/2305.00426v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning Based Multimodal with Two-phase Training Strategy for\n  Daily Life Video Classification", "abstract": "In this paper, we present a deep learning based multimodal system for\nclassifying daily life videos. To train the system, we propose a two-phase\ntraining strategy. In the first training phase (Phase I), we extract the audio\nand visual (image) data from the original video. We then train the audio data\nand the visual data with independent deep learning based models. After the\ntraining processes, we obtain audio embeddings and visual embeddings by\nextracting feature maps from the pre-trained deep learning models. In the\nsecond training phase (Phase II), we train a fusion layer to combine the\naudio/visual embeddings and a dense layer to classify the combined embedding\ninto target daily scenes. Our extensive experiments, which were conducted on\nthe benchmark dataset of DCASE (IEEE AASP Challenge on Detection and\nClassification of Acoustic Scenes and Events) 2021 Task 1B Development,\nachieved the best classification accuracy of 80.5%, 91.8%, and 95.3% with only\naudio data, with only visual data, both audio and visual data, respectively.\nThe highest classification accuracy of 95.3% presents an improvement of 17.9%\ncompared with DCASE baseline and shows very competitive to the state-of-the-art\nsystems.", "published": "2023-04-30 19:12:34", "link": "http://arxiv.org/abs/2305.01476v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
