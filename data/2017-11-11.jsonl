{"title": "Towards Automated ICD Coding Using Deep Learning", "abstract": "International Classification of Diseases(ICD) is an authoritative health care\nclassification system of different diseases and conditions for clinical and\nmanagement purposes. Considering the complicated and dedicated process to\nassign correct codes to each patient admission based on overall diagnosis, we\npropose a hierarchical deep learning model with attention mechanism which can\nautomatically assign ICD diagnostic codes given written diagnosis. We utilize\ncharacter-aware neural language models to generate hidden representations of\nwritten diagnosis descriptions and ICD codes, and design an attention mechanism\nto address the mismatch between the numbers of descriptions and corresponding\ncodes. Our experimental results show the strong potential of automated ICD\ncoding from diagnosis descriptions. Our best model achieves 0.53 and 0.90 of F1\nscore and area under curve of receiver operating characteristic respectively.\nThe result outperforms those achieved using character-unaware encoding method\nor without attention mechanism. It indicates that our proposed deep learning\nmodel can code automatically in a reasonable way and provide a framework for\ncomputer-auxiliary ICD coding.", "published": "2017-11-11 04:34:51", "link": "http://arxiv.org/abs/1711.04075v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering conversational topics and emotions associated with\n  Demonetization tweets in India", "abstract": "Social media platforms contain great wealth of information which provides us\nopportunities explore hidden patterns or unknown correlations, and understand\npeople's satisfaction with what they are discussing. As one showcase, in this\npaper, we summarize the data set of Twitter messages related to recent\ndemonetization of all Rs. 500 and Rs. 1000 notes in India and explore insights\nfrom Twitter's data. Our proposed system automatically extracts the popular\nlatent topics in conversations regarding demonetization discussed in Twitter\nvia the Latent Dirichlet Allocation (LDA) based topic model and also identifies\nthe correlated topics across different categories. Additionally, it also\ndiscovers people's opinions expressed through their tweets related to the event\nunder consideration via the emotion analyzer. The system also employs an\nintuitive and informative visualization to show the uncovered insight.\nFurthermore, we use an evaluation measure, Normalized Mutual Information (NMI),\nto select the best LDA models. The obtained LDA results show that the tool can\nbe effectively used to extract discussion topics and summarize them for further\nmanual analysis.", "published": "2017-11-11 10:55:38", "link": "http://arxiv.org/abs/1711.04115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable probabilistic embeddings: bridging the gap between topic\n  models and neural networks", "abstract": "We consider probabilistic topic models and more recent word embedding\ntechniques from a perspective of learning hidden semantic representations.\nInspired by a striking similarity of the two approaches, we merge them and\nlearn probabilistic embeddings with online EM-algorithm on word co-occurrence\ndata. The resulting embeddings perform on par with Skip-Gram Negative Sampling\n(SGNS) on word similarity tasks and benefit in the interpretability of the\ncomponents. Next, we learn probabilistic document embeddings that outperform\nparagraph2vec on a document similarity task and require less memory and time\nfor training. Finally, we employ multimodal Additive Regularization of Topic\nModels (ARTM) to obtain a high sparsity and learn embeddings for other\nmodalities, such as timestamps and categories. We observe further improvement\nof word similarity performance and meaningful inter-modality similarities.", "published": "2017-11-11 15:35:22", "link": "http://arxiv.org/abs/1711.04154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings", "abstract": "We introduce KBGAN, an adversarial learning framework to improve the\nperformances of a wide range of existing knowledge graph embedding models.\nBecause knowledge graphs typically only contain positive facts, sampling useful\nnegative training examples is a non-trivial task. Replacing the head or tail\nentity of a fact with a uniformly randomly selected entity is a conventional\nmethod for generating negative facts, but the majority of the generated\nnegative facts can be easily discriminated from positive facts, and will\ncontribute little towards the training. Inspired by generative adversarial\nnetworks (GANs), we use one knowledge graph embedding model as a negative\nsample generator to assist the training of our desired model, which acts as the\ndiscriminator in GANs. This framework is independent of the concrete form of\ngenerator and discriminator, and therefore can utilize a wide variety of\nknowledge graph embedding models as its building blocks. In experiments, we\nadversarially train two translation-based models, TransE and TransD, each with\nassistance from one of the two probability-based models, DistMult and ComplEx.\nWe evaluate the performances of KBGAN on the link prediction task, using three\nknowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental\nresults show that adversarial training substantially improves the performances\nof target embedding models under various settings.", "published": "2017-11-11 03:46:53", "link": "http://arxiv.org/abs/1711.04071v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine Grained Knowledge Transfer for Personalized Task-oriented Dialogue\n  Systems", "abstract": "Training a personalized dialogue system requires a lot of data, and the data\ncollected for a single user is usually insufficient. One common practice for\nthis problem is to share training dialogues between different users and train\nmultiple sequence-to-sequence dialogue models together with transfer learning.\nHowever, current sequence-to-sequence transfer learning models operate on the\nentire sentence, which might cause negative transfer if different personal\ninformation from different users is mixed up. We propose a personalized decoder\nmodel to transfer finer granularity phrase-level knowledge between different\nusers while keeping personal preferences of each user intact. A novel personal\ncontrol gate is introduced, enabling the personalized decoder to switch between\ngenerating personalized phrases and shared phrases. The proposed personalized\ndecoder model can be easily combined with various deep models and can be\ntrained with reinforcement learning. Real-world experimental results\ndemonstrate that the phrase-level personalized decoder improves the BLEU over\nmultiple sentence-level transfer baseline models by as much as 7.5%.", "published": "2017-11-11 05:14:02", "link": "http://arxiv.org/abs/1711.04079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MojiTalk: Generating Emotional Responses at Scale", "abstract": "Generating emotional language is a key step towards building empathetic\nnatural language processing agents. However, a major challenge for this line of\nresearch is the lack of large-scale labeled training data, and previous studies\nare limited to only small sets of human annotated sentiment labels.\nAdditionally, explicitly controlling the emotion and sentiment of generated\ntext is also difficult. In this paper, we take a more radical approach: we\nexploit the idea of leveraging Twitter data that are naturally labeled with\nemojis. More specifically, we collect a large corpus of Twitter conversations\nthat include emojis in the response, and assume the emojis convey the\nunderlying emotions of the sentence. We then introduce a reinforced conditional\nvariational encoder approach to train a deep generative model on these\nconversations, which allows us to use emojis to control the emotion of the\ngenerated text. Experimentally, we show in our quantitative and qualitative\nanalyses that the proposed models can successfully generate high-quality\nabstractive conversation responses in accordance with designated emotions.", "published": "2017-11-11 07:20:51", "link": "http://arxiv.org/abs/1711.04090v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Commonsense LocatedNear Knowledge", "abstract": "LocatedNear relation is a kind of commonsense knowledge describing two\nphysical objects that are typically found near each other in real life. In this\npaper, we study how to automatically extract such relationship through a\nsentence-level relation classifier and aggregating the scores of entity pairs\nfrom a large corpus. Also, we release two benchmark datasets for evaluation and\nfuture research.", "published": "2017-11-11 22:25:55", "link": "http://arxiv.org/abs/1711.04204v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Document Embedding With CNNs", "abstract": "We propose a new model for unsupervised document embedding. Leading existing\napproaches either require complex inference or use recurrent neural networks\n(RNN) that are difficult to parallelize. We take a different route and develop\na convolutional neural network (CNN) embedding model. Our CNN architecture is\nfully parallelizable resulting in over 10x speedup in inference time over RNN\nmodels. Parallelizable architecture enables to train deeper models where each\nsuccessive layer has increasingly larger receptive field and models longer\nrange semantic structure within the document. We additionally propose a fully\nunsupervised learning algorithm to train this model based on stochastic forward\nprediction. Empirical results on two public benchmarks show that our approach\nproduces comparable to state-of-the-art accuracy at a fraction of computational\ncost.", "published": "2017-11-11 16:43:38", "link": "http://arxiv.org/abs/1711.04168v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Phrase-based Image Captioning with Hierarchical LSTM Model", "abstract": "Automatic generation of caption to describe the content of an image has been\ngaining a lot of research interests recently, where most of the existing works\ntreat the image caption as pure sequential data. Natural language, however\npossess a temporal hierarchy structure, with complex dependencies between each\nsubsequence. In this paper, we propose a phrase-based hierarchical Long\nShort-Term Memory (phi-LSTM) model to generate image description. In contrast\nto the conventional solutions that generate caption in a pure sequential\nmanner, our proposed model decodes image caption from phrase to sentence. It\nconsists of a phrase decoder at the bottom hierarchy to decode noun phrases of\nvariable length, and an abbreviated sentence decoder at the upper hierarchy to\ndecode an abbreviated form of the image description. A complete image caption\nis formed by combining the generated phrases with sentence during the inference\nstage. Empirically, our proposed model shows a better or competitive result on\nthe Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the\nart models. We also show that our proposed model is able to generate more novel\ncaptions (not seen in the training data) which are richer in word contents in\nall these three datasets.", "published": "2017-11-11 10:48:59", "link": "http://arxiv.org/abs/1711.05557v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Weakly Supervised Audio Source Separation via Spectrum Energy Preserved\n  Wasserstein Learning", "abstract": "Separating audio mixtures into individual instrument tracks has been a long\nstanding challenging task. We introduce a novel weakly supervised audio source\nseparation approach based on deep adversarial learning. Specifically, our loss\nfunction adopts the Wasserstein distance which directly measures the\ndistribution distance between the separated sources and the real sources for\neach individual source. Moreover, a global regularization term is added to\nfulfill the spectrum energy preservation property regardless separation. Unlike\nstate-of-the-art weakly supervised models which often involve deliberately\ndevised constraints or careful model selection, our approach need little prior\nmodel specification on the data, and can be straightforwardly learned in an\nend-to-end fashion. We show that the proposed method performs competitively on\npublic benchmark against state-of-the-art weakly supervised methods.", "published": "2017-11-11 11:47:33", "link": "http://arxiv.org/abs/1711.04121v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
