{"title": "One-step and Two-step Classification for Abusive Language Detection on\n  Twitter", "abstract": "Automatic abusive language detection is a difficult but important task for\nonline social media. Our research explores a two-step approach of performing\nclassification on abusive language and then classifying into specific types and\ncompares it with one-step approach of doing one multi-class classification for\ndetecting sexist and racist languages. With a public English Twitter corpus of\n20 thousand tweets in the type of sexism and racism, our approach shows a\npromising performance of 0.827 F-measure by using HybridCNN in one-step and\n0.824 F-measure by using logistic regression in two-steps.", "published": "2017-06-05 06:20:23", "link": "http://arxiv.org/abs/1706.01206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Generation with Recurrent Generative Adversarial Networks\n  without Pre-training", "abstract": "Generative Adversarial Networks (GANs) have shown great promise recently in\nimage generation. Training GANs for language generation has proven to be more\ndifficult, because of the non-differentiable nature of generating text with\nrecurrent neural networks. Consequently, past work has either resorted to\npre-training with maximum-likelihood or used convolutional networks for\ngeneration. In this work, we show that recurrent neural networks can be trained\nto generate text with GANs from scratch using curriculum learning, by slowly\nteaching the model to generate sequences of increasing and variable length. We\nempirically show that our approach vastly improves the quality of generated\nsequences compared to a convolutional baseline.", "published": "2017-06-05 16:10:58", "link": "http://arxiv.org/abs/1706.01399v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A simple neural network module for relational reasoning", "abstract": "Relational reasoning is a central component of generally intelligent\nbehavior, but has proven difficult for neural networks to learn. In this paper\nwe describe how to use Relation Networks (RNs) as a simple plug-and-play module\nto solve problems that fundamentally hinge on relational reasoning. We tested\nRN-augmented networks on three tasks: visual question answering using a\nchallenging dataset called CLEVR, on which we achieve state-of-the-art,\nsuper-human performance; text-based question answering using the bAbI suite of\ntasks; and complex reasoning about dynamic physical systems. Then, using a\ncurated dataset called Sort-of-CLEVR we show that powerful convolutional\nnetworks do not have a general capacity to solve relational questions, but can\ngain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to\nreason about entities and their relations.", "published": "2017-06-05 17:17:18", "link": "http://arxiv.org/abs/1706.01427v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep learning evaluation using deep linguistic processing", "abstract": "We discuss problems with the standard approaches to evaluation for tasks like\nvisual question answering, and argue that artificial data can be used to\naddress these as a complement to current practice. We demonstrate that with the\nhelp of existing 'deep' linguistic processing technology we are able to create\nchallenging abstract datasets, which enable us to investigate the language\nunderstanding abilities of multimodal deep learning models in detail, as\ncompared to a single performance value on a static and monolithic dataset.", "published": "2017-06-05 13:53:56", "link": "http://arxiv.org/abs/1706.01322v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Event Representations for Automated Story Generation with Deep Neural\n  Nets", "abstract": "Automated story generation is the problem of automatically selecting a\nsequence of events, actions, or words that can be told as a story. We seek to\ndevelop a system that can generate stories by learning everything it needs to\nknow from textual story corpora. To date, recurrent neural networks that learn\nlanguage models at character, word, or sentence levels have had little success\ngenerating coherent stories. We explore the question of event representations\nthat provide a mid-level of abstraction between words and sentences in order to\nretain the semantic information of the original data while minimizing event\nsparsity. We present a technique for preprocessing textual story data into\nevent sequences. We then present a technique for automated story generation\nwhereby we decompose the problem into the generation of successive events\n(event2event) and the generation of natural language sentences from events\n(event2sentence). We give empirical results comparing different event\nrepresentations and their effects on event successor generation and the\ntranslation of events to natural language.", "published": "2017-06-05 14:04:48", "link": "http://arxiv.org/abs/1706.01331v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Joint Model for Question Answering and Question Generation", "abstract": "We propose a generative machine comprehension model that learns jointly to\nask and answer questions based on documents. The proposed model uses a\nsequence-to-sequence framework that encodes the document and generates a\nquestion (answer) given an answer (question). Significant improvement in model\nperformance is observed empirically on the SQuAD corpus, confirming our\nhypothesis that the model benefits from jointly learning to perform both tasks.\nWe believe the joint model's novelty offers a new perspective on machine\ncomprehension beyond architectural engineering, and serves as a first step\ntowards autonomous information seeking.", "published": "2017-06-05 17:58:52", "link": "http://arxiv.org/abs/1706.01450v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning\n  to a Generative Visual Dialog Model", "abstract": "We present a novel training framework for neural sequence models,\nparticularly for grounded dialog generation. The standard training paradigm for\nthese models is maximum likelihood estimation (MLE), or minimizing the\ncross-entropy of the human responses. Across a variety of domains, a recurring\nproblem with MLE trained generative neural dialog models (G) is that they tend\nto produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In\ncontrast, discriminative dialog models (D) that are trained to rank a list of\ncandidate human responses outperform their generative counterparts; in terms of\nautomatic metrics, diversity, and informativeness of the responses. However, D\nis not useful in practice since it cannot be deployed to have real\nconversations with users.\n  Our work aims to achieve the best of both worlds -- the practical usefulness\nof G and the strong performance of D -- via knowledge transfer from D to G. Our\nprimary contribution is an end-to-end trainable generative visual dialog model,\nwhere G receives gradients from D as a perceptual (not adversarial) loss of the\nsequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)\napproximation to the discrete distribution -- specifically, an RNN augmented\nwith a sequence of GS samplers, coupled with the straight-through gradient\nestimator to enable end-to-end differentiability. We also introduce a stronger\nencoder for visual dialog, and employ a self-attention mechanism for answer\nencoding along with a metric learning loss to aid D in better capturing\nsemantic similarities in answer responses. Overall, our proposed model\noutperforms state-of-the-art on the VisDial dataset by a significant margin\n(2.67% on recall@10). The source code can be downloaded from\nhttps://github.com/jiasenlu/visDial.pytorch.", "published": "2017-06-05 22:50:37", "link": "http://arxiv.org/abs/1706.01554v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep learning for extracting protein-protein interactions from\n  biomedical literature", "abstract": "State-of-the-art methods for protein-protein interaction (PPI) extraction are\nprimarily feature-based or kernel-based by leveraging lexical and syntactic\ninformation. But how to incorporate such knowledge in the recent deep learning\nmethods remains an open question. In this paper, we propose a multichannel\ndependency-based convolutional neural network model (McDepCNN). It applies one\nchannel to the embedding vector of each word in the sentence, and another\nchannel to the embedding vector of the head of the corresponding word.\nTherefore, the model can use richer information obtained from different\nchannels. Experiments on two public benchmarking datasets, AIMed and BioInfer,\ndemonstrate that McDepCNN compares favorably to the state-of-the-art\nrich-feature and single-kernel based methods. In addition, McDepCNN achieves\n24.4% relative improvement in F1-score over the state-of-the-art methods on\ncross-corpus evaluation and 12% improvement in F1-score over kernel-based\nmethods on \"difficult\" instances. These results suggest that McDepCNN\ngeneralizes more easily over different corpora, and is capable of capturing\nlong distance features in the sentences.", "published": "2017-06-05 23:09:06", "link": "http://arxiv.org/abs/1706.01556v2", "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
