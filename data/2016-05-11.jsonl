{"title": "Machine Comprehension Based on Learning to Rank", "abstract": "Machine comprehension plays an essential role in NLP and has been widely\nexplored with dataset like MCTest. However, this dataset is too simple and too\nsmall for learning true reasoning abilities. \\cite{hermann2015teaching}\ntherefore release a large scale news article dataset and propose a deep LSTM\nreader system for machine comprehension. However, the training process is\nexpensive. We therefore try feature-engineered approach with semantics on the\nnew dataset to see how traditional machine learning technique and semantics can\nhelp with machine comprehension. Meanwhile, our proposed L2R reader system\nachieves good performance with efficiency and less training data.", "published": "2016-05-11 05:05:05", "link": "http://arxiv.org/abs/1605.03284v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics\n  Model for Generation and Generalisation of Noun-Verb Combinations with\n  Sensorimotor Inputs", "abstract": "The paper presents a neurorobotics cognitive model to explain the\nunderstanding and generalisation of nouns and verbs combinations when a vocal\ncommand consisting of a verb-noun sentence is provided to a humanoid robot.\nThis generalisation process is done via the grounding process: different\nobjects are being interacted, and associated, with different motor behaviours,\nfollowing a learning approach inspired by developmental language acquisition in\ninfants. This cognitive model is based on Multiple Time-scale Recurrent Neural\nNetworks (MTRNN).With the data obtained from object manipulation tasks with a\nhumanoid robot platform, the robotic agent implemented with this model can\nground the primitive embodied structure of verbs through training with\nverb-noun combination samples. Moreover, we show that a functional hierarchical\narchitecture, based on MTRNN, is able to generalise and produce novel\ncombinations of noun-verb sentences. Further analyses of the learned network\ndynamics and representations also demonstrate how the generalisation is\npossible via the exploitation of this functional hierarchical recurrent\nnetwork.", "published": "2016-05-11 02:31:21", "link": "http://arxiv.org/abs/1605.03261v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "abstract": "Text from social media provides a set of challenges that can cause\ntraditional NLP approaches to fail. Informal language, spelling errors,\nabbreviations, and special characters are all commonplace in these posts,\nleading to a prohibitively large vocabulary size for word-level approaches. We\npropose a character composition model, tweet2vec, which finds vector-space\nrepresentations of whole tweets by learning complex, non-local dependencies in\ncharacter sequences. The proposed model outperforms a word-level baseline at\npredicting user-annotated hashtags associated with the posts, doing\nsignificantly better when the input contains many out-of-vocabulary words or\nunusual character sequences. Our tweet2vec encoder is publicly available.", "published": "2016-05-11 15:30:09", "link": "http://arxiv.org/abs/1605.03481v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
