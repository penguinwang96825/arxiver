{"title": "Societal Biases in Language Generation: Progress and Challenges", "abstract": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.", "published": "2021-05-10 00:17:33", "link": "http://arxiv.org/abs/2105.04054v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REPT: Bridging Language Models and Machine Reading Comprehension via\n  Retrieval-Based Pre-training", "abstract": "Pre-trained Language Models (PLMs) have achieved great success on Machine\nReading Comprehension (MRC) over the past few years. Although the general\nlanguage representation learned from large-scale corpora does benefit MRC, the\npoor support in evidence extraction which requires reasoning across multiple\nsentences hinders PLMs from further advancing MRC. To bridge the gap between\ngeneral PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach.\nIn particular, we introduce two self-supervised tasks to strengthen evidence\nextraction during pre-training, which is further inherited by downstream MRC\ntasks through the consistent retrieval operation and model architecture. To\nevaluate our proposed method, we conduct extensive experiments on five MRC\ndatasets that require collecting evidence from and reasoning across multiple\nsentences. Experimental results demonstrate the effectiveness of our\npre-training approach. Moreover, further analysis shows that our approach is\nable to enhance the capacity of evidence extraction without explicit\nsupervision.", "published": "2021-05-10 08:54:46", "link": "http://arxiv.org/abs/2105.04201v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Similarities between Arabic Dialects: Investigating Geographical\n  Proximity", "abstract": "The automatic classification of Arabic dialects is an ongoing research\nchallenge, which has been explored in recent work that defines dialects based\non increasingly limited geographic areas like cities and provinces. This paper\nfocuses on a related yet relatively unexplored topic: the effects of the\ngeographical proximity of cities located in Arab countries on their dialectical\nsimilarity. Our work is twofold, reliant on: 1) comparing the textual\nsimilarities between dialects using cosine similarity and 2) measuring the\ngeographical distance between locations. We study MADAR and NADI, two\nestablished datasets with Arabic dialects from many cities and provinces. Our\nresults indicate that cities located in different countries may in fact have\nmore dialectical similarity than cities within the same country, depending on\ntheir geographical proximity. The correlation between dialectical similarity\nand city proximity suggests that cities that are closer together are more\nlikely to share dialectical attributes, regardless of country borders. This\nnuance provides the potential for important advancements in Arabic dialect\nresearch because it indicates that a more granular approach to dialect\nclassification is essential to understanding how to frame the problem of Arabic\ndialects identification.", "published": "2021-05-10 09:32:21", "link": "http://arxiv.org/abs/2105.04221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State\n  Tracking", "abstract": "Zero-shot cross-domain dialogue state tracking (DST) enables us to handle\ntask-oriented dialogue in unseen domains without the expense of collecting\nin-domain data. In this paper, we propose a slot description enhanced\ngenerative approach for zero-shot cross-domain DST. Specifically, our model\nfirst encodes dialogue context and slots with a pre-trained self-attentive\nencoder, and generates slot values in an auto-regressive manner. In addition,\nwe incorporate Slot Type Informed Descriptions that capture the shared\ninformation across slots to facilitate cross-domain knowledge transfer.\nExperimental results on the MultiWOZ dataset show that our proposed method\nsignificantly improves existing state-of-the-art results in the zero-shot\ncross-domain setting.", "published": "2021-05-10 09:34:01", "link": "http://arxiv.org/abs/2105.04222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocOIE: A Document-level Context-Aware Dataset for OpenIE", "abstract": "Open Information Extraction (OpenIE) aims to extract structured relational\ntuples (subject, relation, object) from sentences and plays critical roles for\nmany downstream NLP applications. Existing solutions perform extraction at\nsentence level, without referring to any additional contextual information. In\nreality, however, a sentence typically exists as part of a document rather than\nstandalone; we often need to access relevant contextual information around the\nsentence before we can accurately interpret it. As there is no document-level\ncontext-aware OpenIE dataset available, we manually annotate 800 sentences from\n80 documents in two domains (Healthcare and Transportation) to form a DocOIE\ndataset for evaluation. In addition, we propose DocIE, a novel document-level\ncontext-aware OpenIE model. Our experimental results based on DocIE demonstrate\nthat incorporating document-level context is helpful in improving OpenIE\nperformance. Both DocOIE dataset and DocIE model are released for public.", "published": "2021-05-10 11:14:30", "link": "http://arxiv.org/abs/2105.04271v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DefSent: Sentence Embeddings using Definition Sentences", "abstract": "Sentence embedding methods using natural language inference (NLI) datasets\nhave been successfully applied to various tasks. However, these methods are\nonly available for limited languages due to relying heavily on the large NLI\ndatasets. In this paper, we propose DefSent, a sentence embedding method that\nuses definition sentences from a word dictionary, which performs comparably on\nunsupervised semantics textual similarity (STS) tasks and slightly better on\nSentEval tasks than conventional methods. Since dictionaries are available for\nmany languages, DefSent is more broadly applicable than methods using NLI\ndatasets without constructing additional datasets. We demonstrate that DefSent\nperforms comparably on unsupervised semantics textual similarity (STS) tasks\nand slightly better on SentEval tasks to the methods using large NLI datasets.\nOur code is publicly available at https://github.com/hpprc/defsent .", "published": "2021-05-10 13:13:39", "link": "http://arxiv.org/abs/2105.04339v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Poolingformer: Long Document Modeling with Pooling Attention", "abstract": "In this paper, we introduce a two-level attention schema, Poolingformer, for\nlong document modeling. Its first level uses a smaller sliding window pattern\nto aggregate information from neighbors. Its second level employs a larger\nwindow to increase receptive fields with pooling attention to reduce both\ncomputational cost and memory consumption. We first evaluate Poolingformer on\ntwo long sequence QA tasks: the monolingual NQ and the multilingual TyDi QA.\nExperimental results show that Poolingformer sits atop three official\nleaderboards measured by F1, outperforming previous state-of-the-art models by\n1.9 points (79.8 vs. 77.9) on NQ long answer, 1.9 points (79.5 vs. 77.6) on\nTyDi QA passage answer, and 1.6 points (67.6 vs. 66.0) on TyDi QA minimal\nanswer. We further evaluate Poolingformer on a long sequence summarization\ntask. Experimental results on the arXiv benchmark continue to demonstrate its\nsuperior performance.", "published": "2021-05-10 13:53:08", "link": "http://arxiv.org/abs/2105.04371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Quality Estimation with Multiple Hypotheses for Grammatical Error\n  Correction", "abstract": "Grammatical Error Correction (GEC) aims to correct writing errors and help\nlanguage learners improve their writing skills. However, existing GEC models\ntend to produce spurious corrections or fail to detect lots of errors. The\nquality estimation model is necessary to ensure learners get accurate GEC\nresults and avoid misleading from poorly corrected sentences. Well-trained GEC\nmodels can generate several high-quality hypotheses through decoding, such as\nbeam search, which provide valuable GEC evidence and can be used to evaluate\nGEC quality. However, existing models neglect the possible GEC evidence from\ndifferent hypotheses. This paper presents the Neural Verification Network\n(VERNet) for GEC quality estimation with multiple hypotheses. VERNet\nestablishes interactions among hypotheses with a reasoning graph and conducts\ntwo kinds of attention mechanisms to propagate GEC evidence to verify the\nquality of generated hypotheses. Our experiments on four GEC datasets show that\nVERNet achieves state-of-the-art grammatical error detection performance,\nachieves the best quality estimation results, and significantly improves GEC\nperformance by reranking hypotheses. All data and source codes are available at\nhttps://github.com/thunlp/VERNet.", "published": "2021-05-10 15:04:25", "link": "http://arxiv.org/abs/2105.04443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC\n  at IWSLT 2021", "abstract": "This paper describes the submission to the IWSLT 2021 offline speech\ntranslation task by the UPC Machine Translation group. The task consists of\nbuilding a system capable of translating English audio recordings extracted\nfrom TED talks into German text. Submitted systems can be either cascade or\nend-to-end and use a custom or given segmentation. Our submission is an\nend-to-end speech translation system, which combines pre-trained models\n(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,\nand uses an efficient fine-tuning technique, which trains only 20% of its total\nparameters. We show that adding an Adapter to the system and pre-training it,\ncan increase the convergence speed and the final result, with which we achieve\na BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble\nthat obtains 28.22 BLEU score on the same set. Our submission also uses a\ncustom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for\nidentifying periods of untranscribable text and can bring improvements of 2.5\nto 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the\ngiven segmentation.", "published": "2021-05-10 17:04:11", "link": "http://arxiv.org/abs/2105.04512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Classification of Human Translation and Machine Translation: A\n  Study from the Perspective of Lexical Diversity", "abstract": "By using a trigram model and fine-tuning a pretrained BERT model for sequence\nclassification, we show that machine translation and human translation can be\nclassified with an accuracy above chance level, which suggests that machine\ntranslation and human translation are different in a systematic way. The\nclassification accuracy of machine translation is much higher than of human\ntranslation. We show that this may be explained by the difference in lexical\ndiversity between machine translation and human translation. If machine\ntranslation has independent patterns from human translation, automatic metrics\nwhich measure the deviation of machine translation from human translation may\nconflate difference with quality. Our experiment with two different types of\nautomatic metrics shows correlation with the result of the classification task.\nTherefore, we suggest the difference in lexical diversity between machine\ntranslation and human translation be given more attention in machine\ntranslation evaluation.", "published": "2021-05-10 18:55:04", "link": "http://arxiv.org/abs/2105.04616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Economic Policy Uncertainty Using an Unsupervised Word\n  Embedding-based Method", "abstract": "Economic Policy Uncertainty (EPU) is a critical indicator in economic\nstudies, while it can be used to forecast a recession. Under higher levels of\nuncertainty, firms' owners cut their investment, which leads to a longer\npost-recession recovery. EPU index is computed by counting news articles\ncontaining pre-defined keywords related to policy-making and economy and convey\nuncertainty. Unfortunately, this method is sensitive to the original keyword\nset, its richness, and the news coverage. Thus, reproducing its results for\ndifferent countries is challenging. In this paper, we propose an unsupervised\ntext mining method that uses word-embedding representation space to select\nrelevant keywords. This method is not strictly sensitive to the semantic\nsimilarity threshold applied to the word embedding vectors and does not require\na pre-defined dictionary. Our experiments using a massive repository of Persian\nnews show that the EPU series computed by the proposed method precisely follows\nmajor events affecting Iran's economy and is compatible with the World\nUncertainty Index (WUI) of Iran.", "published": "2021-05-10 19:34:14", "link": "http://arxiv.org/abs/2105.04631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Acquisition is Embodied, Interactive, Emotive: a Research\n  Proposal", "abstract": "Humans' experience of the world is profoundly multimodal from the beginning,\nso why do existing state-of-the-art language models only use text as a modality\nto learn and represent semantic meaning? In this paper we review the literature\non the role of embodiment and emotion in the interactive setting of spoken\ndialogue as necessary prerequisites for language learning for human children,\nincluding how words in child vocabularies are largely concrete, then shift to\nbecome more abstract as the children get older. We sketch a model of semantics\nthat leverages current transformer-based models and a word-level grounded\nmodel, then explain the robot-dialogue system that will make use of our\nsemantic model, the setting for the system to learn language, and existing\nbenchmarks for evaluation.", "published": "2021-05-10 19:40:17", "link": "http://arxiv.org/abs/2105.04633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R2D2: Relational Text Decoding with Transformers", "abstract": "We propose a novel framework for modeling the interaction between graphical\nstructures and the natural language text associated with their nodes and edges.\nExisting approaches typically fall into two categories. On group ignores the\nrelational structure by converting them into linear sequences and then utilize\nthe highly successful Seq2Seq models. The other side ignores the sequential\nnature of the text by representing them as fixed-dimensional vectors and apply\ngraph neural networks. Both simplifications lead to information loss.\n  Our proposed method utilizes both the graphical structure as well as the\nsequential nature of the texts. The input to our model is a set of text\nsegments associated with the nodes and edges of the graph, which are then\nprocessed with a transformer encoder-decoder model, equipped with a\nself-attention mechanism that is aware of the graphical relations between the\nnodes containing the segments. This also allows us to use BERT-like models that\nare already trained on large amounts of text.\n  While the proposed model has wide applications, we demonstrate its\ncapabilities on data-to-text generation tasks. Our approach compares favorably\nagainst state-of-the-art methods in four tasks without tailoring the model\narchitecture. We also provide an early demonstration in a novel practical\napplication -- generating clinical notes from the medical entities mentioned\nduring clinical visits.", "published": "2021-05-10 19:59:11", "link": "http://arxiv.org/abs/2105.04645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual\n  Language Models", "abstract": "Multilingual Transformer-based language models, usually pretrained on more\nthan 100 languages, have been shown to achieve outstanding results in a wide\nrange of cross-lingual transfer tasks. However, it remains unknown whether the\noptimization for different languages conditions the capacity of the models to\ngeneralize over syntactic structures, and how languages with syntactic\nphenomena of different complexity are affected. In this work, we explore the\nsyntactic generalization capabilities of the monolingual and multilingual\nversions of BERT and RoBERTa. More specifically, we evaluate the syntactic\ngeneralization potential of the models on English and Spanish tests, comparing\nthe syntactic abilities of monolingual and multilingual models on the same\nlanguage (English), and of multilingual models on two different languages\n(English and Spanish). For English, we use the available SyntaxGym test suite;\nfor Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic\ntests in Spanish, designed to evaluate the syntactic generalization\ncapabilities of language models through the SyntaxGym online platform.", "published": "2021-05-10 22:06:53", "link": "http://arxiv.org/abs/2105.04688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SRLF: A Stance-aware Reinforcement Learning Framework for Content-based\n  Rumor Detection on Social Media", "abstract": "The rapid development of social media changes the lifestyle of people and\nsimultaneously provides an ideal place for publishing and disseminating rumors,\nwhich severely exacerbates social panic and triggers a crisis of social trust.\nEarly content-based methods focused on finding clues from the text and user\nprofiles for rumor detection. Recent studies combine the stances of users'\ncomments with news content to capture the difference between true and false\nrumors. Although the user's stance is effective for rumor detection, the manual\nlabeling process is time-consuming and labor-intensive, which limits the\napplication of utilizing it to facilitate rumor detection.\n  In this paper, we first finetune a pre-trained BERT model on a small labeled\ndataset and leverage this model to annotate weak stance labels for users'\ncomment data to overcome the problem mentioned above. Then, we propose a novel\nStance-aware Reinforcement Learning Framework (SRLF) to select high-quality\nlabeled stance data for model training and rumor detection. Both the stance\nselection and rumor detection tasks are optimized simultaneously to promote\nboth tasks mutually. We conduct experiments on two commonly used real-world\ndatasets. The experimental results demonstrate that our framework outperforms\nthe state-of-the-art models significantly, which confirms the effectiveness of\nthe proposed framework.", "published": "2021-05-10 03:58:34", "link": "http://arxiv.org/abs/2105.04098v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExpMRC: Explainability Evaluation for Machine Reading Comprehension", "abstract": "Achieving human-level performance on some of Machine Reading Comprehension\n(MRC) datasets is no longer challenging with the help of powerful Pre-trained\nLanguage Models (PLMs). However, it is necessary to provide both answer\nprediction and its explanation to further improve the MRC system's reliability,\nespecially for real-life applications. In this paper, we propose a new\nbenchmark called ExpMRC for evaluating the explainability of the MRC systems.\nExpMRC contains four subsets, including SQuAD, CMRC 2018, RACE$^+$, and C$^3$\nwith additional annotations of the answer's evidence. The MRC systems are\nrequired to give not only the correct answer but also its explanation. We use\nstate-of-the-art pre-trained language models to build baseline systems and\nadopt various unsupervised approaches to extract evidence without a\nhuman-annotated training set. The experimental results show that these models\nare still far from human performance, suggesting that the ExpMRC is\nchallenging. Resources will be available through\nhttps://github.com/ymcui/expmrc", "published": "2021-05-10 06:00:20", "link": "http://arxiv.org/abs/2105.04126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReadTwice: Reading Very Large Documents with Memories", "abstract": "Knowledge-intensive tasks such as question answering often require\nassimilating information from different sections of large inputs such as books\nor article collections. We propose ReadTwice, a simple and effective technique\nthat combines several strengths of prior approaches to model long-range\ndependencies with Transformers. The main idea is to read text in small\nsegments, in parallel, summarizing each segment into a memory table to be used\nin a second read of the text. We show that the method outperforms models of\ncomparable size on several question answering (QA) datasets and sets a new\nstate of the art on the challenging NarrativeQA task, with questions about\nentire books. Source code and pre-trained checkpoints for ReadTwice can be\nfound at https://goo.gle/research-readtwice.", "published": "2021-05-10 10:13:09", "link": "http://arxiv.org/abs/2105.04241v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Guided Curriculum Learning for Neural Machine Translation", "abstract": "In the field of machine learning, the well-trained model is assumed to be\nable to recover the training labels, i.e. the synthetic labels predicted by the\nmodel should be as close to the ground-truth labels as possible. Inspired by\nthis, we propose a self-guided curriculum strategy to encourage the learning of\nneural machine translation (NMT) models to follow the above recovery criterion,\nwhere we cast the recovery degree of each training example as its learning\ndifficulty. Specifically, we adopt the sentence level BLEU score as the proxy\nof recovery degree. Different from existing curricula relying on linguistic\nprior knowledge or third-party language models, our chosen learning difficulty\nis more suitable to measure the degree of knowledge mastery of the NMT models.\nExperiments on translation benchmarks, including WMT14\nEnglish$\\Rightarrow$German and WMT17 Chinese$\\Rightarrow$English, demonstrate\nthat our approach can consistently improve translation performance against\nstrong baseline Transformer.", "published": "2021-05-10 16:12:14", "link": "http://arxiv.org/abs/2105.04475v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Factual Consistency of Abstractive Summarization via Question\n  Answering", "abstract": "A commonly observed problem with the state-of-the art abstractive\nsummarization models is that the generated summaries can be factually\ninconsistent with the input documents. The fact that automatic summarization\nmay produce plausible-sounding yet inaccurate summaries is a major concern that\nlimits its wide application. In this paper we present an approach to address\nfactual consistency in summarization. We first propose an efficient automatic\nevaluation metric to measure factual consistency; next, we propose a novel\nlearning algorithm that maximizes the proposed metric during model training.\nThrough extensive experiments, we confirm that our method is effective in\nimproving factual consistency and even overall quality of the summaries, as\njudged by both automatic metrics and human evaluation.", "published": "2021-05-10 19:07:21", "link": "http://arxiv.org/abs/2105.04623v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GroupLink: An End-to-end Multitask Method for Word Grouping and Relation\n  Extraction in Form Understanding", "abstract": "Forms are a common type of document in real life and carry rich information\nthrough textual contents and the organizational structure. To realize automatic\nprocessing of forms, word grouping and relation extraction are two fundamental\nand crucial steps after preliminary processing of optical character reader\n(OCR). Word grouping is to aggregate words that belong to the same semantic\nentity, and relation extraction is to predict the links between semantic\nentities. Existing works treat them as two individual tasks, but these two\ntasks are correlated and can reinforce each other. The grouping process will\nrefine the integrated representation of the corresponding entity, and the\nlinking process will give feedback to the grouping performance. For this\npurpose, we acquire multimodal features from both textual data and layout\ninformation and build an end-to-end model through multitask training to combine\nword grouping and relation extraction to enhance performance on each task. We\nvalidate our proposed method on a real-world, fully-annotated, noisy-scanned\nbenchmark, FUNSD, and extensive experiments demonstrate the effectiveness of\nour method.", "published": "2021-05-10 20:15:06", "link": "http://arxiv.org/abs/2105.04650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wiki-Reliability: A Large Scale Dataset for Content Reliability on\n  Wikipedia", "abstract": "Wikipedia is the largest online encyclopedia, used by algorithms and web\nusers as a central hub of reliable information on the web. The quality and\nreliability of Wikipedia content is maintained by a community of volunteer\neditors. Machine learning and information retrieval algorithms could help scale\nup editors' manual efforts around Wikipedia content reliability. However, there\nis a lack of large-scale data to support the development of such research. To\nfill this gap, in this paper, we propose Wiki-Reliability, the first dataset of\nEnglish Wikipedia articles annotated with a wide set of content reliability\nissues. To build this dataset, we rely on Wikipedia \"templates\". Templates are\ntags used by expert Wikipedia editors to indicate content issues, such as the\npresence of \"non-neutral point of view\" or \"contradictory articles\", and serve\nas a strong signal for detecting reliability issues in a revision. We select\nthe 10 most popular reliability-related templates on Wikipedia, and propose an\neffective method to label almost 1M samples of Wikipedia article revisions as\npositive or negative with respect to each template. Each positive/negative\nexample in the dataset comes with the full article text and 20 features from\nthe revision's metadata. We provide an overview of the possible downstream\ntasks enabled by such data, and show that Wiki-Reliability can be used to train\nlarge-scale models for content reliability prediction. We release all data and\ncode for public use.", "published": "2021-05-10 05:07:03", "link": "http://arxiv.org/abs/2105.04117v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language\n  and Symbolic Reasoning", "abstract": "Geometry problem solving has attracted much attention in the NLP community\nrecently. The task is challenging as it requires abstract problem understanding\nand symbolic reasoning with axiomatic knowledge. However, current datasets are\neither small in scale or not publicly available. Thus, we construct a new\nlarge-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with\ndense annotation in formal language. We further propose a novel geometry\nsolving approach with formal language and symbolic reasoning, called\nInterpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the\nproblem text and diagram into formal language automatically via rule-based text\nparsing and neural object detecting, respectively. Unlike implicit learning in\nexisting methods, Inter-GPS incorporates theorem knowledge as conditional rules\nand performs symbolic reasoning step by step. Also, a theorem predictor is\ndesigned to infer the theorem application sequence fed to the symbolic solver\nfor the more efficient and reasonable searching path. Extensive experiments on\nthe Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves\nsignificant improvements over existing methods. The project with code and data\nis available at https://lupantech.github.io/inter-gps.", "published": "2021-05-10 07:46:55", "link": "http://arxiv.org/abs/2105.04165v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Deep Learning Based Dialogue Systems: A Systematic\n  Survey", "abstract": "Dialogue systems are a popular natural language processing (NLP) task as it\nis promising in real-life applications. It is also a complicated task since\nmany NLP tasks deserving study are involved. As a result, a multitude of novel\nworks on this task are carried out, and most of them are deep learning based\ndue to the outstanding performance. In this survey, we mainly focus on the deep\nlearning based dialogue systems. We comprehensively review state-of-the-art\nresearch outcomes in dialogue systems and analyze them from two angles: model\ntype and system type. Specifically, from the angle of model type, we discuss\nthe principles, characteristics, and applications of different models that are\nwidely used in dialogue systems. This will help researchers acquaint these\nmodels and see how they are applied in state-of-the-art frameworks, which is\nrather helpful when designing a new dialogue system. From the angle of system\ntype, we discuss task-oriented and open-domain dialogue systems as two streams\nof research, providing insight into the hot topics related. Furthermore, we\ncomprehensively review the evaluation methods and datasets for dialogue systems\nto pave the way for future research. Finally, some possible research trends are\nidentified based on the recent research outcomes. To the best of our knowledge,\nthis survey is the most comprehensive and up-to-date one at present for deep\nlearning based dialogue systems, extensively covering the popular techniques.\nWe speculate that this work is a good starting point for academics who are new\nto the dialogue systems or those who want to quickly grasp up-to-date\ntechniques in this area.", "published": "2021-05-10 14:07:49", "link": "http://arxiv.org/abs/2105.04387v5", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning Robust Latent Representations for Controllable Speech Synthesis", "abstract": "State-of-the-art Variational Auto-Encoders (VAEs) for learning disentangled\nlatent representations give impressive results in discovering features like\npitch, pause duration, and accent in speech data, leading to highly\ncontrollable text-to-speech (TTS) synthesis. However, these LSTM-based VAEs\nfail to learn latent clusters of speaker attributes when trained on either\nlimited or noisy datasets. Further, different latent variables start encoding\nthe same features, limiting the control and expressiveness during speech\nsynthesis. To resolve these issues, we propose RTI-VAE (Reordered Transformer\nwith Information reduction VAE) where we minimize the mutual information\nbetween different latent variables and devise a modified Transformer\narchitecture with layer reordering to learn controllable latent representations\nin speech data. We show that RTI-VAE reduces the cluster overlap of speaker\nattributes by at least 30\\% over LSTM-VAE and by at least 7\\% over vanilla\nTransformer-VAE.", "published": "2021-05-10 15:49:03", "link": "http://arxiv.org/abs/2105.04458v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What shall we do with an hour of data? Speech recognition for the un-\n  and under-served languages of Common Voice", "abstract": "This technical report describes the methods and results of a three-week\nsprint to produce deployable speech recognition models for 31 under-served\nlanguages of the Common Voice project. We outline the preprocessing steps,\nhyperparameter selection, and resulting accuracy on official testing sets. In\naddition to this we evaluate the models on multiple tasks: closed-vocabulary\nspeech recognition, pre-transcription, forced alignment, and key-word spotting.\nThe following experiments use Coqui STT, a toolkit for training and deployment\nof neural Speech-to-Text models.", "published": "2021-05-10 21:16:28", "link": "http://arxiv.org/abs/2105.04674v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Word-level Human Interpretable Scoring Mechanism for Novel Text\n  Detection Using Tsetlin Machines", "abstract": "Recent research in novelty detection focuses mainly on document-level\nclassification, employing deep neural networks (DNN). However, the black-box\nnature of DNNs makes it difficult to extract an exact explanation of why a\ndocument is considered novel. In addition, dealing with novelty at the\nword-level is crucial to provide a more fine-grained analysis than what is\navailable at the document level. In this work, we propose a Tsetlin machine\n(TM)-based architecture for scoring individual words according to their\ncontribution to novelty. Our approach encodes a description of the novel\ndocuments using the linguistic patterns captured by TM clauses. We then adopt\nthis description to measure how much a word contributes to making documents\nnovel. Our experimental results demonstrate how our approach breaks down\nnovelty into interpretable phrases, successfully measuring novelty.", "published": "2021-05-10 23:41:14", "link": "http://arxiv.org/abs/2105.04708v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.5; I.7"], "primary_category": "cs.CL"}
{"title": "Speech2Slot: An End-to-End Knowledge-based Slot Filling from Speech", "abstract": "In contrast to conventional pipeline Spoken Language Understanding (SLU)\nwhich consists of automatic speech recognition (ASR) and natural language\nunderstanding (NLU), end-to-end SLU infers the semantic meaning directly from\nspeech and overcomes the error propagation caused by ASR. End-to-end slot\nfilling (SF) from speech is an essential component of end-to-end SLU, and is\nusually regarded as a sequence-to-sequence generation problem, heavily relied\non the performance of language model of ASR. However, it is hard to generate a\ncorrect slot when the slot is out-of-vovabulary (OOV) in training data,\nespecially when a slot is an anti-linguistic entity without grammatical rule.\nInspired by object detection in computer vision that is to detect the object\nfrom an image, we consider SF as the task of slot detection from speech. In\nthis paper, we formulate the SF task as a matching task and propose an\nend-to-end knowledge-based SF model, named Speech-to-Slot (Speech2Slot), to\nleverage knowledge to detect the boundary of a slot from the speech. We also\nrelease a large-scale dataset of Chinese speech for slot filling, containing\nmore than 830,000 samples. The experiments show that our approach is markedly\nsuperior to the conventional pipeline SLU approach, and outperforms the\nstate-of-the-art end-to-end SF approach with 12.51% accuracy improvement.", "published": "2021-05-10 13:31:27", "link": "http://arxiv.org/abs/2105.04719v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Spoken Moments: Learning Joint Audio-Visual Representations from Video\n  Descriptions", "abstract": "When people observe events, they are able to abstract key information and\nbuild concise summaries of what is happening. These summaries include\ncontextual and semantic information describing the important high-level details\n(what, where, who and how) of the observed event and exclude background\ninformation that is deemed unimportant to the observer. With this in mind, the\ndescriptions people generate for videos of different dynamic events can greatly\nimprove our understanding of the key information of interest in each video.\nThese descriptions can be captured in captions that provide expanded attributes\nfor video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing\nus to gain new insight into what people find important or necessary to\nsummarize specific events. Existing caption datasets for video understanding\nare either small in scale or restricted to a specific domain. To address this,\nwe present the Spoken Moments (S-MiT) dataset of 500k spoken captions each\nattributed to a unique short video depicting a broad range of different events.\nWe collect our descriptions using audio recordings to ensure that they remain\nas natural and concise as possible while allowing us to scale the size of a\nlarge classification dataset. In order to utilize our proposed dataset, we\npresent a novel Adaptive Mean Margin (AMM) approach to contrastive learning and\nevaluate our models on video/caption retrieval on multiple datasets. We show\nthat our AMM approach consistently improves our results and that models trained\non our Spoken Moments dataset generalize better than those trained on other\nvideo-caption datasets.", "published": "2021-05-10 16:30:46", "link": "http://arxiv.org/abs/2105.04489v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Voice activity detection in the wild: A data-driven approach using\n  teacher-student training", "abstract": "Voice activity detection is an essential pre-processing component for\nspeech-related tasks such as automatic speech recognition (ASR). Traditional\nsupervised VAD systems obtain frame-level labels from an ASR pipeline by using,\ne.g., a Hidden Markov model. These ASR models are commonly trained on clean and\nfully transcribed data, limiting VAD systems to be trained on clean or\nsynthetically noised datasets. Therefore, a major challenge for supervised VAD\nsystems is their generalization towards noisy, real-world data. This work\nproposes a data-driven teacher-student approach for VAD, which utilizes vast\nand unconstrained audio data for training. Unlike previous approaches, only\nweak labels during teacher training are required, enabling the utilization of\nany real-world, potentially noisy dataset. Our approach firstly trains a\nteacher model on a source dataset (Audioset) using clip-level supervision.\nAfter training, the teacher provides frame-level guidance to a student model on\nan unlabeled, target dataset. A multitude of student models trained on mid- to\nlarge-sized datasets are investigated (Audioset, Voxceleb, NIST SRE). Our\napproach is then respectively evaluated on clean, artificially noised, and\nreal-world data. We observe significant performance gains in artificially\nnoised and real-world scenarios. Lastly, we compare our approach against other\nunsupervised and supervised VAD methods, demonstrating our method's\nsuperiority.", "published": "2021-05-10 01:26:03", "link": "http://arxiv.org/abs/2105.04065v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MASS: Multi-task Anthropomorphic Speech Synthesis Framework", "abstract": "Text-to-Speech (TTS) synthesis plays an important role in human-computer\ninteraction. Currently, most TTS technologies focus on the naturalness of\nspeech, namely,making the speeches sound like humans. However, the key tasks of\nthe expression of emotion and the speaker identity are ignored, which limits\nthe application scenarios of TTS synthesis technology. To make the synthesized\nspeech more realistic and expand the application scenarios, we propose a\nmulti-task anthropomorphic speech synthesis framework (MASS), which can\nsynthesize speeches from text with specified emotion and speaker identity. The\nMASS framework consists of a base TTS module and two novel voice conversion\nmodules: the emotional voice conversion module and the speaker voice conversion\nmodule. We propose deep emotion voice conversion model (DEVC) and deep speaker\nvoice conversion model (DSVC) based on convolution residual networks. It solves\nthe problem of feature loss during voice conversion. The model trainings are\nindependent of parallel datasets, and are capable of many-to-many voice\nconversion. In the emotional voice conversion, speaker voice conversion\nexperiments, as well as the multi-task speech synthesis experiments,\nexperimental results show DEVC and DSVC convert speech effectively. The\nquantitative and qualitative evaluation results of multi-task speech synthesis\nexperiments show MASS can effectively synthesis speech with specified text,\nemotion and speaker identity.", "published": "2021-05-10 05:54:08", "link": "http://arxiv.org/abs/2105.04124v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Study on the temporal pooling used in deep neural networks for speaker\n  verification", "abstract": "The x-vector architecture has recently achieved state-of-the-art results on\nthe speaker verification task. This architecture incorporates a central layer,\nreferred to as temporal pooling, which stacks statistical parameters of the\nacoustic frame distribution. This work proposes to highlight the significant\neffect of the temporal pooling content on the training dynamics and task\nperformance. An evaluation with different pooling layers is conducted, that is,\nincluding different statistical measures of central tendency. Notably, 3rd and\n4th moment-based statistics (skewness and kurtosis) are also tested to complete\nthe usual mean and standard-deviation parameters. Our experiments show the\ninfluence of the pooling layer content in terms of speaker verification\nperformance, but also for several classification tasks (speaker, channel or\ntext related), and allow to better reveal the presence of external information\nto the speaker identity depending on the layer content.", "published": "2021-05-10 12:44:57", "link": "http://arxiv.org/abs/2105.04310v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Corpora Language Recognition: A Preliminary Investigation with\n  Indian Languages", "abstract": "In this paper, we conduct one of the very first studies for cross-corpora\nperformance evaluation in the spoken language identification (LID) problem.\nCross-corpora evaluation was not explored much in LID research, especially for\nthe Indian languages. We have selected three Indian spoken language corpora:\nIIITH-ILSC, LDC South Asian, and IITKGP-MLILSC. For each of the corpus, LID\nsystems are trained on the state-of-the-art time-delay neural network (TDNN)\nbased architecture with MFCC features. We observe that the LID performance\ndegrades drastically for cross-corpora evaluation. For example, the system\ntrained on the IIITH-ILSC corpus shows an average EER of 11.80 % and 43.34 %\nwhen evaluated with the same corpora and LDC South Asian corpora, respectively.\nOur preliminary analysis shows the significant differences among these corpora\nin terms of mismatch in the long-term average spectrum (LTAS) and\nsignal-to-noise ratio (SNR). Subsequently, we apply different feature level\ncompensation methods to reduce the cross-corpora acoustic mismatch. Our results\nindicate that these feature normalization schemes can help to achieve promising\nLID performance on cross-corpora experiments.", "published": "2021-05-10 19:50:17", "link": "http://arxiv.org/abs/2105.04639v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sampling-Frequency-Independent Audio Source Separation Using Convolution\n  Layer Based on Impulse Invariant Method", "abstract": "Audio source separation is often used as preprocessing of various\napplications, and one of its ultimate goals is to construct a single versatile\nmodel capable of dealing with the varieties of audio signals. Since sampling\nfrequency, one of the audio signal varieties, is usually application specific,\nthe preceding audio source separation model should be able to deal with audio\nsignals of all sampling frequencies specified in the target applications.\nHowever, conventional models based on deep neural networks (DNNs) are trained\nonly at the sampling frequency specified by the training data, and there are no\nguarantees that they work with unseen sampling frequencies. In this paper, we\npropose a convolution layer capable of handling arbitrary sampling frequencies\nby a single DNN. Through music source separation experiments, we show that the\nintroduction of the proposed layer enables a conventional audio source\nseparation model to consistently work with even unseen sampling frequencies.", "published": "2021-05-10 02:33:42", "link": "http://arxiv.org/abs/2105.04079v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer with\n  One Transformer VAE", "abstract": "Transformers and variational autoencoders (VAE) have been extensively\nemployed for symbolic (e.g., MIDI) domain music generation. While the former\nboast an impressive capability in modeling long sequences, the latter allow\nusers to willingly exert control over different parts (e.g., bars) of the music\nto be generated. In this paper, we are interested in bringing the two together\nto construct a single model that exhibits both strengths. The task is split\ninto two steps. First, we equip Transformer decoders with the ability to accept\nsegment-level, time-varying conditions during sequence generation.\nSubsequently, we combine the developed and tested in-attention decoder with a\nTransformer encoder, and train the resulting MuseMorphose model with the VAE\nobjective to achieve style transfer of long pop piano pieces, in which users\ncan specify musical attributes including rhythmic intensity and polyphony\n(i.e., harmonic fullness) they desire, down to the bar level. Experiments show\nthat MuseMorphose outperforms recurrent neural network (RNN) based baselines on\nnumerous widely-used metrics for style transfer tasks.", "published": "2021-05-10 03:44:03", "link": "http://arxiv.org/abs/2105.04090v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-modal Conditional Bounding Box Regression for Music Score\n  Following", "abstract": "This paper addresses the problem of sheet-image-based on-line audio-to-score\nalignment also known as score following. Drawing inspiration from object\ndetection, a conditional neural network architecture is proposed that directly\npredicts x,y coordinates of the matching positions in a complete score sheet\nimage at each point in time for a given musical performance. Experiments are\nconducted on a synthetic polyphonic piano benchmark dataset and the new method\nis compared to several existing approaches from the literature for\nsheet-image-based score following as well as an Optical Music Recognition\nbaseline. The proposed approach achieves new state-of-the-art results and\nfurthermore significantly improves the alignment performance on a set of\nreal-world piano recordings by applying Impulse Responses as a data\naugmentation technique.", "published": "2021-05-10 12:43:35", "link": "http://arxiv.org/abs/2105.04309v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Deep Reinforcement Learning Approach to Audio-Based Navigation in a\n  Multi-Speaker Environment", "abstract": "In this work we use deep reinforcement learning to create an autonomous agent\nthat can navigate in a two-dimensional space using only raw auditory sensory\ninformation from the environment, a problem that has received very little\nattention in the reinforcement learning literature. Our experiments show that\nthe agent can successfully identify a particular target speaker among a set of\n$N$ predefined speakers in a room and move itself towards that speaker, while\navoiding collision with other speakers or going outside the room boundaries.\nThe agent is shown to be robust to speaker pitch shifting and it can learn to\nnavigate the environment, even when a limited number of training utterances are\navailable for each speaker.", "published": "2021-05-10 16:26:47", "link": "http://arxiv.org/abs/2105.04488v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Popular Music Generation Using Imitation and Structure", "abstract": "Many practices have been presented in music generation recently. While\nstylistic music generation using deep learning techniques has became the main\nstream, these models still struggle to generate music with high musicality,\ndifferent levels of music structure, and controllability. In addition, more\napplication scenarios such as music therapy require imitating more specific\nmusical styles from a few given music examples, rather than capturing the\noverall genre style of a large data corpus. To address requirements that\nchallenge current deep learning methods, we propose a statistical machine\nlearning model that is able to capture and imitate the structure, melody,\nchord, and bass style from a given example seed song. An evaluation using 10\npop songs shows that our new representations and methods are able to create\nhigh-quality stylistic music that is similar to a given input song. We also\ndiscuss potential uses of our approach in music evaluation and music therapy.", "published": "2021-05-10 23:43:00", "link": "http://arxiv.org/abs/2105.04709v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
