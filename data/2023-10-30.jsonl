{"title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form\n  Responses", "abstract": "A model is considered well-calibrated when its probability estimate aligns\nwith the actual likelihood of the output being correct. Calibrating language\nmodels (LMs) is crucial, as it plays a vital role in detecting and mitigating\nhallucinations of LMs as well as building more trustworthy models. However,\nstandard calibration techniques may not be suited for LM calibration. For\ninstance, post-processing methods such as temperature scaling do not reorder\nthe candidate generations. On the other hand, training-based methods require\nfine-tuning the entire model, which is impractical for LMs of large scale. We\npresent LitCab, a lightweight calibration mechanism consisting of a single\nlinear layer that takes the input text representation and predicts a bias term,\nwhich is then added to the LM output logits. LitCab improves model calibration\nby only adding < 2% of the original model parameters. For evaluation, we\nconstruct CaT, a benchmark consisting of eight text generation tasks, covering\nresponses ranging from short phrases to paragraphs. We test LitCab with\nLlama2-7B, where it improves calibration across all tasks, reducing the average\nECE score by as large as 30%. We further conduct a comprehensive evaluation\nwith multiple popular open-sourced LMs from GPT and LLaMA families, yielding\nthe following key findings: (i) Larger models within the same family exhibit\nbetter calibration on tasks with short generation tasks, but not necessarily\nfor longer ones. (ii) GPT-family models show superior calibration compared to\nLLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii)\nFine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose\n(e.g., conversations) may lead to worse calibration, highlighting the\nimportance of fine-tuning setups for calibrating LMs.", "published": "2023-10-30 00:30:34", "link": "http://arxiv.org/abs/2310.19208v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapter Pruning using Tropical Characterization", "abstract": "Adapters are widely popular parameter-efficient transfer learning approaches\nin natural language processing that insert trainable modules in between layers\nof a pre-trained language model. Apart from several heuristics, however, there\nhas been a lack of studies analyzing the optimal number of adapter parameters\nneeded for downstream applications. In this paper, we propose an adapter\npruning approach by studying the tropical characteristics of trainable modules.\nWe cast it as an optimization problem that aims to prune parameters from the\nadapter layers without changing the orientation of underlying tropical\nhypersurfaces. Our experiments on five NLP datasets show that tropical geometry\ntends to identify more relevant parameters to prune when compared with the\nmagnitude-based baseline, while a combined approach works best across the\ntasks.", "published": "2023-10-30 02:20:44", "link": "http://arxiv.org/abs/2310.19232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Real-World Meeting Summarization Systems using Large Language\n  Models: A Practical Perspective", "abstract": "This paper studies how to effectively build meeting summarization systems for\nreal-world usage using large language models (LLMs). For this purpose, we\nconduct an extensive evaluation and comparison of various closed-source and\nopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings\nreveal that most closed-source LLMs are generally better in terms of\nperformance. However, much smaller open-source models like LLaMA- 2 (7B and\n13B) could still achieve performance comparable to the large closed-source\nmodels even in zero-shot scenarios. Considering the privacy concerns of\nclosed-source models for only being accessible via API, alongside the high cost\nassociated with using fine-tuned versions of the closed-source models, the\nopensource models that can achieve competitive performance are more\nadvantageous for industrial use. Balancing performance with associated costs\nand privacy concerns, the LLaMA-2-7B model looks more promising for industrial\nusage. In sum, this paper offers practical insights on using LLMs for\nreal-world business meeting summarization, shedding light on the trade-offs\nbetween performance and cost.", "published": "2023-10-30 02:25:21", "link": "http://arxiv.org/abs/2310.19233v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context\n  Evaluation Benchmark for Large Language Models", "abstract": "Managing long sequences has become an important and necessary feature for\nlarge language models (LLMs). However, it is still an open question of how to\ncomprehensively and systematically evaluate the long-sequence capability of\nLLMs. One of the reasons is that conventional and widely-used benchmarks mainly\nconsist of short sequences. In this paper, we propose M4LE, a Multi-ability,\nMulti-range, Multi-task, Multi-domain benchmark for Long-context Evaluation.\nM4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task\ntypes and 12 domains. To alleviate the scarcity of tasks with naturally long\nsequences and incorporate multiple-ability assessment, we propose an automatic\napproach (but with negligible human annotations) to convert short-sequence\ntasks into a unified long-sequence scenario where LLMs have to identify single\nor multiple relevant spans in long contexts based on explicit or semantic\nhints. Specifically, the scenario includes five different types of abilities:\n(1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span;\n(4) semantic multiple-span; and (5) global context understanding. The resulting\nsamples in M4LE are evenly distributed from 1k to 8k input length. We conducted\na systematic evaluation on 11 well-established LLMs, especially those optimized\nfor long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to\nunderstand long context, particularly when tasks require multiple-span\nattention. 2) Semantic retrieval task is more difficult for competent LLMs. 3)\nModels fine-tuned on longer text with position interpolation have comparable\nperformance to those using Neural Tangent Kernel (NTK) aware scaling methods\nwithout fine-tuning. We make our benchmark publicly available to encourage\nfuture research in this challenging area.", "published": "2023-10-30 03:11:30", "link": "http://arxiv.org/abs/2310.19240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through\n  Claim Detection and Identification of Claim Spans", "abstract": "A significant increase in content creation and information exchange has been\nmade possible by the quick development of online social media platforms, which\nhas been very advantageous. However, these platforms have also become a haven\nfor those who disseminate false information, propaganda, and fake news. Claims\nare essential in forming our perceptions of the world, but sadly, they are\nfrequently used to trick people by those who spread false information. To\naddress this problem, social media giants employ content moderators to filter\nout fake news from the actual world. However, the sheer volume of information\nmakes it difficult to identify fake news effectively. Therefore, it has become\ncrucial to automatically identify social media posts that make such claims,\ncheck their veracity, and differentiate between credible and false claims. In\nresponse, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval\nEvaluation (FIRE'2023). The primary objectives centered on two crucial tasks:\nTask A, determining whether a social media post constitutes a claim, and Task\nB, precisely identifying the words or phrases within the post that form the\nclaim. Task A received 40 registrations, demonstrating a strong interest and\nengagement in this timely challenge. Meanwhile, Task B attracted participation\nfrom 28 teams, highlighting its significance in the digital era of\nmisinformation.", "published": "2023-10-30 04:57:41", "link": "http://arxiv.org/abs/2310.19267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to love diligent trolls: Accounting for rater effects in the\n  dialogue safety task", "abstract": "Chatbots have the risk of generating offensive utterances, which must be\navoided. Post-deployment, one way for a chatbot to continuously improve is to\nsource utterance/label pairs from feedback by live users. However, among users\nare trolls, who provide training examples with incorrect labels. To de-troll\ntraining data, previous work removed training examples that have high\nuser-aggregated cross-validation (CV) error. However, CV is expensive; and in a\ncoordinated attack, CV may be overwhelmed by trolls in number and in\nconsistency among themselves. In the present work, I address both limitations\nby proposing a solution inspired by methodology in automated essay scoring\n(AES): have multiple users rate each utterance, then perform latent class\nanalysis (LCA) to infer correct labels. As it does not require GPU\ncomputations, LCA is inexpensive. In experiments, I found that the AES-like\nsolution can infer training labels with high accuracy when trolls are\nconsistent, even when trolls are the majority.", "published": "2023-10-30 05:08:23", "link": "http://arxiv.org/abs/2310.19271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fusing Temporal Graphs into Transformers for Time-Sensitive Question\n  Answering", "abstract": "Answering time-sensitive questions from long documents requires temporal\nreasoning over the times in questions and documents. An important open question\nis whether large language models can perform such reasoning solely using a\nprovided text document, or whether they can benefit from additional temporal\ninformation extracted using other systems. We address this research question by\napplying existing temporal information extraction systems to construct temporal\ngraphs of events, times, and temporal relations in questions and documents. We\nthen investigate different approaches for fusing these graphs into Transformer\nmodels. Experimental results show that our proposed approach for fusing\ntemporal graphs into input text substantially enhances the temporal reasoning\ncapabilities of Transformer models with or without fine-tuning. Additionally,\nour proposed method outperforms various graph convolution-based approaches and\nestablishes a new state-of-the-art performance on SituatedQA and three splits\nof TimeQA.", "published": "2023-10-30 06:12:50", "link": "http://arxiv.org/abs/2310.19292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and\n  INES", "abstract": "As part of the WMT-2023 \"Test suites\" shared task, in this paper we summarize\nthe results of two test suites evaluations: MuST-SHE-WMT23 and INES. By\nfocusing on the en-de and de-en language pairs, we rely on these newly created\ntest suites to investigate systems' ability to translate feminine and masculine\ngender and produce gender-inclusive translations. Furthermore we discuss\nmetrics associated with our test suites and validate them by means of human\nevaluations. Our results indicate that systems achieve reasonable and\ncomparable performance in correctly translating both feminine and masculine\ngender forms for naturalistic gender phenomena. Instead, the generation of\ninclusive language forms in translation emerges as a challenging task for all\nthe evaluated MT models, indicating room for future improvements and research\non the topic.", "published": "2023-10-30 08:36:30", "link": "http://arxiv.org/abs/2310.19345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Japanese SimCSE Technical Report", "abstract": "We report the development of Japanese SimCSE, Japanese sentence embedding\nmodels fine-tuned with SimCSE. Since there is a lack of sentence embedding\nmodels for Japanese that can be used as a baseline in sentence embedding\nresearch, we conducted extensive experiments on Japanese sentence embeddings\ninvolving 24 pre-trained Japanese or multilingual language models, five\nsupervised datasets, and four unsupervised datasets. In this report, we provide\nthe detailed training setup for Japanese SimCSE and their evaluation results.", "published": "2023-10-30 08:43:26", "link": "http://arxiv.org/abs/2310.19349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Lightweight Method to Generate Unanswerable Questions in English", "abstract": "If a question cannot be answered with the available information, robust\nsystems for question answering (QA) should know _not_ to answer. One way to\nbuild QA models that do this is with additional training data comprised of\nunanswerable questions, created either by employing annotators or through\nautomated methods for unanswerable question generation. To show that the model\ncomplexity of existing automated approaches is not justified, we examine a\nsimpler data augmentation method for unanswerable question generation in\nEnglish: performing antonym and entity swaps on answerable questions. Compared\nto the prior state-of-the-art, data generated with our training-free and\nlightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data\nwith BERT-large), and has higher human-judged relatedness and readability. We\nquantify the raw benefits of our approach compared to no augmentation across\nmultiple encoder models, using different amounts of generated data, and also on\nTydiQA-MinSpan data (+9.3 F1 points with BERT-large). Our results establish\nswaps as a simple but strong baseline for future work.", "published": "2023-10-30 10:14:52", "link": "http://arxiv.org/abs/2310.19403v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Constituency Parsing using LLMs", "abstract": "Constituency parsing is a fundamental yet unsolved natural language\nprocessing task. In this paper, we explore the potential of recent large\nlanguage models (LLMs) that have exhibited remarkable performance across\nvarious domains and tasks to tackle this task. We employ three linearization\nstrategies to transform output trees into symbol sequences, such that LLMs can\nsolve constituency parsing by generating linearized trees. We conduct\nexperiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT,\nLLaMA, and Alpaca, comparing their performance against the state-of-the-art\nconstituency parsers. Our experiments encompass zero-shot, few-shot, and\nfull-training learning settings, and we evaluate the models on one in-domain\nand five out-of-domain test datasets. Our findings reveal insights into LLMs'\nperformance, generalization abilities, and challenges in constituency parsing.", "published": "2023-10-30 11:39:11", "link": "http://arxiv.org/abs/2310.19462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties\n  in Generative Language Models", "abstract": "Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.", "published": "2023-10-30 13:33:21", "link": "http://arxiv.org/abs/2310.19531v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Representation to Improve Team Problem Solving in Real-Time", "abstract": "This paper proposes a novel representation to support computing metrics that\nhelp understanding and improving in real-time a team's behavior during problem\nsolving in real-life. Even though teams are important in modern activities,\nthere is little computing aid to improve their activity. The representation\ncaptures the different mental images developed, enhanced, and utilized during\nsolving. A case study illustrates the representation.", "published": "2023-10-30 13:46:24", "link": "http://arxiv.org/abs/2310.19539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Input-label Mapping with Demonstration Replay for In-context\n  Learning", "abstract": "In-context learning (ICL) is an emerging capability of large autoregressive\nlanguage models where a few input-label demonstrations are appended to the\ninput to enhance the model's understanding of downstream NLP tasks, without\ndirectly adjusting the model parameters. The effectiveness of ICL can be\nattributed to the strong language modeling capabilities of large language\nmodels (LLMs), which enable them to learn the mapping between input and labels\nbased on in-context demonstrations. Despite achieving promising results, the\ncausal nature of language modeling in ICL restricts the attention to be\nbackward only, i.e., a token only attends to its previous tokens, failing to\ncapture the full input-label information and limiting the model's performance.\nIn this paper, we propose a novel ICL method called Repeated Demonstration with\nSliding Causal Attention, (RdSca). Specifically, we duplicate later\ndemonstrations and concatenate them to the front, allowing the model to\n`observe' the later information even under the causal restriction. Besides, we\nintroduce sliding causal attention, which customizes causal attention to avoid\ninformation leakage. Experimental results show that our method significantly\nimproves the input-label mapping in ICL demonstrations. We also conduct an\nin-depth analysis of how to customize the causal attention without training,\nwhich has been an unexplored area in previous research.", "published": "2023-10-30 14:29:41", "link": "http://arxiv.org/abs/2310.19572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KeyGen2Vec: Learning Document Embedding via Multi-label Keyword\n  Generation in Question-Answering", "abstract": "Representing documents into high dimensional embedding space while preserving\nthe structural similarity between document sources has been an ultimate goal\nfor many works on text representation learning. Current embedding models,\nhowever, mainly rely on the availability of label supervision to increase the\nexpressiveness of the resulting embeddings. In contrast, unsupervised\nembeddings are cheap, but they often cannot capture implicit structure in\ntarget corpus, particularly for samples that come from different distribution\nwith the pretraining source.\n  Our study aims to loosen up the dependency on label supervision by learning\ndocument embeddings via Sequence-to-Sequence (Seq2Seq) text generator.\nSpecifically, we reformulate keyphrase generation task into multi-label keyword\ngeneration in community-based Question Answering (cQA). Our empirical results\nshow that KeyGen2Vec in general is superior than multi-label keyword classifier\nby up to 14.7% based on Purity, Normalized Mutual Information (NMI), and\nF1-Score metrics. Interestingly, although in general the absolute advantage of\nlearning embeddings through label supervision is highly positive across\nevaluation datasets, KeyGen2Vec is shown to be competitive with classifier that\nexploits topic label supervision in Yahoo! cQA with larger number of latent\ntopic labels.", "published": "2023-10-30 15:35:45", "link": "http://arxiv.org/abs/2310.19650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamics of Instruction Fine-Tuning for Chinese Large Language Models", "abstract": "Instruction tuning is a burgeoning method to elicit the general intelligence\nof Large Language Models (LLMs). While numerous studies have examined the\nimpact of factors such as data volume and model size on English models, the\nscaling properties of instruction tuning in other languages remain largely\nunexplored. In this work, we systematically investigate the effects of data\nquantity, model size, and data construction methods on instruction tuning for\nChinese LLMs. We utilize a newly curated dataset, DoIT, which includes over\n40,000 high-quality instruction instances covering ten underlying abilities,\nsuch as creative writing, code generation, and logical reasoning. Our\nexperiments, conducted on models ranging from 7b to 33b parameters, yield three\nkey findings: (i) While these factors directly affect overall model\nperformance, some abilities are more responsive to scaling, whereas others\ndemonstrate significant resistance. (ii) The scaling sensitivity of different\nabilities to these factors can be explained by two features: Complexity and\nTransference. (iii) By tailoring training strategies to their varying\nsensitivities, specific abilities can be efficiently learned, enhancing\nperformance on two public benchmarks.", "published": "2023-10-30 15:37:10", "link": "http://arxiv.org/abs/2310.19651v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable-by-Design Text Understanding with Iteratively Generated\n  Concept Bottleneck", "abstract": "Black-box deep neural networks excel in text classification, yet their\napplication in high-stakes domains is hindered by their lack of\ninterpretability. To address this, we propose Text Bottleneck Models (TBM), an\nintrinsically interpretable text classification framework that offers both\nglobal and local explanations. Rather than directly predicting the output\nlabel, TBM predicts categorical values for a sparse set of salient concepts and\nuses a linear layer over those concept values to produce the final prediction.\nThese concepts can be automatically discovered and measured by a Large Language\nModel (LLM) without the need for human curation. Experiments on 12 diverse text\nunderstanding datasets demonstrate that TBM can rival the performance of\nblack-box baselines such as few-shot GPT-4 and finetuned DeBERTa while falling\nshort against finetuned GPT-3.5. Comprehensive human evaluation validates that\nTBM can generate high-quality concepts relevant to the task, and the concept\nmeasurement aligns well with human judgments, suggesting that the predictions\nmade by TBMs are interpretable. Overall, our findings suggest that TBM is a\npromising new framework that enhances interpretability with minimal performance\ntradeoffs.", "published": "2023-10-30 15:41:32", "link": "http://arxiv.org/abs/2310.19660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoCa: Measuring Human-Language Model Alignment on Causal and Moral\n  Judgment Tasks", "abstract": "Human commonsense understanding of the physical and social world is organized\naround intuitive theories. These theories support making causal and moral\njudgments. When something bad happens, we naturally ask: who did what, and why?\nA rich literature in cognitive science has studied people's causal and moral\nintuitions. This work has revealed a number of factors that systematically\ninfluence people's judgments, such as the violation of norms and whether the\nharm is avoidable or inevitable. We collected a dataset of stories from 24\ncognitive science papers and developed a system to annotate each story with the\nfactors they investigated. Using this dataset, we test whether large language\nmodels (LLMs) make causal and moral judgments about text-based scenarios that\nalign with those of human participants. On the aggregate level, alignment has\nimproved with more recent LLMs. However, using statistical analyses, we find\nthat LLMs weigh the different factors quite differently from human\nparticipants. These results show how curated, challenge datasets combined with\ninsights from cognitive science can help us go beyond comparisons based merely\non aggregate metrics: we uncover LLMs implicit tendencies and show to what\nextent these align with human intuitions.", "published": "2023-10-30 15:57:32", "link": "http://arxiv.org/abs/2310.19677v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Reliability of Large Language Models as Customized\n  Evaluators for Diverse NLP Tasks", "abstract": "Previous work adopts large language models (LLMs) as evaluators to evaluate\nnatural language process (NLP) tasks. However, certain shortcomings, e.g.,\nfairness, scope, and accuracy, persist for current LLM evaluators. To analyze\nwhether LLMs can serve as reliable alternatives to humans, we examine the\nfine-grained alignment between LLM evaluators and human annotators,\nparticularly in understanding the target evaluation tasks and conducting\nevaluations that meet diverse criteria. This paper explores both conventional\ntasks (e.g., story generation) and alignment tasks (e.g., math reasoning), each\nwith different evaluation criteria. Our analysis shows that 1) LLM evaluators\ncan generate unnecessary criteria or omit crucial criteria, resulting in a\nslight deviation from the experts. 2) LLM evaluators excel in general criteria,\nsuch as fluency, but face challenges with complex criteria, such as numerical\nreasoning. We also find that LLM-pre-drafting before human evaluation can help\nreduce the impact of human subjectivity and minimize annotation outliers in\npure human evaluation, leading to more objective evaluation.", "published": "2023-10-30 17:04:35", "link": "http://arxiv.org/abs/2310.19740v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Embeddings for Stance Detection on Social Media", "abstract": "Stance detection on social media is challenging for Large Language Models\n(LLMs), as emerging slang and colloquial language in online conversations often\ncontain deeply implicit stance labels. Chain-of-Thought (COT) prompting has\nrecently been shown to improve performance on stance detection tasks --\nalleviating some of these issues. However, COT prompting still struggles with\nimplicit stance identification. This challenge arises because many samples are\ninitially challenging to comprehend before a model becomes familiar with the\nslang and evolving knowledge related to different topics, all of which need to\nbe acquired through the training data. In this study, we address this problem\nby introducing COT Embeddings which improve COT performance on stance detection\ntasks by embedding COT reasonings and integrating them into a traditional\nRoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text\nencoders can leverage COT reasonings with minor errors or hallucinations that\nwould otherwise distort the COT output label. 2) Text encoders can overlook\nmisleading COT reasoning when a sample's prediction heavily depends on\ndomain-specific patterns. Our model achieves SOTA performance on multiple\nstance detection datasets collected from social media.", "published": "2023-10-30 17:18:10", "link": "http://arxiv.org/abs/2310.19750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as\n  Explainable Metrics", "abstract": "With an increasing number of parameters and pre-training data, generative\nlarge language models (LLMs) have shown remarkable capabilities to solve tasks\nwith minimal or no task-related examples. Notably, LLMs have been successfully\nemployed as evaluation metrics in text generation tasks. Within this context,\nwe introduce the Eval4NLP 2023 shared task that asks participants to explore\nprompting and score extraction for machine translation (MT) and summarization\nevaluation. Specifically, we propose a novel competition setting in which we\nselect a list of allowed LLMs and disallow fine-tuning to ensure a focus on\nprompting. We present an overview of participants' approaches and evaluate them\non a new reference-free test set spanning three language pairs for MT and a\nsummarization dataset. Notably, despite the task's restrictions, the\nbest-performing systems achieve results on par with or even surpassing recent\nreference-free metrics developed using larger models, including GEMBA and\nComet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human\nevaluation of the plausibility of explanations given by the LLMs.", "published": "2023-10-30 17:55:08", "link": "http://arxiv.org/abs/2310.19792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Depth on Compositional Generalization in Transformer\n  Language Models", "abstract": "To process novel sentences, language models (LMs) must generalize\ncompositionally -- combine familiar elements in new ways. What aspects of a\nmodel's structure promote compositional generalization? Focusing on\ntransformers, we test the hypothesis, motivated by theoretical and empirical\nwork, that deeper transformers generalize more compositionally. Simply adding\nlayers increases the total number of parameters; to address this confound\nbetween depth and size, we construct three classes of models which trade off\ndepth for width such that the total number of parameters is kept constant (41M,\n134M and 374M parameters). We pretrain all models as LMs and fine-tune them on\ntasks that test for compositional generalization. We report three main\nconclusions: (1) after fine-tuning, deeper models generalize more\ncompositionally than shallower models do, but the benefit of additional layers\ndiminishes rapidly; (2) within each family, deeper models show better language\nmodeling performance, but returns are similarly diminishing; (3) the benefits\nof depth for compositional generalization cannot be attributed solely to better\nperformance on language modeling. Because model latency is approximately linear\nin the number of layers, these results lead us to the recommendation that, with\na given total parameter budget, transformers can be made shallower than is\ntypical without sacrificing performance.", "published": "2023-10-30 19:10:06", "link": "http://arxiv.org/abs/2310.19956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023", "abstract": "The CLEF eRisk Laboratory explores solutions to different tasks related to\nrisk detection on the Internet. In the 2023 edition, Task 1 consisted of\nsearching for symptoms of depression, the objective of which was to extract\nuser writings according to their relevance to the BDI Questionnaire symptoms.\nTask 2 was related to the problem of early detection of pathological gambling\nrisks, where the participants had to detect users at risk as quickly as\npossible. Finally, Task 3 consisted of estimating the severity levels of signs\nof eating disorders. Our research group participated in the first two tasks,\nproposing solutions based on Transformers. For Task 1, we applied different\napproaches that can be interesting in information retrieval tasks. Two\nproposals were based on the similarity of contextualized embedding vectors, and\nthe other one was based on prompting, an attractive current technique of\nmachine learning. For Task 2, we proposed three fine-tuned models followed by\ndecision policy according to criteria defined by an early detection framework.\nOne model presented extended vocabulary with important words to the addressed\ndomain. In the last task, we obtained good performances considering the\ndecision-based metrics, ranking-based metrics, and runtime. In this work, we\nexplore different ways to deploy the predictive potential of Transformers in\neRisk tasks.", "published": "2023-10-30 19:34:33", "link": "http://arxiv.org/abs/2310.19970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early Detection of Depression and Eating Disorders in Spanish: UNSL at\n  MentalRiskES 2023", "abstract": "MentalRiskES is a novel challenge that proposes to solve problems related to\nearly risk detection for the Spanish language. The objective is to detect, as\nsoon as possible, Telegram users who show signs of mental disorders considering\ndifferent tasks. Task 1 involved the users' detection of eating disorders, Task\n2 focused on depression detection, and Task 3 aimed at detecting an unknown\ndisorder. These tasks were divided into subtasks, each one defining a\nresolution approach. Our research group participated in subtask A for Tasks 1\nand 2: a binary classification problem that evaluated whether the users were\npositive or negative. To solve these tasks, we proposed models based on\nTransformers followed by a decision policy according to criteria defined by an\nearly detection framework. One of the models presented an extended vocabulary\nwith important words for each task to be solved. In addition, we applied a\ndecision policy based on the history of predictions that the model performs\nduring user evaluation. For Tasks 1 and 2, we obtained the second-best\nperformance according to rankings based on classification and latency,\ndemonstrating the effectiveness and consistency of our approaches for solving\nearly detection problems in the Spanish language.", "published": "2023-10-30 20:38:31", "link": "http://arxiv.org/abs/2310.20003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Examples to Annotate for In-Context Learning? Towards Effective\n  and Efficient Selection", "abstract": "Large Language Models (LLMs) can adapt to new tasks via in-context learning\n(ICL). ICL is efficient as it does not require any parameter updates to the\ntrained LLM, but only few annotated examples as input for the LLM. In this\nwork, we investigate an active learning approach for ICL, where there is a\nlimited budget for annotating examples. We propose a model-adaptive\noptimization-free algorithm, termed AdaICL, which identifies examples that the\nmodel is uncertain about, and performs semantic diversity-based example\nselection. Diversity-based sampling improves overall effectiveness, while\nuncertainty sampling improves budget efficiency and helps the LLM learn new\ninformation. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage\nproblem, that dynamically adapts based on the model's feedback and can be\napproximately solved via greedy algorithms. Extensive experiments on nine\ndatasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy\npoints over SOTA (7.7% relative improvement), is up to 3x more budget-efficient\nthan performing annotations uniformly at random, while it outperforms SOTA with\n2x fewer ICL examples.", "published": "2023-10-30 22:03:55", "link": "http://arxiv.org/abs/2310.20046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic\n  Relations", "abstract": "In the current paper, we present the KG-FRUS dataset, comprised of more than\n300,000 US government diplomatic documents encoded in a Knowledge Graph (KG).\nWe leverage the data of the Foreign Relations of the United States (FRUS)\n(available as XML files) to extract information about the documents and the\nindividuals and countries mentioned within them. We use the extracted entities,\nand associated metadata, to create a graph-based dataset. Further, we\nsupplement the created KG with additional entities and relations from Wikidata.\nThe relations in the KG capture the synergies and dynamics required to study\nand understand the complex fields of diplomacy, foreign relations, and\npolitics. This goes well beyond a simple collection of documents which neglects\nthe relations between entities in the text. We showcase a range of\npossibilities of the current dataset by illustrating different approaches to\nprobe the KG. In the paper, we exemplify how to use a query language to answer\nsimple research questions and how to use graph algorithms such as Node2Vec and\nPageRank, that benefit from the complete graph structure. More importantly, the\nchosen structure provides total flexibility for continuously expanding and\nenriching the graph. Our solution is general, so the proposed pipeline for\nbuilding the KG can encode other original corpora of time-dependent and complex\nphenomena. Overall, we present a mechanism to create KG databases providing a\nmore versatile representation of time-dependent related text data and a\nparticular application to the all-important FRUS database.", "published": "2023-10-30 10:53:02", "link": "http://arxiv.org/abs/2311.01606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EHRTutor: Enhancing Patient Understanding of Discharge Instructions", "abstract": "Large language models have shown success as a tutor in education in various\nfields. Educating patients about their clinical visits plays a pivotal role in\npatients' adherence to their treatment plans post-discharge. This paper\npresents EHRTutor, an innovative multi-component framework leveraging the Large\nLanguage Model (LLM) for patient education through conversational\nquestion-answering. EHRTutor first formulates questions pertaining to the\nelectronic health record discharge instructions. It then educates the patient\nthrough conversation by administering each question as a test. Finally, it\ngenerates a summary at the end of the conversation. Evaluation results using\nLLMs and domain experts have shown a clear preference for EHRTutor over the\nbaseline. Moreover, EHRTutor also offers a framework for generating synthetic\npatient education dialogues that can be used for future in-house system\ntraining.", "published": "2023-10-30 00:46:03", "link": "http://arxiv.org/abs/2310.19212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skywork: A More Open Bilingual Foundation Model", "abstract": "In this technical report, we present Skywork-13B, a family of large language\nmodels (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both\nEnglish and Chinese texts. This bilingual foundation model is the most\nextensively trained and openly published LLMs of comparable size to date. We\nintroduce a two-stage training methodology using a segmented corpus, targeting\ngeneral purpose training and then domain-specific enhancement training,\nrespectively. We show that our model not only excels on popular benchmarks, but\nalso achieves \\emph{state of the art} performance in Chinese language modeling\non diverse domains. Furthermore, we propose a novel leakage detection method,\ndemonstrating that test data contamination is a pressing issue warranting\nfurther investigation by the LLM community. To spur future research, we release\nSkywork-13B along with checkpoints obtained during intermediate stages of the\ntraining process. We are also releasing part of our SkyPile corpus, a\ncollection of over 150 billion tokens of web text, which is the largest high\nquality open Chinese pre-training corpus to date. We hope Skywork-13B and our\nopen corpus will serve as a valuable open-source resource to democratize access\nto high-quality LLMs.", "published": "2023-10-30 08:31:47", "link": "http://arxiv.org/abs/2310.19341v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization", "abstract": "Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.", "published": "2023-10-30 08:40:16", "link": "http://arxiv.org/abs/2310.19347v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mean BERTs make erratic language teachers: the effectiveness of latent\n  bootstrapping in low-resource settings", "abstract": "This paper explores the use of latent bootstrapping, an alternative\nself-supervision technique, for pretraining language models. Unlike the typical\npractice of using self-supervision on discrete subwords, latent bootstrapping\nleverages contextualized embeddings for a richer supervision signal. We conduct\nexperiments to assess how effective this approach is for acquiring linguistic\nknowledge from limited resources. Specifically, our experiments are based on\nthe BabyLM shared task, which includes pretraining on two small curated corpora\nand an evaluation on four linguistic benchmarks.", "published": "2023-10-30 10:31:32", "link": "http://arxiv.org/abs/2310.19420v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CreoleVal: Multilingual Multitask Benchmarks for Creoles", "abstract": "Creoles represent an under-explored and marginalized group of languages, with\nfew available resources for NLP research.While the genealogical ties between\nCreoles and a number of highly-resourced languages imply a significant\npotential for transfer learning, this potential is hampered due to this lack of\nannotated data. In this work we present CreoleVal, a collection of benchmark\ndatasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it\nis an aggregate of novel development datasets for reading comprehension,\nrelation classification, and machine translation for Creoles, in addition to a\npractical gateway to a handful of preexisting benchmarks. For each benchmark,\nwe conduct baseline experiments in a zero-shot setting in order to further\nascertain the capabilities and limitations of transfer learning for Creoles.\nUltimately, we see CreoleVal as an opportunity to empower research on Creoles\nin NLP and computational linguistics, and in general, a step towards more\nequitable language technology around the globe.", "published": "2023-10-30 14:24:20", "link": "http://arxiv.org/abs/2310.19567v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMaAA: Making Large Language Models as Active Annotators", "abstract": "Prevalent supervised learning methods in natural language processing (NLP)\nare notoriously data-hungry, which demand large amounts of high-quality\nannotated data. In practice, acquiring such data is a costly endeavor.\nRecently, the superior few-shot performance of large language models (LLMs) has\npropelled the development of dataset generation, where the training data are\nsolely synthesized from LLMs. However, such an approach usually suffers from\nlow-quality issues, and requires orders of magnitude more labeled data to\nachieve satisfactory performance. To fully exploit the potential of LLMs and\nmake use of massive unlabeled data, we propose LLMaAA, which takes LLMs as\nannotators and puts them into an active learning loop to determine what to\nannotate efficiently. To learn robustly with pseudo labels, we optimize both\nthe annotation and training processes: (1) we draw k-NN examples from a small\ndemonstration pool as in-context examples, and (2) we adopt the example\nreweighting technique to assign training samples with learnable weights.\nCompared with previous approaches, LLMaAA features both efficiency and\nreliability. We conduct experiments and analysis on two classic NLP tasks,\nnamed entity recognition and relation extraction. With LLMaAA, task-specific\nmodels trained from LLM-generated labels can outperform the teacher within only\nhundreds of annotated examples, which is much more cost-effective than other\nbaselines.", "published": "2023-10-30 14:54:15", "link": "http://arxiv.org/abs/2310.19596v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models", "abstract": "Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind", "published": "2023-10-30 15:12:09", "link": "http://arxiv.org/abs/2310.19619v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining Tree Model Decisions in Natural Language for Network\n  Intrusion Detection", "abstract": "Network intrusion detection (NID) systems which leverage machine learning\nhave been shown to have strong performance in practice when used to detect\nmalicious network traffic. Decision trees in particular offer a strong balance\nbetween performance and simplicity, but require users of NID systems to have\nbackground knowledge in machine learning to interpret. In addition, they are\nunable to provide additional outside information as to why certain features may\nbe important for classification.\n  In this work, we explore the use of large language models (LLMs) to provide\nexplanations and additional background knowledge for decision tree NID systems.\nFurther, we introduce a new human evaluation framework for decision tree\nexplanations, which leverages automatically generated quiz questions that\nmeasure human evaluators' understanding of decision tree inference. Finally, we\nshow LLM generated decision tree explanations correlate highly with human\nratings of readability, quality, and use of background knowledge while\nsimultaneously providing better understanding of decision boundaries.", "published": "2023-10-30 15:40:34", "link": "http://arxiv.org/abs/2310.19658v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models: The Need for Nuance in Current Debates and a\n  Pragmatic Perspective on Understanding", "abstract": "Current Large Language Models (LLMs) are unparalleled in their ability to\ngenerate grammatically correct, fluent text. LLMs are appearing rapidly, and\ndebates on LLM capacities have taken off, but reflection is lagging behind.\nThus, in this position paper, we first zoom in on the debate and critically\nassess three points recurring in critiques of LLM capacities: i) that LLMs only\nparrot statistical patterns in the training data; ii) that LLMs master formal\nbut not functional language competence; and iii) that language learning in LLMs\ncannot inform human language learning. Drawing on empirical and theoretical\narguments, we show that these points need more nuance. Second, we outline a\npragmatic perspective on the issue of `real' understanding and intentionality\nin LLMs. Understanding and intentionality pertain to unobservable mental states\nwe attribute to other humans because they have pragmatic value: they allow us\nto abstract away from complex underlying mechanics and predict behaviour\neffectively. We reflect on the circumstances under which it would make sense\nfor humans to similarly attribute mental states to LLMs, thereby outlining a\npragmatic philosophical context for LLMs as an increasingly prominent\ntechnology in society.", "published": "2023-10-30 15:51:04", "link": "http://arxiv.org/abs/2310.19671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Digital Spaces: An Overview of Reviews", "abstract": "Sentiment analysis (SA) is commonly applied to digital textual data,\nrevealing insight into opinions and feelings. Many systematic reviews have\nsummarized existing work, but often overlook discussions of validity and\nscientific practices. Here, we present an overview of reviews, synthesizing 38\nsystematic reviews, containing 2,275 primary studies. We devise a bespoke\nquality assessment framework designed to assess the rigor and quality of\nsystematic review methodologies and reporting standards. Our findings show\ndiverse applications and methods, limited reporting rigor, and challenges over\ntime. We discuss how future research and practitioners can address these issues\nand highlight their importance across numerous applications.", "published": "2023-10-30 16:04:35", "link": "http://arxiv.org/abs/2310.19687v1", "categories": ["cs.CY", "cs.CL", "I.2.7"], "primary_category": "cs.CY"}
{"title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and\n  Limitations", "abstract": "Context-based fine-tuning methods, including prompting, in-context learning,\nsoft prompting (also known as prompt tuning), and prefix-tuning, have gained\npopularity due to their ability to often match the performance of full\nfine-tuning with a fraction of the parameters. Despite their empirical\nsuccesses, there is little theoretical understanding of how these techniques\ninfluence the internal computation of the model and their expressiveness\nlimitations. We show that despite the continuous embedding space being more\nexpressive than the discrete token space, soft-prompting and prefix-tuning are\npotentially less expressive than full fine-tuning, even with the same number of\nlearnable parameters. Concretely, context-based fine-tuning cannot change the\nrelative attention pattern over the content and can only bias the outputs of an\nattention layer in a fixed direction. This suggests that while techniques like\nprompting, in-context learning, soft prompting, and prefix-tuning can\neffectively elicit skills present in the pretrained model, they may not be able\nto learn novel tasks that require new attention patterns.", "published": "2023-10-30 16:19:34", "link": "http://arxiv.org/abs/2310.19698v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Combining Language Models For Specialized Domains: A Colorful Approach", "abstract": "General purpose language models (LMs) encounter difficulties when processing\ndomain-specific jargon and terminology, which are frequently utilized in\nspecialized fields such as medicine or industrial settings. Moreover, they\noften find it challenging to interpret mixed speech that blends general\nlanguage with specialized jargon. This poses a challenge for automatic speech\nrecognition systems operating within these specific domains. In this work, we\nintroduce a novel approach that integrates domain-specific or secondary LM into\ngeneral-purpose LM. This strategy involves labeling, or \"coloring\", each word\nto indicate its association with either the general or the domain-specific LM.\nWe develop an optimized algorithm that enhances the beam search algorithm to\neffectively handle inferences involving colored words. Our evaluations indicate\nthat this approach is highly effective in integrating jargon into language\ntasks. Notably, our method substantially lowers the error rate for\ndomain-specific words without compromising performance in the general domain.", "published": "2023-10-30 16:35:55", "link": "http://arxiv.org/abs/2310.19708v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models: A Comprehensive Survey", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na broad spectrum of tasks. They have attracted significant attention and been\ndeployed in numerous downstream applications. Nevertheless, akin to a\ndouble-edged sword, LLMs also present potential risks. They could suffer from\nprivate data leaks or yield inappropriate, harmful, or misleading content.\nAdditionally, the rapid progress of LLMs raises concerns about the potential\nemergence of superintelligent systems without adequate safeguards. To\neffectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of\nLLMs. We categorize the evaluation of LLMs into three major groups: knowledge\nand capability evaluation, alignment evaluation and safety evaluation. In\naddition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs' performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making\nevaluation serve as a cornerstone in guiding the responsible development of\nLLMs. We envision that this will channel their evolution into a direction that\nmaximizes societal benefit while minimizing potential risks. A curated list of\nrelated papers has been publicly available at\nhttps://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.", "published": "2023-10-30 17:00:52", "link": "http://arxiv.org/abs/2310.19736v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioInstruct: Instruction Tuning of Large Language Models for Biomedical\n  Natural Language Processing", "abstract": "To enhance the performance of large language models (LLMs) in biomedical\nnatural language processing (BioNLP) by introducing a domain-specific\ninstruction dataset and examining its impact when combined with multi-task\nlearning principles. We created the BioInstruct, comprising 25,005 instructions\nto instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were\ncreated by prompting the GPT-4 language model with three-seed samples randomly\ndrawn from an 80 human curated instructions. We employed Low-Rank\nAdaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these\ninstruction-tuned LLMs on several BioNLP tasks, which can be grouped into three\nmajor categories: question answering(QA), information extraction(IE), and text\ngeneration(GEN). We also examined whether categories(e.g., QA, IE, and\ngeneration) of instructions impact model performance. Comparing with LLMs\nwithout instruction-tuned, our instruction-tuned LLMs demonstrated marked\nperformance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our\n7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed\nother LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with\nvast domain-specific data or a variety of tasks. Our results also show that the\nperformance gain is significantly higher when instruction fine-tuning is\nconducted with closely related tasks. Our findings align with the observations\nof multi-task learning, suggesting the synergies between two tasks. The\nBioInstruct dataset serves as a valuable resource and instruction tuned LLMs\nlead to the best performing BioNLP applications.", "published": "2023-10-30 19:38:50", "link": "http://arxiv.org/abs/2310.19975v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthetic Imitation Edit Feedback for Factual Alignment in Clinical\n  Summarization", "abstract": "Large Language Models (LLMs) like the GPT and LLaMA families have\ndemonstrated exceptional capabilities in capturing and condensing critical\ncontextual information and achieving state-of-the-art performance in the\nsummarization task. However, community concerns about these models'\nhallucination issues continue to rise. LLMs sometimes generate factually\nhallucinated summaries, which can be extremely harmful in the clinical domain\nNLP tasks (e.g., clinical note summarization), where factually incorrect\nstatements can lead to critically erroneous diagnoses. Fine-tuning LLMs using\nhuman feedback has shown the promise of aligning LLMs to be factually\nconsistent during generation, but such training procedure requires high-quality\nhuman-annotated data, which can be extremely expensive to get in the clinical\ndomain. In this work, we propose a new pipeline using ChatGPT instead of human\nexperts to generate high-quality feedback data for improving factual\nconsistency in the clinical note summarization task. We focus specifically on\nedit feedback because recent work discusses the shortcomings of human alignment\nvia preference feedback in complex situations (such as clinical NLP tasks that\nrequire extensive expert knowledge), as well as some advantages of collecting\nedit feedback from domain experts. In addition, although GPT has reached the\nexpert level in many clinical NLP tasks (e.g., USMLE QA), there is not much\nprevious work discussing whether GPT can generate expert-level edit feedback\nfor LMs in the clinical note summarization task. We hope to fill this gap.\nFinally, our evaluations demonstrate the potential use of GPT edits in human\nalignment, especially from a factuality perspective.", "published": "2023-10-30 21:33:22", "link": "http://arxiv.org/abs/2310.20033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation of Generative Models with Instruction Tuning", "abstract": "Automatic evaluation of natural language generation has long been an elusive\ngoal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate\nhuman judgements for a particular task and evaluation criterion. Inspired by\nthe generalization ability of instruction-tuned models, we propose a learned\nmetric based on instruction tuning. To test our approach, we collected HEAP, a\ndataset of human judgements across various NLG tasks and evaluation criteria.\nOur findings demonstrate that instruction tuning language models on HEAP yields\ngood performance on many evaluation tasks, though some criteria are less\ntrivial to learn than others. Further, jointly training on multiple tasks can\nyield additional performance improvements, which can be beneficial for future\ntasks with little to no human annotated data.", "published": "2023-10-30 23:00:52", "link": "http://arxiv.org/abs/2310.20072v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Partial Tensorized Transformers for Natural Language Processing", "abstract": "The transformer architecture has revolutionized Natural Language Processing\n(NLP) and other machine-learning tasks, due to its unprecedented accuracy.\nHowever, their extensive memory and parameter requirements often hinder their\npractical applications. In this work, we study the effect of tensor-train\ndecomposition to improve the accuracy and compress transformer vision-language\nneural networks, namely BERT and ViT. We focus both on embedding-layer\ncompression and partial tensorization of neural networks (PTNN) through an\nalgorithmic approach. Our novel PTNN approach significantly improves the\naccuracy of existing models by up to 5%, all without the need for post-training\nadjustments, breaking new ground in the field of tensor decomposition.", "published": "2023-10-30 23:19:06", "link": "http://arxiv.org/abs/2310.20077v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Remember what you did so you know what to do next", "abstract": "We explore using a moderately sized large language model (GPT-J 6B\nparameters) to create a plan for a simulated robot to achieve 30 classes of\ngoals in ScienceWorld, a text game simulator for elementary science\nexperiments. Previously published empirical work claimed that large language\nmodels (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement\nlearning. Using the Markov assumption (a single previous step), the LLM\noutperforms the reinforcement learning-based approach by a factor of 1.4. When\nwe fill the LLM's input buffer with as many prior steps as possible,\nimprovement rises to 3.5x. Even when training on only 6.5% of the training\ndata, we observe a 2.2x improvement over the reinforcement-learning-based\napproach. Our experiments show that performance varies widely across the 30\nclasses of actions, indicating that averaging over tasks can hide significant\nperformance issues. In work contemporaneous with ours, Lin et al. (2023)\ndemonstrated a two-part approach (SwiftSage) that uses a small LLM (T5-large)\ncomplemented by OpenAI's massive LLMs to achieve outstanding results in\nScienceWorld. Our 6-B parameter, single-stage GPT-J matches the performance of\nSwiftSage's two-stage architecture when it incorporates GPT-3.5 turbo which has\n29-times more parameters than GPT-J.", "published": "2023-10-30 19:29:00", "link": "http://arxiv.org/abs/2311.01468v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Language Models to Detect Greenwashing", "abstract": "In recent years, climate change repercussions have increasingly captured\npublic interest. Consequently, corporations are emphasizing their environmental\nefforts in sustainability reports to bolster their public image. Yet, the\nabsence of stringent regulations in review of such reports allows potential\ngreenwashing. In this study, we introduce a novel preliminary methodology to\ntrain a language model on generated labels for greenwashing risk. Our primary\ncontributions encompass: developing a preliminary mathematical formulation to\nquantify greenwashing risk, a fine-tuned ClimateBERT model for this problem,\nand a comparative analysis of results. On a test set comprising of\nsustainability reports, our best model achieved an average accuracy score of\n86.34% and F1 score of 0.67, demonstrating that our proof-of-concept\nmethodology shows a promising direction of exploration for this task.", "published": "2023-10-30 21:41:49", "link": "http://arxiv.org/abs/2311.01469v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Domain Knowledge Extraction for Knowledge Graphs", "abstract": "The quality of a knowledge graph directly impacts the quality of downstream\napplications (e.g. the number of answerable questions using the graph). One\nongoing challenge when building a knowledge graph is to ensure completeness and\nfreshness of the graph's entities and facts. In this paper, we introduce ODKE,\na scalable and extensible framework that sources high-quality entities and\nfacts from open web at scale. ODKE utilizes a wide range of extraction models\nand supports both streaming and batch processing at different latency. We\nreflect on the challenges and design decisions made and share lessons learned\nwhen building and deploying ODKE to grow an industry-scale open domain\nknowledge graph.", "published": "2023-10-30 22:18:34", "link": "http://arxiv.org/abs/2312.09424v1", "categories": ["cs.CL", "cs.AI", "68T30 (primary)", "F.4.1; I.2.4"], "primary_category": "cs.CL"}
{"title": "Moral Sparks in Social Media Narratives", "abstract": "There is increasing interest in building computational models of moral\nreasoning by people to enable effective interaction by Artificial Intelligence\n(AI) agents. We examine interactions on social media to understand human moral\njudgments in real-life ethical scenarios. Specifically, we examine posts from a\npopular Reddit subreddit (i.e., a subcommunity) called r/AmITheAsshole, where\nauthors and commenters share their moral judgments on who (i.e., which\nparticipant of the described scenario) is blameworthy. To investigate the\nunderlying reasoning influencing moral judgments, we focus on excerpts-which we\nterm moral sparks-from original posts that some commenters include to indicate\nwhat motivates their judgments. To this end, we examine how (1) events\nactivating social commonsense and (2) linguistic signals affect the identified\nmoral sparks and their subsequent judgments. By examining over 24672 posts and\n175988 comments, we find that event-related negative character traits (e.g.,\nimmature and rude) attract attention and stimulate blame, implying a dependent\nrelationship between character traits and moral values. Specifically, we focus\non causal graphs involving events (c-events) that activate social commonsense.\nWe observe that c-events are perceived with varying levels of informativeness,\ninfluencing moral spark and judgment assignment in distinct ways. This\nobservation is reinforced by examining linguistic features describing\nsemantically similar c-events. Moreover, language influencing commenters'\ncognitive processes enhances the probability of an excerpt becoming a moral\nspark, while factual and concrete descriptions tend to inhibit this effect.", "published": "2023-10-30 05:03:26", "link": "http://arxiv.org/abs/2310.19268v3", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond\n  Visual Common Sense", "abstract": "Humans possess a strong capability for reasoning beyond common sense. For\nexample, given an unconventional image of a goldfish laying on the table next\nto an empty fishbowl, a human would effortlessly determine that the fish is not\ninside the fishbowl. The case, however, may be different for a vision-language\nmodel, whose reasoning could gravitate towards the common scenario that the\nfish is inside the bowl, despite the visual input. In this paper, we introduce\na novel probing dataset named ROME (reasoning beyond commonsense knowledge) to\nevaluate whether the state-of-the-art pre-trained vision-language models have\nthe reasoning capability to correctly interpret counter-intuitive content. ROME\ncontains images that defy commonsense knowledge with regards to color, shape,\nmaterial, size and positional relation. Experiments on the state-of-the-art\npre-trained vision-language models reveal that most of these models are still\nlargely incapable of interpreting counter-intuitive scenarios. We hope that\nROME will spur further investigations on reasoning beyond commonsense knowledge\nin vision-language research.", "published": "2023-10-30 06:35:37", "link": "http://arxiv.org/abs/2310.19301v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DPATD: Dual-Phase Audio Transformer for Denoising", "abstract": "Recent high-performance transformer-based speech enhancement models\ndemonstrate that time domain methods could achieve similar performance as\ntime-frequency domain methods. However, time-domain speech enhancement systems\ntypically receive input audio sequences consisting of a large number of time\nsteps, making it challenging to model extremely long sequences and train models\nto perform adequately. In this paper, we utilize smaller audio chunks as input\nto achieve efficient utilization of audio information to address the above\nchallenges. We propose a dual-phase audio transformer for denoising (DPATD), a\nnovel model to organize transformer layers in a deep structure to learn clean\naudio sequences for denoising. DPATD splits the audio input into smaller\nchunks, where the input length can be proportional to the square root of the\noriginal sequence length. Our memory-compressed explainable attention is\nefficient and converges faster compared to the frequently used self-attention\nmodule. Extensive experiments demonstrate that our model outperforms\nstate-of-the-art methods.", "published": "2023-10-30 14:44:59", "link": "http://arxiv.org/abs/2310.19588v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Pre-trained Language Model into Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) has become a significant technology in\nnatural language processing through extensive research and development.\nHowever, the deficiency of high-quality bilingual language pair data still\nposes a major challenge to improving NMT performance. Recent studies have been\nexploring the use of contextual information from pre-trained language model\n(PLM) to address this problem. Yet, the issue of incompatibility between PLM\nand NMT model remains unresolved. This study proposes PLM-integrated NMT\n(PiNMT) model to overcome the identified problems. PiNMT model consists of\nthree critical components, PLM Multi Layer Converter, Embedding Fusion, and\nCosine Alignment, each playing a vital role in providing effective PLM\ninformation to NMT. Furthermore, two training strategies, Separate Learning\nRates and Dual Step Training, are also introduced in this paper. By\nimplementing the proposed PiNMT model and training strategy, we achieve\nstate-of-the-art performance on the IWSLT'14 En$\\leftrightarrow$De dataset.\nThis study's outcomes are noteworthy as they demonstrate a novel approach for\nefficiently integrating PLM with NMT to overcome incompatibility and enhance\nperformance.", "published": "2023-10-30 16:00:13", "link": "http://arxiv.org/abs/2310.19680v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Medical Prescriptions with Conditional Transformer", "abstract": "Access to real-world medication prescriptions is essential for medical\nresearch and healthcare quality improvement. However, access to real medication\nprescriptions is often limited due to the sensitive nature of the information\nexpressed. Additionally, manually labelling these instructions for training and\nfine-tuning Natural Language Processing (NLP) models can be tedious and\nexpensive. We introduce a novel task-specific model architecture,\nLabel-To-Text-Transformer (\\textbf{LT3}), tailored to generate synthetic\nmedication prescriptions based on provided labels, such as a vocabulary list of\nmedications and their attributes. LT3 is trained on a set of around 2K lines of\nmedication prescriptions extracted from the MIMIC-III database, allowing the\nmodel to produce valuable synthetic medication prescriptions. We evaluate LT3's\nperformance by contrasting it with a state-of-the-art Pre-trained Language\nModel (PLM), T5, analysing the quality and diversity of generated texts. We\ndeploy the generated synthetic data to train the SpacyNER model for the Named\nEntity Recognition (NER) task over the n2c2-2018 dataset. The experiments show\nthat the model trained on synthetic data can achieve a 96-98\\% F1 score at\nLabel Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and\ndata will be shared at\n\\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}", "published": "2023-10-30 16:53:11", "link": "http://arxiv.org/abs/2310.19727v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What's \"up\" with vision-language models? Investigating their struggle\n  with spatial reasoning", "abstract": "Recent vision-language (VL) models are powerful, but can they reliably\ndistinguish \"right\" from \"left\"? We curate three new corpora to quantify model\ncomprehension of such basic spatial relations. These tests isolate spatial\nreasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp\nbenchmark contains sets of photographs varying only the spatial relations of\nobjects, keeping their identity fixed (see Figure 1: models must comprehend not\nonly the usual case of a dog under a table, but also, the same dog on top of\nthe same table). We evaluate 18 VL models, finding that all perform poorly,\ne.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56%\naccuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of\nthis surprising behavior, finding: 1) that popular vision-language pretraining\ncorpora like LAION-2B contain little reliable data for learning spatial\nrelationships; and 2) that basic modeling interventions like up-weighting\npreposition-containing instances or fine-tuning on our corpora are not\nsufficient to address the challenges our benchmarks pose. We are hopeful that\nthese corpora will facilitate further research, and we release our data and\ncode at https://github.com/amitakamath/whatsup_vlms.", "published": "2023-10-30 17:50:15", "link": "http://arxiv.org/abs/2310.19785v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code", "abstract": "While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.", "published": "2023-10-30 17:55:02", "link": "http://arxiv.org/abs/2310.19791v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents", "abstract": "Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.", "published": "2023-10-30 18:35:30", "link": "http://arxiv.org/abs/2310.19923v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Split-NER: Named Entity Recognition via Two Question-Answering-based\n  Classifications", "abstract": "In this work, we address the NER problem by splitting it into two logical\nsub-tasks: (1) Span Detection which simply extracts entity mention spans\nirrespective of entity type; (2) Span Classification which classifies the spans\ninto their entity types. Further, we formulate both sub-tasks as\nquestion-answering (QA) problems and produce two leaner models which can be\noptimized separately for each sub-task. Experiments with four cross-domain\ndatasets demonstrate that this two-step approach is both effective and time\nefficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17\nand a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all\ncases, it achieves a significant reduction in training time compared to its QA\nbaseline counterpart. The effectiveness of our system stems from fine-tuning\nthe BERT model twice, separately for span detection and classification. The\nsource code can be found at https://github.com/c3sr/split-ner.", "published": "2023-10-30 18:57:28", "link": "http://arxiv.org/abs/2310.19942v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Integrating Summarization and Retrieval for Enhanced Personalization via\n  Large Language Models", "abstract": "Personalization, the ability to tailor a system to individual users, is an\nessential factor in user experience with natural language processing (NLP)\nsystems. With the emergence of Large Language Models (LLMs), a key question is\nhow to leverage these models to better personalize user experiences. To\npersonalize a language model's output, a straightforward approach is to\nincorporate past user data into the language model prompt, but this approach\ncan result in lengthy inputs exceeding limitations on input length and\nincurring latency and cost issues. Existing approaches tackle such challenges\nby selectively extracting relevant user data (i.e. selective retrieval) to\nconstruct a prompt for downstream tasks. However, retrieval-based methods are\nlimited by potential information loss, lack of more profound user\nunderstanding, and cold-start challenges. To overcome these limitations, we\npropose a novel summary-augmented approach by extending retrieval-augmented\npersonalization with task-aware user summaries generated by LLMs. The summaries\ncan be generated and stored offline, enabling real-world systems with runtime\nconstraints like voice assistants to leverage the power of LLMs. Experiments\nshow our method with 75% less of retrieved user data is on-par or outperforms\nretrieval augmentation on most tasks in the LaMP personalization benchmark. We\ndemonstrate that offline summarization via LLMs and runtime retrieval enables\nbetter performance for personalization on a range of tasks under practical\nconstraints.", "published": "2023-10-30 23:40:41", "link": "http://arxiv.org/abs/2310.20081v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Faithful and Robust Local Interpretability for Textual Predictions", "abstract": "Interpretability is essential for machine learning models to be trusted and\ndeployed in critical domains. However, existing methods for interpreting text\nmodels are often complex, lack mathematical foundations, and their performance\nis not guaranteed. In this paper, we propose FRED (Faithful and Robust\nExplainer for textual Documents), a novel method for interpreting predictions\nover text. FRED offers three key insights to explain a model prediction: (1) it\nidentifies the minimal set of words in a document whose removal has the\nstrongest influence on the prediction, (2) it assigns an importance score to\neach token, reflecting its influence on the model's output, and (3) it provides\ncounterfactual explanations by generating examples similar to the original\ndocument, but leading to a different prediction. We establish the reliability\nof FRED through formal definitions and theoretical analyses on interpretable\nclassifiers. Additionally, our empirical evaluation against state-of-the-art\nmethods demonstrates the effectiveness of FRED in providing insights into text\nmodels.", "published": "2023-10-30 20:27:36", "link": "http://arxiv.org/abs/2311.01605v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generative retrieval-augmented ontologic graph and multi-agent\n  strategies for interpretive large language model-based materials design", "abstract": "Transformer neural networks show promising capabilities, in particular for\nuses in materials analysis, design and manufacturing, including their capacity\nto work effectively with both human language, symbols, code, and numerical\ndata. Here we explore the use of large language models (LLMs) as a tool that\ncan support engineering analysis of materials, applied to retrieving key\ninformation about subject areas, developing research hypotheses, discovery of\nmechanistic relationships across disparate areas of knowledge, and writing and\nexecuting simulation codes for active knowledge generation based on physical\nground truths. When used as sets of AI agents with specific features,\ncapabilities, and instructions, LLMs can provide powerful problem solution\nstrategies for applications in analysis and design problems. Our experiments\nfocus on using a fine-tuned model, MechGPT, developed based on training data in\nthe mechanics of materials domain. We first affirm how finetuning endows LLMs\nwith reasonable understanding of domain knowledge. However, when queried\noutside the context of learned matter, LLMs can have difficulty to recall\ncorrect information. We show how this can be addressed using\nretrieval-augmented Ontological Knowledge Graph strategies that discern how the\nmodel understands what concepts are important and how they are related.\nIllustrated for a use case of relating distinct areas of knowledge - here,\nmusic and proteins - such strategies can also provide an interpretable graph\nstructure with rich information at the node, edge and subgraph level. We\ndiscuss nonlinear sampling strategies and agent-based modeling applied to\ncomplex question answering, code generation and execution in the context of\nautomated force field development from actively learned Density Functional\nTheory (DFT) modeling, and data analysis.", "published": "2023-10-30 20:31:50", "link": "http://arxiv.org/abs/2310.19998v1", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph"], "primary_category": "cs.CL"}
{"title": "DCHT: Deep Complex Hybrid Transformer for Speech Enhancement", "abstract": "Most of the current deep learning-based approaches for speech enhancement\nonly operate in the spectrogram or waveform domain. Although a cross-domain\ntransformer combining waveform- and spectrogram-domain inputs has been\nproposed, its performance can be further improved. In this paper, we present a\nnovel deep complex hybrid transformer that integrates both spectrogram and\nwaveform domains approaches to improve the performance of speech enhancement.\nThe proposed model consists of two parts: a complex Swin-Unet in the\nspectrogram domain and a dual-path transformer network (DPTnet) in the waveform\ndomain. We first construct a complex Swin-Unet network in the spectrogram\ndomain and perform speech enhancement in the complex audio spectrum. We then\nintroduce improved DPT by adding memory-compressed attention. Our model is\ncapable of learning multi-domain features to reduce existing noise on different\ndomains in a complementary way. The experimental results on the\nBirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our\nmethod can achieve better performance compared to state-of-the-art methods.", "published": "2023-10-30 14:58:11", "link": "http://arxiv.org/abs/2310.19602v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction", "abstract": "Target speech extraction aims to extract, based on a given conditioning cue,\na target speech signal that is corrupted by interfering sources, such as noise\nor competing speakers. Building upon the achievements of the state-of-the-art\n(SOTA) time-frequency speaker separation model TF-GridNet, we propose\nAV-GridNet, a visual-grounded variant that incorporates the face recording of a\ntarget speaker as a conditioning factor during the extraction process.\nRecognizing the inherent dissimilarities between speech and noise signals as\ninterfering sources, we also propose SAV-GridNet, a scenario-aware model that\nidentifies the type of interfering scenario first and then applies a dedicated\nexpert model trained specifically for that scenario. Our proposed model\nachieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement\nChallenge, outperforming other models by a significant margin, objectively and\nin a listening test. We also perform an extensive analysis of the results under\nthe two scenarios.", "published": "2023-10-30 15:32:56", "link": "http://arxiv.org/abs/2310.19644v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Sound of Story: Multi-modal Storytelling with Audio", "abstract": "Storytelling is multi-modal in the real world. When one tells a story, one\nmay use all of the visualizations and sounds along with the story itself.\nHowever, prior studies on storytelling datasets and tasks have paid little\nattention to sound even though sound also conveys meaningful semantics of the\nstory. Therefore, we propose to extend story understanding and telling areas by\nestablishing a new component called \"background sound\" which is story\ncontext-based audio without any linguistic information. For this purpose, we\nintroduce a new dataset, called \"Sound of Story (SoS)\", which has paired image\nand text sequences with corresponding sound or background music for a story. To\nthe best of our knowledge, this is the largest well-curated dataset for\nstorytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6\nimages per story and 984 hours of speech-decoupled audio such as background\nmusic and other sounds. As benchmark tasks for storytelling with sound and the\ndataset, we propose retrieval tasks between modalities, and audio generation\ntasks from image-text sequences, introducing strong baselines for them. We\nbelieve the proposed dataset and tasks may shed light on the multi-modal\nunderstanding of storytelling in terms of sound. Downloading the dataset and\nbaseline codes for each task will be released in the link:\nhttps://github.com/Sosdatasets/SoS_Dataset.", "published": "2023-10-30 04:26:24", "link": "http://arxiv.org/abs/2310.19264v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Seeing Through the Conversation: Audio-Visual Speech Separation based on\n  Diffusion Model", "abstract": "The objective of this work is to extract target speaker's voice from a\nmixture of voices using visual cues. Existing works on audio-visual speech\nseparation have demonstrated their performance with promising intelligibility,\nbut maintaining naturalness remains a challenge. To address this issue, we\npropose AVDiffuSS, an audio-visual speech separation model based on a diffusion\nmechanism known for its capability in generating natural samples. For an\neffective fusion of the two modalities for diffusion, we also propose a\ncross-attention-based feature fusion mechanism. This mechanism is specifically\ntailored for the speech domain to integrate the phonetic information from\naudio-visual correspondence in speech generation. In this way, the fusion\nprocess maintains the high temporal resolution of the features, without\nexcessive computational requirements. We demonstrate that the proposed\nframework achieves state-of-the-art results on two benchmarks, including\nVoxCeleb2 and LRS3, producing speech with notably better naturalness.", "published": "2023-10-30 14:39:34", "link": "http://arxiv.org/abs/2310.19581v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musical Form Generation", "abstract": "While recent generative models can produce engaging music, their utility is\nlimited. The variation in the music is often left to chance, resulting in\ncompositions that lack structure. Pieces extending beyond a minute can become\nincoherent or repetitive. This paper introduces an approach for generating\nstructured, arbitrarily long musical pieces. Central to this approach is the\ncreation of musical segments using a conditional generative model, with\ntransitions between these segments. The generation of prompts that determine\nthe high-level composition is distinct from the creation of finer, lower-level\ndetails. A large language model is then used to suggest the musical form.", "published": "2023-10-30 08:02:08", "link": "http://arxiv.org/abs/2310.19842v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
