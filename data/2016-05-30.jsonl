{"title": "Learning Natural Language Inference using Bidirectional LSTM model and\n  Inner-Attention", "abstract": "In this paper, we proposed a sentence encoding-based model for recognizing\ntext entailment. In our approach, the encoding of sentence is a two-stage\nprocess. Firstly, average pooling was used over word-level bidirectional LSTM\n(biLSTM) to generate a first-stage sentence representation. Secondly, attention\nmechanism was employed to replace average pooling on the same sentence for\nbetter representations. Instead of using target sentence to attend words in\nsource sentence, we utilized the sentence's first-stage representation to\nattend words appeared in itself, which is called \"Inner-Attention\" in our paper\n. Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus\nhas proved the effectiveness of \"Inner-Attention\" mechanism. With less number\nof parameters, our model outperformed the existing best sentence encoding-based\napproach by a large margin.", "published": "2016-05-30 02:47:35", "link": "http://arxiv.org/abs/1605.09090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "abstract": "Understanding how words change their meanings over time is key to models of\nlanguage and cultural evolution, but historical data on meaning is scarce,\nmaking theories hard to develop and test. Word embeddings show promise as a\ndiachronic tool, but have not been carefully evaluated. We develop a robust\nmethodology for quantifying semantic change by evaluating word embeddings\n(PPMI, SVD, word2vec) against known historical changes. We then use this\nmethodology to reveal statistical laws of semantic evolution. Using six\nhistorical corpora spanning four languages and two centuries, we propose two\nquantitative laws of semantic change: (i) the law of conformity---the rate of\nsemantic change scales with an inverse power-law of word frequency; (ii) the\nlaw of innovation---independent of frequency, words that are more polysemous\nhave higher rates of semantic change.", "published": "2016-05-30 03:54:18", "link": "http://arxiv.org/abs/1605.09096v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Multimodality Help Human and Machine for Translation and Image\n  Captioning?", "abstract": "This paper presents the systems developed by LIUM and CVC for the WMT16\nMultimodal Machine Translation challenge. We explored various comparative\nmethods, namely phrase-based systems and attentional recurrent neural networks\nmodels trained using monomodal or multimodal data. We also performed a human\nevaluation in order to estimate the usefulness of multimodal data for human\nmachine translation and image description generation. Our systems obtained the\nbest results for both tasks according to the automatic evaluation metrics BLEU\nand METEOR.", "published": "2016-05-30 11:47:00", "link": "http://arxiv.org/abs/1605.09186v4", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Going Deeper for Multilingual Visual Sentiment Detection", "abstract": "This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset.", "published": "2016-05-30 12:57:44", "link": "http://arxiv.org/abs/1605.09211v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
