{"title": "Milimili. Collecting Parallel Data via Crowdsourcing", "abstract": "We present a methodology for gathering a parallel corpus through\ncrowdsourcing, which is more cost-effective than hiring professional\ntranslators, albeit at the expense of quality. Additionally, we have made\navailable experimental parallel data collected for Chechen-Russian and\nFula-English language pairs.", "published": "2023-07-23 10:23:00", "link": "http://arxiv.org/abs/2307.12282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization", "abstract": "Automatic dialogue summarization is a well-established task with the goal of\ndistilling the most crucial information from human conversations into concise\ntextual summaries. However, most existing research has predominantly focused on\nsummarizing factual information, neglecting the affective content, which can\nhold valuable insights for analyzing, monitoring, or facilitating human\ninteractions. In this paper, we introduce and assess a set of measures\nPSentScore, aimed at quantifying the preservation of affective content in\ndialogue summaries. Our findings indicate that state-of-the-art summarization\nmodels do not preserve well the affective content within their summaries.\nMoreover, we demonstrate that a careful selection of the training set for\ndialogue samples can lead to improved preservation of affective content in the\ngenerated summaries, albeit with a minor reduction in content-related metrics.", "published": "2023-07-23 16:46:01", "link": "http://arxiv.org/abs/2307.12371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Offline RL for Dialogue Response Generation", "abstract": "A common training technique for language models is teacher forcing (TF). TF\nattempts to match human language exactly, even though identical meanings can be\nexpressed in different ways. This motivates use of sequence-level objectives\nfor dialogue response generation. In this paper, we study the efficacy of\nvarious offline reinforcement learning (RL) methods to maximize such\nobjectives. We present a comprehensive evaluation across multiple datasets,\nmodels, and metrics. Offline RL shows a clear performance improvement over\nteacher forcing while not inducing training instability or sacrificing\npractical training budgets.", "published": "2023-07-23 20:43:21", "link": "http://arxiv.org/abs/2307.12425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FATRER: Full-Attention Topic Regularizer for Accurate and Robust\n  Conversational Emotion Recognition", "abstract": "This paper concentrates on the understanding of interlocutors' emotions\nevoked in conversational utterances. Previous studies in this literature mainly\nfocus on more accurate emotional predictions, while ignoring model robustness\nwhen the local context is corrupted by adversarial attacks. To maintain\nrobustness while ensuring accuracy, we propose an emotion recognizer augmented\nby a full-attention topic regularizer, which enables an emotion-related global\nview when modeling the local context in a conversation. A joint topic modeling\nstrategy is introduced to implement regularization from both representation and\nloss perspectives. To avoid over-regularization, we drop the constraints on\nprior distributions that exist in traditional topic modeling and perform\nprobabilistic approximations based entirely on attention alignment. Experiments\nshow that our models obtain more favorable results than state-of-the-art\nmodels, and gain convincing robustness under three types of adversarial\nattacks.", "published": "2023-07-23 04:01:24", "link": "http://arxiv.org/abs/2307.12221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer-based Joint Source Channel Coding for Textual Semantic\n  Communication", "abstract": "The Space-Air-Ground-Sea integrated network calls for more robust and secure\ntransmission techniques against jamming. In this paper, we propose a textual\nsemantic transmission framework for robust transmission, which utilizes the\nadvanced natural language processing techniques to model and encode sentences.\nSpecifically, the textual sentences are firstly split into tokens using\nwordpiece algorithm, and are embedded to token vectors for semantic extraction\nby Transformer-based encoder. The encoded data are quantized to a fixed length\nbinary sequence for transmission, where binary erasure, symmetric, and deletion\nchannels are considered for transmission. The received binary sequences are\nfurther decoded by the transformer decoders into tokens used for sentence\nreconstruction. Our proposed approach leverages the power of neural networks\nand attention mechanism to provide reliable and efficient communication of\ntextual data in challenging wireless environments, and simulation results on\nsemantic similarity and bilingual evaluation understudy prove the superiority\nof the proposed model in semantic transmission.", "published": "2023-07-23 08:42:05", "link": "http://arxiv.org/abs/2307.12266v1", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid\n  Essay in Education", "abstract": "The recent large language models (LLMs), e.g., ChatGPT, have been able to\ngenerate human-like and fluent responses when provided with specific\ninstructions. While admitting the convenience brought by technological\nadvancement, educators also have concerns that students might leverage LLMs to\ncomplete their writing assignments and pass them off as their original work.\nAlthough many AI content detection studies have been conducted as a result of\nsuch concerns, most of these prior studies modeled AI content detection as a\nclassification problem, assuming that a text is either entirely human-written\nor entirely AI-generated. In this study, we investigated AI content detection\nin a rarely explored yet realistic setting where the text to be detected is\ncollaboratively written by human and generative LLMs (i.e., hybrid text). We\nfirst formalized the detection task as identifying the transition points\nbetween human-written content and AI-generated content from a given hybrid text\n(boundary detection). Then we proposed a two-step approach where we (1)\nseparated AI-generated content from human-written content during the encoder\ntraining process; and (2) calculated the distances between every two adjacent\nprototypes and assumed that the boundaries exist between the two adjacent\nprototypes that have the furthest distance from each other. Through extensive\nexperiments, we observed the following main findings: (1) the proposed approach\nconsistently outperformed the baseline methods across different experiment\nsettings; (2) the encoder training process can significantly boost the\nperformance of the proposed approach; (3) when detecting boundaries for\nsingle-boundary hybrid essays, the proposed approach could be enhanced by\nadopting a relatively large prototype size, leading to a 22% improvement in the\nIn-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.", "published": "2023-07-23 08:47:51", "link": "http://arxiv.org/abs/2307.12267v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-CapsNet For Fake News Detection", "abstract": "News consumption has significantly increased with the growing popularity and\nuse of web-based forums and social media. This sets the stage for misinforming\nand confusing people. To help reduce the impact of misinformation on users'\npotential health-related decisions and other intents, it is desired to have\nmachine learning models to detect and combat fake news automatically. This\npaper proposes a novel transformer-based model using Capsule neural\nNetworks(CapsNet) called X-CapsNet. This model includes a CapsNet with dynamic\nrouting algorithm paralyzed with a size-based classifier for detecting short\nand long fake news statements. We use two size-based classifiers, a Deep\nConvolutional Neural Network (DCNN) for detecting long fake news statements and\na Multi-Layer Perceptron (MLP) for detecting short news statements. To resolve\nthe problem of representing short news statements, we use indirect features of\nnews created by concatenating the vector of news speaker profiles and a vector\nof polarity, sentiment, and counting words of news statements. For evaluating\nthe proposed architecture, we use the Covid-19 and the Liar datasets. The\nresults in terms of the F1-score for the Covid-19 dataset and accuracy for the\nLiar dataset show that models perform better than the state-of-the-art\nbaselines.", "published": "2023-07-23 13:58:00", "link": "http://arxiv.org/abs/2307.12332v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning\n  Capabilities of Natural Language Models", "abstract": "Recently, large pretrained language models have achieved compelling\nperformance on commonsense benchmarks. Nevertheless, it is unclear what\ncommonsense knowledge the models learn and whether they solely exploit spurious\npatterns. Feature attributions are popular explainability techniques that\nidentify important input concepts for model outputs. However, commonsense\nknowledge tends to be implicit and rarely explicitly presented in inputs. These\nmethods cannot infer models' implicit reasoning over mentioned concepts. We\npresent CommonsenseVIS, a visual explanatory system that utilizes external\ncommonsense knowledge bases to contextualize model behavior for commonsense\nquestion-answering. Specifically, we extract relevant commonsense knowledge in\ninputs as references to align model behavior with human knowledge. Our system\nfeatures multi-level visualization and interactive model probing and editing\nfor different concepts and their underlying relations. Through a user study, we\nshow that CommonsenseVIS helps NLP experts conduct a systematic and scalable\nvisual analysis of models' relational reasoning over concepts in different\nsituations.", "published": "2023-07-23 17:16:13", "link": "http://arxiv.org/abs/2307.12382v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "HateModerate: Testing Hate Speech Detectors against Content Moderation\n  Policies", "abstract": "To protect users from massive hateful content, existing works studied\nautomated hate speech detection. Despite the existing efforts, one question\nremains: do automated hate speech detectors conform to social media content\npolicies? A platform's content policies are a checklist of content moderated by\nthe social media platform. Because content moderation rules are often uniquely\ndefined, existing hate speech datasets cannot directly answer this question.\n  This work seeks to answer this question by creating HateModerate, a dataset\nfor testing the behaviors of automated content moderators against content\npolicies. First, we engage 28 annotators and GPT in a six-step annotation\nprocess, resulting in a list of hateful and non-hateful test suites matching\neach of Facebook's 41 hate speech policies. Second, we test the performance of\nstate-of-the-art hate speech detectors against HateModerate, revealing\nsubstantial failures these models have in their conformity to the policies.\nThird, using HateModerate, we augment the training data of a top-downloaded\nhate detector on HuggingFace. We observe significant improvement in the models'\nconformity to content policies while having comparable scores on the original\ntest data. Our dataset and code can be found in the attachment.", "published": "2023-07-23 20:08:38", "link": "http://arxiv.org/abs/2307.12418v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Validation of a Zero-Shot Learning Natural Language Processing Tool for\n  Data Abstraction from Unstructured Healthcare Data", "abstract": "Objectives: To describe the development and validation of a zero-shot\nlearning natural language processing (NLP) tool for abstracting data from\nunstructured text contained within PDF documents, such as those found within\nelectronic health records. Materials and Methods: A data abstraction tool based\non the GPT-3.5 model from OpenAI was developed and compared to three physician\nhuman abstractors in terms of time to task completion and accuracy for\nabstracting data on 14 unique variables from a set of 199 de-identified radical\nprostatectomy pathology reports. The reports were processed by the software\ntool in vectorized and scanned formats to establish the impact of optical\ncharacter recognition on data abstraction. The tool was assessed for\nsuperiority for data abstraction speed and non-inferiority for accuracy.\nResults: The human abstractors required a mean of 101s per report for data\nabstraction, with times varying from 15 to 284 s. In comparison, the software\ntool required a mean of 12.8 s to process the vectorized reports and a mean of\n15.8 to process the scanned reports (P < 0.001). The overall accuracies of the\nthree human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of\n2786 datapoints. The software tool had an overall accuracy of 94.2% for the\nvectorized reports, proving to be non-inferior to the human abstractors at a\nmargin of -10% ($\\alpha$=0.025). The tool had a slightly lower accuracy of\n88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3\nhuman abstractors. Conclusion: The developed zero-shot learning NLP tool\naffords researchers comparable levels of accuracy to that of human abstractors,\nwith significant time savings benefits. Because of the lack of need for\ntask-specific model training, the developed tool is highly generalizable and\ncan be used for a wide variety of data abstraction tasks, even outside the\nfield of medicine.", "published": "2023-07-23 17:52:28", "link": "http://arxiv.org/abs/2308.00107v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring the Integration of Speech Separation and Recognition with\n  Self-Supervised Learning Representation", "abstract": "Neural speech separation has made remarkable progress and its integration\nwith automatic speech recognition (ASR) is an important direction towards\nrealizing multi-speaker ASR. This work provides an insightful investigation of\nspeech separation in reverberant and noisy-reverberant scenarios as an ASR\nfront-end. In detail, we explore multi-channel separation methods, mask-based\nbeamforming and complex spectral mapping, as well as the best features to use\nin the ASR back-end model. We employ the recent self-supervised learning\nrepresentation (SSLR) as a feature and improve the recognition performance from\nthe case with filterbank features. To further improve multi-speaker recognition\nperformance, we present a carefully designed training strategy for integrating\nspeech separation and recognition with SSLR. The proposed integration using\nTF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5%\nword error rate in reverberant WHAMR! test set, significantly outperforming an\nexisting mask-based MVDR beamforming and filterbank integration (28.9%).", "published": "2023-07-23 05:39:39", "link": "http://arxiv.org/abs/2307.12231v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A meta learning scheme for fast accent domain expansion in Mandarin\n  speech recognition", "abstract": "Spoken languages show significant variation across mandarin and accent.\nDespite the high performance of mandarin automatic speech recognition (ASR),\naccent ASR is still a challenge task. In this paper, we introduce meta-learning\ntechniques for fast accent domain expansion in mandarin speech recognition,\nwhich expands the field of accents without deteriorating the performance of\nmandarin ASR. Meta-learning or learn-to-learn can learn general relation in\nmulti domains not only for over-fitting a specific domain. So we select\nmeta-learning in the domain expansion task. This more essential learning will\ncause improved performance on accent domain extension tasks. We combine the\nmethods of meta learning and freeze of model parameters, which makes the\nrecognition performance more stable in different cases and the training faster\nabout 20%. Our approach significantly outperforms other methods about 3%\nrelatively in the accent domain expansion task. Compared to the baseline model,\nit improves relatively 37% under the condition that the mandarin test set\nremains unchanged. In addition, it also proved this method to be effective on a\nlarge amount of data with a relative performance improvement of 4% on the\naccent test set.", "published": "2023-07-23 08:23:26", "link": "http://arxiv.org/abs/2307.12262v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences\n  from Longitudinal Electronic Health Records of US Military Veterans", "abstract": "Early prediction of Alzheimer's disease (AD) is crucial for timely\nintervention and treatment. This study aims to use machine learning approaches\nto analyze longitudinal electronic health records (EHRs) of patients with AD\nand identify signs and symptoms that can predict AD onset earlier. We used a\ncase-control design with longitudinal EHRs from the U.S. Department of Veterans\nAffairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA\npatients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9\nwith controls by age, sex and clinical utilization with replacement. We used a\npanel of AD-related keywords and their occurrences over time in a patient's\nlongitudinal EHRs as predictors for AD prediction with four machine learning\nmodels. We performed subgroup analyses by age, sex, and race/ethnicity, and\nvalidated the model in a hold-out and \"unseen\" VHA stations group. Model\ndiscrimination, calibration, and other relevant metrics were reported for\npredictions up to ten years before ICD-based diagnosis. The study population\nincluded 16,701 cases and 39,097 matched controls. The average number of\nAD-related keywords (e.g., \"concentration\", \"speaking\") per year increased\nrapidly for cases as diagnosis approached, from around 10 to over 40, while\nremaining flat at 10 for controls. The best model achieved high discriminative\naccuracy (ROCAUC 0.997) for predictions using data from at least ten years\nbefore ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow\ngoodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and\nrace/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine\nlearning models using AD-related keywords identified from EHR notes can predict\nfuture AD diagnoses, suggesting its potential use for identifying AD risk using\nEHR notes, offering an affordable way for early screening on large population.", "published": "2023-07-23 16:38:16", "link": "http://arxiv.org/abs/2307.12369v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning", "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.", "published": "2023-07-23 16:54:41", "link": "http://arxiv.org/abs/2307.12375v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MyVoice: Arabic Speech Resource Collaboration Platform", "abstract": "We introduce MyVoice, a crowdsourcing platform designed to collect Arabic\nspeech to enhance dialectal speech technologies. This platform offers an\nopportunity to design large dialectal speech datasets; and makes them publicly\navailable. MyVoice allows contributors to select city/country-level\nfine-grained dialect and record the displayed utterances. Users can switch\nroles between contributors and annotators. The platform incorporates a quality\nassurance system that filters out low-quality and spurious recordings before\nsending them for validation. During the validation phase, contributors can\nassess the quality of recordings, annotate them, and provide feedback which is\nthen reviewed by administrators. Furthermore, the platform offers flexibility\nto admin roles to add new data or tasks beyond dialectal speech and word\ncollection, which are displayed to contributors. Thus, enabling collaborative\nefforts in gathering diverse and large Arabic speech data.", "published": "2023-07-23 07:13:30", "link": "http://arxiv.org/abs/2308.02503v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance Comparison Between VoLTE and non-VoLTE Voice Calls During\n  Mobility in Commercial Deployment: A Drive Test-Based Analysis", "abstract": "The optimization of network performance is vital for the delivery of services\nusing standard cellular technologies for mobile communications. Call setup\ndelay and User Equipment (UE) battery savings significantly influence network\nperformance. Improving these factors is vital for ensuring optimal service\ndelivery. In comparison to traditional circuit-switched voice calls, VoLTE\n(Voice over LTE) technology offers faster call setup durations and better\nbattery-saving performance. To validate these claims, a drive test was carried\nout using the XCAL drive test tool to collect real-time network parameter\ndetails in VoLTE and non-VoLTE voice calls. The findings highlight the analysis\nof real-time network characteristics, such as the call setup delay calculation,\nbattery-saving performance, and DRX mechanism. The study contributes to the\nunderstanding of network optimization strategies and provides insights for\nenhancing the quality of service (QoS) in mobile communication networks.\nExamining VoLTE and non-VoLTE operations, this research highlights the\nsubstantial energy savings obtained by VoLTE. Specifically, VoLTE saves\napproximately 60.76% of energy before the Service Request and approximately\n38.97% of energy after the Service Request. Moreover, VoLTE to VoLTE calls have\na 72.6% faster call setup delay than non-VoLTE-based LTE to LTE calls, because\nof fewer signaling messages required. Furthermore, as compared to non-VoLTE to\nnon-VoLTE calls, VoLTE to non-VoLTE calls offer an 18.6% faster call setup\ndelay. These results showcase the performance advantages of VoLTE and reinforce\nits potential for offering better services in wireless communication networks.", "published": "2023-07-23 18:22:51", "link": "http://arxiv.org/abs/2307.12397v1", "categories": ["cs.NI", "eess.AS"], "primary_category": "cs.NI"}
{"title": "Adversarial Agents For Attacking Inaudible Voice Activated Devices", "abstract": "The paper applies reinforcement learning to novel Internet of Thing\nconfigurations. Our analysis of inaudible attacks on voice-activated devices\nconfirms the alarming risk factor of 7.6 out of 10, underlining significant\nsecurity vulnerabilities scored independently by NIST National Vulnerability\nDatabase (NVD). Our baseline network model showcases a scenario in which an\nattacker uses inaudible voice commands to gain unauthorized access to\nconfidential information on a secured laptop. We simulated many attack\nscenarios on this baseline network model, revealing the potential for mass\nexploitation of interconnected devices to discover and own privileged\ninformation through physical access without adding new hardware or amplifying\ndevice skills. Using Microsoft's CyberBattleSim framework, we evaluated six\nreinforcement learning algorithms and found that Deep-Q learning with\nexploitation proved optimal, leading to rapid ownership of all nodes in fewer\nsteps. Our findings underscore the critical need for understanding\nnon-conventional networks and new cybersecurity measures in an ever-expanding\ndigital landscape, particularly those characterized by mobile devices, voice\nactivation, and non-linear microphones susceptible to malicious actors\noperating stealth attacks in the near-ultrasound or inaudible ranges. By 2024,\nthis new attack surface might encompass more digital voice assistants than\npeople on the planet yet offer fewer remedies than conventional patching or\nfirmware fixes since the inaudible attacks arise inherently from the microphone\ndesign and digital signal processing.", "published": "2023-07-23 02:18:30", "link": "http://arxiv.org/abs/2307.12204v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency\n  of Full-band Magnitude and Phase", "abstract": "We propose an optimization-based method for reconstructing a time-domain\nsignal from a low-dimensional spectral representation such as a\nmel-spectrogram. Phase reconstruction has been studied to reconstruct a\ntime-domain signal from the full-band short-time Fourier transform (STFT)\nmagnitude. The Griffin-Lim algorithm (GLA) has been widely used because it\nrelies only on the redundancy of STFT and is applicable to various audio\nsignals. In this paper, we jointly reconstruct the full-band magnitude and\nphase by considering the bi-level relationships among the time-domain signal,\nits STFT coefficients, and its mel-spectrogram. The proposed method is\nformulated as a rigorous optimization problem and estimates the full-band\nmagnitude based on the criterion used in GLA. Our experiments demonstrate the\neffectiveness of the proposed method on speech, music, and environmental\nsignals.", "published": "2023-07-23 05:41:32", "link": "http://arxiv.org/abs/2307.12232v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Learning for Audio-Based Emotion Recognition", "abstract": "Emotion recognition models using audio input data can enable the development\nof interactive systems with applications in mental healthcare, marketing,\ngaming, and social media analysis. While the field of affective computing using\naudio data is rich, a major barrier to achieve consistently high-performance\nmodels is the paucity of available training labels. Self-supervised learning\n(SSL) is a family of methods which can learn despite a scarcity of supervised\nlabels by predicting properties of the data itself. To understand the utility\nof self-supervised learning for audio-based emotion recognition, we have\napplied self-supervised learning pre-training to the classification of emotions\nfrom the CMU- MOSEI's acoustic modality. Unlike prior papers that have\nexperimented with raw acoustic data, our technique has been applied to encoded\nacoustic data. Our model is first pretrained to uncover the randomly-masked\ntimestamps of the acoustic data. The pre-trained model is then fine-tuned using\na small sample of annotated data. The performance of the final model is then\nevaluated via several evaluation metrics against a baseline deep learning model\nwith an identical backbone architecture. We find that self-supervised learning\nconsistently improves the performance of the model across all metrics. This\nwork shows the utility of self-supervised learning for affective computing,\ndemonstrating that self-supervised learning is most useful when the number of\ntraining examples is small, and that the effect is most pronounced for emotions\nwhich are easier to classify such as happy, sad and anger. This work further\ndemonstrates that self-supervised learning works when applied to embedded\nfeature representations rather than the traditional approach of pre-training on\nthe raw input space.", "published": "2023-07-23 14:40:50", "link": "http://arxiv.org/abs/2307.12343v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic\n  Spaces", "abstract": "Numerous examples in the literature proved that deep learning models have the\nability to work well with multimodal data. Recently, CLIP has enabled deep\nlearning systems to learn shared latent spaces between images and text\ndescriptions, with outstanding zero- or few-shot results in downstream tasks.\nIn this paper we explore the same idea proposed by CLIP but applied to the\nspeech domain, where the phonetic and acoustic spaces usually coexist. We train\na CLIP-based model with the aim to learn shared representations of phonetic and\nacoustic spaces. The results show that the proposed model is sensible to\nphonetic changes, with a 91% of score drops when replacing 20% of the phonemes\nat random, while providing substantial robustness against different kinds of\nnoise, with a 10% performance drop when mixing the audio with 75% of Gaussian\nnoise. We also provide empirical evidence showing that the resulting embeddings\nare useful for a variety of downstream applications, such as intelligibility\nevaluation and the ability to leverage rich pre-trained phonetic embeddings in\nspeech generation task. Finally, we discuss potential applications with\ninteresting implications for the speech generation and recognition fields.", "published": "2023-07-23 22:18:47", "link": "http://arxiv.org/abs/2307.12445v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Backdoor Attacks against Voice Recognition Systems: A Survey", "abstract": "Voice Recognition Systems (VRSs) employ deep learning for speech recognition\nand speaker recognition. They have been widely deployed in various real-world\napplications, from intelligent voice assistance to telephony surveillance and\nbiometric authentication. However, prior research has revealed the\nvulnerability of VRSs to backdoor attacks, which pose a significant threat to\nthe security and privacy of VRSs. Unfortunately, existing literature lacks a\nthorough review on this topic. This paper fills this research gap by conducting\na comprehensive survey on backdoor attacks against VRSs. We first present an\noverview of VRSs and backdoor attacks, elucidating their basic knowledge. Then\nwe propose a set of evaluation criteria to assess the performance of backdoor\nattack methods. Next, we present a comprehensive taxonomy of backdoor attacks\nagainst VRSs from different perspectives and analyze the characteristic of\ndifferent categories. After that, we comprehensively review existing attack\nmethods and analyze their pros and cons based on the proposed criteria.\nFurthermore, we review classic backdoor defense methods and generic audio\ndefense techniques. Then we discuss the feasibility of deploying them on VRSs.\nFinally, we figure out several open issues and further suggest future research\ndirections to motivate the research of VRSs security.", "published": "2023-07-23 02:29:35", "link": "http://arxiv.org/abs/2307.13643v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
