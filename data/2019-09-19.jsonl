{"title": "Low-Resource Parsing with Crosslingual Contextualized Representations", "abstract": "Despite advances in dependency parsing, languages with small treebanks still\npresent challenges. We assess recent approaches to multilingual contextual word\nrepresentations (CWRs), and compare them for crosslingual transfer from a\nlanguage with a large treebank to a language with a small or nonexistent\ntreebank, by sharing parameters between languages in the parser itself. We\nexperiment with a diverse selection of languages in both simulated and truly\nlow-resource scenarios, and show that multilingual CWRs greatly facilitate\nlow-resource dependency parsing even without crosslingual supervision such as\ndictionaries or parallel text. Furthermore, we examine the non-contextual part\nof the learned language models (which we call a \"decontextual probe\") to\ndemonstrate that polyglot language models better encode crosslingual lexical\ncorrespondence compared to aligned monolingual language models. This analysis\nprovides further evidence that polyglot training is an effective approach to\ncrosslingual transfer.", "published": "2019-09-19 00:23:09", "link": "http://arxiv.org/abs/1909.08744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Made for Each Other: Broad-coverage Semantic Structures Meet Preposition\n  Supersenses", "abstract": "Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013)\nis a typologically-informed, broad-coverage semantic annotation scheme that\ndescribes coarse-grained predicate-argument structure but currently lacks\nsemantic roles. We argue that lexicon-free annotation of the semantic roles\nmarked by prepositions, as formulated by Schneider et al. (2018b), is\ncomplementary and suitable for integration within UCCA. We show empirically for\nEnglish that the schemes, though annotated independently, are compatible and\ncan be combined in a single semantic graph. A comparison of several approaches\nto parsing the integrated representation lays the groundwork for future\nresearch on this task.", "published": "2019-09-19 04:15:02", "link": "http://arxiv.org/abs/1909.08796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Write Summaries with Patterns? Learning towards Abstractive\n  Summarization through Prototype Editing", "abstract": "Under special circumstances, summaries should conform to a particular style\nwith patterns, such as court judgments and abstracts in academic papers. To\nthis end, the prototype document-summary pairs can be utilized to generate\nbetter summaries. There are two main challenges in this task: (1) the model\nneeds to incorporate learned patterns from the prototype, but (2) should avoid\ncopying contents other than the patternized words---such as irrelevant\nfacts---into the generated summaries. To tackle these challenges, we design a\nmodel named Prototype Editing based Summary Generator (PESG). PESG first learns\nsummary patterns and prototype facts by analyzing the correlation between a\nprototype document and its summary. Prototype facts are then utilized to help\nextract facts from the input document. Next, an editing generator generates new\nsummary based on the summary pattern or extracted facts. Finally, to address\nthe second challenge, a fact checker is used to estimate mutual information\nbetween the input document and generated summary, providing an additional\nsignal for the generator. Extensive experiments conducted on a large-scale\nreal-world text summarization dataset show that PESG achieves the\nstate-of-the-art performance in terms of both automatic metrics and human\nevaluations.", "published": "2019-09-19 07:36:54", "link": "http://arxiv.org/abs/1909.08837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Edit-centric Approach for Wikipedia Article Quality Assessment", "abstract": "We propose an edit-centric approach to assess Wikipedia article quality as a\ncomplementary alternative to current full document-based techniques. Our model\nconsists of a main classifier equipped with an auxiliary generative module\nwhich, for a given edit, jointly provides an estimation of its quality and\ngenerates a description in natural language. We performed an empirical study to\nassess the feasibility of the proposed model and its cost-effectiveness in\nterms of data and quality requirements.", "published": "2019-09-19 09:20:32", "link": "http://arxiv.org/abs/1909.08880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Generalization by Incorporating Coverage in Natural Language\n  Inference", "abstract": "The task of natural language inference (NLI) is to identify the relation\nbetween the given premise and hypothesis. While recent NLI models achieve very\nhigh performance on individual datasets, they fail to generalize across similar\ndatasets. This indicates that they are solving NLI datasets instead of the task\nitself. In order to improve generalization, we propose to extend the input\nrepresentations with an abstract view of the relation between the hypothesis\nand the premise, i.e., how well the individual words, or word n-grams, of the\nhypothesis are covered by the premise. Our experiments show that the use of\nthis information considerably improves generalization across different NLI\ndatasets without requiring any external knowledge or additional data. Finally,\nwe show that using the coverage information is not only beneficial for\nimproving the performance across different datasets of the same task. The\nresulting generalization improves the performance across datasets that belong\nto similar but not the same tasks.", "published": "2019-09-19 12:38:22", "link": "http://arxiv.org/abs/1909.08940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUN through the Streets: A New Dataset and Baseline Models for Realistic\n  Urban Navigation", "abstract": "Following navigation instructions in natural language requires a composition\nof language, action, and knowledge of the environment. Knowledge of the\nenvironment may be provided via visual sensors or as a symbolic world\nrepresentation referred to as a map. Here we introduce the Realistic Urban\nNavigation (RUN) task, aimed at interpreting navigation instructions based on a\nreal, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of\n2515 instructions aligned with actual routes over three regions of Manhattan.\nWe propose a strong baseline for the task and empirically investigate which\naspects of the neural architecture are important for the RUN success. Our\nresults empirically show that entity abstraction, attention over words and\nworlds, and a constantly updating world-state, significantly contribute to task\naccuracy.", "published": "2019-09-19 13:21:05", "link": "http://arxiv.org/abs/1909.08970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CogniVal: A Framework for Cognitive Word Embedding Evaluation", "abstract": "An interesting method of evaluating word representations is by how much they\nreflect the semantic representations in the human brain. However, most, if not\nall, previous works only focus on small datasets and a single modality. In this\npaper, we present the first multi-modal framework for evaluating English word\nrepresentations based on cognitive lexical semantics. Six types of word\nembeddings are evaluated by fitting them to 15 datasets of eye-tracking, EEG\nand fMRI signals recorded during language processing. To achieve a global score\nover all evaluation hypotheses, we apply statistical significance testing\naccounting for the multiple comparisons problem. This framework is easily\nextensible and available to include other intrinsic and extrinsic evaluation\nmethods. We find strong correlations in the results between cognitive datasets,\nacross recording modalities and to their performance on extrinsic NLP tasks.", "published": "2019-09-19 13:57:17", "link": "http://arxiv.org/abs/1909.09001v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Random Gossip BMUF Process for Neural Language Modeling", "abstract": "Neural network language model (NNLM) is an essential component of industrial\nASR systems. One important challenge of training an NNLM is to leverage between\nscaling the learning process and handling big data. Conventional approaches\nsuch as block momentum provides a blockwise model update filtering (BMUF)\nprocess and achieves almost linear speedups with no performance degradation for\nspeech recognition. However, it needs to calculate the model average from all\ncomputing nodes (e.g., GPUs) and when the number of computing nodes is large,\nthe learning suffers from the severe communication latency. As a consequence,\nBMUF is not suitable under restricted network conditions. In this paper, we\npresent a decentralized BMUF process, in which the model is split into\ndifferent components, each of which is updated by communicating to some\nrandomly chosen neighbor nodes with the same component, followed by a BMUF-like\nprocess. We apply this method to several LSTM language modeling tasks.\nExperimental results show that our approach achieves consistently better\nperformance than conventional BMUF. In particular, we obtain a lower perplexity\nthan the single-GPU baseline on the wiki-text-103 benchmark using 4 GPUs. In\naddition, no performance degradation is observed when scaling to 8 and 16 GPUs.", "published": "2019-09-19 14:11:36", "link": "http://arxiv.org/abs/1909.09010v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argumentative Relation Classification as Plausibility Ranking", "abstract": "We formulate argumentative relation classification (support vs. attack) as a\ntext-plausibility ranking task. To this aim, we propose a simple reconstruction\ntrick which enables us to build minimal pairs of plausible and implausible\ntexts by simulating natural contexts in which two argumentative units are\nlikely or unlikely to appear. We show that this method is competitive with\nprevious work albeit it is considerably simpler. In a recently introduced\ncontent-based version of the task, where contextual discourse clues are hidden,\nthe approach offers a performance increase of more than 10% macro F1. With\nrespect to the scarce attack-class, the method achieves a large increase in\nprecision while the incurred loss in recall is small or even nonexistent.", "published": "2019-09-19 15:01:46", "link": "http://arxiv.org/abs/1909.09031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Corpus for Automatic Readability Assessment and Text Simplification of\n  German", "abstract": "In this paper, we present a corpus for use in automatic readability\nassessment and automatic text simplification of German. The corpus is compiled\nfrom web sources and consists of approximately 211,000 sentences. As a novel\ncontribution, it contains information on text structure, typography, and\nimages, which can be exploited as part of machine learning approaches to\nreadability assessment and text simplification. The focus of this publication\nis on representing such information as an extension to an existing corpus\nstandard.", "published": "2019-09-19 16:07:32", "link": "http://arxiv.org/abs/1909.09067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Variational Neural Machine Translation by Promoting Mutual\n  Information", "abstract": "Posterior collapse plagues VAEs for text, especially for conditional text\ngeneration with strong autoregressive decoders. In this work, we address this\nproblem in variational neural machine translation by explicitly promoting\nmutual information between the latent variables and the data. Our model extends\nthe conditional variational autoencoder (CVAE) with two new ingredients: first,\nwe propose a modified evidence lower bound (ELBO) objective which explicitly\npromotes mutual information; second, we regularize the probabilities of the\ndecoder by mixing an auxiliary factorized distribution which is directly\npredicted by the latent variables. We present empirical results on the\nTransformer architecture and show the proposed model effectively addressed\nposterior collapse: latent variables are no longer ignored in the presence of\npowerful decoder. As a result, the proposed model yields improved translation\nquality while demonstrating superior performance in terms of data efficiency\nand robustness.", "published": "2019-09-19 21:16:29", "link": "http://arxiv.org/abs/1909.09237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question\n  Answering", "abstract": "Multi-hop textual question answering requires combining information from\nmultiple sentences. We focus on a natural setting where, unlike typical reading\ncomprehension, only partial information is provided with each question. The\nmodel must retrieve and use additional knowledge to correctly answer the\nquestion. To tackle this challenge, we develop a novel approach that explicitly\nidentifies the knowledge gap between a key span in the provided knowledge and\nthe answer choices. The model, GapQA, learns to fill this gap by determining\nthe relationship between the span and an answer choice, based on retrieved\nknowledge targeting this gap. We propose jointly training a model to\nsimultaneously fill this knowledge gap and compose it with the provided partial\nknowledge. On the OpenBookQA dataset, given partial knowledge, explicitly\nidentifying what's missing substantially outperforms previous approaches.", "published": "2019-09-19 22:49:04", "link": "http://arxiv.org/abs/1909.09253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Philosophical Statements using Interpolated Markov Models and\n  Dynamic Templates", "abstract": "Automatically imitating input text is a common task in natural language\ngeneration, often used to create humorous results. Classic algorithms for\nlearning to imitate text, e.g. simple Markov chains, usually have a trade-off\nbetween originality and syntactic correctness. We present two ways of\nautomatically parodying philosophical statements from examples overcoming this\nissue, and show how these can work in interactive systems as well. The first\nalgorithm uses interpolated Markov models with extensions to improve the\nquality of the generated texts. For the second algorithm, we propose\ndynamically extracting templates and filling these with new content. To\nillustrate these algorithms, we implemented TorfsBot, a Twitterbot imitating\nthe witty, semi-philosophical tweets of professor Rik Torfs, the previous KU\nLeuven rector. We found that users preferred generative models that focused on\nlocal coherent sentences, rather than those mimicking the global structure of a\nphilosophical statement. The proposed algorithms are thus valuable new tools\nfor automatic parody as well as template learning systems.", "published": "2019-09-19 11:37:12", "link": "http://arxiv.org/abs/1909.09480v1", "categories": ["cs.CL", "68T05"], "primary_category": "cs.CL"}
{"title": "Multi-sense Definition Modeling using Word Sense Decompositions", "abstract": "Word embeddings capture syntactic and semantic information about words.\nDefinition modeling aims to make the semantic content in each embedding\nexplicit, by outputting a natural language definition based on the embedding.\nHowever, existing definition models are limited in their ability to generate\naccurate definitions for different senses of the same word. In this paper, we\nintroduce a new method that enables definition modeling for multiple senses. We\nshow how a Gumble-Softmax approach outperforms baselines at matching\nsense-specific embeddings to definitions during training. In experiments, our\nmulti-sense definition model improves recall over a state-of-the-art\nsingle-sense definition model by a factor of three, without harming precision.", "published": "2019-09-19 01:34:52", "link": "http://arxiv.org/abs/1909.09483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Collective Attention via Descriptor Context: A Case Study\n  of Public Discussions of Crisis Events", "abstract": "Social media datasets make it possible to rapidly quantify collective\nattention to emerging topics and breaking news, such as crisis events.\nCollective attention is typically measured by aggregate counts, such as the\nnumber of posts that mention a name or hashtag. But according to rationalist\nmodels of natural language communication, the collective salience of each\nentity will be expressed not only in how often it is mentioned, but in the form\nthat those mentions take. This is because natural language communication is\npremised on (and customized to) the expectations that speakers and writers have\nabout how their messages will be interpreted by the intended audience. We test\nthis idea by conducting a large-scale analysis of public online discussions of\nbreaking news events on Facebook and Twitter, focusing on five recent crisis\nevents. We examine how people refer to locations, focusing specifically on\ncontextual descriptors, such as \"San Juan\" versus \"San Juan, Puerto Rico.\"\nRationalist accounts of natural language communication predict that such\ndescriptors will be unnecessary (and therefore omitted) when the named entity\nis expected to have high prior salience to the reader. We find that the use of\ncontextual descriptors is indeed associated with proxies for social and\ninformational expectations, including macro-level factors like the location's\nglobal salience and micro-level factors like audience engagement. We also find\na consistent decrease in descriptor context use over the lifespan of each\ncrisis event. These findings provide evidence about how social media users\ncommunicate with their audiences, and point towards more fine-grained models of\ncollective attention that may help researchers and crisis response\norganizations to better understand public perception of unfolding crisis\nevents.", "published": "2019-09-19 02:56:51", "link": "http://arxiv.org/abs/1909.08784v3", "categories": ["cs.CL", "cs.SI", "H.5.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "abstract": "This paper addresses the problem of comprehending procedural commonsense\nknowledge. This is a challenging task as it requires identifying key entities,\nkeeping track of their state changes, and understanding temporal and causal\nrelations. Contrary to most of the previous work, in this study, we do not rely\non strong inductive bias and explore the question of how multimodality can be\nexploited to provide a complementary semantic signal. Towards this end, we\nintroduce a new entity-aware neural comprehension model augmented with external\nrelational memory units. Our model learns to dynamically update entity states\nin relation to each other while reading the text instructions. Our experimental\nanalysis on the visual reasoning tasks in the recently proposed RecipeQA\ndataset reveals that our approach improves the accuracy of the previously\nreported models by a large margin. Moreover, we find that our model learns\neffective dynamic representations of entities even though we do not use any\nsupervision at the level of entity states.", "published": "2019-09-19 08:39:00", "link": "http://arxiv.org/abs/1909.08859v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Split-and-Recombine Approach for Follow-up Query Analysis", "abstract": "Context-dependent semantic parsing has proven to be an important yet\nchallenging task. To leverage the advances in context-independent semantic\nparsing, we propose to perform follow-up query analysis, aiming to restate\ncontext-dependent natural language queries with contextual information. To\naccomplish the task, we propose STAR, a novel approach with a well-designed\ntwo-phase process. It is parser-independent and able to handle multifarious\nfollow-up scenarios in different domains. Experiments on the FollowUp dataset\nshow that STAR outperforms the state-of-the-art baseline by a large margin of\nnearly 8%. The superiority on parsing results verifies the feasibility of\nfollow-up query analysis. We also explore the extensibility of STAR on the SQA\ndataset, which is very promising.", "published": "2019-09-19 10:16:21", "link": "http://arxiv.org/abs/1909.08905v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Conceptual Knowledge from Natural Language Text Using Maximum\n  Likelihood Principle", "abstract": "Domain-specific knowledge graphs constructed from natural language text are\nubiquitous in today's world. In many such scenarios the base text, from which\nthe knowledge graph is constructed, concerns itself with practical, on-hand,\nactual or ground-reality information about the domain. Product documentation in\nsoftware engineering domain are one example of such base texts. Other examples\ninclude blogs and texts related to digital artifacts, reports on emerging\nmarkets and business models, patient medical records, etc. Though the above\nsources contain a wealth of knowledge about their respective domains, the\nconceptual knowledge on which they are based is often missing or unclear.\nAccess to this conceptual knowledge can enormously increase the utility of\navailable data and assist in several tasks such as knowledge graph completion,\ngrounding, querying, etc.\n  Our contributions in this paper are twofold. First, we propose a novel\nMarkovian stochastic model for document generation from conceptual knowledge.\nThe uniqueness of our approach lies in the fact that the conceptual knowledge\nin the writer's mind forms a component of the parameter set of our stochastic\nmodel. Secondly, we solve the inverse problem of learning the best conceptual\nknowledge from a given document, by finding model parameters which maximize the\nlikelihood of generating the specific document over all possible parameter\nvalues. This likelihood maximization is done using an application of Baum-Welch\nalgorithm, which is a known special case of Expectation-Maximization (EM)\nalgorithm. We run our conceptualization algorithm on several well-known natural\nlanguage sources and obtain very encouraging results. The results of our\nextensive experiments concur with the hypothesis that the information contained\nin these sources has a well-defined and rigorous underlying conceptual\nstructure, which can be discovered using our method.", "published": "2019-09-19 11:54:55", "link": "http://arxiv.org/abs/1909.08927v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptively Aligned Image Captioning via Adaptive Attention Time", "abstract": "Recent neural models for image captioning usually employ an encoder-decoder\nframework with an attention mechanism. However, the attention mechanism in such\na framework aligns one single (attended) image feature vector to one caption\nword, assuming one-to-one mapping from source image regions and target caption\nwords, which is never possible. In this paper, we propose a novel attention\nmodel, namely Adaptive Attention Time (AAT), to align the source and the target\nadaptively for image captioning. AAT allows the framework to learn how many\nattention steps to take to output a caption word at each decoding step. With\nAAT, an image region can be mapped to an arbitrary number of caption words\nwhile a caption word can also attend to an arbitrary number of image regions.\nAAT is deterministic and differentiable, and doesn't introduce any noise to the\nparameter gradients. In this paper, we empirically show that AAT improves over\nstate-of-the-art methods on the task of image captioning. Code is available at\nhttps://github.com/husthuaan/AAT.", "published": "2019-09-19 15:59:33", "link": "http://arxiv.org/abs/1909.09060v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Goal-Embedded Dual Hierarchical Model for Task-Oriented Dialogue\n  Generation", "abstract": "Hierarchical neural networks are often used to model inherent structures\nwithin dialogues. For goal-oriented dialogues, these models miss a mechanism\nadhering to the goals and neglect the distinct conversational patterns between\ntwo interlocutors. In this work, we propose Goal-Embedded Dual Hierarchical\nAttentional Encoder-Decoder (G-DuHA) able to center around goals and capture\ninterlocutor-level disparity while modeling goal-oriented dialogues.\nExperiments on dialogue generation, response generation, and human evaluations\ndemonstrate that the proposed model successfully generates higher-quality, more\ndiverse and goal-centric dialogues. Moreover, we apply data augmentation via\ngoal-oriented dialogue generation for task-oriented dialog systems with better\nperformance achieved.", "published": "2019-09-19 20:12:10", "link": "http://arxiv.org/abs/1909.09220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models", "abstract": "Neural NLP models are increasingly accurate but are imperfect and\nopaque---they break in counterintuitive ways and leave end users puzzled at\ntheir behavior. Model interpretation methods ameliorate this opacity by\nproviding explanations for specific model predictions. Unfortunately, existing\ninterpretation codebases make it difficult to apply these methods to new models\nand tasks, which hinders adoption for practitioners and burdens\ninterpretability researchers. We introduce AllenNLP Interpret, a flexible\nframework for interpreting NLP models. The toolkit provides interpretation\nprimitives (e.g., input gradients) for any AllenNLP model and task, a suite of\nbuilt-in interpretation methods, and a library of front-end visualization\ncomponents. We demonstrate the toolkit's flexibility and utility by\nimplementing live demos for five interpretation methods (e.g., saliency maps\nand adversarial attacks) on a variety of models and tasks (e.g., masked\nlanguage modeling using BERT and reading comprehension using BiDAF). These\ndemos, alongside our code and tutorials, are available at\nhttps://allennlp.org/interpret .", "published": "2019-09-19 22:35:36", "link": "http://arxiv.org/abs/1909.09251v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summary Level Training of Sentence Rewriting for Abstractive\n  Summarization", "abstract": "As an attempt to combine extractive and abstractive summarization, Sentence\nRewriting models adopt the strategy of extracting salient sentences from a\ndocument first and then paraphrasing the selected ones to generate a summary.\nHowever, the existing models in this framework mostly rely on sentence-level\nrewards or suboptimal labels, causing a mismatch between a training objective\nand evaluation metric. In this paper, we present a novel training signal that\ndirectly maximizes summary-level ROUGE scores through reinforcement learning.\nIn addition, we incorporate BERT into our model, making good use of its ability\non natural language understanding. In extensive experiments, we show that a\ncombination of our proposed model and training procedure obtains new\nstate-of-the-art performance on both CNN/Daily Mail and New York Times\ndatasets. We also demonstrate that it generalizes better on DUC-2002 test set.", "published": "2019-09-19 00:47:13", "link": "http://arxiv.org/abs/1909.08752v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-scale representation learning from visually grounded untranscribed\n  speech", "abstract": "Systems that can associate images with their spoken audio captions are an\nimportant step towards visually grounded language learning. We describe a\nscalable method to automatically generate diverse audio for image captioning\ndatasets. This supports pretraining deep networks for encoding both audio and\nimages, which we do via a dual encoder that learns to align latent\nrepresentations from both modalities. We show that a masked margin softmax loss\nfor such models is superior to the standard triplet loss. We fine-tune these\nmodels on the Flickr8k Audio Captions Corpus and obtain state-of-the-art\nresults---improving recall in the top 10 from 29.6% to 49.5%. We also obtain\nhuman ratings on retrieval outputs to better assess the impact of incidentally\nmatching image-caption pairs that were not associated in the data, finding that\nautomatic evaluation substantially underestimates the quality of the retrieved\nresults.", "published": "2019-09-19 02:50:23", "link": "http://arxiv.org/abs/1909.08782v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Modeling Event Background for If-Then Commonsense Reasoning Using\n  Context-aware Variational Autoencoder", "abstract": "Understanding event and event-centered commonsense reasoning are crucial for\nnatural language processing (NLP). Given an observed event, it is trivial for\nhuman to infer its intents and effects, while this type of If-Then reasoning\nstill remains challenging for NLP systems. To facilitate this, a If-Then\ncommonsense reasoning dataset Atomic is proposed, together with an RNN-based\nSeq2Seq model to conduct such reasoning. However, two fundamental problems\nstill need to be addressed: first, the intents of an event may be multiple,\nwhile the generations of RNN-based Seq2Seq models are always semantically\nclose; second, external knowledge of the event background may be necessary for\nunderstanding events and conducting the If-Then reasoning. To address these\nissues, we propose a novel context-aware variational autoencoder effectively\nlearning event background information to guide the If-Then reasoning.\nExperimental results show that our approach improves the accuracy and diversity\nof inferences compared with state-of-the-art baseline methods.", "published": "2019-09-19 06:46:02", "link": "http://arxiv.org/abs/1909.08824v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Additional Knowledge can Improve Natural Language Commonsense\n  Question Answering?", "abstract": "Recently several datasets have been proposed to encourage research in\nQuestion Answering domains where commonsense knowledge is expected to play an\nimportant role. Recent language models such as ROBERTA, BERT and GPT that have\nbeen pre-trained on Wikipedia articles and books have shown reasonable\nperformance with little fine-tuning on several such Multiple Choice\nQuestion-Answering (MCQ) datasets. Our goal in this work is to develop methods\nto incorporate additional (commonsense) knowledge into language model-based\napproaches for better question-answering in such domains. In this work, we\nfirst categorize external knowledge sources, and show performance does improve\non using such sources. We then explore three different strategies for knowledge\nincorporation and four different models for question-answering using external\ncommonsense knowledge. We analyze our predictions to explore the scope of\nfurther improvements.", "published": "2019-09-19 08:25:47", "link": "http://arxiv.org/abs/1909.08855v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASU at TextGraphs 2019 Shared Task: Explanation ReGeneration using\n  Language Models and Iterative Re-Ranking", "abstract": "In this work we describe the system from Natural Language Processing group at\nArizona State University for the TextGraphs 2019 Shared Task. The task focuses\non Explanation Regeneration, an intermediate step towards general multi-hop\ninference on large graphs. Our approach consists of modeling the explanation\nregeneration task as a \\textit{learning to rank} problem, for which we use\nstate-of-the-art language models and explore dataset preparation techniques. We\nutilize an iterative re-ranking based approach to further improve the rankings.\nOur system secured 2nd rank in the task with a mean average precision (MAP) of\n41.3\\% on the test set.", "published": "2019-09-19 08:47:16", "link": "http://arxiv.org/abs/1909.08863v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysing Neural Language Models: Contextual Decomposition Reveals\n  Default Reasoning in Number and Gender Assignment", "abstract": "Extensive research has recently shown that recurrent neural language models\nare able to process a wide range of grammatical phenomena. How these models are\nable to perform these remarkable feats so well, however, is still an open\nquestion. To gain more insight into what information LSTMs base their decisions\non, we propose a generalisation of Contextual Decomposition (GCD). In\nparticular, this setup enables us to accurately distil which part of a\nprediction stems from semantic heuristics, which part truly emanates from\nsyntactic cues and which part arise from the model biases themselves instead.\nWe investigate this technique on tasks pertaining to syntactic agreement and\nco-reference resolution and discover that the model strongly relies on a\ndefault reasoning effect to perform these tasks.", "published": "2019-09-19 13:24:29", "link": "http://arxiv.org/abs/1909.08975v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Look, Read and Enrich. Learning from Scientific Figures and their\n  Captions", "abstract": "Compared to natural images, understanding scientific figures is particularly\nhard for machines. However, there is a valuable source of information in\nscientific literature that until now has remained untapped: the correspondence\nbetween a figure and its caption. In this paper we investigate what can be\nlearnt by looking at a large number of figures and reading their captions, and\nintroduce a figure-caption correspondence learning task that makes use of our\nobservations. Training visual and language networks without supervision other\nthan pairs of unconstrained figures and captions is shown to successfully solve\nthis task. We also show that transferring lexical and semantic knowledge from a\nknowledge graph significantly enriches the resulting features. Finally, we\ndemonstrate the positive impact of such features in other tasks involving\nscientific text and figures, like multi-modal classification and machine\ncomprehension for question answering, outperforming supervised baselines and\nad-hoc approaches.", "published": "2019-09-19 16:10:15", "link": "http://arxiv.org/abs/1909.09070v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Self-Training for End-to-End Speech Recognition", "abstract": "We revisit self-training in the context of end-to-end speech recognition. We\ndemonstrate that training with pseudo-labels can substantially improve the\naccuracy of a baseline model. Key to our approach are a strong baseline\nacoustic and language model used to generate the pseudo-labels, filtering\nmechanisms tailored to common errors from sequence-to-sequence models, and a\nnovel ensemble approach to increase pseudo-label diversity. Experiments on the\nLibriSpeech corpus show that with an ensemble of four models and label\nfiltering, self-training yields a 33.9% relative improvement in WER compared\nwith a baseline trained on 100 hours of labelled data in the noisy speech\nsetting. In the clean speech setting, self-training recovers 59.3% of the gap\nbetween the baseline and an oracle model, which is at least 93.8% relatively\nhigher than what previous approaches can achieve.", "published": "2019-09-19 17:42:56", "link": "http://arxiv.org/abs/1909.09116v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Sparse Mixture of Experts for Visual Question Answering", "abstract": "There has been a rapid progress in the task of Visual Question Answering with\nimproved model architectures. Unfortunately, these models are usually\ncomputationally intensive due to their sheer size which poses a serious\nchallenge for deployment. We aim to tackle this issue for the specific task of\nVisual Question Answering (VQA). A Convolutional Neural Network (CNN) is an\nintegral part of the visual processing pipeline of a VQA model (assuming the\nCNN is trained along with entire VQA model). In this project, we propose an\nefficient and modular neural architecture for the VQA task with focus on the\nCNN module. Our experiments demonstrate that a sparsely activated CNN based VQA\nmodel achieves comparable performance to a standard CNN based VQA model\narchitecture.", "published": "2019-09-19 18:55:54", "link": "http://arxiv.org/abs/1909.09192v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Contextualized Pairwise Semantic Similarity for Arabic Language\n  Questions", "abstract": "Question semantic similarity is a challenging and active research problem\nthat is very useful in many NLP applications, such as detecting duplicate\nquestions in community question answering platforms such as Quora. Arabic is\nconsidered to be an under-resourced language, has many dialects, and rich in\nmorphology. Combined together, these challenges make identifying semantically\nsimilar questions in Arabic even more difficult. In this paper, we introduce a\nnovel approach to tackle this problem, and test it on two benchmarks; one for\nModern Standard Arabic (MSA), and another for the 24 major Arabic dialects. We\nare able to show that our new system outperforms state-of-the-art approaches by\nachieving 93% F1-score on the MSA benchmark and 82% on the dialectical one.\nThis is achieved by utilizing contextualized word representations (ELMo\nembeddings) trained on a text corpus containing MSA and dialectic sentences.\nThis in combination with a pairwise fine-grained similarity layer, helps our\nquestion-to-question similarity model to generalize predictions on different\ndialects while being trained only on question-to-question MSA data.", "published": "2019-09-19 11:58:18", "link": "http://arxiv.org/abs/1909.09490v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Comparison of Hybrid and End-to-End Models for Syllable Recognition", "abstract": "This paper presents a comparison of a traditional hybrid speech recognition\nsystem (kaldi using WFST and TDNN with lattice-free MMI) and a lexicon-free\nend-to-end (TensorFlow implementation of multi-layer LSTM with CTC training)\nmodels for German syllable recognition on the Verbmobil corpus. The results\nshow that explicitly modeling prior knowledge is still valuable in building\nrecognition systems. With a strong language model (LM) based on syllables, the\nstructured approach significantly outperforms the end-to-end model. The best\nword error rate (WER) regarding syllables was achieved using kaldi with a\n4-gram LM, modeling all syllables observed in the training set. It achieved\n10.0% WER w.r.t. the syllables, compared to the end-to-end approach where the\nbest WER was 27.53%. The work presented here has implications for building\nfuture recognition systems that operate independent of a large vocabulary, as\ntypically used in a tasks such as recognition of syllabic or agglutinative\nlanguages, out-of-vocabulary techniques, keyword search indexing and medical\nspeech processing.", "published": "2019-09-19 09:51:35", "link": "http://arxiv.org/abs/1909.12232v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An extended two-dimensional vocal tract model for fast acoustic\n  simulation of single-axis symmetric three-dimensional tubes", "abstract": "The simulation of two-dimensional (2D) wave propagation is an affordable\ncomputational task and its use can potentially improve time performance in\nvocal tracts' acoustic analysis. Several models have been designed that rely on\n2D wave solvers and include 2D representations of three-dimensional (3D) vocal\ntract-like geometries. However, until now, only the acoustics of straight 3D\ntubes with circular cross-sections have been successfully replicated with this\napproach. Furthermore, the simulation of the resulting 2D shapes requires\nextremely high spatio-temporal resolutions, dramatically reducing the speed\nboost deriving from the usage of a 2D wave solver. In this paper, we introduce\nan in-progress novel vocal tract model that extends the 2D Finite-Difference\nTime-Domain wave solver (2.5D FDTD) by adding tube depth, derived from the area\nfunctions, to the acoustic solver. The model combines the speed of a light 2D\nnumerical scheme with the ability to natively simulate 3D tubes that are\nsymmetric in one dimension, hence relaxing previous resolution requirements. An\nimplementation of the 2.5D FDTD is presented, along with evaluation of its\nperformance in the case of static vowel modeling. The paper discusses the\ncurrent features and limits of the approach, and the potential impact on\ncomputational acoustics applications.", "published": "2019-09-19 03:29:43", "link": "http://arxiv.org/abs/1909.09585v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WEnets: A Convolutional Framework for Evaluating Audio Waveforms", "abstract": "We describe a new convolutional framework for waveform evaluation, WEnets,\nand build a Narrowband Audio Waveform Evaluation Network, or NAWEnet, using\nthis framework. NAWEnet is single-ended (or no-reference) and was trained three\nseparate times in order to emulate PESQ, POLQA, or STOI with testing\ncorrelations 0.95, 0.92, and 0.95, respectively when training on only 50% of\navailable data and testing on 40%. Stacks of 1-D convolutional layers and\nnon-linear downsampling learn which features are important for quality or\nintelligibility estimation. This straightforward architecture simplifies the\ninterpretation of its inner workings and paves the way for future\ninvestigations into higher sample rates and accurate no-reference subjective\nspeech quality predictions.", "published": "2019-09-19 14:45:59", "link": "http://arxiv.org/abs/1909.09024v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Impact of Ground Sound", "abstract": "Rigid-body impact sound synthesis methods often omit the ground sound. In\nthis paper we analyze an idealized ground-sound model based on an elastodynamic\nhalfspace, and use it to identify scenarios wherein ground sound is\nperceptually relevant versus when it is masked by the impacting object's modal\nsound or transient acceleration noise. Our analytical model gives a smooth,\nclosed-form expression for ground surface acceleration, which we can then use\nin the Rayleigh integral or in an \"acoustic shader\" for a finite-difference\ntime-domain wave simulation. We find that when modal sound is inaudible, ground\nsound is audible in scenarios where a dense object impacts a soft ground and\nscenarios where the impact point has a low elevation angle to the listening\npoint.", "published": "2019-09-19 21:06:14", "link": "http://arxiv.org/abs/1909.09235v1", "categories": ["cs.SD", "cs.CE", "eess.AS"], "primary_category": "cs.SD"}
