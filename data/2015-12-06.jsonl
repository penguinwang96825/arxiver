{"title": "Want Answers? A Reddit Inspired Study on How to Pose Questions", "abstract": "Questions form an integral part of our everyday communication, both offline\nand online. Getting responses to our questions from others is fundamental to\nsatisfying our information need and in extending our knowledge boundaries. A\nquestion may be represented using various factors such as social, syntactic,\nsemantic, etc. We hypothesize that these factors contribute with varying\ndegrees towards getting responses from others for a given question. We perform\na thorough empirical study to measure effects of these factors using a novel\nquestion and answer dataset from the website Reddit.com. To the best of our\nknowledge, this is the first such analysis of its kind on this important topic.\nWe also use a sparse nonnegative matrix factorization technique to\nautomatically induce interpretable semantic factors from the question dataset.\nWe also document various patterns on response prediction we observe during our\nanalysis in the data. For instance, we found that preference-probing questions\nare scantily answered. Our method is robust to capture such latent response\nfactors. We hope to make our code and datasets publicly available upon\npublication of the paper.", "published": "2015-12-06 10:31:12", "link": "http://arxiv.org/abs/1512.01768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment\n  analysis methods", "abstract": "In the last few years thousands of scientific papers have investigated\nsentiment analysis, several startups that measure opinions on real data have\nemerged and a number of innovative products related to this theme have been\ndeveloped. There are multiple methods for measuring sentiments, including\nlexical-based and supervised machine learning methods. Despite the vast\ninterest on the theme and wide popularity of some methods, it is unclear which\none is better for identifying the polarity (i.e., positive or negative) of a\nmessage. Accordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, \\textit{as they are\nused in practice}, across multiple datasets originated from different data\nsources. Such a comparison is key for understanding the potential limitations,\nadvantages, and disadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-four popular sentiment\nanalysis methods (which we call the state-of-the-practice methods). Our\nevaluation is based on a benchmark of eighteen labeled datasets, covering\nmessages posted on social networks, movie and product reviews, as well as\nopinions and comments in news articles. Our results highlight the extent to\nwhich the prediction performance of these methods varies considerably across\ndatasets. Aiming at boosting the development of this research area, we open the\nmethods' codes and datasets used in this article, deploying them in a benchmark\nsystem, which provides an open API for accessing and comparing sentence-level\nsentiment analysis methods.", "published": "2015-12-06 18:52:51", "link": "http://arxiv.org/abs/1512.01818v5", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
