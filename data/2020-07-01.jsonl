{"title": "Transferability of Natural Language Inference to Biomedical Question\n  Answering", "abstract": "Biomedical question answering (QA) is a challenging task due to the scarcity\nof data and the requirement of domain expertise. Pre-trained language models\nhave been used to address these issues. Recently, learning relationships\nbetween sentence pairs has been proved to improve performance in general QA. In\nthis paper, we focus on applying BioBERT to transfer the knowledge of natural\nlanguage inference (NLI) to biomedical QA. We observe that BioBERT trained on\nthe NLI dataset obtains better performance on Yes/No (+5.59%), Factoid\n(+0.53%), List type (+13.58%) questions compared to performance obtained in a\nprevious challenge (BioASQ 7B Phase B). We present a sequential transfer\nlearning method that significantly performed well in the 8th BioASQ Challenge\n(Phase B). In sequential transfer learning, the order in which tasks are\nfine-tuned is important. We measure an unanswerable rate of the extractive QA\nsetting when the formats of factoid and list type questions are converted to\nthe format of the Stanford Question Answering Dataset (SQuAD).", "published": "2020-07-01 04:05:48", "link": "http://arxiv.org/abs/2007.00217v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "So What's the Plan? Mining Strategic Planning Documents", "abstract": "In this paper we present a corpus of Russian strategic planning documents,\nRuREBus. This project is grounded both from language technology and\ne-government perspectives. Not only new language sources and tools are being\ndeveloped, but also their applications to e-goverment research. We demonstrate\nthe pipeline for creating a text corpus from scratch. First, the annotation\nschema is designed. Next texts are marked up using human-in-the-loop strategy,\nso that preliminary annotations are derived from a machine learning model and\nare manually corrected. The amount of annotated texts is large enough to\nshowcase what insights can be gained from RuREBus.", "published": "2020-07-01 05:40:16", "link": "http://arxiv.org/abs/2007.00257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Paraphrastic Augmentation with Discriminative Span Alignment", "abstract": "We introduce a novel paraphrastic augmentation strategy based on\nsentence-level lexically constrained paraphrasing and discriminative span\nalignment. Our approach allows for the large-scale expansion of existing\nresources, or the rapid creation of new resources from a small,\nmanually-produced seed corpus. We illustrate our framework on the Berkeley\nFrameNet Project, a large-scale language understanding effort spanning more\nthan two decades of human labor. Based on roughly four days of collecting\ntraining data for the alignment model and approximately one day of parallel\ncompute, we automatically generate 495,300 unique (Frame, Trigger) combinations\nannotated in context, a roughly 50x expansion atop FrameNet v1.7.", "published": "2020-07-01 08:33:44", "link": "http://arxiv.org/abs/2007.00320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 4: Commonsense Validation and Explanation", "abstract": "In this paper, we present SemEval-2020 Task 4, Commonsense Validation and\nExplanation (ComVE), which includes three subtasks, aiming to evaluate whether\na system can distinguish a natural language statement that makes sense to\nhumans from one that does not, and provide the reasons. Specifically, in our\nfirst subtask, the participating systems are required to choose from two\nnatural language statements of similar wording the one that makes sense and the\none does not. The second subtask additionally asks a system to select the key\nreason from three options why a given statement does not make sense. In the\nthird subtask, a participating system needs to generate the reason. We finally\nattracted 39 teams participating at least one of the three subtasks. For\nSubtask A and Subtask B, the performances of top-ranked systems are close to\nthat of humans. However, for Subtask C, there is still a relatively large gap\nbetween systems and human performance. The dataset used in our task can be\nfound at https://github.com/wangcunxiang/SemEval2020-\nTask4-Commonsense-Validation-and-Explanation; The leaderboard can be found at\nhttps://competitions.codalab.org/competitions/21080#results.", "published": "2020-07-01 04:41:05", "link": "http://arxiv.org/abs/2007.00236v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Semantic Hashing with Pairwise Reconstruction", "abstract": "Semantic Hashing is a popular family of methods for efficient similarity\nsearch in large-scale datasets. In Semantic Hashing, documents are encoded as\nshort binary vectors (i.e., hash codes), such that semantic similarity can be\nefficiently computed using the Hamming distance. Recent state-of-the-art\napproaches have utilized weak supervision to train better performing hashing\nmodels. Inspired by this, we present Semantic Hashing with Pairwise\nReconstruction (PairRec), which is a discrete variational autoencoder based\nhashing model. PairRec first encodes weakly supervised training pairs (a query\ndocument and a semantically similar document) into two hash codes, and then\nlearns to reconstruct the same query document from both of these hash codes\n(i.e., pairwise reconstruction). This pairwise reconstruction enables our model\nto encode local neighbourhood structures within the hash code directly through\nthe decoder. We experimentally compare PairRec to traditional and\nstate-of-the-art approaches, and obtain significant performance improvements in\nthe task of document similarity search.", "published": "2020-07-01 10:54:27", "link": "http://arxiv.org/abs/2007.00380v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "COVID-19 Literature Knowledge Graph Construction and Drug Repurposing\n  Report Generation", "abstract": "To combat COVID-19, both clinicians and scientists need to digest vast\namounts of relevant biomedical knowledge in scientific literature to understand\nthe disease mechanism and related biological functions. We have developed a\nnovel and comprehensive knowledge discovery framework, COVID-KG to extract\nfine-grained multimedia knowledge elements (entities and their visual chemical\nstructures, relations, and events) from scientific literature. We then exploit\nthe constructed multimedia knowledge graphs (KGs) for question answering and\nreport generation, using drug repurposing as a case study. Our framework also\nprovides detailed contextual sentences, subfigures, and knowledge subgraphs as\nevidence.", "published": "2020-07-01 16:03:20", "link": "http://arxiv.org/abs/2007.00576v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computing Conceptual Distances between Breast Cancer Screening\n  Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical\n  Disagreement", "abstract": "Using natural language processing tools, we investigate the differences of\nrecommendations in medical guidelines for the same decision problem -- breast\ncancer screening. We show that these differences arise from knowledge brought\nto the problem by different medical societies, as reflected in the conceptual\nvocabularies used by the different groups of authors.The computational models\nwe build and analyze agree with the near-peer epistemic model of expert\ndisagreement proposed by Garbayo. Even though the article is a case study\nfocused on one set of guidelines, the proposed methodology is broadly\napplicable. In addition to proposing a novel graph-based similarity model for\ncomparing collections of documents, we perform an extensive analysis of the\nmodel performance. In a series of a few dozen experiments, in three broad\ncategories, we show, at a very high statistical significance level of 3-4\nstandard deviations for our best models, that the high similarity between\nexpert annotated model and our concept based, automatically created,\ncomputational models is not accidental. Our best model achieves roughly 70%\nsimilarity. We also describe possible extensions of this work.", "published": "2020-07-01 19:21:10", "link": "http://arxiv.org/abs/2007.00709v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Legends: Folklore on Reddit", "abstract": "In this paper we introduce Reddit legends, a collection of venerated old\nposts that have become famous on Reddit. To establish the utility of Reddit\nlegends for both computational science/HCI and folkloristics, we investigate\ntwo main questions: (1) whether they can be considered folklore, i.e. if they\nhave consistent form, cultural significance, and undergo spontaneous\ntransmission, and (2) whether they can be studied in a systematic manner.\nThrough several subtasks, including the creation of a typology, an analysis of\nreferences to Reddit legends, and an examination of some of the textual\ncharacteristics of referencing behaviour, we show that Reddit legends can\nindeed be considered as folklore and that they are amendable to systematic\ntext-based approaches. We discuss how these results will enable future analyses\nof folklore on Reddit, including tracking subreddit-wide and individual-user\nbehaviour, and the relationship of this behaviour to other cultural markers.", "published": "2020-07-01 20:55:41", "link": "http://arxiv.org/abs/2007.00750v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Relevance-guided Supervision for OpenQA with ColBERT", "abstract": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.", "published": "2020-07-01 23:50:58", "link": "http://arxiv.org/abs/2007.00814v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings", "abstract": "Segmental models are sequence prediction models in which scores of hypotheses\nare based on entire variable-length segments of frames. We consider segmental\nmodels for whole-word (\"acoustic-to-word\") speech recognition, with the feature\nvectors defined using vector embeddings of segments. Such models are\ncomputationally challenging as the number of paths is proportional to the\nvocabulary size, which can be orders of magnitude larger than when using\nsubword units like phones. We describe an efficient approach for end-to-end\nwhole-word segmental models, with forward-backward and Viterbi decoding\nperformed on a GPU and a simple segment scoring function that reduces space\ncomplexity. In addition, we investigate the use of pre-training via jointly\ntrained acoustic word embeddings (AWEs) and acoustically grounded word\nembeddings (AGWEs) of written word labels. We find that word error rate can be\nreduced by a large margin by pre-training the acoustic segment representation\nwith AWEs, and additional (smaller) gains can be obtained by pre-training the\nword prediction layer with AGWEs. Our final models improve over prior A2W\nmodels.", "published": "2020-07-01 02:22:09", "link": "http://arxiv.org/abs/2007.00183v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Text Style Transfer for Outdoor Vision-and-Language\n  Navigation", "abstract": "One of the most challenging topics in Natural Language Processing (NLP) is\nvisually-grounded language understanding and reasoning. Outdoor\nvision-and-language navigation (VLN) is such a task where an agent follows\nnatural language instructions and navigates a real-life urban environment. Due\nto the lack of human-annotated instructions that illustrate intricate urban\nscenes, outdoor VLN remains a challenging task to solve. This paper introduces\na Multimodal Text Style Transfer (MTST) learning approach and leverages\nexternal multimodal resources to mitigate data scarcity in outdoor navigation\ntasks. We first enrich the navigation data by transferring the style of the\ninstructions generated by Google Maps API, then pre-train the navigator with\nthe augmented external outdoor navigation dataset. Experimental results show\nthat our MTST learning approach is model-agnostic, and our MTST approach\nsignificantly outperforms the baseline models on the outdoor VLN task,\nimproving task completion rate by 8.7% relatively on the test set.", "published": "2020-07-01 04:29:07", "link": "http://arxiv.org/abs/2007.00229v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Latent Compositional Representations Improve Systematic Generalization\n  in Grounded Question Answering", "abstract": "Answering questions that involve multi-step reasoning requires decomposing\nthem and using the answers of intermediate steps to reach the final answer.\nHowever, state-of-the-art models in grounded question answering often do not\nexplicitly perform decomposition, leading to difficulties in generalization to\nout-of-distribution examples. In this work, we propose a model that computes a\nrepresentation and denotation for all question spans in a bottom-up,\ncompositional manner using a CKY-style parser. Our model induces latent trees,\ndriven by end-to-end (the answer) supervision only. We show that this inductive\nbias towards tree structures dramatically improves systematic generalization to\nout-of-distribution examples, compared to strong baselines on an arithmetic\nexpressions benchmark as well as on CLOSURE, a dataset that focuses on\nsystematic generalization for grounded question answering. On this challenging\ndataset, our model reaches an accuracy of 96.1%, significantly higher than\nprior models that almost perfectly solve the task on a random, in-distribution\nsplit.", "published": "2020-07-01 06:22:51", "link": "http://arxiv.org/abs/2007.00266v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "abstract": "Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.", "published": "2020-07-01 23:15:56", "link": "http://arxiv.org/abs/2007.00808v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Exploring the time-domain deep attractor network with two-stream\n  architectures in a reverberant environment", "abstract": "Deep attractor networks (DANs) perform speech separation with discriminative\nembeddings and speaker attractors. Compared with methods based on the\npermutation invariant training (PIT), DANs define a deep embedding space and\ndeliver a more elaborate representation on each time-frequency (T-F) bin.\nHowever, it has been observed that the DANs achieve limited improvement on the\nsignal quality if directly deployed in a reverberant environment. Following the\nsuccess of time-domain separation networks on the clean mixture speech, we\npropose a time-domain DAN (TD-DAN) with two-streams of convolutional networks,\nwhich efficiently perform both dereverberation and separation tasks under the\ncondition of a variable number of speakers. The speaker encoding stream (SES)\nof the TD-DAN is trained to model the speaker information in the embedding\nspace. The speech decoding stream (SDS) accepts speaker attractors from the SES\nand learns to estimate early reflections from the spectro-temporal\nrepresentations. Meanwhile, additional clustering losses are used to bridge the\ngap between the oracle and the estimated attractors. Experiments were conducted\non the Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) dataset. The\nearly reflection was compared with the anechoic and reverberant signals and\nthen was chosen as the learning targets. The experimental results demonstrated\nthat the TD-DAN achieved scale-invariant source-to-distortion ratio (SI-SDR)\ngains of 9.79/7.47 dB on the reverberant 2/3-speaker evaluation set, exceeding\nthe baseline DAN and convolutional time-domain audio separation network\n(Conv-TasNet) by 1.92/0.68 dB and 0.91/0.47 dB, respectively.", "published": "2020-07-01 06:51:05", "link": "http://arxiv.org/abs/2007.00272v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Consistent Independent Low-Rank Matrix Analysis for Determined Blind\n  Source Separation", "abstract": "Independent low-rank matrix analysis (ILRMA) is the state-of-the-art\nalgorithm for blind source separation (BSS) in the determined situation (the\nnumber of microphones is greater than or equal to that of source signals).\nILRMA achieves a great separation performance by modeling the power\nspectrograms of the source signals via the nonnegative matrix factorization\n(NMF). Such a highly developed source model can solve the permutation problem\nof the frequency-domain BSS to a large extent, which is the reason for the\nexcellence of ILRMA. In this paper, we further improve the separation\nperformance of ILRMA by additionally considering the general structure of\nspectrograms, which is called consistency, and hence we call the proposed\nmethod Consistent ILRMA. Since a spectrogram is calculated by an overlapping\nwindow (and a window function induces spectral smearing called main- and\nside-lobes), the time-frequency bins depend on each other. In other words, the\ntime-frequency components are related to each other via the uncertainty\nprinciple. Such co-occurrence among the spectral components can function as an\nassistant for solving the permutation problem, which has been demonstrated by a\nrecent study. On the basis of these facts, we propose an algorithm for\nrealizing Consistent ILRMA by slightly modifying the original algorithm. Its\nperformance was extensively evaluated through experiments performed with\nvarious window lengths and shift lengths. The results indicated several\ntendencies of the original and proposed ILRMA that include some topics not\nfully discussed in the literature. For example, the proposed Consistent ILRMA\ntends to outperform the original ILRMA when the window length is sufficiently\nlong compared to the reverberation time of the mixing system.", "published": "2020-07-01 07:04:28", "link": "http://arxiv.org/abs/2007.00274v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Instantaneous PSD Estimation for Speech Enhancement based on Generalized\n  Principal Components", "abstract": "Power spectral density (PSD) estimates of various microphone signal\ncomponents are essential to many speech enhancement procedures. As speech is\nhighly non-nonstationary, performance improvements may be gained by maintaining\ntime-variations in PSD estimates. In this paper, we propose an instantaneous\nPSD estimation approach based on generalized principal components. Similarly to\nother eigenspace-based PSD estimation approaches, we rely on recursive\naveraging in order to obtain a microphone signal correlation matrix estimate to\nbe decomposed. However, instead of estimating the PSDs directly from the\ntemporally smooth generalized eigenvalues of this matrix, yielding temporally\nsmooth PSD estimates, we propose to estimate the PSDs from newly defined\ninstantaneous generalized eigenvalues, yielding instantaneous PSD estimates.\nThe instantaneous generalized eigenvalues are defined from the generalized\nprincipal components, i.e. a generalized eigenvector-based transform of the\nmicrophone signals. We further show that the smooth generalized eigenvalues can\nbe understood as a recursive average of the instantaneous generalized\neigenvalues. Simulation results comparing the multi-channel Wiener filter (MWF)\nwith smooth and instantaneous PSD estimates indicate better speech enhancement\nperformance for the latter. A MATLAB implementation is available online.", "published": "2020-07-01 15:10:54", "link": "http://arxiv.org/abs/2007.00542v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OrchideaSOL: a dataset of extended instrumental techniques for\n  computer-aided orchestration", "abstract": "This paper introduces OrchideaSOL, a free dataset of samples of extended\ninstrumental playing techniques, designed to be used as default dataset for the\nOrchidea framework for target-based computer-aided orchestration. OrchideaSOL\nis a reduced and modified subset of Studio On Line, or SOL for short, a dataset\ndeveloped at Ircam between 1996 and 1998. We motivate the reasons behind\nOrchideaSOL and describe the differences between the original SOL and our\ndataset. We will also show the work done in improving the dynamic ranges of\norchestral families and other aspects of the data.", "published": "2020-07-01 21:15:42", "link": "http://arxiv.org/abs/2007.00763v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated Empathy Detection for Oncology Encounters", "abstract": "Empathy involves understanding other people's situation, perspective, and\nfeelings. In clinical interactions, it helps clinicians establish rapport with\na patient and support patient-centered care and decision making. Understanding\nphysician communication through observation of audio-recorded encounters is\nlargely carried out with manual annotation and analysis. However, manual\nannotation has a prohibitively high cost. In this paper, a multimodal system is\nproposed for the first time to automatically detect empathic interactions in\nrecordings of real-world face-to-face oncology encounters that might accelerate\nmanual processes. An automatic speech and language processing pipeline is\nemployed to segment and diarize the audio as well as for transcription of\nspeech into text. Lexical and acoustic features are derived to help detect both\nempathic opportunities offered by the patient, and the expressed empathy by the\noncologist. We make the empathy predictions using Support Vector Machines\n(SVMs) and evaluate the performance on different combinations of features in\nterms of average precision (AP).", "published": "2020-07-01 23:24:42", "link": "http://arxiv.org/abs/2007.00809v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Surveying Off-Board and Extra-Vehicular Monitoring and Progress Towards\n  Pervasive Diagnostics", "abstract": "We survey the state-of-the-art in offboard diagnostics for vehicles, their\noccupants, and environments, with particular focus on vibroacoustic approaches.\nWe identify promising application areas including data-driven management for\nshared mobility and automated fleets, usage-based insurance, and vehicle,\noccupant, and environmental state and condition monitoring. We close by\nexploring the particular application of vibroacoustic monitoring to vehicle\ndiagnostics and prognostics and propose the introduction of automated vehicle-\nand context-specific model selection as a means of improving algorithm\nperformance, e.g. to enable smartphone-resident diagnostics. Towards this\nvision, four strong-performing, interdependent classifiers are presented as a\nproof-of-concept for identifying vehicle configuration from acoustic\nsignatures. The described approach may serve as the first step in developing\n\"universal diagnostics,\" with applicability extending beyond the automotive\ndomain.", "published": "2020-07-01 15:57:06", "link": "http://arxiv.org/abs/2007.03759v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalization of Hearing Aid Compression by Human-In-Loop Deep\n  Reinforcement Learning", "abstract": "Existing prescriptive compression strategies used in hearing aid fitting are\ndesigned based on gain averages from a group of users which are not necessarily\noptimal for a specific user. Nearly half of hearing aid users prefer settings\nthat differ from the commonly prescribed settings. This paper presents a\nhuman-in-loop deep reinforcement learning approach that personalizes hearing\naid compression to achieve improved hearing perception. The developed approach\nis designed to learn a specific user's hearing preferences in order to optimize\ncompression based on the user's feedbacks. Both simulation and subject testing\nresults are reported which demonstrate the effectiveness of the developed\npersonalized compression.", "published": "2020-07-01 02:50:33", "link": "http://arxiv.org/abs/2007.00192v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Transformer-based Audio Captioning Model with Keyword Estimation", "abstract": "One of the problems with automated audio captioning (AAC) is the\nindeterminacy in word selection corresponding to the audio event/scene. Since\none acoustic event/scene can be described with several words, it results in a\ncombinatorial explosion of possible captions and difficulty in training. To\nsolve this problem, we propose a Transformer-based audio-captioning model with\nkeyword estimation called TRACKE. It simultaneously solves the word-selection\nindeterminacy problem with the main task of AAC while executing the sub-task of\nacoustic event detection/acoustic scene classification (i.e., keyword\nestimation). TRACKE estimates keywords, which comprise a word set corresponding\nto audio events/scenes in the input audio, and generates the caption while\nreferring to the estimated keywords to reduce word-selection indeterminacy.\nExperimental results on a public AAC dataset indicate that TRACKE achieved\nstate-of-the-art performance and successfully estimated both the caption and\nits keywords.", "published": "2020-07-01 04:21:00", "link": "http://arxiv.org/abs/2007.00222v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning\n  with Keywords and Sentence Length Estimation", "abstract": "This technical report describes the system participating to the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6:\nautomated audio captioning. Our submission focuses on solving two indeterminacy\nproblems in automated audio captioning: word selection indeterminacy and\nsentence length indeterminacy. We simultaneously solve the main caption\ngeneration and sub indeterminacy problems by estimating keywords and sentence\nlength through multi-task learning. We tested a simplified model of our\nsubmission using the development-testing dataset. Our model achieved 20.7\nSPIDEr score where that of the baseline system was 5.4.", "published": "2020-07-01 04:26:27", "link": "http://arxiv.org/abs/2007.00225v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Private Speech Classification with Secure Multiparty Computation", "abstract": "Deep learning in audio signal processing, such as human voice audio signal\nclassification, is a rich application area of machine learning. Legitimate use\ncases include voice authentication, gunfire detection, and emotion recognition.\nWhile there are clear advantages to automated human speech classification,\napplication developers can gain knowledge beyond the professed scope from\nunprotected audio signal processing. In this paper we propose the first\nprivacy-preserving solution for deep learning-based audio classification that\nis provably secure. Our approach, which is based on Secure Multiparty\nComputation, allows to classify a speech signal of one party (Alice) with a\ndeep neural network of another party (Bob) without Bob ever seeing Alice's\nspeech signal in an unencrypted manner. As threat models, we consider both\npassive security, i.e. with semi-honest parties who follow the instructions of\nthe cryptographic protocols, as well as active security, i.e. with malicious\nparties who deviate from the protocols. We evaluate the\nefficiency-security-accuracy trade-off of the proposed solution in a use case\nfor privacy-preserving emotion detection from speech with a convolutional\nneural network. In the semi-honest case we can classify a speech signal in\nunder 0.3 sec; in the malicious case it takes $\\sim$1.6 sec. In both cases\nthere is no leakage of information, and we achieve classification accuracies\nthat are the same as when computations are done on unencrypted data.", "published": "2020-07-01 05:26:06", "link": "http://arxiv.org/abs/2007.00253v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "LSTM and GPT-2 Synthetic Speech Transfer Learning for Speaker\n  Recognition to Overcome Data Scarcity", "abstract": "In speech recognition problems, data scarcity often poses an issue due to the\nwillingness of humans to provide large amounts of data for learning and\nclassification. In this work, we take a set of 5 spoken Harvard sentences from\n7 subjects and consider their MFCC attributes. Using character level LSTMs\n(supervised learning) and OpenAI's attention-based GPT-2 models, synthetic\nMFCCs are generated by learning from the data provided on a per-subject basis.\nA neural network is trained to classify the data against a large dataset of\nFlickr8k speakers and is then compared to a transfer learning network\nperforming the same task but with an initial weight distribution dictated by\nlearning from the synthetic data generated by the two models. The best result\nfor all of the 7 subjects were networks that had been exposed to synthetic\ndata, the model pre-trained with LSTM-produced data achieved the best result 3\ntimes and the GPT-2 equivalent 5 times (since one subject had their best result\nfrom both models at a draw). Through these results, we argue that speaker\nclassification can be improved by utilising a small amount of user data but\nwith exposure to synthetically-generated MFCCs which then allow the networks to\nachieve near maximum classification scores.", "published": "2020-07-01 13:52:58", "link": "http://arxiv.org/abs/2007.00659v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
