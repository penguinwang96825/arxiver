{"title": "Neural Data-to-Text Generation with Dynamic Content Planning", "abstract": "Neural data-to-text generation models have achieved significant advancement\nin recent years. However, these models have two shortcomings: the generated\ntexts tend to miss some vital information, and they often generate descriptions\nthat are not consistent with the structured input data. To alleviate these\nproblems, we propose a Neural data-to-text generation model with Dynamic\ncontent Planning, named NDP for abbreviation. The NDP can utilize the\npreviously generated text to dynamically select the appropriate entry from the\ngiven structured data. We further design a reconstruction mechanism with a\nnovel objective function that can reconstruct the whole entry of the used data\nsequentially from the hidden states of the decoder, which aids the accuracy of\nthe generated text. Empirical results show that the NDP achieves superior\nperformance over the state-of-the-art on ROTOWIRE dataset, in terms of relation\ngeneration (RG), content selection (CS), content ordering (CO) and BLEU\nmetrics. The human evaluation result shows that the texts generated by the\nproposed NDP are better than the corresponding ones generated by NCP in most of\ntime. And using the proposed reconstruction mechanism, the fidelity of the\ngenerated text can be further improved significantly.", "published": "2020-04-16 02:50:51", "link": "http://arxiv.org/abs/2004.07426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Instance-Level Parser Selection for Cross-Lingual Transfer of\n  Dependency Parsers", "abstract": "Current methods of cross-lingual parser transfer focus on predicting the best\nparser for a low-resource target language globally, that is, \"at treebank\nlevel\". In this work, we propose and argue for a novel cross-lingual transfer\nparadigm: instance-level parser selection (ILPS), and present a\nproof-of-concept study focused on instance-level selection in the framework of\ndelexicalized parser transfer. We start from an empirical observation that\ndifferent source parsers are the best choice for different Universal POS\nsequences in the target language. We then propose to predict the best parser at\nthe instance level. To this end, we train a supervised regression model, based\non the Transformer architecture, to predict parser accuracies for individual\nPOS-sequences. We compare ILPS against two strong single-best parser selection\nbaselines (SBPS): (1) a model that compares POS n-gram distributions between\nthe source and target languages (KL) and (2) a model that selects the source\nbased on the similarity between manually created language vectors encoding\nsyntactic properties of languages (L2V). The results from our extensive\nevaluation, coupling 42 source parsers and 20 diverse low-resource test\nlanguages, show that ILPS outperforms KL and L2V on 13/20 and 14/20 test\nlanguages, respectively. Further, we show that by predicting the best parser\n\"at the treebank level\" (SBPS), using the aggregation of predictions from our\ninstance-level model, we outperform the same baselines on 17/20 and 16/20 test\nlanguages.", "published": "2020-04-16 13:18:55", "link": "http://arxiv.org/abs/2004.07642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate, Delete and Rewrite: A Three-Stage Framework for Improving\n  Persona Consistency of Dialogue Generation", "abstract": "Maintaining a consistent personality in conversations is quite natural for\nhuman beings, but is still a non-trivial task for machines. The persona-based\ndialogue generation task is thus introduced to tackle the\npersonality-inconsistent problem by incorporating explicit persona text into\ndialogue generation models. Despite the success of existing persona-based\nmodels on generating human-like responses, their one-stage decoding framework\ncan hardly avoid the generation of inconsistent persona words. In this work, we\nintroduce a three-stage framework that employs a generate-delete-rewrite\nmechanism to delete inconsistent words from a generated response prototype and\nfurther rewrite it to a personality-consistent one. We carry out evaluations by\nboth human and automatic metrics. Experiments on the Persona-Chat dataset show\nthat our approach achieves good performance.", "published": "2020-04-16 14:10:24", "link": "http://arxiv.org/abs/2004.07672v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Contextualized Topic Models with Zero-shot Learning", "abstract": "Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in\nmultiple languages. They all cover the same content, but the linguistic\ndifferences make it impossible to use traditional, bag-of-word-based topic\nmodels. Models have to be either single-language or suffer from a huge, but\nextremely sparse vocabulary. Both issues can be addressed by transfer learning.\nIn this paper, we introduce a zero-shot cross-lingual topic model. Our model\nlearns topics on one language (here, English), and predicts them for unseen\ndocuments in different languages (here, Italian, French, German, and\nPortuguese). We evaluate the quality of the topic predictions for the same\ndocument in different languages. Our results show that the transferred topics\nare coherent and stable across languages, which suggests exciting future\nresearch directions.", "published": "2020-04-16 16:21:17", "link": "http://arxiv.org/abs/2004.07737v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kvistur 2.0: a BiLSTM Compound Splitter for Icelandic", "abstract": "In this paper, we present a character-based BiLSTM model for splitting\nIcelandic compound words, and show how varying amounts of training data affects\nthe performance of the model. Compounding is highly productive in Icelandic,\nand new compounds are constantly being created. This results in a large number\nof out-of-vocabulary (OOV) words, negatively impacting the performance of many\nNLP tools. Our model is trained on a dataset of 2.9 million unique word forms\nand their constituent structures from the Database of Icelandic Morphology. The\nmodel learns how to split compound words into two parts and can be used to\nderive the constituent structure of any word form. Knowing the constituent\nstructure of a word form makes it possible to generate the optimal split for a\ngiven task, e.g., a full split for subword tokenization, or, in the case of\npart-of-speech tagging, splitting an OOV word until the largest known\nmorphological head is found. The model outperforms other previously published\nmethods when evaluated on a corpus of manually split word forms. This method\nhas been integrated into Kvistur, an Icelandic compound word analyzer.", "published": "2020-04-16 17:11:02", "link": "http://arxiv.org/abs/2004.07776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Anaphora Resolution as Question Answering", "abstract": "Most previous studies on bridging anaphora resolution (Poesio et al., 2004;\nHou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and\nassume that the gold mention information is given. In this paper, we cast\nbridging anaphora resolution as question answering based on context. This\nallows us to find the antecedent for a given anaphor without knowing any gold\nmention information (except the anaphor itself). We present a question\nanswering framework (BARQA) for this task, which leverages the power of\ntransfer learning. Furthermore, we propose a novel method to generate a large\namount of \"quasi-bridging\" training data. We show that our model pre-trained on\nthis dataset and fine-tuned on a small amount of in-domain dataset achieves new\nstate-of-the-art results for bridging anaphora resolution on two bridging\ncorpora (ISNotes (Markert et al., 2012) and BASHI (Roesiger, 2018)).", "published": "2020-04-16 19:42:43", "link": "http://arxiv.org/abs/2004.07898v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Machine Translation with Latent Alignments", "abstract": "This paper presents two strong methods, CTC and Imputer, for\nnon-autoregressive machine translation that model latent alignments with\ndynamic programming. We revisit CTC for machine translation and demonstrate\nthat a simple CTC model can achieve state-of-the-art for single-step\nnon-autoregressive machine translation, contrary to what prior work indicates.\nIn addition, we adapt the Imputer model for non-autoregressive machine\ntranslation and demonstrate that Imputer with just 4 generation steps can match\nthe performance of an autoregressive Transformer baseline. Our latent alignment\nmodels are simpler than many existing non-autoregressive translation baselines;\nfor example, we do not require target length prediction or re-scoring with an\nautoregressive model. On the competitive WMT'14 En$\\rightarrow$De task, our CTC\nmodel achieves 25.7 BLEU with a single generation step, while Imputer achieves\n27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This\ncompares favourably to the autoregressive Transformer baseline at 27.8 BLEU.", "published": "2020-04-16 03:45:56", "link": "http://arxiv.org/abs/2004.07437v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Right Tool for the Job: Matching Model and Instance Complexities", "abstract": "As NLP models become larger, executing a trained model requires significant\ncomputational resources incurring monetary and environmental costs. To better\nrespect a given inference budget, we propose a modification to contextual\nrepresentation fine-tuning which, during inference, allows for an early (and\nfast) \"exit\" from neural network calculations for simple instances, and late\n(and accurate) exit for hard instances. To achieve this, we add classifiers to\ndifferent layers of BERT and use their calibrated confidence scores to make\nearly exit decisions. We test our proposed modification on five different\ndatasets in two tasks: three text classification datasets and two natural\nlanguage inference benchmarks. Our method presents a favorable speed/accuracy\ntradeoff in almost all cases, producing models which are up to five times\nfaster than the state of the art, while preserving their accuracy. Our method\nalso requires almost no additional training resources (in either time or\nparameters) compared to the baseline BERT model. Finally, our method alleviates\nthe need for costly retraining of multiple models at different levels of\nefficiency; we allow users to control the inference speed/accuracy tradeoff\nusing a single trained model, by setting a single variable at inference time.\nWe publicly release our code.", "published": "2020-04-16 04:28:08", "link": "http://arxiv.org/abs/2004.07453v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Paraphrase Augmented Task-Oriented Dialog Generation", "abstract": "Neural generative models have achieved promising performance on dialog\ngeneration tasks if given a huge data set. However, the lack of high-quality\ndialog data and the expensive data annotation process greatly limit their\napplication in real-world settings. We propose a paraphrase augmented response\ngeneration (PARG) framework that jointly trains a paraphrase model and a\nresponse generation model to improve the dialog generation performance. We also\ndesign a method to automatically construct paraphrase training data set based\non dialog state and dialog act labels. PARG is applicable to various dialog\ngeneration models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al.,\n2019). Experimental results show that the proposed framework improves these\nstate-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also\nsignificantly outperforms other data augmentation methods in dialog generation\ntasks, especially under low resource settings.", "published": "2020-04-16 05:12:36", "link": "http://arxiv.org/abs/2004.07462v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace\n  Projection", "abstract": "The ability to control for the kinds of information encoded in neural\nrepresentation has a variety of use cases, especially in light of the challenge\nof interpreting these models. We present Iterative Null-space Projection\n(INLP), a novel method for removing information from neural representations.\nOur method is based on repeated training of linear classifiers that predict a\ncertain property we aim to remove, followed by projection of the\nrepresentations on their null-space. By doing so, the classifiers become\noblivious to that target property, making it hard to linearly separate the data\naccording to it. While applicable for multiple uses, we evaluate our method on\nbias and fairness use-cases, and show that our method is able to mitigate bias\nin word embeddings, as well as to increase fairness in a setting of multi-class\nclassification.", "published": "2020-04-16 14:02:50", "link": "http://arxiv.org/abs/2004.07667v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do sequence-to-sequence VAEs learn global features of sentences?", "abstract": "Autoregressive language models are powerful and relatively easy to train.\nHowever, these models are usually trained without explicit conditioning labels\nand do not offer easy ways to control global aspects such as sentiment or topic\nduring generation. Bowman & al. (2016) adapted the Variational Autoencoder\n(VAE) for natural language with the sequence-to-sequence architecture and\nclaimed that the latent vector was able to capture such global features in an\nunsupervised manner. We question this claim. We measure which words benefit\nmost from the latent information by decomposing the reconstruction loss per\nposition in the sentence. Using this method, we find that VAEs are prone to\nmemorizing the first words and the sentence length, producing local features of\nlimited usefulness. To alleviate this, we investigate alternative architectures\nbased on bag-of-words assumptions and language model pretraining. These\nvariants learn latent variables that are more global, i.e., more predictive of\ntopic or sentiment labels. Moreover, using reconstructions, we observe that\nthey decrease memorization: the first word and the sentence length are not\nrecovered as accurately than with the baselines, consequently yielding more\ndiverse reconstructions.", "published": "2020-04-16 14:43:27", "link": "http://arxiv.org/abs/2004.07683v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-scale analysis of grooming in modern social networks", "abstract": "Social networks are evolving to engage their users more by providing them\nwith more functionalities. One of the most attracting ones is streaming. Users\nmay broadcast part of their daily lives to thousands of others world-wide and\ninteract with them in real-time. Unfortunately, this feature is reportedly\nexploited for grooming. In this work, we provide the first in-depth analysis of\nthis problem for social live streaming services. More precisely, using a\ndataset that we collected, we identify predatory behaviours and grooming on\nchats that bypassed the moderation mechanisms of the LiveMe, the service under\ninvestigation. Beyond the traditional text approaches, we also investigate the\nrelevance of emojis in this context, as well as the user interactions through\nthe gift mechanisms of LiveMe. Finally, our analysis indicates the possibility\nof grooming towards minors, showing the extent of the problem in such\nplatforms.", "published": "2020-04-16 14:23:13", "link": "http://arxiv.org/abs/2004.08205v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Workflow Manager for Complex NLP and Content Curation Pipelines", "abstract": "We present a workflow manager for the flexible creation and customisation of\nNLP processing pipelines. The workflow manager addresses challenges in\ninteroperability across various different NLP tasks and hardware-based resource\nusage. Based on the four key principles of generality, flexibility, scalability\nand efficiency, we present the first version of the workflow manager by\nproviding details on its custom definition language, explaining the\ncommunication components and the general system architecture and setup. We\ncurrently implement the system, which is grounded and motivated by real-world\nindustry use cases in several innovation and transfer projects.", "published": "2020-04-16 21:23:28", "link": "http://arxiv.org/abs/2004.14130v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TriggerNER: Learning with Entity Triggers as Explanations for Named\n  Entity Recognition", "abstract": "Training neural models for named entity recognition (NER) in a new domain\noften requires additional human annotations (e.g., tens of thousands of labeled\ninstances) that are usually expensive and time-consuming to collect. Thus, a\ncrucial research question is how to obtain supervision in a cost-effective way.\nIn this paper, we introduce \"entity triggers,\" an effective proxy of human\nexplanations for facilitating label-efficient learning of NER models. An entity\ntrigger is defined as a group of words in a sentence that helps to explain why\nhumans would recognize an entity in the sentence.\n  We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our\nproposed model, Trigger Matching Network, jointly learns trigger\nrepresentations and soft matching module with self-attention such that can\ngeneralize to unseen sentences easily for tagging. Our framework is\nsignificantly more cost-effective than the traditional neural NER frameworks.\nExperiments show that using only 20% of the trigger-annotated sentences results\nin a comparable performance as using 70% of conventional annotated sentences.", "published": "2020-04-16 07:27:43", "link": "http://arxiv.org/abs/2004.07493v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from\n  Explanation", "abstract": "Successfully training a deep neural network demands a huge corpus of labeled\ndata. However, each label only provides limited information to learn from and\ncollecting the requisite number of labels involves massive human effort. In\nthis work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN\nframework for sequence labeling and classification tasks, with an easy-to-use\nUI that not only allows an annotator to provide the needed labels for a task,\nbut also enables LearnIng From Explanations for each labeling decision. Such\nexplanations enable us to generate useful additional labeled data from\nunlabeled instances, bolstering the pool of available training data. On three\npopular NLP tasks (named entity recognition, relation extraction, sentiment\nanalysis), we find that using this enhanced supervision allows our models to\nsurpass competitive baseline F1 scores by more than 5-10 percentage points,\nwhile using 2X times fewer labeled instances. Our framework is the first to\nutilize this enhanced supervision technique and does so for three important\ntasks -- thus providing improved annotation recommendations to users and an\nability to build datasets of (data, label, explanation) triples instead of the\nregular (data, label) pair.", "published": "2020-04-16 07:38:07", "link": "http://arxiv.org/abs/2004.07499v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Suicidal Ideation and Mental Disorder Detection with Attentive Relation\n  Networks", "abstract": "Mental health is a critical issue in modern society, and mental disorders\ncould sometimes turn to suicidal ideation without effective treatment. Early\ndetection of mental disorders and suicidal ideation from social content\nprovides a potential way for effective social intervention. However,\nclassifying suicidal ideation and other mental disorders is challenging as they\nshare similar patterns in language usage and sentimental polarity. This paper\nenhances text representation with lexicon-based sentiment scores and latent\ntopics and proposes using relation networks to detect suicidal ideation and\nmental disorders with related risk indicators. The relation module is further\nequipped with the attention mechanism to prioritize more critical relational\nfeatures. Through experiments on three real-world datasets, our model\noutperforms most of its counterparts.", "published": "2020-04-16 11:18:55", "link": "http://arxiv.org/abs/2004.07601v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Methodology for Creating Question Answering Corpora Using Inverse Data\n  Annotation", "abstract": "In this paper, we introduce a novel methodology to efficiently construct a\ncorpus for question answering over structured data. For this, we introduce an\nintermediate representation that is based on the logical query plan in a\ndatabase called Operation Trees (OT). This representation allows us to invert\nthe annotation process without losing flexibility in the types of queries that\nwe generate. Furthermore, it allows for fine-grained alignment of query tokens\nto OT operations. In our method, we randomly generate OTs from a context-free\ngrammar. Afterwards, annotators have to write the appropriate natural language\nquestion that is represented by the OT. Finally, the annotators assign the\ntokens to the OT operations. We apply the method to create a new corpus OTTA\n(Operation Trees and Token Assignment), a large semantic parsing corpus for\nevaluating natural language interfaces to databases. We compare OTTA to Spider\nand LC-QuaD 2.0 and show that our methodology more than triples the annotation\nspeed while maintaining the complexity of the queries. Finally, we train a\nstate-of-the-art semantic parsing model on our data and show that our corpus is\na challenging dataset and that the token alignment can be leveraged to increase\nthe performance significantly.", "published": "2020-04-16 12:50:01", "link": "http://arxiv.org/abs/2004.07633v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Deep Generation of Coq Lemma Names Using Elaborated Terms", "abstract": "Coding conventions for naming, spacing, and other essentially stylistic\nproperties are necessary for developers to effectively understand, review, and\nmodify source code in large software projects. Consistent conventions in\nverification projects based on proof assistants, such as Coq, increase in\nimportance as projects grow in size and scope. While conventions can be\ndocumented and enforced manually at high cost, emerging approaches\nautomatically learn and suggest idiomatic names in Java-like languages by\napplying statistical language models on large code corpora. However, due to its\npowerful language extension facilities and fusion of type checking and\ncomputation, Coq is a challenging target for automated learning techniques. We\npresent novel generation models for learning and suggesting lemma names for Coq\nprojects. Our models, based on multi-input neural networks, are the first to\nleverage syntactic and semantic information from Coq's lexer (tokens in lemma\nstatements), parser (syntax trees), and kernel (elaborated terms) for naming;\nthe key insight is that learning from elaborated terms can substantially boost\nmodel performance. We implemented our models in a toolchain, dubbed Roosterize,\nand applied it on a large corpus of code derived from the Mathematical\nComponents family of projects, known for its stringent coding conventions. Our\nresults show that Roosterize substantially outperforms baselines for suggesting\nlemma names, highlighting the importance of using multi-input models and\nelaborated terms.", "published": "2020-04-16 16:54:21", "link": "http://arxiv.org/abs/2004.07761v2", "categories": ["cs.PL", "cs.CL", "cs.SE"], "primary_category": "cs.PL"}
{"title": "Avoiding the Hypothesis-Only Bias in Natural Language Inference via\n  Ensemble Adversarial Training", "abstract": "Natural Language Inference (NLI) datasets contain annotation artefacts\nresulting in spurious correlations between the natural language utterances and\ntheir respective entailment classes. These artefacts are exploited by neural\nnetworks even when only considering the hypothesis and ignoring the premise,\nleading to unwanted biases. Belinkov et al. (2019b) proposed tackling this\nproblem via adversarial training, but this can lead to learned sentence\nrepresentations that still suffer from the same biases. We show that the bias\ncan be reduced in the sentence representations by using an ensemble of\nadversaries, encouraging the model to jointly decrease the accuracy of these\ndifferent adversaries while fitting the data. This approach produces more\nrobust NLI models, outperforming previous de-biasing efforts when generalised\nto 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In\naddition, we find that the optimal number of adversarial classifiers depends on\nthe dimensionality of the sentence representations, with larger sentence\nrepresentations being more difficult to de-bias while benefiting from using a\ngreater number of adversaries.", "published": "2020-04-16 17:37:15", "link": "http://arxiv.org/abs/2004.07790v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Light-Weighted CNN for Text Classification", "abstract": "For management, documents are categorized into a specific category, and to do\nthese, most of the organizations use manual labor. In today's automation era,\nmanual efforts on such a task are not justified, and to avoid this, we have so\nmany software out there in the market. However, efficiency and minimal resource\nconsumption is the focal point which is also creating a competition. The\ncategorization of such documents into specified classes by machine provides\nexcellent help. One of categorization technique is text classification using a\nConvolutional neural network(TextCNN). TextCNN uses multiple sizes of filters,\nas in the case of the inception layer introduced in Googlenet. The network\nprovides good accuracy but causes high memory consumption due to a large number\nof trainable parameters. As a solution to this problem, we introduced a whole\nnew architecture based on separable convolution. The idea of separable\nconvolution already exists in the field of image classification but not yet\nintroduces to text classification tasks. With the help of this architecture, we\ncan achieve a drastic reduction in trainable parameters.", "published": "2020-04-16 20:23:52", "link": "http://arxiv.org/abs/2004.07922v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Knowledge-and-Data-Driven Amplitude Spectrum Prediction for Hierarchical\n  Neural Vocoders", "abstract": "In our previous work, we have proposed a neural vocoder called HiNet which\nrecovers speech waveforms by predicting amplitude and phase spectra\nhierarchically from input acoustic features. In HiNet, the amplitude spectrum\npredictor (ASP) predicts log amplitude spectra (LAS) from input acoustic\nfeatures. This paper proposes a novel knowledge-and-data-driven ASP (KDD-ASP)\nto improve the conventional one. First, acoustic features (i.e., F0 and\nmel-cepstra) pass through a knowledge-driven LAS recovery module to obtain\napproximate LAS (ALAS). This module is designed based on the combination of\nSTFT and source-filter theory, in which the source part and the filter part are\ndesigned based on input F0 and mel-cepstra, respectively. Then, the recovered\nALAS are processed by a data-driven LAS refinement module which consists of\nmultiple trainable convolutional layers to get the final LAS. Experimental\nresults show that the HiNet vocoder using KDD-ASP can achieve higher quality of\nsynthetic speech than that using conventional ASP and the WaveRNN vocoder on a\ntext-to-speech (TTS) task.", "published": "2020-04-16 12:20:37", "link": "http://arxiv.org/abs/2004.07832v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice-Indistinguishability: Protecting Voiceprint in Privacy-Preserving\n  Speech Data Release", "abstract": "With the development of smart devices, such as the Amazon Echo and Apple's\nHomePod, speech data have become a new dimension of big data. However, privacy\nand security concerns may hinder the collection and sharing of real-world\nspeech data, which contain the speaker's identifiable information, i.e.,\nvoiceprint, which is considered a type of biometric identifier. Current studies\non voiceprint privacy protection do not provide either a meaningful\nprivacy-utility trade-off or a formal and rigorous definition of privacy. In\nthis study, we design a novel and rigorous privacy metric for voiceprint\nprivacy, which is referred to as voice-indistinguishability, by extending\ndifferential privacy. We also propose mechanisms and frameworks for\nprivacy-preserving speech data release satisfying voice-indistinguishability.\nExperiments on public datasets verify the effectiveness and efficiency of the\nproposed methods.", "published": "2020-04-16 03:52:43", "link": "http://arxiv.org/abs/2004.07442v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Speech Paralinguistic Approach for Detecting Dementia Using Gated\n  Convolutional Neural Network", "abstract": "We propose a non-invasive and cost-effective method to automatically detect\ndementia by utilizing solely speech audio data. We extract paralinguistic\nfeatures for a short speech segment and use Gated Convolutional Neural Networks\n(GCNN) to classify it into dementia or healthy. We evaluate our method on the\nPitt Corpus and on our own dataset, the PROMPT Database. Our method yields the\naccuracy of 73.1% on the Pitt Corpus using an average of 114 seconds of speech\ndata. In the PROMPT Database, our method yields the accuracy of 74.7% using 4\nseconds of speech data and it improves to 80.8% when we use all the patient's\nspeech data. Furthermore, we evaluate our method on a three-class\nclassification problem in which we included the Mild Cognitive Impairment (MCI)\nclass and achieved the accuracy of 60.6% with 40 seconds of speech data.", "published": "2020-04-16 23:26:43", "link": "http://arxiv.org/abs/2004.07992v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Deep Neural Network for Respiratory Sound Classification in Wearable\n  Devices Enabled by Patient Specific Model Tuning", "abstract": "The primary objective of this paper is to build classification models and\nstrategies to identify breathing sound anomalies (wheeze, crackle) for\nautomated diagnosis of respiratory and pulmonary diseases. In this work we\npropose a deep CNN-RNN model that classifies respiratory sounds based on\nMel-spectrograms. We also implement a patient specific model tuning strategy\nthat first screens respiratory patients and then builds patient specific\nclassification models using limited patient data for reliable anomaly\ndetection. Moreover, we devise a local log quantization strategy for model\nweights to reduce the memory footprint for deployment in memory constrained\nsystems such as wearable devices. The proposed hybrid CNN-RNN model achieves a\nscore of 66.31% on four-class classification of breathing cycles for ICBHI'17\nscientific challenge respiratory sound database. When the model is re-trained\nwith patient specific data, it produces a score of 71.81% for leave-one-out\nvalidation. The proposed weight quantization technique achieves ~4X reduction\nin total memory cost without loss of performance. The main contribution of the\npaper is as follows: Firstly, the proposed model is able to achieve state of\nthe art score on the ICBHI'17 dataset. Secondly, deep learning models are shown\nto successfully learn domain specific knowledge when pre-trained with breathing\ndata and produce significantly superior performance compared to generalized\nmodels. Finally, local log quantization of trained weights is shown to be able\nto reduce the memory requirement significantly. This type of patient-specific\nre-training strategy can be very useful in developing reliable long-term\nautomated patient monitoring systems particularly in wearable healthcare\nsolutions.", "published": "2020-04-16 15:42:58", "link": "http://arxiv.org/abs/2004.08287v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
