{"title": "Attention Transfer Network for Aspect-level Sentiment Classification", "abstract": "Aspect-level sentiment classification (ASC) aims to detect the sentiment\npolarity of a given opinion target in a sentence. In neural network-based\nmethods for ASC, most works employ the attention mechanism to capture the\ncorresponding sentiment words of the opinion target, then aggregate them as\nevidence to infer the sentiment of the target. However, aspect-level datasets\nare all relatively small-scale due to the complexity of annotation. Data\nscarcity causes the attention mechanism sometimes to fail to focus on the\ncorresponding sentiment words of the target, which finally weakens the\nperformance of neural models. To address the issue, we propose a novel\nAttention Transfer Network (ATN) in this paper, which can successfully exploit\nattention knowledge from resource-rich document-level sentiment classification\ndatasets to improve the attention capability of the aspect-level sentiment\nclassification task. In the ATN model, we design two different methods to\ntransfer attention knowledge and conduct experiments on two ASC benchmark\ndatasets. Extensive experimental results show that our methods consistently\noutperform state-of-the-art works. Further analysis also validates the\neffectiveness of ATN.", "published": "2020-10-23 04:26:33", "link": "http://arxiv.org/abs/2010.12156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Similarity between Movie Characters and Its Potential\n  Implications on Understanding Human Experiences", "abstract": "While many different aspects of human experiences have been studied by the\nNLP community, none has captured its full richness. We propose a new task to\ncapture this richness based on an unlikely setting: movie characters. We sought\nto capture theme-level similarities between movie characters that were\ncommunity-curated into 20,000 themes. By introducing a two-step approach that\nbalances performance and efficiency, we managed to achieve 9-27\\% improvement\nover recent paragraph-embedding based methods. Finally, we demonstrate how the\nthematic information learnt from movie characters can potentially be used to\nunderstand themes in the experience of people, as indicated on Reddit posts.", "published": "2020-10-23 06:28:25", "link": "http://arxiv.org/abs/2010.12183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Divergences: a Survey and Empirical Analysis", "abstract": "Domain divergence plays a significant role in estimating the performance of a\nmodel in new domains. While there is a significant literature on divergence\nmeasures, researchers find it hard to choose an appropriate divergence for a\ngiven NLP application. We address this shortcoming by both surveying the\nliterature and through an empirical study. We develop a taxonomy of divergence\nmeasures consisting of three classes -- Information-theoretic, Geometric, and\nHigher-order measures and identify the relationships between them. Further, to\nunderstand the common use-cases of these measures, we recognise three novel\napplications -- 1) Data Selection, 2) Learning Representation, and 3) Decisions\nin the Wild -- and use it to organise our literature. From this, we identify\nthat Information-theoretic measures are prevalent for 1) and 3), and\nHigher-order measures are more common for 2). To further help researchers\nchoose appropriate measures to predict drop in performance -- an important\naspect of Decisions in the Wild, we perform correlation analysis spanning 130\ndomain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures\nidentified from our survey. To calculate these divergences, we consider the\ncurrent contextual word representations (CWR) and contrast with the older\ndistributed representations. We find that traditional measures over word\ndistributions still serve as strong baselines, while higher-order measures with\nCWR are effective.", "published": "2020-10-23 07:12:52", "link": "http://arxiv.org/abs/2010.12198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proof-theoretic aspects of NL$\u03bb$", "abstract": "We present a proof-theoretic analysis of the logic NL$\\lambda$ (Barker \\&\nShan 2014, Barker 2019). We notably introduce a novel calculus of proof nets\nand prove it is sound and complete with respect to the sequent calculus for the\nlogic. We study decidability and complexity of the logic using this new\ncalculus, proving a new upper bound for complexity of the logic (showing it is\nin NP) and a new lower bound for the class of formal language generated by the\nformalism (mildly context-sensitive languages extended with a permutation\nclosure operation). Finally, thanks to this new calculus, we present a novel\ncomparison between NL$\\lambda$ and the hybrid type-logical grammars of Kubota\n\\& Levine (2020). We show there is an unexpected convergence of the natural\nlanguage analyses proposed in the two formalism. In addition to studying the\nproof-theoretic properties of NL$\\lambda$, we greatly extends its linguistic\ncoverage.", "published": "2020-10-23 08:13:39", "link": "http://arxiv.org/abs/2010.12223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A scalable framework for learning from implicit user feedback to improve\n  natural language understanding in large-scale conversational AI systems", "abstract": "Natural Language Understanding (NLU) is an established component within a\nconversational AI or digital assistant system, and it is responsible for\nproducing semantic understanding of a user request. We propose a scalable and\nautomatic approach for improving NLU in a large-scale conversational AI system\nby leveraging implicit user feedback, with an insight that user interaction\ndata and dialog context have rich information embedded from which user\nsatisfaction and intention can be inferred. In particular, we propose a general\ndomain-agnostic framework for curating new supervision data for improving NLU\nfrom live production traffic. With an extensive set of experiments, we show the\nresults of applying the framework and improving NLU for a large-scale\nproduction system and show its impact across 10 domains.", "published": "2020-10-23 09:23:44", "link": "http://arxiv.org/abs/2010.12251v2", "categories": ["cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Pre-training with Meta Learning for Chinese Word Segmentation", "abstract": "Recent researches show that pre-trained models (PTMs) are beneficial to\nChinese Word Segmentation (CWS). However, PTMs used in previous works usually\nadopt language modeling as pre-training tasks, lacking task-specific prior\nsegmentation knowledge and ignoring the discrepancy between pre-training tasks\nand downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained\nmodel METASEG, which employs a unified architecture and incorporates meta\nlearning algorithm into a multi-criteria pre-training task. Empirical results\nshow that METASEG could utilize common prior segmentation knowledge from\ndifferent existing criteria and alleviate the discrepancy between pre-trained\nmodels and downstream CWS tasks. Besides, METASEG can achieve new\nstate-of-the-art performance on twelve widely-used CWS datasets and\nsignificantly improve model performance in low-resource settings.", "published": "2020-10-23 10:00:46", "link": "http://arxiv.org/abs/2010.12272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model", "abstract": "Inductive transfer learning has taken the entire NLP field by storm, with\nmodels such as BERT and BART setting new state of the art on countless NLU\ntasks. However, most of the available models and research have been conducted\nfor English. In this work, we introduce BARThez, the first large-scale\npretrained seq2seq model for French. Being based on BART, BARThez is\nparticularly well-suited for generative tasks. We evaluate BARThez on five\ndiscriminative tasks from the FLUE benchmark and two generative tasks from a\nnovel summarization dataset, OrangeSum, that we created for this research. We\nshow BARThez to be very competitive with state-of-the-art BERT-based French\nlanguage models such as CamemBERT and FlauBERT. We also continue the\npretraining of a multilingual BART on BARThez' corpus, and show our resulting\nmodel, mBARThez, to significantly boost BARThez' generative performance. Code,\ndata and models are publicly available.", "published": "2020-10-23 11:57:33", "link": "http://arxiv.org/abs/2010.12321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian\n  Tweets", "abstract": "In this paper, we present various pre-training strategies that aid in\nim-proving the accuracy of the sentiment classification task. We, at first,\npre-trainlanguage representation models using these strategies and then\nfine-tune them onthe downstream task. Experimental results on a time-balanced\ntweet evaluation setshow the improvement over the previous technique. We\nachieve 76% accuracy forsentiment analysis on Latvian tweets, which is a\nsubstantial improvement over pre-vious work", "published": "2020-10-23 13:45:33", "link": "http://arxiv.org/abs/2010.12401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond", "abstract": "Cross-lingual adaptation with multilingual pre-trained language models\n(mPTLMs) mainly consists of two lines of works: zero-shot approach and\ntranslation-based approach, which have been studied extensively on the\nsequence-level tasks. We further verify the efficacy of these cross-lingual\nadaptation approaches by evaluating their performances on more fine-grained\nsequence tagging tasks. After re-examining their strengths and drawbacks, we\npropose a novel framework to consolidate the zero-shot approach and the\ntranslation-based approach for better adaptation performance. Instead of simply\naugmenting the source data with the machine-translated data, we tailor-make a\nwarm-up mechanism to quickly update the mPTLMs with the gradients estimated on\na few translated data. Then, the adaptation approach is applied to the refined\nparameters and the cross-lingual transfer is performed in a warm-start way. The\nexperimental results on nine target languages demonstrate that our method is\nbeneficial to the cross-lingual adaptation of various sequence tagging tasks.", "published": "2020-10-23 13:47:01", "link": "http://arxiv.org/abs/2010.12405v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNER: Universal Named-Entity RecognitionFramework", "abstract": "We introduce the Universal Named-Entity Recognition (UNER)framework, a\n4-level classification hierarchy, and the methodology that isbeing adopted to\ncreate the first multilingual UNER corpus: the SETimesparallel corpus annotated\nfor named-entities. First, the English SETimescorpus will be annotated using\nexisting tools and knowledge bases. Afterevaluating the resulting annotations\nthrough crowdsourcing campaigns,they will be propagated automatically to other\nlanguages within the SE-Times corpora. Finally, as an extrinsic evaluation, the\nUNER multilin-gual dataset will be used to train and test available NER tools.\nAs part offuture research directions, we aim to increase the number of\nlanguages inthe UNER corpus and to investigate possible ways of integrating\nUNERwith available knowledge graphs to improve named-entity recognition.", "published": "2020-10-23 13:53:31", "link": "http://arxiv.org/abs/2010.12406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing", "abstract": "The de-facto standard decoding method for semantic parsing in recent years\nhas been to autoregressively decode the abstract syntax tree of the target\nprogram using a top-down depth-first traversal. In this work, we propose an\nalternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that\nconstructs at decoding step $t$ the top-$K$ sub-trees of height $\\leq t$. Our\nparser enjoys several benefits compared to top-down autoregressive parsing.\nFrom an efficiency perspective, bottom-up parsing allows to decode all\nsub-trees of a certain height in parallel, leading to logarithmic runtime\ncomplexity rather than linear. From a modeling perspective, a bottom-up parser\nlearns representations for meaningful semantic sub-programs at each step,\nrather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a\nchallenging zero-shot semantic parsing benchmark, and show that SmBoP leads to\na 2.2x speed-up in decoding time and a $\\sim$5x speed-up in training time,\ncompared to a semantic parser that uses autoregressive decoding. SmBoP obtains\n71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and\n69.5 exact match, comparable to the 69.6 exact match of the autoregressive\nRAT-SQL+GraPPa.", "published": "2020-10-23 14:02:32", "link": "http://arxiv.org/abs/2010.12412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Language Tools for Fifteen EU-official Under-resourced\n  Languages", "abstract": "This article presents the results of the evaluation campaign of language\ntools available for fifteen EU-official under-resourced languages. The\nevaluation was conducted within the MSC ITN CLEOPATRA action that aims at\nbuilding the cross-lingual event-centric knowledge processing on top of the\napplication of linguistic processing chains (LPCs) for at least 24 EU-official\nlanguages. In this campaign, we concentrated on three existing NLP platforms\n(Stanford CoreNLP, NLP Cube, UDPipe) that all provide models for\nunder-resourced languages and in this first run we covered 15 under-resourced\nlanguages for which the models were available. We present the design of the\nevaluation campaign and present the results as well as discuss them. We\nconsidered the difference between reported and our tested results within a\nsingle percentage point as being within the limits of acceptable tolerance and\nthus consider this result as reproducible. However, for a number of languages,\nthe results are below what was reported in the literature, and in some cases,\nour testing results are even better than the ones reported previously.\nParticularly problematic was the evaluation of NERC systems. One of the reasons\nis the absence of universally or cross-lingually applicable named entities\nclassification scheme that would serve the NERC task in different languages\nanalogous to the Universal Dependency scheme in parsing task. To build such a\nscheme has become one of our the future research directions.", "published": "2020-10-23 14:21:03", "link": "http://arxiv.org/abs/2010.12428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing Chains Inside a Cross-lingual Event-Centric\n  Knowledge Pipeline for European Union Under-resourced Languages", "abstract": "This article presents the strategy for developing a platform containing\nLanguage Processing Chains for European Union languages, consisting of\nTokenization to Parsing, also including Named Entity recognition andwith\naddition ofSentiment Analysis. These chains are part of the first step of an\nevent-centric knowledge processing pipeline whose aim is to process\nmultilingual media information about major events that can cause an impactin\nEurope and the rest of the world. Due to the differences in terms of\navailability of language resources for each language, we have built this\nstrategy in three steps, starting with processing chains for the well-resourced\nlanguages and finishing with the development of new modules for the\nunder-resourced ones. In order to classify all European Union official\nlanguages in terms of resources, we have analysed the size of annotated corpora\nas well as the existence of pre-trained models in mainstream Language\nProcessing tools, and we have combined this information with the proposed\nclassification published at META-NETwhitepaper series.", "published": "2020-10-23 14:26:30", "link": "http://arxiv.org/abs/2010.12433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateBERT: Retraining BERT for Abusive Language Detection in English", "abstract": "In this paper, we introduce HateBERT, a re-trained BERT model for abusive\nlanguage detection in English. The model was trained on RAL-E, a large-scale\ndataset of Reddit comments in English from communities banned for being\noffensive, abusive, or hateful that we have collected and made available to the\npublic. We present the results of a detailed comparison between a general\npre-trained language model and the abuse-inclined version obtained by\nretraining with posts from the banned communities on three English datasets for\noffensive, abusive language and hate speech detection tasks. In all datasets,\nHateBERT outperforms the corresponding general BERT model. We also discuss a\nbattery of experiments comparing the portability of the generic pre-trained\nlanguage model and its corresponding abusive language-inclined counterpart\nacross the datasets, indicating that portability is affected by compatibility\nof the annotated phenomena.", "published": "2020-10-23 15:14:14", "link": "http://arxiv.org/abs/2010.12472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Quality Assessment of Arguments", "abstract": "Several quality dimensions of natural language arguments have been\ninvestigated. Some are likely to be reflected in linguistic features (e.g., an\nargument's arrangement), whereas others depend on context (e.g., relevance) or\ntopic knowledge (e.g., acceptability). In this paper, we study the intrinsic\ncomputational assessment of 15 dimensions, i.e., only learning from an\nargument's text. In systematic experiments with eight feature types on an\nexisting corpus, we observe moderate but significant learning success for most\ndimensions. Rhetorical quality seems hardest to assess, and subjectivity\nfeatures turn out strong, although length bias in the corpus impedes full\nvalidity. We also find that human assessors differ more clearly to each other\nthan to our approach.", "published": "2020-10-23 15:16:10", "link": "http://arxiv.org/abs/2010.12473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Extent to which Summarization Evaluation Metrics\n  Measure the Information Quality of Summaries", "abstract": "Reference-based metrics such as ROUGE or BERTScore evaluate the content\nquality of a summary by comparing the summary to a reference. Ideally, this\ncomparison should measure the summary's information quality by calculating how\nmuch information the summaries have in common. In this work, we analyze the\ntoken alignments used by ROUGE and BERTScore to compare summaries and argue\nthat their scores largely cannot be interpreted as measuring information\noverlap, but rather the extent to which they discuss the same topics. Further,\nwe provide evidence that this result holds true for many other summarization\nevaluation metrics. The consequence of this result is that it means the\nsummarization community has not yet found a reliable automatic metric that\naligns with its research goal, to generate summaries with high-quality\ninformation. Then, we propose a simple and interpretable method of evaluating\nsummaries which does directly measure information overlap and demonstrate how\nit can be used to gain insights into model behavior that could not be provided\nby other methods alone.", "published": "2020-10-23 15:55:15", "link": "http://arxiv.org/abs/2010.12495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Helping users discover perspectives: Enhancing opinion mining with joint\n  topic models", "abstract": "Support or opposition concerning a debated claim such as abortion should be\nlegal can have different underlying reasons, which we call perspectives. This\npaper explores how opinion mining can be enhanced with joint topic modeling, to\nidentify distinct perspectives within the topic, providing an informative\noverview from unstructured text. We evaluate four joint topic models (TAM, JST,\nVODUM, and LAM) in a user study assessing human understandability of the\nextracted perspectives. Based on the results, we conclude that joint topic\nmodels such as TAM can discover perspectives that align with human judgments.\nMoreover, our results suggest that users are not influenced by their\npre-existing stance on the topic of abortion when interpreting the output of\ntopic models.", "published": "2020-10-23 16:13:06", "link": "http://arxiv.org/abs/2010.12505v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Robustness by Augmenting Training Sentences with\n  Predicate-Argument Structures", "abstract": "Existing NLP datasets contain various biases, and models tend to quickly\nlearn those biases, which in turn limits their robustness. Existing approaches\nto improve robustness against dataset biases mostly focus on changing the\ntraining objective so that models learn less from biased examples. Besides,\nthey mostly focus on addressing a specific bias, and while they improve the\nperformance on adversarial evaluation sets of the targeted bias, they may bias\nthe model in other ways, and therefore, hurt the overall robustness. In this\npaper, we propose to augment the input sentences in the training data with\ntheir corresponding predicate-argument structures, which provide a higher-level\nabstraction over different realizations of the same meaning and help the model\nto recognize important parts of sentences. We show that without targeting a\nspecific bias, our sentence augmentation improves the robustness of transformer\nmodels against multiple biases. In addition, we show that models can still be\nvulnerable to the lexical overlap bias, even when the training data does not\ncontain this bias, and that the sentence augmentation also improves the\nrobustness in this scenario. We will release our adversarial datasets to\nevaluate bias in such a scenario as well as our augmentation scripts at\nhttps://github.com/UKPLab/data-augmentation-for-robustness.", "published": "2020-10-23 16:22:05", "link": "http://arxiv.org/abs/2010.12510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Passage Retrieval with Improved Negative Contrast", "abstract": "In this paper we explore the effects of negative sampling in dual encoder\nmodels used to retrieve passages for automatic question answering. We explore\nfour negative sampling strategies that complement the straightforward random\nsampling of negatives, typically used to train dual encoder models. Out of the\nfour strategies, three are based on retrieval and one on heuristics. Our\nretrieval-based strategies are based on the semantic similarity and the lexical\noverlap between questions and passages. We train the dual encoder models in two\nstages: pre-training with synthetic data and fine tuning with domain-specific\ndata. We apply negative sampling to both stages. The approach is evaluated in\ntwo passage retrieval tasks. Even though it is not evident that there is one\nsingle sampling strategy that works best in all the tasks, it is clear that our\nstrategies contribute to improving the contrast between the response and all\nthe other passages. Furthermore, mixing the negatives from different strategies\nachieve performance on par with the best performing strategy in all tasks. Our\nresults establish a new state-of-the-art level of performance on two of the\nopen-domain question answering datasets that we evaluated.", "published": "2020-10-23 16:45:06", "link": "http://arxiv.org/abs/2010.12523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Open-Domain Questions of Varying Reasoning Steps from Text", "abstract": "We develop a unified system to answer directly from text open-domain\nquestions that may require a varying number of retrieval steps. We employ a\nsingle multi-task transformer model to perform all the necessary subtasks --\nretrieving supporting facts, reranking them, and predicting the answer from all\nretrieved documents -- in an iterative fashion. We avoid crucial assumptions of\nprevious work that do not transfer well to real-world settings, including\nexploiting knowledge of the fixed number of retrieval steps required to answer\neach question or using structured metadata like knowledge bases or web links\nthat have limited availability. Instead, we design a system that can answer\nopen-domain questions on any text collection without prior knowledge of\nreasoning complexity. To emulate this setting, we construct a new benchmark,\ncalled BeerQA, by combining existing one- and two-step datasets with a new\ncollection of 530 questions that require three Wikipedia pages to answer,\nunifying Wikipedia corpora versions in the process. We show that our model\ndemonstrates competitive performance on both existing benchmarks and this new\nbenchmark. We make the new benchmark available at https://beerqa.github.io/.", "published": "2020-10-23 16:51:09", "link": "http://arxiv.org/abs/2010.12527v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight\n  Gated Injection Method", "abstract": "Large pre-trained language models such as BERT have been the driving force\nbehind recent improvements across many NLP tasks. However, BERT is only trained\nto predict missing words - either behind masks or in the next sentence - and\nhas no knowledge of lexical, syntactic or semantic information beyond what it\npicks up through unsupervised pre-training. We propose a novel method to\nexplicitly inject linguistic knowledge in the form of word embeddings into any\nlayer of a pre-trained BERT. Our performance improvements on multiple semantic\nsimilarity datasets when injecting dependency-based and counter-fitted\nembeddings indicate that such information is beneficial and currently missing\nfrom the original model. Our qualitative analysis shows that counter-fitted\nembedding injection particularly helps with cases involving synonym pairs.", "published": "2020-10-23 17:00:26", "link": "http://arxiv.org/abs/2010.12532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual BERT Post-Pretraining Alignment", "abstract": "We propose a simple method to align multilingual contextual embeddings as a\npost-pretraining step for improved zero-shot cross-lingual transferability of\nthe pretrained models. Using parallel data, our method aligns embeddings on the\nword level through the recently proposed Translation Language Modeling\nobjective as well as on the sentence level via contrastive learning and random\ninput shuffling. We also perform sentence-level code-switching with English\nwhen finetuning on downstream tasks. On XNLI, our best model (initialized from\nmBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves\ncomparable result to XLM for translate-train while using less than 18% of the\nsame parallel data and 31% less model parameters. On MLQA, our model\noutperforms XLM-R_Base that has 57% more parameters than ours.", "published": "2020-10-23 17:14:41", "link": "http://arxiv.org/abs/2010.12547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concealed Data Poisoning Attacks on NLP Models", "abstract": "Adversarial attacks alter NLP model predictions by perturbing test-time\ninputs. However, it is much less understood whether, and how, predictions can\nbe manipulated with small, concealed changes to the training data. In this\nwork, we develop a new data poisoning attack that allows an adversary to\ncontrol model predictions whenever a desired trigger phrase is present in the\ninput. For instance, we insert 50 poison examples into a sentiment model's\ntraining set that causes the model to frequently predict Positive whenever the\ninput contains \"James Bond\". Crucially, we craft these poison examples using a\ngradient-based procedure so that they do not mention the trigger phrase. We\nalso apply our poison attack to language modeling (\"Apple iPhone\" triggers\nnegative generations) and machine translation (\"iced coffee\" mistranslated as\n\"hot coffee\"). We conclude by proposing three defenses that can mitigate our\nattack at some cost in prediction accuracy or extra human annotation.", "published": "2020-10-23 17:47:06", "link": "http://arxiv.org/abs/2010.12563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DICT-MLM: Improved Multilingual Pre-Training using Bilingual\n  Dictionaries", "abstract": "Pre-trained multilingual language models such as mBERT have shown immense\ngains for several natural language processing (NLP) tasks, especially in the\nzero-shot cross-lingual setting. Most, if not all, of these pre-trained models\nrely on the masked-language modeling (MLM) objective as the key language\nlearning objective. The principle behind these approaches is that predicting\nthe masked words with the help of the surrounding text helps learn potent\ncontextualized representations. Despite the strong representation learning\ncapability enabled by MLM, we demonstrate an inherent limitation of MLM for\nmultilingual representation learning. In particular, by requiring the model to\npredict the language-specific token, the MLM objective disincentivizes learning\na language-agnostic representation -- which is a key goal of multilingual\npre-training. Therefore to encourage better cross-lingual representation\nlearning we propose the DICT-MLM method. DICT-MLM works by incentivizing the\nmodel to be able to predict not just the original masked word, but potentially\nany of its cross-lingual synonyms as well. Our empirical analysis on multiple\ndownstream tasks spanning 30+ languages, demonstrates the efficacy of the\nproposed approach and its ability to learn better multilingual representations.", "published": "2020-10-23 17:53:11", "link": "http://arxiv.org/abs/2010.12566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ranking Creative Language Characteristics in Small Data Scenarios", "abstract": "The ability to rank creative natural language provides an important general\ntool for downstream language understanding and generation. However, current\ndeep ranking models require substantial amounts of labeled data that are\ndifficult and expensive to obtain for different domains, languages and creative\ncharacteristics. A recent neural approach, the DirectRanker, promises to reduce\nthe amount of training data needed but its application to text isn't fully\nexplored. We therefore adapt the DirectRanker to provide a new deep model for\nranking creative language with small data. We compare DirectRanker with a\nBayesian approach, Gaussian process preference learning (GPPL), which has\npreviously been shown to work well with sparse data. Our experiments with\nsparse training data show that while the performance of standard neural ranking\napproaches collapses with small training datasets, DirectRanker remains\neffective. We find that combining DirectRanker with GPPL increases performance\nacross different settings by leveraging the complementary benefits of both\nmodels. Our combined approach outperforms the previous state-of-the-art on\nhumor and metaphor novelty tasks, increasing Spearman's $\\rho$ by 14% and 16%\non average.", "published": "2020-10-23 18:57:47", "link": "http://arxiv.org/abs/2010.12613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling with Contextualized Word Representation Clusters", "abstract": "Clustering token-level contextualized word representations produces output\nthat shares many similarities with topic models for English text collections.\nUnlike clusterings of vocabulary-level word embeddings, the resulting models\nmore naturally capture polysemy and can be used as a way of organizing\ndocuments. We evaluate token clusterings trained from several different output\nlayers of popular contextualized language models. We find that BERT and GPT-2\nproduce high quality clusterings, but RoBERTa does not. These cluster models\nare simple, reliable, and can perform as well as, if not better than, LDA topic\nmodels, maintaining high topic quality even when the number of topics is large\nrelative to the size of the local collection.", "published": "2020-10-23 19:16:59", "link": "http://arxiv.org/abs/2010.12626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages", "abstract": "Good quality monolingual word embeddings (MWEs) can be built for languages\nwhich have large amounts of unlabeled text. MWEs can be aligned to bilingual\nspaces using only a few thousand word translation pairs. For low resource\nlanguages training MWEs monolingually results in MWEs of poor quality, and thus\npoor bilingual word embeddings (BWEs) as well. This paper proposes a new\napproach for building BWEs in which the vector space of the high resource\nsource language is used as a starting point for training an embedding space for\nthe low resource target language. By using the source vectors as anchors the\nvector spaces are automatically aligned during training. We experiment on\nEnglish-German, English-Hiligaynon and English-Macedonian. We show that our\napproach results not only in improved BWEs and bilingual lexicon induction\nperformance, but also in improved target language MWE quality as measured using\nmonolingual word similarity.", "published": "2020-10-23 19:17:00", "link": "http://arxiv.org/abs/2010.12627v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did You Ask a Good Question? A Cross-Domain Question Intention\n  Classification Benchmark for Text-to-SQL", "abstract": "Neural models have achieved significant results on the text-to-SQL task, in\nwhich most current work assumes all the input questions are legal and generates\na SQL query for any input. However, in the real scenario, users can input any\ntext that may not be able to be answered by a SQL query. In this work, we\npropose TriageSQL, the first cross-domain text-to-SQL question intention\nclassification benchmark that requires models to distinguish four types of\nunanswerable questions from answerable questions. The baseline RoBERTa model\nachieves a 60% F1 score on the test set, demonstrating the need for further\nimprovement on this task. Our dataset is available at\nhttps://github.com/chatc/TriageSQL.", "published": "2020-10-23 19:36:57", "link": "http://arxiv.org/abs/2010.12634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question\n  Answering", "abstract": "Coupled with the availability of large scale datasets, deep learning\narchitectures have enabled rapid progress on the Question Answering task.\nHowever, most of those datasets are in English, and the performances of\nstate-of-the-art multilingual models are significantly lower when evaluated on\nnon-English data. Due to high data collection costs, it is not realistic to\nobtain annotated data for each language one desires to support.\n  We propose a method to improve the Cross-lingual Question Answering\nperformance without requiring additional annotated data, leveraging Question\nGeneration models to produce synthetic samples in a cross-lingual fashion. We\nshow that the proposed method allows to significantly outperform the baselines\ntrained on English data only. We report a new state-of-the-art on four\nmultilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).", "published": "2020-10-23 20:09:01", "link": "http://arxiv.org/abs/2010.12643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rapid Domain Adaptation for Machine Translation with Monolingual Data", "abstract": "One challenge of machine translation is how to quickly adapt to unseen\ndomains in face of surging events like COVID-19, in which case timely and\naccurate translation of in-domain information into multiple languages is\ncritical but little parallel data is available yet. In this paper, we propose\nan approach that enables rapid domain adaptation from the perspective of\nunsupervised translation. Our proposed approach only requires in-domain\nmonolingual data and can be quickly applied to a preexisting translation system\ntrained on general domain, reaching significant gains on in-domain translation\nquality with little or no drop on general-domain. We also propose an effective\nprocedure of simultaneous adaptation for multiple domains and languages. To the\nbest of our knowledge, this is the first attempt that aims to address\nunsupervised multilingual domain adaptation.", "published": "2020-10-23 20:31:37", "link": "http://arxiv.org/abs/2010.12652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Adequate Distractors for Multiple-Choice Questions", "abstract": "This paper presents a novel approach to automatic generation of adequate\ndistractors for a given question-answer pair (QAP) generated from a given\narticle to form an adequate multiple-choice question (MCQ). Our method is a\ncombination of part-of-speech tagging, named-entity tagging, semantic-role\nlabeling, regular expressions, domain knowledge bases, word embeddings, word\nedit distance, WordNet, and other algorithms. We use the US SAT (Scholastic\nAssessment Test) practice reading tests as a dataset to produce QAPs and\ngenerate three distractors for each QAP to form an MCQ. We show that, via\nexperiments and evaluations by human judges, each MCQ has at least one adequate\ndistractor and 84\\% of MCQs have three adequate distractors.", "published": "2020-10-23 20:47:58", "link": "http://arxiv.org/abs/2010.12658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Contextualized Word Embeddings", "abstract": "Static word embeddings that represent words by a single vector cannot capture\nthe variability of word meaning in different linguistic and extralinguistic\ncontexts. Building on prior work on contextualized and dynamic word embeddings,\nwe introduce dynamic contextualized word embeddings that represent words as a\nfunction of both linguistic and extralinguistic context. Based on a pretrained\nlanguage model (PLM), dynamic contextualized word embeddings model time and\nsocial space jointly, which makes them attractive for a range of NLP tasks\ninvolving semantic variability. We highlight potential application scenarios by\nmeans of qualitative and quantitative analyses on four English datasets.", "published": "2020-10-23 22:02:40", "link": "http://arxiv.org/abs/2010.12684v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced\n  Language Model Pre-training", "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph\n(KG) triples into natural text, focused on domain-specific benchmark datasets.\nIn this paper, however, we verbalize the entire English Wikidata KG, and\ndiscuss the unique challenges associated with a broad, open-domain, large-scale\nverbalization. We further show that verbalizing a comprehensive, encyclopedic\nKG like Wikidata can be used to integrate structured KGs and natural language\ncorpora. In contrast to the many architectures that have been developed to\nintegrate these two sources, our approach converts the KG into natural text,\nallowing it to be seamlessly integrated into existing language models. It\ncarries the further advantages of improved factual accuracy and reduced\ntoxicity in the resulting language model. We evaluate this approach by\naugmenting the retrieval corpus in a retrieval language model and showing\nsignificant improvements on the knowledge intensive tasks of open domain QA and\nthe LAMA knowledge probe.", "published": "2020-10-23 22:14:50", "link": "http://arxiv.org/abs/2010.12688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AQuaMuSe: Automatically Generating Datasets for Query-Based\n  Multi-Document Summarization", "abstract": "Summarization is the task of compressing source document(s) into coherent and\nsuccinct passages. This is a valuable tool to present users with concise and\naccurate sketch of the top ranked documents related to their queries.\nQuery-based multi-document summarization (qMDS) addresses this pervasive need,\nbut the research is severely limited due to lack of training and evaluation\ndatasets as existing single-document and multi-document summarization datasets\nare inadequate in form and scale. We propose a scalable approach called\nAQuaMuSe to automatically mine qMDS examples from question answering datasets\nand large document corpora. Our approach is unique in the sense that it can\ngeneral a dual dataset -- for extractive and abstractive summaries both. We\npublicly release a specific instance of an AQuaMuSe dataset with 5,519\nquery-based summaries, each associated with an average of 6 input documents\nselected from an index of 355M documents from Common Crawl. Extensive\nevaluation of the dataset along with baseline summarization model experiments\nare provided.", "published": "2020-10-23 22:38:18", "link": "http://arxiv.org/abs/2010.12694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What\n  Works, What Doesn't, and What is Really Necessary", "abstract": "The introduction of pre-trained transformer-based contextualized word\nembeddings has led to considerable improvements in the accuracy of graph-based\nparsers for frameworks such as Universal Dependencies (UD). However, previous\nworks differ in various dimensions, including their choice of pre-trained\nlanguage models and whether they use LSTM layers. With the aims of\ndisentangling the effects of these choices and identifying a simple yet widely\napplicable architecture, we introduce STEPS, a new modular graph-based\ndependency parser. Using STEPS, we perform a series of analyses on the UD\ncorpora of a diverse set of languages. We find that the choice of pre-trained\nembeddings has by far the greatest impact on parser performance and identify\nXLM-R as a robust choice across the languages in our study. Adding LSTM layers\nprovides no benefits when using transformer-based embeddings. A multi-task\ntraining setup outputting additional UD features may contort results. Taking\nthese insights together, we propose a simple but widely applicable parser\narchitecture and configuration, achieving new state-of-the-art results (in\nterms of LAS) for 10 out of 12 diverse languages.", "published": "2020-10-23 22:58:26", "link": "http://arxiv.org/abs/2010.12699v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Recognize Dialect Features", "abstract": "Building NLP systems that serve everyone requires accounting for dialect\ndifferences. But dialects are not monolithic entities: rather, distinctions\nbetween and within dialects are captured by the presence, absence, and\nfrequency of dozens of dialect features in speech and text, such as the\ndeletion of the copula in \"He {} running\". In this paper, we introduce the task\nof dialect feature detection, and present two multitask learning approaches,\nboth based on pretrained transformers. For most dialects, large-scale annotated\ncorpora for these features are unavailable, making it difficult to train\nrecognizers. We train our models on a small number of minimal pairs, building\non how linguists typically define dialect features. Evaluation on a test set of\n22 dialect features of Indian English demonstrates that these models learn to\nrecognize many features with high accuracy, and that a few minimal pairs can be\nas effective for training as thousands of labeled examples. We also demonstrate\nthe downstream applicability of dialect feature detection both as a measure of\ndialect density and as a dialect classifier.", "published": "2020-10-23 23:25:00", "link": "http://arxiv.org/abs/2010.12707v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can images help recognize entities? A study of the role of images for\n  Multimodal NER", "abstract": "Multimodal named entity recognition (MNER) requires to bridge the gap between\nlanguage understanding and visual context. While many multimodal neural\ntechniques have been proposed to incorporate images into the MNER task, the\nmodel's ability to leverage multimodal interactions remains poorly understood.\nIn this work, we conduct in-depth analyses of existing multimodal fusion\ntechniques from different perspectives and describe the scenarios where adding\ninformation from the image does not always boost performance. We also study the\nuse of captions as a way to enrich the context for MNER. Experiments on three\ndatasets from popular social platforms expose the bottleneck of existing\nmultimodal models and the situations where using captions is beneficial.", "published": "2020-10-23 23:41:51", "link": "http://arxiv.org/abs/2010.12712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling\n  for Natural Language Understanding", "abstract": "Coarse-grained linguistic information, such as named entities or phrases,\nfacilitates adequately representation learning in pre-training. Previous works\nmainly focus on extending the objective of BERT's Masked Language Modeling\n(MLM) from masking individual tokens to contiguous sequences of n tokens. We\nargue that such contiguously masking method neglects to model the\nintra-dependencies and inter-relation of coarse-grained linguistic information.\nAs an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method\nto enhance the integration of coarse-grained information into pre-training. In\nERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram\nidentities rather than contiguous sequences of n tokens. Furthermore,\nERNIE-Gram employs a generator model to sample plausible n-gram identities as\noptional n-gram masks and predict them in both coarse-grained and fine-grained\nmanners to enable comprehensive n-gram prediction and relation modeling. We\npre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19\ndownstream tasks. Experimental results show that ERNIE-Gram outperforms\nprevious pre-training models like XLNet and RoBERTa by a large margin, and\nachieves comparable results with state-of-the-art methods. The source codes and\npre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.", "published": "2020-10-23 03:42:20", "link": "http://arxiv.org/abs/2010.12148v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Show and Speak: Directly Synthesize Spoken Description of Images", "abstract": "This paper proposes a new model, referred to as the show and speak (SAS)\nmodel that, for the first time, is able to directly synthesize spoken\ndescriptions of images, bypassing the need for any text or phonemes. The basic\nstructure of SAS is an encoder-decoder architecture that takes an image as\ninput and predicts the spectrogram of speech that describes this image. The\nfinal speech audio is obtained from the predicted spectrogram via WaveNet.\nExtensive experiments on the public benchmark database Flickr8k demonstrate\nthat the proposed SAS is able to synthesize natural spoken descriptions for\nimages, indicating that synthesizing spoken descriptions for images while\nbypassing text and phonemes is feasible.", "published": "2020-10-23 09:53:01", "link": "http://arxiv.org/abs/2010.12267v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken\n  Language Understanding", "abstract": "Language model pre-training has shown promising results in various downstream\ntasks. In this context, we introduce a cross-modal pre-trained language model,\ncalled Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language\nunderstanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text\nas an input, ST-BERT learns a contextualized cross-modal alignment via our two\nproposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and\nCross-modal Conditioned Language Modeling (CM-CLM). Experimental results on\nthree benchmarks present that our approach is effective for various SLU\ndatasets and shows a surprisingly marginal performance degradation even when 1%\nof the training data are available. Also, our method shows further SLU\nperformance gain via domain-adaptive pre-training with domain-specific\nspeech-text pair data.", "published": "2020-10-23 10:28:20", "link": "http://arxiv.org/abs/2010.12283v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input\n  Representations", "abstract": "Combining several embeddings typically improves performance in downstream\ntasks as different embeddings encode different information. It has been shown\nthat even models using embeddings from transformers still benefit from the\ninclusion of standard word embeddings. However, the combination of embeddings\nof different types and dimensions is challenging. As an alternative to\nattention-based meta-embeddings, we propose feature-based adversarial\nmeta-embeddings (FAME) with an attention function that is guided by features\nreflecting word-specific properties, such as shape and frequency, and show that\nthis is beneficial to handle subword-based embeddings. In addition, FAME uses\nadversarial training to optimize the mappings of differently-sized embeddings\nto the same space. We demonstrate that FAME works effectively across languages\nand domains for sequence labeling and sentence classification, in particular in\nlow-resource settings. FAME sets the new state of the art for POS tagging in 27\nlanguages, various NER settings and question classification in different\ndomains.", "published": "2020-10-23 11:16:53", "link": "http://arxiv.org/abs/2010.12305v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Recent Approaches for Natural Language Processing in\n  Low-Resource Scenarios", "abstract": "Deep neural networks and huge language models are becoming omnipresent in\nnatural language applications. As they are known for requiring large amounts of\ntraining data, there is a growing body of work to improve the performance in\nlow-resource settings. Motivated by the recent fundamental changes towards\nneural models and the popular pre-train and fine-tune paradigm, we survey\npromising approaches for low-resource natural language processing. After a\ndiscussion about the different dimensions of data availability, we give a\nstructured overview of methods that enable learning when training data is\nsparse. This includes mechanisms to create additional labeled data like data\naugmentation and distant supervision as well as transfer learning settings that\nreduce the need for target supervision. A goal of our survey is to explain how\nthese methods differ in their requirements as understanding them is essential\nfor choosing a technique suited for a specific low-resource setting. Further\nkey aspects of this work are to highlight open issues and to outline promising\ndirections for future research.", "published": "2020-10-23 11:22:01", "link": "http://arxiv.org/abs/2010.12309v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLNDE at CANTEMIST: Neural Sequence Labeling and Parsing Approaches for\n  Clinical Concept Extraction", "abstract": "The recognition and normalization of clinical information, such as tumor\nmorphology mentions, is an important, but complex process consisting of\nmultiple subtasks. In this paper, we describe our system for the CANTEMIST\nshared task, which is able to extract, normalize and rank ICD codes from\nSpanish electronic health records using neural sequence labeling and parsing\napproaches with context-aware embeddings. Our best system achieves 85.3 F1,\n76.7 F1, and 77.0 MAP for the three tasks, respectively.", "published": "2020-10-23 11:59:28", "link": "http://arxiv.org/abs/2010.12322v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "abstract": "The experimental landscape in natural language processing for social media is\ntoo fragmented. Each year, new shared tasks and datasets are proposed, ranging\nfrom classics like sentiment analysis to irony detection or emoji prediction.\nTherefore, it is unclear what the current state of the art is, as there is no\nstandardized evaluation protocol, neither a strong set of baselines trained on\nsuch domain-specific data. In this paper, we propose a new evaluation framework\n(TweetEval) consisting of seven heterogeneous Twitter-specific classification\ntasks. We also provide a strong set of baselines as starting point, and compare\ndifferent language modeling pre-training strategies. Our initial experiments\nshow the effectiveness of starting off with existing pre-trained generic\nlanguage models, and continue training them on Twitter corpora.", "published": "2020-10-23 14:11:04", "link": "http://arxiv.org/abs/2010.12421v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Generating Plausible Counterfactual Explanations for Deep Transformers\n  in Financial Text Classification", "abstract": "Corporate mergers and acquisitions (M&A) account for billions of dollars of\ninvestment globally every year, and offer an interesting and challenging domain\nfor artificial intelligence. However, in these highly sensitive domains, it is\ncrucial to not only have a highly robust and accurate model, but be able to\ngenerate useful explanations to garner a user's trust in the automated system.\nRegrettably, the recent research regarding eXplainable AI (XAI) in financial\ntext classification has received little to no attention, and many current\nmethods for generating textual-based explanations result in highly implausible\nexplanations, which damage a user's trust in the system. To address these\nissues, this paper proposes a novel methodology for producing plausible\ncounterfactual explanations, whilst exploring the regularization benefits of\nadversarial training on language models in the domain of FinTech. Exhaustive\nquantitative experiments demonstrate that not only does this approach improve\nthe model accuracy when compared to the current state-of-the-art and human\nperformance, but it also generates counterfactual explanations which are\nsignificantly more plausible based on human trials.", "published": "2020-10-23 16:29:26", "link": "http://arxiv.org/abs/2010.12512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Transformer Growth for Progressive BERT Training", "abstract": "Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances", "published": "2020-10-23 17:44:59", "link": "http://arxiv.org/abs/2010.12562v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multi-hop Question Answering by Question Generation", "abstract": "Obtaining training data for multi-hop question answering (QA) is\ntime-consuming and resource-intensive. We explore the possibility to train a\nwell-performed multi-hop QA model without referencing any human-labeled\nmulti-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose\nMQA-QG, an unsupervised framework that can generate human-like multi-hop\ntraining data from both homogeneous and heterogeneous data sources. MQA-QG\ngenerates questions by first selecting/generating relevant information from\neach data source and then integrating the multiple information to form a\nmulti-hop question. Using only generated training data, we can train a\ncompetent multi-hop QA which achieves 61% and 83% of the supervised learning\nperformance for the HybridQA and the HotpotQA dataset, respectively. We also\nshow that pretraining the QA system with the generated data would greatly\nreduce the demand for human-annotated training data. Our codes are publicly\navailable at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.", "published": "2020-10-23 19:13:47", "link": "http://arxiv.org/abs/2010.12623v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative analysis of word embeddings in assessing semantic similarity\n  of complex sentences", "abstract": "Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.", "published": "2020-10-23 19:55:11", "link": "http://arxiv.org/abs/2010.12637v3", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive\n  Transducer", "abstract": "Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end\nacoustic model that extends the standard Recurrent Neural Network Transducer\n(RNN-T) for the purpose of the external language model (LM) fusion. In HAT, the\nblank probability and the label probability are estimated using two separate\nprobability distributions, which provides a more accurate solution for internal\nLM score estimation, and thus works better when combining with an external LM.\nPrevious work mainly focuses on HAT model training with the negative\nlog-likelihood loss, while in this paper, we study the minimum word error rate\n(MWER) training of HAT -- a criterion that is closer to the evaluation metric\nfor speech recognition, and has been successfully applied to other types of\nend-to-end models such as sequence-to-sequence (S2S) and RNN-T models. From\nexperiments with around 30,000 hours of training data, we show that MWER\ntraining can improve the accuracy of HAT models, while at the same time,\nimproving the robustness of the model against the decoding hyper-parameters\nsuch as length normalization and decoding beam during inference.", "published": "2020-10-23 21:16:30", "link": "http://arxiv.org/abs/2010.12673v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Overcoming Conflicting Data when Updating a Neural Semantic Parser", "abstract": "In this paper, we explore how to use a small amount of new data to update a\ntask-oriented semantic parsing model when the desired output for some examples\nhas changed. When making updates in this way, one potential problem that arises\nis the presence of conflicting data, or out-of-date labels in the original\ntraining set. To evaluate the impact of this understudied problem, we propose\nan experimental setup for simulating changes to a neural semantic parser. We\nshow that the presence of conflicting data greatly hinders learning of an\nupdate, then explore several methods to mitigate its effect. Our multi-task and\ndata selection methods lead to large improvements in model accuracy compared to\na naive data-mixing strategy, and our best method closes 86% of the accuracy\ngap between this baseline and an oracle upper bound.", "published": "2020-10-23 21:19:03", "link": "http://arxiv.org/abs/2010.12675v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Differentiable Relaxation of Graph Segmentation and Alignment for AMR\n  Parsing", "abstract": "Abstract Meaning Representations (AMR) are a broad-coverage semantic\nformalism which represents sentence meaning as a directed acyclic graph. To\ntrain most AMR parsers, one needs to segment the graph into subgraphs and align\neach such subgraph to a word in a sentence; this is normally done at\npreprocessing, relying on hand-crafted rules. In contrast, we treat both\nalignment and segmentation as latent variables in our model and induce them as\npart of end-to-end training.\n  As marginalizing over the structured latent variables is infeasible, we use\nthe variational autoencoding framework.\n  To ensure end-to-end differentiable optimization, we introduce a\ndifferentiable relaxation of the segmentation and alignment problems. We\nobserve that inducing segmentation yields substantial gains over using a\n`greedy' segmentation heuristic. The performance of our method also approaches\nthat of a model that relies on the segmentation rules of\n\\citet{lyu-titov-2018-amr}, which were hand-crafted to handle individual AMR\nconstructions.", "published": "2020-10-23 21:22:50", "link": "http://arxiv.org/abs/2010.12676v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long Document Ranking with Query-Directed Sparse Transformer", "abstract": "The computing cost of transformer self-attention often necessitates breaking\nlong documents to fit in pretrained models in document ranking tasks. In this\npaper, we design Query-Directed Sparse attention that induces IR-axiomatic\nstructures in transformer self-attention. Our model, QDS-Transformer, enforces\nthe principle properties desired in ranking: local contextualization,\nhierarchical representation, and query-oriented proximity matching, while it\nalso enjoys efficiency from sparsity. Experiments on one fully supervised and\nthree few-shot TREC document ranking benchmarks demonstrate the consistent and\nrobust advantage of QDS-Transformer over previous approaches, as they either\nretrofit long documents into BERT or use sparse attention without emphasizing\nIR principles. We further quantify the computing complexity and demonstrates\nthat our sparse attention with TVM implementation is twice more efficient than\nthe fully-connected self-attention. All source codes, trained model, and\npredictions of this work are available at\nhttps://github.com/hallogameboy/QDS-Transformer.", "published": "2020-10-23 21:57:56", "link": "http://arxiv.org/abs/2010.12683v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exercise Hierarchical Feature Enhanced Knowledge Tracing", "abstract": "Knowledge tracing is a fundamental task in the computer-aid educational\nsystem. In this paper, we propose a hierarchical exercise feature enhanced\nknowledge tracing framework, which could enhance the ability of knowledge\ntracing by incorporating knowledge distribution, semantic features, and\ndifficulty features from exercise text. Extensive experiments show the high\nperformance of our framework.", "published": "2020-10-23 12:16:07", "link": "http://arxiv.org/abs/2011.09867v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Knowledge Graph Embedding with Atrous Convolution and Residual Learning", "abstract": "Knowledge graph embedding is an important task and it will benefit lots of\ndownstream applications. Currently, deep neural networks based methods achieve\nstate-of-the-art performance. However, most of these existing methods are very\ncomplex and need much time for training and inference. To address this issue,\nwe propose a simple but effective atrous convolution based knowledge graph\nembedding method. Compared with existing state-of-the-art methods, our method\nhas following main characteristics. First, it effectively increases feature\ninteractions by using atrous convolutions. Second, to address the original\ninformation forgotten issue and vanishing/exploding gradient issue, it uses the\nresidual learning method. Third, it has simpler structure but much higher\nparameter efficiency. We evaluate our method on six benchmark datasets with\ndifferent evaluation metrics. Extensive experiments show that our model is very\neffective. On these diverse datasets, it achieves better results than the\ncompared state-of-the-art methods on most of evaluation metrics. The source\ncodes of our model could be found at https://github.com/neukg/AcrE.", "published": "2020-10-23 00:57:23", "link": "http://arxiv.org/abs/2010.12121v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Lightweight Generative Adversarial Networks for Text-Guided Image\n  Manipulation", "abstract": "We propose a novel lightweight generative adversarial network for efficient\nimage manipulation using natural language descriptions. To achieve this, a new\nword-level discriminator is proposed, which provides the generator with\nfine-grained training feedback at word-level, to facilitate training a\nlightweight generator that has a small number of parameters, but can still\ncorrectly focus on specific visual attributes of an image, and then edit them\nwithout affecting other contents that are not described in the text.\nFurthermore, thanks to the explicit training signal related to each word, the\ndiscriminator can also be simplified to have a lightweight structure. Compared\nwith the state of the art, our method has a much smaller number of parameters,\nbut still achieves a competitive manipulation performance. Extensive\nexperimental results demonstrate that our method can better disentangle\ndifferent visual attributes, then correctly map them to corresponding semantic\nwords, and thus achieve a more accurate image modification using natural\nlanguage descriptions.", "published": "2020-10-23 02:43:02", "link": "http://arxiv.org/abs/2010.12136v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Transformer-based End-to-End Speech Recognition with Local Dense\n  Synthesizer Attention", "abstract": "Recently, several studies reported that dot-product selfattention (SA) may\nnot be indispensable to the state-of-theart Transformer models. Motivated by\nthe fact that dense synthesizer attention (DSA), which dispenses with dot\nproducts and pairwise interactions, achieved competitive results in many\nlanguage processing tasks, in this paper, we first propose a DSA-based speech\nrecognition, as an alternative to SA. To reduce the computational complexity\nand improve the performance, we further propose local DSA (LDSA) to restrict\nthe attention scope of DSA to a local range around the current central frame\nfor speech recognition. Finally, we combine LDSA with SA to extract the local\nand global information simultaneously. Experimental results on the Ai-shell1\nMandarine speech recognition corpus show that the proposed LDSA-Transformer\nachieves a character error rate (CER) of 6.49%, which is slightly better than\nthat of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less\ncomputation than the SATransformer. The proposed combination method not only\nachieves a CER of 6.18%, which significantly outperforms the SA-Transformer,\nbut also has roughly the same number of parameters and computational complexity\nas the latter. The implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA.", "published": "2020-10-23 04:13:44", "link": "http://arxiv.org/abs/2010.12155v3", "categories": ["cs.SD", "cs.CL", "eess.AS", "68T10"], "primary_category": "cs.SD"}
{"title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for\n  Kinyarwanda and Kirundi", "abstract": "Recent progress in text classification has been focused on high-resource\nlanguages such as English and Chinese. For low-resource languages, amongst them\nmost African languages, the lack of well-annotated data and effective\npreprocessing, is hindering the progress and the transfer of successful\nmethods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS)\nfor multi-class classification of news articles in Kinyarwanda and Kirundi, two\nlow-resource African languages. The two languages are mutually intelligible,\nbut while Kinyarwanda has been studied in Natural Language Processing (NLP) to\nsome extent, this work constitutes the first study on Kirundi. Along with the\ndatasets, we provide statistics, guidelines for preprocessing, and monolingual\nand cross-lingual baseline models. Our experiments show that training\nembeddings on the relatively higher-resourced Kinyarwanda yields successful\ncross-lingual transfer to Kirundi. In addition, the design of the created\ndatasets allows for a wider use in NLP beyond text classification in future\nstudies, such as representation learning, cross-lingual learning with more\ndistant languages, or as base for new annotations for tasks such as parsing,\nPOS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are\npublicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .", "published": "2020-10-23 05:37:42", "link": "http://arxiv.org/abs/2010.12174v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Don't shoot butterfly with rifles: Multi-channel Continuous Speech\n  Separation with Early Exit Transformer", "abstract": "With its strong modeling capacity that comes from a multi-head and\nmulti-layer structure, Transformer is a very powerful model for learning a\nsequential representation and has been successfully applied to speech\nseparation recently. However, multi-channel speech separation sometimes does\nnot necessarily need such a heavy structure for all time frames especially when\nthe cross-talker challenge happens only occasionally. For example, in\nconversation scenarios, most regions contain only a single active speaker,\nwhere the separation task downgrades to a single speaker enhancement problem.\nIt turns out that using a very deep network structure for dealing with signals\nwith a low overlap ratio not only negatively affects the inference efficiency\nbut also hurts the separation performance. To deal with this problem, we\npropose an early exit mechanism, which enables the Transformer model to handle\ndifferent cases with adaptive depth. Experimental results indicate that not\nonly does the early exit mechanism accelerate the inference, but it also\nimproves the accuracy.", "published": "2020-10-23 06:21:11", "link": "http://arxiv.org/abs/2010.12180v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Any-to-One Sequence-to-Sequence Voice Conversion using Self-Supervised\n  Discrete Speech Representations", "abstract": "We present a novel approach to any-to-one (A2O) voice conversion (VC) in a\nsequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker,\nincluding those unseen during training, to a fixed target speaker. We utilize\nvq-wav2vec (VQW2V), a discretized self-supervised speech representation that\nwas learned from massive unlabeled data, which is assumed to be\nspeaker-independent and well corresponds to underlying linguistic contents.\nGiven a training dataset of the target speaker, we extract VQW2V and acoustic\nfeatures to estimate a seq2seq mapping function from the former to the latter.\nWith the help of a pretraining method and a newly designed postprocessing\ntechnique, our model can be generalized to only 5 min of data, even\noutperforming the same model trained with parallel data.", "published": "2020-10-23 08:34:52", "link": "http://arxiv.org/abs/2010.12231v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning Framework for Measuring the Digital Strategy of Companies\n  from Earnings Calls", "abstract": "Companies today are racing to leverage the latest digital technologies, such\nas artificial intelligence, blockchain, and cloud computing. However, many\ncompanies report that their strategies did not achieve the anticipated business\nresults. This study is the first to apply state of the art NLP models on\nunstructured data to understand the different clusters of digital strategy\npatterns that companies are Adopting. We achieve this by analyzing earnings\ncalls from Fortune Global 500 companies between 2015 and 2019. We use\nTransformer based architecture for text classification which show a better\nunderstanding of the conversation context. We then investigate digital strategy\npatterns by applying clustering analysis. Our findings suggest that Fortune 500\ncompanies use four distinct strategies which are product led, customer\nexperience led, service led, and efficiency led. This work provides an\nempirical baseline for companies and researchers to enhance our understanding\nof the field.", "published": "2020-10-23 14:07:12", "link": "http://arxiv.org/abs/2010.12418v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Analysis of LIME for Text Data", "abstract": "Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.", "published": "2020-10-23 15:40:13", "link": "http://arxiv.org/abs/2010.12487v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "EML System Description for VoxCeleb Speaker Diarization Challenge 2020", "abstract": "This technical report describes the EML submission to the first VoxCeleb\nspeaker diarization challenge. Although the aim of the challenge has been the\noffline processing of the signals, the submitted system is basically the EML\nonline algorithm which decides about the speaker labels in runtime\napproximately every 1.2 sec. For the first phase of the challenge, only\nVoxCeleb2 dev dataset was used for training. The results on the provided\nVoxConverse dev set show much better accuracy in terms of both DER and JER\ncompared to the offline baseline provided in the challenge. The real-time\nfactor of the whole diarization process is about 0.01 using a single CPU\nmachine.", "published": "2020-10-23 16:01:28", "link": "http://arxiv.org/abs/2010.12497v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Posterior Differential Regularization with f-divergence for Improving\n  Model Robustness", "abstract": "We address the problem of enhancing model robustness through regularization.\nSpecifically, we focus on methods that regularize the model posterior\ndifference between clean and noisy inputs. Theoretically, we provide a\nconnection of two recent methods, Jacobian Regularization and Virtual\nAdversarial Training, under this framework. Additionally, we generalize the\nposterior differential regularization to the family of $f$-divergences and\ncharacterize the overall regularization framework in terms of Jacobian matrix.\nEmpirically, we systematically compare those regularizations and standard BERT\ntraining on a diverse set of tasks to provide a comprehensive profile of their\neffect on model in-domain and out-of-domain generalization. For both fully\nsupervised and semi-supervised settings, our experiments show that regularizing\nthe posterior differential with $f$-divergence can result in well-improved\nmodel robustness. In particular, with a proper $f$-divergence, a BERT-base\nmodel can achieve comparable generalization as its BERT-large counterpart for\nin-domain, adversarial and domain shift scenarios, indicating the great\npotential of the proposed framework for boosting model generalization for NLP\nmodels.", "published": "2020-10-23 19:58:01", "link": "http://arxiv.org/abs/2010.12638v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and\n  Navigation", "abstract": "Autonomous robot systems for applications from search and rescue to assistive\nguidance should be able to engage in natural language dialog with people. To\nstudy such cooperative communication, we introduce Robot Simultaneous\nLocalization and Mapping with Natural Language (RobotSlang), a benchmark of 169\nnatural language dialogs between a human Driver controlling a robot and a human\nCommander providing guidance towards navigation goals. In each trial, the pair\nfirst cooperates to localize the robot on a global map visible to the\nCommander, then the Driver follows Commander instructions to move the robot to\na sequence of target objects. We introduce a Localization from Dialog History\n(LDH) and a Navigation from Dialog History (NDH) task where a learned agent is\ngiven dialog and visual observations from the robot platform as input and must\nlocalize in the global map or navigate towards the next target object,\nrespectively. RobotSlang is comprised of nearly 5k utterances and over 1k\nminutes of robot camera and control streams. We present an initial model for\nthe NDH task, and show that an agent trained in simulation can follow the\nRobotSlang dialog-based navigation instructions for controlling a physical\nrobot platform. Code and data are available at https://umrobotslang.github.io/.", "published": "2020-10-23 19:58:17", "link": "http://arxiv.org/abs/2010.12639v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Robust Document Representations using Latent Topics and Metadata", "abstract": "Task specific fine-tuning of a pre-trained neural language model using a\ncustom softmax output layer is the de facto approach of late when dealing with\ndocument classification problems. This technique is not adequate when labeled\nexamples are not available at training time and when the metadata artifacts in\na document must be exploited. We address these challenges by generating\ndocument representations that capture both text and metadata artifacts in a\ntask agnostic manner. Instead of traditional auto-regressive or auto-encoding\nbased training, our novel self-supervised approach learns a soft-partition of\nthe input space when generating text embeddings. Specifically, we employ a\npre-learned topic model distribution as surrogate labels and construct a loss\nfunction based on KL divergence. Our solution also incorporates metadata\nexplicitly rather than just augmenting them with text. The generated document\nembeddings exhibit compositional characteristics and are directly used by\ndownstream classification tasks to create decision boundaries from a small\nnumber of labeled examples, thereby eschewing complicated recognition methods.\nWe demonstrate through extensive evaluation that our proposed cross-model\nfusion solution outperforms several competitive baselines on multiple datasets.", "published": "2020-10-23 21:52:38", "link": "http://arxiv.org/abs/2010.12681v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "On the Embeddings of Variables in Recurrent Neural Networks for Source\n  Code", "abstract": "Source code processing heavily relies on the methods widely used in natural\nlanguage processing (NLP), but involves specifics that need to be taken into\naccount to achieve higher quality. An example of this specificity is that the\nsemantics of a variable is defined not only by its name but also by the\ncontexts in which the variable occurs. In this work, we develop dynamic\nembeddings, a recurrent mechanism that adjusts the learned semantics of the\nvariable when it obtains more information about the variable's role in the\nprogram. We show that using the proposed dynamic embeddings significantly\nimproves the performance of the recurrent neural network, in code completion\nand bug fixing tasks.", "published": "2020-10-23 22:32:11", "link": "http://arxiv.org/abs/2010.12693v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Improving Classification through Weak Supervision in Context-specific\n  Conversational Agent Development for Teacher Education", "abstract": "Machine learning techniques applied to the Natural Language Processing (NLP)\ncomponent of conversational agent development show promising results for\nimproved accuracy and quality of feedback that a conversational agent can\nprovide. The effort required to develop an educational scenario specific\nconversational agent is time consuming as it requires domain experts to label\nand annotate noisy data sources such as classroom videos. Previous approaches\nto modeling annotations have relied on labeling thousands of examples and\ncalculating inter-annotator agreement and majority votes in order to model the\nnecessary scenarios. This method, while proven successful, ignores individual\nannotator strengths in labeling a data point and under-utilizes examples that\ndo not have a majority vote for labeling. We propose using a multi-task weak\nsupervision method combined with active learning to address these concerns.\nThis approach requires less labeling than traditional methods and shows\nsignificant improvements in precision, efficiency, and time-requirements than\nthe majority vote method (Ratner 2019). We demonstrate the validity of this\nmethod on the Google Jigsaw data set and then propose a scenario to apply this\nmethod using the Instructional Quality Assessment(IQA) to define the categories\nfor labeling. We propose using probabilistic modeling of annotator labeling to\ngenerate active learning examples to further label the data. Active learning is\nable to iteratively improve the training performance and accuracy of the\noriginal classification model. This approach combines state-of-the art labeling\ntechniques of weak supervision and active learning to optimize results in the\neducational domain and could be further used to lessen the data requirements\nfor expanded scenarios within the education domain through transfer learning.", "published": "2020-10-23 23:39:40", "link": "http://arxiv.org/abs/2010.12710v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SpeakerNet: 1D Depth-wise Separable Convolutional Network for\n  Text-Independent Speaker Recognition and Verification", "abstract": "We propose SpeakerNet - a new neural architecture for speaker recognition and\nspeaker verification tasks. It is composed of residual blocks with 1D\ndepth-wise separable convolutions, batch-normalization, and ReLU layers. This\narchitecture uses x-vector based statistics pooling layer to map\nvariable-length utterances to a fixed-length embedding (q-vector). SpeakerNet-M\nis a simple lightweight model with just 5M parameters. It doesn't use voice\nactivity detection (VAD) and achieves close to state-of-the-art performance\nscoring an Equal Error Rate (EER) of 2.10% on the VoxCeleb1 cleaned and 2.29%\non the VoxCeleb1 trial files.", "published": "2020-10-23 20:34:54", "link": "http://arxiv.org/abs/2010.12653v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Channel Speaker Verification for Single and Multi-talker Speech", "abstract": "To improve speaker verification in real scenarios with interference speakers,\nnoise, and reverberation, we propose to bring together advancements made in\nmulti-channel speech features. Specifically, we combine spectral, spatial, and\ndirectional features, which includes inter-channel phase difference,\nmulti-channel sinc convolutions, directional power ratio features, and angle\nfeatures. To maximally leverage supervised learning, our framework is also\nequipped with multi-channel speech enhancement and voice activity detection. On\nall simulated, replayed, and real recordings, we observe large and consistent\nimprovements at various degradation levels. On real recordings of multi-talker\nspeech, we achieve a 36% relative reduction in equal error rate w.r.t.\nsingle-channel baseline. We find the improvements from speaker-dependent\ndirectional features more consistent in multi-talker conditions than clean.\nLastly, we investigate if the learned multi-channel speaker embedding space can\nbe made more discriminative through a contrastive loss-based fine-tuning. With\na simple choice of Triplet loss, we observe a further 8.3% relative reduction\nin EER.", "published": "2020-10-23 22:31:09", "link": "http://arxiv.org/abs/2010.12692v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Noise Robustness of an End-to-End Neural Model for Automatic\n  Speech Recognition", "abstract": "We present our experiments in training robust to noise an end-to-end\nautomatic speech recognition (ASR) model using intensive data augmentation. We\nexplore the efficacy of fine-tuning a pre-trained model to improve noise\nrobustness, and we find it to be a very efficient way to train for various\nnoisy conditions, especially when the conditions in which the model will be\nused, are unknown. Starting with a model trained on clean data helps establish\nbaseline performance on clean speech. We carefully fine-tune this model to both\nmaintain the performance on clean speech, and improve the model accuracy in\nnoisy conditions. With this schema, we trained robust to noise English and\nMandarin ASR models on large public corpora. All described models and training\nrecipes are open sourced in NeMo, a toolkit for conversational AI.", "published": "2020-10-23 23:46:29", "link": "http://arxiv.org/abs/2010.12715v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enriching Under-Represented Named-Entities To Improve Speech Recognition\n  Performance", "abstract": "Automatic speech recognition (ASR) for under-represented named-entity (UR-NE)\nis challenging due to such named-entities (NE) have insufficient instances and\npoor contextual coverage in the training data to learn reliable estimates and\nrepresentations. In this paper, we propose approaches to enriching UR-NEs to\nimprove speech recognition performance. Specifically, our first priority is to\nensure those UR-NEs to appear in the word lattice if there is any. To this end,\nwe make exemplar utterances for those UR-NEs according to their categories\n(e.g. location, person, organization, etc.), ending up with an improved\nlanguage model (LM) that boosts the UR-NE occurrence in the word lattice. With\nmore UR-NEs appearing in the lattice, we then boost the recognition performance\nthrough lattice rescoring methods. We first enrich the representations of\nUR-NEs in a pre-trained recurrent neural network LM (RNNLM) by borrowing the\nembedding representations of the rich-represented NEs (RR-NEs), yielding the\nlattices that statistically favor the UR-NEs. Finally, we directly boost the\nlikelihood scores of the utterances containing UR-NEs and gain further\nperformance improvement.", "published": "2020-10-23 03:22:10", "link": "http://arxiv.org/abs/2010.12143v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Cross-Verification Approach for Protecting World Leaders from Fake and\n  Tampered Audio", "abstract": "This paper tackles the problem of verifying the authenticity of speech\nrecordings from world leaders. Whereas previous work on detecting deep fake or\ntampered audio focus on scrutinizing an audio recording in isolation, we\ninstead reframe the problem and focus on cross-verifying a questionable\nrecording against trusted references. We present a method for cross-verifying a\nspeech recording against a reference that consists of two steps: aligning the\ntwo recordings and then classifying each query frame as matching or\nnon-matching. We propose a subsequence alignment method based on the\nNeedleman-Wunsch algorithm and show that it significantly outperforms dynamic\ntime warping in handling common tampering operations. We also explore several\nbinary classification models based on LSTM and Transformer architectures to\nverify content at the frame level. Through extensive experiments on tampered\nspeech recordings of Donald Trump, we show that our system can reliably detect\naudio tampering operations of different types and durations. Our best model\nachieves 99.7% accuracy for the alignment task at an error tolerance of 50 ms\nand a 0.43% equal error rate in classifying audio frames as matching or\nnon-matching.", "published": "2020-10-23 05:34:51", "link": "http://arxiv.org/abs/2010.12173v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Expressive Singing Voice Correction: On Perceptual Validity of\n  Evaluation Metrics for Vocal Melody Extraction", "abstract": "Singing voice correction (SVC) is an appealing application for amateur\nsingers. Commercial products automate SVC by snapping pitch contours to\nequal-tempered scales, which could lead to deadpan modifications. Together with\nthe neglect of rhythmic errors, extensive manual corrections are still\nnecessary. In this paper, we present a streamlined system to automate\nexpressive SVC for both pitch and rhythmic errors. Particularly, we extend a\nprevious work by integrating advanced techniques for singing voice separation\n(SVS) and vocal melody extraction. SVC is achieved by temporally aligning the\nsource-target pair, followed by replacing pitch and rhythm of the source with\nthose of the target. We evaluate the framework by a comparative study for\nmelody extraction which involves both subjective and objective evaluations,\nwhereby we investigate perceptual validity of the standard metrics through the\nlens of SVC. The results suggest that the high pitch accuracy obtained by the\nmetrics does not signify good perceptual scores.", "published": "2020-10-23 07:08:13", "link": "http://arxiv.org/abs/2010.12196v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Activity Detection Based on Multilingual Speech Recognition\n  System", "abstract": "To better model the contextual information and increase the generalization\nability of Speech Activity Detection (SAD) system, this paper leverages a\nmulti-lingual Automatic Speech Recognition (ASR) system to perform SAD.\nSequence discriminative training of Acoustic Model (AM) using Lattice-Free\nMaximum Mutual Information (LF-MMI) loss function, effectively extracts the\ncontextual information of the input acoustic frame. Multi-lingual AM training,\ncauses the robustness to noise and language variabilities. The index of maximum\noutput posterior is considered as a frame-level speech/non-speech decision\nfunction. Majority voting and logistic regression are applied to fuse the\nlanguage-dependent decisions. The multi-lingual ASR is trained on 18 languages\nof BABEL datasets and the built SAD is evaluated on 3 different languages. On\nout-of-domain datasets, the proposed SAD model shows significantly better\nperformance with respect to baseline models. On the Ester2 dataset, without\nusing any in-domain data, this model outperforms the WebRTC, phoneme recognizer\nbased VAD (Phn Rec), and Pyannote baselines (respectively by 7.1, 1.7, and 2.7%\nabsolute) in Detection Error Rate (DetER) metrics. Similarly, on the LiveATC\ndataset, this model outperforms the WebRTC, Phn Rec, and Pyannote baselines\n(respectively by 6.4, 10.0, and 3.7% absolutely) in DetER metrics.", "published": "2020-10-23 10:14:04", "link": "http://arxiv.org/abs/2010.12277v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training Noisy Single-Channel Speech Separation With Noisy Oracle\n  Sources: A Large Gap and A Small Step", "abstract": "As the performance of single-channel speech separation systems has improved,\nthere has been a desire to move to more challenging conditions than the clean,\nnear-field speech that initial systems were developed on. When training deep\nlearning separation models, a need for ground truth leads to training on\nsynthetic mixtures. As such, training in noisy conditions requires either using\nnoise synthetically added to clean speech, preventing the use of in-domain data\nfor a noisy-condition task, or training using mixtures of noisy speech,\nrequiring the network to additionally separate the noise. We demonstrate the\nrelative inseparability of noise and that this noisy speech paradigm leads to\nsignificant degradation of system performance. We also propose an\nSI-SDR-inspired training objective that tries to exploit the inseparability of\nnoise to implicitly partition the signal and discount noise separation errors,\nenabling the training of better separation systems with noisy oracle sources.", "published": "2020-10-23 14:22:07", "link": "http://arxiv.org/abs/2010.12430v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The IDLAB VoxCeleb Speaker Recognition Challenge 2020 System Description", "abstract": "In this technical report we describe the IDLAB top-scoring submissions for\nthe VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) in the supervised\nand unsupervised speaker verification tracks. For the supervised verification\ntracks we trained 6 state-of-the-art ECAPA-TDNN systems and 4 Resnet34 based\nsystems with architectural variations. On all models we apply a large margin\nfine-tuning strategy, which enables the training procedure to use higher margin\npenalties by using longer training utterances. In addition, we use\nquality-aware score calibration which introduces quality metrics in the\ncalibration system to generate more consistent scores across varying levels of\nutterance conditions. A fusion of all systems with both enhancements applied\nled to the first place on the open and closed supervised verification tracks.\nThe unsupervised system is trained through contrastive learning. Subsequent\npseudo-label generation by iterative clustering of the training embeddings\nallows the use of supervised techniques. This procedure led to the winning\nsubmission on the unsupervised track, and its performance is closing in on\nsupervised training.", "published": "2020-10-23 15:10:23", "link": "http://arxiv.org/abs/2010.12468v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-path Self-Attention RNN for Real-Time Speech Enhancement", "abstract": "We propose a dual-path self-attention recurrent neural network (DP-SARNN) for\ntime-domain speech enhancement. We improve dual-path RNN (DP-RNN) by augmenting\ninter-chunk and intra-chunk RNN with a recently proposed efficient attention\nmechanism. The combination of inter-chunk and intra-chunk attention improves\nthe attention mechanism for long sequences of speech frames. DP-SARNN\noutperforms a baseline DP-RNN by using a frame shift four times larger than in\nDP-RNN, which leads to a substantially reduced computation time per utterance.\nAs a result, we develop a real-time DP-SARNN by using long short-term memory\n(LSTM) RNN and causal attention in inter-chunk SARNN. DP-SARNN significantly\noutperforms existing approaches to speech enhancement, and on average takes 7.9\nms CPU time to process a signal chunk of 32 ms.", "published": "2020-10-23 23:42:37", "link": "http://arxiv.org/abs/2010.12713v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GSEP: A robust vocal and accompaniment separation system using gated\n  CBHG module and loudness normalization", "abstract": "In the field of audio signal processing research, source separation has been\na popular research topic for a long time and the recent adoption of the deep\nneural networks have shown a significant improvement in performance. The\nimprovement vitalizes the industry to productize audio deep learning based\nproducts and services including Karaoke in the music streaming apps and\ndialogue enhancement in the UHDTV. For these early markets, we defined a set of\ndesign principles of the vocal and accompaniment separation model in terms of\nrobustness, quality, and cost. In this paper, we introduce GSEP (Gaudio source\nSEParation system), a robust vocal and accompaniment separation system using a\nGated- CBHG module, mask warping, and loudness normalization and it was\nverified that the proposed system satisfies all three principles and\noutperforms the state-of-the-art systems both in objective measure and\nsubjective assessment through experiments.", "published": "2020-10-23 03:04:07", "link": "http://arxiv.org/abs/2010.12139v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Computational Evaluation of Musical Pattern Discovery Algorithms", "abstract": "Pattern discovery algorithms in the music domain aim to find meaningful\ncomponents in musical compositions. Over the years, although many algorithms\nhave been developed for pattern discovery in music data, it remains a\nchallenging task. To gain more insight into the efficacy of these algorithms,\nwe introduce three computational methods for examining their output: Pattern\nPolling, to combine the patterns; Comparative Classification, to differentiate\nthe patterns; Synthetic Data, to inject predetermined patterns. In combining\nand differentiating the patterns extracted by algorithms, we expose how they\ndiffer from the patterns annotated by humans as well as between algorithms\nthemselves, with rhythmic features contributing the most to the algorithm-human\nand algorithm-algorithm discrepancies. Despite the difficulty in reconciling\nand evaluating the divergent patterns extracted from algorithms, we identify\nsome possibilities for addressing them. In particular, we generate controllable\nsynthesised data with predetermined patterns planted into random data, thereby\nleaving us better able to inspect, compare, validate, and select the\nalgorithms. We provide a concrete example of synthesising data for\nunderstanding the algorithms and expand our discussion to the potential and\nlimitations of such an approach.", "published": "2020-10-23 12:07:21", "link": "http://arxiv.org/abs/2010.12325v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GraphSpeech: Syntax-Aware Graph Attention Network For Neural Speech\n  Synthesis", "abstract": "Attention-based end-to-end text-to-speech synthesis (TTS) is superior to\nconventional statistical methods in many ways. Transformer-based TTS is one of\nsuch successful implementations. While Transformer TTS models the speech frame\nsequence well with a self-attention mechanism, it does not associate input text\nwith output utterances from a syntactic point of view at sentence level. We\npropose a novel neural TTS model, denoted as GraphSpeech, that is formulated\nunder graph neural network framework. GraphSpeech encodes explicitly the\nsyntactic relation of input lexical tokens in a sentence, and incorporates such\ninformation to derive syntactically motivated character embeddings for TTS\nattention mechanism. Experiments show that GraphSpeech consistently outperforms\nthe Transformer TTS baseline in terms of spectrum and prosody rendering of\nutterances.", "published": "2020-10-23 14:14:06", "link": "http://arxiv.org/abs/2010.12423v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Speech enhancement aided end-to-end multi-task learning for voice\n  activity detection", "abstract": "Robust voice activity detection (VAD) is a challenging task in low\nsignal-to-noise (SNR) environments. Recent studies show that speech enhancement\nis helpful to VAD, but the performance improvement is limited. To address this\nissue, here we propose a speech enhancement aided end-to-end multi-task model\nfor VAD. The model has two decoders, one for speech enhancement and the other\nfor VAD. The two decoders share the same encoder and speech separation network.\nUnlike the direct thought that takes two separated objectives for VAD and\nspeech enhancement respectively, here we propose a new joint optimization\nobjective -- VAD-masked scale-invariant source-to-distortion ratio (mSI-SDR).\nmSI-SDR uses VAD information to mask the output of the speech enhancement\ndecoder in the training process. It makes the VAD and speech enhancement tasks\njointly optimized not only at the shared encoder and separation network, but\nalso at the objective level. It also satisfies real-time working requirement\ntheoretically. Experimental results show that the multi-task method\nsignificantly outperforms its single-task VAD counterpart. Moreover, mSI-SDR\noutperforms SI-SDR in the same multi-task setting.", "published": "2020-10-23 15:35:03", "link": "http://arxiv.org/abs/2010.12484v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
