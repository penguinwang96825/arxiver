{"title": "Easy-to-Hard: Leveraging Simple Questions for Complex Question\n  Generation", "abstract": "This paper makes one of the first efforts toward automatically generating\ncomplex questions from knowledge graphs. Particularly, we study how to leverage\nexisting simple question datasets for this task, under two separate scenarios:\nusing either sub-questions of the target complex questions, or distantly\nrelated pseudo sub-questions when the former are unavailable. First, a\ncompetitive base model named CoG2Q is designed to map complex query qraphs to\nnatural language questions. Afterwards, we propose two extension models, namely\nCoGSub2Q and CoGSub^m2Q, respectively for the above two scenarios. The former\nencodes and copies from a sub-question, while the latter further scores and\naggregates multiple pseudo sub-questions. Experiment results show that the\nextension models significantly outperform not only base CoG2Q, but also its\naugmented variant using simple questions as additional training examples. This\ndemonstrates the importance of instance-level connections between simple and\ncorresponding complex questions, which may be underexploited by straightforward\ndata augmentation of CoG2Q that builds model-level connections through learned\nparameters.", "published": "2019-12-05 03:44:20", "link": "http://arxiv.org/abs/1912.02367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Predict Explainable Plots for Neural Story Generation", "abstract": "Story generation is an important natural language processing task that aims\nto generate coherent stories automatically. While the use of neural networks\nhas proven effective in improving story generation, how to learn to generate an\nexplainable high-level plot still remains a major challenge. In this work, we\npropose a latent variable model for neural story generation. The model treats\nan outline, which is a natural language sentence explainable to humans, as a\nlatent variable to represent a high-level plot that bridges the input and\noutput. We adopt an external summarization model to guide the latent variable\nmodel to learn how to generate outlines from training data. Experiments show\nthat our approach achieves significant improvements over state-of-the-art\nmethods in both automatic and human evaluations.", "published": "2019-12-05 05:32:41", "link": "http://arxiv.org/abs/1912.02395v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the\n  WASABI Song Corpus with Lyrics Annotations", "abstract": "We present the WASABI Song Corpus, a large corpus of songs enriched with\nmetadata extracted from music databases on the Web, and resulting from the\nprocessing of song lyrics and from audio analysis. More specifically, given\nthat lyrics encode an important part of the semantics of a song, we focus here\non the description of the methods we proposed to extract relevant information\nfrom the lyrics, such as their structure segmentation, their topics, the\nexplicitness of the lyrics content, the salient passages of a song and the\nemotions conveyed. The creation of the resource is still ongoing: so far, the\ncorpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at\ndifferent levels with the output of the above mentioned methods. Such corpus\nlabels and the provided methods can be exploited by music search engines and\nmusic professionals (e.g. journalists, radio presenters) to better handle large\ncollections of lyrics, allowing an intelligent browsing, categorization and\nsegmentation recommendation of songs.", "published": "2019-12-05 10:15:13", "link": "http://arxiv.org/abs/1912.02477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Data Augmentation Approaches to End-to-End Task-Oriented\n  Dialogue", "abstract": "The training of task-oriented dialogue systems is often confronted with the\nlack of annotated data. In contrast to previous work which augments training\ndata through expensive crowd-sourcing efforts, we propose four different\nautomatic approaches to data augmentation at both the word and sentence level\nfor end-to-end task-oriented dialogue and conduct an empirical study on their\nimpact. Experimental results on the CamRest676 and KVRET datasets demonstrate\nthat each of the four data augmentation approaches is able to obtain a\nsignificant improvement over a strong baseline in terms of Success F1 score and\nthat the ensemble of the four approaches achieves the state-of-the-art results\nin the two datasets. In-depth analyses further confirm that our methods\nadequately increase the diversity of user utterances, which enables the\nend-to-end model to learn features robustly.", "published": "2019-12-05 10:16:32", "link": "http://arxiv.org/abs/1912.02478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massive vs. Curated Word Embeddings for Low-Resourced Languages. The\n  Case of Yor\u00f9b\u00e1 and Twi", "abstract": "The success of several architectures to learn semantic representations from\nunannotated text and the availability of these kind of texts in online\nmultilingual resources such as Wikipedia has facilitated the massive and\nautomatic creation of resources for multiple languages. The evaluation of such\nresources is usually done for the high-resourced languages, where one has a\nsmorgasbord of tasks and test sets to evaluate on. For low-resourced languages,\nthe evaluation is more difficult and normally ignored, with the hope that the\nimpressive capability of deep learning architectures to learn (multilingual)\nrepresentations in the high-resourced setting holds in the low-resourced\nsetting too. In this paper we focus on two African languages, Yor\\`ub\\'a and\nTwi, and compare the word embeddings obtained in this way, with word embeddings\nobtained from curated corpora and a language-dependent processing. We analyse\nthe noise in the publicly available corpora, collect high quality and noisy\ndata for the two languages and quantify the improvements that depend not only\non the amount of data but on the quality too. We also use different\narchitectures that learn word representations both from surface forms and\ncharacters to further exploit all the available information which showed to be\nimportant for these languages. For the evaluation, we manually translate the\nwordsim-353 word pairs dataset from English into Yor\\`ub\\'a and Twi. As output\nof the work, we provide corpora, embeddings and the test suits for both\nlanguages.", "published": "2019-12-05 10:25:32", "link": "http://arxiv.org/abs/1912.02481v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph\n  Convolution Networks", "abstract": "Microblogs are widely used to express people's opinions and feelings in daily\nlife. Sentiment analysis (SA) can timely detect personal sentiment polarities\nthrough analyzing text. Deep learning approaches have been broadly used in SA\nbut still have not fully exploited syntax information. In this paper, we\npropose a syntax-based graph convolution network (GCN) model to enhance the\nunderstanding of diverse grammatical structures of Chinese microblogs. In\naddition, a pooling method based on percentile is proposed to improve the\naccuracy of the model. In experiments, for Chinese microblogs emotion\nclassification categories including happiness, sadness, like, anger, disgust,\nfear, and surprise, the F-measure of our model reaches 82.32% and exceeds the\nstate-of-the-art algorithm by 5.90%. The experimental results show that our\nmodel can effectively utilize the information of dependency parsing to improve\nthe performance of emotion detection. What is more, we annotate a new dataset\nfor Chinese emotion classification, which is open to other researchers.", "published": "2019-12-05 12:56:28", "link": "http://arxiv.org/abs/1912.02545v1", "categories": ["cs.CL", "I.5.4", "I.5.4"], "primary_category": "cs.CL"}
{"title": "Measuring Social Bias in Knowledge Graph Embeddings", "abstract": "It has recently been shown that word embeddings encode social biases, with a\nharmful impact on downstream tasks. However, to this point there has been no\nsimilar work done in the field of graph embeddings. We present the first study\non social bias in knowledge graph embeddings, and propose a new metric suitable\nfor measuring such bias. We conduct experiments on Wikidata and Freebase, and\nshow that, as with word embeddings, harmful social biases related to\nprofessions are encoded in the embeddings with respect to gender, religion,\nethnicity and nationality. For example, graph embeddings encode the information\nthat men are more likely to be bankers, and women more likely to be\nhomekeepers. As graph embeddings become increasingly utilized, we suggest that\nit is important the existence of such biases are understood and steps taken to\nmitigate their impact.", "published": "2019-12-05 17:53:25", "link": "http://arxiv.org/abs/1912.02761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complete Variable-Length Codes: An Excursion into Word Edit Operations", "abstract": "Given an alphabet A and a binary relation $\\tau$ $\\subseteq$ A * x A * , a\nlanguage X $\\subseteq$ A * is $\\tau$-independent if $\\tau$ (X) $\\cap$ X =\n$\\emptyset$; X is $\\tau$-closed if $\\tau$ (X) $\\subseteq$ X. The language X is\ncomplete if any word over A is a factor of some concatenation of words in X.\nGiven a family of languages F containing X, X is maximal in F if no other set\nof F can stricly contain X. A language X $\\subseteq$ A * is a variable-length\ncode if any equation among the words of X is necessarily trivial. The study\ndiscusses the relationship between maximality and completeness in the case of\n$\\tau$-independent or $\\tau$-closed variable-length codes. We focus to the\nbinary relations by which the images of words are computed by deleting,\ninserting, or substituting some characters.", "published": "2019-12-05 15:28:05", "link": "http://arxiv.org/abs/1912.02646v1", "categories": ["cs.CL", "cs.DM"], "primary_category": "cs.CL"}
{"title": "PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference\n  Engine for Mobile Phones", "abstract": "Over the last years, a great success of deep neural networks (DNNs) has been\nwitnessed in computer vision and other fields. However, performance and power\nconstraints make it still challenging to deploy DNNs on mobile devices due to\ntheir high computational complexity. Binary neural networks (BNNs) have been\ndemonstrated as a promising solution to achieve this goal by using bit-wise\noperations to replace most arithmetic operations. Currently, existing\nGPU-accelerated implementations of BNNs are only tailored for desktop\nplatforms. Due to architecture differences, mere porting of such\nimplementations to mobile devices yields suboptimal performance or is\nimpossible in some cases. In this paper, we propose PhoneBit, a GPU-accelerated\nBNN inference engine for Android-based mobile devices that fully exploits the\ncomputing power of BNNs on mobile GPUs. PhoneBit provides a set of\noperator-level optimizations including locality-friendly data layout, bit\npacking with vectorization and layers integration for efficient binary\nconvolution. We also provide a detailed implementation and parallelization\noptimization for PhoneBit to optimally utilize the memory bandwidth and\ncomputing power of mobile GPUs. We evaluate PhoneBit with AlexNet, YOLOv2 Tiny\nand VGG16 with their binary version. Our experiment results show that PhoneBit\ncan achieve significant speedup and energy efficiency compared with\nstate-of-the-art frameworks for mobile devices.", "published": "2019-12-05 04:52:24", "link": "http://arxiv.org/abs/1912.04050v1", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "12-in-1: Multi-Task Vision and Language Representation Learning", "abstract": "Much of vision-and-language research focuses on a small but diverse set of\nindependent tasks and supporting datasets often studied in isolation; however,\nthe visually-grounded language understanding skills required for success at\nthese tasks overlap significantly. In this work, we investigate these\nrelationships between vision-and-language tasks by developing a large-scale,\nmulti-task training regime. Our approach culminates in a single model on 12\ndatasets from four broad categories of task including visual question\nanswering, caption-based image retrieval, grounding referring expressions, and\nmulti-modal verification. Compared to independently trained single-task models,\nthis represents a reduction from approximately 3 billion parameters to 270\nmillion while simultaneously improving performance by 2.05 points on average\nacross tasks. We use our multi-task framework to perform in-depth analysis of\nthe effect of joint training diverse tasks. Further, we show that finetuning\ntask-specific models from our single multi-task model can lead to further\nimprovements, achieving performance at or above the state-of-the-art.", "published": "2019-12-05 00:07:35", "link": "http://arxiv.org/abs/1912.02315v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art\n  Baseline", "abstract": "Prior work in visual dialog has focused on training deep neural models on\nVisDial in isolation. Instead, we present an approach to leverage pretraining\non related vision-language datasets before transferring to visual dialog. We\nadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn\nvisually-grounded conversations. Our model is pretrained on the Conceptual\nCaptions and Visual Question Answering datasets, and finetuned on VisDial. Our\nbest single model outperforms prior published work (including model ensembles)\nby more than 1% absolute on NDCG and MRR. Next, we find that additional\nfinetuning using \"dense\" annotations in VisDial leads to even higher NDCG --\nmore than 10% over our base model -- but hurts MRR -- more than 17% below our\nbase model! This highlights a trade-off between the two primary metrics -- NDCG\nand MRR -- which we find is due to dense annotations not correlating well with\nthe original ground-truth answers to questions.", "published": "2019-12-05 04:51:11", "link": "http://arxiv.org/abs/1912.02379v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "abstract": "In this paper, we describe the 2015 iteration of the SemEval shared task on\nSentiment Analysis in Twitter. This was the most popular sentiment analysis\nshared task to date with more than 40 teams participating in each of the last\nthree years. This year's shared task competition consisted of five sentiment\nprediction subtasks. Two were reruns from previous years: (A) sentiment\nexpressed by a phrase in the context of a tweet, and (B) overall sentiment of a\ntweet. We further included three new subtasks asking to predict (C) the\nsentiment towards a topic in a single tweet, (D) the overall sentiment towards\na topic in a set of tweets, and (E) the degree of prior polarity of a phrase.", "published": "2019-12-05 05:08:36", "link": "http://arxiv.org/abs/1912.02387v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploration of Neural Machine Translation in Autoformalization of\n  Mathematics in Mizar", "abstract": "In this paper we share several experiments trying to automatically translate\ninformal mathematics into formal mathematics. In our context informal\nmathematics refers to human-written mathematical sentences in the LaTeX format;\nand formal mathematics refers to statements in the Mizar language. We conducted\nour experiments against three established neural network-based machine\ntranslation models that are known to deliver competitive results on translating\nbetween natural languages. To train these models we also prepared four\ninformal-to-formal datasets. We compare and analyze our results according to\nwhether the model is supervised or unsupervised. In order to augment the data\navailable for auto-formalization and improve the results, we develop a custom\ntype-elaboration mechanism and integrate it in the supervised translation.", "published": "2019-12-05 15:13:15", "link": "http://arxiv.org/abs/1912.02636v2", "categories": ["cs.LO", "cs.CL", "cs.LG"], "primary_category": "cs.LO"}
{"title": "Self-Supervised Contextual Language Representation of Radiology Reports\n  to Improve the Identification of Communication Urgency", "abstract": "Machine learning methods have recently achieved high-performance in\nbiomedical text analysis. However, a major bottleneck in the widespread\napplication of these methods is obtaining the required large amounts of\nannotated training data, which is resource intensive and time consuming. Recent\nprogress in self-supervised learning has shown promise in leveraging large text\ncorpora without explicit annotations. In this work, we built a self-supervised\ncontextual language representation model using BERT, a deep bidirectional\ntransformer architecture, to identify radiology reports requiring prompt\ncommunication to the referring physicians. We pre-trained the BERT model on a\nlarge unlabeled corpus of radiology reports and used the resulting contextual\nrepresentations in a final text classifier for communication urgency. Our model\nachieved a precision of 97.0%, recall of 93.3%, and F-measure of 95.1% on an\nindependent test set in identifying radiology reports for prompt communication,\nand significantly outperformed the previous state-of-the-art model based on\nword2vec representations.", "published": "2019-12-05 16:33:23", "link": "http://arxiv.org/abs/1912.02703v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Classifying Diagrams and Their Parts using Graph Neural Networks: A\n  Comparison of Crowd-Sourced and Expert Annotations", "abstract": "This article compares two multimodal resources that consist of diagrams which\ndescribe topics in elementary school natural sciences. Both resources contain\nthe same diagrams and represent their structure using graphs, but differ in\nterms of their annotation schema and how the annotations have been created -\ndepending on the resource in question - either by crowd-sourced workers or\ntrained experts. This article reports on two experiments that evaluate how\neffectively crowd-sourced and expert-annotated graphs can represent the\nmultimodal structure of diagrams for representation learning using various\ngraph neural networks. The results show that the identity of diagram elements\ncan be learned from their layout features, while the expert annotations provide\nbetter representations of diagram types.", "published": "2019-12-05 20:34:53", "link": "http://arxiv.org/abs/1912.02866v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pairwise Neural Machine Translation Evaluation", "abstract": "We present a novel framework for machine translation evaluation using neural\nnetworks in a pairwise setting, where the goal is to select the better\ntranslation from a pair of hypotheses, given the reference translation. In this\nframework, lexical, syntactic and semantic information from the reference and\nthe two hypotheses is compacted into relatively small distributed vector\nrepresentations, and fed into a multi-layer neural network that models the\ninteraction between each of the hypotheses and the reference, as well as\nbetween the two hypotheses. These compact representations are in turn based on\nword and sentence embeddings, which are learned using neural networks. The\nframework is flexible, allows for efficient learning and classification, and\nyields correlation with humans that rivals the state of the art.", "published": "2019-12-05 05:17:05", "link": "http://arxiv.org/abs/1912.03135v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Design and implementation of an open source Greek POS Tagger and Entity\n  Recognizer using spaCy", "abstract": "This paper proposes a machine learning approach to part-of-speech tagging and\nnamed entity recognition for Greek, focusing on the extraction of morphological\nfeatures and classification of tokens into a small set of classes for named\nentities. The architecture model that was used is introduced. The greek version\nof the spaCy platform was added into the source code, a feature that did not\nexist before our contribution, and was used for building the models.\nAdditionally, a part of speech tagger was trained that can detect the\nmorphology of the tokens and performs higher than the state-of-the-art results\nwhen classifying only the part of speech. For named entity recognition using\nspaCy, a model that extends the standard ENAMEX type (organization, location,\nperson) was built. Certain experiments that were conducted indicate the need\nfor flexibility in out-of-vocabulary words and there is an effort for resolving\nthis issue. Finally, the evaluation results are discussed.", "published": "2019-12-05 13:29:27", "link": "http://arxiv.org/abs/1912.10162v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Robust Neural Vocoding for Speech Generation: A Survey", "abstract": "Recently, neural vocoders have been widely used in speech synthesis tasks,\nincluding text-to-speech and voice conversion. However, when encountering data\ndistribution mismatch between training and inference, neural vocoders trained\non real data often degrade in voice quality for unseen scenarios. In this\npaper, we train four common neural vocoders, including WaveNet, WaveRNN,\nFFTNet, Parallel WaveGAN alternately on five different datasets. To study the\nrobustness of neural vocoders, we evaluate the models using acoustic features\nfrom seen/unseen speakers, seen/unseen languages, a text-to-speech model, and a\nvoice conversion model. We found out that the speaker variety is much more\nimportant for achieving a universal vocoder than the language. Through our\nexperiments, we show that WaveNet and WaveRNN are more suitable for\ntext-to-speech models, while Parallel WaveGAN is more suitable for voice\nconversion applications. Great amount of subjective MOS results in naturalness\nfor all vocoders are presented for future studies.", "published": "2019-12-05 09:45:16", "link": "http://arxiv.org/abs/1912.02461v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge", "abstract": "The VoxCeleb Speaker Recognition Challenge 2019 aimed to assess how well\ncurrent speaker recognition technology is able to identify speakers in\nunconstrained or `in the wild' data. It consisted of: (i) a publicly available\nspeaker recognition dataset from YouTube videos together with ground truth\nannotation and standardised evaluation software; and (ii) a public challenge\nand workshop held at Interspeech 2019 in Graz, Austria. This paper outlines the\nchallenge and provides its baselines, results and discussions.", "published": "2019-12-05 12:00:45", "link": "http://arxiv.org/abs/1912.02522v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Target Speaker Enhancement on Multi-Talker Environment\n  using Event-Driven Cameras", "abstract": "We propose a method to address audio-visual target speaker enhancement in\nmulti-talker environments using event-driven cameras. State of the art\naudio-visual speech separation methods shows that crucial information is the\nmovement of the facial landmarks related to speech production. However, all\napproaches proposed so far work offline, using frame-based video input, making\nit difficult to process an audio-visual signal with low latency, for online\napplications. In order to overcome this limitation, we propose the use of\nevent-driven cameras and exploit compression, high temporal resolution and low\nlatency, for low cost and low latency motion feature extraction, going towards\nonline embedded audio-visual speech processing. We use the event-driven optical\nflow estimation of the facial landmarks as input to a stacked Bidirectional\nLSTM trained to predict an Ideal Amplitude Mask that is then used to filter the\nnoisy audio, to obtain the audio signal of the target speaker. The presented\napproach performs almost on par with the frame-based approach, with very low\nlatency and computational cost.", "published": "2019-12-05 16:01:14", "link": "http://arxiv.org/abs/1912.02671v2", "categories": ["eess.AS", "cs.LG", "eess.IV"], "primary_category": "eess.AS"}
