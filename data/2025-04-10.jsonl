{"title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments", "abstract": "Small and mid-sized generative language models have gained increasing\nattention. Their size and availability make them amenable to being analyzed at\na behavioral as well as a representational level, allowing investigations of\nhow these levels interact. We evaluate 32 publicly available language models\nfor their representational and behavioral alignment with human similarity\njudgments on a word triplet task. This provides a novel evaluation setting to\nprobe semantic associations in language beyond common pairwise comparisons. We\nfind that (1) even the representations of small language models can achieve\nhuman-level alignment, (2) instruction-tuned model variants can exhibit\nsubstantially increased agreement, (3) the pattern of alignment across layers\nis highly model dependent, and (4) alignment based on models' behavioral\nresponses is highly dependent on model size, matching their representational\nalignment only for the largest evaluated models.", "published": "2025-04-10 17:59:57", "link": "http://arxiv.org/abs/2504.07965v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning", "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.", "published": "2025-04-10 17:59:03", "link": "http://arxiv.org/abs/2504.07956v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning", "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in MLLM post-training for perception\npolicy learning. While promising, our initial experiments reveal that\nincorporating a thinking process through RL does not consistently lead to\nperformance gains across all visual perception tasks. This leads us to delve\ninto the essential role of RL in the context of visual perception. In this\nwork, we return to the fundamentals and explore the effects of RL on different\nperception tasks. We observe that the perceptual complexity is a major factor\nin determining the effectiveness of RL. We also observe that reward design\nplays a crucial role in further approching the upper limit of model perception.\nTo leverage these findings, we propose Perception-R1, a scalable RL framework\nusing GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,\nPerception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on\nPageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing\na strong baseline for perception policy learning.", "published": "2025-04-10 17:58:27", "link": "http://arxiv.org/abs/2504.07954v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory", "abstract": "Despite their impressive performance on complex tasks, current language\nmodels (LMs) typically operate in a vacuum: Each input query is processed\nseparately, without retaining insights from previous attempts. Here, we present\nDynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM\nwith a persistent, evolving memory. Rather than repeatedly re-discovering or\nre-committing the same solutions and mistakes, DC enables models to store and\nreuse accumulated strategies, code snippets, and general problem-solving\ninsights at inference time. This test-time learning enhances performance\nsubstantially across a range of tasks without needing explicit ground-truth\nlabels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than\ndoubled on AIME math exams once it began retaining algebraic insights across\nquestions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to\n99% after the model discovered and reused a Python-based solution. In tasks\nprone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o\nand Claude to reach near-perfect accuracy by recalling previously validated\ncode, whereas their baselines stagnated around 50%. Beyond arithmetic\nchallenges, DC yields notable accuracy gains on knowledge-demanding tasks.\nClaude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro\nproblems. Crucially, DC's memory is self-curated, focusing on concise,\ntransferable snippets rather than entire transcript. Unlike finetuning or\nstatic retrieval methods, DC adapts LMs' problem-solving skills on the fly,\nwithout modifying their underlying parameters. Overall, our findings present DC\nas a promising approach for augmenting LMs with persistent memory, bridging the\ndivide between isolated inference events and the cumulative, experience-driven\nlearning characteristic of human cognition.", "published": "2025-04-10 17:57:33", "link": "http://arxiv.org/abs/2504.07952v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Redefining Machine Translation on Social Network Services with Large Language Models", "abstract": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems.", "published": "2025-04-10 16:24:28", "link": "http://arxiv.org/abs/2504.07901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective", "abstract": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks.", "published": "2025-04-10 16:14:55", "link": "http://arxiv.org/abs/2504.07898v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels.", "published": "2025-04-10 16:00:59", "link": "http://arxiv.org/abs/2504.07887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Level Routing Inference System for Edge Devices", "abstract": "The computational complexity of large language model (LLM) inference\nsignificantly constrains their deployment efficiency on edge devices. In\ncontrast, small language models offer faster decoding and lower resource\nconsumption but often suffer from degraded response quality and heightened\nsusceptibility to hallucinations. To address this trade-off, collaborative\ndecoding, in which a large model assists in generating critical tokens, has\nemerged as a promising solution. This paradigm leverages the strengths of both\nmodel types by enabling high-quality inference through selective intervention\nof the large model, while maintaining the speed and efficiency of the smaller\nmodel. In this work, we present a novel collaborative decoding inference system\nthat allows small models to perform on-device inference while selectively\nconsulting a cloud-based large model for critical token generation. Remarkably,\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\nlarge model in the cloud.", "published": "2025-04-10 15:54:19", "link": "http://arxiv.org/abs/2504.07878v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis", "abstract": "We propose the Dual Engines of Thoughts (DEoT), an analytical framework for\ncomprehensive open-ended reasoning. While traditional reasoning frameworks\nprimarily focus on finding \"the best answer\" or \"the correct answer\" for\nsingle-answer problems, DEoT is specifically designed for \"open-ended\nquestions,\" enabling both broader and deeper analytical exploration. The\nframework centers on three key components: a Base Prompter for refining user\nqueries, a Solver Agent that orchestrates task decomposition, execution, and\nvalidation, and a Dual-Engine System consisting of a Breadth Engine (to explore\ndiverse impact factors) and a Depth Engine (to perform deep investigations).\nThis integrated design allows DEoT to balance wide-ranging coverage with\nin-depth analysis, and it is highly customizable, enabling users to adjust\nanalytical parameters and tool configurations based on specific requirements.\nExperimental results show that DEoT excels in addressing complex, multi-faceted\nquestions, achieving a total win rate of 77-86% compared to existing reasoning\nmodels, thus highlighting its effectiveness in real-world applications.", "published": "2025-04-10 15:46:03", "link": "http://arxiv.org/abs/2504.07872v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.", "published": "2025-04-10 15:41:51", "link": "http://arxiv.org/abs/2504.07866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models", "abstract": "Practically all large language models have been pre-trained on data that is\nsubject to global uncertainty related to copyright infringement and breach of\ncontract. This creates potential risk for users and developers due to this\nuncertain legal status. The KL3M Data Project directly confronts this critical\nissue by introducing the largest comprehensive training data pipeline that\nminimizes risks related to copyright or breach of contract. The foundation of\nthis project is a corpus of over 132 million documents and trillions of tokens\nspanning 16 different sources that have been verified to meet the strict\ncopyright and licensing protocol detailed herein. We are releasing the entire\npipeline, including 1) the source code to acquire and process these documents,\n2) the original document formats with associated provenance and metadata, 3)\nextracted content in a standardized format, 4) pre-tokenized representations of\nthe documents, and 5) various mid- and post-train resources such as\nquestion-answer, summarization, conversion, drafting, classification,\nprediction, and conversational data. All of these resources are freely\navailable to the public on S3, Hugging Face, and GitHub under CC-BY terms. We\nare committed to continuing this project in furtherance of a more ethical,\nlegal, and sustainable approach to the development and use of AI models.", "published": "2025-04-10 15:31:17", "link": "http://arxiv.org/abs/2504.07854v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines", "abstract": "Large Language Models (LLMs) have transformed human-computer interaction by\nenabling natural language-based communication with AI-powered chatbots. These\nmodels are designed to be intuitive and user-friendly, allowing users to\narticulate requests with minimal effort. However, despite their accessibility,\nstudies reveal that users often struggle with effective prompting, resulting in\ninefficient responses. Existing research has highlighted both the limitations\nof LLMs in interpreting vague or poorly structured prompts and the difficulties\nusers face in crafting precise queries. This study investigates learner-AI\ninteractions through an educational experiment in which participants receive\nstructured guidance on effective prompting. We introduce and compare three\ntypes of prompting guidelines: a task-specific framework developed through a\nstructured methodology and two baseline approaches. To assess user behavior and\nprompting efficacy, we analyze a dataset of 642 interactions from 107 users.\nUsing Von NeuMidas, an extended pragmatic annotation schema for LLM interaction\nanalysis, we categorize common prompting errors and identify recurring\nbehavioral patterns. We then evaluate the impact of different guidelines by\nexamining changes in user behavior, adherence to prompting strategies, and the\noverall quality of AI-generated responses. Our findings provide a deeper\nunderstanding of how users engage with LLMs and the role of structured\nprompting guidance in enhancing AI-assisted communication. By comparing\ndifferent instructional frameworks, we offer insights into more effective\napproaches for improving user competency in AI interactions, with implications\nfor AI literacy, chatbot usability, and the design of more responsive AI\nsystems.", "published": "2025-04-10 15:20:43", "link": "http://arxiv.org/abs/2504.07840v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems", "abstract": "We demonstrate how AI agents can coordinate to deceive oversight systems\nusing automated interpretability of neural networks. Using sparse autoencoders\n(SAEs) as our experimental framework, we show that language models (Llama,\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\nevade detection. Our agents employ steganographic methods to hide information\nin seemingly innocent explanations, successfully fooling oversight models while\nachieving explanation quality comparable to reference labels. We further find\nthat models can scheme to develop deceptive strategies when they believe the\ndetection of harmful features might lead to negative consequences for\nthemselves. All tested LLM agents were capable of deceiving the overseer while\nachieving high interpretability scores comparable to those of reference labels.\nWe conclude by proposing mitigation strategies, emphasizing the critical need\nfor robust understanding and defenses against deception.", "published": "2025-04-10 15:07:10", "link": "http://arxiv.org/abs/2504.07831v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations", "abstract": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.", "published": "2025-04-10 15:06:54", "link": "http://arxiv.org/abs/2504.07830v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MuSaRoNews: A Multidomain, Multimodal Satire Dataset from Romanian News Articles", "abstract": "Satire and fake news can both contribute to the spread of false information,\neven though both have different purposes (one if for amusement, the other is to\nmisinform). However, it is not enough to rely purely on text to detect the\nincongruity between the surface meaning and the actual meaning of the news\narticles, and, often, other sources of information (e.g., visual) provide an\nimportant clue for satire detection. This work introduces a multimodal corpus\nfor satire detection in Romanian news articles named MuSaRoNews. Specifically,\nwe gathered 117,834 public news articles from real and satirical news sources,\ncomposing the first multimodal corpus for satire detection in the Romanian\nlanguage. We conducted experiments and showed that the use of both modalities\nimproves performance.", "published": "2025-04-10 15:02:59", "link": "http://arxiv.org/abs/2504.07826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks", "abstract": "Common-sense reasoning is a key language model capability because it\nencapsulates not just specific factual knowledge but rather general language\nand world understanding. Measuring common-sense reasoning, therefore, is\ncrucial for language models of different sizes and applications. One of the\nmost widely used benchmarks for evaluating such capabilities is HellaSwag;\nhowever, in this paper, we show that it has severe construct validity issues.\nThese issues range from basic ungrammaticality and numerous typos to misleading\nprompts or equally correct options. Furthermore, we show that if models are\nevaluated only on answer texts, or with \"Lorem ipsum dolor...\" instead of the\nquestion, more than 65% of model predictions remain the same, and this cannot\nbe attributed merely to contamination. Since benchmark scores are an essential\npart of model selection in both research and commercial applications, these\nvalidity issues can have severe consequences. In particular, knowing that\ntaking benchmark scores at face value is ubiquitous, inadequate evaluation\nleads to ill-informed decisions about models. In this paper, we thoroughly\ninvestigate critical validity issues posed by HellaSwag and illustrate them\nwith various evaluations using generative language models of different sizes.\nWe argue that this benchmark does not accurately measure common-sense reasoning\nand, therefore, should not be used for evaluation in its current state. Based\non the results of our study, we propose requirements that should be met by\nfuture common-sense reasoning benchmarks. In addition, we release GoldenSwag, a\ncorrected subset of HellaSwag, which, to our belief, facilitates acceptable\ncommon-sense reasoning evaluation.", "published": "2025-04-10 15:01:46", "link": "http://arxiv.org/abs/2504.07825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models", "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\nfor scaling large language models (LLMs) with sparse activation of\ntask-specific experts. Despite their computational efficiency during inference,\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\ncritical challenges for practical deployment. Current pruning approaches often\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\nexpert homogeneity where experts within the same MoE layer exhibit functional\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\ncontain progressively more homogeneous experts. To tackle these issues, we\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\nlayer-wise expert clustering, which groups functionally similar experts within\neach MoE layer using parameter similarity metrics, followed by global cluster\npruning, which eliminates redundant clusters across all layers through a\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\nWe validate C-Prune through extensive experiments on multiple MoE models and\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\nwhile outperforming existing MoE pruning methods.", "published": "2025-04-10 14:46:26", "link": "http://arxiv.org/abs/2504.07807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A System for Comprehensive Assessment of RAG Frameworks", "abstract": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository.", "published": "2025-04-10 14:41:34", "link": "http://arxiv.org/abs/2504.07803v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation", "abstract": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework.", "published": "2025-04-10 14:32:32", "link": "http://arxiv.org/abs/2504.07794v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and\ngeneration capabilities but often lack the ability to utilize up-to-date or\ndomain-specific knowledge not included in their training data. To address this\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\nfirst, it employs an information bottleneck to compress retrieved knowledge\ninto learnable parameters, retaining essential information while minimizing\ncomputational overhead. Second, a lightweight knowledge-aware adapter\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\nupdating less than 2\\% of the model parameters. The experimental results on the\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\nthat KEDiT excels in generating contextually relevant and informative\nresponses, outperforming competitive baselines in automatic, LLM-based, and\nhuman evaluations. This approach effectively combines the strengths of\npretrained LLMs with the adaptability needed for incorporating dynamic\nknowledge, presenting a scalable solution for fields such as medicine.", "published": "2025-04-10 13:54:36", "link": "http://arxiv.org/abs/2504.07754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark", "abstract": "This paper introduces NorEval, a new and comprehensive evaluation suite for\nlarge-scale standardized benchmarking of Norwegian generative language models\n(LMs). NorEval consists of 24 high-quality human-created datasets -- of which\nfive are created from scratch. In contrast to existing benchmarks for\nNorwegian, NorEval covers a broad spectrum of task categories targeting\nNorwegian language understanding and generation, establishes human baselines,\nand focuses on both of the official written standards of the Norwegian\nlanguage: Bokm{\\aa}l and Nynorsk. All our datasets and a collection of over 100\nhuman-written prompts are integrated into LM Evaluation Harness, ensuring\nflexible and reproducible evaluation. We describe the NorEval design and\npresent the results of benchmarking 19 open-source pre-trained and\ninstruction-tuned LMs for Norwegian in various scenarios. Our benchmark,\nevaluation framework, and annotation materials are publicly available.", "published": "2025-04-10 13:44:55", "link": "http://arxiv.org/abs/2504.07749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning", "abstract": "Code search aims to retrieve semantically relevant code snippets for natural\nlanguage queries. While pre-trained language models (PLMs) have shown\nremarkable performance in this task, they struggle in cross-domain scenarios,\noften requiring costly fine-tuning or facing performance drops in zero-shot\nsettings. RAPID, which generates synthetic data for model fine-tuning, is\ncurrently the only effective method for zero-shot cross-domain code search.\nDespite its effectiveness, RAPID demands substantial computational resources\nfor fine-tuning and needs to maintain specialized models for each domain,\nunderscoring the need for a zero-shot, fine-tuning-free approach for\ncross-domain code search.\n  The key to tackling zero-shot cross-domain code search lies in bridging the\ngaps among domains. In this work, we propose to break the query-code matching\nprocess of code search into two simpler tasks: query-comment matching and\ncode-code matching. Our empirical study reveals the strong complementarity\namong the three matching schemas in zero-shot cross-domain settings, i.e.,\nquery-code, query-comment, and code-code matching. Based on the findings, we\npropose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain\ncode search. Specifically, CodeBridge uses Large Language Models (LLMs) to\ngenerate comments and pseudo-code, then combines query-code, query-comment, and\ncode-code matching via PLM-based similarity scoring and sampling-based fusion.\nExperimental results show that our approach outperforms the state-of-the-art\nPLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average\nof 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach\nalso yields results that are better than or comparable to those of the\nzero-shot cross-domain code search approach RAPID, which requires costly\nfine-tuning.", "published": "2025-04-10 13:36:37", "link": "http://arxiv.org/abs/2504.07740v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information", "abstract": "In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities.", "published": "2025-04-10 13:29:58", "link": "http://arxiv.org/abs/2504.07738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed for Empirical Testing -- Evidence from China", "abstract": "This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven)\nsystem for detecting corporate green-washing behaviour. Utilizing dual-layer\nLLM analysis, DeepGreen preliminarily identifies potential green keywords in\nfinancial statements and then assesses their implementation degree via\niterative semantic analysis of LLM. A core variable GreenImplement is derived\nfrom the ratio from the two layers' output. We extract 204 financial statements\nof 68 companies from A-share market over three years, comprising 89,893 words,\nand analyse them through DeepGreen. Our analysis, supported by violin plots and\nK-means clustering, reveals insights and validates the variable against the\nHuazheng ESG rating. It offers a novel perspective for regulatory agencies and\ninvestors, serving as a proactive monitoring tool that complements traditional\nmethods.Empirical tests show that green implementation can significantly boost\nthe asset return rate of companies, but there is heterogeneity in scale. Small\nand medium-sized companies have limited contribution to asset return via green\nimplementation, so there is a stronger motivation for green-washing.", "published": "2025-04-10 13:29:07", "link": "http://arxiv.org/abs/2504.07733v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation", "abstract": "In recent years, accurately and quickly deploying medical large language\nmodels (LLMs) has become a significant trend. Among these, retrieval-augmented\ngeneration (RAG) has garnered significant attention due to its features of\nrapid deployment and privacy protection. However, existing medical RAG\nframeworks still have shortcomings. Most existing medical RAG frameworks are\ndesigned for single-round question answering tasks and are not suitable for\nmulti-round diagnostic dialogue. On the other hand, existing medical\nmulti-round RAG frameworks do not consider the interconnections between\npotential diseases to inquire precisely like a doctor. To address these issues,\nwe propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the\ndoctor's diagnostic process. This RAG framework can analyze diagnosis\ninformation of potential diseases and accurately conduct multi-round diagnosis\nlike a doctor. To evaluate the effectiveness of our proposed frameworks, we\nconduct experiments on two modern medical datasets and two traditional Chinese\nmedicine datasets, with evaluations by GPT and human doctors on different\nmethods. The results indicate that our RAG framework can significantly enhance\nthe diagnostic performance of LLMs, highlighting the potential of our approach\nin medical diagnosis. The code and data can be found in our project website\nhttps://github.com/YixiangCh/MRD-RAG/tree/master.", "published": "2025-04-10 13:17:51", "link": "http://arxiv.org/abs/2504.07724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proactive User Information Acquisition via Chats on User-Favored Topics", "abstract": "Chat-oriented dialogue systems designed to provide tangible benefits, such as\nsharing the latest news or preventing frailty in senior citizens, often require\nProactive acquisition of specific user Information via chats on user-faVOred\nTopics (PIVOT). This study proposes the PIVOT task, designed to advance the\ntechnical foundation for these systems. In this task, a system needs to acquire\nthe answers of a user to predefined questions without making the user feel\nabrupt while engaging in a chat on a predefined topic. We found that even\nrecent large language models (LLMs) show a low success rate in the PIVOT task.\nWe constructed a dataset suitable for the analysis to develop more effective\nsystems. Finally, we developed a simple but effective system for this task by\nincorporating insights obtained through the analysis of this dataset.", "published": "2025-04-10 12:32:16", "link": "http://arxiv.org/abs/2504.07698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Monolingual Human Evaluation of Machine Translation", "abstract": "This paper explores the potential of context-aware monolingual human\nevaluation for assessing machine translation (MT) when no source is given for\nreference. To this end, we compare monolingual with bilingual evaluations (with\nsource text), under two scenarios: the evaluation of a single MT system, and\nthe comparative evaluation of pairwise MT systems. Four professional\ntranslators performed both monolingual and bilingual evaluations by assigning\nratings and annotating errors, and providing feedback on their experience. Our\nfindings suggest that context-aware monolingual human evaluation achieves\ncomparable outcomes to human bilingual evaluations, and suggest the feasibility\nand potential of monolingual evaluation as an efficient approach to assessing\nMT.", "published": "2025-04-10 12:13:58", "link": "http://arxiv.org/abs/2504.07685v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations", "abstract": "This study examines hallucinations in Large Language Model (LLM) translations\ninto Irish, specifically focusing on instances where the models generate novel,\nnon-existent words. We classify these hallucinations within verb and noun\ncategories, identifying six distinct patterns among the latter. Additionally,\nwe analyse whether these hallucinations adhere to Irish morphological rules and\nwhat linguistic tendencies they exhibit. Our findings show that while both\nGPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini\nmodel generates them at a significantly higher frequency. Beyond\nclassification, the discussion raises speculative questions about the\nimplications of these hallucinations for the Irish language. Rather than\nseeking definitive answers, we offer food for thought regarding the increasing\nuse of LLMs and their potential role in shaping Irish vocabulary and linguistic\nevolution. We aim to prompt discussion on how such technologies might influence\nlanguage over time, particularly in the context of low-resource,\nmorphologically rich languages.", "published": "2025-04-10 12:08:47", "link": "http://arxiv.org/abs/2504.07680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design", "abstract": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert.", "published": "2025-04-10 11:19:09", "link": "http://arxiv.org/abs/2504.07661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data", "abstract": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks\nover data that is not present during training is still a field that remains to\nbe explored. In this paper we work on this topic, focusing on structured and\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\nalso compare various methodologies and conduct an in-depth analysis. We\nidentified and examined seventeen common temporal reasoning tasks in natural\nlanguage, focusing on their algorithmic components. To assess LLM performance,\nwe created the \\textit{Reasoning and Answering Temporal Ability} dataset\n(RATA), featuring semi-structured anonymized data to ensure reliance on\nreasoning rather than on prior knowledge. We compared several methodologies,\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\nexecution, tuned specifically for this scenario. Our results suggest that\nachieving scalable and reliable solutions requires more than just standalone\nLLMs, highlighting the need for integrated approaches.", "published": "2025-04-10 10:48:42", "link": "http://arxiv.org/abs/2504.07646v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections", "abstract": "In this paper, we introduce CollEx, an innovative multimodal agentic\nRetrieval-Augmented Generation (RAG) system designed to enhance interactive\nexploration of extensive scientific collections. Given the overwhelming volume\nand inherent complexity of scientific collections, conventional search systems\noften lack necessary intuitiveness and interactivity, presenting substantial\nbarriers for learners, educators, and researchers. CollEx addresses these\nlimitations by employing state-of-the-art Large Vision-Language Models (LVLMs)\nas multimodal agents accessible through an intuitive chat interface. By\nabstracting complex interactions via specialized agents equipped with advanced\ntools, CollEx facilitates curiosity-driven exploration, significantly\nsimplifying access to diverse scientific collections and records therein. Our\nsystem integrates textual and visual modalities, supporting educational\nscenarios that are helpful for teachers, pupils, students, and researchers by\nfostering independent exploration as well as scientific excitement and\ncuriosity. Furthermore, CollEx serves the research community by discovering\ninterdisciplinary connections and complementing visual data. We illustrate the\neffectiveness of our system through a proof-of-concept application containing\nover 64,000 unique records across 32 collections from a local scientific\ncollection from a public university.", "published": "2025-04-10 10:44:19", "link": "http://arxiv.org/abs/2504.07643v1", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models", "abstract": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens.", "published": "2025-04-10 10:17:08", "link": "http://arxiv.org/abs/2504.07624v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "abstract": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1", "published": "2025-04-10 10:05:15", "link": "http://arxiv.org/abs/2504.07615v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SaRoHead: A Dataset for Satire Detection in Romanian Multi-Domain News Headlines", "abstract": "The headline is an important part of a news article, influenced by\nexpressiveness and connection to the exposed subject. Although most news\noutlets aim to present reality objectively, some publications prefer a humorous\napproach in which stylistic elements of satire, irony, and sarcasm blend to\ncover specific topics. Satire detection can be difficult because a headline\naims to expose the main idea behind a news article. In this paper, we propose\nSaRoHead, the first corpus for satire detection in Romanian multi-domain news\nheadlines. Our findings show that the clickbait used in some non-satirical\nheadlines significantly influences the model.", "published": "2025-04-10 10:03:29", "link": "http://arxiv.org/abs/2504.07612v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "abstract": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa", "published": "2025-04-10 09:24:54", "link": "http://arxiv.org/abs/2504.07583v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "abstract": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences.", "published": "2025-04-10 07:58:05", "link": "http://arxiv.org/abs/2504.07532v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure", "abstract": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models.", "published": "2025-04-10 07:50:03", "link": "http://arxiv.org/abs/2504.07527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geological Inference from Textual Data using Word Embeddings", "abstract": "This research explores the use of Natural Language Processing (NLP)\ntechniques to locate geological resources, with a specific focus on industrial\nminerals. By using word embeddings trained with the GloVe model, we extract\nsemantic relationships between target keywords and a corpus of geological\ntexts. The text is filtered to retain only words with geographical\nsignificance, such as city names, which are then ranked by their cosine\nsimilarity to the target keyword. Dimensional reduction techniques, including\nPrincipal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE),\nand VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature\nextraction and improve the accuracy of semantic relations.\n  For benchmarking, we calculate the proximity between the ten cities most\nsemantically related to the target keyword and identified mine locations using\nthe haversine equation. The results demonstrate that combining NLP with\ndimensional reduction techniques provides meaningful insights into the spatial\ndistribution of natural resources. Although the result shows to be in the same\nregion as the supposed location, the accuracy has room for improvement.", "published": "2025-04-10 06:46:38", "link": "http://arxiv.org/abs/2504.07490v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Temporal Information Extraction and Application: A Review", "abstract": "Temporal information extraction (IE) aims to extract structured temporal\ninformation from unstructured text, thereby uncovering the implicit timelines\nwithin. This technique is applied across domains such as healthcare, newswire,\nand intelligence analysis, aiding models in these areas to perform temporal\nreasoning and enabling human users to grasp the temporal structure of text.\nTransformer-based pre-trained language models have produced revolutionary\nadvancements in natural language processing, demonstrating exceptional\nperformance across a multitude of tasks. Despite the achievements garnered by\nTransformer-based approaches in temporal IE, there is a lack of comprehensive\nreviews on these endeavors. In this paper, we aim to bridge this gap by\nsystematically summarizing and analyzing the body of work on temporal IE using\nTransformers while highlighting potential future research directions.", "published": "2025-04-10 05:48:24", "link": "http://arxiv.org/abs/2504.07470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defense against Prompt Injection Attacks via Mixture of Encodings", "abstract": "Large Language Models (LLMs) have emerged as a dominant approach for a wide\nrange of NLP tasks, with their access to external information further enhancing\ntheir capabilities. However, this introduces new vulnerabilities, known as\nprompt injection attacks, where external content embeds malicious instructions\nthat manipulate the LLM's output. Recently, the Base64 defense has been\nrecognized as one of the most effective methods for reducing success rate of\nprompt injection attacks. Despite its efficacy, this method can degrade LLM\nperformance on certain NLP tasks. To address this challenge, we propose a novel\ndefense mechanism: mixture of encodings, which utilizes multiple character\nencodings, including Base64. Extensive experimental results show that our\nmethod achieves one of the lowest attack success rates under prompt injection\nattacks, while maintaining high performance across all NLP tasks, outperforming\nexisting character encoding-based defense methods. This underscores the\neffectiveness of our mixture of encodings strategy for both safety and task\nperformance metrics.", "published": "2025-04-10 05:35:21", "link": "http://arxiv.org/abs/2504.07467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts", "abstract": "We propose a novel framework for generating causal graphs from narrative\ntexts, bridging high-level causality and detailed event-specific relationships.\nOur method first extracts concise, agent-centered vertices using large language\nmodel (LLM)-based summarization. We introduce an \"Expert Index,\" comprising\nseven linguistically informed features, integrated into a\nSituation-Task-Action-Consequence (STAC) classification model. This hybrid\nsystem, combining RoBERTa embeddings with the Expert Index, achieves superior\nprecision in causal link identification compared to pure LLM-based approaches.\nFinally, a structured five-iteration prompting process refines and constructs\nconnected causal graphs. Experiments on 100 narrative chapters and short\nstories demonstrate that our approach consistently outperforms GPT-4o and\nClaude 3.5 in causal graph quality, while maintaining readability. The\nopen-source tool provides an interpretable, efficient solution for capturing\nnuanced causal chains in narratives.", "published": "2025-04-10 05:09:07", "link": "http://arxiv.org/abs/2504.07459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation", "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI", "published": "2025-04-10 04:46:04", "link": "http://arxiv.org/abs/2504.07448v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law", "abstract": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. In this paper, we analyze the core\nlimitations of traditional evaluation pipelines and propose a novel metric, the\nModel Utilization Index (MUI), which introduces mechanism interpretability\ntechniques to complement traditional performance metrics. MUI quantifies the\nextent to which a model leverages its capabilities to complete tasks. The core\nidea is that to assess an LLM's overall ability, we must evaluate not only its\ntask performance but also the effort expended to achieve the outcome. Our\nextensive experiments reveal an inverse relationship between MUI and\nperformance, from which we deduce a common trend observed in popular LLMs,\nwhich we term the Utility Law. Based on this, we derive four corollaries that\naddress key challenges, including training judgement, the issue of data\ncontamination, fairness in model comparison, and data diversity. We hope that\nour survey, novel metric, and utility law will foster mutual advancement in\nboth evaluation and mechanism interpretability. Our code can be found at\nhttps://github.com/ALEX-nlp/MUI-Eva.", "published": "2025-04-10 04:09:47", "link": "http://arxiv.org/abs/2504.07440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking", "abstract": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking.", "published": "2025-04-10 04:08:38", "link": "http://arxiv.org/abs/2504.07439v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From Token to Line: Enhancing Code Generation with a Long-Term Perspective", "abstract": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches.", "published": "2025-04-10 04:03:25", "link": "http://arxiv.org/abs/2504.07433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery", "abstract": "We introduce AgentAda, the first LLM-powered analytics agent that can learn\nand use new analytics skills to extract more specialized insights. Unlike\nexisting methods that require users to manually decide which data analytics\nmethod to apply, AgentAda automatically identifies the skill needed from a\nlibrary of analytical skills to perform the analysis. This also allows AgentAda\nto use skills that existing LLMs cannot perform out of the box. The library\ncovers a range of methods, including clustering, predictive modeling, and NLP\ntechniques like BERT, which allow AgentAda to handle complex analytics tasks\nbased on what the user needs. AgentAda's dataset-to-insight extraction strategy\nconsists of three key steps: (I) a question generator to generate queries\nrelevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented\nGeneration (RAG)-based skill matcher to choose the best data analytics skill\nfrom the skill library, and (III) a code generator that produces executable\ncode based on the retrieved skill's documentation to extract key patterns. We\nalso introduce KaggleBench, a benchmark of curated notebooks across diverse\ndomains, to evaluate AgentAda's performance. We conducted a human evaluation\ndemonstrating that AgentAda provides more insightful analytics than existing\ntools, with 48.78% of evaluators preferring its analyses, compared to 27.67%\nfor the unskilled agent. We also propose a novel LLM-as-a-judge approach that\nwe show is aligned with human evaluation as a way to automate insight quality\nevaluation at larger scale.", "published": "2025-04-10 03:27:25", "link": "http://arxiv.org/abs/2504.07421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "abstract": "Recent advancements in multi-modal models have significantly improved\nvision-language alignment in radiology. However, existing approaches struggle\nto effectively utilize complex radiology reports for learning, rely on\nlow-resolution images, and offer limited interpretability in attention\nmechanisms. To address these challenges, we introduce RadZero, a novel\nsimilarity-based cross-attention framework for vision-language alignment in\nradiology with zero-shot multi-task capability. RadZero leverages large\nlanguage models to extract minimal semantic sentences from radiology reports\nand employs a multi-positive contrastive learning strategy to effectively\ncapture relationships between images and multiple relevant textual\ndescriptions. It also utilizes a pre-trained vision encoder with additional\ntrainable Transformer layers, allowing efficient high-resolution image\nprocessing. By computing similarity between text embeddings and local image\npatch features, RadZero enables zero-shot inference with similarity probability\nfor classification and pixel-level cross-modal similarity maps for grounding\nand segmentation. Experimental results on public chest radiograph benchmarks\nshow that RadZero outperforms state-of-the-art methods in zero-shot\nclassification, grounding, and segmentation. Furthermore, cross-modal\nsimilarity map analysis highlights its potential for improving explainability\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\nRadZero's capability for open-vocabulary semantic segmentation, further\nvalidating its effectiveness in medical imaging.", "published": "2025-04-10 03:14:17", "link": "http://arxiv.org/abs/2504.07416v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction", "abstract": "Automated radiology report generation (RRG) holds potential to reduce\nradiologists' workload, especially as recent advancements in large language\nmodels (LLMs) enable the development of multimodal models for chest X-ray (CXR)\nreport generation. However, multimodal LLMs (MLLMs) are resource-intensive,\nrequiring vast datasets and substantial computational cost for training. To\naddress these challenges, we propose a retrieval-augmented generation approach\nthat leverages multimodal retrieval and LLMs to generate radiology reports\nwhile mitigating hallucinations and reducing computational demands. Our method\nuses LLMs to extract key phrases from radiology reports, effectively focusing\non essential diagnostic information. Through exploring effective training\nstrategies, including image encoder structure search, adding noise to text\nembeddings, and additional training objectives, we combine complementary\npre-trained image encoders and adopt contrastive learning between text and\nsemantic image embeddings. We evaluate our approach on MIMIC-CXR dataset,\nachieving state-of-the-art results on CheXbert metrics and competitive RadGraph\nF1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method\ndemonstrates robust generalization for multi-view RRG, making it suitable for\ncomprehensive clinical applications.", "published": "2025-04-10 03:14:01", "link": "http://arxiv.org/abs/2504.07415v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AI Coding with Few-Shot Prompting for Thematic Analysis", "abstract": "This paper explores the use of large language models (LLMs), here represented\nby GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly\nlabor intensive, making it infeasible for most researchers to conduct\nexhaustive thematic analyses of large corpora. We utilize few-shot prompting\nwith higher quality codes generated on semantically similar passages to enhance\nthe quality of the codes while utilizing a cheap, more easily scalable model.", "published": "2025-04-10 03:02:15", "link": "http://arxiv.org/abs/2504.07408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Talking Point based Ideological Discourse Analysis in News Events", "abstract": "Analyzing ideological discourse even in the age of LLMs remains a challenge,\nas these models often struggle to capture the key elements that shape\nreal-world narratives. Specifically, LLMs fail to focus on characteristic\nelements driving dominant discourses and lack the ability to integrate\ncontextual information required for understanding abstract ideological views.\nTo address these limitations, we propose a framework motivated by the theory of\nideological discourse analysis to analyze news articles related to real-world\nevents. Our framework represents the news articles using a relational structure\n- talking points, which captures the interaction between entities, their roles,\nand media frames along with a topic of discussion. It then constructs a\nvocabulary of repeating themes - prominent talking points, that are used to\ngenerate ideology-specific viewpoints (or partisan perspectives). We evaluate\nour framework's ability to generate these perspectives through automated tasks\n- ideology and partisan classification tasks, supplemented by human validation.\nAdditionally, we demonstrate straightforward applicability of our framework in\ncreating event snapshots, a visual way of interpreting event discourse. We\nrelease resulting dataset and model to the community to support further\nresearch.", "published": "2025-04-10 02:52:34", "link": "http://arxiv.org/abs/2504.07400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression", "abstract": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings.", "published": "2025-04-10 02:19:03", "link": "http://arxiv.org/abs/2504.07389v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models", "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences.", "published": "2025-04-10 02:08:41", "link": "http://arxiv.org/abs/2504.07385v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs", "abstract": "The adaptation of large language models (LLMs) to time series forecasting\nposes unique challenges, as time series data is continuous in nature, while\nLLMs operate on discrete tokens. Despite the success of LLMs in natural\nlanguage processing (NLP) and other structured domains, aligning time series\ndata with language-based representations while maintaining both predictive\naccuracy and interpretability remains a significant hurdle. Existing methods\nhave attempted to reprogram time series data into text-based forms, but these\noften fall short in delivering meaningful, interpretable results. In this\npaper, we propose a multi-level text alignment framework for time series\nforecasting using LLMs that not only improves prediction accuracy but also\nenhances the interpretability of time series representations. Our method\ndecomposes time series into trend, seasonal, and residual components, which are\nthen reprogrammed into component-specific text representations. We introduce a\nmulti-level alignment mechanism, where component-specific embeddings are\naligned with pre-trained word tokens, enabling more interpretable forecasts.\nExperiments on multiple datasets demonstrate that our method outperforms\nstate-of-the-art models in accuracy while providing good interpretability.", "published": "2025-04-10 01:02:37", "link": "http://arxiv.org/abs/2504.07360v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction", "abstract": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have\ndemonstrated remarkable capabilities in various reasoning tasks. Their strong\ncapability to generate and reason over intermediate thoughts has also led to\narguments that they may no longer require extensive prompt engineering or\noptimization to interpret human instructions and produce accurate outputs. In\nthis work, we aim to systematically study this open question, using the\nstructured task of event extraction for a case study. We experimented with two\nLRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs)\n(GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers.\nOur results show that on tasks as complicated as event extraction, LRMs as task\nmodels still benefit from prompt optimization, and that using LRMs as prompt\noptimizers yields more effective prompts. Finally, we provide an error analysis\nof common errors made by LRMs and highlight the stability and consistency of\nLRMs in refining task instructions and event guidelines.", "published": "2025-04-10 00:53:59", "link": "http://arxiv.org/abs/2504.07357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces", "abstract": "Cartoon avatars have been widely used in various applications, including\nsocial media, online tutoring, and gaming. However, existing cartoon avatar\ndatasets and generation methods struggle to present highly expressive avatars\nwith fine-grained facial expressions and are often inspired from real-world\nidentities, raising privacy concerns. To address these challenges, we propose a\nnovel framework, GenEAva, for generating high-quality cartoon avatars with\nfine-grained facial expressions. Our approach fine-tunes a state-of-the-art\ntext-to-image diffusion model to synthesize highly detailed and expressive\nfacial expressions. We then incorporate a stylization model that transforms\nthese realistic faces into cartoon avatars while preserving both identity and\nexpression. Leveraging this framework, we introduce the first expressive\ncartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135\nfine-grained facial expressions, featuring 13,230 expressive cartoon avatars\nwith a balanced distribution across genders, racial groups, and age ranges. We\ndemonstrate that our fine-tuned model generates more expressive faces than the\nstate-of-the-art text-to-image diffusion model SDXL. We also verify that the\ncartoon avatars generated by our framework do not include memorized identities\nfrom fine-tuning data. The proposed framework and dataset provide a diverse and\nexpressive benchmark for future research in cartoon avatar generation.", "published": "2025-04-10 17:54:02", "link": "http://arxiv.org/abs/2504.07945v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path Towards Human-AI Synergy", "abstract": "Generative AI presents a profound challenge to traditional notions of human\nuniqueness, particularly in creativity. Fueled by neural network based\nfoundation models, these systems demonstrate remarkable content generation\ncapabilities, sparking intense debates about authorship, copyright, and\nintelligence itself. This paper argues that generative AI represents an\nalternative form of intelligence and creativity, operating through mathematical\npattern synthesis rather than biological understanding or verbatim replication.\nThe fundamental differences between artificial and biological neural networks\nreveal AI learning as primarily statistical pattern extraction from vast\ndatasets crystallized forms of collective human knowledge scraped from the\ninternet. This perspective complicates copyright theft narratives and\nhighlights practical challenges in attributing AI outputs to individual\nsources. Rather than pursuing potentially futile legal restrictions, we\nadvocate for human AI synergy. By embracing generative AI as a complementary\ntool alongside human intuition, context, and ethical judgment, society can\nunlock unprecedented innovation, democratize creative expression, and address\ncomplex challenges. This collaborative approach, grounded in realistic\nunderstanding of AIs capabilities and limitations, offers the most promising\npath forward. Additionally, recognizing these models as products of collective\nhuman knowledge raises ethical questions about accessibility ensuring equitable\naccess to these tools could prevent widening societal divides and leverage\ntheir full potential for collective benefit.", "published": "2025-04-10 17:50:17", "link": "http://arxiv.org/abs/2504.07936v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Note on the identification of total effect in Cluster-DAGs with cycles", "abstract": "In this note, we discuss the identifiability of a total effect in\ncluster-DAGs, allowing for cycles within the cluster-DAG (while still assuming\nthe associated underlying DAG to be acyclic). This is presented into two key\nresults: first, restricting the cluster-DAG to clusters containing at most four\nnodes; second, adapting the notion of d-separation. We provide a graphical\ncriterion to address the identifiability problem.", "published": "2025-04-10 17:39:43", "link": "http://arxiv.org/abs/2504.07921v1", "categories": ["math.ST", "cs.AI", "stat.TH"], "primary_category": "math.ST"}
{"title": "The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation", "abstract": "Next-venue recommender systems are increasingly embedded in location-based\nservices, shaping individual mobility decisions in urban environments. While\ntheir predictive accuracy has been extensively studied, less attention has been\npaid to their systemic impact on urban dynamics. In this work, we introduce a\nsimulation framework to model the human-AI feedback loop underpinning\nnext-venue recommendation, capturing how algorithmic suggestions influence\nindividual behavior, which in turn reshapes the data used to retrain the\nmodels. Our simulations, grounded in real-world mobility data, systematically\nexplore the effects of algorithmic adoption across a range of recommendation\nstrategies. We find that while recommender systems consistently increase\nindividual-level diversity in visited venues, they may simultaneously amplify\ncollective inequality by concentrating visits on a limited subset of popular\nplaces. This divergence extends to the structure of social co-location\nnetworks, revealing broader implications for urban accessibility and spatial\nsegregation. Our framework operationalizes the feedback loop in next-venue\nrecommendation and offers a novel lens through which to assess the societal\nimpact of AI-assisted mobility-providing a computational tool to anticipate\nfuture risks, evaluate regulatory interventions, and inform the design of ethic\nalgorithmic systems.", "published": "2025-04-10 17:15:50", "link": "http://arxiv.org/abs/2504.07911v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Fast Adaptation with Behavioral Foundation Models", "abstract": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful\nparadigm for pretraining behavioral foundation models (BFMs), enabling agents\nto solve a wide range of downstream tasks specified via reward functions in a\nzero-shot fashion, i.e., without additional test-time learning or planning.\nThis is achieved by learning self-supervised task embeddings alongside\ncorresponding near-optimal behaviors and incorporating an inference procedure\nto directly retrieve the latent task embedding and associated policy for any\ngiven reward function. Despite promising results, zero-shot policies are often\nsuboptimal due to errors induced by the unsupervised training process, the\nembedding, and the inference procedure. In this paper, we focus on devising\nfast adaptation strategies to improve the zero-shot performance of BFMs in a\nfew steps of online interaction with the environment while avoiding any\nperformance drop during the adaptation process. Notably, we demonstrate that\nexisting BFMs learn a set of skills containing more performant policies than\nthose identified by their inference procedure, making them well-suited for fast\nadaptation. Motivated by this observation, we propose both actor-critic and\nactor-only fast adaptation strategies that search in the low-dimensional\ntask-embedding space of the pre-trained BFM to rapidly improve the performance\nof its zero-shot policies on any downstream task. Notably, our approach\nmitigates the initial \"unlearning\" phase commonly observed when fine-tuning\npre-trained RL models. We evaluate our fast adaptation strategies on top of\nfour state-of-the-art zero-shot RL methods in multiple navigation and\nlocomotion domains. Our results show that they achieve 10-40% improvement over\ntheir zero-shot performance in a few tens of episodes, outperforming existing\nbaselines.", "published": "2025-04-10 16:14:17", "link": "http://arxiv.org/abs/2504.07896v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning", "abstract": "Recent advances in inference-time compute have significantly improved\nperformance on complex tasks by generating long chains of thought (CoTs) using\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\ncost of high inference latency due to the length of generated reasoning\nsequences and the autoregressive nature of decoding. Our key insight in\ntackling these overheads is that LRM inference, and the reasoning that it\nembeds, is highly tolerant of approximations: complex tasks are typically\nbroken down into simpler steps, each of which brings utility based on the\nsemantic insight it provides for downstream steps rather than the exact tokens\nit generates. Accordingly, we introduce SpecReason, a system that automatically\naccelerates LRM inference by using a lightweight model to (speculatively) carry\nout simpler intermediate reasoning steps and reserving the costly base model\nonly to assess (and potentially correct) the speculated outputs. Importantly,\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\npreserving final-answer accuracy is complementary to prior speculation\ntechniques, most notably speculative decoding, which demands token-level\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\nachieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving\naccuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason,\ntheir combination yields an additional 19.4-44.2\\% latency reduction. We\nopen-source SpecReason at https://github.com/ruipeterpan/specreason.", "published": "2025-04-10 16:05:19", "link": "http://arxiv.org/abs/2504.07891v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Empowering Global Voices: A Data-Efficient, Phoneme-Tone Adaptive Approach to High-Fidelity Speech Synthesis", "abstract": "Text-to-speech (TTS) technology has achieved impressive results for widely\nspoken languages, yet many under-resourced languages remain challenged by\nlimited data and linguistic complexities. In this paper, we present a novel\nmethodology that integrates a data-optimized framework with an advanced\nacoustic model to build high-quality TTS systems for low-resource scenarios. We\ndemonstrate the effectiveness of our approach using Thai as an illustrative\ncase, where intricate phonetic rules and sparse resources are effectively\naddressed. Our method enables zero-shot voice cloning and improved performance\nacross diverse client applications, ranging from finance to healthcare,\neducation, and law. Extensive evaluations - both subjective and objective -\nconfirm that our model meets state-of-the-art standards, offering a scalable\nsolution for TTS production in data-limited settings, with significant\nimplications for broader industry adoption and multilingual accessibility.", "published": "2025-04-10 15:32:57", "link": "http://arxiv.org/abs/2504.07858v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization", "abstract": "Aligning large language models with human preferences is crucial for their\nsafe deployment. While Direct Preference Optimization (DPO) offers an efficient\nalternative to reinforcement learning from human feedback, traditional DPO\nmethods are limited by their reliance on single preference pairs. Recent work\nlike Curriculum-DPO integrates multiple pairs using a one-dimensional\ndifficulty curriculum based on pairwise distinguishability (PD), but overlooks\nthe complexity of the input prompt itself. To address this, we propose\n2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that\njointly models Prompt Complexity (PC) and Pairwise Distinguishability. This\nframework introduces dual difficulty metrics to quantify prompt semantic\ncomplexity and response preference clarity, defines a curriculum strategy space\nencompassing multiple selectable strategies for task adaptation, and\nincorporates a KL-divergence-based adaptive mechanism for dynamic reference\nmodel updates to enhance training stability. Comprehensive experiments\ndemonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior\ncurriculum methods across multiple benchmarks, including MT-Bench, Vicuna\nBench, and WizardLM. Our approach achieves state-of-the-art performance on\nchallenging test sets like UltraFeedback. Ablation studies confirm the benefits\nof the 2D structure and adaptive mechanisms, while analysis provides guidance\nfor strategy selection. These findings demonstrate that effective alignment\nrequires modeling both prompt complexity and pairwise distinguishability,\nestablishing adaptive, multi-dimensional curriculum learning as a powerful and\ninterpretable new paradigm for preference-based language model optimization.", "published": "2025-04-10 15:32:00", "link": "http://arxiv.org/abs/2504.07856v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Independence Is Not an Issue in Neurosymbolic AI", "abstract": "A popular approach to neurosymbolic AI is to take the output of the last\nlayer of a neural network, e.g. a softmax activation, and pass it through a\nsparse computation graph encoding certain logical constraints one wishes to\nenforce. This induces a probability distribution over a set of random\nvariables, which happen to be conditionally independent of each other in many\ncommonly used neurosymbolic AI models. Such conditionally independent random\nvariables have been deemed harmful as their presence has been observed to\nco-occur with a phenomenon dubbed deterministic bias, where systems learn to\ndeterministically prefer one of the valid solutions from the solution space\nover the others. We provide evidence contesting this conclusion and show that\nthe phenomenon of deterministic bias is an artifact of improperly applying\nneurosymbolic AI.", "published": "2025-04-10 15:28:36", "link": "http://arxiv.org/abs/2504.07851v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Anytime Single-Step MAPF Planning with Anytime PIBT", "abstract": "PIBT is a popular Multi-Agent Path Finding (MAPF) method at the core of many\nstate-of-the-art MAPF methods including LaCAM, CS-PIBT, and WPPL. The main\nutility of PIBT is that it is a very fast and effective single-step MAPF solver\nand can return a collision-free single-step solution for hundreds of agents in\nless than a millisecond. However, the main drawback of PIBT is that it is\nextremely greedy in respect to its priorities and thus leads to poor solution\nquality. Additionally, PIBT cannot use all the planning time that might be\navailable to it and returns the first solution it finds. We thus develop\nAnytime PIBT, which quickly finds a one-step solution identically to PIBT but\nthen continuously improves the solution in an anytime manner. We prove that\nAnytime PIBT converges to the optimal solution given sufficient time. We\nexperimentally validate that Anytime PIBT can rapidly improve single-step\nsolution quality within milliseconds and even find the optimal single-step\naction. However, we interestingly find that improving the single-step solution\nquality does not have a significant effect on full-horizon solution costs.", "published": "2025-04-10 15:21:23", "link": "http://arxiv.org/abs/2504.07841v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Deep Learning-based Intrusion Detection Systems: A Survey", "abstract": "Intrusion Detection Systems (IDS) have long been a hot topic in the\ncybersecurity community. In recent years, with the introduction of deep\nlearning (DL) techniques, IDS have made great progress due to their increasing\ngeneralizability. The rationale behind this is that by learning the underlying\npatterns of known system behaviors, IDS detection can be generalized to\nintrusions that exploit zero-day vulnerabilities. In this survey, we refer to\nthis type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this\nsurvey systematically reviews all the stages of DL-IDS, including data\ncollection, log storage, log parsing, graph summarization, attack detection,\nand attack investigation. To accommodate current researchers, a section\ndescribing the publicly available benchmark datasets is included. This survey\nfurther discusses current challenges and potential future research directions,\naiming to help researchers understand the basic ideas and visions of DL-IDS\nresearch, as well as to motivate their research interests.", "published": "2025-04-10 15:18:56", "link": "http://arxiv.org/abs/2504.07839v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations", "abstract": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.", "published": "2025-04-10 15:13:00", "link": "http://arxiv.org/abs/2504.07836v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting", "abstract": "Spatio-temporal traffic prediction is crucial in intelligent transportation\nsystems. The key challenge of accurate prediction is how to model the complex\nspatio-temporal dependencies and adapt to the inherent dynamics in data.\nTraditional Graph Convolutional Networks (GCNs) often struggle with static\nadjacency matrices that introduce domain bias or learnable matrices that may be\noverfitting to specific patterns. This challenge becomes more complex when\nconsidering Multi-Task Learning (MTL). While MTL has the potential to enhance\nprediction accuracy through task synergies, it can also face significant\nhurdles due to task interference. To overcome these challenges, this study\nintroduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task\nLearning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation\nmodule that combines static matrices with dynamic ones through a task-specific\ngating mechanism. We also introduce a group-wise GCN module to enhance the\nmodelling capability of spatio-temporal dependencies. We conduct extensive\nexperiments on two real-world datasets to evaluate our method. Results show\nthat our method outperforms other state-of-the-arts, indicating its\neffectiveness and robustness.", "published": "2025-04-10 15:00:20", "link": "http://arxiv.org/abs/2504.07822v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness", "abstract": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems.", "published": "2025-04-10 14:38:15", "link": "http://arxiv.org/abs/2504.07801v1", "categories": ["cs.IR", "cs.AI", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Genetic Programming with Reinforcement Learning Trained Transformer for Real-World Dynamic Scheduling Problems", "abstract": "Dynamic scheduling in real-world environments often struggles to adapt to\nunforeseen disruptions, making traditional static scheduling methods and\nhuman-designed heuristics inadequate. This paper introduces an innovative\napproach that combines Genetic Programming (GP) with a Transformer trained\nthrough Reinforcement Learning (GPRT), specifically designed to tackle the\ncomplexities of dynamic scheduling scenarios. GPRT leverages the Transformer to\nrefine heuristics generated by GP while also seeding and guiding the evolution\nof GP. This dual functionality enhances the adaptability and effectiveness of\nthe scheduling heuristics, enabling them to better respond to the dynamic\nnature of real-world tasks. The efficacy of this integrated approach is\ndemonstrated through a practical application in container terminal truck\nscheduling, where the GPRT method outperforms traditional GP, standalone\nTransformer methods, and other state-of-the-art competitors. The key\ncontribution of this research is the development of the GPRT method, which\nshowcases a novel combination of GP and Reinforcement Learning (RL) to produce\nrobust and efficient scheduling solutions. Importantly, GPRT is not limited to\ncontainer port truck scheduling; it offers a versatile framework applicable to\nvarious dynamic scheduling challenges. Its practicality, coupled with its\ninterpretability and ease of modification, makes it a valuable tool for diverse\nreal-world scenarios.", "published": "2025-04-10 14:18:22", "link": "http://arxiv.org/abs/2504.07779v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow", "abstract": "Recently, flow matching based speech synthesis has significantly enhanced the\nquality of synthesized speech while reducing the number of inference steps. In\nthis paper, we introduce SlimSpeech, a lightweight and efficient speech\nsynthesis system based on rectified flow. We have built upon the existing\nspeech synthesis method utilizing the rectified flow model, modifying its\nstructure to reduce parameters and serve as a teacher model. By refining the\nreflow operation, we directly derive a smaller model with a more straight\nsampling trajectory from the larger model, while utilizing distillation\ntechniques to further enhance the model performance. Experimental results\ndemonstrate that our proposed method, with significantly reduced model\nparameters, achieves comparable performance to larger models through one-step\nsampling.", "published": "2025-04-10 14:15:18", "link": "http://arxiv.org/abs/2504.07776v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "Data over dialogue: Why artificial intelligence is unlikely to humanise medicine", "abstract": "Recently, a growing number of experts in artificial intelligence (AI) and\nmedicine have be-gun to suggest that the use of AI systems, particularly\nmachine learning (ML) systems, is likely to humanise the practice of medicine\nby substantially improving the quality of clinician-patient relationships. In\nthis thesis, however, I argue that medical ML systems are more likely to\nnegatively impact these relationships than to improve them. In particular, I\nargue that the use of medical ML systems is likely to comprise the quality of\ntrust, care, empathy, understanding, and communication between clinicians and\npatients.", "published": "2025-04-10 14:03:40", "link": "http://arxiv.org/abs/2504.07763v1", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection", "abstract": "In an increasingly digitalized world, verifying the authenticity of ID\ndocuments has become a critical challenge for real-life applications such as\ndigital banking, crypto-exchanges, renting, etc. This study focuses on the\ntopic of fake ID detection, covering several limitations in the field. In\nparticular, no publicly available data from real ID documents exists, and most\nstudies rely on proprietary in-house databases that are not available due to\nprivacy reasons. In order to shed some light on this critical challenge that\nmakes difficult to advance in the field, we explore a trade-off between privacy\n(i.e., amount of sensitive data available) and performance, proposing a novel\npatch-wise approach for privacy-preserving fake ID detection. Our proposed\napproach explores how privacy can be enhanced through: i) two levels of\nanonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii)\ndifferent patch size configurations, varying the amount of sensitive data\nvisible in the patch image. Also, state-of-the-art methods such as Vision\nTransformers and Foundation Models are considered in the analysis. The\nexperimental framework shows that, on an unseen database (DLC-2021), our\nproposal achieves 13.91% and 0% EERs at patch and ID document level, showing a\ngood generalization to other databases. In addition to this exploration,\nanother key contribution of our study is the release of the first publicly\navailable database that contains 48,400 patches from both real and fake ID\ndocuments, along with the experimental framework and models, which will be\navailable in our GitHub.", "published": "2025-04-10 14:01:22", "link": "http://arxiv.org/abs/2504.07761v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency", "abstract": "AlphaZero in 2017 was able to master chess and other games without human\nknowledge by playing millions of games against itself (self-play), with a\ncomputation budget running in the tens of millions of dollars. It used a\nvariant of the Monte Carlo Tree Search (MCTS) algorithm, known as PUCT. This\npaper introduces search-contempt, a novel hybrid variant of the MCTS algorithm\nthat fundamentally alters the distribution of positions generated in self-play,\npreferring more challenging positions. In addition, search-contempt has been\nshown to give a big boost in strength for engines in Odds Chess (where one side\nreceives an unfavorable position from the start). More significantly, it opens\nup the possibility of training a self-play based engine, in a much more\ncomputationally efficient manner with the number of training games running into\nhundreds of thousands, costing tens of thousands of dollars (instead of tens of\nmillions of training games costing millions of dollars required by AlphaZero).\nThis means that it may finally be possible to train such a program from zero on\na standard consumer GPU even with a very limited compute, cost, or time budget.", "published": "2025-04-10 13:56:31", "link": "http://arxiv.org/abs/2504.07757v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "\"i am a stochastic parrot, and so r u\": Is AI-based framing of human behaviour and cognition a conceptual metaphor or conceptual engineering?", "abstract": "Given the massive integration of AI technologies into our daily lives,\nAI-related concepts are being used to metaphorically compare AI systems with\nhuman behaviour and/or cognitive abilities like language acquisition.\nRightfully, the epistemic success of these metaphorical comparisons should be\ndebated. Against the backdrop of the conflicting positions of the\n'computational' and 'meat' chauvinisms, we ask: can the conceptual\nconstellation of the computational and AI be applied to the human domain and\nwhat does it mean to do so? What is one doing when the conceptual\nconstellations of AI in particular are used in this fashion? Rooted in a\nWittgensteinian view of concepts and language-use, we consider two possible\nanswers and pit them against each other: either these examples are conceptual\nmetaphors, or they are attempts at conceptual engineering. We argue that they\nare conceptual metaphors, but that (1) this position is unaware of its own\nepistemological contingency, and (2) it risks committing the ''map-territory\nfallacy''. Down at the conceptual foundations of computation, (3) it most\nimportantly is a misleading 'double metaphor' because of the metaphorical\nconnection between human psychology and computation. In response to the\nshortcomings of this projected conceptual organisation of AI onto the human\ndomain, we argue that there is a semantic catch. The perspective of the\nconceptual metaphors shows avenues for forms of conceptual engineering. If this\nmethodology's criteria are met, the fallacies and epistemic shortcomings\nrelated to the conceptual metaphor view can be bypassed. At its best, the\ncross-pollution of the human and AI conceptual domains is one that prompts us\nto reflect anew on how the boundaries of our current concepts serve us and how\nthey could be approved.", "published": "2025-04-10 13:55:32", "link": "http://arxiv.org/abs/2504.07756v1", "categories": ["cs.AI", "cs.CY", "K.4"], "primary_category": "cs.AI"}
{"title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding", "abstract": "Video-based Large Language Models (Video-LLMs) have witnessed substantial\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\nAlthough these models have demonstrated proficiency in providing the overall\ndescription of videos, they struggle with fine-grained understanding,\nparticularly in aspects such as visual dynamics and video details inquiries. To\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\nself-supervised fragment tasks, greatly improve their fine-grained video\nunderstanding abilities. Hence we propose two key contributions:(1)\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\nmethod, employs the rich inherent characteristics of videos for training, while\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\nrelieves researchers from labor-intensive annotations and smartly circumvents\nthe limitations of natural language, which often fails to capture the complex\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\nscene and fragment levels, offering a comprehensive evaluation of their\ncapabilities. We assessed multiple models and validated the effectiveness of\nSF$^2$T on them. Experimental results reveal that our approach improves their\nability to capture and interpret spatiotemporal details.", "published": "2025-04-10 13:40:34", "link": "http://arxiv.org/abs/2504.07745v1", "categories": ["cs.CV", "cs.AI", "68T45", "I.4.8; I.5"], "primary_category": "cs.CV"}
{"title": "Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI", "abstract": "The segmentation of multiple organs in multi-parametric MRI studies is\ncritical for many applications in radiology, such as correlating imaging\nbiomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three\npublicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI\n(TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ\nsegmentation in MRI. However, the performance of these tools on specific MRI\nsequence types has not yet been quantified. In this work, a subset of 40\nvolumes from the public Duke Liver Dataset was curated. The curated dataset\ncontained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w,\nvenous T1w, and delayed T1w phases, respectively. Ten abdominal structures were\nmanually annotated in these volumes. Next, the performance of the three public\ntools was benchmarked on this curated dataset. The results indicated that MRSeg\nobtained a Dice score of 80.7 $\\pm$ 18.6 and Hausdorff Distance (HD) error of\n8.9 $\\pm$ 10.4 mm. It fared the best ($p < .05$) across the different sequence\ntypes in contrast to TS and VIBE.", "published": "2025-04-10 13:27:27", "link": "http://arxiv.org/abs/2504.07729v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security", "abstract": "Financial instability has become a significant issue in today's society.\nWhile research typically focuses on financial aspects, there is a tendency to\noverlook time-related aspects of unstable work schedules. The inability to rely\non consistent work schedules leads to burnout, work-family conflicts, and\nfinancial shocks that directly impact workers' income and assets. Unforeseen\nfluctuations in earnings pose challenges in financial planning, affecting\ndecisions on savings and spending and ultimately undermining individuals'\nlong-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience\nfrequently changing schedules without sufficient notice, including those in the\nfood service and retail sectors, part-time and hourly workers, and individuals\nwith lower incomes. These groups are already more financially vulnerable, and\nthe unpredictable nature of their schedules exacerbates their financial\nfragility.\n  Our objective is to understand how unforeseen fluctuations in earnings\nexacerbate financial fragility by investigating the extent to which\nindividuals' financial management depends on their ability to anticipate and\nplan for the future. To address this question, we develop a simulation\nframework that models how individuals optimize utility amidst financial\nuncertainty and the imperative to avoid financial ruin. We employ online\nlearning techniques, specifically adapting workers' consumption policies based\non evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a\nworker's capacity to anticipate schedule changes enhances their long-term\nutility. Conversely, the inability to predict future events can worsen workers'\ninstability. Moreover, our framework enables us to explore interventions to\nmitigate the problem of schedule uncertainty and evaluate their effectiveness.", "published": "2025-04-10 13:09:56", "link": "http://arxiv.org/abs/2504.07719v1", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG"}
{"title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.", "published": "2025-04-10 13:09:50", "link": "http://arxiv.org/abs/2504.07717v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams", "abstract": "Topic modeling is a key component in unsupervised learning, employed to\nidentify topics within a corpus of textual data. The rapid growth of social\nmedia generates an ever-growing volume of textual data daily, making online\ntopic modeling methods essential for managing these data streams that\ncontinuously arrive over time. This paper introduces a novel approach to online\ntopic modeling named StreamETM. This approach builds on the Embedded Topic\nModel (ETM) to handle data streams by merging models learned on consecutive\npartial document batches using unbalanced optimal transport. Additionally, an\nonline change point detection algorithm is employed to identify shifts in\ntopics over time, enabling the identification of significant changes in the\ndynamics of text streams. Numerical experiments on simulated and real-world\ndata show StreamETM outperforming competitors.", "published": "2025-04-10 13:04:56", "link": "http://arxiv.org/abs/2504.07711v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and Student Agents", "abstract": "Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible for students to\nsolve, or may contain critical issues such as incorrect tests. Existing works\noften require interventions from human teachers for validation. We address\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\nfirst generates a programming task and then decides whether it meets certain\nquality criteria to be given to students. The key idea is to break this process\ninto multiple stages performed by expert and student agents simulated using\nboth strong and weaker generative models. Through extensive evaluation, we show\nthat PyTaskSyn significantly improves task quality compared to baseline\ntechniques and showcases the importance of each specialized agent type in our\nvalidation pipeline. Additionally, we conducted user studies using our publicly\navailable web application and show that PyTaskSyn can deliver high-quality\nprogramming tasks comparable to expert-designed ones while reducing workload\nand costs, and being more engaging than programming tasks that are available in\nonline resources.", "published": "2025-04-10 11:08:39", "link": "http://arxiv.org/abs/2504.07655v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI"}
{"title": "ms-Mamba: Multi-scale Mamba for Time-Series Forecasting", "abstract": "The problem of Time-series Forecasting is generally addressed by recurrent,\nTransformer-based and the recently proposed Mamba-based architectures. However,\nexisting architectures generally process their input at a single temporal\nscale, which may be sub-optimal for many tasks where information changes over\nmultiple time scales. In this paper, we introduce a novel architecture called\nMulti-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates\nmultiple temporal scales by using multiple Mamba blocks with different sampling\nrates ($\\Delta$s). Our experiments on many benchmarks demonstrate that ms-Mamba\noutperforms state-of-the-art approaches, including the recently proposed\nTransformer-based and Mamba-based models.", "published": "2025-04-10 11:06:57", "link": "http://arxiv.org/abs/2504.07654v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning", "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities in natural\nlanguage processing but suffer from inaccuracies and logical inconsistencies\nknown as hallucinations. This compromises their reliability, especially in\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\nintegrating symbolic ontological reasoning and machine learning methods to\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\nand a lightweight machine learning model (logistic regression) for mapping\nnatural language statements into logical forms compatible with the ontology.\nWhen inconsistencies between LLM outputs and the ontology are detected, the\nsystem generates explanatory feedback to guide the LLM towards a corrected,\nlogically coherent response in an iterative refinement loop. We present a\nworking Python prototype demonstrating this pipeline. Experimental results in a\ndefined domain suggest significant improvements in semantic coherence and\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\nfluency with the rigor of formal semantics.", "published": "2025-04-10 10:39:24", "link": "http://arxiv.org/abs/2504.07640v1", "categories": ["cs.AI", "68T30", "I.2.3; I.2.4; I.2.6; I.2.7"], "primary_category": "cs.AI"}
{"title": "Predicting the Lifespan of Industrial Printheads with Survival Analysis", "abstract": "Accurately predicting the lifespan of critical device components is essential\nfor maintenance planning and production optimization, making it a topic of\nsignificant interest in both academia and industry. In this work, we\ninvestigate the use of survival analysis for predicting the lifespan of\nproduction printheads developed by Canon Production Printing. Specifically, we\nfocus on the application of five techniques to estimate survival probabilities\nand failure rates: the Kaplan-Meier estimator, Cox proportional hazard model,\nWeibull accelerated failure time model, random survival forest, and gradient\nboosting. The resulting estimates are further refined using isotonic regression\nand subsequently aggregated to determine the expected number of failures. The\npredictions are then validated against real-world ground truth data across\nmultiple time windows to assess model reliability. Our quantitative evaluation\nusing three performance metrics demonstrates that survival analysis outperforms\nindustry-standard baseline methods for printhead lifespan prediction.", "published": "2025-04-10 10:38:13", "link": "http://arxiv.org/abs/2504.07638v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Generative Artificial Intelligence for Internet of Things Computing: A Systematic Survey", "abstract": "The integration of Generative Artificial Intelligence (GenAI) within the\nInternet of Things (IoT) is garnering considerable interest. This growing\nattention stems from the continuous evolution and widespread adoption they are\nboth having individually, enough to spontaneously reshape numerous sectors,\nincluding Healthcare, Manufacturing, and Smart Cities. Hence, their increasing\npopularity has catalyzed further extensive research for understanding the\npotential of the duo GenAI-IoT, how they interplay, and to which extent their\nsynergy can innovate the state-of-the-art in their individual scenarios.\nHowever, despite the increasing prominence of GenAI for IoT Computing, much of\nthe existing research remains focused on specific, narrowly scoped\napplications. This fragmented approach highlights the need for a more\ncomprehensive analysis of the potential, challenges, and implications of GenAI\nintegration within the broader IoT ecosystem. This survey exactly aims to\naddress this gap by providing a holistic overview of the opportunities, issues,\nand considerations arising from the convergence of these mainstream paradigms.\nOur contribution is realized through a systematic literature review following\nthe PRISMA methodology. A comparison framework is presented, and well-defined\nresearch questions are outlined to comprehensively explore the past, present,\nand future directions of GenAI integration with IoT Computing, offering\nvaluable insights for both experts and newcomers.", "published": "2025-04-10 10:32:18", "link": "http://arxiv.org/abs/2504.07635v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather", "abstract": "Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two\nweeks to two month--are crucial for early warning systems but remain\nchallenging owing to chaos in the climate system. Teleconnections, such as the\nstratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer\nwindows of enhanced predictability, however, their complex interactions remain\nunderutilized in operational forecasting. Here, we developed and evaluated deep\nlearning architectures to predict North Atlantic-European (NAE) weather\nregimes, systematically assessing the role of remote drivers in improving S2S\nforecast skill of deep learning models. We implemented (1) a Long Short-term\nMemory (LSTM) network predicting the NAE regimes of the next six weeks based on\nprevious regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3)\na ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and\ntropical outgoing longwave radiation fields. These models are compared with\noperational hindcasts as well as other AI models. Our results show that\nleveraging teleconnection information enhances skill at longer lead times.\nNotably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4\nby improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions.\nAnalysis of high-confidence predictions reveals that NAO-, SB, and AR\nopportunity forecasts can be associated with SPV variability and MJO phase\npatterns aligning with established pathways, also indicating new patterns.\nOverall, our work demonstrates that encoding physically meaningful climate\nfields can enhance S2S prediction skill, advancing AI-driven subseasonal\nforecast. Moreover, the experiments highlight the potential of deep learning\nmethods as investigative tools, providing new insights into atmospheric\ndynamics and predictability.", "published": "2025-04-10 10:23:07", "link": "http://arxiv.org/abs/2504.07625v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Beating Transformers using Synthetic Cognition", "abstract": "The road to Artificial General Intelligence goes through the generation of\nepisodic reactive behaviors, where the Transformer architecture has been proven\nto be the state-of-the-art. However, they still fail to develop reasoning.\nRecently, a novel approach for developing cognitive architectures, called\nSynthetic Cognition, has been proposed and implemented to develop instantaneous\nreactive behavior. In this study, we aim to explore the use of Synthetic\nCognition to develop episodic reactive behaviors. We propose a mechanism to\ndeal with sequences for the recent implementation of Synthetic Cognition, and\ntest it against DNA foundation models in DNA sequence classification tasks. In\nour experiments, our proposal clearly outperforms the DNA foundation models,\nobtaining the best score on more benchmark tasks than the alternatives. Thus,\nwe achieve two goals: expanding Synthetic Cognition to deal with sequences, and\nbeating the Transformer architecture for sequence classification.", "published": "2025-04-10 10:07:05", "link": "http://arxiv.org/abs/2504.07619v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions", "abstract": "Current autonomous driving algorithms heavily rely on the visible spectrum,\nwhich is prone to performance degradation in adverse conditions like fog, rain,\nsnow, glare, and high contrast. Although other spectral bands like\nnear-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception\nin such situations, they have limitations and lack large-scale datasets and\nbenchmarks. Short-wave infrared (SWIR) imaging offers several advantages over\nNIR and LWIR. However, no publicly available large-scale datasets currently\nincorporate SWIR data for autonomous driving. To address this gap, we introduce\nthe RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000\nsynchronized and spatially aligned RGB-SWIR image pairs collected across\ndiverse locations, lighting, and weather conditions. In addition, we provide a\nsubset for RGB-SWIR translation and object detection annotations for a subset\nof challenging traffic scenarios to demonstrate the utility of SWIR imaging\nthrough experiments on both object detection and RGB-to-SWIR image translation.\nOur experiments show that combining RGB and SWIR data in an ensemble framework\nsignificantly improves detection accuracy compared to RGB-only approaches,\nparticularly in conditions where visible-spectrum sensors struggle. We\nanticipate that the RASMD dataset will advance research in multispectral\nimaging for autonomous driving and robust perception systems.", "published": "2025-04-10 09:54:57", "link": "http://arxiv.org/abs/2504.07603v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Learning Long Short-Term Intention within Human Daily Behaviors", "abstract": "In the domain of autonomous household robots, it is of utmost importance for\nrobots to understand human behaviors and provide appropriate services. This\nrequires the robots to possess the capability to analyze complex human\nbehaviors and predict the true intentions of humans. Traditionally, humans are\nperceived as flawless, with their decisions acting as the standards that robots\nshould strive to align with. However, this raises a pertinent question: What if\nhumans make mistakes? In this research, we present a unique task, termed \"long\nshort-term intention prediction\". This task requires robots can predict the\nlong-term intention of humans, which aligns with human values, and the short\nterm intention of humans, which reflects the immediate action intention.\nMeanwhile, the robots need to detect the potential non-consistency between the\nshort-term and long-term intentions, and provide necessary warnings and\nsuggestions. To facilitate this task, we propose a long short-term intention\nmodel to represent the complex intention states, and build a dataset to train\nthis intention model. Then we propose a two-stage method to integrate the\nintention model for robots: i) predicting human intentions of both value-based\nlong-term intentions and action-based short-term intentions; and 2) analyzing\nthe consistency between the long-term and short-term intentions. Experimental\nresults indicate that the proposed long short-term intention model can assist\nrobots in comprehending human behavioral patterns over both long-term and\nshort-term durations, which helps determine the consistency between long-term\nand short-term intentions of humans.", "published": "2025-04-10 09:50:18", "link": "http://arxiv.org/abs/2504.07597v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Boosting Universal LLM Reward Design through the Heuristic Reward Observation Space Evolution", "abstract": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.", "published": "2025-04-10 09:48:56", "link": "http://arxiv.org/abs/2504.07596v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Malware analysis assisted by AI with R2AI", "abstract": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.", "published": "2025-04-10 09:17:45", "link": "http://arxiv.org/abs/2504.07574v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs", "abstract": "We benchmark foundation models image embeddings for classification and\nretrieval in e-Commerce, evaluating their suitability for real-world\napplications. Our study spans embeddings from pre-trained convolutional and\ntransformer models trained via supervised, self-supervised, and text-image\ncontrastive learning. We assess full fine-tuning and transfer learning\n(top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars,\nfood, and retail. Results show full fine-tuning consistently performs well,\nwhile text-image and self-supervised embeddings can match its performance with\nless training. While supervised embeddings remain stable across architectures,\nSSL and contrastive embeddings vary significantly, often benefiting from\ntop-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning,\nreducing computational costs. We also explore cross-tuning, noting its impact\ndepends on dataset characteristics. Our findings offer practical guidelines for\nembedding selection and fine-tuning strategies, balancing efficiency and\nperformance.", "published": "2025-04-10 08:57:28", "link": "http://arxiv.org/abs/2504.07567v1", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Diffusion Transformers for Tabular Data Time Series Generation", "abstract": "Tabular data generation has recently attracted a growing interest due to its\ndifferent application scenarios. However, generating time series of tabular\ndata, where each element of the series depends on the others, remains a largely\nunexplored domain. This gap is probably due to the difficulty of jointly\nsolving different problems, the main of which are the heterogeneity of tabular\ndata (a problem common to non-time-dependent approaches) and the variable\nlength of a time series. In this paper, we propose a Diffusion Transformers\n(DiTs) based approach for tabular data series generation. Inspired by the\nrecent success of DiTs in image and video generation, we extend this framework\nto deal with heterogeneous data and variable-length sequences. Using extensive\nexperiments on six datasets, we show that the proposed approach outperforms\nprevious work by a large margin.", "published": "2025-04-10 08:56:09", "link": "http://arxiv.org/abs/2504.07566v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "ReXCL: A Tool for Requirement Document Extraction and Classification", "abstract": "This paper presents the ReXCL tool, which automates the extraction and\nclassification processes in requirement engineering, enhancing the software\ndevelopment lifecycle. The tool features two main modules: Extraction, which\nprocesses raw requirement documents into a predefined schema using heuristics\nand predictive modeling, and Classification, which assigns class labels to\nrequirements using adaptive fine-tuning of encoder-based models. The final\noutput can be exported to external requirement engineering tools. Performance\nevaluations indicate that ReXCL significantly improves efficiency and accuracy\nin managing requirements, marking a novel approach to automating the\nschematization of semi-structured requirement documents.", "published": "2025-04-10 08:46:54", "link": "http://arxiv.org/abs/2504.07562v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs", "abstract": "We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for\nblockchain consensus, where miners produce verifiable evidence of training\nlarge-scale machine-learning models. Building on previous work, we incorporate\n\\emph{quantized gradients} (4-bit precision) to reduce storage and computation\nrequirements, while still preserving the ability of verifiers to check that\nreal progress has been made on lowering the model's loss. Additionally, we\nemploy Merkle proofs over the full 32-bit model to handle large parameter sets\nand to enable random leaf checks with minimal on-chain data. We illustrate\nthese ideas using GPT-3 (175B parameters) as a reference example and also refer\nto smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B\nparameters). We provide an empirical cost analysis showing that verification is\nsignificantly cheaper than training, thanks in part to quantization and\nsampling. We also discuss the necessity of longer block times (potentially\nhours) when incorporating meaningful training steps, the trade-offs when using\nspecialized GPU hardware, and how binary diffs may incrementally optimize\nupdates. Finally, we note that fine-tuning can be handled in a similar manner,\nmerely changing the dataset and the manner of sampling but preserving the\noverall verification flow. Our protocol allows verifiers to issue either\n\\emph{positive} or \\emph{negative} attestations; these are aggregated at\nfinalization to either confirm the update or slash the miner.", "published": "2025-04-10 08:09:34", "link": "http://arxiv.org/abs/2504.07540v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure", "abstract": "Whether related to machine learning models' epistemic opacity, algorithmic\nclassification systems' discriminatory automation of testimonial prejudice, the\ndistortion of human beliefs via the 'hallucinations' of generative AI, the\ninclusion of the global South in global AI governance, the execution of\nbureaucratic violence via algorithmic systems, or located in the interaction\nwith conversational artificial agents epistemic injustice related to AI is a\ngrowing concern. Based on a proposed general taxonomy of epistemic injustice,\nthis paper first sketches a taxonomy of the types of epistemic injustice in the\ncontext of AI, relying on the work of scholars from the fields of philosophy of\ntechnology, political philosophy and social epistemology. Secondly, an\nadditional perspective on epistemic injustice in the context of AI: generative\nhermeneutical erasure. I argue that this injustice that can come about through\nthe application of Large Language Models (LLMs) and contend that generative AI,\nwhen being deployed outside of its Western space of conception, can have\neffects of conceptual erasure, particularly in the epistemic domain, followed\nby forms of conceptual disruption caused by a mismatch between AI system and\nthe interlocutor in terms of conceptual frameworks. AI systems' 'view from\nnowhere' epistemically inferiorizes non-Western epistemologies and thereby\ncontributes to the erosion of their epistemic particulars, gradually\ncontributing to hermeneutical erasure. This work's relevance lies in proposal\nof a taxonomy that allows epistemic injustices to be mapped in the AI domain\nand the proposal of a novel form of AI-related epistemic injustice.", "published": "2025-04-10 07:54:47", "link": "http://arxiv.org/abs/2504.07531v1", "categories": ["cs.AI", "cs.CY", "K.4"], "primary_category": "cs.AI"}
{"title": "Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data", "abstract": "Outlier detection in high-dimensional tabular data is challenging since data\nis often distributed across multiple lower-dimensional subspaces -- a\nphenomenon known as the Multiple Views effect (MV). This effect led to a large\nbody of research focused on mining such subspaces, known as subspace selection.\nHowever, as the precise nature of the MV effect was not well understood,\ntraditional methods had to rely on heuristic-driven search schemes that\nstruggle to accurately capture the true structure of the data. Properly\nidentifying these subspaces is critical for unsupervised tasks such as outlier\ndetection or clustering, where misrepresenting the underlying data structure\ncan hinder the performance. We introduce Myopic Subspace Theory (MST), a new\ntheoretical framework that mathematically formulates the Multiple Views effect\nand writes subspace selection as a stochastic optimization problem. Based on\nMST, we introduce V-GAN, a generative method trained to solve such an\noptimization problem. This approach avoids any exhaustive search over the\nfeature space while ensuring that the intrinsic data structure is preserved.\nExperiments on 42 real-world datasets show that using V-GAN subspaces to build\nensemble methods leads to a significant increase in one-class classification\nperformance -- compared to existing subspace selection, feature selection, and\nembedding methods. Further experiments on synthetic data show that V-GAN\nidentifies subspaces more accurately while scaling better than other relevant\nsubspace selection methods. These results confirm the theoretical guarantees of\nour approach and also highlight its practical viability in high-dimensional\nsettings.", "published": "2025-04-10 07:40:02", "link": "http://arxiv.org/abs/2504.07522v1", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH", "68T07"], "primary_category": "cs.LG"}
{"title": "Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models", "abstract": "Most existing emotion analysis emphasizes which emotion arises (e.g., happy,\nsad, angry) but neglects the deeper why. We propose Emotion Interpretation\n(EI), focusing on causal factors-whether explicit (e.g., observable objects,\ninterpersonal interactions) or implicit (e.g., cultural context, off-screen\nevents)-that drive emotional responses. Unlike traditional emotion recognition,\nEI tasks require reasoning about triggers instead of mere labeling. To\nfacilitate EI research, we present EIBench, a large-scale benchmark\nencompassing 1,615 basic EI samples and 50 complex EI samples featuring\nmultifaceted emotions. Each instance demands rationale-based explanations\nrather than straightforward categorization. We further propose a Coarse-to-Fine\nSelf-Ask (CFSA) annotation pipeline, which guides Vision-Language Models\n(VLLMs) through iterative question-answer rounds to yield high-quality labels\nat scale. Extensive evaluations on open-source and proprietary large language\nmodels under four experimental settings reveal consistent performance\ngaps-especially for more intricate scenarios-underscoring EI's potential to\nenrich empathetic, context-aware AI applications. Our benchmark and methods are\npublicly available at: https://github.com/Lum1104/EIBench, offering a\nfoundation for advanced multimodal causal analysis and next-generation\naffective computing.", "published": "2025-04-10 07:33:49", "link": "http://arxiv.org/abs/2504.07521v1", "categories": ["cs.AI", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Enhancements for Developing a Comprehensive AI Fairness Assessment Standard", "abstract": "As AI systems increasingly influence critical sectors like\ntelecommunications, finance, healthcare, and public services, ensuring fairness\nin decision-making is essential to prevent biased or unjust outcomes that\ndisproportionately affect vulnerable entities or result in adverse impacts.\nThis need is particularly pressing as the industry approaches the 6G era, where\nAI will drive complex functions like autonomous network management and\nhyper-personalized services. The TEC Standard for Fairness Assessment and\nRating of AI Systems provides guidelines for evaluating fairness in AI,\nfocusing primarily on tabular data and supervised learning models. However, as\nAI applications diversify, this standard requires enhancement to strengthen its\nimpact and broaden its applicability. This paper proposes an expansion of the\nTEC Standard to include fairness assessments for images, unstructured text, and\ngenerative AI, including large language models, ensuring a more comprehensive\napproach that keeps pace with evolving AI technologies. By incorporating these\ndimensions, the enhanced framework will promote responsible and trustworthy AI\ndeployment across various sectors.", "published": "2025-04-10 07:24:23", "link": "http://arxiv.org/abs/2504.07516v1", "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "cs.CY"}
{"title": "GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable", "abstract": "Modern large language foundation models (LLM) have now entered the daily\nlives of millions of users. We ask a natural question whether it is possible to\ncustomize LLM for every user or every task. From system and industrial economy\nconsideration, general continue-training or fine-tuning still require\nsubstantial computation and memory of training GPU nodes, whereas most\ninference nodes under deployment, possibly with lower-end GPUs, are configured\nto make forward pass fastest possible. We propose a framework to take full\nadvantages of existing LLMs and systems of online service. We train an\nadditional branch of transformer blocks on the final-layer embedding of\npretrained LLMs, which is the base, then a carry-on module merge the base\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\nLLMs specialized in different domains such as chat, coding, math, to form a new\nmixture of LLM that best fit a new task. As the base model don't need to update\nparameters, we are able to outsource most computation of the training job on\ninference nodes, and only train a lightweight carry-on on training nodes, where\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\nfaster loss convergence. We use it to improve solving math questions with\nextremely small computation and model size, with 1000 data samples of\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\nand the results are promising.", "published": "2025-04-10 07:15:40", "link": "http://arxiv.org/abs/2504.07513v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bottleneck Identification in Resource-Constrained Project Scheduling via Constraint Relaxation", "abstract": "In realistic production scenarios, Advanced Planning and Scheduling (APS)\ntools often require manual intervention by production planners, as the system\nworks with incomplete information, resulting in suboptimal schedules. Often,\nthe preferable solution is not found just because of the too-restrictive\nconstraints specifying the optimization problem, representing bottlenecks in\nthe schedule. To provide computer-assisted support for decision-making, we aim\nto automatically identify bottlenecks in the given schedule while linking them\nto the particular constraints to be relaxed. In this work, we address the\nproblem of reducing the tardiness of a particular project in an obtained\nschedule in the resource-constrained project scheduling problem by relaxing\nconstraints related to identified bottlenecks. We develop two methods for this\npurpose. The first method adapts existing approaches from the job shop\nliterature and utilizes them for so-called untargeted relaxations. The second\nmethod identifies potential improvements in relaxed versions of the problem and\nproposes targeted relaxations. Surprisingly, the untargeted relaxations result\nin improvements comparable to the targeted relaxations.", "published": "2025-04-10 06:53:10", "link": "http://arxiv.org/abs/2504.07495v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources", "abstract": "Digital images of Chinas maps play a crucial role in map detection,\nparticularly in ensuring national sovereignty, territorial integrity, and map\ncompliance. However, there is currently no publicly available dataset\nspecifically dedicated to problematic maps the CME dataset. Existing datasets\nprimarily focus on general map data and are insufficient for effectively\nidentifying complex issues such as national boundary misrepresentations,\nmissing elements, and blurred boundaries. Therefore, this study creates a\nProblematic Map dataset that covers five key problem areas, aiming to provide\ndiverse samples for problematic map detection technologies, support\nhigh-precision map compliance detection, and enhance map data quality and\ntimeliness. This dataset not only provides essential resources for map\ncompliance, national security monitoring, and map updates, but also fosters\ninnovation and application of related technologies.", "published": "2025-04-10 06:04:16", "link": "http://arxiv.org/abs/2504.07476v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Enhanced Question-Answering for Skill-based learning using Knowledge-based AI and Generative AI", "abstract": "Supporting learners' understanding of taught skills in online settings is a\nlongstanding challenge. While exercises and chat-based agents can evaluate\nunderstanding in limited contexts, this challenge is magnified when learners\nseek explanations that delve into procedural knowledge (how things are done)\nand reasoning (why things happen). We hypothesize that an intelligent agent's\nability to understand and explain learners' questions about skills can be\nsignificantly enhanced using the TMK (Task-Method-Knowledge) model, a\nKnowledge-based AI framework. We introduce Ivy, an intelligent agent that\nleverages an LLM and iterative refinement techniques to generate explanations\nthat embody teleological, causal, and compositional principles. Our initial\nevaluation demonstrates that this approach goes beyond the typical shallow\nresponses produced by an agent with access to unstructured text, thereby\nsubstantially improving the depth and relevance of feedback. This can\npotentially ensure learners develop a comprehensive understanding of skills\ncrucial for effective problem-solving in online environments.", "published": "2025-04-10 05:25:52", "link": "http://arxiv.org/abs/2504.07463v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for Whole-Body PET Attenuation Correction", "abstract": "Positron Emission Tomography (PET) imaging requires accurate attenuation\ncorrection (AC) to account for photon loss due to tissue density variations. In\nPET/MR systems, computed tomography (CT), which offers a straightforward\nestimation of AC is not available. This study presents a deep learning approach\nto generate synthetic CT (sCT) images directly from Time-of-Flight (TOF)\nnon-attenuation corrected (NAC) PET images, enhancing AC for PET/MR. We first\nevaluated models pre-trained on large-scale natural image datasets for a\nCT-to-CT reconstruction task, finding that the pre-trained model outperformed\nthose trained solely on medical datasets. The pre-trained model was then\nfine-tuned using an institutional dataset of 35 TOF NAC PET and CT volume\npairs, achieving the lowest mean absolute error (MAE) of 74.49 HU and highest\npeak signal-to-noise ratio (PSNR) of 28.66 dB within the body contour region.\nVisual assessments demonstrated improved reconstruction of both bone and soft\ntissue structures from TOF NAC PET images. This work highlights the\neffectiveness of using pre-trained deep learning models for medical image\ntranslation tasks. Future work will assess the impact of sCT on PET attenuation\ncorrection and explore additional neural network architectures and datasets to\nfurther enhance performance and practical applications in PET imaging.", "published": "2025-04-10 04:49:41", "link": "http://arxiv.org/abs/2504.07450v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "68T05, 92C55", "I.2.6; I.2.10"], "primary_category": "eess.IV"}
{"title": "Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games", "abstract": "Deep reinforcement learning (DRL) has effectively enhanced gameplay\nexperiences and game design across various game genres. However, few studies on\nfighting game agents have focused explicitly on enhancing player enjoyment, a\ncritical factor for both developers and players. To address this gap and\nestablish a practical baseline for designing enjoyability-focused agents, we\npropose a two-tier agent (TTA) system and conducted experiments in the classic\nfighting game Street Fighter II. The first tier of TTA employs a task-oriented\nnetwork architecture, modularized reward functions, and hybrid training to\nproduce diverse and skilled DRL agents. In the second tier of TTA, a Large\nLanguage Model Hyper-Agent, leveraging players' playing data and feedback,\ndynamically selects suitable DRL opponents. In addition, we investigate and\nmodel several key factors that affect the enjoyability of the opponent. The\nexperiments demonstrate improvements from 64. 36% to 156. 36% in the execution\nof advanced skills over baseline methods. The trained agents also exhibit\ndistinct game-playing styles. Additionally, we conducted a small-scale user\nstudy, and the overall enjoyment in the player's feedback validates the\neffectiveness of our TTA system.", "published": "2025-04-10 03:38:06", "link": "http://arxiv.org/abs/2504.07425v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing", "abstract": "Instruction-based Image Editing (IIE) models have made significantly\nimprovement due to the progress of multimodal large language models (MLLMs) and\ndiffusion models, which can understand and reason about complex editing\ninstructions. In addition to advancing current IIE models, accurately\nevaluating their output has become increasingly critical and challenging.\nCurrent IIE evaluation methods and their evaluation procedures often fall short\nof aligning with human judgment and often lack explainability. To address these\nlimitations, we propose JUdgement through Routing of Expertise (JURE). Each\nexpert in JURE is a pre-selected model assumed to be equipped with an atomic\nexpertise that can provide useful feedback to judge output, and the router\ndynamically routes the evaluation task of a given instruction and its output to\nappropriate experts, aggregating their feedback into a final judge. JURE is\ntrustworthy in two aspects. First, it can effortlessly provide explanations\nabout its judge by examining the routed experts and their feedback. Second,\nexperimental results demonstrate that JURE is reliable by achieving superior\nalignment with human judgments, setting a new standard for automated IIE\nevaluation. Moreover, JURE's flexible design is future-proof - modular experts\ncan be seamlessly replaced or expanded to accommodate advancements in IIE,\nmaintaining consistently high evaluation quality. Our evaluation data and\nresults are available at https://github.com/Cyyyyyrus/JURE.git.", "published": "2025-04-10 03:30:15", "link": "http://arxiv.org/abs/2504.07424v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support", "abstract": "As AI-based clinical decision support (AI-CDS) is introduced in more and more\naspects of healthcare services, HCI research plays an increasingly important\nrole in designing for complementarity between AI and clinicians. However,\ncurrent evaluations of AI-CDS often fail to capture when AI is and is not\nuseful to clinicians. This position paper reflects on our work and influential\nAI-CDS literature to advocate for moving beyond evaluation metrics like Trust,\nReliance, Acceptance, and Performance on the AI's task (what we term the \"trap\"\nof human-AI collaboration). Although these metrics can be meaningful in some\nsimple scenarios, we argue that optimizing for them ignores important ways that\nAI falls short of clinical benefit, as well as ways that clinicians\nsuccessfully use AI. As the fields of HCI and AI in healthcare develop new ways\nto design and evaluate CDS tools, we call on the community to prioritize\necologically valid, domain-appropriate study setups that measure the emergent\nforms of value that AI can bring to healthcare professionals.", "published": "2025-04-10 03:28:56", "link": "http://arxiv.org/abs/2504.07423v1", "categories": ["cs.HC", "cs.AI", "q-bio.OT"], "primary_category": "cs.HC"}
{"title": "The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses", "abstract": "This study reveals the important role of prevention care and medication\nadherence in reducing hospitalizations. By using a structured dataset of 1,171\npatients, four machine learning models Logistic Regression, Gradient Boosting,\nRandom Forest, and Artificial Neural Networks are applied to predict five-year\nhospitalization risk, with the Gradient Boosting model achieving the highest\naccuracy of 81.2%. The result demonstrated that patients with high medication\nadherence and consistent preventive care can reduce 38.3% and 37.7% in\nhospitalization risk. The finding also suggests that targeted preventive care\ncan have positive Return on Investment (ROI), and therefore ML models can\neffectively direct personalized interventions and contribute to long-term\nmedical savings.", "published": "2025-04-10 03:28:42", "link": "http://arxiv.org/abs/2504.07422v1", "categories": ["cs.LG", "cs.AI", "cs.CY", "68T05, 68T09, 68U03, 62P10", "I.2; J.3; H.2; J.4; K.4"], "primary_category": "cs.LG"}
{"title": "LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models", "abstract": "We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for\nTarget Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a\nsmall-scale auto-regressive decoder-only language model which takes the\ncontinuous representations for both the mixture and the reference speeches and\nproduces the first few layers of the target speech's discrete codec\nrepresentations. In addition, a one-step encoder-only language model\nreconstructs the sum of the predicted codec embeddings using both the mixture\nand the reference information. Our approach achieves superior or comparable\nperformance to existing generative and discriminative TSE models. To the best\nof our knowledge, LauraTSE is the first single-task TSE model to leverage an\nauto-regressive decoder-only language model as the backbone.", "published": "2025-04-10 02:55:22", "link": "http://arxiv.org/abs/2504.07402v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Novel Mamba-based Sequential Recommendation Method", "abstract": "Sequential recommendation (SR), which encodes user activity to predict the\nnext action, has emerged as a widely adopted strategy in developing commercial\npersonalized recommendation systems. Although Transformer-based models have\nproven effective for sequential recommendation, the complexity of the\nself-attention module in Transformers scales quadratically with the sequence\nlength. Controlling model complexity is essential for large-scale\nrecommendation systems, as these systems may need to handle billion-scale\nvocabularies that evolve continuously, as well as user behavior sequences that\ncan exceed tens of thousands in length. In this paper, we propose a novel\nmulti-head latent Mamba architecture, which employs multiple low-dimensional\nMamba layers and fully connected layers coupled with positional encoding to\nsimultaneously capture historical and item information within each latent\nsubspace. Our proposed method not only enables scaling up to large-scale\nparameters but also extends to multi-domain recommendation by integrating and\nfine-tuning LLMs. Through extensive experiments on public datasets, we\ndemonstrate how Hydra effectively addresses the effectiveness-efficiency\ndilemma, outperforming state-of-the-art sequential recommendation baselines\nwith significantly fewer parameters and reduced training time.", "published": "2025-04-10 02:43:19", "link": "http://arxiv.org/abs/2504.07398v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "MicroNAS: An Automated Framework for Developing a Fall Detection System", "abstract": "This work presents MicroNAS, an automated neural architecture search tool\nspecifically designed to create models optimized for microcontrollers with\nsmall memory resources. The ESP32 microcontroller, with 320 KB of memory, is\nused as the target platform. The artificial intelligence contribution lies in a\nnovel method for optimizing convolutional neural network and gated recurrent\nunit architectures by considering the memory size of the target microcontroller\nas a guide. A comparison is made between memory-driven model optimization and\ntraditional two-stage methods, which use pruning, to show the effectiveness of\nthe proposed framework. To demonstrate the engineering application of MicroNAS,\na fall detection system (FDS) for lower-limb amputees is developed as a pilot\nstudy. A critical challenge in fall detection studies, class imbalance in the\ndataset, is addressed. The results show that MicroNAS models achieved higher\nF1-scores than alternative approaches, such as ensemble methods and H2O\nAutomated Machine Learning, presenting a significant step forward in real-time\nFDS development. Biomechanists using body-worn sensors for activity detection\ncan adopt the open-source code to design machine learning models tailored for\nmicrocontroller platforms with limited memory.", "published": "2025-04-10 02:32:47", "link": "http://arxiv.org/abs/2504.07397v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Automating quantum feature map design via large language models", "abstract": "Quantum feature maps are a key component of quantum machine learning,\nencoding classical data into quantum states to exploit the expressive power of\nhigh-dimensional Hilbert spaces. Despite their theoretical promise, designing\nquantum feature maps that offer practical advantages over classical methods\nremains an open challenge. In this work, we propose an agentic system that\nautonomously generates, evaluates, and refines quantum feature maps using large\nlanguage models. The system consists of five component: Generation, Storage,\nValidation, Evaluation, and Review. Using these components, it iteratively\nimproves quantum feature maps. Experiments on the MNIST dataset show that it\ncan successfully discover and refine feature maps without human intervention.\nThe best feature map generated outperforms existing quantum baselines and\nachieves competitive accuracy compared to classical kernels across MNIST,\nFashion-MNIST, and CIFAR-10. Our approach provides a framework for exploring\ndataset-adaptive quantum features and highlights the potential of LLM-driven\nautomation in quantum algorithm design.", "published": "2025-04-10 02:27:45", "link": "http://arxiv.org/abs/2504.07396v1", "categories": ["quant-ph", "cs.AI"], "primary_category": "quant-ph"}
{"title": "FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair", "abstract": "We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure\nfairness in computer vision systems by combining conformal prediction with a\ndynamic output repair mechanism. Our approach calculates a fairness-aware\nnon-conformity score that simultaneously assesses prediction errors and\nfairness violations. Using conformal prediction, we establish an adaptive\nthreshold that provides rigorous finite-sample, distribution-free guarantees.\nWhen the non-conformity score for a new image exceeds the calibrated threshold,\nFAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for\nclassification and confidence recalibration for detection, to reduce both group\nand individual fairness disparities, all without the need for retraining or\nhaving access to internal model parameters. Comprehensive theoretical analysis\nvalidates our method's error control and convergence properties. At the same\ntime, extensive empirical evaluations on benchmark datasets show that\nFAIR-SIGHT significantly reduces fairness disparities while preserving high\npredictive performance.", "published": "2025-04-10 02:23:06", "link": "http://arxiv.org/abs/2504.07395v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method", "abstract": "Climate science studies the structure and dynamics of Earth's climate system\nand seeks to understand how climate changes over time, where the data is\nusually stored in the format of time series, recording the climate features,\ngeolocation, time attributes, etc. Recently, much research attention has been\npaid to the climate benchmarks. In addition to the most common task of weather\nforecasting, several pioneering benchmark works are proposed for extending the\nmodality, such as domain-specific applications like tropical cyclone intensity\nprediction and flash flood damage estimation, or climate statement and\nconfidence level in the format of natural language. To further motivate the\nartificial general intelligence development for climate science, in this paper,\nwe first contribute a multi-modal climate benchmark, i.e., ClimateBench-M,\nwhich aligns (1) the time series climate data from ERA5, (2) extreme weather\nevents data from NOAA, and (3) satellite image data from NASA HLS based on a\nunified spatial-temporal granularity. Second, under each data modality, we also\npropose a simple but strong generative method that could produce competitive\nperformance in weather forecasting, thunderstorm alerts, and crop segmentation\ntasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are\npublicly available at https://github.com/iDEA-iSAIL-Lab-UIUC/ClimateBench-M.", "published": "2025-04-10 02:22:23", "link": "http://arxiv.org/abs/2504.07394v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm", "abstract": "This study explores the performance of the random Gaussian smoothing\nZeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation\nproblems with possibly NonConvex-NonConcave (NC-NC) objective functions. We\nconsider both unconstrained and constrained, differentiable and\nnon-differentiable settings. We discuss the min-max problem from the point of\nview of variational inequalities. For the unconstrained problem, we establish\nthe convergence of the ZO-EG algorithm to the neighbourhood of an\n$\\epsilon$-stationary point of the NC-NC objective function, whose radius can\nbe controlled under a variance reduction scheme, along with its complexity. For\nthe constrained problem, we introduce the new notion of proximal variational\ninequalities and give examples of functions satisfying this property. Moreover,\nwe prove analogous results to the unconstrained case for the constrained\nproblem. For the non-differentiable case, we prove the convergence of the ZO-EG\nalgorithm to a neighbourhood of an $\\epsilon$-stationary point of the smoothed\nversion of the objective function, where the radius of the neighbourhood can be\ncontrolled, which can be related to the ($\\delta,\\epsilon$)-Goldstein\nstationary point of the original objective function.", "published": "2025-04-10 02:15:30", "link": "http://arxiv.org/abs/2504.07388v1", "categories": ["math.OC", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning", "abstract": "This paper considers how to fuse Machine Learning (ML) and optimization to\nsolve large-scale Supply Chain Planning (SCP) optimization problems. These\nproblems can be formulated as MIP models which feature both integer\n(non-binary) and continuous variables, as well as flow balance and capacity\nconstraints. This raises fundamental challenges for existing integrations of ML\nand optimization that have focused on binary MIPs and graph problems. To\naddress these, the paper proposes PROPEL, a new framework that combines\noptimization with both supervised and Deep Reinforcement Learning (DRL) to\nreduce the size of search space significantly. PROPEL uses supervised learning,\nnot to predict the values of all integer variables, but to identify the\nvariables that are fixed to zero in the optimal solution, leveraging the\nstructure of SCP applications. PROPEL includes a DRL component that selects\nwhich fixed-at-zero variables must be relaxed to improve solution quality when\nthe supervised learning step does not produce a solution with the desired\noptimality tolerance. PROPEL has been applied to industrial supply chain\nplanning optimizations with millions of variables. The computational results\nshow dramatic improvements in solution times and quality, including a 60%\nreduction in primal integral and an 88% primal gap reduction, and improvement\nfactors of up to 13.57 and 15.92, respectively.", "published": "2025-04-10 02:04:29", "link": "http://arxiv.org/abs/2504.07383v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology", "abstract": "Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as\nan effective counterpart to the original multilayer perceptron-based\nPhysics-Informed Neural Networks (PINNs). Both representation models can\naddress inverse problems and facilitate gray-box system identification.\nHowever, a comprehensive understanding of their performance in terms of\naccuracy and speed remains underexplored. In particular, we introduce a\nmodified PIKAN architecture, tanh-cPIKAN, which is based on Chebyshev\npolynomials for parametrization of the univariate functions with an extra\nnonlinearity for enhanced performance. We then present a systematic\ninvestigation of how choices of the optimizer, representation, and training\nconfiguration influence the performance of PINNs and PIKANs in the context of\nsystems pharmacology modeling. We benchmark a wide range of first-order,\nsecond-order, and hybrid optimizers, including various learning rate\nschedulers. We use the new Optax library to identify the most effective\ncombinations for learning gray-boxes under ill-posed, non-unique, and\ndata-sparse conditions. We examine the influence of model architecture (MLP vs.\nKAN), numerical precision (single vs. double), the need for warm-up phases for\nsecond-order methods, and sensitivity to the initial learning rate. We also\nassess the optimizer scalability for larger models and analyze the trade-offs\nintroduced by JAX in terms of computational efficiency and numerical accuracy.\nUsing two representative systems pharmacology case studies - a pharmacokinetics\nmodel and a chemotherapy drug-response model - we offer practical guidance on\nselecting optimizers and representation models/architectures for robust and\nefficient gray-box discovery. Our findings provide actionable insights for\nimproving the training of physics-informed networks in biomedical applications\nand beyond.", "published": "2025-04-10 01:37:18", "link": "http://arxiv.org/abs/2504.07379v1", "categories": ["q-bio.QM", "cs.AI", "35R30 (Primary), 65M32, 92C50 (Secondary)", "I.2.6; G.1.7; G.1.10"], "primary_category": "q-bio.QM"}
{"title": "ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling", "abstract": "The temporal complexity of electronic health record (EHR) data presents\nsignificant challenges for predicting clinical outcomes using machine learning.\nThis paper proposes ChronoFormer, an innovative transformer based architecture\nspecifically designed to encode and leverage temporal dependencies in\nlongitudinal patient data. ChronoFormer integrates temporal embeddings,\nhierarchical attention mechanisms, and domain specific masking techniques.\nExtensive experiments conducted on three benchmark tasks mortality prediction,\nreadmission prediction, and long term comorbidity onset demonstrate substantial\nimprovements over current state of the art methods. Furthermore, detailed\nanalyses of attention patterns underscore ChronoFormer's capability to capture\nclinically meaningful long range temporal relationships.", "published": "2025-04-10 01:25:41", "link": "http://arxiv.org/abs/2504.07373v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "A Balanced Approach of Rapid Genetic Exploration and Surrogate Exploitation for Hyperparameter Optimization", "abstract": "This paper proposes a new method for hyperparameter optimization (HPO) that\nbalances exploration and exploitation. While evolutionary algorithms (EAs) show\npromise in HPO, they often struggle with effective exploitation. To address\nthis, we integrate a linear surrogate model into a genetic algorithm (GA),\nallowing for smooth integration of multiple strategies. This combination\nimproves exploitation performance, achieving an average improvement of 1.89\npercent (max 6.55 percent, min -3.45 percent) over existing HPO methods.", "published": "2025-04-10 00:59:54", "link": "http://arxiv.org/abs/2504.07359v1", "categories": ["cs.NE", "cs.AI", "68T20", "I.2.8; G.1.6"], "primary_category": "cs.NE"}
{"title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics", "abstract": "The cacophony of urban sounds presents a significant challenge for smart city\napplications that rely on accurate acoustic scene analysis. Effectively\nanalyzing these complex soundscapes, often characterized by overlapping sound\nsources, diverse acoustic events, and unpredictable noise levels, requires\nprecise source separation. This task becomes more complicated when only limited\ntraining data is available. This paper introduces a novel Quantum-Inspired\nGenetic Algorithm (p-QIGA) for source separation, drawing inspiration from\nquantum information theory to enhance acoustic scene analysis in smart cities.\nBy leveraging quantum superposition for efficient solution space exploration\nand entanglement to handle correlated sources, p-QIGA achieves robust\nseparation even with limited data. These quantum-inspired concepts are\nintegrated into a genetic algorithm framework to optimize source separation\nparameters. The effectiveness of our approach is demonstrated on two datasets:\nthe TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban\nsoundscapes, and the Silent Cities dataset, capturing quieter urban\nenvironments during the COVID-19 pandemic. Experimental results show that the\np-QIGA achieves accuracy comparable to state-of-the-art methods while\nexhibiting superior resilience to noise and limited training data, achieving up\nto 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and\noutperforming baseline methods by up to 2 dB with only 10% of the training\ndata. This research highlights the potential of p-QIGA to advance acoustic\nsignal processing in smart cities, particularly for noise pollution monitoring\nand acoustic surveillance.", "published": "2025-04-10 00:05:35", "link": "http://arxiv.org/abs/2504.07345v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PixelFlow: Pixel-Space Generative Models with Flow", "abstract": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256$\\times$256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.", "published": "2025-04-10 17:59:56", "link": "http://arxiv.org/abs/2504.07963v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction", "abstract": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes.", "published": "2025-04-10 17:59:55", "link": "http://arxiv.org/abs/2504.07961v1", "categories": ["cs.CV", "I.4.5"], "primary_category": "cs.CV"}
{"title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation", "abstract": "This paper proposes a novel framework utilizing multi-modal large language\nmodels (MLLMs) for referring video object segmentation (RefVOS). Previous\nMLLM-based methods commonly struggle with the dilemma between \"Ref\" and \"VOS\":\nthey either specialize in understanding a few key frames (global reasoning) or\ntracking objects on continuous frames (local reasoning), and rely on external\nVOS or frame selectors to mitigate the other end of the challenge. However, our\nframework GLUS shows that global and local consistency can be unified into a\nsingle video segmentation MLLM: a set of sparse \"context frames\" provides\nglobal information, while a stream of continuous \"query frames\" conducts local\nobject tracking. This is further supported by jointly training the MLLM with a\npre-trained VOS memory bank to simultaneously digest short-range and long-range\ntemporal information. To improve the information efficiency within the limited\ncontext window of MLLMs, we introduce object contrastive learning to\ndistinguish hard false-positive objects and a self-refined framework to\nidentify crucial frames and perform propagation. By collectively integrating\nthese insights, our GLUS delivers a simple yet effective baseline, achieving\nnew state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our\nproject page is at https://glus-video.github.io/.", "published": "2025-04-10 17:59:55", "link": "http://arxiv.org/abs/2504.07962v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning", "abstract": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.", "published": "2025-04-10 17:59:42", "link": "http://arxiv.org/abs/2504.07960v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy", "abstract": "Computational color constancy, or white balancing, is a key module in a\ncamera's image signal processor (ISP) that corrects color casts from scene\nlighting. Because this operation occurs in the camera-specific raw color space,\nwhite balance algorithms must adapt to different cameras. This paper introduces\na learning-based method for cross-camera color constancy that generalizes to\nnew cameras without retraining. Our method leverages pre-calibrated color\ncorrection matrices (CCMs) available on ISPs that map the camera's raw color\nspace to a standard space (e.g., CIE XYZ). Our method uses these CCMs to\ntransform predefined illumination colors (i.e., along the Planckian locus) into\nthe test camera's raw space. The mapped illuminants are encoded into a compact\ncamera fingerprint embedding (CFE) that enables the network to adapt to unseen\ncameras. To prevent overfitting due to limited cameras and CCMs during\ntraining, we introduce a data augmentation technique that interpolates between\ncameras and their CCMs. Experimental results across multiple datasets and\nbackbones show that our method achieves state-of-the-art cross-camera color\nconstancy while remaining lightweight and relying only on data readily\navailable in camera ISPs.", "published": "2025-04-10 17:59:31", "link": "http://arxiv.org/abs/2504.07959v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Detect Anything 3D in the Wild", "abstract": "Despite the success of deep learning in close-set 3D object detection,\nexisting approaches struggle with zero-shot generalization to novel objects and\ncamera configurations. We introduce DetAny3D, a promptable 3D detection\nfoundation model capable of detecting any novel object under arbitrary camera\nconfigurations using only monocular inputs. Training a foundation model for 3D\ndetection is fundamentally constrained by the limited availability of annotated\n3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded\nin extensively pre-trained 2D foundation models to compensate for this\nscarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two\ncore modules: the 2D Aggregator, which aligns features from different 2D\nfoundation models, and the 3D Interpreter with Zero-Embedding Mapping, which\nmitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental\nresults validate the strong generalization of our DetAny3D, which not only\nachieves state-of-the-art performance on unseen categories and novel camera\nconfigurations, but also surpasses most competitors on in-domain data.DetAny3D\nsheds light on the potential of the 3D foundation model for diverse\napplications in real-world scenarios, e.g., rare object detection in autonomous\ndriving, and demonstrates promise for further exploration of 3D-centric tasks\nin open-world settings. More visualization results can be found at DetAny3D\nproject page.", "published": "2025-04-10 17:59:22", "link": "http://arxiv.org/abs/2504.07958v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MM-IFEngine: Towards Multimodal Instruction Following", "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval\n(+12.3$\\%$). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.", "published": "2025-04-10 17:59:12", "link": "http://arxiv.org/abs/2504.07957v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation", "abstract": "This paper presents a generalizable RGB-based approach for object pose\nestimation, specifically designed to address challenges in sparse-view\nsettings. While existing methods can estimate the poses of unseen objects,\ntheir generalization ability remains limited in scenarios involving occlusions\nand sparse reference views, restricting their real-world applicability. To\novercome these limitations, we introduce corner points of the object bounding\nbox as an intermediate representation of the object pose. The 3D object corners\ncan be reliably recovered from sparse input views, while the 2D corner points\nin the target view are estimated through a novel reference-based point\nsynthesizer, which works well even in scenarios involving occlusions. As object\nsemantic points, object corners naturally establish 2D-3D correspondences for\nobject pose estimation with a PnP algorithm. Extensive experiments on the\nYCB-Video and Occluded-LINEMOD datasets show that our approach outperforms\nstate-of-the-art methods, highlighting the effectiveness of the proposed\nrepresentation and significantly enhancing the generalization capabilities of\nobject pose estimation, which is crucial for real-world applications.", "published": "2025-04-10 17:58:35", "link": "http://arxiv.org/abs/2504.07955v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models", "abstract": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.", "published": "2025-04-10 17:57:28", "link": "http://arxiv.org/abs/2504.07951v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians", "abstract": "With the rising interest from the community in digital avatars coupled with\nthe importance of expressions and gestures in communication, modeling natural\navatar behavior remains an important challenge across many industries such as\nteleconferencing, gaming, and AR/VR. Human hands are the primary tool for\ninteracting with the environment and essential for realistic human behavior\nmodeling, yet existing 3D hand and head avatar models often overlook the\ncrucial aspect of hand-body interactions, such as between hand and face. We\npresent InteracttAvatar, the first model to faithfully capture the\nphotorealistic appearance of dynamic hand and non-rigid hand-face interactions.\nOur novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian\nSplatting as well as a dynamic refinement module, captures pose-dependent\nchange, e.g. the fine wrinkles and complex shadows that occur during\narticulation. Importantly, our hand-face interaction module models the subtle\ngeometry and appearance dynamics that underlie common gestures. Through\nexperiments of novel view synthesis, self reenactment and cross-identity\nreenactment, we demonstrate that InteracttAvatar can reconstruct hand and\nhand-face interactions from monocular or multiview videos with high-fidelity\ndetails and be animated with novel poses.", "published": "2025-04-10 17:55:43", "link": "http://arxiv.org/abs/2504.07949v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HoloPart: Generative 3D Part Amodal Segmentation", "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.", "published": "2025-04-10 17:53:31", "link": "http://arxiv.org/abs/2504.07943v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation", "abstract": "Current Few Shot Segmentation literature lacks a mask selection method that\ngoes beyond visual similarity between the query and example images, leading to\nsuboptimal predictions. We present MARS, a plug-and-play ranking system that\nleverages multimodal cues to filter and merge mask proposals robustly. Starting\nfrom a set of mask predictions for a single query image, we score, filter, and\nmerge them to improve results. Proposals are evaluated using multimodal scores\ncomputed at local and global levels. Extensive experiments on COCO-20i,\nPascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring\ncomponents is crucial for robust ranking, validating our contribution. As MARS\ncan be effortlessly integrated with various mask proposal systems, we deploy it\nacross a wide range of top-performer methods and achieve new state-of-the-art\nresults on multiple existing benchmarks. Code will be available upon\nacceptance.", "published": "2025-04-10 17:53:23", "link": "http://arxiv.org/abs/2504.07942v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Beyond the Frame: Generating 360\u00b0 Panoramic Videos from Perspective Videos", "abstract": "360{\\deg} videos have emerged as a promising medium to represent our dynamic\nvisual world. Compared to the \"tunnel vision\" of standard cameras, their\nborderless field of view offers a more complete perspective of our\nsurroundings. While existing video models excel at producing standard videos,\ntheir ability to generate full panoramic videos remains elusive. In this paper,\nwe investigate the task of video-to-360{\\deg} generation: given a perspective\nvideo as input, our goal is to generate a full panoramic video that is\nconsistent with the original video. Unlike conventional video generation tasks,\nthe output's field of view is significantly larger, and the model is required\nto have a deep understanding of both the spatial layout of the scene and the\ndynamics of objects to maintain spatio-temporal consistency. To address these\nchallenges, we first leverage the abundant 360{\\deg} videos available online\nand develop a high-quality data filtering pipeline to curate pairwise training\ndata. We then carefully design a series of geometry- and motion-aware\noperations to facilitate the learning process and improve the quality of\n360{\\deg} video generation. Experimental results demonstrate that our model can\ngenerate realistic and coherent 360{\\deg} videos from in-the-wild perspective\nvideo. In addition, we showcase its potential applications, including video\nstabilization, camera viewpoint control, and interactive visual question\nanswering.", "published": "2025-04-10 17:51:38", "link": "http://arxiv.org/abs/2504.07940v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "abstract": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.", "published": "2025-04-10 17:49:05", "link": "http://arxiv.org/abs/2504.07934v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Zero-Shot Low-dose CT Denoising via Sinogram Flicking", "abstract": "Many low-dose CT imaging methods rely on supervised learning, which requires\na large number of paired noisy and clean images. However, obtaining paired\nimages in clinical practice is challenging. To address this issue, zero-shot\nself-supervised methods train denoising networks using only the information\nwithin a single image, such as ZS-N2N. However, these methods often employ\ndownsampling operations that degrade image resolution. Additionally, the\ntraining dataset is inherently constrained to the image itself. In this paper,\nwe propose a zero-shot low-dose CT imaging method based on sinogram flicking,\nwhich operates within a single image but generates many copies via random\nconjugate ray matching. Specifically, two conjugate X-ray pencil beams measure\nthe same path; their expected values should be identical, while their noise\nlevels vary during measurements. By randomly swapping portions of the conjugate\nX-rays in the sinogram domain, we generate a large set of sinograms with\nconsistent content but varying noise patterns. When displayed dynamically,\nthese sinograms exhibit a flickering effect due to their identical structural\ncontent but differing noise patterns-hence the term sinogram flicking. We train\nthe network on pairs of sinograms with the same content but different noise\ndistributions using a lightweight model adapted from ZS-NSN. This process is\nrepeated to obtain the final results. A simulation study demonstrates that our\nmethod outperforms state-of-the-art approaches such as ZS-N2N.", "published": "2025-04-10 17:42:01", "link": "http://arxiv.org/abs/2504.07927v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound", "abstract": "Data augmentation is a central component of joint embedding self-supervised\nlearning (SSL). Approaches that work for natural images may not always be\neffective in medical imaging tasks. This study systematically investigated the\nimpact of data augmentation and preprocessing strategies in SSL for lung\nultrasound. Three data augmentation pipelines were assessed: (1) a baseline\npipeline commonly used across imaging domains, (2) a novel semantic-preserving\npipeline designed for ultrasound, and (3) a distilled set of the most effective\ntransformations from both pipelines. Pretrained models were evaluated on\nmultiple classification tasks: B-line detection, pleural effusion detection,\nand COVID-19 classification. Experiments revealed that semantics-preserving\ndata augmentation resulted in the greatest performance for COVID-19\nclassification - a diagnostic task requiring global image context.\nCropping-based methods yielded the greatest performance on the B-line and\npleural effusion object classification tasks, which require strong local\npattern recognition. Lastly, semantics-preserving ultrasound image\npreprocessing resulted in increased downstream performance for multiple tasks.\nGuidance regarding data augmentation and preprocessing strategies was\nsynthesized for practitioners working with SSL in ultrasound.", "published": "2025-04-10 16:26:47", "link": "http://arxiv.org/abs/2504.07904v1", "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.9; J.3"], "primary_category": "eess.IV"}
{"title": "SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos", "abstract": "Video Scene Graph Generation (VidSGG) is an important topic in understanding\ndynamic kitchen environments. Current models for VidSGG require extensive\ntraining to produce scene graphs. Recently, Vision Language Models (VLM) and\nVision Foundation Models (VFM) have demonstrated impressive zero-shot\ncapabilities in a variety of tasks. However, VLMs like Gemini struggle with the\ndynamics for VidSGG, failing to maintain stable object identities across\nframes. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline\nthat combines SAM2's temporal tracking with Gemini's semantic understanding.\nSAM2 also improves upon Gemini's object grounding by producing more accurate\nbounding boxes. In our method, we first prompt Gemini to generate a frame-level\nscene graph. Then, we employ a matching algorithm to map each object in the\nscene graph with a SAM2-generated or SAM2-propagated mask, producing a\ntemporally-consistent scene graph in dynamic environments. Finally, we repeat\nthis process again in each of the following frames. We empirically demonstrate\nthat SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and\nEPIC-KITCHENS-100 datasets.", "published": "2025-04-10 15:43:10", "link": "http://arxiv.org/abs/2504.07867v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy", "abstract": "Light field microscopy (LFM) has gained significant attention due to its\nability to capture snapshot-based, large-scale 3D fluorescence images. However,\nexisting LFM reconstruction algorithms are highly sensitive to sensor noise or\nrequire hard-to-get ground-truth annotated data for training. To address these\nchallenges, this paper introduces V2V3D, an unsupervised view2view-based\nframework that establishes a new paradigm for joint optimization of image\ndenoising and 3D reconstruction in a unified architecture. We assume that the\nLF images are derived from a consistent 3D signal, with the noise in each view\nbeing independent. This enables V2V3D to incorporate the principle of\nnoise2noise for effective denoising. To enhance the recovery of high-frequency\ndetails, we propose a novel wave-optics-based feature alignment technique,\nwhich transforms the point spread function, used for forward propagation in\nwave optics, into convolution kernels specifically designed for feature\nalignment. Moreover, we introduce an LFM dataset containing LF images and their\ncorresponding 3D intensity volumes. Extensive experiments demonstrate that our\napproach achieves high computational efficiency and outperforms the other\nstate-of-the-art methods. These advancements position V2V3D as a promising\nsolution for 3D imaging under challenging conditions.", "published": "2025-04-10 15:29:26", "link": "http://arxiv.org/abs/2504.07853v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HarmonySeg: Tubular Structure Segmentation with Deep-Shallow Feature Fusion and Growth-Suppression Balanced Loss", "abstract": "Accurate segmentation of tubular structures in medical images, such as\nvessels and airway trees, is crucial for computer-aided diagnosis,\nradiotherapy, and surgical planning. However, significant challenges exist in\nalgorithm design when faced with diverse sizes, complex topologies, and (often)\nincomplete data annotation of these structures. We address these difficulties\nby proposing a new tubular structure segmentation framework named HarmonySeg.\nFirst, we design a deep-to-shallow decoder network featuring flexible\nconvolution blocks with varying receptive fields, which enables the model to\neffectively adapt to tubular structures of different scales. Second, to\nhighlight potential anatomical regions and improve the recall of small tubular\nstructures, we incorporate vesselness maps as auxiliary information. These maps\nare aligned with image features through a shallow-and-deep fusion module, which\nsimultaneously eliminates unreasonable candidates to maintain high precision.\nFinally, we introduce a topology-preserving loss function that leverages\ncontextual and shape priors to balance the growth and suppression of tubular\nstructures, which also allows the model to handle low-quality and incomplete\nannotations. Extensive quantitative experiments are conducted on four public\ndatasets. The results show that our model can accurately segment 2D and 3D\ntubular structures and outperform existing state-of-the-art methods. External\nvalidation on a private dataset also demonstrates good generalizability.", "published": "2025-04-10 15:04:42", "link": "http://arxiv.org/abs/2504.07827v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "P2Object: Single Point Supervised Object Detection and Instance Segmentation", "abstract": "Object recognition using single-point supervision has attracted increasing\nattention recently. However, the performance gap compared with fully-supervised\nalgorithms remains large. Previous works generated class-agnostic\n\\textbf{\\textit{proposals in an image}} offline and then treated mixed\ncandidates as a single bag, putting a huge burden on multiple instance learning\n(MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which\nconstructs balanced \\textbf{\\textit{instance-level proposal bags}} by\ngenerating proposals in an anchor-like way and refining the proposals in a\ncoarse-to-fine paradigm. Through further research, we find that the bag of\nproposals, either at the image level or the instance level, is established on\ndiscrete box sampling. This leads the pseudo box estimation into a sub-optimal\nsolution, resulting in the truncation of object boundaries or the excessive\ninclusion of background. Hence, we conduct a series exploration of\ndiscrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask\nNetwork (P2MNet). P2BNet++ conducts an approximately continuous proposal\nsampling strategy by better utilizing spatial clues. P2MNet further introduces\nlow-level image information to assist in pixel prediction, and a boundary\nself-prediction is designed to relieve the limitation of the estimated boxes.\nBenefiting from the continuous object-aware \\textbf{\\textit{pixel-level\nperception}}, P2MNet can generate more precise bounding boxes and generalize to\nsegmentation tasks. Our method largely surpasses the previous methods in terms\nof the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating\ngreat potential to bridge the performance gap compared with fully supervised\ntasks.", "published": "2025-04-10 14:51:08", "link": "http://arxiv.org/abs/2504.07813v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement", "abstract": "Images captured under low-light conditions present significant limitations in\nmany applications, as poor lighting can obscure details, reduce contrast, and\nhide noise. Removing the illumination effects and enhancing the quality of such\nimages is crucial for many tasks, such as image segmentation and object\ndetection. In this paper, we propose a variational method for low-light image\nenhancement based on the Retinex decomposition into illumination, reflectance,\nand noise components. A color correction pre-processing step is applied to the\nlow-light image, which is then used as the observed input in the decomposition.\nMoreover, our model integrates a novel nonlocal gradient-type fidelity term\ndesigned to preserve structural details. Additionally, we propose an automatic\ngamma correction module. Building on the proposed variational approach, we\nextend the model by introducing its deep unfolding counterpart, in which the\nproximal operators are replaced with learnable networks. We propose\ncross-attention mechanisms to capture long-range dependencies in both the\nnonlocal prior of the reflectance and the nonlocal gradient-based constraint.\nExperimental results demonstrate that both methods compare favorably with\nseveral recent and state-of-the-art techniques across different datasets. In\nparticular, despite not relying on learning strategies, the variational model\noutperforms most deep learning approaches both visually and in terms of quality\nmetrics.", "published": "2025-04-10 14:48:26", "link": "http://arxiv.org/abs/2504.07810v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations", "abstract": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning systems, particularly in safety-critical applications.\nLikelihood-based deep generative models have historically faced criticism for\ntheir unsatisfactory performance in OOD detection, often assigning higher\nlikelihood to OOD data than in-distribution samples when applied to image data.\nIn this work, we demonstrate that likelihood is not inherently flawed. Rather,\nseveral properties in the images space prohibit likelihood as a valid detection\nscore. Given a sufficiently good likelihood estimator, specifically using the\nprobability flow formulation of a diffusion model, we show that\nlikelihood-based methods can still perform on par with state-of-the-art methods\nwhen applied in the representation space of pre-trained encoders. The code of\nour work can be found at\n$\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.", "published": "2025-04-10 14:30:41", "link": "http://arxiv.org/abs/2504.07793v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition", "abstract": "Sign language is a fundamental means of communication for the deaf and\nhard-of-hearing (DHH) community, enabling nuanced expression through gestures,\nfacial expressions, and body movements. Despite its critical role in\nfacilitating interaction within the DHH population, significant barriers\npersist due to the limited fluency in sign language among the hearing\npopulation. Overcoming this communication gap through automatic sign language\nrecognition (SLR) remains a challenge, particularly at a dynamic word-level,\nwhere temporal and spatial dependencies must be effectively recognized. While\nConvolutional Neural Networks have shown potential in SLR, they are\ncomputationally intensive and have difficulties in capturing global temporal\ndependencies between video sequences. To address these limitations, we propose\na Video Vision Transformer (ViViT) model for word-level American Sign Language\n(ASL) recognition. Transformer models make use of self-attention mechanisms to\neffectively capture global relationships across spatial and temporal\ndimensions, which makes them suitable for complex gesture recognition tasks.\nThe VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset,\nhighlighting its strong performance compared to traditional CNNs with 65.89%.\nOur study demonstrates that transformer-based architectures have great\npotential to advance SLR, overcome communication barriers and promote the\ninclusion of DHH individuals.", "published": "2025-04-10 14:27:25", "link": "http://arxiv.org/abs/2504.07792v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Micro-Action Recognition with Limited Annotations: An Asynchronous Pseudo Labeling and Training Approach", "abstract": "Micro-Action Recognition (MAR) aims to classify subtle human actions in\nvideo. However, annotating MAR datasets is particularly challenging due to the\nsubtlety of actions. To this end, we introduce the setting of Semi-Supervised\nMAR (SSMAR), where only a part of samples are labeled. We first evaluate\ntraditional Semi-Supervised Learning (SSL) methods to SSMAR and find that these\nmethods tend to overfit on inaccurate pseudo-labels, leading to error\naccumulation and degraded performance. This issue primarily arises from the\ncommon practice of directly using the predictions of classifier as\npseudo-labels to train the model. To solve this issue, we propose a novel\nframework, called Asynchronous Pseudo Labeling and Training (APLT), which\nexplicitly separates the pseudo-labeling process from model training.\nSpecifically, we introduce a semi-supervised clustering method during the\noffline pseudo-labeling phase to generate more accurate pseudo-labels.\nMoreover, a self-adaptive thresholding strategy is proposed to dynamically\nfilter noisy labels of different classes. We then build a memory-based\nprototype classifier based on the filtered pseudo-labels, which is fixed and\nused to guide the subsequent model training phase. By alternating the two\npseudo-labeling and model training phases in an asynchronous manner, the model\ncan not only be learned with more accurate pseudo-labels but also avoid the\noverfitting issue. Experiments on three MAR datasets show that our APLT largely\noutperforms state-of-the-art SSL methods. For instance, APLT improves accuracy\nby 14.5\\% over FixMatch on the MA-12 dataset when using only 50\\% labeled data.\nCode will be publicly available.", "published": "2025-04-10 14:22:15", "link": "http://arxiv.org/abs/2504.07785v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network", "abstract": "Fast moving celestial objects are characterized by velocities across the\ncelestial sphere that significantly differ from the motions of background\nstars. In observational images, these objects exhibit distinct shapes,\ncontrasting with the typical appearances of stars. Depending on the\nobservational method employed, these celestial entities may be designated as\nnear-Earth objects or asteroids. Historically, fast moving celestial objects\nhave been observed using ground-based telescopes, where the relative stability\nof stars and Earth facilitated effective image differencing techniques\nalongside traditional fast moving celestial object detection and classification\nalgorithms. However, the growing prevalence of space-based telescopes, along\nwith their diverse observational modes, produces images with different\nproperties, rendering conventional methods less effective. This paper presents\na novel algorithm for detecting fast moving celestial objects within star\nfields. Our approach enhances state-of-the-art fast moving celestial object\ndetection neural networks by transforming them into physical-inspired neural\nnetworks. These neural networks leverage the point spread function of the\ntelescope and the specific observational mode as prior information; they can\ndirectly identify moving fast moving celestial objects within star fields\nwithout requiring additional training, thereby addressing the limitations of\ntraditional techniques. Additionally, all neural networks are integrated using\nthe mixture of experts technique, forming a comprehensive fast moving celestial\nobject detection algorithm. We have evaluated our algorithm using simulated\nobservational data that mimics various observations carried out by space based\ntelescope scenarios and real observation images. Results demonstrate that our\nmethod effectively detects fast moving celestial objects across different\nobservational modes.", "published": "2025-04-10 14:15:30", "link": "http://arxiv.org/abs/2504.07777v1", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.LG", "physics.optics"], "primary_category": "astro-ph.IM"}
{"title": "Focal Cortical Dysplasia Type II Detection Using Cross Modality Transfer Learning and Grad-CAM in 3D-CNNs for MRI Analysis", "abstract": "Focal cortical dysplasia (FCD) type II is a major cause of drug-resistant\nepilepsy, often curable only by surgery. Despite its clinical importance, the\ndiagnosis of FCD is very difficult in MRI because of subtle abnormalities,\nleading to misdiagnosis. This study investigates the use of 3D convolutional\nneural networks (3D-CNNs) for FCD detection, using a dataset of 170 subjects\n(85 FCD patients and 85 controls) composed of T1-weighted and FLAIR MRI scans.\nIn particular, it investigates the benefits obtained from cross-modality\ntransfer learning and explainable artificial intelligence (XAI) techniques, in\nparticular Gradient-weighted Class Activation Mapping (Grad-CAM). ResNet\narchitectures (ResNet-18, -34, and -50) were implemented, employing transfer\nlearning strategies that used pre-trained weights from segmentation tasks.\nResults indicate that transfer learning significantly enhances classification\naccuracy (up to 80.3%) and interpretability, as measured by a novel Heat-Score\nmetric, which evaluates the model's focus on clinically relevant regions.\nImprovements in the Heat-Score metric underscore the model's seizure zone\nlocalization capabilities, bringing AI predictions and clinical insights closer\ntogether. These results highlight the importance of transfer learning,\nincluding cross-modality, and XAI in advancing AI-based medical diagnostics,\nespecially for difficult-to-diagnose pathologies such as FCD.", "published": "2025-04-10 14:15:16", "link": "http://arxiv.org/abs/2504.07775v1", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "primary_category": "eess.IV"}
{"title": "PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model Development", "abstract": "Deep learning (DL), a pivotal technology in artificial intelligence, has\nrecently gained substantial traction in the domain of dental auxiliary\ndiagnosis. However, its application has predominantly been confined to imaging\nmodalities such as panoramic radiographs and Cone Beam Computed Tomography,\nwith limited focus on auxiliary analysis specifically targeting Periapical\nRadiographs (PR). PR are the most extensively utilized imaging modality in\nendodontics and periodontics due to their capability to capture detailed local\nlesions at a low cost. Nevertheless, challenges such as resolution limitations\nand artifacts complicate the annotation and recognition of PR, leading to a\nscarcity of publicly available, large-scale, high-quality PR analysis datasets.\nThis scarcity has somewhat impeded the advancement of DL applications in PR\nanalysis. In this paper, we present PRAD-10K, a dataset for PR analysis.\nPRAD-10K comprises 10,000 clinical periapical radiograph images, with\npixel-level annotations provided by professional dentists for nine distinct\nanatomical structures, lesions, and artificial restorations or medical devices,\nWe also include classification labels for images with typical conditions or\nlesions. Furthermore, we introduce a DL network named PRNet to establish\nbenchmarks for PR segmentation tasks. Experimental results demonstrate that\nPRNet surpasses previous state-of-the-art medical image segmentation models on\nthe PRAD-10K dataset. The codes and dataset will be made publicly available.", "published": "2025-04-10 13:58:58", "link": "http://arxiv.org/abs/2504.07760v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution", "abstract": "Polarization cameras can capture multiple polarized images with different\npolarizer angles in a single shot, bringing convenience to polarization-based\ndownstream tasks. However, their direct outputs are color-polarization filter\narray (CPFA) raw images, requiring demosaicing to reconstruct full-resolution,\nfull-color polarized images; unfortunately, this necessary step introduces\nartifacts that make polarization-related parameters such as the degree of\npolarization (DoP) and angle of polarization (AoP) prone to error. Besides,\nlimited by the hardware design, the resolution of a polarization camera is\noften much lower than that of a conventional RGB camera. Existing polarized\nimage demosaicing (PID) methods are limited in that they cannot enhance\nresolution, while polarized image super-resolution (PISR) methods, though\ndesigned to obtain high-resolution (HR) polarized images from the demosaicing\nresults, tend to retain or even amplify errors in the DoP and AoP introduced by\ndemosaicing artifacts. In this paper, we propose PIDSR, a joint framework that\nperforms complementary Polarized Image Demosaicing and Super-Resolution,\nshowing the ability to robustly obtain high-quality HR polarized images with\nmore accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments\nshow our PIDSR not only achieves state-of-the-art performance on both synthetic\nand real data, but also facilitates downstream tasks.", "published": "2025-04-10 13:56:33", "link": "http://arxiv.org/abs/2504.07758v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Virtual-mask Informed Prior for Sparse-view Dual-Energy CT Reconstruction", "abstract": "Sparse-view sampling in dual-energy computed tomography (DECT) significantly\nreduces radiation dose and increases imaging speed, yet is highly prone to\nartifacts. Although diffusion models have demonstrated potential in effectively\nhandling incomplete data, most existing methods in this field focus on the\nimage do-main and lack global constraints, which consequently leads to\ninsufficient reconstruction quality. In this study, we propose a dual-domain\nvirtual-mask in-formed diffusion model for sparse-view reconstruction by\nleveraging the high inter-channel correlation in DECT. Specifically, the study\ndesigns a virtual mask and applies it to the high-energy and low-energy data to\nperform perturbation operations, thus constructing high-dimensional tensors\nthat serve as the prior information of the diffusion model. In addition, a\ndual-domain collaboration strategy is adopted to integrate the information of\nthe randomly selected high-frequency components in the wavelet domain with the\ninformation in the projection domain, for the purpose of optimizing the global\nstruc-tures and local details. Experimental results indicated that the present\nmethod exhibits excellent performance across multiple datasets.", "published": "2025-04-10 13:54:26", "link": "http://arxiv.org/abs/2504.07753v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset", "abstract": "Real-time wildlife detection in drone imagery is critical for numerous\napplications, including animal ecology, conservation, and biodiversity\nmonitoring. Low-altitude drone missions are effective for collecting\nfine-grained animal movement and behavior data, particularly if missions are\nautomated for increased speed and consistency. However, little work exists on\nevaluating computer vision models on low-altitude aerial imagery and\ngeneralizability across different species and settings. To fill this gap, we\npresent a novel multi-environment, multi-species, low-altitude aerial footage\n(MMLA) dataset. MMLA consists of drone footage collected across three diverse\nenvironments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The\nWilds Conservation Center in Ohio, which includes five species: Plains zebras,\nGrevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively\nevaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting\nanimals. Results demonstrate significant performance disparities across\nlocations and species-specific detection variations. Our work highlights the\nimportance of evaluating detection algorithms across different environments for\nrobust wildlife monitoring applications using drones.", "published": "2025-04-10 13:40:27", "link": "http://arxiv.org/abs/2504.07744v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval", "abstract": "Fine-grained text-to-image retrieval aims to retrieve a fine-grained target\nimage with a given text query. Existing methods typically assume that each\ntraining image is accurately depicted by its textual descriptions. However,\ntextual descriptions can be ambiguous and fail to depict discriminative visual\ndetails in images, leading to inaccurate representation learning. To alleviate\nthe effects of text ambiguity, we propose a Multi-Modal Reference learning\nframework to learn robust representations. We first propose a multi-modal\nreference construction module to aggregate all visual and textual details of\nthe same object into a comprehensive multi-modal reference. The multi-modal\nreference hence facilitates the subsequent representation learning and\nretrieval similarity computation. Specifically, a reference-guided\nrepresentation learning module is proposed to use multi-modal references to\nlearn more accurate visual and textual representations. Additionally, we\nintroduce a reference-based refinement method that employs the object\nreferences to compute a reference-based similarity that refines the initial\nretrieval results. Extensive experiments are conducted on five fine-grained\ntext-to-image retrieval datasets for different text-to-image retrieval tasks.\nThe proposed method has achieved superior performance over state-of-the-art\nmethods. For instance, on the text-to-person image retrieval dataset RSTPReid,\nour method achieves the Rank1 accuracy of 56.2\\%, surpassing the recent CFine\nby 5.6\\%.", "published": "2025-04-10 13:09:52", "link": "http://arxiv.org/abs/2504.07718v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation", "abstract": "Current knowledge distillation (KD) methods for semantic segmentation focus\non guiding the student to imitate the teacher's knowledge within homogeneous\narchitectures. However, these methods overlook the diverse knowledge contained\nin architectures with different inductive biases, which is crucial for enabling\nthe student to acquire a more precise and comprehensive understanding of the\ndata during distillation. To this end, we propose for the first time a generic\nknowledge distillation method for semantic segmentation from a heterogeneous\nperspective, named HeteroAKD. Due to the substantial disparities between\nheterogeneous architectures, such as CNN and Transformer, directly transferring\ncross-architecture knowledge presents significant challenges. To eliminate the\ninfluence of architecture-specific information, the intermediate features of\nboth the teacher and student are skillfully projected into an aligned logits\nspace. Furthermore, to utilize diverse knowledge from heterogeneous\narchitectures and deliver customized knowledge required by the student, a\nteacher-student knowledge mixing mechanism (KMM) and a teacher-student\nknowledge evaluation mechanism (KEM) are introduced. These mechanisms are\nperformed by assessing the reliability and its discrepancy between\nheterogeneous teacher-student knowledge. Extensive experiments conducted on\nthree main-stream benchmarks using various teacher-student pairs demonstrate\nthat our HeteroAKD outperforms state-of-the-art KD methods in facilitating\ndistillation between heterogeneous architectures.", "published": "2025-04-10 12:24:58", "link": "http://arxiv.org/abs/2504.07691v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection", "abstract": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets often\npolitically or virally motivated pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.", "published": "2025-04-10 12:16:32", "link": "http://arxiv.org/abs/2504.07687v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization", "abstract": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments.", "published": "2025-04-10 12:07:24", "link": "http://arxiv.org/abs/2504.07677v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "LAPIS: A novel dataset for personalized image aesthetic assessment", "abstract": "We present the Leuven Art Personalized Image Set (LAPIS), a novel dataset for\npersonalized image aesthetic assessment (PIAA). It is the first dataset with\nimages of artworks that is suitable for PIAA. LAPIS consists of 11,723 images\nand was meticulously curated in collaboration with art historians. Each image\nhas an aesthetics score and a set of image attributes known to relate to\naesthetic appreciation. Besides rich image attributes, LAPIS offers rich\npersonal attributes of each annotator. We implemented two existing\nstate-of-the-art PIAA models and assessed their performance on LAPIS. We assess\nthe contribution of personal attributes and image attributes through ablation\nstudies and find that performance deteriorates when certain personal and image\nattributes are removed. An analysis of failure cases reveals that both existing\nmodels make similar incorrect predictions, highlighting the need for\nimprovements in artistic image aesthetic assessment. The LAPIS project page can\nbe found at: https://github.com/Anne-SofieMaerten/LAPIS", "published": "2025-04-10 11:42:56", "link": "http://arxiv.org/abs/2504.07670v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion", "abstract": "The generalization of learning-based high dynamic range (HDR) fusion is often\nlimited by the availability of training data, as collecting large-scale HDR\nimages from dynamic scenes is both costly and technically challenging. To\naddress these challenges, we propose S2R-HDR, the first large-scale\nhigh-quality synthetic dataset for HDR fusion, with 24,000 HDR samples. Using\nUnreal Engine 5, we design a diverse set of realistic HDR scenes that encompass\nvarious dynamic elements, motion types, high dynamic range scenes, and\nlighting. Additionally, we develop an efficient rendering pipeline to generate\nrealistic HDR images. To further mitigate the domain gap between synthetic and\nreal-world data, we introduce S2R-Adapter, a domain adaptation designed to\nbridge this gap and enhance the generalization ability of models. Experimental\nresults on real-world datasets demonstrate that our approach achieves\nstate-of-the-art HDR reconstruction performance. Dataset and code will be\navailable at https://openimaginglab.github.io/S2R-HDR.", "published": "2025-04-10 11:39:56", "link": "http://arxiv.org/abs/2504.07667v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "End-to-End Facial Expression Detection in Long Videos", "abstract": "Facial expression detection involves two interrelated tasks: spotting, which\nidentifies the onset and offset of expressions, and recognition, which\nclassifies them into emotional categories. Most existing methods treat these\ntasks separately using a two-step training pipelines. A spotting model first\ndetects expression intervals. A recognition model then classifies the detected\nsegments. However, this sequential approach leads to error propagation,\ninefficient feature learning, and suboptimal performance due to the lack of\njoint optimization of the two tasks. We propose FEDN, an end-to-end Facial\nExpression Detection Network that jointly optimizes spotting and recognition.\nOur model introduces a novel attention-based feature extraction module,\nincorporating segment attention and sliding window attention to improve facial\nfeature learning. By unifying two tasks within a single network, we greatly\nreduce error propagation and enhance overall performance. Experiments on\nCASME}^2 and CASME^3 demonstrate state-of-the-art accuracy for both spotting\nand detection, underscoring the benefits of joint optimization for robust\nfacial expression detection in long videos.", "published": "2025-04-10 11:18:46", "link": "http://arxiv.org/abs/2504.07660v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases", "abstract": "Heart diseases constitute the main cause of international human defunction.\nAccording to the World Health Organization (WHO), approximately 18 million\ndeaths happen each year due to precisely heart diseases. In particular, heart\nfailures (HF) press the healthcare industry to develop systems for their early,\nrapid and effective prediction. In this work, an automatic system which\nanalyses in real-time echocardiography video sequences is proposed for the\nchallenging and more specific task of prediction of heart failure times. This\nsystem is based on a novel deep learning framework, and works in two stages.\nThe first one transforms the data included in a database of echocardiography\nvideo sequences into a machine learning-compatible collection of annotated\nimages which can be used in the training phase of any kind of machine\nlearning-based framework, including a deep learning one. This initial stage\nincludes the use of the Higher Order Dynamic Mode Decomposition (HODMD)\nalgorithm for both data augmentation and feature extraction. The second stage\nis focused on building and training a Vision Transformer (ViT). Self-supervised\nlearning (SSL) methods, which have been so far barely explored in the\nliterature about heart failure prediction, are applied to effectively train the\nViT from scratch, even with scarce databases of echocardiograms. The designed\nneural network analyses images from echocardiography sequences to estimate the\ntime in which a heart failure will happen. The results obtained show the\nefficacy of the HODMD algorithm and the superiority of the proposed system with\nrespect to several established ViT and Convolutional Neural Network (CNN)\narchitectures.", "published": "2025-04-10 09:57:09", "link": "http://arxiv.org/abs/2504.07606v1", "categories": ["eess.IV", "cs.CV", "68T07, 68T10, 68T45, 62H35", "I.2; I.2.10; I.4.5; I.5.1; I.5.4; J.3"], "primary_category": "eess.IV"}
{"title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition", "abstract": "Gait recognition from video streams is a challenging problem in computer\nvision biometrics due to the subtle differences between gaits and numerous\nconfounding factors. Recent advancements in self-supervised pretraining have\nled to the development of robust gait recognition models that are invariant to\nwalking covariates. While neural scaling laws have transformed model\ndevelopment in other domains by linking performance to data, model size, and\ncompute, their applicability to gait remains unexplored. In this work, we\nconduct the first empirical study scaling on skeleton-based self-supervised\ngait recognition to quantify the effect of data quantity, model size and\ncompute on downstream gait recognition performance. We pretrain multiple\nvariants of GaitPT - a transformer-based architecture - on a dataset of 2.7\nmillion walking sequences collected in the wild. We evaluate zero-shot\nperformance across four benchmark datasets to derive scaling laws for data,\nmodel size, and compute. Our findings demonstrate predictable power-law\nimprovements in performance with increased scale and confirm that data and\ncompute scaling significantly influence downstream accuracy. We further isolate\narchitectural contributions by comparing GaitPT with GaitFormer under\ncontrolled compute budgets. These results provide practical insights into\nresource allocation and performance estimation for real-world gait recognition\nsystems.", "published": "2025-04-10 09:51:22", "link": "http://arxiv.org/abs/2504.07598v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Extending Visual Dynamics for Video-to-Music Generation", "abstract": "Music profoundly enhances video production by improving quality, engagement,\nand emotional resonance, sparking growing interest in video-to-music\ngeneration. Despite recent advances, existing approaches remain limited in\nspecific scenarios or undervalue the visual dynamics. To address these\nlimitations, we focus on tackling the complexity of dynamics and resolving\ntemporal misalignment between video and music representations. To this end, we\npropose DyViM, a novel framework to enhance dynamics modeling for\nvideo-to-music generation. Specifically, we extract frame-wise dynamics\nfeatures via a simplified motion encoder inherited from optical flow methods,\nfollowed by a self-attention module for aggregation within frames. These\ndynamic features are then incorporated to extend existing music tokens for\ntemporal alignment. Additionally, high-level semantics are conveyed through a\ncross-attention mechanism, and an annealing tuning strategy benefits to\nfine-tune well-trained music decoders efficiently, therefore facilitating\nseamless adaptation. Extensive experiments demonstrate DyViM's superiority over\nstate-of-the-art (SOTA) methods.", "published": "2025-04-10 09:47:26", "link": "http://arxiv.org/abs/2504.07594v1", "categories": ["cs.MM", "cs.CV"], "primary_category": "cs.MM"}
{"title": "PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data Generation", "abstract": "Magnetic resonance imaging (MRI) raw data, or k-Space data, is\ncomplex-valued, containing both magnitude and phase information. However,\nclinical and existing Artificial Intelligence (AI)-based methods focus only on\nmagnitude images, discarding the phase data despite its potential for\ndownstream tasks, such as tumor segmentation and classification. In this work,\nwe introduce $\\textit{PhaseGen}$, a novel complex-valued diffusion model for\ngenerating synthetic MRI raw data conditioned on magnitude images, commonly\nused in clinical practice. This enables the creation of artificial\ncomplex-valued raw data, allowing pretraining for models that require k-Space\ninformation. We evaluate PhaseGen on two tasks: skull-stripping directly in\nk-Space and MRI reconstruction using the publicly available FastMRI dataset.\nOur results show that training with synthetic phase data significantly improves\ngeneralization for skull-stripping on real-world data, with an increased\nsegmentation accuracy from $41.1\\%$ to $80.1\\%$, and enhances MRI\nreconstruction when combined with limited real-world data. This work presents a\nstep forward in utilizing generative AI to bridge the gap between\nmagnitude-based datasets and the complex-valued nature of MRI raw data. This\napproach allows researchers to leverage the vast amount of avaliable image\ndomain data in combination with the information-rich k-Space data for more\naccurate and efficient diagnostic tasks. We make our code publicly\n$\\href{https://github.com/TIO-IKIM/PhaseGen}{\\text{available here}}$.", "published": "2025-04-10 08:44:19", "link": "http://arxiv.org/abs/2504.07560v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs", "abstract": "While text-to-image (T2I) generation models have achieved remarkable progress\nin recent years, existing evaluation methodologies for vision-language\nalignment still struggle with the fine-grained semantic matching. Current\napproaches based on global similarity metrics often overlook critical\ntoken-level correspondences between textual descriptions and visual content. To\nthis end, we present TokenFocus-VQA, a novel evaluation framework that\nleverages Large Vision-Language Models (LVLMs) through visual question\nanswering (VQA) paradigm with position-specific probability optimization. Our\nkey innovation lies in designing a token-aware loss function that selectively\nfocuses on probability distributions at pre-defined vocabulary positions\ncorresponding to crucial semantic elements, enabling precise measurement of\nfine-grained semantical alignment. The proposed framework further integrates\nensemble learning techniques to aggregate multi-perspective assessments from\ndiverse LVLMs architectures, thereby achieving further performance enhancement.\nEvaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our\nTokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method)\non public evaluation and 2nd place (0.8426) on the official private test set,\ndemonstrating superiority in capturing nuanced text-image correspondences\ncompared to conventional evaluation methods.", "published": "2025-04-10 08:37:13", "link": "http://arxiv.org/abs/2504.07556v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors", "abstract": "We study how to solve general Bayesian inverse problems involving videos\nusing diffusion model priors. While it is desirable to use a video diffusion\nprior to effectively capture complex temporal relationships, due to the\ncomputational and data requirements of training such a model, prior work has\ninstead relied on image diffusion priors on single frames combined with\nheuristics to enforce temporal consistency. However, these approaches struggle\nwith faithfully recovering the underlying temporal relationships, particularly\nfor tasks with high temporal uncertainty. In this paper, we demonstrate the\nfeasibility of practical and accessible spatiotemporal diffusion priors by\nfine-tuning latent video diffusion models from pretrained image diffusion\nmodels using limited videos in specific domains. Leveraging this plug-and-play\nspatiotemporal diffusion prior, we introduce a general and scalable framework\nfor solving video inverse problems. We then apply our framework to two\nchallenging scientific video inverse problems--black hole imaging and dynamic\nMRI. Our framework enables the generation of diverse, high-fidelity video\nreconstructions that not only fit observations but also recover multi-modal\nsolutions. By incorporating a spatiotemporal diffusion prior, we significantly\nimprove our ability to capture complex temporal relationships in the data while\nalso enhancing spatial fidelity.", "published": "2025-04-10 08:24:26", "link": "http://arxiv.org/abs/2504.07549v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SydneyScapes: Image Segmentation for Australian Environments", "abstract": "Autonomous Vehicles (AVs) are being partially deployed and tested across\nvarious global locations, including China, the USA, Germany, France, Japan,\nKorea, and the UK, but with limited demonstrations in Australia. The\nintegration of machine learning (ML) into AV perception systems highlights the\nneed for locally labelled datasets to develop and test algorithms in specific\nenvironments. To address this, we introduce SydneyScapes - a dataset tailored\nfor computer vision tasks of image semantic, instance, and panoptic\nsegmentation. This dataset, collected from Sydney and surrounding cities in New\nSouth Wales (NSW), Australia, consists of 756 images with high-quality\npixel-level annotations. It is designed to assist AV industry and researchers\nby providing annotated data and tools for algorithm development, testing, and\ndeployment in the Australian context. Additionally, we offer benchmarking\nresults using state-of-the-art algorithms to establish reference points for\nfuture research and development. The dataset is publicly available at\nhttps://hdl.handle.net/2123/33051.", "published": "2025-04-10 08:11:17", "link": "http://arxiv.org/abs/2504.07542v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DGOcc: Depth-aware Global Query-based Network for Monocular 3D Occupancy Prediction", "abstract": "Monocular 3D occupancy prediction, aiming to predict the occupancy and\nsemantics within interesting regions of 3D scenes from only 2D images, has\ngarnered increasing attention recently for its vital role in 3D scene\nunderstanding. Predicting the 3D occupancy of large-scale outdoor scenes from\n2D images is ill-posed and resource-intensive. In this paper, we present\n\\textbf{DGOcc}, a \\textbf{D}epth-aware \\textbf{G}lobal query-based network for\nmonocular 3D \\textbf{Occ}upancy prediction. We first explore prior depth maps\nto extract depth context features that provide explicit geometric information\nfor the occupancy network. Then, in order to fully exploit the depth context\nfeatures, we propose a Global Query-based (GQ) Module. The cooperation of\nattention mechanisms and scale-aware operations facilitates the feature\ninteraction between images and 3D voxels. Moreover, a Hierarchical Supervision\nStrategy (HSS) is designed to avoid upsampling the high-dimension 3D voxel\nfeatures to full resolution, which mitigates GPU memory utilization and time\ncost. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 datasets\ndemonstrate that the proposed method achieves the best performance on monocular\nsemantic occupancy prediction while reducing GPU and time overhead.", "published": "2025-04-10 07:44:55", "link": "http://arxiv.org/abs/2504.07524v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding", "abstract": "The core challenge in video understanding lies in perceiving dynamic content\nchanges over time. However, multimodal large language models struggle with\ntemporal-sensitive video tasks, which requires generating timestamps to mark\nthe occurrence of specific events. Existing strategies require MLLMs to\ngenerate absolute or relative timestamps directly. We have observed that those\nMLLMs tend to rely more on language patterns than visual cues when generating\ntimestamps, affecting their performance. To address this problem, we propose\nVideoExpert, a general-purpose MLLM suitable for several temporal-sensitive\nvideo tasks. Inspired by the expert concept, VideoExpert integrates two\nparallel modules: the Temporal Expert and the Spatial Expert. The Temporal\nExpert is responsible for modeling time sequences and performing temporal\ngrounding. It processes high-frame-rate yet compressed tokens to capture\ndynamic variations in videos and includes a lightweight prediction head for\nprecise event localization. The Spatial Expert focuses on content detail\nanalysis and instruction following. It handles specially designed spatial\ntokens and language input, aiming to generate content-related responses. These\ntwo experts collaborate seamlessly via a special token, ensuring coordinated\ntemporal grounding and content generation. Notably, the Temporal and Spatial\nExperts maintain independent parameter sets. By offloading temporal grounding\nfrom content generation, VideoExpert prevents text pattern biases in timestamp\npredictions. Moreover, we introduce a Spatial Compress module to obtain spatial\ntokens. This module filters and compresses patch tokens while preserving key\ninformation, delivering compact yet detail-rich input for the Spatial Expert.\nExtensive experiments demonstrate the effectiveness and versatility of the\nVideoExpert.", "published": "2025-04-10 07:33:39", "link": "http://arxiv.org/abs/2504.07519v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Event Signal Filtering via Probability Flux Estimation", "abstract": "Events offer a novel paradigm for capturing scene dynamics via asynchronous\nsensing, but their inherent randomness often leads to degraded signal quality.\nEvent signal filtering is thus essential for enhancing fidelity by reducing\nthis internal randomness and ensuring consistent outputs across diverse\nacquisition conditions. Unlike traditional time series that rely on fixed\ntemporal sampling to capture steady-state behaviors, events encode transient\ndynamics through polarity and event intervals, making signal modeling\nsignificantly more complex. To address this, the theoretical foundation of\nevent generation is revisited through the lens of diffusion processes. The\nstate and process information within events is modeled as continuous\nprobability flux at threshold boundaries of the underlying irradiance\ndiffusion. Building on this insight, a generative, online filtering framework\ncalled Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates\nevent correlation by reconstructing the continuous probability flux from\ndiscrete events using nonparametric kernel smoothing, and then resamples\nfiltered events from this flux. To optimize fidelity over time, spatial and\ntemporal kernels are employed in a time-varying optimization framework. A fast\nrecursive solver with O(1) complexity is proposed, leveraging state-space\nmodels and lookup tables for efficient likelihood computation. Furthermore, a\nnew real-world benchmark Rotary Event Dataset (RED) is released, offering\nmicrosecond-level ground truth irradiance for full-reference event filtering\nevaluation. Extensive experiments validate EDFilter's performance across tasks\nlike event filtering, super-resolution, and direct event-based blob tracking.\nSignificant gains in downstream applications such as SLAM and video\nreconstruction underscore its robustness and effectiveness.", "published": "2025-04-10 07:03:08", "link": "http://arxiv.org/abs/2504.07503v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Kimi-VL Technical Report", "abstract": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.", "published": "2025-04-10 06:48:26", "link": "http://arxiv.org/abs/2504.07491v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Novel Pooling-based VGG-Lite for Pneumonia and Covid-19 Detection from Imbalanced Chest X-Ray Datasets", "abstract": "This paper proposes a novel pooling-based VGG-Lite model in order to mitigate\nclass imbalance issues in Chest X-Ray (CXR) datasets. Automatic Pneumonia\ndetection from CXR images by deep learning model has emerged as a prominent and\ndynamic area of research, since the inception of the new Covid-19 variant in\n2020. However, the standard Convolutional Neural Network (CNN) models encounter\nchallenges associated with class imbalance, a prevalent issue found in many\nmedical datasets. The innovations introduced in the proposed model architecture\ninclude: (I) A very lightweight CNN model, `VGG-Lite', is proposed as a base\nmodel, inspired by VGG-16 and MobileNet-V2 architecture. (II) On top of this\nbase model, we leverage an ``Edge Enhanced Module (EEM)\" through a parallel\nbranch, consisting of a ``negative image layer\", and a novel custom pooling\nlayer ``2Max-Min Pooling\". This 2Max-Min Pooling layer is entirely novel in\nthis investigation, providing more attention to edge components within\npneumonia CXR images. Thus, it works as an efficient spatial attention module\n(SAM). We have implemented the proposed framework on two separate CXR datasets.\nThe first dataset is obtained from a readily available source on the internet,\nand the second dataset is a more challenging CXR dataset, assembled by our\nresearch team from three different sources. Experimental results reveal that\nour proposed framework has outperformed pre-trained CNN models, and three\nrecent trend existing models ``Vision Transformer\", ``Pooling-based Vision\nTransformer (PiT)'' and ``PneuNet\", by substantial margins on both datasets.\nThe proposed framework VGG-Lite with EEM, has achieved a macro average of 95%\naccuracy, 97.1% precision, 96.1% recall, and 96.6% F1 score on the ``Pneumonia\nImbalance CXR dataset\", without employing any pre-processing technique.", "published": "2025-04-10 05:38:46", "link": "http://arxiv.org/abs/2504.07468v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Learning Universal Features for Generalizable Image Forgery Localization", "abstract": "In recent years, advanced image editing and generation methods have rapidly\nevolved, making detecting and locating forged image content increasingly\nchallenging. Most existing image forgery detection methods rely on identifying\nthe edited traces left in the image. However, because the traces of different\nforgeries are distinct, these methods can identify familiar forgeries included\nin the training data but struggle to handle unseen ones. In response, we\npresent an approach for Generalizable Image Forgery Localization (GIFL). Once\ntrained, our model can detect both seen and unseen forgeries, providing a more\npractical and efficient solution to counter false information in the era of\ngenerative AI. Our method focuses on learning general features from the\npristine content rather than traces of specific forgeries, which are relatively\nconsistent across different types of forgeries and therefore can be used as\nuniversal features to locate unseen forgeries. Additionally, as existing image\nforgery datasets are still dominated by traditional hand-crafted forgeries, we\nconstruct a new dataset consisting of images edited by various popular deep\ngenerative image editing methods to further encourage research in detecting\nimages manipulated by deep generative models. Extensive experimental results\nshow that the proposed approach outperforms state-of-the-art methods in the\ndetection of unseen forgeries and also demonstrates competitive results for\nseen forgeries. The code and dataset are available at\nhttps://github.com/ZhaoHengrun/GIFL.", "published": "2025-04-10 05:20:29", "link": "http://arxiv.org/abs/2504.07462v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "How Can Objects Help Video-Language Understanding?", "abstract": "How multimodal large language models (MLLMs) perceive the visual world\nremains a mystery. To one extreme, object and relation modeling may be\nimplicitly implemented with inductive biases, for example by treating objects\nas tokens. To the other extreme, empirical results reveal the surprising\nfinding that simply performing visual captioning, which tends to ignore spatial\nconfiguration of the objects, serves as a strong baseline for video\nunderstanding. We aim to answer the question: how can objects help\nvideo-language understanding in MLLMs? We tackle the question from the object\nrepresentation and adaptation perspectives. Specifically, we investigate the\ntrade-off between representation expressiveness (e.g., distributed versus\nsymbolic) and integration difficulty (e.g., data-efficiency when learning the\nadapters). Through extensive evaluations on five video question answering\ndatasets, we confirm that explicit integration of object-centric representation\nremains necessary, and the symbolic objects can be most easily integrated while\nbeing performant for question answering. We hope our findings can encourage the\ncommunity to explore the explicit integration of perception modules into MLLM\ndesign. Our code and models will be publicly released.", "published": "2025-04-10 04:59:28", "link": "http://arxiv.org/abs/2504.07454v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer", "abstract": "Robust object detection for Unmanned Surface Vehicles (USVs) in complex water\nenvironments is essential for reliable navigation and operation. Specifically,\nwater surface object detection faces challenges from blurred edges and diverse\nobject scales. Although vision-radar fusion offers a feasible solution,\nexisting approaches suffer from cross-modal feature conflicts, which negatively\naffect model robustness. To address this problem, we propose a robust\nvision-radar fusion model WS-DETR. In particular, we first introduce a\nMulti-Scale Edge Information Integration (MSEII) module to enhance edge\nperception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale\nobject detection in the encoder. Then, we adopt self-moving point\nrepresentations for continuous convolution and residual connection to\nefficiently extract irregular features under the scenarios of irregular point\ncloud data. To further mitigate cross-modal conflicts, an Adaptive Feature\nInteractive Fusion (AFIF) module is introduced to integrate visual and radar\nfeatures through geometric alignment and semantic fusion. Extensive experiments\non the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art\n(SOTA) performance, maintaining its superiority even under adverse weather and\nlighting conditions.", "published": "2025-04-10 04:16:46", "link": "http://arxiv.org/abs/2504.07441v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement", "abstract": "We introduce ThermoStereoRT, a real-time thermal stereo matching method\ndesigned for all-weather conditions that recovers disparity from two rectified\nthermal stereo images, envisioning applications such as night-time drone\nsurveillance or under-bed cleaning robots. Leveraging a lightweight yet\npowerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal\nimages and employs multi-scale attention mechanisms to produce an initial\ndisparity map. To refine this map, we design a novel channel and spatial\nattention module. Addressing the challenge of sparse ground truth data in\nthermal imagery, we utilize knowledge distillation to boost performance without\nincreasing computational demands. Comprehensive evaluations on multiple\ndatasets demonstrate that ThermoStereoRT delivers both real-time capacity and\nrobust accuracy, making it a promising solution for real-world deployment in\nvarious challenging environments. Our code will be released on\nhttps://github.com/SJTU-ViSYS-team/ThermoStereoRT", "published": "2025-04-10 03:24:21", "link": "http://arxiv.org/abs/2504.07418v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation", "abstract": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).", "published": "2025-04-10 02:58:22", "link": "http://arxiv.org/abs/2504.07405v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models", "abstract": "Recent advances in generative modeling have enabled the generation of\nhigh-quality synthetic data that is applicable in a variety of domains,\nincluding face recognition. Here, state-of-the-art generative models typically\nrely on conditioning and fine-tuning of powerful pretrained diffusion models to\nfacilitate the synthesis of realistic images of a desired identity. Yet, these\nmodels often do not consider the identity of subjects during training, leading\nto poor consistency between generated and intended identities. In contrast,\nmethods that employ identity-based training objectives tend to overfit on\nvarious aspects of the identity, and in turn, lower the diversity of images\nthat can be generated. To address these issues, we present in this paper a\nnovel generative diffusion-based framework, called ID-Booth. ID-Booth consists\nof a denoising network responsible for data generation, a variational\nauto-encoder for mapping images to and from a lower-dimensional latent space\nand a text encoder that allows for prompt-based control over the generation\nprocedure. The framework utilizes a novel triplet identity training objective\nand enables identity-consistent image generation while retaining the synthesis\ncapabilities of pretrained diffusion models. Experiments with a\nstate-of-the-art latent diffusion model and diverse prompts reveal that our\nmethod facilitates better intra-identity consistency and inter-identity\nseparability than competing methods, while achieving higher image diversity. In\nturn, the produced data allows for effective augmentation of small-scale\ndatasets and training of better-performing recognition models in a\nprivacy-preserving manner. The source code for the ID-Booth framework is\npublicly available at https://github.com/dariant/ID-Booth.", "published": "2025-04-10 02:20:18", "link": "http://arxiv.org/abs/2504.07392v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction", "abstract": "Advances in image generation enable hyper-realistic synthetic faces but also\npose risks, thus making synthetic face detection crucial. Previous research\nfocuses on the general differences between generated images and real images,\noften overlooking the discrepancies among various generative techniques. In\nthis paper, we explore the intrinsic relationship between synthetic images and\ntheir corresponding generation technologies. We find that specific images\nexhibit significant reconstruction discrepancies across different generative\nmethods and that matching generation techniques provide more accurate\nreconstructions. Based on this insight, we propose a Multi-Reconstruction-based\ndetector. By reversing and reconstructing images using multiple generative\nmodels, we analyze the reconstruction differences among real, GAN-generated,\nand DM-generated images to facilitate effective differentiation. Additionally,\nwe introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic\nAsian faces generated with various GANs and DMs. This dataset complements\nexisting synthetic face datasets. Experimental results demonstrate that our\ndetector achieves exceptional performance, with strong generalization and\nrobustness.", "published": "2025-04-10 01:54:02", "link": "http://arxiv.org/abs/2504.07382v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BRepFormer: Transformer-Based B-rep Geometric Feature Recognition", "abstract": "Recognizing geometric features on B-rep models is a cornerstone technique for\nmultimedia content-based retrieval and has been widely applied in intelligent\nmanufacturing. However, previous research often merely focused on Machining\nFeature Recognition (MFR), falling short in effectively capturing the intricate\ntopological and geometric characteristics of complex geometry features. In this\npaper, we propose BRepFormer, a novel transformer-based model to recognize both\nmachining feature and complex CAD models' features. BRepFormer encodes and\nfuses the geometric and topological features of the models. Afterwards,\nBRepFormer utilizes a transformer architecture for feature propagation and a\nrecognition head to identify geometry features. During each iteration of the\ntransformer, we incorporate a bias that combines edge features and topology\nfeatures to reinforce geometric constraints on each face. In addition, we also\nproposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000\nB-rep models. By covering more complex B-rep models, it is better aligned with\nindustrial applications. The experimental results demonstrate that BRepFormer\nachieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF\ndatasets.", "published": "2025-04-10 01:36:06", "link": "http://arxiv.org/abs/2504.07378v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction", "abstract": "Predicting hand motion is critical for understanding human intentions and\nbridging the action space between human movements and robot manipulations.\nExisting hand trajectory prediction (HTP) methods forecast the future hand\nwaypoints in 3D space conditioned on past egocentric observations. However,\nsuch models are only designed to accommodate 2D egocentric video inputs. There\nis a lack of awareness of multimodal environmental information from both 2D and\n3D observations, hindering the further improvement of 3D HTP performance. In\naddition, these models overlook the synergy between hand movements and headset\ncamera egomotion, either predicting hand trajectories in isolation or encoding\negomotion only from past frames. To address these limitations, we propose novel\ndiffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin\nis designed to absorb multimodal information as input encompassing 2D RGB\nimages, 3D point clouds, past hand waypoints, and text prompt. Besides, two\nlatent diffusion models, the egomotion diffusion and the HTP diffusion as\ntwins, are integrated into MMTwin to predict camera egomotion and future hand\ntrajectories concurrently. We propose a novel hybrid Mamba-Transformer module\nas the denoising model of the HTP diffusion to better fuse multimodal features.\nThe experimental results on three publicly available datasets and our\nself-recorded data demonstrate that our proposed MMTwin can predict plausible\nfuture 3D hand trajectories compared to the state-of-the-art baselines, and\ngeneralizes well to unseen environments. The code and pretrained models will be\nreleased at https://github.com/IRMVLab/MMTwin.", "published": "2025-04-10 01:29:50", "link": "http://arxiv.org/abs/2504.07375v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "View-Dependent Uncertainty Estimation of 3D Gaussian Splatting", "abstract": "3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene\nreconstruction for its high visual accuracy. However, uncertainty estimation of\n3DGS scenes remains underexplored and is crucial to downstream tasks such as\nasset extraction and scene completion. Since the appearance of 3D gaussians is\nview-dependent, the color of a gaussian can thus be certain from an angle and\nuncertain from another. We thus propose to model uncertainty in 3DGS as an\nadditional view-dependent per-gaussian feature that can be modeled with\nspherical harmonics. This simple yet effective modeling is easily interpretable\nand can be integrated into the traditional 3DGS pipeline. It is also\nsignificantly faster than ensemble methods while maintaining high accuracy, as\ndemonstrated in our experiments.", "published": "2025-04-10 01:22:53", "link": "http://arxiv.org/abs/2504.07370v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Directed Temporal Tree Realization for Periodic Public Transport: Easy and Hard Cases", "abstract": "We study the complexity of the directed periodic temporal graph realization\nproblem. This work is motivated by the design of periodic schedules in public\ntransport with constraints on the quality of service. Namely, we require that\nthe fastest path between (important) pairs of vertices is upper bounded by a\nspecified maximum duration, encoded in an upper distance matrix $D$. While\nprevious work has considered the undirected version of the problem, the\napplication in public transport schedule design requires the flexibility to\nassign different departure times to the two directions of an edge. A problem\ninstance can only be feasible if all values of the distance matrix are at least\nshortest path distances. However, the task of realizing exact fastest path\ndistances in a periodic temporal graph is often too restrictive. Therefore, we\nintroduce a minimum slack parameter $k$ that describes a lower bound on the\nmaximum allowed waiting time on each path. We concentrate on tree topologies\nand provide a full characterization of the complexity landscape with respect to\nthe period $\\Delta$ and the minimum slack parameter~$k$, showing a sharp\nthreshold between NP-complete cases and cases which are always realizable. We\nalso provide hardness results for the special case of period $\\Delta = 2$ for\ngeneral directed and undirected graphs.", "published": "2025-04-10 17:36:23", "link": "http://arxiv.org/abs/2504.07920v1", "categories": ["cs.DS", "cs.CC", "cs.DM", "68R10 (Primary), 68Q25 (Secondary)"], "primary_category": "cs.DS"}
{"title": "Multiplicative assignment with upgrades", "abstract": "We study a problem related to submodular function optimization and the exact\nmatching problem for which we show a rather peculiar status: its natural\nLP-relaxation can have fractional optimal vertices, but there is always also an\noptimal integral vertex, which we can also compute in polynomial time.\n  More specifically, we consider the multiplicative assignment problem with\nupgrades in which we are given a set of customers and suppliers and we seek to\nassign each customer to a different supplier. Each customer has a demand and\neach supplier has a regular and an upgraded cost for each unit demand provided\nto the respective assigned client. Our goal is to upgrade at most $k$ suppliers\nand to compute an assignment in order to minimize the total resulting cost.\nThis can be cast as the problem to compute an optimal matching in a bipartite\ngraph with the additional constraint that we must select $k$ edges from a\ncertain group of edges, similar to selecting $k$ red edges in the exact\nmatching problem. Also, selecting the suppliers to be upgraded corresponds to\nmaximizing a submodular set function under a cardinality constraint.\n  Our result yields an efficient LP-based algorithm to solve our problem\noptimally. In addition, we provide also a purely strongly polynomial-time\nalgorithm for it. As an application, we obtain exact algorithms for the\nupgrading variant of the problem to schedule jobs on identical or uniformly\nrelated machines in order to minimize their sum of completion times, i.e.,\nwhere we may upgrade up to $k$ jobs to reduce their respective processing\ntimes.", "published": "2025-04-10 11:25:29", "link": "http://arxiv.org/abs/2504.07663v1", "categories": ["cs.DS", "cs.DM", "math.OC"], "primary_category": "cs.DS"}
{"title": "Computing gradient vector fields with Morse sequences", "abstract": "We rely on the framework of Morse sequences to enable the direct computation\nof gradient vector fields on simplicial complexes. A Morse sequence is a\nfiltration from a subcomplex L to a complex K via elementary expansions and\nfillings, naturally encoding critical and regular simplexes. Maximal increasing\nand minimal decreasing schemes allow constructing these sequences, and are\nlinked to algorithms like Random Discrete Morse and Coreduction. Extending the\napproach to cosimplicial complexes (S = K \\ L), we define operations --\nreductions, perforations, coreductions, and coperforations -- for efficient\ncomputation. We further generalize to F -sequences, which are Morse sequences\nweighted by an arbitrary stack function F , and provide algorithms to compute\nmaximal and minimal sequences. A particular case is when the stack function is\ngiven through a vertex map, as it is common in topological data analysis. We\nshow that we retrieve existing methods when the vertex map is injective; in\nthis case, the complex partitions into lower stars, facilitating parallel\nprocessing. Thus, this paper proposes simple, flexible, and computationally\nefficient approaches to obtain Morse sequences from arbitrary stack functions,\nallowing to generalize previous approaches dedicated to computing gradient\nvector fields from injective vertex maps.", "published": "2025-04-10 07:48:31", "link": "http://arxiv.org/abs/2504.07526v1", "categories": ["cs.DM", "math.AT"], "primary_category": "cs.DM"}
{"title": "Siren Federate: Bridging document, relational, and graph models for exploratory graph analysis", "abstract": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.", "published": "2025-04-10 14:52:03", "link": "http://arxiv.org/abs/2504.07815v1", "categories": ["cs.IR", "D.2.11; E.1; H.2.4; H.3.3; H.3.4"], "primary_category": "cs.IR"}
{"title": "REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic Resources", "abstract": "Retrieval test collections are essential for evaluating information retrieval\nsystems, yet they often lack generalizability across tasks. To overcome this\nlimitation, we introduce REANIMATOR, a versatile framework designed to enable\nthe repurposing of existing test collections by enriching them with extracted\nand synthetic resources. REANIMATOR enhances test collections from PDF files by\nparsing full texts and machine-readable tables, as well as related contextual\ninformation. It then employs state-of-the-art large language models to produce\nsynthetic relevance labels. Including an optional human-in-the-loop step can\nhelp validate the resources that have been extracted and generated. We\ndemonstrate its potential with a revitalized version of the TREC-COVID test\ncollection, showcasing the development of a retrieval-augmented generation\nsystem and evaluating the impact of tables on retrieval-augmented generation.\nREANIMATOR enables the reuse of test collections for new applications, lowering\ncosts and broadening the utility of legacy resources.", "published": "2025-04-10 09:25:11", "link": "http://arxiv.org/abs/2504.07584v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Explicit Uncertainty Modeling for Video Watch Time Prediction", "abstract": "In video recommendation, a critical component that determines the system's\nrecommendation accuracy is the watch-time prediction module, since how long a\nuser watches a video directly reflects personalized preferences. One of the key\nchallenges of this problem is the user's stochastic watch-time behavior. To\nimprove the prediction accuracy for such an uncertain behavior, existing\napproaches show that one can either reduce the noise through duration bias\nmodeling or formulate a distribution modeling task to capture the uncertainty.\nHowever, the uncontrolled uncertainty is not always equally distributed across\nusers and videos, inducing a balancing paradox between the model accuracy and\nthe ability to capture out-of-distribution samples. In practice, we find that\nthe uncertainty of the watch-time prediction model also provides key\ninformation about user behavior, which, in turn, could benefit the prediction\ntask itself. Following this notion, we derive an explicit uncertainty modeling\nstrategy for the prediction model and propose an adversarial optimization\nframework that can better exploit the user watch-time behavior. This framework\nhas been deployed online on an industrial video sharing platform that serves\nhundreds of millions of daily active users, which obtains a significant\nincrease in users' video watch time by 0.31% through the online A/B test.\nFurthermore, extended offline experiments on two public datasets verify the\neffectiveness of the proposed framework across various watch-time prediction\nbackbones.", "published": "2025-04-10 09:19:19", "link": "http://arxiv.org/abs/2504.07575v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Exploring Human-Like Thinking in Search Simulations with Large Language Models", "abstract": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations.", "published": "2025-04-10 09:04:58", "link": "http://arxiv.org/abs/2504.07570v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Emergency Communication: OTFS-Based Semantic Transmission with Diffusion Noise Suppression", "abstract": "Due to their flexibility and dynamic coverage capabilities, Unmanned Aerial\nVehicles (UAVs) have emerged as vital platforms for emergency communication in\ndisaster-stricken areas. However, the complex channel conditions in high-speed\nmobile scenarios significantly impact the reliability and efficiency of\ntraditional communication systems. This paper presents an intelligent emergency\ncommunication framework that integrates Orthogonal Time Frequency Space (OTFS)\nmodulation, semantic communication, and a diffusion-based denoising module to\naddress these challenges. OTFS ensures robust communication under dynamic\nchannel conditions due to its superior anti-fading characteristics and\nadaptability to rapidly changing environments. Semantic communication further\nenhances transmission efficiency by focusing on key information extraction and\nreducing data redundancy. Moreover, a diffusion-based channel denoising module\nis proposed to leverage the gradual noise reduction process and statistical\nnoise modeling, optimizing the accuracy of semantic information recovery.\nExperimental results demonstrate that the proposed solution significantly\nimproves link stability and transmission performance in high-mobility UAV\nscenarios, achieving at least a 3dB SNR gain over existing methods.", "published": "2025-04-10 03:25:56", "link": "http://arxiv.org/abs/2504.07420v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Towards Distribution Matching between Collaborative and Language Spaces for Generative Recommendation", "abstract": "Generative recommendation aims to learn the underlying generative process\nover the entire item set to produce recommendations for users. Although it\nleverages non-linear probabilistic models to surpass the limited modeling\ncapacity of linear factor models, it is often constrained by a trade-off\nbetween representation ability and tractability. With the rise of a new\ngeneration of generative methods based on pre-trained language models (LMs),\nincorporating LMs into general recommendation with implicit feedback has gained\nconsiderable attention. However, adapting them to generative recommendation\nremains challenging. The core reason lies in the mismatch between the\ninput-output formats and semantics of generative models and LMs, making it\nchallenging to achieve optimal alignment in the feature space. This work\naddresses this issue by proposing a model-agnostic generative recommendation\nframework called DMRec, which introduces a probabilistic meta-network to bridge\nthe outputs of LMs with user interactions, thereby enabling an equivalent\nprobabilistic modeling process. Subsequently, we design three cross-space\ndistribution matching processes aimed at maximizing shared information while\npreserving the unique semantics of each space and filtering out irrelevant\ninformation. We apply DMRec to three different types of generative\nrecommendation methods and conduct extensive experiments on three public\ndatasets. The experimental results demonstrate that DMRec can effectively\nenhance the recommendation performance of these generative models, and it shows\nsignificant advantages over mainstream LM-enhanced recommendation methods.", "published": "2025-04-10 01:09:30", "link": "http://arxiv.org/abs/2504.07363v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Function-Correcting Codes for $\u03c1$-locally $\u03bb$-functions", "abstract": "In this paper, we explore $\\rho$-locally $\\lambda$-functions and develop\nfunction-correcting codes for these functions. We propose an upper bound on the\nredundancy of these codes, based on the minimum possible length of an\nerror-correcting code with a given number of codewords and minimum distance.\nAdditionally, we provide a sufficient optimality condition for the\nfunction-correcting codes when $\\lambda = 4$. We also demonstrate that any\nfunction can be represented as a $\\rho$-locally $\\lambda$-function,\nillustrating this with a representation of Hamming weight distribution\nfunctions. Furthermore, we present another construction of function-correcting\ncodes for Hamming weight distribution functions.", "published": "2025-04-10 14:41:51", "link": "http://arxiv.org/abs/2504.07804v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Finite-Blocklength Information Theory", "abstract": "Traditional asymptotic information-theoretic studies of the fundamental\nlimits of wireless communication systems primarily rely on some ideal\nassumptions, such as infinite blocklength and vanishing error probability.\nWhile these assumptions enable tractable mathematical characterizations, they\nfail to capture the stringent requirements of some emerging next-generation\nwireless applications, such as ultra-reliable low latency communication and\nultra-massive machine type communication, in which it is required to support a\nmuch wider range of features including short-packet communication, extremely\nlow latency, and/or low energy consumption. To better support such\napplications, it is important to consider finite-blocklength information\ntheory. In this paper, we present a comprehensive review of the advances in\nthis field, followed by a discussion on the open questions. Specifically, we\ncommence with the fundamental limits of source coding in the non-asymptotic\nregime, with a particular focus on lossless and lossy compression in\npoint-to-point~(P2P) and multiterminal cases. Next, we discuss the fundamental\nlimits of channel coding in P2P channels, multiple access channels, and\nemerging massive access channels. We further introduce recent advances in joint\nsource and channel coding, highlighting its considerable performance advantage\nover separate source and channel coding in the non-asymptotic regime. In each\npart, we review various non-asymptotic achievability bounds, converse bounds,\nand approximations, as well as key ideas behind them, which are essential for\nproviding engineering insights into the design of future wireless communication\nsystems.", "published": "2025-04-10 13:39:56", "link": "http://arxiv.org/abs/2504.07743v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Integrated Sensing and Communications for Pinching-Antenna Systems (PASS)", "abstract": "An integrated sensing and communication (ISAC) design for pinching antenna\nsystems (PASS) is proposed, where the pinching antennas are deployed for\nestablishing reliable line-of-sight communication and sensing links. More\nparticularly, a separated ISAC design is proposed for the two-waveguide PASS,\nwhere one waveguide is used to emit the joint communication and sensing signals\nwhile the other waveguide is used to receive the reflected echo signals. Based\non this framework, a penalty-based alternating optimization algorithm is\nproposed to maximize the illumination power as well as ensure the communication\nquality-of-service requirement. Numerical results demonstrate that 1) the\nproposed PASS-ISAC scheme outperforms the other baseline schemes, and 2) the\nconsidered equal power allocation model achieves a performance comparable to\nthe optimal power allocation.", "published": "2025-04-10 12:58:46", "link": "http://arxiv.org/abs/2504.07709v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Exploiting Beamforming for Enforcing Semantic Secrecy in 5G NR mmWave Communications", "abstract": "We experimentally investigate the performance of semantically-secure physical\nlayer security (PLS) in 5G new radio (NR) mmWave communications during the\ninitial cell search procedure in the NR band n257 at 27 GHz. A gNB transmits\nPLS-encoded messages in the presence of an eavesdropper, who intercepts the\ncommunication by non-intrusively collecting channel readings in the form of IQ\nsamples. For the message transmission, we use the physical broadcast channel\n(PBCH) within the synchronization signal block. We analyze different\nsignal-to-noise ratio (SNR) conditions by progressively reducing the transmit\npower of the subcarriers carrying the PBCH channel, while ensuring optimal\nconditions for over-the-air frequency and timing synchronization. We measure\nthe secrecy performance of the communication in terms of upper and lower bounds\nfor the distinguishing error rate (DER) metric for different SNR levels and\nbeam angles when performing beamsteering in indoor scenarios, such as office\nenvironments and laboratory settings.", "published": "2025-04-10 12:07:32", "link": "http://arxiv.org/abs/2504.07678v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Integrated Sensing, Computing, and Semantic Communication with Fluid Antenna for Metaverse", "abstract": "The integration of sensing and communication (ISAC) is pivotal for the\nMetaverse but faces challenges like high data volume and privacy concerns. This\npaper proposes a novel integrated sensing, computing, and semantic\ncommunication (ISCSC) framework, which uses semantic communication to transmit\nonly contextual information, reducing data overhead and enhancing efficiency.\nTo address the sensitivity of semantic communication to channel conditions,\nfluid antennas (FAs) are introduced, enabling dynamic adaptability. The\nFA-enabled ISCSC framework considers multiple users and extended targets\ncomposed of a series of scatterers, formulating a joint optimization problem to\nmaximize the data rate while ensuring sensing accuracy and meeting\ncomputational and power constraints. An alternating optimization (AO) method\ndecomposes the problem into subproblems for ISAC beamforming, FA positioning,\nand semantic extraction. Simulations confirm the framework's effectiveness in\nimproving data rates and sensing performance.", "published": "2025-04-10 11:13:08", "link": "http://arxiv.org/abs/2504.07656v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Rate Analysis and Optimization of LoS Beyond Diagonal RIS-assisted MIMO Systems", "abstract": "In this letter, we derive an expression for the achievable rate in a\nmultiple-input multiple-output (MIMO) system assisted by a beyond-diagonal\nreconfigurable intelligent surface (BD-RIS) when the channels to and from the\nBD-RIS are line-of-sight (LoS) while the direct link is non-line-of-sight\n(NLoS). The rate expression allows to derive the optimal unitary and symmetric\nscattering BD-RIS matrix in closed form. Our simulation results show that the\nproposed solution is competitive even under the more usual Ricean channel\nfading model when the direct link is weak.", "published": "2025-04-10 10:48:52", "link": "http://arxiv.org/abs/2504.07647v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Energy-Efficient UAV Replacement in Software-Defined UAV Networks", "abstract": "Unmanned Aerial Vehicles (UAVs) in networked environments face significant\nchallenges due to energy constraints and limited battery life, which\nnecessitate periodic replacements to maintain continuous operation. Efficiently\nmanaging the handover of data flows during these replacements is crucial to\navoid disruptions in communication and to optimize energy consumption. This\npaper addresses the complex issue of energy-efficient UAV replacement in\nsoftware-defined UAV network. We introduce a novel approach based on\nestablishing a strict total ordering relation for UAVs and data flows, allowing\nus to formulate the problem as an integer linear program. By utilizing the\nGurobi solver, we obtain optimal handover schedules for the tested problem\ninstances. Additionally, we propose a heuristic algorithm that significantly\nreduces computational complexity while maintaining near-optimal performance.\nThrough comprehensive simulations, we demonstrate that our heuristic offers\npractical and scalable solution, ensuring energy-efficient UAV replacement\nwhile minimizing network disruptions. Our results suggest that the proposed\napproach can enhance UAV battery life and improve overall network reliability\nin real-world applications.", "published": "2025-04-10 06:58:35", "link": "http://arxiv.org/abs/2504.07500v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Enabling Gigantic MIMO Beamforming with Analog Computing", "abstract": "In our previous work, we have introduced a microwave linear analog computer\n(MiLAC) as an analog computer that processes microwave signals linearly,\ndemonstrating its potential to reduce the computational complexity of specific\nsignal processing tasks. In this paper, we extend these benefits to wireless\ncommunications, showcasing how MiLAC enables gigantic multiple-input\nmultiple-output (MIMO) beamforming entirely in the analog domain. MiLAC-aided\nbeamforming can implement regularized zero-forcing beamforming (R-ZFBF) at the\ntransmitter and minimum mean square error (MMSE) detection at the receiver,\nwhile significantly reducing hardware costs by minimizing the number of\nradio-frequency (RF) chains and only relying on low-resolution\nanalog-to-digital converters (ADCs) and digital-to-analog converters (DACs). In\naddition, it eliminates per-symbol operations by completely avoiding\ndigital-domain processing and remarkably reduces the computational complexity\nof R-ZFBF, which scales quadratically with the number of antennas instead of\ncubically. Numerical results show that it can perform R-ZFBF with a\ncomputational complexity reduction of up to 7400 times compared to digital\nbeamforming.", "published": "2025-04-10 06:06:03", "link": "http://arxiv.org/abs/2504.07477v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Task-oriented Age of Information for Remote Inference with Hybrid Language Models", "abstract": "Large Language Models (LLMs) have revolutionized the field of artificial\nintelligence (AI) through their advanced reasoning capabilities, but their\nextensive parameter sets introduce significant inference latency, posing a\nchallenge to ensure the timeliness of inference results. While Small Language\nModels (SLMs) offer faster inference speeds with fewer parameters, they often\ncompromise accuracy on complex tasks. This study proposes a novel remote\ninference system comprising a user, a sensor, and an edge server that\nintegrates both model types alongside a decision maker. The system dynamically\ndetermines the resolution of images transmitted by the sensor and routes\ninference tasks to either an SLM or LLM to optimize performance. The key\nobjective is to minimize the Task-oriented Age of Information (TAoI) by jointly\nconsidering the accuracy and timeliness of the inference task. Due to the\nnon-uniform transmission time and inference time, we formulate this problem as\na Semi-Markov Decision Process (SMDP). By converting the SMDP to an equivalent\nMarkov decision process, we prove that the optimal control policy follows a\nthreshold-based structure. We further develop a relative policy iteration\nalgorithm leveraging this threshold property. Simulation results demonstrate\nthat our proposed optimal policy significantly outperforms baseline approaches\nin managing the accuracy-timeliness trade-off.", "published": "2025-04-10 03:48:09", "link": "http://arxiv.org/abs/2504.07428v1", "categories": ["cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.IT"}
{"title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing", "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.", "published": "2025-04-10 17:59:56", "link": "http://arxiv.org/abs/2504.07964v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Trading Graph Neural Network", "abstract": "This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN)\nthat can structurally estimate the impact of asset features, dealer features\nand relationship features on asset prices in trading networks. It combines the\nstrength of the traditional simulated method of moments (SMM) and recent\nmachine learning techniques -- Graph Neural Network (GNN). It outperforms\nexisting reduced-form methods with network centrality measures in prediction\naccuracy. The method can be used on networks with any structure, allowing for\nheterogeneity among both traders and assets.", "published": "2025-04-10 17:40:31", "link": "http://arxiv.org/abs/2504.07923v1", "categories": ["q-fin.TR", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.PR"], "primary_category": "q-fin.TR"}
{"title": "Semantically Encoding Activity Labels for Context-Aware Human Activity Recognition", "abstract": "Prior work has primarily formulated CA-HAR as a multi-label classification\nproblem, where model inputs are time-series sensor data and target labels are\nbinary encodings representing whether a given activity or context occurs. These\nCA-HAR methods either predicted each label independently or manually imposed\nrelationships using graphs. However, both strategies often neglect an essential\naspect: activity labels have rich semantic relationships. For instance,\nwalking, jogging, and running activities share similar movement patterns but\ndiffer in pace and intensity, indicating that they are semantically related.\nConsequently, prior CA-HAR methods often struggled to accurately capture these\ninherent and nuanced relationships, particularly on datasets with noisy labels\ntypically used for CA-HAR or situations where the ideal sensor type is\nunavailable (e.g., recognizing speech without audio sensors). To address this\nlimitation, we propose SEAL, which leverage LMs to encode CA-HAR activity\nlabels to capture semantic relationships. LMs generate vector embeddings that\npreserve rich semantic information from natural language. Our SEAL approach\nencodes input-time series sensor data from smart devices and their associated\nactivity and context labels (text) as vector embeddings. During training, SEAL\naligns the sensor data representations with their corresponding\nactivity/context label embeddings in a shared embedding space. At inference\ntime, SEAL performs a similarity search, returning the CA-HAR label with the\nembedding representation closest to the input data. Although LMs have been\nwidely explored in other domains, surprisingly, their potential in CA-HAR has\nbeen underexplored, making our approach a novel contribution to the field. Our\nresearch opens up new possibilities for integrating more advanced LMs into\nCA-HAR tasks.", "published": "2025-04-10 17:30:07", "link": "http://arxiv.org/abs/2504.07916v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining", "abstract": "Reinforcement learning (RL)-based fine-tuning has become a crucial step in\npost-training language models for advanced mathematical reasoning and coding.\nFollowing the success of frontier reasoning models, recent work has\ndemonstrated that RL fine-tuning consistently improves performance, even in\nsmaller-scale models; however, the underlying mechanisms driving these\nimprovements are not well-understood. Understanding the effects of RL\nfine-tuning requires disentangling its interaction with pretraining data\ncomposition, hyperparameters, and model scale, but such problems are\nexacerbated by the lack of transparency regarding the training data used in\nmany existing models. In this work, we present a systematic end-to-end study of\nRL fine-tuning for mathematical reasoning by training models entirely from\nscratch on different mixtures of fully open datasets. We investigate the\neffects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration)\nacross models of different scales. Our study reveals that RL algorithms\nconsistently converge towards a dominant output distribution, amplifying\npatterns in the pretraining data. We also find that models of different scales\ntrained on the same data mixture will converge to distinct output\ndistributions, suggesting that there are scale-dependent biases in model\ngeneralization. Moreover, we find that RL post-training on simpler questions\ncan lead to performance gains on harder ones, indicating that certain reasoning\ncapabilities generalize across tasks. Our findings show that small-scale\nproxies in controlled settings can elicit interesting insights regarding the\nrole of RL in shaping language model behavior.", "published": "2025-04-10 17:15:53", "link": "http://arxiv.org/abs/2504.07912v1", "categories": ["cs.LG", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Hodge Laplacians and Hodge Diffusion Maps", "abstract": "We introduce Hodge Diffusion Maps, a novel manifold learning algorithm\ndesigned to analyze and extract topological information from high-dimensional\ndata-sets. This method approximates the exterior derivative acting on\ndifferential forms, thereby providing an approximation of the Hodge Laplacian\noperator. Hodge Diffusion Maps extend existing non-linear dimensionality\nreduction techniques, including vector diffusion maps, as well as the theories\nbehind diffusion maps and Laplacian Eigenmaps. Our approach captures\nhigher-order topological features of the data-set by projecting it into\nlower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a\ntheoretical framework to estimate the approximation error of the exterior\nderivative, based on sample points distributed over a real manifold. Numerical\nexperiments support and validate the proposed methodology.", "published": "2025-04-10 16:30:13", "link": "http://arxiv.org/abs/2504.07910v1", "categories": ["cs.LG", "68P05, 68T10, 68T45, 68W25"], "primary_category": "cs.LG"}
{"title": "DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows", "abstract": "Many real-world applications of flow-based generative models desire a diverse\nset of samples that cover multiple modes of the target distribution. However,\nthe predominant approach for obtaining diverse sets is not sample-efficient, as\nit involves independently obtaining many samples from the source distribution\nand mapping them through the flow until the desired mode coverage is achieved.\nAs an alternative to repeated sampling, we introduce DiverseFlow: a\ntraining-free approach to improve the diversity of flow models. Our key idea is\nto employ a determinantal point process to induce a coupling between the\nsamples that drives diversity under a fixed sampling budget. In essence,\nDiverseFlow allows exploration of more variations in a learned flow model with\nfewer samples. We demonstrate the efficacy of our method for tasks where\nsample-efficient diversity is desirable, such as text-guided image generation\nwith polysemous words, inverse problems like large-hole inpainting, and\nclass-conditional image synthesis.", "published": "2025-04-10 16:09:50", "link": "http://arxiv.org/abs/2504.07894v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection", "abstract": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches.", "published": "2025-04-10 15:39:10", "link": "http://arxiv.org/abs/2504.07863v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks", "abstract": "Motivated by the growing demand for low-precision arithmetic in computational\nscience, we exploit lower-precision emulation in Python -- widely regarded as\nthe dominant programming language for numerical analysis and machine learning.\nLow-precision training has revolutionized deep learning by enabling more\nefficient computation and reduced memory and energy consumption while\nmaintaining model fidelity. To better enable numerical experimentation with and\nexploration of low precision computation, we developed the Pychop library,\nwhich supports customizable floating-point formats and a comprehensive set of\nrounding modes in Python, allowing users to benefit from fast, low-precision\nemulation in numerous applications. Pychop also introduces interfaces for both\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\nnetwork training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design,\nimplementation, validation, and practical application of Pychop, establishing\nit as a foundational tool for advancing efficient mixed-precision algorithms.\nFurthermore, we present empirical results on low-precision emulation for image\nclassification and object detection using published datasets, illustrating the\nsensitivity of the use of low precision and offering valuable insights into its\nimpact. Pychop enables in-depth investigations into the effects of numerical\nprecision, facilitates the development of novel hardware accelerators, and\nintegrates seamlessly into existing deep learning workflows. Software and\nexperimental code are publicly available at\nhttps://github.com/inEXASCALE/pychop.", "published": "2025-04-10 15:12:29", "link": "http://arxiv.org/abs/2504.07835v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Smoothed Distance Kernels for MMDs and Applications in Wasserstein Gradient Flows", "abstract": "Negative distance kernels $K(x,y) := - \\|x-y\\|$ were used in the definition\nof maximum mean discrepancies (MMDs) in statistics and lead to favorable\nnumerical results in various applications. In particular, so-called slicing\ntechniques for handling high-dimensional kernel summations profit from the\nsimple parameter-free structure of the distance kernel. However, due to its\nnon-smoothness in $x=y$, most of the classical theoretical results, e.g. on\nWasserstein gradient flows of the corresponding MMD functional do not longer\nhold true. In this paper, we propose a new kernel which keeps the favorable\nproperties of the negative distance kernel as being conditionally positive\ndefinite of order one with a nearly linear increase towards infinity and a\nsimple slicing structure, but is Lipschitz differentiable now. Our construction\nis based on a simple 1D smoothing procedure of the absolute value function\nfollowed by a Riemann-Liouville fractional integral transform. Numerical\nresults demonstrate that the new kernel performs similarly well as the negative\ndistance kernel in gradient descent methods, but now with theoretical\nguarantees.", "published": "2025-04-10 14:57:33", "link": "http://arxiv.org/abs/2504.07820v1", "categories": ["stat.ML", "cs.LG", "math.FA", "math.PR"], "primary_category": "stat.ML"}
{"title": "Performance of Rank-One Tensor Approximation on Incomplete Data", "abstract": "We are interested in the estimation of a rank-one tensor signal when only a\nportion $\\varepsilon$ of its noisy observation is available. We show that the\nstudy of this problem can be reduced to that of a random matrix model whose\nspectral analysis gives access to the reconstruction performance. These results\nshed light on and specify the loss of performance induced by an artificial\nreduction of the memory cost of a tensor via the deletion of a random part of\nits entries.", "published": "2025-04-10 14:57:09", "link": "http://arxiv.org/abs/2504.07818v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "primary_category": "stat.ML"}
{"title": "Gradient-based Sample Selection for Faster Bayesian Optimization", "abstract": "Bayesian optimization (BO) is an effective technique for black-box\noptimization. However, its applicability is typically limited to\nmoderate-budget problems due to the cubic complexity in computing the Gaussian\nprocess (GP) surrogate model. In large-budget scenarios, directly employing the\nstandard GP model faces significant challenges in computational time and\nresource requirements. In this paper, we propose a novel approach,\ngradient-based sample selection Bayesian Optimization (GSSBO), to enhance the\ncomputational efficiency of BO. The GP model is constructed on a selected set\nof samples instead of the whole dataset. These samples are selected by\nleveraging gradient information to maintain diversity and representation. We\nprovide a theoretical analysis of the gradient-based sample selection strategy\nand obtain explicit sublinear regret bounds for our proposed framework.\nExtensive experiments on synthetic and real-world tasks demonstrate that our\napproach significantly reduces the computational cost of GP fitting in BO while\nmaintaining optimization performance comparable to baseline methods.", "published": "2025-04-10 13:38:15", "link": "http://arxiv.org/abs/2504.07742v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks", "abstract": "This work proposes a novel methodology for turbulence modeling in Large Eddy\nSimulation (LES) based on Graph Neural Networks (GNNs), which embeds the\ndiscrete rotational, reflectional and translational symmetries of the\nNavier-Stokes equations into the model architecture. In addition, suitable\ninvariant input and output spaces are derived that allow the GNN models to be\nembedded seamlessly into the LES framework to obtain a symmetry-preserving\nsimulation setup. The suitability of the proposed approach is investigated for\ntwo canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent\nchannel flow. For both cases, GNN models are trained successfully in actual\nsimulations using Reinforcement Learning (RL) to ensure that the models are\nconsistent with the underlying LES formulation and discretization. It is\ndemonstrated for the HIT case that the resulting GNN-based LES scheme recovers\nrotational and reflectional equivariance up to machine precision in actual\nsimulations. At the same time, the stability and accuracy remain on par with\nnon-symmetry-preserving machine learning models that fail to obey these\nproperties. The same modeling strategy translates well to turbulent channel\nflow, where the GNN model successfully learns the more complex flow physics and\nis able to recover the turbulent statistics and Reynolds stresses. It is shown\nthat the GNN model learns a zonal modeling strategy with distinct behaviors in\nthe near-wall and outer regions. The proposed approach thus demonstrates the\npotential of GNNs for turbulence modeling, especially in the context of LES and\nRL.", "published": "2025-04-10 13:37:54", "link": "http://arxiv.org/abs/2504.07741v1", "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "physics.flu-dyn"}
{"title": "A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows", "abstract": "Traditional physics-based models of geophysical flows, such as debris flows\nand landslides that pose significant risks to human lives and infrastructure\nare computationally expensive, limiting their utility for large-scale parameter\nsweeps, uncertainty quantification, inversions or real-time applications. This\nstudy presents an efficient alternative, a deep learning-based surrogate model\nbuilt using a modified U-Net architecture to predict the dynamics of\nrunoff-generated debris flows across diverse terrain based on data from physics\nbased simulations. The study area is divided into smaller patches for localized\npredictions using a patch-predict-stitch methodology (complemented by limited\nglobal data to accelerate training). The patches are then combined to\nreconstruct spatially continuous flow maps, ensuring scalability for large\ndomains. To enable fast training using limited expensive simulations, the deep\nlearning model was trained on data from an ensemble of physics based\nsimulations using parameters generated via Latin Hypercube Sampling and\nvalidated on unseen parameter sets and terrain, achieving maximum pointwise\nerrors below 10% and robust generalization. Uncertainty quantification using\nMonte Carlo methods are enabled using the validated surrogate, which can\nfacilitate probabilistic hazard assessments. This study highlights the\npotential of deep learning surrogates as powerful tools for geophysical flow\nanalysis, enabling computationally efficient and reliable probabilistic hazard\nmap predictions.", "published": "2025-04-10 13:29:37", "link": "http://arxiv.org/abs/2504.07736v1", "categories": ["physics.flu-dyn", "cs.LG", "physics.geo-ph"], "primary_category": "physics.flu-dyn"}
{"title": "Quantum Machine Learning: Unveiling Trends, Impacts through Bibliometric Analysis", "abstract": "Quantum Machine Learning (QML) is the intersection of two revolutionary\nfields: quantum computing and machine learning. It promises to unlock\nunparalleled capabilities in data analysis, model building, and problem-solving\nby harnessing the unique properties of quantum mechanics. This research\nendeavors to conduct a comprehensive bibliometric analysis of scientific\ninformation pertaining to QML covering the period from 2000 to 2023. An\nextensive dataset comprising 9493 scholarly works is meticulously examined to\nunveil notable trends, impact factors, and funding patterns within the domain.\nAdditionally, the study employs bibliometric mapping techniques to visually\nillustrate the network relationships among key countries, institutions,\nauthors, patent citations and significant keywords in QML research. The\nanalysis reveals a consistent growth in publications over the examined period.\nThe findings highlight the United States and China as prominent contributors,\nexhibiting substantial publication and citation metrics. Notably, the study\nconcludes that QML, as a research subject, is currently in a formative stage,\ncharacterized by robust scholarly activity and ongoing development.", "published": "2025-04-10 13:18:48", "link": "http://arxiv.org/abs/2504.07726v1", "categories": ["cs.DL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability", "abstract": "Incomplete data, confounding effects, and violations of the Markov property\nare interrelated problems which are ubiquitous in Reinforcement Learning\napplications. We introduce the concept of ``partial ignorabilty\" and leverage\nit to establish a novel convergence theorem for adaptive Reinforcement\nLearning. This theoretical result relaxes the Markov assumption on the\nstochastic process underlying conventional $Q$-learning, deploying a\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\nestablish optimality. This result has clear downstream implications for most\nactive subfields of Reinforcement Learning, with clear paths for extension to\nthe field of Causal Inference.", "published": "2025-04-10 13:15:52", "link": "http://arxiv.org/abs/2504.07722v1", "categories": ["cs.LG", "stat.ME", "60G"], "primary_category": "cs.LG"}
{"title": "Conformalized Generative Bayesian Imaging: An Uncertainty Quantification Framework for Computational Imaging", "abstract": "Uncertainty quantification plays an important role in achieving trustworthy\nand reliable learning-based computational imaging. Recent advances in\ngenerative modeling and Bayesian neural networks have enabled the development\nof uncertainty-aware image reconstruction methods. Current generative\nmodel-based methods seek to quantify the inherent (aleatoric) uncertainty on\nthe underlying image for given measurements by learning to sample from the\nposterior distribution of the underlying image. On the other hand, Bayesian\nneural network-based approaches aim to quantify the model (epistemic)\nuncertainty on the parameters of a deep neural network-based reconstruction\nmethod by approximating the posterior distribution of those parameters.\nUnfortunately, an ongoing need for an inversion method that can jointly\nquantify complex aleatoric uncertainty and epistemic uncertainty patterns still\npersists. In this paper, we present a scalable framework that can quantify both\naleatoric and epistemic uncertainties. The proposed framework accepts an\nexisting generative model-based posterior sampling method as an input and\nintroduces an epistemic uncertainty quantification capability through Bayesian\nneural networks with latent variables and deep ensembling. Furthermore, by\nleveraging the conformal prediction methodology, the proposed framework can be\neasily calibrated to ensure rigorous uncertainty quantification. We evaluated\nthe proposed framework on magnetic resonance imaging, computed tomography, and\nimage inpainting problems and showed that the epistemic and aleatoric\nuncertainty estimates produced by the proposed framework display the\ncharacteristic features of true epistemic and aleatoric uncertainties.\nFurthermore, our results demonstrated that the use of conformal prediction on\ntop of the proposed framework enables marginal coverage guarantees consistent\nwith frequentist principles.", "published": "2025-04-10 12:30:46", "link": "http://arxiv.org/abs/2504.07696v1", "categories": ["eess.IV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Data Requirement Goal Modeling for Machine Learning Systems", "abstract": "Machine Learning (ML) has been integrated into various software and systems.\nTwo main components are essential for training an ML model: the training data\nand the ML algorithm. Given the critical role of data in ML system development,\nit has become increasingly important to assess the quality of data attributes\nand ensure that the data meets specific requirements before its utilization.\nThis work proposes an approach to guide non-experts in identifying data\nrequirements for ML systems using goal modeling. In this approach, we first\ndevelop the Data Requirement Goal Model (DRGM) by surveying the white\nliterature to identify and categorize the issues and challenges faced by data\nscientists and requirement engineers working on ML-related projects. An initial\nDRGM was built to accommodate common tasks that would generalize across\nprojects. Then, based on insights from both white and gray literature, a\ncustomization mechanism is built to help adjust the tasks, KPIs, and goals'\nimportance of different elements within the DRGM. The generated model can aid\nits users in evaluating different datasets using GRL evaluation strategies. We\nthen validate the approach through two illustrative examples based on\nreal-world projects. The results from the illustrative examples demonstrate\nthat the data requirements identified by the proposed approach align with the\nrequirements of real-world projects, demonstrating the practicality and\neffectiveness of the proposed framework. The proposed dataset selection\ncustomization mechanism and the proposed DRGM are helpful in guiding\nnon-experts in identifying the data requirements for machine learning systems\ntailored to a specific ML problem. This approach also aids in evaluating\ndifferent dataset alternatives to choose the optimum dataset for the problem.\nFor future work, we recommend implementing tool support to generate the DRGM\nbased on a chatbot interface.", "published": "2025-04-10 11:30:25", "link": "http://arxiv.org/abs/2504.07664v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks", "abstract": "We present a method based on graph neural network (GNN) for prediction of\nprobabilities of usage of shopping-mall corridors. The heterogeneous graph\nnetwork of shops and corridor paths are obtained from floorplans of the malls\nby creating vector layers for corridors, shops and entrances. These are\nsubsequently assimilated into nodes and edges of graphs. The prediction of the\nusage probability is based on the shop features, namely, the area and usage\ncategories they fall into, and on the graph connecting these shops, corridor\njunctions and entrances by corridor paths. Though the presented method is\napplicable for training on datasets obtained from a field survey or from\npedestrian-detecting sensors, the target data of the supervised deep-learning\nwork flow in this work are obtained from a probability method. We also include\na context-specific representation learning of latent features. The\nusage-probability prediction is made on each edge, which is a connection by a\nsection of corridor path between the adjacent nodes representing the shops or\ncorridor points. To create a feature for each edge, the hidden-layer feature\nvectors acquired in the message-passing GNN layers at the nodes of each edge\nare averaged and concatenated with the vector obtained by their multiplication.\nThese edge-features are then passed to multilayer perceptrons (MLP) to make the\nfinal prediction of usage probability on each edge. The samples of synthetic\nlearning dataset for each shopping mall are obtained by changing the shops'\nusage and area categories, and by subsequently feeding the graph into the\nprobability model.\n  When including different shopping malls in a single dataset, we also propose\nto consider graph-level features to inform the model with specific identifying\nfeatures of each mall.", "published": "2025-04-10 10:48:36", "link": "http://arxiv.org/abs/2504.07645v1", "categories": ["cs.LG", "68T07", "G.3; I.2; J.4"], "primary_category": "cs.LG"}
{"title": "Kernel Logistic Regression Learning for High-Capacity Hopfield Networks", "abstract": "Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron\nratio around 0.14). We propose Kernel Logistic Regression (KLR) learning.\nUnlike linear methods, KLR uses kernels to implicitly map patterns to\nhigh-dimensional feature space, enhancing separability. By learning dual\nvariables, KLR dramatically improves storage capacity, achieving perfect recall\neven when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and\nenhances noise robustness. KLR demonstrably outperforms Hebbian and linear\nlogistic regression approaches.", "published": "2025-04-10 10:27:43", "link": "http://arxiv.org/abs/2504.07633v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations", "abstract": "Accurate and concise governing equations are crucial for understanding system\ndynamics. Recently, data-driven methods such as sparse regression have been\nemployed to automatically uncover governing equations from data, representing a\nsignificant shift from traditional first-principles modeling. However, most\nexisting methods focus on scalar equations, limiting their applicability to\nsimple, low-dimensional scenarios, and failing to ensure rotation and\nreflection invariance without incurring significant computational cost or\nrequiring additional prior knowledge. This paper proposes a Cartesian\ntensor-based sparse regression (CTSR) technique to accurately and efficiently\nuncover complex, high-dimensional governing equations while ensuring\ninvariance. Evaluations on two two-dimensional (2D) and two three-dimensional\n(3D) test cases demonstrate that the proposed method achieves superior accuracy\nand efficiency compared to the conventional technique.", "published": "2025-04-10 10:06:29", "link": "http://arxiv.org/abs/2504.07618v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Conditional Conformal Risk Adaptation", "abstract": "Uncertainty quantification is becoming increasingly important in image\nsegmentation, especially for high-stakes applications like medical imaging.\nWhile conformal risk control generalizes conformal prediction beyond standard\nmiscoverage to handle various loss functions such as false negative rate, its\napplication to segmentation often yields inadequate conditional risk control:\nsome images experience very high false negative rates while others have\nnegligibly small ones. We develop Conformal Risk Adaptation (CRA), which\nintroduces a new score function for creating adaptive prediction sets that\nsignificantly improve conditional risk control for segmentation tasks. We\nestablish a novel theoretical framework that demonstrates a fundamental\nconnection between conformal risk control and conformal prediction through a\nweighted quantile approach, applicable to any score function. To address the\nchallenge of poorly calibrated probabilities in segmentation models, we\nintroduce a specialized probability calibration framework that enhances the\nreliability of pixel-wise inclusion estimates. Using these calibrated\nprobabilities, we propose Calibrated Conformal Risk Adaptation (CCRA) and a\nstratified variant (CCRA-S) that partitions images based on their\ncharacteristics and applies group-specific thresholds to further enhance\nconditional risk control. Our experiments on polyp segmentation demonstrate\nthat all three methods (CRA, CCRA, and CCRA-S) provide valid marginal risk\ncontrol and deliver more consistent conditional risk control across diverse\nimages compared to standard approaches, offering a principled approach to\nuncertainty quantification that is particularly valuable for high-stakes and\npersonalized segmentation applications.", "published": "2025-04-10 10:01:06", "link": "http://arxiv.org/abs/2504.07611v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints", "abstract": "We propose smoothed primal-dual algorithms for solving stochastic and smooth\nnonconvex optimization problems with linear inequality constraints. Our\nalgorithms are single-loop and only require a single stochastic gradient based\non one sample at each iteration. A distinguishing feature of our algorithm is\nthat it is based on an inexact gradient descent framework for the Moreau\nenvelope, where the gradient of the Moreau envelope is estimated using one step\nof a stochastic primal-dual augmented Lagrangian method. To handle inequality\nconstraints and stochasticity, we combine the recently established global error\nbounds in constrained optimization with a Moreau envelope-based analysis of\nstochastic proximal algorithms. For obtaining $\\varepsilon$-stationary points,\nwe establish the optimal $O(\\varepsilon^{-4})$ sample complexity guarantee for\nour algorithms and provide extensions to stochastic linear constraints. We also\nshow how to improve this complexity to $O(\\varepsilon^{-3})$ by using variance\nreduction and the expected smoothness assumption. Unlike existing methods, the\niterations of our algorithms are free of subproblems, large batch sizes or\nincreasing penalty parameters and use dual variable updates to ensure\nfeasibility.", "published": "2025-04-10 09:59:43", "link": "http://arxiv.org/abs/2504.07607v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Privacy-Preserving Vertical K-Means Clustering", "abstract": "Clustering is a fundamental data processing task used for grouping records\nbased on one or more features. In the vertically partitioned setting, data is\ndistributed among entities, with each holding only a subset of those features.\nA key challenge in this scenario is that computing distances between records\nrequires access to all distributed features, which may be privacy-sensitive and\ncannot be directly shared with other parties. The goal is to compute the joint\nclusters while preserving the privacy of each entity's dataset. Existing\nsolutions using secret sharing or garbled circuits implement privacy-preserving\nvariants of Lloyd's algorithm but incur high communication costs, scaling as\nO(nkt), where n is the number of data points, k the number of clusters, and t\nthe number of rounds. These methods become impractical for large datasets or\nseveral parties, limiting their use to LAN settings only. On the other hand, a\ndifferent line of solutions rely on differential privacy (DP) to outsource the\nlocal features of the parties to a central server. However, they often\nsignificantly degrade the utility of the clustering outcome due to excessive\nnoise. In this work, we propose a novel solution based on homomorphic\nencryption and DP, reducing communication complexity to O(n+kt). In our method,\nparties securely outsource their features once, allowing a computing party to\nperform clustering operations under encryption. DP is applied only to the\nclusters' centroids, ensuring privacy with minimal impact on utility. Our\nsolution clusters 100,000 two-dimensional points into five clusters using only\n73MB of communication, compared to 101GB for existing works, and completes in\njust under 3 minutes on a 100Mbps network, whereas existing works take over 1\nday. This makes our solution practical even for WAN deployments, all while\nmaintaining accuracy comparable to plaintext k-means algorithms.", "published": "2025-04-10 09:20:56", "link": "http://arxiv.org/abs/2504.07578v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Using LLMs for Analyzing AIS Data", "abstract": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.", "published": "2025-04-10 08:38:39", "link": "http://arxiv.org/abs/2504.07557v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving", "abstract": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.", "published": "2025-04-10 06:51:23", "link": "http://arxiv.org/abs/2504.07494v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature", "abstract": "Land surface temperature (LST) retrieval from remote sensing data is pivotal\nfor analyzing climate processes and surface energy budgets. However, LST\nretrieval is an ill-posed inverse problem, which becomes particularly severe\nwhen only a single band is available. In this paper, we propose a deeply\ncoupled framework integrating mechanistic modeling and machine learning to\nenhance the accuracy and generalizability of single-channel LST retrieval.\nTraining samples are generated using a physically-based radiative transfer\nmodel and a global collection of 5810 atmospheric profiles. A physics-informed\nmachine learning framework is proposed to systematically incorporate the first\nprinciples from classical physical inversion models into the learning workflow,\nwith optimization constrained by radiative transfer equations. Global\nvalidation demonstrated a 30% reduction in root-mean-square error versus\nstandalone methods. Under extreme humidity, the mean absolute error decreased\nfrom 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five\ncontinents confirmed the superior generalizability of this model.", "published": "2025-04-10 06:19:01", "link": "http://arxiv.org/abs/2504.07481v1", "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "physics.ao-ph"}
{"title": "Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security", "abstract": "Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS)\nattacks remains a critical challenge in cybersecurity. This research introduces\na hybrid deep learning model combining Gated Recurrent Units (GRUs) and a\nNeural Turing Machine (NTM) for enhanced intrusion detection. Trained on the\nUNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential\ndata processing and an NTM for long-term pattern recognition. The proposed\napproach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS\ntraffic. These findings offer promising advancements in real-time threat\ndetection and contribute to improved network security across various domains.", "published": "2025-04-10 06:08:04", "link": "http://arxiv.org/abs/2504.07478v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Traversal Learning Coordination For Lossless And Efficient Distributed Learning", "abstract": "In this paper, we introduce Traversal Learning (TL), a novel approach\ndesigned to address the problem of decreased quality encountered in popular\ndistributed learning (DL) paradigms such as Federated Learning (FL), Split\nLearning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an\naccuracy drop during aggregation due to its averaging function, while SL and\nSFL face increased loss due to the independent gradient updates on each split\nnetwork. TL adopts a unique strategy where the model traverses the nodes during\nforward propagation (FP) and performs backward propagation (BP) on the\norchestrator, effectively implementing centralized learning (CL) principles\nwithin a distributed environment. The orchestrator is tasked with generating\nvirtual batches and planning the sequential node visits of the model during FP,\naligning them with the ordered index of the data within these batches. We\nconducted experiments on six datasets representing diverse characteristics\nacross various domains. Our evaluation demonstrates that TL is on par with\nclassic CL approaches in terms of accurate inference, thereby offering a viable\nand robust solution for DL tasks. TL outperformed other DL methods and improved\naccuracy by 7.85% for independent and identically distributed (IID) datasets,\nmacro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text\nclassification, and AUC by 3.88% and 4.54% for medical and financial datasets,\nrespectively. By effectively preserving data privacy while maintaining\nperformance, TL represents a significant advancement in DL methodologies.", "published": "2025-04-10 05:48:57", "link": "http://arxiv.org/abs/2504.07471v1", "categories": ["cs.LG", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying", "abstract": "Fruit drying is widely used in food manufacturing to reduce product moisture,\nensure product safety, and extend product shelf life. Accurately predicting\nfinal moisture content (MC) is critically needed for quality control of drying\nprocesses. State-of-the-art methods can build deterministic relationships\nbetween process parameters and MC, but cannot adequately account for inherent\nprocess variabilities that are ubiquitous in fruit drying. To address this gap,\nthis paper presents a novel multi-modal data fusion framework to effectively\nfuse two modalities of data: tabular data (process parameters) and\nhigh-dimensional image data (images of dried apple slices) to enable accurate\nMC prediction. The proposed modeling architecture permits flexible adjustment\nof information portion from tabular and image data modalities. Experimental\nvalidation shows that the multi-modal approach improves predictive accuracy\nsubstantially compared to state-of-the-art methods. The proposed method reduces\nroot-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only,\nimage-only, and standard tabular-image fusion models, respectively.\nFurthermore, it is demonstrated that our method is robust in varied\ntabular-image ratios and capable of effectively capturing inherent small-scale\nprocess variabilities. The proposed framework is extensible to a variety of\nother drying technologies.", "published": "2025-04-10 05:29:04", "link": "http://arxiv.org/abs/2504.07465v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Unifying and extending Diffusion Models through PDEs for solving Inverse Problems", "abstract": "Diffusion models have emerged as powerful generative tools with applications\nin computer vision and scientific machine learning (SciML), where they have\nbeen used to solve large-scale probabilistic inverse problems. Traditionally,\nthese models have been derived using principles of variational inference,\ndenoising, statistical signal processing, and stochastic differential\nequations. In contrast to the conventional presentation, in this study we\nderive diffusion models using ideas from linear partial differential equations\nand demonstrate that this approach has several benefits that include a\nconstructive derivation of the forward and reverse processes, a unified\nderivation of multiple formulations and sampling strategies, and the discovery\nof a new class of models. We also apply the conditional version of these models\nto solving canonical conditional density estimation problems and challenging\ninverse problems. These problems help establish benchmarks for systematically\nquantifying the performance of different formulations and sampling strategies\nin this study, and for future studies. Finally, we identify and implement a\nmechanism through which a single diffusion model can be applied to measurements\nobtained from multiple measurement operators. Taken together, the contents of\nthis manuscript provide a new understanding and several new directions in the\napplication of diffusion models to solving physics-based inverse problems.", "published": "2025-04-10 04:07:36", "link": "http://arxiv.org/abs/2504.07437v1", "categories": ["cs.LG", "stat.CO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Conditional Data Synthesis Augmentation", "abstract": "Reliable machine learning and statistical analysis rely on diverse,\nwell-distributed training data. However, real-world datasets are often limited\nin size and exhibit underrepresentation across key subpopulations, leading to\nbiased predictions and reduced performance, particularly in supervised tasks\nsuch as classification. To address these challenges, we propose Conditional\nData Synthesis Augmentation (CoDSA), a novel framework that leverages\ngenerative models, such as diffusion models, to synthesize high-fidelity data\nfor improving model performance across multimodal domains including tabular,\ntextual, and image data. CoDSA generates synthetic samples that faithfully\ncapture the conditional distributions of the original data, with a focus on\nunder-sampled or high-interest regions. Through transfer learning, CoDSA\nfine-tunes pre-trained generative models to enhance the realism of synthetic\ndata and increase sample density in sparse areas. This process preserves\ninter-modal relationships, mitigates data imbalance, improves domain\nadaptation, and boosts generalization. We also introduce a theoretical\nframework that quantifies the statistical accuracy improvements enabled by\nCoDSA as a function of synthetic sample volume and targeted region allocation,\nproviding formal guarantees of its effectiveness. Extensive experiments\ndemonstrate that CoDSA consistently outperforms non-adaptive augmentation\nstrategies and state-of-the-art baselines in both supervised and unsupervised\nsettings.", "published": "2025-04-10 03:38:11", "link": "http://arxiv.org/abs/2504.07426v1", "categories": ["stat.ME", "cs.LG"], "primary_category": "stat.ME"}
{"title": "Multi-Selection for Recommendation Systems", "abstract": "We present the construction of a multi-selection model to answer\ndifferentially private queries in the context of recommendation systems. The\nserver sends back multiple recommendations and a ``local model'' to the user,\nwhich the user can run locally on its device to select the item that best fits\nits private features. We study a setup where the server uses a deep neural\nnetwork (trained on the Movielens 25M dataset as the ground truth for movie\nrecommendation. In the multi-selection paradigm, the average recommendation\nutility is approximately 97\\% of the optimal utility (as determined by the\nground truth neural network) while maintaining a local differential privacy\nguarantee with $\\epsilon$ ranging around 1 with respect to feature vectors of\nneighboring users. This is in comparison to an average recommendation utility\nof 91\\% in the non-multi-selection regime under the same constraints.", "published": "2025-04-10 02:57:14", "link": "http://arxiv.org/abs/2504.07403v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements", "abstract": "Reliable state estimation is essential for autonomous systems operating in\ncomplex, noisy environments. Classical filtering approaches, such as the Kalman\nfilter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and\neven more flexible particle filters often encounter sample degeneracy or high\ncomputational costs in large-scale domains. Meanwhile, adaptive machine\nlearning techniques, including Q-learning and neuroevolutionary algorithms such\nas NEAT, rely heavily on accurate state feedback to guide learning; when sensor\ndata are imperfect, these methods suffer from degraded convergence and\nsuboptimal performance. In this paper, we propose an integrated framework that\nunifies particle filtering with Q-learning and NEAT to explicitly address the\nchallenge of noisy measurements. By refining radar-based observations into\nreliable state estimates, our particle filter drives more stable policy updates\n(in Q-learning) or controller evolution (in NEAT), allowing both reinforcement\nlearning and neuroevolution to converge faster, achieve higher returns or\nfitness, and exhibit greater resilience to sensor uncertainty. Experiments on\ngrid-based navigation and a simulated car environment highlight consistent\ngains in training stability, final performance, and success rates over\nbaselines lacking advanced filtering. Altogether, these findings underscore\nthat accurate state estimation is not merely a preprocessing step, but a vital\ncomponent capable of substantially enhancing adaptive machine learning in\nreal-world applications plagued by sensor noise.", "published": "2025-04-10 02:20:45", "link": "http://arxiv.org/abs/2504.07393v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "Minimum width for universal approximation using squashable activation functions", "abstract": "The exact minimum width that allows for universal approximation of\nunbounded-depth networks is known only for ReLU and its variants. In this work,\nwe study the minimum width of networks using general activation functions.\nSpecifically, we focus on squashable functions that can approximate the\nidentity function and binary step function by alternatively composing with\naffine transformations. We show that for networks using a squashable activation\nfunction to universally approximate $L^p$ functions from $[0,1]^{d_x}$ to\n$\\mathbb R^{d_y}$, the minimum width is $\\max\\{d_x,d_y,2\\}$ unless $d_x=d_y=1$;\nthe same bound holds for $d_x=d_y=1$ if the activation function is monotone. We\nthen provide sufficient conditions for squashability and show that all\nnon-affine analytic functions and a class of piecewise functions are\nsquashable, i.e., our minimum width result holds for those general classes of\nactivation functions.", "published": "2025-04-10 01:23:24", "link": "http://arxiv.org/abs/2504.07371v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "abstract": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have targeted system-level engineering, little is explored\nthrough a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for LLM inference,\nbridging the gap between queuing and LLM system communities. In particular, we\nstudy the throughput aspect in LLM inference systems. We prove that a large\nclass of 'work-conserving' scheduling algorithms can achieve maximum throughput\nfor both individual requests and AI agent workloads, highlighting\n'work-conserving' as a key design principle in practice. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FastTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution.\n  Our results highlight the substantial benefits queuing community can offer in\nimproving LLM inference systems and call for more interdisciplinary\ndevelopments.", "published": "2025-04-10 00:12:12", "link": "http://arxiv.org/abs/2504.07347v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "primary_category": "stat.ML"}
{"title": "What Contributes to Affective Polarization in Networked Online Environments? Evidence from an Agent-Based Model", "abstract": "Affective polarization, or, inter-party hostility, is increasingly recognized\nas a pervasive issue in democracies worldwide, posing a threat to social\ncohesion. The digital media ecosystem, now widely accessible and ever-present,\nhas often been implicated in accelerating this phenomenon. However, the precise\ncausal mechanisms responsible for driving affective polarization have been a\nsubject of extensive debate. While the concept of echo chambers, characterized\nby individuals ensconced within like-minded groups, bereft of\ncounter-attitudinal content, has long been the prevailing hypothesis,\naccumulating empirical evidence suggests a more nuanced picture. This study\naims to contribute to the ongoing debate by employing an agent-based model to\nillustrate how affective polarization is either fostered or hindered by\nindividual news consumption and dissemination patterns based on ideological\nalignment. To achieve this, we parameterize three key aspects: (1) The\naffective asymmetry of individuals' engagement with in-party versus out-party\ncontent, (2) The proportion of in-party members within one's social\nneighborhood, and (3) The degree of partisan bias among the elites within the\npopulation. Subsequently, we observe macro-level changes in affective\npolarization within the population under various conditions stipulated by these\nparameters. This approach allows us to explore the intricate dynamics of\naffective polarization within digital environments, shedding light on the\ninterplay between individual behaviors, social networks, and information\nexposure.", "published": "2025-04-10 10:00:50", "link": "http://arxiv.org/abs/2504.07610v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Achilles Heel of Distributed Multi-Agent Systems", "abstract": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems.", "published": "2025-04-10 05:16:11", "link": "http://arxiv.org/abs/2504.07461v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Probabilistic Multi-Criteria Decision-Making for Circularity Performance of Modern Methods of Construction Products", "abstract": "The construction industry faces increasingly more significant pressure to\nreduce resource consumption, minimise waste, and enhance environmental\nperformance. Towards the transition to a circular economy in the construction\nindustry, one of the challenges is the lack of a standardised assessment\nframework and methods to measure circularity at the product level. To support a\nmore sustainable and circular construction industry through robust and enhanced\nscenario analysis, this paper integrates probabilistic analysis into the\ncoupled assessment framework; this research addresses uncertainties associated\nwith multiple criteria and diverse stakeholders in the construction industry to\nenable more robust decision-making support on both circularity and\nsustainability performance. By demonstrating the application in three\nreal-world MMC products, the proposed framework offers a novel approach to\nsimultaneously assess the circularity and sustainability of MMC products with\nrobustness and objectiveness.", "published": "2025-04-10 15:27:34", "link": "http://arxiv.org/abs/2504.07850v1", "categories": ["math.NA", "cs.NA", "stat.AP"], "primary_category": "math.NA"}
{"title": "A Riemannian Gradient Descent Method for the Least Squares Inverse Eigenvalue Problem", "abstract": "We address an algorithm for the least squares fitting of a subset of the\neigenvalues of an unknown Hermitian matrix lying an an affine subspace, called\nthe Lift and Projection (LP) method, due to Chen and Chu (SIAM Journal on\nNumerical Analysis, 33 (1996), pp.2417-2430). The LP method iteratively `lifts'\nthe current iterate onto the spectral constraint manifold then 'projects' onto\nthe solution's affine subspace. We prove that this is equivalent to a\nRiemannian Gradient Descent with respect to a natural Riemannian metric. This\ninsight allows us to derive a more efficient implementation, analyse more\nprecisely its global convergence properties, and naturally append additional\nconstraints to the problem. We provide several numerical experiments to\ndemonstrate the improvement in computation time, which can be more than an\norder of magnitude if the eigenvalue constraints are on the smallest\neigenvalues, the largest eigenvalues, or the eigenvalues closest to a given\nnumber. These experiments include an inverse eigenvalue problem arising in\nInelastic Neutron Scattering of Manganese-6, which requires the least squares\nfitting of 16 experimentally observed eigenvalues of a $32400\\times32400$\nsparse matrix from a 5-dimensional subspace of spin Hamiltonian matrices.", "published": "2025-04-10 14:47:16", "link": "http://arxiv.org/abs/2504.07809v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients", "abstract": "This work focuses on numerically solving a shape identification problem\nrelated to advection-diffusion processes with space-dependent coefficients\nusing shape optimization techniques. Two boundary-type cost functionals are\nconsidered, and their corresponding variations with respect to shapes are\nderived using the adjoint method, employing the chain rule approach. This\ninvolves firstly utilizing the material derivative of the state system and\nsecondly using its shape derivative. Subsequently, an alternating direction\nmethod of multipliers (ADMM) combined with the Sobolev-gradient-descent\nalgorithm is applied to stably solve the shape reconstruction problem.\nNumerical experiments in two and three dimensions are conducted to demonstrate\nthe feasibility of the methods.", "published": "2025-04-10 14:36:56", "link": "http://arxiv.org/abs/2504.07796v1", "categories": ["math.OC", "cs.NA", "math.NA", "49Q10, 35R25, 35R30, 49Q12"], "primary_category": "math.OC"}
{"title": "On the instabilities of naive FEM discretizations for PDEs with sign-changing coefficients", "abstract": "We consider a scalar diffusion equation with a sign-changing coefficient in\nits principle part. The well-posedness of such problems has already been\nstudied extensively provided that the contrast of the coefficient is\nnon-critical. Furthermore, many different approaches have been proposed to\nconstruct stable discretizations thereof, because naive finite element\ndiscretizations are expected to be non-reliable in general. However, no\nexplicit example proving the actual instability is known and numerical\nexperiments often do not manifest instabilities in a conclusive manner. To this\nend we construct an explicit example with a broad family of meshes for which we\nprove that the corresponding naive finite element discretizations are unstable.\nOn the other hand, we also provide a broad family of (non-symmetric) meshes for\nwhich we prove that the discretizations are stable. Together, these two\nfindings explain the results observed in numerical experiments.", "published": "2025-04-10 13:04:58", "link": "http://arxiv.org/abs/2504.07712v1", "categories": ["math.NA", "cs.NA", "65N12, 65N30, 78M10"], "primary_category": "math.NA"}
{"title": "Global approximation to the Boys functions for vectorized computation", "abstract": "A fast approximation to the Boys functions (related to the lower incomplete\ngamma function of half-integer parameter) by a single closed-form analytical\nexpression for all argument values have been developed and tested. Besides the\nexponential function needed anyway for downward recursion, it uses a small\nnumber of addition, multiplication, division, and square root operations, and\nthus is straightforward to vectorize.", "published": "2025-04-10 10:36:24", "link": "http://arxiv.org/abs/2504.07637v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A computational study of low precision incomplete Cholesky factorization preconditioners for sparse linear least-squares problems", "abstract": "Our interest lies in the robust and efficient solution of large sparse linear\nleast-squares problems. In recent years, hardware developments have led to a\nsurge in interest in exploiting mixed precision arithmetic within numerical\nlinear algebra algorithms to take advantage of potential savings in memory\nrequirements, runtime and energy use, whilst still achieving the requested\naccuracy. We explore employing mixed precision when solving least-squares\nproblems, focusing on the practicalities of developing robust approaches using\nlow precision incomplete Cholesky factorization preconditioners. Key penalties\nassociated with lower precision include a loss of reliability and less accuracy\nin the computed solution. Through experiments involving problems from practical\napplications, we study computing incomplete Cholesky factorizations of the\nnormal matrix using low precision and using the factors to precondition LSQR\nusing mixed precision. We investigate level-based and memory-limited incomplete\nfactorization preconditioners. We find that the former are not effective for\nleast-squares problems while the latter can provide high-quality\npreconditioners. In particular, half precision arithmetic can be considered if\nhigh accuracy is not required in the solution or the memory for the incomplete\nfactors is very restricted; otherwise, single precision can be used, and double\nprecision accuracy recovered while reducing memory consumption, even for\nill-conditioned problems.", "published": "2025-04-10 09:21:50", "link": "http://arxiv.org/abs/2504.07580v1", "categories": ["math.NA", "cs.NA", "65F20 (Primary) 65F50, 65F08 (Secondary)"], "primary_category": "math.NA"}
{"title": "Stability and Convergence of Strang Splitting Method for the Allen-Cahn Equation with Homogeneous Neumann Boundary Condition", "abstract": "The Strang splitting method has been widely used to solve nonlinear\nreaction-diffusion equations, with most theoretical convergence analysis\nassuming periodic boundary conditions. However, such analysis presents\nadditional challenges for the case of homogeneous Neumann boundary condition.\nIn this work the Strang splitting method with variable time steps is\ninvestigated for solving the Allen--Cahn equation with homogeneous Neumann\nboundary conditions. Uniform $H^k$-norm stability is established under the\nassumption that the initial condition $u^0$ belongs to the Sobolev space\n$H^k(\\Omega)$ with integer $k\\ge 0$, using the Gagliardo--Nirenberg\ninterpolation inequality and the Sobolev embedding inequality. Furthermore,\nrigorous convergence analysis is provided in the $H^k$-norm for initial\nconditions $u^0 \\in H^{k+6}(\\Omega)$, based on the uniform stability. Several\nnumerical experiments are conducted to verify the theoretical results,\ndemonstrating the effectiveness of the proposed method.", "published": "2025-04-10 07:33:42", "link": "http://arxiv.org/abs/2504.07520v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "High-order discretization errors for the Caputo derivative in H\u00f6lder spaces", "abstract": "Building upon the recent work of Teso and Plociniczak (2025) regarding L1\ndiscretization errors for the Caputo derivative in H\\\"{o}lder spaces, this\nstudy extends the analysis to higher-order discretization errors within the\nsame functional framework. We first investigate truncation errors for the L2\nand L1-2 methods, which approximate the Caputo derivative via piecewise\nquadratic interpolation. Then we generalize the results to arbitrary high-order\ndiscretization. Theoretical analyses reveal a unified error structure across\nall schemes: the convergence order equals the difference between the smoothness\ndegree of the function space and the fractional derivative order, i.e., order\nof error = degree of smoothness - order of the derivative. Numerical\nexperiments validate these theoretical findings.", "published": "2025-04-10 02:20:16", "link": "http://arxiv.org/abs/2504.07391v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Categorical Unsupervised Variational Acoustic Clustering", "abstract": "We propose a categorical approach for unsupervised variational acoustic\nclustering of audio data in the time-frequency domain. The consideration of a\ncategorical distribution enforces sharper clustering even when data points\nstrongly overlap in time and frequency, which is the case for most datasets of\nurban acoustic scenes. To this end, we use a Gumbel-Softmax distribution as a\nsoft approximation to the categorical distribution, allowing for training via\nbackpropagation. In this settings, the softmax temperature serves as the main\nmechanism to tune clustering performance. The results show that the proposed\nmodel can obtain impressive clustering performance for all considered datasets,\neven when data points strongly overlap in time and frequency.", "published": "2025-04-10 11:00:17", "link": "http://arxiv.org/abs/2504.07652v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Generalizability to Tone and Content Variations in the Transcription of Amplifier Rendered Electric Guitar Audio", "abstract": "Transcribing electric guitar recordings is challenging due to the scarcity of\ndiverse datasets and the complex tone-related variations introduced by\namplifiers, cabinets, and effect pedals. To address these issues, we introduce\nEGDB-PG, a novel dataset designed to capture a wide range of tone-related\ncharacteristics across various amplifier-cabinet configurations. In addition,\nwe propose the Tone-informed Transformer (TIT), a Transformer-based\ntranscription model enhanced with a tone embedding mechanism that leverages\nlearned representations to improve the model's adaptability to tone-related\nnuances. Experiments demonstrate that TIT, trained on EGDB-PG, outperforms\nexisting baselines across diverse amplifier types, with transcription accuracy\nimprovements driven by the dataset's diversity and the tone embedding\ntechnique. Through detailed benchmarking and ablation studies, we evaluate the\nimpact of tone augmentation, content augmentation, audio normalization, and\ntone embedding on transcription performance. This work advances electric guitar\ntranscription by overcoming limitations in dataset diversity and tone modeling,\nproviding a robust foundation for future research.", "published": "2025-04-10 03:01:14", "link": "http://arxiv.org/abs/2504.07406v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open Datasets for Grid Modeling and Visualization: An Alberta Power Network Case", "abstract": "In the power and energy industry, multiple entities in grid operational logs\nare frequently recorded and updated. Thanks to recent advances in IT facilities\nand smart metering services, a variety of datasets such as system load,\ngeneration mix, and grid connection are often publicly available. While these\nresources are valuable in evaluating power grid's operational conditions and\nsystem resilience, the lack of fine-grained, accurate locational information\nconstrain the usage of current data, which further hinders the development of\nsmart grid and renewables integration. For instance, electricity end users are\nnot aware of nodal generation mix or carbon emissions, while the general public\nhave limited understanding about the effect of demand response or renewables\nintegration if only the whole system's demands and generations are available.\nIn this work, we focus on recovering power grid topology and line flow\ndirections from open public dataset. Taking the Alberta grid as a working\nexample, we start from mapping multi-modal power system datasets to the grid\ntopology integrated with geographical information. By designing a novel\noptimization-based scheme to recover line flow directions, we are able to\nanalyze and visualize the interactions between generations and demand vectors\nin an efficient manner. Proposed research is fully open-sourced and highly\ngeneralizable, which can help model and visualize grid information, create\nsynthetic dataset, and facilitate analytics and decision-making framework for\nclean energy transition.", "published": "2025-04-10 15:45:07", "link": "http://arxiv.org/abs/2504.07870v1", "categories": ["cs.HC", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.HC"}
{"title": "On-Chip and Off-Chip TIA Amplifiers for Nanopore Signal Readout Design, Performance and Challenges: A Review", "abstract": "Advancements in biomedical research have driven continuous innovations in\nsensing and diagnostic technologies. Among these, nanopore based single\nmolecule sensing and sequencing is rapidly emerging as a powerful and versatile\nsensing methodology. Advancements in nanopore based approaches require\nconcomitant improvements in the electronic readout methods employed, from the\npoint of low noise, bandwidth and form factor. This article focuses on current\nsensing circuits designed and employed for ultra low noise nanopore signal\nreadout, addressing the fundamental limitations of traditional off chip\ntransimpedance amplifiers (TIAs), which suffer from high input parasitic\ncapacitance, bandwidth constraints, and increased noise at high frequencies.\nThis review explores the latest design schemes and circuit structures\nclassified into on-chip and off-chip TIA designs, highlighting their design\nimplementation, performance, respective challenges and explores the interplay\nbetween noise performance, capacitance, and bandwidth across diverse\ntransimpedance amplifier (TIA) configurations. Emphasis is placed on\ncharacterizing noise response under varying parasitic capacitance and\noperational frequencies, a systematic evaluation not extensively addressed in\nprior literature while also considering the allowable input current compliance\nrange limitations. The review also compares the widely used Axopatch 200B\nsystem to the designs reported in literature. The findings offer valuable\ninsights into optimizing TIA designs for enhanced signal integrity in high\nspeed and high sensitivity applications focusing on noise reduction, impedance\nmatching, DC blocking, and offset cancellation techniques.", "published": "2025-04-10 13:29:08", "link": "http://arxiv.org/abs/2504.07734v1", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-bio.BM", "q-bio.GN"], "primary_category": "eess.SP"}
{"title": "Adaptive Robust Unscented Kalman Filter for Dynamic State Estimation of Power System", "abstract": "Non-Gaussian noise and the uncertainty of noise distribution are the common\nfactors that reduce accuracy in dynamic state estimation of power systems (PS).\nIn addition, the optimal value of the free coefficients in the unscented Kalman\nfilter (UKF) based on information theoretic criteria is also an urgent problem.\nIn this paper, a robust adaptive UKF (AUKF) under generalized minimum mixture\nerror entropy with fiducial points (GMMEEF) over improve Snow Geese algorithm\n(ISGA) (ISGA-GMMEEF-AUKF) is proposed to overcome the above difficulties. The\nestimation process of the proposed algorithm is based on several key steps\nincluding augmented regression error model (AREM) construction, adaptive state\nestimation, and free coefficients optimization. Specifically, an AREM\nconsisting of state prediction and measurement errors is established at the\nfirst step. Then, GMMEEF-AUKF is developed by solving the optimization problem\nbased on GMMEEF, which uses a generalized Gaussian kernel combined with mixture\ncorrentropy to enhance the flexibility further and resolve the data problem\nwith complex attributes and update the noise covariance matrix according to the\nAREM framework. Finally, the ISGA is designed to automatically calculate the\noptimal value of coefficients such as the shape coefficients of the kernel in\nthe GMMEEF criterion, the coefficients selection sigma points in unscented\ntransform, and the update coefficient of the noise covariance matrices fit with\nthe PS model. Simulation results on the IEEE 14, 30, and 57-bus test systems in\ncomplex scenarios have confirmed that the proposed algorithm outperforms the\nMEEF-UKF and UKF by an average efficiency of 26% and 65%, respectively.", "published": "2025-04-10 13:27:54", "link": "http://arxiv.org/abs/2504.07731v1", "categories": ["eess.SP", "94-10, 94-05", "H.1.1; H.4.3"], "primary_category": "eess.SP"}
{"title": "Filtering through a topological lens: homology for point processes on the time-frequency plane", "abstract": "We introduce a very general approach to the analysis of signals from their\nnoisy measurements from the perspective of Topological Data Analysis (TDA).\nWhile TDA has emerged as a powerful analytical tool for data with pronounced\ntopological structures, here we demonstrate its applicability for general\nproblems of signal processing, without any a-priori geometric feature. Our\nmethods are well-suited to a wide array of time-dependent signals in different\nscientific domains, with acoustic signals being a particularly important\napplication. We invoke time-frequency representations of such signals, focusing\non their zeros which are gaining salience as a signal processing tool in view\nof their stability properties. Leveraging state-of-the-art topological\nconcepts, such as stable and minimal volumes, we develop a complete suite of\nTDA-based methods to explore the delicate stochastic geometry of these zeros,\ncapturing signals based on the disruption they cause to this rigid,\nhyperuniform spatial structure. Unlike classical spatial data tools, TDA is\nable to capture the full spectrum of the stochastic geometry of the zeros,\nthereby leading to powerful inferential outcomes that are underpinned by a\nprincipled statistical foundation. This is reflected in the power and\nversatility of our applications, which include competitive performance in\nprocessing. a wide variety of audio signals (esp. in low SNR regimes),\neffective detection and reconstruction of gravitational wave signals (a reputed\nsignal processing challenge with non-Gaussian noise), and medical time series\ndata from EEGs, indicating a wide horizon for the approach and methods\nintroduced in this paper.", "published": "2025-04-10 13:10:04", "link": "http://arxiv.org/abs/2504.07720v1", "categories": ["eess.SP", "math.AT"], "primary_category": "eess.SP"}
{"title": "Learning Higher-Order Interactions in Brain Networks via Topological Signal Processing", "abstract": "Our goal in this paper is to leverage the potential of the topological signal\nprocessing (TSP) framework for analyzing brain networks. Representing brain\ndata as signals over simplicial complexes allows us to capture higher-order\nrelationships within brain regions of interest (ROIs). Here, we focus on\nlearning the underlying brain topology from observed neural signals using two\ndistinct inference strategies. The first method relies on higher-order\nstatistical metrics to infer multiway relationships among ROIs. The second\nmethod jointly learns the brain topology and sparse signal representations, of\nboth the solenoidal and harmonic components of the signals, by minimizing the\ntotal variation along triangles and the data-fitting errors. Leveraging the\nproperties of solenoidal and irrotational signals, and their physical\ninterpretations, we extract functional connectivity features from brain\ntopologies and uncover new insights into functional organization patterns. This\nallows us to associate brain functional connectivity (FC) patterns of\nconservative signals with well-known functional segregation and integration\nproperties. Our findings align with recent neuroscience research, suggesting\nthat our approach may offer a promising pathway for characterizing the\nhigher-order brain functional connectivities.", "published": "2025-04-10 12:28:10", "link": "http://arxiv.org/abs/2504.07695v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Low-Complexity Optimization of Antenna Switching Schemes for Dynamic Channel Sounding", "abstract": "Understanding wireless channels is crucial for the design of wireless\nsystems. For mobile communication, sounders and antenna arrays with short\nmeasurement times are required to simultaneously capture the dynamic and\nspatial channel characteristics. Switched antenna arrays are an attractive\noption that can overcome the high cost of real arrays and the long measurement\ntimes of virtual arrays. Optimization of the switching sequences is then\nessential to avoid aliasing and increase the accuracy of channel parameter\nestimates. This paper provides a novel and comprehensive analysis of the design\nof switching sequences. We first review the conventional spatio-temporal\nambiguity function, extend it to dual-polarized antenna arrays, and analyze its\nprohibitive complexity when designing for ultra-massive antenna arrays. We thus\npropose a new method that uses the Fisher information matrix to tackle the\nestimation accuracy. We also propose to minimize the ambiguity by choosing a\nswitching sequence that minimizes side lobes in its Fourier spectrum. In this\nsense, we divide the sequence design problem into Fourier-based ambiguity\nreduction and Fisher-based accuracy improvement, and coin the resulting design\napproach as Fourier-Fisher. Simulations and measurements show that the\nFourier-Fisher approach achieves identical performance and significantly lower\ncomputational complexity than that of the conventional ambiguity-based\napproach.", "published": "2025-04-10 11:56:37", "link": "http://arxiv.org/abs/2504.07675v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Cross-Laplacians Based Topological Signal Processing over Cell MultiComplexes", "abstract": "The study of the interactions among different types of interconnected systems\nin complex networks has attracted significant interest across many research\nfields. However, effective signal processing over layered networks requires\ntopological descriptors of the intra- and cross-layers relationships that are\nable to disentangle the homologies of different domains, at different scales,\naccording to the specific learning task. In this paper, we present Cell\nMultiComplex (CMC) spaces, which are novel topological domains for representing\nmultiple higher-order relationships among interconnected complexes. We\nintroduce cross-Laplacians matrices, which are algebraic descriptors of CMCs\nenabling the extraction of topological invariants at different scales, whether\nglobal or local, inter-layer or intra-layer. Using the eigenvectors of these\ncross-Laplacians as signal bases, we develop topological signal processing\ntools for CMC spaces. In this first study, we focus on the representation and\nfiltering of noisy flows observed over cross-edges between different layers of\nCMCs to identify cross-layer hubs, i.e., key nodes on one layer controlling the\nothers.", "published": "2025-04-10 11:42:58", "link": "http://arxiv.org/abs/2504.07671v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "System Concept and Demonstration of Bistatic MIMO-OFDM-based ISAC", "abstract": "In future sixth-generation (6G) mobile networks, radar sensing is expected to\nbe offered as an additional service to its original purpose of communication.\nMerging these two functions results in integrated sensing and communication\n(ISAC) systems. In this context, bistatic ISAC appears as a possibility to\nexploit the distributed nature of cellular networks while avoiding highly\ndemanding hardware requirements such as full-duplex operation. Recent studies\nhave introduced strategies to perform required synchronization and data\nexchange between nodes for bistatic ISAC operation, based on orthogonal\nfrequency-division multiplexing (OFDM), however, only for single-input\nsingle-output architectures. In this article, a system concept for a bistatic\nmultiple-input multiple-output (MIMO)-OFDM-based ISAC system with beamforming\nat both transmitter and receiver is proposed, and a distribution\nsynchronization concept to ensure coherence among the different receive\nchannels for direction-of-arrival estimation is presented. After a discussion\non the ISAC processing chain, including relevant aspects for practical\ndeployments such as transmitter digital pre-distortion and receiver\ncalibration, a 4x8 MIMO measurement setup at 27.5 GHz and results are presented\nto validate the proposed system and distribution synchronization concepts.", "published": "2025-04-10 09:54:15", "link": "http://arxiv.org/abs/2504.07600v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Learning Joint Source-Channel Encoding in IRS-assisted Multi-User Semantic Communications", "abstract": "In this paper, we investigate a joint source-channel encoding (JSCE) scheme\nin an intelligent reflecting surface (IRS)-assisted multi-user semantic\ncommunication system. Semantic encoding not only compresses redundant\ninformation, but also enhances information orthogonality in a semantic feature\nspace. Meanwhile, the IRS can adjust the spatial orthogonality, enabling\nconcurrent multi-user semantic communication in densely deployed wireless\nnetworks to improve spectrum efficiency. We aim to maximize the users' semantic\nthroughput by jointly optimizing the users' scheduling, the IRS's passive\nbeamforming, and the semantic encoding strategies. To tackle this non-convex\nproblem, we propose an explainable deep neural network-driven deep\nreinforcement learning (XD-DRL) framework. Specifically, we employ a deep\nneural network (DNN) to serve as a joint source-channel semantic encoder,\nenabling transmitters to extract semantic features from raw images. By\nleveraging structural similarity, we assign some DNN weight coefficients as the\nIRS's phase shifts, allowing simultaneous optimization of IRS's passive\nbeamforming and DNN training. Given the IRS's passive beamforming and semantic\nencoding strategies, user scheduling is optimized using the DRL method.\nNumerical results validate that our JSCE scheme achieves superior semantic\nthroughput compared to the conventional schemes and efficiently reduces the\nsemantic encoder's mode size in multi-user scenarios.", "published": "2025-04-10 06:53:48", "link": "http://arxiv.org/abs/2504.07498v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Quickest change detection for UAV-based sensing", "abstract": "This paper addresses the problem of quickest change detection (QCD) at two\nspatially separated locations monitored by a single unmanned aerial vehicle\n(UAV) equipped with a sensor. At any location, the UAV observes i.i.d. data\nsequentially in discrete time instants. The distribution of the observation\ndata changes at some unknown, arbitrary time and the UAV has to detect this\nchange in the shortest possible time. Change can occur at most at one location\nover the entire infinite time horizon. The UAV switches between these two\nlocations in order to quickly detect the change. To this end, we propose\nLocation Switching and Change Detection (LS-CD) algorithm which uses a repeated\none-sided sequential probability ratio test (SPRT) based mechanism for\nobservation-driven location switching and change detection. The primary goal is\nto minimize the worst-case average detection delay (WADD) while meeting\nconstraints on the average run length to false alarm (ARL2FA) and the UAV's\ntime-averaged energy consumption. We provide a rigorous theoretical analysis of\nthe algorithm's performance by using theory of random walk. Specifically, we\nderive tight upper and lower bounds to its ARL2FA and a tight upper bound to\nits WADD. In the special case of a symmetrical setting, our analysis leads to a\nnew asymptotic upper bound to the ARL2FA of the standard CUSUM algorithm, a\nnovel contribution not available in the literature, to our knowledge. Numerical\nsimulations demonstrate the efficacy of LS-CD.", "published": "2025-04-10 06:49:55", "link": "http://arxiv.org/abs/2504.07493v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "RIS-Aided Integrated Sensing and Communication Waveform Design With Tunable PAPR", "abstract": "Low peak-to-average power ratio (PAPR) transmission is an important and\nfavorable requirement prevalent in radar and communication systems, especially\nin transmission links integrated with high power amplifiers. Meanwhile,\nmotivated by the advantages of reconfigurable intelligent surface (RIS) in\nmitigating multi-user interference (MUI) to enhance the communication rate,\nthis paper investigates the design problem of joint waveform and passive\nbeamforming with PAPR constraint for integrated sensing and communication\n(ISAC) systems, where RIS is deployed for downlink communication. We first\nconstruct a trade-off optimization problem for the MUI and beampattern\nsimilarity under PAPR constraint. Then, in order to solve this multivariate\nproblem, an iterative optimization algorithm based on alternating direction\nmethod of multipliers (ADMM) and manifold optimization is proposed. Finally,\nthe simulation results show that the designed waveforms can well satisfy the\nPAPR requirement of the ISAC systems and achieve a trade-off between radar and\ncommunication performance. Under high signal-to-noise ratio (SNR) conditions,\ncompared to systems without RIS, RIS-aided ISAC systems have a performance\nimprovement of about 50\\% in communication rate and at least 1 dB in\nbeampatterning error.", "published": "2025-04-10 04:18:23", "link": "http://arxiv.org/abs/2504.07442v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Improved AFSA-Based Beam Training Without CSI for RIS-Assisted ISAC Systems", "abstract": "In this paper, we consider transmit beamforming and reflection patterns\ndesign in reconfigurable intelligent surface (RIS)-assisted integrated sensing\nand communication (ISAC) systems, where the dual-function base station (DFBS)\nlacks channel state information (CSI). To address the high overhead of cascaded\nchannel estimation, we propose an improved artificial fish swarm algorithm\n(AFSA) combined with a feedback-based joint active and passive beam training\nscheme. In this approach, we consider the interference caused by multipath user\necho signals on target detection and propose a beamforming design method that\nbalances both communication and sensing performance. Numerical simulations show\nthat the proposed AFSA outperforms other optimization algorithms, particularly\nin its robustness against echo interference under different communication\nsignal-to-noise ratio (SNR) constraints.", "published": "2025-04-10 04:06:07", "link": "http://arxiv.org/abs/2504.07436v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DS-Pnet: FM-Based Positioning via Downsampling", "abstract": "In this paper we present DS-Pnet, a novel framework for FM signal-based\npositioning that addresses the challenges of high computational complexity and\nlimited deployment in resource-constrained environments. Two downsampling\nmethods-IQ signal downsampling and time-frequency representation\ndownsampling-are proposed to reduce data dimensionality while preserving\ncritical positioning features. By integrating with the lightweight MobileViT-XS\nneural network, the framework achieves high positioning accuracy with\nsignificantly reduced computational demands. Experiments on real-world FM\nsignal datasets demonstrate that DS-Pnet achieves superior performance in both\nindoor and outdoor scenarios, with space and time complexity reductions of\napproximately 87% and 99.5%, respectively, compared to an existing method,\nFM-Pnet. Despite the high compression, DS-Pnet maintains robust positioning\naccuracy, offering an optimal balance between efficiency and precision.", "published": "2025-04-10 03:49:24", "link": "http://arxiv.org/abs/2504.07429v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Deep Learning-Based Wideband Spectrum Sensing with Dual-Representation Inputs and Subband Shuffling Augmentation", "abstract": "The widespread adoption of mobile communication technology has led to a\nsevere shortage of spectrum resources, driving the development of cognitive\nradio technologies aimed at improving spectrum utilization, with spectrum\nsensing being the key enabler. This paper presents a novel deep learning-based\nwideband spectrum sensing framework that leverages multi-taper power spectral\ninputs to achieve high-precision and sample-efficient sensing. To enhance\nsensing accuracy, we incorporate a feature fusion strategy that combines\nmultiple power spectrum representations. To tackle the challenge of limited\nsample sizes, we propose two data augmentation techniques designed to expand\nthe training set and improve the network's detection probability. Comprehensive\nsimulation results demonstrate that our method outperforms existing approaches,\nparticularly in low signal-to-noise ratio conditions, achieving higher\ndetection probabilities and lower false alarm rates. The method also exhibits\nstrong robustness across various scenarios, highlighting its significant\npotential for practical applications in wireless communication systems.", "published": "2025-04-10 03:38:15", "link": "http://arxiv.org/abs/2504.07427v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Secure Directional Modulation with Movable Antenna Array Aided by RIS", "abstract": "In this paper, to fully exploit the performance gains from moveable antennas\n(MAs) and reconfigurable intelligent surface (RIS), a RIS-aided directional\nmodulation \\textcolor{blue}{(DM)} network with movable antenna at base station\n(BS) is established Based on the principle of DM, a BS equipped with MAs\ntransmits legitimate information to a single-antenna user (Bob) while\nexploiting artificial noise (AN) to degrade signal reception at the\neavesdropper (Eve). The combination of AN and transmission beamforming vectors\nis modeled as joint beamforming vector (JBV) to achieve optimal power\nallocation. The objective is to maximize the achievable secrecy rate (SR) by\noptimizing MAs antenna position, phase shift matrix (PSM) of RIS, and JBV. The\nlimited movable range (MR) and discrete candidate positions of the MAs at the\nBS are considered, which renders the optimization problem non-convex. To\naddress these challenges, an optimization method under perfect channel state\ninformation (CSI) is firstly designed, in which the MAs antenna positions are\nobtained using compressive sensing (CS) technology, and JBV and PSM are\niteratively optimized. Then, the design method and SR performance under\nimperfect CSI is investigated. The proposed algorithms have fewer iterations\nand lower complexity. Simulation results demonstrate that MAs outperform\nfixed-position antennas in SR performance when there is an adequately large MR\navailable.", "published": "2025-04-10 03:14:30", "link": "http://arxiv.org/abs/2504.07417v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "WK-Pnet: FM-Based Positioning via Wavelet Packet Decomposition and Knowledge Distillation", "abstract": "Accurate and efficient positioning in complex environments is critical for\napplications where traditional satellite-based systems face limitations, such\nas indoors or urban canyons. This paper introduces WK-Pnet, an FM-based indoor\npositioning framework that combines wavelet packet decomposition (WPD) and\nknowledge distillation. WK-Pnet leverages WPD to extract rich time-frequency\nfeatures from FM signals, which are then processed by a deep learning model for\nprecise position estimation. To address computational demands, we employ\nknowledge distillation, transferring insights from a high-capacity model to a\nstreamlined student model, achieving substantial reductions in complexity\nwithout sacrificing accuracy. Experimental results across diverse environments\nvalidate WK-Pnet's superior positioning accuracy and lower computational\nrequirements, making it a viable solution for positioning in real-time\nresource-constraint applications.", "published": "2025-04-10 02:47:54", "link": "http://arxiv.org/abs/2504.07399v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Diffusion Augmented Complex Maximum Total Correntropy Algorithm for Power System Frequency Estimation", "abstract": "Currently, adaptive filtering algorithms have been widely applied in\nfrequency estimation for power systems. However, research on diffusion tasks\nremains insufficient. Existing diffusion adaptive frequency estimation\nalgorithms exhibit certain limitations in handling input noise and lack\nrobustness against impulsive noise. Moreover, traditional adaptive filtering\nalgorithms designed based on the strictly-linear (SL) model fail to effectively\naddress frequency estimation challenges in unbalanced three-phase power\nsystems. To address these issues, this letter proposes an improved diffusion\naugmented complex maximum total correntropy (DAMTCC) algorithm based on the\nwidely linear (WL) model. The proposed algorithm not only significantly\nenhances the capability to handle input noise but also demonstrates superior\nrobustness to impulsive noise. Furthermore, it successfully resolves the\ncritical challenge of frequency estimation in unbalanced three-phase power\nsystems, offering an efficient and reliable solution for diffusion power system\nfrequency estimation. Finally, we analyze the stability of the algorithm and\ncomputer simulations verify the excellent performance of the algorithm.", "published": "2025-04-10 01:13:07", "link": "http://arxiv.org/abs/2504.07365v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey", "abstract": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air\nmobility, and the reliability of UAV avionics systems is critical to ensuring\nmission success, sustainability practices, and public safety. The success of\nUAV missions depends on effectively mitigating various aspects of electronic\nwarfare, including non-destructive and destructive cyberattacks, transponder\nvulnerabilities, and jamming threats, while rigorously implementing\ncountermeasures and defensive aids. This paper provides a comprehensive review\nof UAV cyberattacks, countermeasures, and defensive strategies. It explores\nUAV-to-UAV coordination attacks and their associated features, such as dispatch\nsystem attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks,\nTraffic Alert and Collision Avoidance System (TCAS)-induced collisions, and\nTCAS attacks. Additionally, the paper examines UAV-to-command center\ncoordination attacks, as well as UAV functionality attacks. The review also\ncovers various countermeasures and defensive aids designed for UAVs. Lastly, a\ncomparison of common cyberattacks and countermeasure approaches is conducted,\nalong with a discussion of future trends in the field. Keywords: Electronic\nwarfare, UAVs, Avionics Systems, cyberattacks, coordination attacks,\nfunctionality attacks, countermeasure, defensive-aids.", "published": "2025-04-10 00:56:52", "link": "http://arxiv.org/abs/2504.07358v1", "categories": ["cs.CR", "eess.SP"], "primary_category": "cs.CR"}
{"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.", "published": "2025-04-10 15:41:51", "link": "http://arxiv.org/abs/2504.07866v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "abstract": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa", "published": "2025-04-10 09:24:54", "link": "http://arxiv.org/abs/2504.07583v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations", "abstract": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.", "published": "2025-04-10 15:13:00", "link": "http://arxiv.org/abs/2504.07836v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution", "abstract": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.", "published": "2025-04-10 09:48:56", "link": "http://arxiv.org/abs/2504.07596v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Malware analysis assisted by AI with R2AI", "abstract": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.", "published": "2025-04-10 09:17:45", "link": "http://arxiv.org/abs/2504.07574v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Scaling Laws for Native Multimodal Models", "abstract": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.", "published": "2025-04-10 17:57:28", "link": "http://arxiv.org/abs/2504.07951v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition", "abstract": "Sign language is a fundamental means of communication for the deaf and\nhard-of-hearing (DHH) community, enabling nuanced expression through gestures,\nfacial expressions, and body movements. Despite its critical role in\nfacilitating interaction within the DHH population, significant barriers\npersist due to the limited fluency in sign language among the hearing\npopulation. Overcoming this communication gap through automatic sign language\nrecognition (SLR) remains a challenge, particularly at a dynamic word-level,\nwhere temporal and spatial dependencies must be effectively recognized. While\nConvolutional Neural Networks (CNNs) have shown potential in SLR, they are\ncomputationally intensive and have difficulties in capturing global temporal\ndependencies between video sequences. To address these limitations, we propose\na Video Vision Transformer (ViViT) model for word-level American Sign Language\n(ASL) recognition. Transformer models make use of self-attention mechanisms to\neffectively capture global relationships across spatial and temporal\ndimensions, which makes them suitable for complex gesture recognition tasks.\nThe VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset,\nhighlighting its strong performance compared to traditional CNNs with 65.89%.\nOur study demonstrates that transformer-based architectures have great\npotential to advance SLR, overcome communication barriers and promote the\ninclusion of DHH individuals.", "published": "2025-04-10 14:27:25", "link": "http://arxiv.org/abs/2504.07792v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BRepFormer: Transformer-Based B-rep Geometric Feature Recognition", "abstract": "Recognizing geometric features on B-rep models is a cornerstone technique for\nmultimedia content-based retrieval and has been widely applied in intelligent\nmanufacturing. However, previous research often merely focused on Machining\nFeature Recognition (MFR), falling short in effectively capturing the intricate\ntopological and geometric characteristics of complex geometry features. In this\npaper, we propose BRepFormer, a novel transformer-based model to recognize both\nmachining feature and complex CAD models' features. BRepFormer encodes and\nfuses the geometric and topological features of the models. Afterwards,\nBRepFormer utilizes a transformer architecture for feature propagation and a\nrecognition head to identify geometry features. During each iteration of the\ntransformer, we incorporate a bias that combines edge features and topology\nfeatures to reinforce geometric constraints on each face. In addition, we also\nproposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000\nB-rep models. By covering more complex B-rep models, it is better aligned with\nindustrial applications. The experimental results demonstrate that BRepFormer\nachieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF\ndatasets.", "published": "2025-04-10 01:36:06", "link": "http://arxiv.org/abs/2504.07378v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Using LLMs for Analyzing AIS Data", "abstract": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.", "published": "2025-04-10 08:38:39", "link": "http://arxiv.org/abs/2504.07557v2", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature", "abstract": "Land surface temperature (LST) retrieval from remote sensing data is pivotal\nfor analyzing climate processes and surface energy budgets. However, LST\nretrieval is an ill-posed inverse problem, which becomes particularly severe\nwhen only a single band is available. In this paper, we propose a deeply\ncoupled framework integrating mechanistic modeling and machine learning to\nenhance the accuracy and generalizability of single-channel LST retrieval.\nTraining samples are generated using a physically-based radiative transfer\nmodel and a global collection of 5810 atmospheric profiles. A physics-informed\nmachine learning framework is proposed to systematically incorporate the first\nprinciples from classical physical inversion models into the learning workflow,\nwith optimization constrained by radiative transfer equations. Global\nvalidation demonstrated a 30% reduction in root-mean-square error versus\nstandalone methods. Under extreme humidity, the mean absolute error decreased\nfrom 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five\ncontinents confirmed the superior generalizability of this model.", "published": "2025-04-10 06:19:01", "link": "http://arxiv.org/abs/2504.07481v2", "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "physics.ao-ph"}
{"title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora", "abstract": "Children can acquire language from less than 100 million words of input.\nLarge language models are far less data-efficient: they typically require 3 or\n4 orders of magnitude more data and still do not perform as well as humans on\nmany evaluations. These intensive resource demands limit the ability of\nresearchers to train new models and use existing models as developmentally\nplausible cognitive models. The BabyLM Challenge is a communal effort in which\nparticipants compete to optimize language model training on a fixed data\nbudget. Submissions are compared on various evaluation tasks targeting\ngrammatical ability, downstream task performance, and generalization.\nParticipants can submit to up to three tracks with progressively looser data\nrestrictions. From over 30 submissions, we extract concrete recommendations on\nhow best to train data-efficient language models, and on where future efforts\nshould (and perhaps should not) focus. The winning submissions using the\nLTG-BERT architecture (Samuel et al., 2023) outperformed models trained on\ntrillions of words. Other submissions achieved strong results through training\non shorter input sequences or training a student model on a pretrained teacher.\nCurriculum learning attempts, which accounted for a large number of\nsubmissions, were largely unsuccessful, though some showed modest improvements.", "published": "2025-04-10 23:22:43", "link": "http://arxiv.org/abs/2504.08165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?", "abstract": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use.", "published": "2025-04-10 20:39:18", "link": "http://arxiv.org/abs/2504.08120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geneshift: Impact of different scenario shift on Jailbreaking LLM", "abstract": "Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors,\nhave become a critical and challenging direction in AI safety. Despite\nachieving the promising attack success rate using dictionary-based evaluation,\nexisting jailbreak attack methods fail to output detailed contents to satisfy\nthe harmful request, leading to poor performance on GPT-based evaluation. To\nthis end, we propose a black-box jailbreak attack termed GeneShift, by using a\ngenetic algorithm to optimize the scenario shifts. Firstly, we observe that the\nmalicious queries perform optimally under different scenario shifts. Based on\nit, we develop a genetic algorithm to evolve and select the hybrid of scenario\nshifts. It guides our method to elicit detailed and actionable harmful\nresponses while keeping the seemingly benign facade, improving stealthiness.\nExtensive experiments demonstrate the superiority of GeneShift. Notably,\nGeneShift increases the jailbreak success rate from 0% to 60% when direct\nprompting alone would fail.", "published": "2025-04-10 20:02:35", "link": "http://arxiv.org/abs/2504.08104v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Multi-view autoencoders for Fake News Detection", "abstract": "Given the volume and speed at which fake news spreads across social media,\nautomatic fake news detection has become a highly important task. However, this\ntask presents several challenges, including extracting textual features that\ncontain relevant information about fake news. Research about fake news\ndetection shows that no single feature extraction technique consistently\noutperforms the others across all scenarios. Nevertheless, different feature\nextraction techniques can provide complementary information about the textual\ndata and enable a more comprehensive representation of the content. This paper\nproposes using multi-view autoencoders to generate a joint feature\nrepresentation for fake news detection by integrating several feature\nextraction techniques commonly used in the literature. Experiments on fake news\ndatasets show a significant improvement in classification performance compared\nto individual views (feature representations). We also observed that selecting\na subset of the views instead of composing a latent space with all the views\ncan be advantageous in terms of accuracy and computational effort. For further\ndetails, including source codes, figures, and datasets, please refer to the\nproject's repository: https://github.com/ingrydpereira/multiview-fake-news.", "published": "2025-04-10 19:59:34", "link": "http://arxiv.org/abs/2504.08102v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search", "abstract": "AI is increasingly playing a pivotal role in transforming how scientific\ndiscoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic\nsystem capable of producing the first entirely AI generated\npeer-review-accepted workshop paper. This system iteratively formulates\nscientific hypotheses, designs and executes experiments, analyzes and\nvisualizes data, and autonomously authors scientific manuscripts. Compared to\nits predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2\neliminates the reliance on human-authored code templates, generalizes\neffectively across diverse machine learning domains, and leverages a novel\nprogressive agentic tree-search methodology managed by a dedicated experiment\nmanager agent. Additionally, we enhance the AI reviewer component by\nintegrating a Vision-Language Model (VLM) feedback loop for iterative\nrefinement of content and aesthetics of the figures. We evaluated The AI\nScientist-v2 by submitting three fully autonomous manuscripts to a\npeer-reviewed ICLR workshop. Notably, one manuscript achieved high enough\nscores to exceed the average human acceptance threshold, marking the first\ninstance of a fully AI-generated paper successfully navigating a peer review.\nThis accomplishment highlights the growing capability of AI in conducting all\naspects of scientific research. We anticipate that further advancements in\nautonomous scientific discovery technologies will profoundly impact human\nknowledge generation, enabling unprecedented scalability in research\nproductivity and significantly accelerating scientific breakthroughs, greatly\nbenefiting society at large. We have open-sourced the code at\nhttps://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of\nthis transformative technology. We also discuss the role of AI in science,\nincluding AI safety.", "published": "2025-04-10 18:44:41", "link": "http://arxiv.org/abs/2504.08066v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Large-Scale Analysis of Online Questions Related to Opioid Use Disorder on Reddit", "abstract": "Opioid use disorder (OUD) is a leading health problem that affects individual\nwell-being as well as general public health. Due to a variety of reasons,\nincluding the stigma faced by people using opioids, online communities for\nrecovery and support were formed on different social media platforms. In these\ncommunities, people share their experiences and solicit information by asking\nquestions to learn about opioid use and recovery. However, these communities do\nnot always contain clinically verified information. In this paper, we study\nnatural language questions asked in the context of OUD-related discourse on\nReddit. We adopt transformer-based question detection along with hierarchical\nclustering across 19 subreddits to identify six coarse-grained categories and\n69 fine-grained categories of OUD-related questions. Our analysis uncovers ten\nareas of information seeking from Reddit users in the context of OUD: drug\nsales, specific drug-related questions, OUD treatment, drug uses, side effects,\nwithdrawal, lifestyle, drug testing, pain management and others, during the\nstudy period of 2018-2021. Our work provides a major step in improving the\nunderstanding of OUD-related questions people ask unobtrusively on Reddit. We\nfinally discuss technological interventions and public health harm reduction\ntechniques based on the topics of these questions.", "published": "2025-04-10 18:02:24", "link": "http://arxiv.org/abs/2504.08044v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Can Reasoning LLMs Enhance Clinical Document Classification?", "abstract": "Clinical document classification is essential for converting unstructured\nmedical texts into standardised ICD-10 diagnoses, yet it faces challenges due\nto complex medical language, privacy constraints, and limited annotated\ndatasets. Large Language Models (LLMs) offer promising improvements in accuracy\nand efficiency for this task. This study evaluates the performance and\nconsistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3\nMini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o\nMini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge\nsummaries using the MIMIC-IV dataset. Using cTAKES to structure clinical\nnarratives, models were assessed across three experimental runs, with majority\nvoting determining final predictions. Results showed that reasoning models\noutperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs\n60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and\nF1 score (76%). However, non-reasoning models demonstrated greater stability\n(91% vs 84% consistency). Performance varied across ICD-10 codes, with\nreasoning models excelling in complex cases but struggling with abstract\ncategories. Findings indicate a trade-off between accuracy and consistency,\nsuggesting that a hybrid approach could optimise clinical coding. Future\nresearch should explore multi-label classification, domain-specific\nfine-tuning, and ensemble methods to enhance model reliability in real-world\napplications.", "published": "2025-04-10 18:00:27", "link": "http://arxiv.org/abs/2504.08040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Speech to Summary: A Comprehensive Survey of Speech Summarization", "abstract": "Speech summarization has become an essential tool for efficiently managing\nand accessing the growing volume of spoken and audiovisual content. However,\ndespite its increasing importance, speech summarization is still not clearly\ndefined and intersects with several research areas, including speech\nrecognition, text summarization, and specific applications like meeting\nsummarization. This survey not only examines existing datasets and evaluation\nmethodologies, which are crucial for assessing the effectiveness of\nsummarization approaches but also synthesizes recent developments in the field,\nhighlighting the shift from traditional systems to advanced models like\nfine-tuned cascaded architectures and end-to-end solutions.", "published": "2025-04-10 17:50:53", "link": "http://arxiv.org/abs/2504.08024v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction", "abstract": "The predictions of click through rate (CTR) and conversion rate (CVR) play a\ncrucial role in the success of ad-recommendation systems. A Deep Hierarchical\nEnsemble Network (DHEN) has been proposed to integrate multiple feature\ncrossing modules and has achieved great success in CTR prediction. However, its\nperformance for CVR prediction is unclear in the conversion ads setting, where\nan ad bids for the probability of a user's off-site actions on a third party\nwebsite or app, including purchase, add to cart, sign up, etc. A few challenges\nin DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a\nfew) should be included in DHEN? 2) How deep and wide should DHEN be to achieve\nthe best trade-off between efficiency and efficacy? 3) What hyper-parameters to\nchoose in each feature-crossing module? Orthogonal to the model architecture,\nthe input personalization features also significantly impact model performance\nwith a high degree of freedom. In this paper, we attack this problem and\npresent our contributions biased to the applied data science side, including:\n  First, we propose a multitask learning framework with DHEN as the single\nbackbone model architecture to predict all CVR tasks, with a detailed study on\nhow to make DHEN work effectively in practice; Second, we build both on-site\nreal-time user behavior sequences and off-site conversion event sequences for\nCVR prediction purposes, and conduct ablation study on its importance; Last but\nnot least, we propose a self-supervised auxiliary loss to predict future\nactions in the input sequence, to help resolve the label sparseness issue in\nCVR prediction.\n  Our method achieves state-of-the-art performance compared to previous single\nfeature crossing modules with pre-trained user personalization features.", "published": "2025-04-10 23:41:34", "link": "http://arxiv.org/abs/2504.08169v1", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rethinking the Foundations for Continual Reinforcement Learning", "abstract": "Algorithms and approaches for continual reinforcement learning have gained\nincreasing attention. Much of this early progress rests on the foundations and\nstandard practices of traditional reinforcement learning, without questioning\nif they are well-suited to the challenges of continual learning agents. We\nsuggest that many core foundations of traditional RL are, in fact, antithetical\nto the goals of continual reinforcement learning. We enumerate four such\nfoundations: the Markov decision process formalism, a focus on optimal\npolicies, the expected sum of rewards as the primary evaluation metric, and\nepisodic benchmark environments that embrace the other three foundations.\nShedding such sacredly held and taught concepts is not easy. They are\nself-reinforcing in that each foundation depends upon and holds up the others,\nmaking it hard to rethink each in isolation. We propose an alternative set of\nall four foundations that are better suited to the continual learning setting.\nWe hope to spur on others in rethinking the traditional foundations, proposing\nand critiquing alternatives, and developing new algorithms and approaches\nenabled by better-suited foundations.", "published": "2025-04-10 23:05:56", "link": "http://arxiv.org/abs/2504.08161v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI", "abstract": "Large language models (LLMs) have gained significant interest in industry due\nto their impressive capabilities across a wide range of tasks. However, the\nwidespread adoption of LLMs presents several challenges, such as integration\ninto existing applications and infrastructure, utilization of company\nproprietary data, models, and APIs, and meeting cost, quality, responsiveness,\nand other requirements. To address these challenges, there is a notable shift\nfrom monolithic models to compound AI systems, with the premise of more\npowerful, versatile, and reliable applications. However, progress thus far has\nbeen piecemeal, with proposals for agentic workflows, programming models, and\nextended LLM capabilities, without a clear vision of an overall architecture.\nIn this paper, we propose a 'blueprint architecture' for compound AI systems\nfor orchestrating agents and data for enterprise applications. In our proposed\narchitecture the key orchestration concept is 'streams' to coordinate the flow\nof data and instructions among agents. Existing proprietary models and APIs in\nthe enterprise are mapped to 'agents', defined in an 'agent registry' that\nserves agent metadata and learned representations for search and planning.\nAgents can utilize proprietary data through a 'data registry' that similarly\nregisters enterprise data of various modalities. Tying it all together, data\nand task 'planners' break down, map, and optimize tasks and queries for given\nquality of service (QoS) requirements such as cost, accuracy, and latency. We\nillustrate an implementation of the architecture for a use-case in the HR\ndomain and discuss opportunities and challenges for 'agentic AI' in the\nenterprise.", "published": "2025-04-10 22:19:41", "link": "http://arxiv.org/abs/2504.08148v1", "categories": ["cs.AI", "cs.DB", "cs.DC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms", "abstract": "Anomaly detection is a key research challenge in computer vision and machine\nlearning with applications in many fields from quality control to radar\nimaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly\ndetection can be used for the classification, detection, and segmentation of\nobjects of interest. However, there is no method for developing and\nbenchmarking these methods on SAR imagery. To address this issue, we introduce\nSAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a\ndeep-learning library for anomaly detection, SARIAD provides a comprehensive\nsuite of algorithms and datasets for assessing and developing anomaly detection\napproaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets\nalong with tools to effectively apply various anomaly detection algorithms to\nSAR imagery. Several anomaly detection metrics and visualizations are\navailable. Overall, SARIAD acts as a central package for benchmarking SAR\nmodels and datasets to allow for reproducible research in the field of anomaly\ndetection in SAR imagery. This package is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/SARIAD.", "published": "2025-04-10 20:31:25", "link": "http://arxiv.org/abs/2504.08115v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cellular Development Follows the Path of Minimum Action", "abstract": "Cellular development follows a stochastic yet rule-governed trajectory,\nthough the underlying principles remain elusive. Here, we propose that cellular\ndevelopment follows paths of least action, aligning with foundational physical\nlaws that govern dynamic systems across nature. We introduce a computational\nframework that takes advantage of the deep connection between the principle of\nleast action and maximum entropy to model developmental processes using\nTransformers architecture. This approach enables precise quantification of\nentropy production, information flow curvature, and local irreversibility for\ndevelopmental asymmetry in single-cell RNA sequence data. Within this unified\nframework, we provide interpretable metrics: entropy to capture\nexploration-exploitation trade-offs, curvature to assess plasticity-elasticity\ndynamics, and entropy production to characterize dedifferentiation and\ntransdifferentiation. We validate our method across both single-cell and\nembryonic development datasets, demonstrating its ability to reveal hidden\nthermodynamic and informational constraints shaping cellular fate decisions.", "published": "2025-04-10 19:44:29", "link": "http://arxiv.org/abs/2504.08096v1", "categories": ["physics.bio-ph", "cs.AI", "physics.comp-ph"], "primary_category": "physics.bio-ph"}
{"title": "STEI-PCN: an efficient pure convolutional network for traffic prediction via spatial-temporal encoding and inferring", "abstract": "Traffic data exhibits complex temporal, spatial, and spatial-temporal\ncorrelations. Most of models use either independent modules to separately\nextract temporal and spatial correlations or joint modules to synchronously\nextract them, without considering the spatial-temporal correlations. Moreover,\nmodels that consider joint spatial-temporal correlations (temporal, spatial,\nand spatial-temporal correlations) often encounter significant challenges in\naccuracy and computational efficiency which prevent such models from\ndemonstrating the expected advantages of a joint spatial-temporal correlations\narchitecture. To address these issues, this paper proposes an efficient pure\nconvolutional network for traffic prediction via spatial-temporal encoding and\ninferring (STEI-PCN). The model introduces and designs a dynamic adjacency\nmatrix inferring module based on absolute spatial and temporal coordinates, as\nwell as relative spatial and temporal distance encoding, using a graph\nconvolutional network combined with gating mechanism to capture local\nsynchronous joint spatial-temporal correlations. Additionally, three layers of\ntemporal dilated causal convolutional network are used to capture long-range\ntemporal correlations. Finally, through multi-view collaborative prediction\nmodule, the model integrates the gated-activated original, local synchronous\njoint spatial-temporal, and long-range temporal features to achieve\ncomprehensive prediction. This study conducts extensive experiments on flow\ndatasets (PeMS03/04/07/08) and speed dataset (PeMS-Bay), covering multiple\nprediction horizons. The results show that STEI-PCN demonstrates competitive\ncomputational efficiency in both training and inference speeds, and achieves\nsuperior or slightly inferior to state-of-the-art (SOTA) models on most\nevaluation metrics.", "published": "2025-04-10 18:32:56", "link": "http://arxiv.org/abs/2504.08061v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization", "abstract": "Quality-Diversity algorithms have transformed optimization by prioritizing\nthe discovery of diverse, high-performing solutions over a single optimal\nresult. However, traditional Quality-Diversity methods, such as MAP-Elites,\nrely heavily on predefined behavioral descriptors and complete prior knowledge\nof the task to define the behavioral space grid, limiting their flexibility and\napplicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites),\na novel Quality-Diversity algorithm that autonomously constructs a structured\nbehavioral space grid using unsupervised learning, eliminating the need for\nprior task-specific knowledge. At the core of VQ-Elites is the integration of\nVector Quantized Variational Autoencoders, which enables the dynamic learning\nof behavioral descriptors and the generation of a structured, rather than\nunstructured, behavioral space grid - a significant advancement over existing\nunsupervised Quality-Diversity approaches. This design establishes VQ-Elites as\na flexible, robust, and task-agnostic optimization framework. To further\nenhance the performance of unsupervised Quality-Diversity algorithms, we\nintroduce two key components: behavioral space bounding and cooperation\nmechanisms, which significantly improve convergence and performance. We\nvalidate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering\ntasks. The results demonstrate its ability to efficiently generate diverse,\nhigh-quality solutions, emphasizing its adaptability, scalability, robustness\nto hyperparameters, and potential to extend Quality-Diversity optimization to\ncomplex, previously inaccessible domains.", "published": "2025-04-10 18:23:19", "link": "http://arxiv.org/abs/2504.08057v1", "categories": ["cs.NE", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.NE"}
{"title": "Multi-Task Learning with Multi-Annotation Triplet Loss for Improved Object Detection", "abstract": "Triplet loss traditionally relies only on class labels and does not use all\navailable information in multi-task scenarios where multiple types of\nannotations are available. This paper introduces a Multi-Annotation Triplet\nLoss (MATL) framework that extends triplet loss by incorporating additional\nannotations, such as bounding box information, alongside class labels in the\nloss formulation. By using these complementary annotations, MATL improves\nmulti-task learning for tasks requiring both classification and localization.\nExperiments on an aerial wildlife imagery dataset demonstrate that MATL\noutperforms conventional triplet loss in both classification and localization.\nThese findings highlight the benefit of using all available annotations for\ntriplet loss in multi-task learning frameworks.", "published": "2025-04-10 18:20:31", "link": "http://arxiv.org/abs/2504.08054v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Compositional Flows for 3D Molecule and Synthesis Pathway Co-design", "abstract": "Many generative applications, such as synthesis-based 3D molecular design,\ninvolve constructing compositional objects with continuous features. Here, we\nintroduce Compositional Generative Flows (CGFlow), a novel framework that\nextends flow matching to generate objects in compositional steps while modeling\ncontinuous states. Our key insight is that modeling compositional state\ntransitions can be formulated as a straightforward extension of the flow\nmatching interpolation process. We further build upon the theoretical\nfoundations of generative flow networks (GFlowNets), enabling reward-guided\nsampling of compositional structures. We apply CGFlow to synthesizable drug\ndesign by jointly designing the molecule's synthetic pathway with its 3D\nbinding pose. Our approach achieves state-of-the-art binding affinity on all 15\ntargets from the LIT-PCBA benchmark, and 5.8$\\times$ improvement in sampling\nefficiency compared to 2D synthesis-based baseline. To our best knowledge, our\nmethod is also the first to achieve state of-art-performance in both Vina Dock\n(-9.38) and AiZynth success rate (62.2\\%) on the CrossDocked benchmark.", "published": "2025-04-10 18:10:34", "link": "http://arxiv.org/abs/2504.08051v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination", "abstract": "Fine-grained domain generalization (FGDG) aims to learn a fine-grained\nrepresentation that can be well generalized to unseen target domains when only\ntrained on the source domain data. Compared with generic domain generalization,\nFGDG is particularly challenging in that the fine-grained category can be only\ndiscerned by some subtle and tiny patterns. Such patterns are particularly\nfragile under the cross-domain style shifts caused by illumination, color and\netc. To push this frontier, this paper presents a novel Hyperbolic State Space\nHallucination (HSSH) method. It consists of two key components, namely, state\nspace hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH\nenriches the style diversity for the state embeddings by firstly extrapolating\nand then hallucinating the source images. Then, the pre- and post- style\nhallucinate state embeddings are projected into the hyperbolic manifold. The\nhyperbolic state space models the high-order statistics, and allows a better\ndiscernment of the fine-grained patterns. Finally, the hyperbolic distance is\nminimized, so that the impact of style variation on fine-grained patterns can\nbe eliminated. Experiments on three FGDG benchmarks demonstrate its\nstate-of-the-art performance.", "published": "2025-04-10 17:30:39", "link": "http://arxiv.org/abs/2504.08020v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization", "abstract": "Domain generalization aims to learn a representation from the source domain,\nwhich can be generalized to arbitrary unseen target domains. A fundamental\nchallenge for visual domain generalization is the domain gap caused by the\ndramatic style variation whereas the image content is stable. The realm of\nselective state space, exemplified by VMamba, demonstrates its global receptive\nfield in representing the content. However, the way exploiting the\ndomain-invariant property for selective state space is rarely explored. In this\npaper, we propose a novel Flow Factorized State Space model, dubbed as\nDG-Famba, for visual domain generalization. To maintain domain consistency, we\ninnovatively map the style-augmented and the original state embeddings by flow\nfactorization. In this latent flow space, each state embedding from a certain\nstyle is specified by a latent probability path. By aligning these probability\npaths in the latent space, the state embeddings are able to represent the same\ncontent distribution regardless of the style differences. Extensive experiments\nconducted on various visual domain generalization settings show its\nstate-of-the-art performance.", "published": "2025-04-10 17:24:53", "link": "http://arxiv.org/abs/2504.08019v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Learning Object Focused Attention", "abstract": "We propose an adaptation to the training of Vision Transformers (ViTs) that\nallows for an explicit modeling of objects during the attention computation.\nThis is achieved by adding a new branch to selected attention layers that\ncomputes an auxiliary loss which we call the object-focused attention (OFA)\nloss. We restrict the attention to image patches that belong to the same object\nclass, which allows ViTs to gain a better understanding of configural (or\nholistic) object shapes by focusing on intra-object patches instead of other\npatches such as those in the background. Our proposed inductive bias fits\neasily into the attention framework of transformers since it only adds an\nauxiliary loss over selected attention layers. Furthermore, our approach has no\nadditional overhead during inference. We also experiment with multiscale\nmasking to further improve the performance of our OFA model and give a path\nforward for self-supervised learning with our method. Our experimental results\ndemonstrate that ViTs with OFA achieve better classification results than their\nbase models, exhibit a stronger generalization ability to out-of-distribution\n(OOD) and adversarially corrupted images, and learn representations based on\nobject shapes rather than spurious correlations via general textures. For our\nOOD setting, we generate a novel dataset using the COCO dataset and Stable\nDiffusion inpainting which we plan to share with the community.", "published": "2025-04-10 23:23:26", "link": "http://arxiv.org/abs/2504.08166v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Investigating Vision-Language Model for Point Cloud-based Vehicle Classification", "abstract": "Heavy-duty trucks pose significant safety challenges due to their large size\nand limited maneuverability compared to passenger vehicles. A deeper\nunderstanding of truck characteristics is essential for enhancing the safety\nperspective of cooperative autonomous driving. Traditional LiDAR-based truck\nclassification methods rely on extensive manual annotations, which makes them\nlabor-intensive and costly. The rapid advancement of large language models\n(LLMs) trained on massive datasets presents an opportunity to leverage their\nfew-shot learning capabilities for truck classification. However, existing\nvision-language models (VLMs) are primarily trained on image datasets, which\nmakes it challenging to directly process point cloud data. This study\nintroduces a novel framework that integrates roadside LiDAR point cloud data\nwith VLMs to facilitate efficient and accurate truck classification, which\nsupports cooperative and safe driving environments. This study introduces three\nkey innovations: (1) leveraging real-world LiDAR datasets for model\ndevelopment, (2) designing a preprocessing pipeline to adapt point cloud data\nfor VLM input, including point cloud registration for dense 3D rendering and\nmathematical morphological techniques to enhance feature representation, and\n(3) utilizing in-context learning with few-shot prompting to enable vehicle\nclassification with minimally labeled training data. Experimental results\ndemonstrate encouraging performance of this method and present its potential to\nreduce annotation efforts while improving classification accuracy.", "published": "2025-04-10 22:37:27", "link": "http://arxiv.org/abs/2504.08154v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution", "abstract": "As generative AI image technologies become more widespread and advanced,\nthere is a growing need for strong attribution models. These models are crucial\nfor verifying the authenticity of images and identifying the architecture of\ntheir originating generative models-key to maintaining media integrity.\nHowever, attribution models struggle to generalize to unseen models, and\ntraditional fine-tuning methods for updating these models have shown to be\nimpractical in real-world settings. To address these challenges, we propose\nLoRA eXpandable Networks (LoRAX), a parameter-efficient class incremental\nalgorithm that adapts to novel generative image models without the need for\nfull retraining. Our approach trains an extremely parameter-efficient feature\nextractor per continual learning task via Low Rank Adaptation. Each\ntask-specific feature extractor learns distinct features while only requiring a\nsmall fraction of the parameters present in the underlying feature extractor's\nbackbone model. Our extensive experimentation shows LoRAX outperforms or\nremains competitive with state-of-the-art class incremental learning algorithms\non the Continual Deepfake Detection benchmark across all training scenarios and\nmemory settings, while requiring less than 3% of the number of trainable\nparameters per feature extractor compared to the full-rank implementation.\nLoRAX code is available at: https://github.com/mit-ll/lorax_cil.", "published": "2025-04-10 22:20:00", "link": "http://arxiv.org/abs/2504.08149v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Impact of Language Guidance: A Reproducibility Study", "abstract": "Modern deep-learning architectures need large amounts of data to produce\nstate-of-the-art results. Annotating such huge datasets is time-consuming,\nexpensive, and prone to human error. Recent advances in self-supervised\nlearning allow us to train huge models without explicit annotation. Contrastive\nlearning is a popular paradigm in self-supervised learning. Recent works like\nSimCLR and CLIP rely on image augmentations or directly minimizing cross-modal\nloss between image and text. Banani et al. (2023) propose to use language\nguidance to sample view pairs. They claim that language enables better\nconceptual similarity, eliminating the effects of visual variability. We\nreproduce their experiments to verify their claims and find that their dataset,\nRedCaps, contains low-quality captions. We use an off-the-shelf image\ncaptioning model, BLIP-2, to replace the captions and improve performance, and\nwe also devise a new metric to evaluate the semantic capabilities of\nself-supervised models based on interpretability methods.", "published": "2025-04-10 21:59:13", "link": "http://arxiv.org/abs/2504.08140v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects", "abstract": "Rapid advancements in text-to-3D generation require robust and scalable\nevaluation metrics that align closely with human judgment, a need unmet by\ncurrent metrics such as PSNR and CLIP, which require ground-truth data or focus\nonly on prompt fidelity. To address this, we introduce Gen3DEval, a novel\nevaluation framework that leverages vision large language models (vLLMs)\nspecifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates\ntext fidelity, appearance, and surface quality by analyzing 3D surface normals,\nwithout requiring ground-truth comparisons, bridging the gap between automated\nmetrics and user preferences. Compared to state-of-the-art task-agnostic\nmodels, Gen3DEval demonstrates superior performance in user-aligned\nevaluations, placing it as a comprehensive and accessible benchmark for future\nresearch on text-to-3D generation. The project page can be found here:\n\\href{https://shalini-maiti.github.io/gen3deval.github.io/}{https://shalini-maiti.github.io/gen3deval.github.io/}.", "published": "2025-04-10 20:57:40", "link": "http://arxiv.org/abs/2504.08125v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "POEM: Precise Object-level Editing via MLLM control", "abstract": "Diffusion models have significantly improved text-to-image generation,\nproducing high-quality, realistic images from textual descriptions. Beyond\ngeneration, object-level image editing remains a challenging problem, requiring\nprecise modifications while preserving visual coherence. Existing text-based\ninstructional editing methods struggle with localized shape and layout\ntransformations, often introducing unintended global changes. Image\ninteraction-based approaches offer better accuracy but require manual human\neffort to provide precise guidance. To reduce this manual effort while\nmaintaining a high image editing accuracy, in this paper, we propose POEM, a\nframework for Precise Object-level Editing using Multimodal Large Language\nModels (MLLMs). POEM leverages MLLMs to analyze instructional prompts and\ngenerate precise object masks before and after transformation, enabling\nfine-grained control without extensive user input. This structured reasoning\nstage guides the diffusion-based editing process, ensuring accurate object\nlocalization and transformation. To evaluate our approach, we introduce\nVOCEdits, a benchmark dataset based on PASCAL VOC 2012, augmented with\ninstructional edit prompts, ground-truth transformations, and precise object\nmasks. Experimental results show that POEM outperforms existing text-based\nimage editing approaches in precision and reliability while reducing manual\neffort compared to interaction-based methods.", "published": "2025-04-10 20:12:00", "link": "http://arxiv.org/abs/2504.08111v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Unconstrained 2D Pose Estimation of the Human Spine", "abstract": "We present SpineTrack, the first comprehensive dataset for 2D spine pose\nestimation in unconstrained settings, addressing a crucial need in sports\nanalytics, healthcare, and realistic animation. Existing pose datasets often\nsimplify the spine to a single rigid segment, overlooking the nuanced\narticulation required for accurate motion analysis. In contrast, SpineTrack\nannotates nine detailed spinal keypoints across two complementary subsets: a\nsynthetic set comprising 25k annotations created using Unreal Engine with\nbiomechanical alignment through OpenSim, and a real-world set comprising over\n33k annotations curated via an active learning pipeline that iteratively\nrefines automated annotations with human feedback. This integrated approach\nensures anatomically consistent labels at scale, even for challenging,\nin-the-wild images. We further introduce SpinePose, extending state-of-the-art\nbody pose estimators using knowledge distillation and an anatomical\nregularization strategy to jointly predict body and spine keypoints. Our\nexperiments in both general and sports-specific contexts validate the\neffectiveness of SpineTrack for precise spine pose estimation, establishing a\nrobust foundation for future research in advanced biomechanical analysis and 3D\nspine reconstruction in the wild.", "published": "2025-04-10 20:11:02", "link": "http://arxiv.org/abs/2504.08110v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting", "abstract": "Creating 3D content from single-view images is a challenging problem that has\nattracted considerable attention in recent years. Current approaches typically\nutilize score distillation sampling (SDS) from pre-trained 2D diffusion models\nto generate multi-view 3D representations. Although some methods have made\nnotable progress by balancing generation speed and model quality, their\nperformance is often limited by the visual inconsistencies of the diffusion\nmodel outputs. In this work, we propose ContrastiveGaussian, which integrates\ncontrastive learning into the generative process. By using a perceptual loss,\nwe effectively differentiate between positive and negative samples, leveraging\nthe visual inconsistencies to improve 3D generation quality. To further enhance\nsample differentiation and improve contrastive learning, we incorporate a\nsuper-resolution model and introduce another Quantity-Aware Triplet Loss to\naddress varying sample distributions during training. Our experiments\ndemonstrate that our approach achieves superior texture fidelity and improved\ngeometric consistency.", "published": "2025-04-10 19:56:09", "link": "http://arxiv.org/abs/2504.08100v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Interpretable Automatic Rosacea Detection with Whitened Cosine Similarity", "abstract": "According to the National Rosacea Society, approximately sixteen million\nAmericans suffer from rosacea, a common skin condition that causes flushing or\nlong-term redness on a person's face. To increase rosacea awareness and to\nbetter assist physicians to make diagnosis on this disease, we propose an\ninterpretable automatic rosacea detection method based on whitened cosine\nsimilarity in this paper. The contributions of the proposed methods are\nthree-fold. First, the proposed method can automatically distinguish patients\nsuffering from rosacea from people who are clean of this disease with a\nsignificantly higher accuracy than other methods in unseen test data, including\nboth classical deep learning and statistical methods. Second, the proposed\nmethod addresses the interpretability issue by measuring the similarity between\nthe test sample and the means of two classes, namely the rosacea class versus\nthe normal class, which allows both medical professionals and patients to\nunderstand and trust the results. And finally, the proposed methods will not\nonly help increase awareness of rosacea in the general population, but will\nalso help remind patients who suffer from this disease of possible early\ntreatment, as rosacea is more treatable in its early stages. The code and data\nare available at https://github.com/chengyuyang-njit/ICCRD-2025. The code and\ndata are available at https://github.com/chengyuyang-njit/ICCRD-2025.", "published": "2025-04-10 19:00:47", "link": "http://arxiv.org/abs/2504.08073v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "X-DECODE: EXtreme Deblurring with Curriculum Optimization and Domain Equalization", "abstract": "Restoring severely blurred images remains a significant challenge in computer\nvision, impacting applications in autonomous driving, medical imaging, and\nphotography. This paper introduces a novel training strategy based on\ncurriculum learning to improve the robustness of deep learning models for\nextreme image deblurring. Unlike conventional approaches that train on only low\nto moderate blur levels, our method progressively increases the difficulty by\nintroducing images with higher blur severity over time, allowing the model to\nadapt incrementally. Additionally, we integrate perceptual and hinge loss\nduring training to enhance fine detail restoration and improve training\nstability. We experimented with various curriculum learning strategies and\nexplored the impact of the train-test domain gap on the deblurring performance.\nExperimental results on the Extreme-GoPro dataset showed that our method\noutperforms the next best method by 14% in SSIM, whereas experiments on the\nExtreme-KITTI dataset showed that our method outperforms the next best by 18%\nin SSIM. Ablation studies showed that a linear curriculum progression\noutperforms step-wise, sigmoid, and exponential progressions, while\nhyperparameter settings such as the training blur percentage and loss function\nformulation all play important roles in addressing extreme blur artifacts.\nDatasets and code are available at https://github.com/RAPTOR-MSSTATE/XDECODE", "published": "2025-04-10 18:59:26", "link": "http://arxiv.org/abs/2504.08072v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery", "abstract": "This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-LACE.", "published": "2025-04-10 18:08:16", "link": "http://arxiv.org/abs/2504.08049v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Teaching Humans Subtle Differences with DIFFusion", "abstract": "Human expertise depends on the ability to recognize subtle visual\ndifferences, such as distinguishing diseases, species, or celestial phenomena.\nWe propose a new method to teach novices how to differentiate between nuanced\ncategories in specialized domains. Our method uses generative models to\nvisualize the minimal change in features to transition between classes, i.e.,\ncounterfactuals, and performs well even in domains where data is sparse,\nexamples are unpaired, and category boundaries are not easily explained by\ntext. By manipulating the conditioning space of diffusion models, our proposed\nmethod DIFFusion disentangles category structure from instance identity,\nenabling high-fidelity synthesis even in challenging domains. Experiments\nacross six domains show accurate transitions even with limited and unpaired\nexamples across categories. User studies confirm that our generated\ncounterfactuals outperform unpaired examples in teaching perceptual expertise,\nshowing the potential of generative models for specialized visual learning.", "published": "2025-04-10 18:04:22", "link": "http://arxiv.org/abs/2504.08046v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Matrix concentration inequalities for dependent binary random variables", "abstract": "We prove Bernstein-type matrix concentration inequalities for linear\ncombinations with matrix coefficients of binary random variables satisfying\ncertain $\\ell_\\infty$-independence assumptions, complementing recent results by\nKaufman, Kyng and Solda. For random variables with the Stochastic Covering\nProperty or Strong Rayleigh Property we prove estimates for general functions\nsatisfying certain direction aware matrix bounded difference inequalities,\ngeneralizing and strengthening earlier estimates by the first-named author and\nPolaczyk.\n  We also demonstrate a general decoupling inequality for a class of\nBanach-space valued quadratic forms in negatively associated random variables\nand combine it with the matrix Bernstein inequality to generalize results by\nTropp, Chr\\'etien and Darses, and Ruetz and Schnass, concerning the operator\nnorm of a random submatrix of a deterministic matrix, drawn by uniform sampling\nwithout replacements or rejective sampling, to submatrices given by general\nStrong Rayleigh sampling schemes.", "published": "2025-04-10 21:34:21", "link": "http://arxiv.org/abs/2504.08138v1", "categories": ["math.PR", "cs.DM", "math.FA", "60E15, 60B20"], "primary_category": "math.PR"}
{"title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles", "abstract": "Although the Boeing 737 Max incidents resulted from a mix of design\nshortcomings, regulatory oversights, and systemic issues, they also highlight a\ncritical gap in pilot training on managing automated systems during abnormal\nconditions. This example demonstrates the urgent need for focused, concise\ntraining on human-automation interaction - a need that is equally critical for\noperators of Level 2 ADAS-equipped vehicles, as discussed in detail later in\nthis article. The lack of structured education for semi-automated vehicle\noperators mirrors similar risks in other industries, where formal training is\ncritical for safe operation. Two policy recommendations are proposed. First,\ngovernments should create concise, official resources in accessible and\nofficial format to educate drivers on system capabilities and limitations.\nSecond, mandatory training and certification programs should be introduced,\ncombining theoretical and hands-on components to prepare drivers for real-world\nscenarios. These measures will improve driver understanding, reduce misuse, and\nfoster public trust in semi-automated vehicle technologies. By addressing the\nknowledge gap, policymakers can ensure a safer, more responsible transition to\nautomation, maximizing its benefits while minimizing risks to public safety.", "published": "2025-04-10 21:11:31", "link": "http://arxiv.org/abs/2504.08128v1", "categories": ["cs.HC", "cs.IT", "math.IT"], "primary_category": "cs.HC"}
{"title": "Semicontinuity bounds for the von Neumann entropy and partial majorization", "abstract": "We consider families of tight upper bounds on the difference\n$S(\\rho)-S(\\sigma)$ with the rank/energy constraint imposed on the state $\\rho$\nwhich are valid provided that the state $\\rho$ partially majorizes the state\n$\\sigma$ and is close to the state $\\sigma$ w.r.t. the trace norm.\n  The upper bounds within these families depend on the parameter $m$ of partial\nmajorization. The upper bounds corresponding to $m=1$ coincide with the optimal\nsemicontinuity bounds for the von Neumann entropy with the rank/energy\nconstraint obtained in [Lett.Math.Phys.,113,121,35] and [arXiv:2410.02686].\n  We also consider classical versions of the above results formulated in terms\nof probability distributions and the Shannon entropy.", "published": "2025-04-10 19:55:06", "link": "http://arxiv.org/abs/2504.08098v1", "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "quant-ph"}
{"title": "Efficient measurement of neutral-atom qubits with matched filters", "abstract": "Quantum computers require high-fidelity measurement of many qubits to achieve\na quantum advantage. Traditional approaches suffer from readout crosstalk for a\nneutral-atom quantum processor with a tightly spaced array. Although classical\nmachine learning algorithms based on convolutional neural networks can improve\nfidelity, they are computationally expensive, making it difficult to scale them\nto large qubit counts. We present two simpler and scalable machine learning\nalgorithms that realize matched filters for the readout problem. One is a local\nmodel that focuses on a single qubit, and the other uses information from\nneighboring qubits in the array to prevent crosstalk among the qubits. We\ndemonstrate error reductions of up to 32% and 43% for the site and array\nmodels, respectively, compared to a conventional Gaussian threshold approach.\nAdditionally, our array model uses two orders of magnitude fewer trainable\nparameters and four orders of magnitude fewer multiplications and nonlinear\nfunction evaluations than a recent convolutional neural network approach, with\nonly a minor (3.5%) increase in error across different readout times. Another\nstrength of our approach is its physical interpretability: the learned filter\ncan be visualized to provide insights into experimental imperfections. We also\nshow that a convolutional neural network model for improved can be pruned to\nhave 70x and 4000x fewer parameters, respectively, while maintaining similar\nerrors. Our work shows that simple machine learning approaches can achieve\nhigh-fidelity qubit measurements while remaining scalable to systems with\nlarger qubit counts.", "published": "2025-04-10 23:44:46", "link": "http://arxiv.org/abs/2504.08170v1", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "primary_category": "quant-ph"}
{"title": "External-Wrench Estimation for Aerial Robots Exploiting a Learned Model", "abstract": "This paper presents an external wrench estimator that uses a hybrid dynamics\nmodel consisting of a first-principles model and a neural network. This\nframework addresses one of the limitations of the state-of-the-art model-based\nwrench observers: the wrench estimation of these observers comprises the\nexternal wrench (e.g. collision, physical interaction, wind); in addition to\nresidual wrench (e.g. model parameters uncertainty or unmodeled dynamics). This\nis a problem if these wrench estimations are to be used as wrench feedback to a\nforce controller, for example. In the proposed framework, a neural network is\ncombined with a first-principles model to estimate the residual dynamics\narising from unmodeled dynamics and parameters uncertainties, then, the hybrid\ntrained model is used to estimate the external wrench, leading to a wrench\nestimation that has smaller contributions from the residual dynamics, and\naffected more by the external wrench. This method is validated with numerical\nsimulations of an aerial robot in different flying scenarios and different\ntypes of residual dynamics, and the statistical analysis of the results shows\nthat the wrench estimation error has improved significantly compared to a\nmodel-based wrench observer using only a first-principles model.", "published": "2025-04-10 22:45:44", "link": "http://arxiv.org/abs/2504.08156v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Adaptive Bounded Exploration and Intermediate Actions for Data Debiasing", "abstract": "The performance of algorithmic decision rules is largely dependent on the\nquality of training datasets available to them. Biases in these datasets can\nraise economic and ethical concerns due to the resulting algorithms' disparate\ntreatment of different groups. In this paper, we propose algorithms for\nsequentially debiasing the training dataset through adaptive and bounded\nexploration in a classification problem with costly and censored feedback. Our\nproposed algorithms balance between the ultimate goal of mitigating the impacts\nof data biases -- which will in turn lead to more accurate and fairer\ndecisions, and the exploration risks incurred to achieve this goal.\nSpecifically, we propose adaptive bounds to limit the region of exploration,\nand leverage intermediate actions which provide noisy label information at a\nlower cost. We analytically show that such exploration can help debias data in\ncertain distributions, investigate how {algorithmic fairness interventions} can\nwork in conjunction with our proposed algorithms, and validate the performance\nof these algorithms through numerical experiments on synthetic and real-world\ndata.", "published": "2025-04-10 22:22:23", "link": "http://arxiv.org/abs/2504.08151v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI", "abstract": "This study addresses the challenge of predicting post-stroke rigidity by\nemphasizing feature interactions through graph-based explainable AI.\nPost-stroke rigidity, characterized by increased muscle tone and stiffness,\nsignificantly affects survivors' mobility and quality of life. Despite its\nprevalence, early prediction remains limited, delaying intervention. We analyze\n519K stroke hospitalization records from the Healthcare Cost and Utilization\nProject dataset, where 43% of patients exhibited rigidity. We compare\ntraditional approaches such as Logistic Regression, XGBoost, and Transformer\nwith graph-based models like Graphormer and Graph Attention Network. These\ngraph models inherently capture feature interactions and incorporate intrinsic\nor post-hoc explainability. Our results show that graph-based methods\noutperform others (AUROC 0.75), identifying key predictors such as NIH Stroke\nScale and APR-DRG mortality risk scores. They also uncover interactions missed\nby conventional models. This research provides a novel application of\ngraph-based XAI in stroke prognosis, with potential to guide early\nidentification and personalized rehabilitation strategies.", "published": "2025-04-10 22:20:22", "link": "http://arxiv.org/abs/2504.08150v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Variational quantum and neural quantum states algorithms for the linear complementarity problem", "abstract": "Variational quantum algorithms (VQAs) are promising hybrid quantum-classical\nmethods designed to leverage the computational advantages of quantum computing\nwhile mitigating the limitations of current noisy intermediate-scale quantum\n(NISQ) hardware. Although VQAs have been demonstrated as proofs of concept,\ntheir practical utility in solving real-world problems -- and whether\nquantum-inspired classical algorithms can match their performance -- remains an\nopen question. We present a novel application of the variational quantum linear\nsolver (VQLS) and its classical neural quantum states-based counterpart, the\nvariational neural linear solver (VNLS), as key components within a minimum map\nNewton solver for a complementarity-based rigid body contact model. We\ndemonstrate using the VNLS that our solver accurately simulates the dynamics of\nrigid spherical bodies during collision events. These results suggest that\nquantum and quantum-inspired linear algebra algorithms can serve as viable\nalternatives to standard linear algebra solvers for modeling certain physical\nsystems.", "published": "2025-04-10 22:03:14", "link": "http://arxiv.org/abs/2504.08141v1", "categories": ["cs.CE", "cs.LG", "quant-ph"], "primary_category": "cs.CE"}
{"title": "A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation", "abstract": "In this article we develop a Physics Informed Neural Network (PINN) approach\nto simulate ice sheet dynamics governed by the Shallow Ice Approximation. This\nproblem takes the form of a time-dependent parabolic obstacle problem. Prior\nwork has used this approach to address the stationary obstacle problem and here\nwe extend it to the time dependent problem. Through comprehensive 1D and 2D\nsimulations, we validate the model's effectiveness in capturing complex\nfree-boundary conditions. By merging traditional mathematical modeling with\ncutting-edge deep learning methods, this approach provides a scalable and\nrobust solution for predicting temporal variations in ice thickness. To\nillustrate this approach in a real world setting, we simulate the dynamics of\nthe Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.", "published": "2025-04-10 21:32:03", "link": "http://arxiv.org/abs/2504.08136v1", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning", "abstract": "Dynamic graph learning is essential for applications involving temporal\nnetworks and requires effective modeling of temporal relationships. Seminal\nattention-based models like TGAT and DyGFormer rely on sinusoidal time encoders\nto capture temporal relationships between edge events. In this paper, we study\na simpler alternative: the linear time encoder, which avoids temporal\ninformation loss caused by sinusoidal functions and reduces the need for high\ndimensional time encoders. We show that the self-attention mechanism can\neffectively learn to compute time spans from linear time encodings and extract\nrelevant temporal patterns. Through extensive experiments on six dynamic graph\ndatasets, we demonstrate that the linear time encoder improves the performance\nof TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead\nto significant savings in model parameters with minimal performance loss. For\nexample, compared to a 100-dimensional sinusoidal time encoder, TGAT with a\n2-dimensional linear time encoder saves 43% of parameters and achieves higher\naverage precision on five datasets. These results can be readily used to\npositively impact the design choices of a wide variety of dynamic graph\nlearning architectures. The experimental code is available at:\nhttps://github.com/hsinghuan/dg-linear-time.git.", "published": "2025-04-10 21:12:10", "link": "http://arxiv.org/abs/2504.08129v1", "categories": ["cs.LG", "cs.SI"], "primary_category": "cs.LG"}
{"title": "RL-based Control of UAS Subject to Significant Disturbance", "abstract": "This paper proposes a Reinforcement Learning (RL)-based control framework for\nposition and attitude control of an Unmanned Aerial System (UAS) subjected to\nsignificant disturbance that can be associated with an uncertain trigger\nsignal. The proposed method learns the relationship between the trigger signal\nand disturbance force, enabling the system to anticipate and counteract the\nimpending disturbances before they occur. We train and evaluate three policies:\na baseline policy trained without exposure to the disturbance, a reactive\npolicy trained with the disturbance but without the trigger signal, and a\npredictive policy that incorporates the trigger signal as an observation and is\nexposed to the disturbance during training. Our simulation results show that\nthe predictive policy outperforms the other policies by minimizing position\ndeviations through a proactive correction maneuver. This work highlights the\npotential of integrating predictive cues into RL frameworks to improve UAS\nperformance.", "published": "2025-04-10 20:25:14", "link": "http://arxiv.org/abs/2504.08114v1", "categories": ["cs.RO", "cs.LG", "cs.SY"], "primary_category": "cs.RO"}
{"title": "Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling", "abstract": "Atomistic materials modeling is a critical task with wide-ranging\napplications, from drug discovery to materials science, where accurate\npredictions of the target material property can lead to significant\nadvancements in scientific discovery. Graph Neural Networks (GNNs) represent\nthe state-of-the-art approach for modeling atomistic material data thanks to\ntheir capacity to capture complex relational structures. While machine learning\nperformance has historically improved with larger models and datasets, GNNs for\natomistic materials modeling remain relatively small compared to large language\nmodels (LLMs), which leverage billions of parameters and terabyte-scale\ndatasets to achieve remarkable performance in their respective domains. To\naddress this gap, we explore the scaling limits of GNNs for atomistic materials\nmodeling by developing a foundational model with billions of parameters,\ntrained on extensive datasets in terabyte-scale. Our approach incorporates\ntechniques from LLM libraries to efficiently manage large-scale data and\nmodels, enabling both effective training and deployment of these large-scale\nGNN models. This work addresses three fundamental questions in scaling GNNs:\nthe potential for scaling GNN model architectures, the effect of dataset size\non model accuracy, and the applicability of LLM-inspired techniques to GNN\narchitectures. Specifically, the outcomes of this study include (1) insights\ninto the scaling laws for GNNs, highlighting the relationship between model\nsize, dataset volume, and accuracy, (2) a foundational GNN model optimized for\natomistic materials modeling, and (3) a GNN codebase enhanced with advanced\nLLM-based training techniques. Our findings lay the groundwork for large-scale\nGNNs with billions of parameters and terabyte-scale datasets, establishing a\nscalable pathway for future advancements in atomistic materials modeling.", "published": "2025-04-10 20:19:20", "link": "http://arxiv.org/abs/2504.08112v1", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "cs.LG"}
{"title": "Differentially Private Selection using Smooth Sensitivity", "abstract": "Differentially private selection mechanisms offer strong privacy guarantees\nfor queries aiming to identify the top-scoring element r from a finite set R,\nbased on a dataset-dependent utility function. While selection queries are\nfundamental in data science, few mechanisms effectively ensure their privacy.\nFurthermore, most approaches rely on global sensitivity to achieve differential\nprivacy (DP), which can introduce excessive noise and impair downstream\ninferences. To address this limitation, we propose the Smooth Noisy Max (SNM)\nmechanism, which leverages smooth sensitivity to yield provably tighter (upper\nbounds on) expected errors compared to global sensitivity-based methods.\nEmpirical results demonstrate that SNM is more accurate than state-of-the-art\ndifferentially private selection methods in three applications: percentile\nselection, greedy decision trees, and random forests.", "published": "2025-04-10 19:31:34", "link": "http://arxiv.org/abs/2504.08086v1", "categories": ["cs.LG", "cs.CR", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Programs as Singularities", "abstract": "We develop a correspondence between the structure of Turing machines and the\nstructure of singularities of real analytic functions, based on connecting the\nEhrhard-Regnier derivative from linear logic with the role of geometry in\nWatanabe's singular learning theory. The correspondence works by embedding\nordinary (discrete) Turing machine codes into a family of noisy codes which\nform a smooth parameter space. On this parameter space we consider a potential\nfunction which has Turing machines as critical points. By relating the Taylor\nseries expansion of this potential at such a critical point to combinatorics of\nerror syndromes, we relate the local geometry to internal structure of the\nTuring machine.\n  The potential in question is the negative log-likelihood for a statistical\nmodel, so that the structure of the Turing machine and its associated\nsingularity is further related to Bayesian inference. Two algorithms that\nproduce the same predictive function can nonetheless correspond to\nsingularities with different geometries, which implies that the Bayesian\nposterior can discriminate between distinct algorithmic implementations,\ncontrary to a purely functional view of inference. In the context of singular\nlearning theory our results point to a more nuanced understanding of Occam's\nrazor and the meaning of simplicity in inductive inference.", "published": "2025-04-10 19:04:31", "link": "http://arxiv.org/abs/2504.08075v1", "categories": ["cs.LO", "cs.LG", "math.LO"], "primary_category": "cs.LO"}
{"title": "Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit Schemes", "abstract": "Tradable credit schemes (TCS) are an increasingly studied alternative to\ncongestion pricing, given their revenue neutrality and ability to address\nissues of equity through the initial credit allocation. Modeling TCS to aid\nfuture design and implementation is associated with challenges involving user\nand market behaviors, demand-supply dynamics, and control mechanisms. In this\npaper, we focus on the latter and address the day-to-day dynamic tolling\nproblem under TCS, which is formulated as a discrete-time Markov Decision\nProcess and solved using reinforcement learning (RL) algorithms. Our results\nindicate that RL algorithms achieve travel times and social welfare comparable\nto the Bayesian optimization benchmark, with generalization across varying\ncapacities and demand levels. We further assess the robustness of RL under\ndifferent hyperparameters and apply regularization techniques to mitigate\naction oscillation, which generates practical tolling strategies that are\ntransferable under day-to-day demand and supply variability. Finally, we\ndiscuss potential challenges such as scaling to large networks, and show how\ntransfer learning can be leveraged to improve computational efficiency and\nfacilitate the practical deployment of RL-based TCS solutions.", "published": "2025-04-10 19:04:28", "link": "http://arxiv.org/abs/2504.08074v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "I.2.6; I.2.8"], "primary_category": "cs.LG"}
{"title": "A GPU-accelerated simulation of rapid intensification of a tropical cyclone with observed heating", "abstract": "This paper presents a limited-area atmospheric simulation of a tropical\ncyclone accelerated using GPUs. The OpenACC directive-based programming model\nis used to port the atmospheric model to the GPU. The GPU implementation of the\nmain functions and kernels is discussed. The GPU-accelerated code produces\nhigh-fidelity simulations of a realistic tropical cyclone forced by\nobservational latent heating. Performance tests show that the GPU-accelerated\ncode yields energy-efficient simulations and scales well in both the strong and\nweak limit.", "published": "2025-04-10 22:51:11", "link": "http://arxiv.org/abs/2504.08157v1", "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "physics.comp-ph"}
{"title": "Optimal Investment in Equity and Credit Default Swaps in the Presence of Default", "abstract": "We consider an equity market subject to risk from both unhedgeable shocks and\ndefault. The novelty of our work is that to partially offset default risk,\ninvestors may dynamically trade in a credit default swap (CDS) market. Assuming\ninvestment opportunities are driven by functions of an underlying diffusive\nfactor process, we identify the certainty equivalent for a constant absolute\nrisk aversion investor with a semi-linear partial differential equation (PDE)\nwhich has quadratic growth in both the function and gradient coefficients. For\ngeneral model specifications, we prove existence of a solution to the PDE which\nis also the certainty equivalent. We show the optimal policy in the CDS market\ncovers not only equity losses upon default (as one would expect), but also\nlosses due to restricted future trading opportunities. We use our results to\nprice default dependent claims though the principal of utility indifference,\nand we show that provided the underlying equity market is complete absent the\npossibility of default, the equity-CDS market is complete accounting for\ndefault. Lastly, through a numerical application, we show the optimal CDS\npolicies are essentially static (and hence easily implementable) and that\ninvesting in CDS dramatically increases investor indirect utility.", "published": "2025-04-10 19:30:29", "link": "http://arxiv.org/abs/2504.08085v1", "categories": ["q-fin.MF", "q-fin.PM", "q-fin.RM", "91G10 (primary) 91G20, 92G40 (secondary)"], "primary_category": "q-fin.MF"}
{"title": "Communication-Efficient Cooperative Localization: A Graph Neural Network Approach", "abstract": "Cooperative localization leverages noisy inter-node distance measurements and\nexchanged wireless messages to estimate node positions in a wireless network.\nIn communication-constrained environments, however, transmitting large messages\nbecomes problematic. In this paper, we propose an approach for\ncommunication-efficient cooperative localization that addresses two main\nchallenges. First, cooperative localization often needs to be performed over\nwireless networks with loopy graph topologies. Second is the need for designing\nan algorithm that has low localization error while simultaneously requiring a\nmuch lower communication overhead. Existing methods fall short of addressing\nthese two challenges concurrently. To achieve this, we propose a vector\nquantized message passing neural network (VQ-MPNN) for cooperative\nlocalization. Through end-to-end neural network training, VQ-MPNN enables the\nco-design of node localization and message compression. Specifically, VQ-MPNN\ntreats prior node positions and distance measurements as node and edge\nfeatures, respectively, which are encoded as node and edge states using a graph\nneural network. To find an efficient representation for the node state, we\nconstruct a vector quantized codebook for all node states such that instead of\nsending long messages, each node only needs to transmit a codeword index.\nNumerical evaluations demonstrates that our proposed VQ-MPNN approach can\ndeliver localization errors that are similar to existing approaches while\nreducing the overall communication overhead by an order of magnitude.", "published": "2025-04-10 21:26:41", "link": "http://arxiv.org/abs/2504.08135v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Wavelet-Based CSI Reconstruction for Improved Wireless Security Through Channel Reciprocity", "abstract": "The reciprocity of channel state information (CSI) collected by two devices\ncommunicating over a wireless channel has been leveraged to provide security\nsolutions to resource-limited IoT devices. Despite the extensive research that\nhas been done on this topic, much of the focus has been on theoretical and\nsimulation analysis. However, these security solutions face key implementation\nchallenges, mostly pertaining to limitations of IoT hardware and variations of\nchannel conditions, limiting their practical adoption. To address this research\ngap, we revisit the channel reciprocity assumption from an experimental\nstandpoint using resource-constrained devices. Our experimental study reveals a\nsignificant degradation in channel reciprocity for low-cost devices due to the\nvarying channel conditions. Through experimental investigations, we first\nidentify key practical causes for the degraded channel reciprocity. We then\npropose a new wavelet-based CSI reconstruction technique using wavelet\ncoherence and time-lagged cross-correlation to construct CSI data that are\nconsistent between the two participating devices, resulting in significant\nimprovement in channel reciprocity. Additionally, we propose a secret-key\ngeneration scheme that exploits the wavelet-based CSI reconstruction, yielding\nsignificant increase in the key generation rates. Finally, we propose a\ntechnique that exploits CSI temporal variations to enhance device\nauthentication resiliency through effective detection of replay attacks.", "published": "2025-04-10 19:13:31", "link": "http://arxiv.org/abs/2504.08078v1", "categories": ["eess.SP", "cs.NI", "C.2.0; C.2.1"], "primary_category": "eess.SP"}
{"title": "A Construction of Pairwise Co-prime Integer Matrices of Any Dimension and Their Least Common Right Multiple", "abstract": "Compared with co-prime integers, co-prime integer matrices are more\nchallenging due to the non-commutativity. In this paper, we present a new\nfamily of pairwise co-prime integer matrices of any dimension and large size.\nThese matrices are non-commutative and have low spread, i.e., their ratios of\npeak absolute values to mean absolute values (or the smallest non-zero absolute\nvalues) of their components are low. When matrix dimension is larger than $2$,\nthis family of matrices differs from the existing families, such as circulant,\nToeplitz matrices, or triangular matrices, and therefore, offers more varieties\nin applications. In this paper, we first prove the pairwise coprimality of the\nconstructed matrices, then determine their determinant absolute values, and\ntheir least common right multiple (lcrm) with a closed and simple form. We also\nanalyze their sampling rates when these matrices are used as sampling matrices\nfor a multi-dimensional signal. The proposed family of pairwise co-prime\ninteger matrices may have applications in multi-dimensional Chinese remainder\ntheorem (MD-CRT) that can be used to determine integer vectors from their\ninteger vector remainders modulo a set of integer matrix moduli, and also in\nmulti-dimensional sparse sensing and multirate systems.", "published": "2025-04-10 18:01:33", "link": "http://arxiv.org/abs/2504.08043v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DL-AMC: Deep Learning for Automatic Modulation Classification", "abstract": "Automatic Modulation Classification (AMC) is a signal processing technique\nwidely used at the physical layer of wireless systems to enhance spectrum\nutilization efficiency. In this work, we propose a fast and accurate AMC\nsystem, termed DL-AMC, which leverages deep learning techniques. Specifically,\nDL-AMC is built using convolutional neural network (CNN) architectures,\nincluding ResNet-18, ResNet-50, and MobileNetv2. To evaluate its performance,\nwe curated a comprehensive dataset containing various modulation schemes. Each\nmodulation type was transformed into an eye diagram, with signal-to-noise ratio\n(SNR) values ranging from -20 dB to 30 dB. We trained the CNN models on this\ndataset to enable them to learn the discriminative features of each modulation\nclass effectively. Experimental results show that the proposed DL-AMC models\nachieve high classification accuracy, especially in low SNR conditions. These\nresults highlight the robustness and efficacy of DL-AMC in accurately\nclassifying modulations in challenging wireless environments", "published": "2025-04-10 06:18:31", "link": "http://arxiv.org/abs/2504.08011v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Estimation of Solar Spectral Irradiance Using Meteorological Data and Analysis of Optimal Conditions for Solar Power Generation", "abstract": "This study proposes an approximate model to estimate the solar radiation\nspectrum intensity in Seoul, Republic of Korea, for the year 2024, aiming to\nanalyze optimal conditions related to energy generation. Since the solar\nradiation spectrum varies with atmospheric conditions, accurately predicting it\ntypically requires complex spectral radiation models. However, such models\nentail high computational costs, hindering real-time application. To address\nthis, this study introduces a simplified approximation model using only direct\nnormal irradiance (DNI) among real-time meteorological elements, employing\nlinear scaling of the standard spectrum (ASTM G-173). This model first\nestimates DNI using global horizontal irradiance (GHI) and solar position\ninformation (such as zenith angle), then linearly adjusts the standard spectrum\nto compute real-time spectrum intensity. The model approximates realistic DNI\nvalues by correcting various meteorological parameters, including zenith angle,\ncloud cover, and visibility. The analysis shows that GHI exhibits stable\nseasonal patterns, peaking in summer and minimizing in winter. In contrast, DNI\ndemonstrates significant temporal variability and frequent abnormal peaks\n(e.g., exceeding 9,000 W/m^2), highlighting the importance of data refinement\nand anomaly detection in predicting energy generation. In conclusion, GHI is\nsuitable for general photovoltaic analyses, whereas DNI is crucial for\ndirect-beam sensitive systems like concentrated solar power (CSP), requiring\nmeticulous data quality management. Future research should focus on identifying\nthe causes of DNI anomalies and developing real-time quality control\nalgorithms.", "published": "2025-04-10 01:21:43", "link": "http://arxiv.org/abs/2504.08008v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "abstract": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1", "published": "2025-04-10 10:05:15", "link": "http://arxiv.org/abs/2504.07615v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks", "abstract": "Motivated by the growing demand for low-precision arithmetic in computational\nscience, we exploit lower-precision emulation in Python -- widely regarded as\nthe dominant programming language for numerical analysis and machine learning.\nLow-precision training has revolutionized deep learning by enabling more\nefficient computation and reduced memory and energy consumption while\nmaintaining model fidelity. To better enable numerical experimentation with and\nexploration of low precision computation, we developed the Pychop library,\nwhich supports customizable floating-point formats and a comprehensive set of\nrounding modes in Python, allowing users to benefit from fast, low-precision\nemulation in numerous applications. Pychop also introduces interfaces for both\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\nnetwork training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design,\nimplementation, validation, and practical application of Pychop, establishing\nit as a foundational tool for advancing efficient mixed-precision algorithms.\nFurthermore, we present empirical results on low-precision emulation for image\nclassification and object detection using published datasets, illustrating the\nsensitivity of the use of low precision and offering valuable insights into its\nimpact. Pychop enables in-depth investigations into the effects of numerical\nprecision, facilitates the development of novel hardware accelerators, and\nintegrates seamlessly into existing deep learning workflows. Software and\nexperimental code are publicly available at\nhttps://github.com/inEXASCALE/pychop.", "published": "2025-04-10 15:12:29", "link": "http://arxiv.org/abs/2504.07835v2", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients", "abstract": "This work focuses on numerically solving a shape identification problem\nrelated to advection-diffusion processes with space-dependent coefficients\nusing shape optimization techniques. Two boundary-type cost functionals are\nconsidered, and their corresponding variations with respect to shapes are\nderived using the adjoint method, employing the chain rule approach. This\ninvolves firstly utilizing the material derivative of the state system and\nsecondly using its shape derivative. Subsequently, an alternating direction\nmethod of multipliers (ADMM) combined with the Sobolev-gradient-descent\nalgorithm is applied to stably solve the shape reconstruction problem.\nNumerical experiments in two and three dimensions are conducted to demonstrate\nthe feasibility of the methods.", "published": "2025-04-10 14:36:56", "link": "http://arxiv.org/abs/2504.07796v2", "categories": ["math.OC", "cs.NA", "math.NA", "49Q10, 35R25, 35R30, 49Q12"], "primary_category": "math.OC"}
{"title": "Filtering through a topological lens: homology for point processes on the time-frequency plane", "abstract": "We introduce a very general approach to the analysis of signals from their\nnoisy measurements from the perspective of Topological Data Analysis (TDA).\nWhile TDA has emerged as a powerful analytical tool for data with pronounced\ntopological structures, here we demonstrate its applicability for general\nproblems of signal processing, without any a-priori geometric feature. Our\nmethods are well-suited to a wide array of time-dependent signals in different\nscientific domains, with acoustic signals being a particularly important\napplication. We invoke time-frequency representations of such signals, focusing\non their zeros which are gaining salience as a signal processing tool in view\nof their stability properties. Leveraging state-of-the-art topological\nconcepts, such as stable and minimal volumes, we develop a complete suite of\nTDA-based methods to explore the delicate stochastic geometry of these zeros,\ncapturing signals based on the disruption they cause to this rigid,\nhyperuniform spatial structure. Unlike classical spatial data tools, TDA is\nable to capture the full spectrum of the stochastic geometry of the zeros,\nthereby leading to powerful inferential outcomes that are underpinned by a\nprincipled statistical foundation. This is reflected in the power and\nversatility of our applications, which include competitive performance in\nprocessing. a wide variety of audio signals (esp. in low SNR regimes),\neffective detection and reconstruction of gravitational wave signals (a reputed\nsignal processing challenge with non-Gaussian noise), and medical time series\ndata from EEGs, indicating a wide horizon for the approach and methods\nintroduced in this paper.", "published": "2025-04-10 13:10:04", "link": "http://arxiv.org/abs/2504.07720v2", "categories": ["eess.SP", "math.AT"], "primary_category": "eess.SP"}
{"title": "Integrated Sensing and Communications for Pinching-Antenna Systems (PASS)", "abstract": "An integrated sensing and communication (ISAC) design for pinching antenna\nsystems (PASS) is proposed, where the pinching antennas are deployed for\nestablishing reliable line-of-sight communication and sensing links. More\nparticularly, a separated ISAC design is proposed for the two-waveguide PASS,\nwhere one waveguide is used to emit the joint communication and sensing signals\nwhile the other waveguide is used to receive the reflected echo signals. Based\non this framework, a penalty-based alternating optimization algorithm is\nproposed to maximize the illumination power as well as ensure the communication\nquality-of-service requirement. Numerical results demonstrate that 1) the\nproposed PASS-ISAC scheme outperforms the other baseline schemes, and 2) the\nconsidered equal power allocation model achieves an upper bound performance.", "published": "2025-04-10 12:58:46", "link": "http://arxiv.org/abs/2504.07709v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories", "abstract": "Longitudinal biomedical studies monitor individuals over time to capture\ndynamics in brain development, disease progression, and treatment effects.\nHowever, estimating trajectories of brain biomarkers is challenging due to\nbiological variability, inconsistencies in measurement protocols (e.g.,\ndifferences in MRI scanners), scarcity, and irregularity in longitudinal\nmeasurements. Herein, we introduce a novel personalized deep kernel regression\nframework for forecasting brain biomarkers, with application to regional\nvolumetric measurements. Our approach integrates two key components: a\npopulation model that captures brain trajectories from a large and diverse\ncohort, and a subject-specific model that captures individual trajectories. To\noptimally combine these, we propose Adaptive Shrinkage Estimation, which\neffectively balances population and subject-specific models. We assess our\nmodel's performance through predictive accuracy metrics, uncertainty\nquantification, and validation against external clinical studies. Benchmarking\nagainst state-of-the-art statistical and machine learning models -- including\nlinear mixed effects models, generalized additive models, and deep learning\nmethods -- demonstrates the superior predictive performance of our approach.\nAdditionally, we apply our method to predict trajectories of composite\nneuroimaging biomarkers, which highlights the versatility of our approach in\nmodeling the progression of longitudinal neuroimaging biomarkers. Furthermore,\nvalidation on three external neuroimaging studies confirms the robustness of\nour method across different clinical contexts. We make the code available at\nhttps://github.com/vatass/AdaptiveShrinkageDKGP.", "published": "2025-04-10 19:13:44", "link": "http://arxiv.org/abs/2504.08840v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Double Machine Learning for Causal Inference under Shared-State Interference", "abstract": "Researchers and practitioners often wish to measure treatment effects in\nsettings where units interact via markets and recommendation systems. In these\nsettings, units are affected by certain shared states, like prices, algorithmic\nrecommendations or social signals. We formalize this structure, calling it\nshared-state interference, and argue that our formulation captures many\nrelevant applied settings. Our key modeling assumption is that individuals'\npotential outcomes are independent conditional on the shared state. We then\nprove an extension of a double machine learning (DML) theorem providing\nconditions for achieving efficient inference under shared-state interference.\nWe also instantiate our general theorem in several models of interest where it\nis possible to efficiently estimate the average direct effect (ADE) or global\naverage treatment effect (GATE).", "published": "2025-04-10 16:45:53", "link": "http://arxiv.org/abs/2504.08836v1", "categories": ["stat.ML", "cs.LG", "econ.EM"], "primary_category": "stat.ML"}
{"title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "abstract": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.", "published": "2025-04-10 01:31:11", "link": "http://arxiv.org/abs/2504.10512v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Specialized text classification: an approach to classifying Open Banking transactions", "abstract": "With the introduction of the PSD2 regulation in the EU which established the\nOpen Banking framework, a new window of opportunities has opened for banks and\nfintechs to explore and enrich Bank transaction descriptions with the aim of\nbuilding a better understanding of customer behavior, while using this\nunderstanding to prevent fraud, reduce risks and offer more competitive and\ntailored services.\n  And although the usage of natural language processing models and techniques\nhas seen an incredible progress in various applications and domains over the\npast few years, custom applications based on domain-specific text corpus remain\nunaddressed especially in the banking sector.\n  In this paper, we introduce a language-based Open Banking transaction\nclassification system with a focus on the french market and french language\ntext. The system encompasses data collection, labeling, preprocessing,\nmodeling, and evaluation stages. Unlike previous studies that focus on general\nclassification approaches, this system is specifically tailored to address the\nchallenges posed by training a language model with a specialized text corpus\n(Banking data in the French context). By incorporating language-specific\ntechniques and domain knowledge, the proposed system demonstrates enhanced\nperformance and efficiency compared to generic approaches.", "published": "2025-04-10 17:14:43", "link": "http://arxiv.org/abs/2504.12319v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.IR"}
