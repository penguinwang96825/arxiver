{"title": "Matrix concentration inequalities for dependent binary random variables", "abstract": "We prove Bernstein-type matrix concentration inequalities for linear\ncombinations with matrix coefficients of binary random variables satisfying\ncertain $\\ell_\\infty$-independence assumptions, complementing recent results by\nKaufman, Kyng and Solda. For random variables with the Stochastic Covering\nProperty or Strong Rayleigh Property we prove estimates for general functions\nsatisfying certain direction aware matrix bounded difference inequalities,\ngeneralizing and strengthening earlier estimates by the first-named author and\nPolaczyk.\n  We also demonstrate a general decoupling inequality for a class of\nBanach-space valued quadratic forms in negatively associated random variables\nand combine it with the matrix Bernstein inequality to generalize results by\nTropp, Chr\\'etien and Darses, and Ruetz and Schnass, concerning the operator\nnorm of a random submatrix of a deterministic matrix, drawn by uniform sampling\nwithout replacements or rejective sampling, to submatrices given by general\nStrong Rayleigh sampling schemes.", "published": "2025-04-10 21:34:21", "link": "http://arxiv.org/abs/2504.08138v1", "categories": ["math.PR", "cs.DM", "math.FA", "60E15, 60B20"], "primary_category": "math.PR"}
{"title": "Directed Temporal Tree Realization for Periodic Public Transport: Easy and Hard Cases", "abstract": "We study the complexity of the directed periodic temporal graph realization\nproblem. This work is motivated by the design of periodic schedules in public\ntransport with constraints on the quality of service. Namely, we require that\nthe fastest path between (important) pairs of vertices is upper bounded by a\nspecified maximum duration, encoded in an upper distance matrix $D$. While\nprevious work has considered the undirected version of the problem, the\napplication in public transport schedule design requires the flexibility to\nassign different departure times to the two directions of an edge. A problem\ninstance can only be feasible if all values of the distance matrix are at least\nshortest path distances. However, the task of realizing exact fastest path\ndistances in a periodic temporal graph is often too restrictive. Therefore, we\nintroduce a minimum slack parameter $k$ that describes a lower bound on the\nmaximum allowed waiting time on each path. We concentrate on tree topologies\nand provide a full characterization of the complexity landscape with respect to\nthe period $\\Delta$ and the minimum slack parameter~$k$, showing a sharp\nthreshold between NP-complete cases and cases which are always realizable. We\nalso provide hardness results for the special case of period $\\Delta = 2$ for\ngeneral directed and undirected graphs.", "published": "2025-04-10 17:36:23", "link": "http://arxiv.org/abs/2504.07920v1", "categories": ["cs.DS", "cs.CC", "cs.DM", "68R10 (Primary), 68Q25 (Secondary)"], "primary_category": "cs.DS"}
{"title": "Multiplicative assignment with upgrades", "abstract": "We study a problem related to submodular function optimization and the exact\nmatching problem for which we show a rather peculiar status: its natural\nLP-relaxation can have fractional optimal vertices, but there is always also an\noptimal integral vertex, which we can also compute in polynomial time.\n  More specifically, we consider the multiplicative assignment problem with\nupgrades in which we are given a set of customers and suppliers and we seek to\nassign each customer to a different supplier. Each customer has a demand and\neach supplier has a regular and an upgraded cost for each unit demand provided\nto the respective assigned client. Our goal is to upgrade at most $k$ suppliers\nand to compute an assignment in order to minimize the total resulting cost.\nThis can be cast as the problem to compute an optimal matching in a bipartite\ngraph with the additional constraint that we must select $k$ edges from a\ncertain group of edges, similar to selecting $k$ red edges in the exact\nmatching problem. Also, selecting the suppliers to be upgraded corresponds to\nmaximizing a submodular set function under a cardinality constraint.\n  Our result yields an efficient LP-based algorithm to solve our problem\noptimally. In addition, we provide also a purely strongly polynomial-time\nalgorithm for it. As an application, we obtain exact algorithms for the\nupgrading variant of the problem to schedule jobs on identical or uniformly\nrelated machines in order to minimize their sum of completion times, i.e.,\nwhere we may upgrade up to $k$ jobs to reduce their respective processing\ntimes.", "published": "2025-04-10 11:25:29", "link": "http://arxiv.org/abs/2504.07663v1", "categories": ["cs.DS", "cs.DM", "math.OC"], "primary_category": "cs.DS"}
{"title": "Computing gradient vector fields with Morse sequences", "abstract": "We rely on the framework of Morse sequences to enable the direct computation\nof gradient vector fields on simplicial complexes. A Morse sequence is a\nfiltration from a subcomplex L to a complex K via elementary expansions and\nfillings, naturally encoding critical and regular simplexes. Maximal increasing\nand minimal decreasing schemes allow constructing these sequences, and are\nlinked to algorithms like Random Discrete Morse and Coreduction. Extending the\napproach to cosimplicial complexes (S = K \\ L), we define operations --\nreductions, perforations, coreductions, and coperforations -- for efficient\ncomputation. We further generalize to F -sequences, which are Morse sequences\nweighted by an arbitrary stack function F , and provide algorithms to compute\nmaximal and minimal sequences. A particular case is when the stack function is\ngiven through a vertex map, as it is common in topological data analysis. We\nshow that we retrieve existing methods when the vertex map is injective; in\nthis case, the complex partitions into lower stars, facilitating parallel\nprocessing. Thus, this paper proposes simple, flexible, and computationally\nefficient approaches to obtain Morse sequences from arbitrary stack functions,\nallowing to generalize previous approaches dedicated to computing gradient\nvector fields from injective vertex maps.", "published": "2025-04-10 07:48:31", "link": "http://arxiv.org/abs/2504.07526v1", "categories": ["cs.DM", "math.AT"], "primary_category": "cs.DM"}
{"title": "Specialized text classification: an approach to classifying Open Banking transactions", "abstract": "With the introduction of the PSD2 regulation in the EU which established the\nOpen Banking framework, a new window of opportunities has opened for banks and\nfintechs to explore and enrich Bank transaction descriptions with the aim of\nbuilding a better understanding of customer behavior, while using this\nunderstanding to prevent fraud, reduce risks and offer more competitive and\ntailored services.\n  And although the usage of natural language processing models and techniques\nhas seen an incredible progress in various applications and domains over the\npast few years, custom applications based on domain-specific text corpus remain\nunaddressed especially in the banking sector.\n  In this paper, we introduce a language-based Open Banking transaction\nclassification system with a focus on the french market and french language\ntext. The system encompasses data collection, labeling, preprocessing,\nmodeling, and evaluation stages. Unlike previous studies that focus on general\nclassification approaches, this system is specifically tailored to address the\nchallenges posed by training a language model with a specialized text corpus\n(Banking data in the French context). By incorporating language-specific\ntechniques and domain knowledge, the proposed system demonstrates enhanced\nperformance and efficiency compared to generic approaches.", "published": "2025-04-10 17:14:43", "link": "http://arxiv.org/abs/2504.12319v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.IR"}
{"title": "How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective", "abstract": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks.", "published": "2025-04-10 16:14:55", "link": "http://arxiv.org/abs/2504.07898v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Siren Federate: Bridging document, relational, and graph models for exploratory graph analysis", "abstract": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.", "published": "2025-04-10 14:52:03", "link": "http://arxiv.org/abs/2504.07815v1", "categories": ["cs.IR", "D.2.11; E.1; H.2.4; H.3.3; H.3.4"], "primary_category": "cs.IR"}
{"title": "FairEval: Evaluating Fairness in LLM-Based Recommendations with Personality Awareness", "abstract": "Recent advances in Large Language Models (LLMs) have enabled their\napplication to recommender systems (RecLLMs), yet concerns remain regarding\nfairness across demographic and psychological user dimensions. We introduce\nFairEval, a novel evaluation framework to systematically assess fairness in\nLLM-based recommendations. FairEval integrates personality traits with eight\nsensitive demographic attributes,including gender, race, and age, enabling a\ncomprehensive assessment of user-level bias. We evaluate models, including\nChatGPT 4o and Gemini 1.5 Flash, on music and movie recommendations. FairEval's\nfairness metric, PAFS, achieves scores up to 0.9969 for ChatGPT 4o and 0.9997\nfor Gemini 1.5 Flash, with disparities reaching 34.79 percent. These results\nhighlight the importance of robustness in prompt sensitivity and support more\ninclusive recommendation systems.", "published": "2025-04-10 14:38:15", "link": "http://arxiv.org/abs/2504.07801v1", "categories": ["cs.IR", "cs.AI", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation", "abstract": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework.", "published": "2025-04-10 14:32:32", "link": "http://arxiv.org/abs/2504.07794v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections", "abstract": "In this paper, we introduce CollEx, an innovative multimodal agentic\nRetrieval-Augmented Generation (RAG) system designed to enhance interactive\nexploration of extensive scientific collections. Given the overwhelming volume\nand inherent complexity of scientific collections, conventional search systems\noften lack necessary intuitiveness and interactivity, presenting substantial\nbarriers for learners, educators, and researchers. CollEx addresses these\nlimitations by employing state-of-the-art Large Vision-Language Models (LVLMs)\nas multimodal agents accessible through an intuitive chat interface. By\nabstracting complex interactions via specialized agents equipped with advanced\ntools, CollEx facilitates curiosity-driven exploration, significantly\nsimplifying access to diverse scientific collections and records therein. Our\nsystem integrates textual and visual modalities, supporting educational\nscenarios that are helpful for teachers, pupils, students, and researchers by\nfostering independent exploration as well as scientific excitement and\ncuriosity. Furthermore, CollEx serves the research community by discovering\ninterdisciplinary connections and complementing visual data. We illustrate the\neffectiveness of our system through a proof-of-concept application containing\nover 64,000 unique records across 32 collections from a local scientific\ncollection from a public university.", "published": "2025-04-10 10:44:19", "link": "http://arxiv.org/abs/2504.07643v1", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models", "abstract": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens.", "published": "2025-04-10 10:17:08", "link": "http://arxiv.org/abs/2504.07624v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "REANIMATOR: Reanimate Retrieval Test Collections with Extracted and Synthetic Resources", "abstract": "Retrieval test collections are essential for evaluating information retrieval\nsystems, yet they often lack generalizability across tasks. To overcome this\nlimitation, we introduce REANIMATOR, a versatile framework designed to enable\nthe repurposing of existing test collections by enriching them with extracted\nand synthetic resources. REANIMATOR enhances test collections from PDF files by\nparsing full texts and machine-readable tables, as well as related contextual\ninformation. It then employs state-of-the-art large language models to produce\nsynthetic relevance labels. Including an optional human-in-the-loop step can\nhelp validate the resources that have been extracted and generated. We\ndemonstrate its potential with a revitalized version of the TREC-COVID test\ncollection, showcasing the development of a retrieval-augmented generation\nsystem and evaluating the impact of tables on retrieval-augmented generation.\nREANIMATOR enables the reuse of test collections for new applications, lowering\ncosts and broadening the utility of legacy resources.", "published": "2025-04-10 09:25:11", "link": "http://arxiv.org/abs/2504.07584v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Explicit Uncertainty Modeling for Video Watch Time Prediction", "abstract": "In video recommendation, a critical component that determines the system's\nrecommendation accuracy is the watch-time prediction module, since how long a\nuser watches a video directly reflects personalized preferences. One of the key\nchallenges of this problem is the user's stochastic watch-time behavior. To\nimprove the prediction accuracy for such an uncertain behavior, existing\napproaches show that one can either reduce the noise through duration bias\nmodeling or formulate a distribution modeling task to capture the uncertainty.\nHowever, the uncontrolled uncertainty is not always equally distributed across\nusers and videos, inducing a balancing paradox between the model accuracy and\nthe ability to capture out-of-distribution samples. In practice, we find that\nthe uncertainty of the watch-time prediction model also provides key\ninformation about user behavior, which, in turn, could benefit the prediction\ntask itself. Following this notion, we derive an explicit uncertainty modeling\nstrategy for the prediction model and propose an adversarial optimization\nframework that can better exploit the user watch-time behavior. This framework\nhas been deployed online on an industrial video sharing platform that serves\nhundreds of millions of daily active users, which obtains a significant\nincrease in users' video watch time by 0.31% through the online A/B test.\nFurthermore, extended offline experiments on two public datasets verify the\neffectiveness of the proposed framework across various watch-time prediction\nbackbones.", "published": "2025-04-10 09:19:19", "link": "http://arxiv.org/abs/2504.07575v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Exploring Human-Like Thinking in Search Simulations with Large Language Models", "abstract": "Simulating user search behavior is a critical task in information retrieval,\nwhich can be employed for user behavior modeling, data augmentation, and system\nevaluation. Recent advancements in large language models (LLMs) have opened up\nnew possibilities for generating human-like actions including querying,\nbrowsing, and clicking. In this work, we explore the integration of human-like\nthinking into search simulations by leveraging LLMs to simulate users' hidden\ncognitive processes. Specifically, given a search task and context, we prompt\nLLMs to first think like a human before executing the corresponding action. As\nexisting search datasets do not include users' thought processes, we conducted\na user study to collect a new dataset enriched with users' explicit thinking.\nWe investigate the impact of incorporating such human-like thinking on\nsimulation performance and apply supervised fine-tuning (SFT) to teach LLMs to\nemulate both human thinking and actions. Our experiments span two dimensions in\nleveraging LLMs for user simulation: (1) with or without explicit thinking, and\n(2) with or without fine-tuning on the thinking-augmented dataset. The results\ndemonstrate the feasibility and potential of incorporating human-like thinking\nin user simulations, though performance improvements on some metrics remain\nmodest. We believe this exploration provides new avenues and inspirations for\nadvancing user behavior modeling in search simulations.", "published": "2025-04-10 09:04:58", "link": "http://arxiv.org/abs/2504.07570v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs", "abstract": "We benchmark foundation models image embeddings for classification and\nretrieval in e-Commerce, evaluating their suitability for real-world\napplications. Our study spans embeddings from pre-trained convolutional and\ntransformer models trained via supervised, self-supervised, and text-image\ncontrastive learning. We assess full fine-tuning and transfer learning\n(top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars,\nfood, and retail. Results show full fine-tuning consistently performs well,\nwhile text-image and self-supervised embeddings can match its performance with\nless training. While supervised embeddings remain stable across architectures,\nSSL and contrastive embeddings vary significantly, often benefiting from\ntop-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning,\nreducing computational costs. We also explore cross-tuning, noting its impact\ndepends on dataset characteristics. Our findings offer practical guidelines for\nembedding selection and fine-tuning strategies, balancing efficiency and\nperformance.", "published": "2025-04-10 08:57:28", "link": "http://arxiv.org/abs/2504.07567v1", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking", "abstract": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking.", "published": "2025-04-10 04:08:38", "link": "http://arxiv.org/abs/2504.07439v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Emergency Communication: OTFS-Based Semantic Transmission with Diffusion Noise Suppression", "abstract": "Due to their flexibility and dynamic coverage capabilities, Unmanned Aerial\nVehicles (UAVs) have emerged as vital platforms for emergency communication in\ndisaster-stricken areas. However, the complex channel conditions in high-speed\nmobile scenarios significantly impact the reliability and efficiency of\ntraditional communication systems. This paper presents an intelligent emergency\ncommunication framework that integrates Orthogonal Time Frequency Space (OTFS)\nmodulation, semantic communication, and a diffusion-based denoising module to\naddress these challenges. OTFS ensures robust communication under dynamic\nchannel conditions due to its superior anti-fading characteristics and\nadaptability to rapidly changing environments. Semantic communication further\nenhances transmission efficiency by focusing on key information extraction and\nreducing data redundancy. Moreover, a diffusion-based channel denoising module\nis proposed to leverage the gradual noise reduction process and statistical\nnoise modeling, optimizing the accuracy of semantic information recovery.\nExperimental results demonstrate that the proposed solution significantly\nimproves link stability and transmission performance in high-mobility UAV\nscenarios, achieving at least a 3dB SNR gain over existing methods.", "published": "2025-04-10 03:25:56", "link": "http://arxiv.org/abs/2504.07420v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A Novel Mamba-based Sequential Recommendation Method", "abstract": "Sequential recommendation (SR), which encodes user activity to predict the\nnext action, has emerged as a widely adopted strategy in developing commercial\npersonalized recommendation systems. Although Transformer-based models have\nproven effective for sequential recommendation, the complexity of the\nself-attention module in Transformers scales quadratically with the sequence\nlength. Controlling model complexity is essential for large-scale\nrecommendation systems, as these systems may need to handle billion-scale\nvocabularies that evolve continuously, as well as user behavior sequences that\ncan exceed tens of thousands in length. In this paper, we propose a novel\nmulti-head latent Mamba architecture, which employs multiple low-dimensional\nMamba layers and fully connected layers coupled with positional encoding to\nsimultaneously capture historical and item information within each latent\nsubspace. Our proposed method not only enables scaling up to large-scale\nparameters but also extends to multi-domain recommendation by integrating and\nfine-tuning LLMs. Through extensive experiments on public datasets, we\ndemonstrate how Hydra effectively addresses the effectiveness-efficiency\ndilemma, outperforming state-of-the-art sequential recommendation baselines\nwith significantly fewer parameters and reduced training time.", "published": "2025-04-10 02:43:19", "link": "http://arxiv.org/abs/2504.07398v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "abstract": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.", "published": "2025-04-10 01:31:11", "link": "http://arxiv.org/abs/2504.10512v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Distribution Matching between Collaborative and Language Spaces for Generative Recommendation", "abstract": "Generative recommendation aims to learn the underlying generative process\nover the entire item set to produce recommendations for users. Although it\nleverages non-linear probabilistic models to surpass the limited modeling\ncapacity of linear factor models, it is often constrained by a trade-off\nbetween representation ability and tractability. With the rise of a new\ngeneration of generative methods based on pre-trained language models (LMs),\nincorporating LMs into general recommendation with implicit feedback has gained\nconsiderable attention. However, adapting them to generative recommendation\nremains challenging. The core reason lies in the mismatch between the\ninput-output formats and semantics of generative models and LMs, making it\nchallenging to achieve optimal alignment in the feature space. This work\naddresses this issue by proposing a model-agnostic generative recommendation\nframework called DMRec, which introduces a probabilistic meta-network to bridge\nthe outputs of LMs with user interactions, thereby enabling an equivalent\nprobabilistic modeling process. Subsequently, we design three cross-space\ndistribution matching processes aimed at maximizing shared information while\npreserving the unique semantics of each space and filtering out irrelevant\ninformation. We apply DMRec to three different types of generative\nrecommendation methods and conduct extensive experiments on three public\ndatasets. The experimental results demonstrate that DMRec can effectively\nenhance the recommendation performance of these generative models, and it shows\nsignificant advantages over mainstream LM-enhanced recommendation methods.", "published": "2025-04-10 01:09:30", "link": "http://arxiv.org/abs/2504.07363v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Certified to Drive: A Policy Proposal for Mandatory Training on Semi-Automated Vehicles", "abstract": "Although the Boeing 737 Max incidents resulted from a mix of design\nshortcomings, regulatory oversights, and systemic issues, they also highlight a\ncritical gap in pilot training on managing automated systems during abnormal\nconditions. This example demonstrates the urgent need for focused, concise\ntraining on human-automation interaction - a need that is equally critical for\noperators of Level 2 ADAS-equipped vehicles, as discussed in detail later in\nthis article. The lack of structured education for semi-automated vehicle\noperators mirrors similar risks in other industries, where formal training is\ncritical for safe operation. Two policy recommendations are proposed. First,\ngovernments should create concise, official resources in accessible and\nofficial format to educate drivers on system capabilities and limitations.\nSecond, mandatory training and certification programs should be introduced,\ncombining theoretical and hands-on components to prepare drivers for real-world\nscenarios. These measures will improve driver understanding, reduce misuse, and\nfoster public trust in semi-automated vehicle technologies. By addressing the\nknowledge gap, policymakers can ensure a safer, more responsible transition to\nautomation, maximizing its benefits while minimizing risks to public safety.", "published": "2025-04-10 21:11:31", "link": "http://arxiv.org/abs/2504.08128v1", "categories": ["cs.HC", "cs.IT", "math.IT"], "primary_category": "cs.HC"}
{"title": "Semicontinuity bounds for the von Neumann entropy and partial majorization", "abstract": "We consider families of tight upper bounds on the difference\n$S(\\rho)-S(\\sigma)$ with the rank/energy constraint imposed on the state $\\rho$\nwhich are valid provided that the state $\\rho$ partially majorizes the state\n$\\sigma$ and is close to the state $\\sigma$ w.r.t. the trace norm.\n  The upper bounds within these families depend on the parameter $m$ of partial\nmajorization. The upper bounds corresponding to $m=1$ coincide with the optimal\nsemicontinuity bounds for the von Neumann entropy with the rank/energy\nconstraint obtained in [Lett.Math.Phys.,113,121,35] and [arXiv:2410.02686].\n  We also consider classical versions of the above results formulated in terms\nof probability distributions and the Shannon entropy.", "published": "2025-04-10 19:55:06", "link": "http://arxiv.org/abs/2504.08098v1", "categories": ["quant-ph", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "quant-ph"}
{"title": "Function-Correcting Codes for $\u03c1$-locally $\u03bb$-functions", "abstract": "In this paper, we explore $\\rho$-locally $\\lambda$-functions and develop\nfunction-correcting codes for these functions. We propose an upper bound on the\nredundancy of these codes, based on the minimum possible length of an\nerror-correcting code with a given number of codewords and minimum distance.\nAdditionally, we provide a sufficient optimality condition for the\nfunction-correcting codes when $\\lambda = 4$. We also demonstrate that any\nfunction can be represented as a $\\rho$-locally $\\lambda$-function,\nillustrating this with a representation of Hamming weight distribution\nfunctions. Furthermore, we present another construction of function-correcting\ncodes for Hamming weight distribution functions.", "published": "2025-04-10 14:41:51", "link": "http://arxiv.org/abs/2504.07804v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Finite-Blocklength Information Theory", "abstract": "Traditional asymptotic information-theoretic studies of the fundamental\nlimits of wireless communication systems primarily rely on some ideal\nassumptions, such as infinite blocklength and vanishing error probability.\nWhile these assumptions enable tractable mathematical characterizations, they\nfail to capture the stringent requirements of some emerging next-generation\nwireless applications, such as ultra-reliable low latency communication and\nultra-massive machine type communication, in which it is required to support a\nmuch wider range of features including short-packet communication, extremely\nlow latency, and/or low energy consumption. To better support such\napplications, it is important to consider finite-blocklength information\ntheory. In this paper, we present a comprehensive review of the advances in\nthis field, followed by a discussion on the open questions. Specifically, we\ncommence with the fundamental limits of source coding in the non-asymptotic\nregime, with a particular focus on lossless and lossy compression in\npoint-to-point~(P2P) and multiterminal cases. Next, we discuss the fundamental\nlimits of channel coding in P2P channels, multiple access channels, and\nemerging massive access channels. We further introduce recent advances in joint\nsource and channel coding, highlighting its considerable performance advantage\nover separate source and channel coding in the non-asymptotic regime. In each\npart, we review various non-asymptotic achievability bounds, converse bounds,\nand approximations, as well as key ideas behind them, which are essential for\nproviding engineering insights into the design of future wireless communication\nsystems.", "published": "2025-04-10 13:39:56", "link": "http://arxiv.org/abs/2504.07743v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Integrated Sensing and Communications for Pinching-Antenna Systems (PASS)", "abstract": "An integrated sensing and communication (ISAC) design for pinching antenna\nsystems (PASS) is proposed, where the pinching antennas are deployed for\nestablishing reliable line-of-sight communication and sensing links. More\nparticularly, a separated ISAC design is proposed for the two-waveguide PASS,\nwhere one waveguide is used to emit the joint communication and sensing signals\nwhile the other waveguide is used to receive the reflected echo signals. Based\non this framework, a penalty-based alternating optimization algorithm is\nproposed to maximize the illumination power as well as ensure the communication\nquality-of-service requirement. Numerical results demonstrate that 1) the\nproposed PASS-ISAC scheme outperforms the other baseline schemes, and 2) the\nconsidered equal power allocation model achieves an upper bound performance.", "published": "2025-04-10 12:58:46", "link": "http://arxiv.org/abs/2504.07709v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Exploiting Beamforming for Enforcing Semantic Secrecy in 5G NR mmWave Communications", "abstract": "We experimentally investigate the performance of semantically-secure physical\nlayer security (PLS) in 5G new radio (NR) mmWave communications during the\ninitial cell search procedure in the NR band n257 at 27 GHz. A gNB transmits\nPLS-encoded messages in the presence of an eavesdropper, who intercepts the\ncommunication by non-intrusively collecting channel readings in the form of IQ\nsamples. For the message transmission, we use the physical broadcast channel\n(PBCH) within the synchronization signal block. We analyze different\nsignal-to-noise ratio (SNR) conditions by progressively reducing the transmit\npower of the subcarriers carrying the PBCH channel, while ensuring optimal\nconditions for over-the-air frequency and timing synchronization. We measure\nthe secrecy performance of the communication in terms of upper and lower bounds\nfor the distinguishing error rate (DER) metric for different SNR levels and\nbeam angles when performing beamsteering in indoor scenarios, such as office\nenvironments and laboratory settings.", "published": "2025-04-10 12:07:32", "link": "http://arxiv.org/abs/2504.07678v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Integrated Sensing, Computing, and Semantic Communication with Fluid Antenna for Metaverse", "abstract": "The integration of sensing and communication (ISAC) is pivotal for the\nMetaverse but faces challenges like high data volume and privacy concerns. This\npaper proposes a novel integrated sensing, computing, and semantic\ncommunication (ISCSC) framework, which uses semantic communication to transmit\nonly contextual information, reducing data overhead and enhancing efficiency.\nTo address the sensitivity of semantic communication to channel conditions,\nfluid antennas (FAs) are introduced, enabling dynamic adaptability. The\nFA-enabled ISCSC framework considers multiple users and extended targets\ncomposed of a series of scatterers, formulating a joint optimization problem to\nmaximize the data rate while ensuring sensing accuracy and meeting\ncomputational and power constraints. An alternating optimization (AO) method\ndecomposes the problem into subproblems for ISAC beamforming, FA positioning,\nand semantic extraction. Simulations confirm the framework's effectiveness in\nimproving data rates and sensing performance.", "published": "2025-04-10 11:13:08", "link": "http://arxiv.org/abs/2504.07656v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Rate Analysis and Optimization of LoS Beyond Diagonal RIS-assisted MIMO Systems", "abstract": "In this letter, we derive an expression for the achievable rate in a\nmultiple-input multiple-output (MIMO) system assisted by a beyond-diagonal\nreconfigurable intelligent surface (BD-RIS) when the channels to and from the\nBD-RIS are line-of-sight (LoS) while the direct link is non-line-of-sight\n(NLoS). The rate expression allows to derive the optimal unitary and symmetric\nscattering BD-RIS matrix in closed form. Our simulation results show that the\nproposed solution is competitive even under the more usual Ricean channel\nfading model when the direct link is weak.", "published": "2025-04-10 10:48:52", "link": "http://arxiv.org/abs/2504.07647v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Energy-Efficient UAV Replacement in Software-Defined UAV Networks", "abstract": "Unmanned Aerial Vehicles (UAVs) in networked environments face significant\nchallenges due to energy constraints and limited battery life, which\nnecessitate periodic replacements to maintain continuous operation. Efficiently\nmanaging the handover of data flows during these replacements is crucial to\navoid disruptions in communication and to optimize energy consumption. This\npaper addresses the complex issue of energy-efficient UAV replacement in\nsoftware-defined UAV network. We introduce a novel approach based on\nestablishing a strict total ordering relation for UAVs and data flows, allowing\nus to formulate the problem as an integer linear program. By utilizing the\nGurobi solver, we obtain optimal handover schedules for the tested problem\ninstances. Additionally, we propose a heuristic algorithm that significantly\nreduces computational complexity while maintaining near-optimal performance.\nThrough comprehensive simulations, we demonstrate that our heuristic offers\npractical and scalable solution, ensuring energy-efficient UAV replacement\nwhile minimizing network disruptions. Our results suggest that the proposed\napproach can enhance UAV battery life and improve overall network reliability\nin real-world applications.", "published": "2025-04-10 06:58:35", "link": "http://arxiv.org/abs/2504.07500v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Enabling Gigantic MIMO Beamforming with Analog Computing", "abstract": "In our previous work, we have introduced a microwave linear analog computer\n(MiLAC) as an analog computer that processes microwave signals linearly,\ndemonstrating its potential to reduce the computational complexity of specific\nsignal processing tasks. In this paper, we extend these benefits to wireless\ncommunications, showcasing how MiLAC enables gigantic multiple-input\nmultiple-output (MIMO) beamforming entirely in the analog domain. MiLAC-aided\nbeamforming can implement regularized zero-forcing beamforming (R-ZFBF) at the\ntransmitter and minimum mean square error (MMSE) detection at the receiver,\nwhile significantly reducing hardware costs by minimizing the number of\nradio-frequency (RF) chains and only relying on low-resolution\nanalog-to-digital converters (ADCs) and digital-to-analog converters (DACs). In\naddition, it eliminates per-symbol operations by completely avoiding\ndigital-domain processing and remarkably reduces the computational complexity\nof R-ZFBF, which scales quadratically with the number of antennas instead of\ncubically. Numerical results show that it can perform R-ZFBF with a\ncomputational complexity reduction of up to 7400 times compared to digital\nbeamforming.", "published": "2025-04-10 06:06:03", "link": "http://arxiv.org/abs/2504.07477v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Task-oriented Age of Information for Remote Inference with Hybrid Language Models", "abstract": "Large Language Models (LLMs) have revolutionized the field of artificial\nintelligence (AI) through their advanced reasoning capabilities, but their\nextensive parameter sets introduce significant inference latency, posing a\nchallenge to ensure the timeliness of inference results. While Small Language\nModels (SLMs) offer faster inference speeds with fewer parameters, they often\ncompromise accuracy on complex tasks. This study proposes a novel remote\ninference system comprising a user, a sensor, and an edge server that\nintegrates both model types alongside a decision maker. The system dynamically\ndetermines the resolution of images transmitted by the sensor and routes\ninference tasks to either an SLM or LLM to optimize performance. The key\nobjective is to minimize the Task-oriented Age of Information (TAoI) by jointly\nconsidering the accuracy and timeliness of the inference task. Due to the\nnon-uniform transmission time and inference time, we formulate this problem as\na Semi-Markov Decision Process (SMDP). By converting the SMDP to an equivalent\nMarkov decision process, we prove that the optimal control policy follows a\nthreshold-based structure. We further develop a relative policy iteration\nalgorithm leveraging this threshold property. Simulation results demonstrate\nthat our proposed optimal policy significantly outperforms baseline approaches\nin managing the accuracy-timeliness trade-off.", "published": "2025-04-10 03:48:09", "link": "http://arxiv.org/abs/2504.07428v1", "categories": ["cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis", "abstract": "We propose the Dual Engines of Thoughts (DEoT), an analytical framework for\ncomprehensive open-ended reasoning. While traditional reasoning frameworks\nprimarily focus on finding \"the best answer\" or \"the correct answer\" for\nsingle-answer problems, DEoT is specifically designed for \"open-ended\nquestions,\" enabling both broader and deeper analytical exploration. The\nframework centers on three key components: a Base Prompter for refining user\nqueries, a Solver Agent that orchestrates task decomposition, execution, and\nvalidation, and a Dual-Engine System consisting of a Breadth Engine (to explore\ndiverse impact factors) and a Depth Engine (to perform deep investigations).\nThis integrated design allows DEoT to balance wide-ranging coverage with\nin-depth analysis, and it is highly customizable, enabling users to adjust\nanalytical parameters and tool configurations based on specific requirements.\nExperimental results show that DEoT excels in addressing complex, multi-faceted\nquestions, achieving a total win rate of 77-86% compared to existing reasoning\nmodels, thus highlighting its effectiveness in real-world applications.", "published": "2025-04-10 15:46:03", "link": "http://arxiv.org/abs/2504.07872v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Anytime Single-Step MAPF Planning with Anytime PIBT", "abstract": "PIBT is a popular Multi-Agent Path Finding (MAPF) method at the core of many\nstate-of-the-art MAPF methods including LaCAM, CS-PIBT, and WPPL. The main\nutility of PIBT is that it is a very fast and effective single-step MAPF solver\nand can return a collision-free single-step solution for hundreds of agents in\nless than a millisecond. However, the main drawback of PIBT is that it is\nextremely greedy in respect to its priorities and thus leads to poor solution\nquality. Additionally, PIBT cannot use all the planning time that might be\navailable to it and returns the first solution it finds. We thus develop\nAnytime PIBT, which quickly finds a one-step solution identically to PIBT but\nthen continuously improves the solution in an anytime manner. We prove that\nAnytime PIBT converges to the optimal solution given sufficient time. We\nexperimentally validate that Anytime PIBT can rapidly improve single-step\nsolution quality within milliseconds and even find the optimal single-step\naction. However, we interestingly find that improving the single-step solution\nquality does not have a significant effect on full-horizon solution costs.", "published": "2025-04-10 15:21:23", "link": "http://arxiv.org/abs/2504.07841v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "What Contributes to Affective Polarization in Networked Online Environments? Evidence from an Agent-Based Model", "abstract": "Affective polarization, or, inter-party hostility, is increasingly recognized\nas a pervasive issue in democracies worldwide, posing a threat to social\ncohesion. The digital media ecosystem, now widely accessible and ever-present,\nhas often been implicated in accelerating this phenomenon. However, the precise\ncausal mechanisms responsible for driving affective polarization have been a\nsubject of extensive debate. While the concept of echo chambers, characterized\nby individuals ensconced within like-minded groups, bereft of\ncounter-attitudinal content, has long been the prevailing hypothesis,\naccumulating empirical evidence suggests a more nuanced picture. This study\naims to contribute to the ongoing debate by employing an agent-based model to\nillustrate how affective polarization is either fostered or hindered by\nindividual news consumption and dissemination patterns based on ideological\nalignment. To achieve this, we parameterize three key aspects: (1) The\naffective asymmetry of individuals' engagement with in-party versus out-party\ncontent, (2) The proportion of in-party members within one's social\nneighborhood, and (3) The degree of partisan bias among the elites within the\npopulation. Subsequently, we observe macro-level changes in affective\npolarization within the population under various conditions stipulated by these\nparameters. This approach allows us to explore the intricate dynamics of\naffective polarization within digital environments, shedding light on the\ninterplay between individual behaviors, social networks, and information\nexposure.", "published": "2025-04-10 10:00:50", "link": "http://arxiv.org/abs/2504.07610v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Achilles Heel of Distributed Multi-Agent Systems", "abstract": "Multi-agent system (MAS) has demonstrated exceptional capabilities in\naddressing complex challenges, largely due to the integration of multiple large\nlanguage models (LLMs). However, the heterogeneity of LLMs, the scalability of\nquantities of LLMs, and local computational constraints pose significant\nchallenges to hosting these models locally. To address these issues, we propose\na new framework termed Distributed Multi-Agent System (DMAS). In DMAS,\nheterogeneous third-party agents function as service providers managed remotely\nby a central MAS server and each agent offers its services through API\ninterfaces. However, the distributed nature of DMAS introduces several concerns\nabout trustworthiness. In this paper, we study the Achilles heel of distributed\nmulti-agent systems, identifying four critical trustworthiness challenges: free\nriding, susceptibility to malicious attacks, communication inefficiencies, and\nsystem instability. Extensive experiments across seven frameworks and four\ndatasets reveal significant vulnerabilities of the DMAS. These attack\nstrategies can lead to a performance degradation of up to 80% and attain a 100%\nsuccess rate in executing free riding and malicious attacks. We envision our\nwork will serve as a useful red-teaming tool for evaluating future multi-agent\nsystems and spark further research on trustworthiness challenges in distributed\nmulti-agent systems.", "published": "2025-04-10 05:16:11", "link": "http://arxiv.org/abs/2504.07461v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "A GPU-accelerated simulation of rapid intensification of a tropical cyclone with observed heating", "abstract": "This paper presents a limited-area atmospheric simulation of a tropical\ncyclone accelerated using GPUs. The OpenACC directive-based programming model\nis used to port the atmospheric model to the GPU. The GPU implementation of the\nmain functions and kernels is discussed. The GPU-accelerated code produces\nhigh-fidelity simulations of a realistic tropical cyclone forced by\nobservational latent heating. Performance tests show that the GPU-accelerated\ncode yields energy-efficient simulations and scales well in both the strong and\nweak limit.", "published": "2025-04-10 22:51:11", "link": "http://arxiv.org/abs/2504.08157v1", "categories": ["physics.comp-ph", "cs.NA", "math.NA"], "primary_category": "physics.comp-ph"}
{"title": "A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation", "abstract": "In this article we develop a Physics Informed Neural Network (PINN) approach\nto simulate ice sheet dynamics governed by the Shallow Ice Approximation. This\nproblem takes the form of a time-dependent parabolic obstacle problem. Prior\nwork has used this approach to address the stationary obstacle problem and here\nwe extend it to the time dependent problem. Through comprehensive 1D and 2D\nsimulations, we validate the model's effectiveness in capturing complex\nfree-boundary conditions. By merging traditional mathematical modeling with\ncutting-edge deep learning methods, this approach provides a scalable and\nrobust solution for predicting temporal variations in ice thickness. To\nillustrate this approach in a real world setting, we simulate the dynamics of\nthe Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.", "published": "2025-04-10 21:32:03", "link": "http://arxiv.org/abs/2504.08136v1", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.ao-ph"], "primary_category": "cs.LG"}
{"title": "Probabilistic Multi-Criteria Decision-Making for Circularity Performance of Modern Methods of Construction Products", "abstract": "The construction industry faces increasingly more significant pressure to\nreduce resource consumption, minimise waste, and enhance environmental\nperformance. Towards the transition to a circular economy in the construction\nindustry, one of the challenges is the lack of a standardised assessment\nframework and methods to measure circularity at the product level. To support a\nmore sustainable and circular construction industry through robust and enhanced\nscenario analysis, this paper integrates probabilistic analysis into the\ncoupled assessment framework; this research addresses uncertainties associated\nwith multiple criteria and diverse stakeholders in the construction industry to\nenable more robust decision-making support on both circularity and\nsustainability performance. By demonstrating the application in three\nreal-world MMC products, the proposed framework offers a novel approach to\nsimultaneously assess the circularity and sustainability of MMC products with\nrobustness and objectiveness.", "published": "2025-04-10 15:27:34", "link": "http://arxiv.org/abs/2504.07850v1", "categories": ["math.NA", "cs.NA", "stat.AP"], "primary_category": "math.NA"}
{"title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks", "abstract": "Motivated by the growing demand for low-precision arithmetic in computational\nscience, we exploit lower-precision emulation in Python -- widely regarded as\nthe dominant programming language for numerical analysis and machine learning.\nLow-precision training has revolutionized deep learning by enabling more\nefficient computation and reduced memory and energy consumption while\nmaintaining model fidelity. To better enable numerical experimentation with and\nexploration of low precision computation, we developed the Pychop library,\nwhich supports customizable floating-point formats and a comprehensive set of\nrounding modes in Python, allowing users to benefit from fast, low-precision\nemulation in numerous applications. Pychop also introduces interfaces for both\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\nnetwork training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design,\nimplementation, validation, and practical application of Pychop, establishing\nit as a foundational tool for advancing efficient mixed-precision algorithms.\nFurthermore, we present empirical results on low-precision emulation for image\nclassification and object detection using published datasets, illustrating the\nsensitivity of the use of low precision and offering valuable insights into its\nimpact. Pychop enables in-depth investigations into the effects of numerical\nprecision, facilitates the development of novel hardware accelerators, and\nintegrates seamlessly into existing deep learning workflows. Software and\nexperimental code are publicly available at\nhttps://github.com/inEXASCALE/pychop.", "published": "2025-04-10 15:12:29", "link": "http://arxiv.org/abs/2504.07835v2", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "A Riemannian Gradient Descent Method for the Least Squares Inverse Eigenvalue Problem", "abstract": "We address an algorithm for the least squares fitting of a subset of the\neigenvalues of an unknown Hermitian matrix lying an an affine subspace, called\nthe Lift and Projection (LP) method, due to Chen and Chu (SIAM Journal on\nNumerical Analysis, 33 (1996), pp.2417-2430). The LP method iteratively `lifts'\nthe current iterate onto the spectral constraint manifold then 'projects' onto\nthe solution's affine subspace. We prove that this is equivalent to a\nRiemannian Gradient Descent with respect to a natural Riemannian metric. This\ninsight allows us to derive a more efficient implementation, analyse more\nprecisely its global convergence properties, and naturally append additional\nconstraints to the problem. We provide several numerical experiments to\ndemonstrate the improvement in computation time, which can be more than an\norder of magnitude if the eigenvalue constraints are on the smallest\neigenvalues, the largest eigenvalues, or the eigenvalues closest to a given\nnumber. These experiments include an inverse eigenvalue problem arising in\nInelastic Neutron Scattering of Manganese-6, which requires the least squares\nfitting of 16 experimentally observed eigenvalues of a $32400\\times32400$\nsparse matrix from a 5-dimensional subspace of spin Hamiltonian matrices.", "published": "2025-04-10 14:47:16", "link": "http://arxiv.org/abs/2504.07809v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients", "abstract": "This work focuses on numerically solving a shape identification problem\nrelated to advection-diffusion processes with space-dependent coefficients\nusing shape optimization techniques. Two boundary-type cost functionals are\nconsidered, and their corresponding variations with respect to shapes are\nderived using the adjoint method, employing the chain rule approach. This\ninvolves firstly utilizing the material derivative of the state system and\nsecondly using its shape derivative. Subsequently, an alternating direction\nmethod of multipliers (ADMM) combined with the Sobolev-gradient-descent\nalgorithm is applied to stably solve the shape reconstruction problem.\nNumerical experiments in two and three dimensions are conducted to demonstrate\nthe feasibility of the methods.", "published": "2025-04-10 14:36:56", "link": "http://arxiv.org/abs/2504.07796v2", "categories": ["math.OC", "cs.NA", "math.NA", "49Q10, 35R25, 35R30, 49Q12"], "primary_category": "math.OC"}
{"title": "On the instabilities of naive FEM discretizations for PDEs with sign-changing coefficients", "abstract": "We consider a scalar diffusion equation with a sign-changing coefficient in\nits principle part. The well-posedness of such problems has already been\nstudied extensively provided that the contrast of the coefficient is\nnon-critical. Furthermore, many different approaches have been proposed to\nconstruct stable discretizations thereof, because naive finite element\ndiscretizations are expected to be non-reliable in general. However, no\nexplicit example proving the actual instability is known and numerical\nexperiments often do not manifest instabilities in a conclusive manner. To this\nend we construct an explicit example with a broad family of meshes for which we\nprove that the corresponding naive finite element discretizations are unstable.\nOn the other hand, we also provide a broad family of (non-symmetric) meshes for\nwhich we prove that the discretizations are stable. Together, these two\nfindings explain the results observed in numerical experiments.", "published": "2025-04-10 13:04:58", "link": "http://arxiv.org/abs/2504.07712v1", "categories": ["math.NA", "cs.NA", "65N12, 65N30, 78M10"], "primary_category": "math.NA"}
{"title": "Global approximation to the Boys functions for vectorized computation", "abstract": "A fast approximation to the Boys functions (related to the lower incomplete\ngamma function of half-integer parameter) by a single closed-form analytical\nexpression for all argument values have been developed and tested. Besides the\nexponential function needed anyway for downward recursion, it uses a small\nnumber of addition, multiplication, division, and square root operations, and\nthus is straightforward to vectorize.", "published": "2025-04-10 10:36:24", "link": "http://arxiv.org/abs/2504.07637v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A computational study of low precision incomplete Cholesky factorization preconditioners for sparse linear least-squares problems", "abstract": "Our interest lies in the robust and efficient solution of large sparse linear\nleast-squares problems. In recent years, hardware developments have led to a\nsurge in interest in exploiting mixed precision arithmetic within numerical\nlinear algebra algorithms to take advantage of potential savings in memory\nrequirements, runtime and energy use, whilst still achieving the requested\naccuracy. We explore employing mixed precision when solving least-squares\nproblems, focusing on the practicalities of developing robust approaches using\nlow precision incomplete Cholesky factorization preconditioners. Key penalties\nassociated with lower precision include a loss of reliability and less accuracy\nin the computed solution. Through experiments involving problems from practical\napplications, we study computing incomplete Cholesky factorizations of the\nnormal matrix using low precision and using the factors to precondition LSQR\nusing mixed precision. We investigate level-based and memory-limited incomplete\nfactorization preconditioners. We find that the former are not effective for\nleast-squares problems while the latter can provide high-quality\npreconditioners. In particular, half precision arithmetic can be considered if\nhigh accuracy is not required in the solution or the memory for the incomplete\nfactors is very restricted; otherwise, single precision can be used, and double\nprecision accuracy recovered while reducing memory consumption, even for\nill-conditioned problems.", "published": "2025-04-10 09:21:50", "link": "http://arxiv.org/abs/2504.07580v1", "categories": ["math.NA", "cs.NA", "65F20 (Primary) 65F50, 65F08 (Secondary)"], "primary_category": "math.NA"}
{"title": "Stability and Convergence of Strang Splitting Method for the Allen-Cahn Equation with Homogeneous Neumann Boundary Condition", "abstract": "The Strang splitting method has been widely used to solve nonlinear\nreaction-diffusion equations, with most theoretical convergence analysis\nassuming periodic boundary conditions. However, such analysis presents\nadditional challenges for the case of homogeneous Neumann boundary condition.\nIn this work the Strang splitting method with variable time steps is\ninvestigated for solving the Allen--Cahn equation with homogeneous Neumann\nboundary conditions. Uniform $H^k$-norm stability is established under the\nassumption that the initial condition $u^0$ belongs to the Sobolev space\n$H^k(\\Omega)$ with integer $k\\ge 0$, using the Gagliardo--Nirenberg\ninterpolation inequality and the Sobolev embedding inequality. Furthermore,\nrigorous convergence analysis is provided in the $H^k$-norm for initial\nconditions $u^0 \\in H^{k+6}(\\Omega)$, based on the uniform stability. Several\nnumerical experiments are conducted to verify the theoretical results,\ndemonstrating the effectiveness of the proposed method.", "published": "2025-04-10 07:33:42", "link": "http://arxiv.org/abs/2504.07520v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "High-order discretization errors for the Caputo derivative in H\u00f6lder spaces", "abstract": "Building upon the recent work of Teso and Plociniczak (2025) regarding L1\ndiscretization errors for the Caputo derivative in H\\\"{o}lder spaces, this\nstudy extends the analysis to higher-order discretization errors within the\nsame functional framework. We first investigate truncation errors for the L2\nand L1-2 methods, which approximate the Caputo derivative via piecewise\nquadratic interpolation. Then we generalize the results to arbitrary high-order\ndiscretization. Theoretical analyses reveal a unified error structure across\nall schemes: the convergence order equals the difference between the smoothness\ndegree of the function space and the fractional derivative order, i.e., order\nof error = degree of smoothness - order of the derivative. Numerical\nexperiments validate these theoretical findings.", "published": "2025-04-10 02:20:16", "link": "http://arxiv.org/abs/2504.07391v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Optimal Investment in Equity and Credit Default Swaps in the Presence of Default", "abstract": "We consider an equity market subject to risk from both unhedgeable shocks and\ndefault. The novelty of our work is that to partially offset default risk,\ninvestors may dynamically trade in a credit default swap (CDS) market. Assuming\ninvestment opportunities are driven by functions of an underlying diffusive\nfactor process, we identify the certainty equivalent for a constant absolute\nrisk aversion investor with a semi-linear partial differential equation (PDE)\nwhich has quadratic growth in both the function and gradient coefficients. For\ngeneral model specifications, we prove existence of a solution to the PDE which\nis also the certainty equivalent. We show the optimal policy in the CDS market\ncovers not only equity losses upon default (as one would expect), but also\nlosses due to restricted future trading opportunities. We use our results to\nprice default dependent claims though the principal of utility indifference,\nand we show that provided the underlying equity market is complete absent the\npossibility of default, the equity-CDS market is complete accounting for\ndefault. Lastly, through a numerical application, we show the optimal CDS\npolicies are essentially static (and hence easily implementable) and that\ninvesting in CDS dramatically increases investor indirect utility.", "published": "2025-04-10 19:30:29", "link": "http://arxiv.org/abs/2504.08085v1", "categories": ["q-fin.MF", "q-fin.PM", "q-fin.RM", "91G10 (primary) 91G20, 92G40 (secondary)"], "primary_category": "q-fin.MF"}
{"title": "Trading Graph Neural Network", "abstract": "This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN)\nthat can structurally estimate the impact of asset features, dealer features\nand relationship features on asset prices in trading networks. It combines the\nstrength of the traditional simulated method of moments (SMM) and recent\nmachine learning techniques -- Graph Neural Network (GNN). It outperforms\nexisting reduced-form methods with network centrality measures in prediction\naccuracy. The method can be used on networks with any structure, allowing for\nheterogeneity among both traders and assets.", "published": "2025-04-10 17:40:31", "link": "http://arxiv.org/abs/2504.07923v1", "categories": ["q-fin.TR", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.PR"], "primary_category": "q-fin.TR"}
{"title": "From Speech to Summary: A Comprehensive Survey of Speech Summarization", "abstract": "Speech summarization has become an essential tool for efficiently managing\nand accessing the growing volume of spoken and audiovisual content. However,\ndespite its increasing importance, speech summarization is still not clearly\ndefined and intersects with several research areas, including speech\nrecognition, text summarization, and specific applications like meeting\nsummarization. This survey not only examines existing datasets and evaluation\nmethodologies, which are crucial for assessing the effectiveness of\nsummarization approaches but also synthesizes recent developments in the field,\nhighlighting the shift from traditional systems to advanced models like\nfine-tuned cascaded architectures and end-to-end solutions.", "published": "2025-04-10 17:50:53", "link": "http://arxiv.org/abs/2504.08024v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Categorical Unsupervised Variational Acoustic Clustering", "abstract": "We propose a categorical approach for unsupervised variational acoustic\nclustering of audio data in the time-frequency domain. The consideration of a\ncategorical distribution enforces sharper clustering even when data points\nstrongly overlap in time and frequency, which is the case for most datasets of\nurban acoustic scenes. To this end, we use a Gumbel-Softmax distribution as a\nsoft approximation to the categorical distribution, allowing for training via\nbackpropagation. In this settings, the softmax temperature serves as the main\nmechanism to tune clustering performance. The results show that the proposed\nmodel can obtain impressive clustering performance for all considered datasets,\neven when data points strongly overlap in time and frequency.", "published": "2025-04-10 11:00:17", "link": "http://arxiv.org/abs/2504.07652v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Generalizability to Tone and Content Variations in the Transcription of Amplifier Rendered Electric Guitar Audio", "abstract": "Transcribing electric guitar recordings is challenging due to the scarcity of\ndiverse datasets and the complex tone-related variations introduced by\namplifiers, cabinets, and effect pedals. To address these issues, we introduce\nEGDB-PG, a novel dataset designed to capture a wide range of tone-related\ncharacteristics across various amplifier-cabinet configurations. In addition,\nwe propose the Tone-informed Transformer (TIT), a Transformer-based\ntranscription model enhanced with a tone embedding mechanism that leverages\nlearned representations to improve the model's adaptability to tone-related\nnuances. Experiments demonstrate that TIT, trained on EGDB-PG, outperforms\nexisting baselines across diverse amplifier types, with transcription accuracy\nimprovements driven by the dataset's diversity and the tone embedding\ntechnique. Through detailed benchmarking and ablation studies, we evaluate the\nimpact of tone augmentation, content augmentation, audio normalization, and\ntone embedding on transcription performance. This work advances electric guitar\ntranscription by overcoming limitations in dataset diversity and tone modeling,\nproviding a robust foundation for future research.", "published": "2025-04-10 03:01:14", "link": "http://arxiv.org/abs/2504.07406v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics", "abstract": "The cacophony of urban sounds presents a significant challenge for smart city\napplications that rely on accurate acoustic scene analysis. Effectively\nanalyzing these complex soundscapes, often characterized by overlapping sound\nsources, diverse acoustic events, and unpredictable noise levels, requires\nprecise source separation. This task becomes more complicated when only limited\ntraining data is available. This paper introduces a novel Quantum-Inspired\nGenetic Algorithm (p-QIGA) for source separation, drawing inspiration from\nquantum information theory to enhance acoustic scene analysis in smart cities.\nBy leveraging quantum superposition for efficient solution space exploration\nand entanglement to handle correlated sources, p-QIGA achieves robust\nseparation even with limited data. These quantum-inspired concepts are\nintegrated into a genetic algorithm framework to optimize source separation\nparameters. The effectiveness of our approach is demonstrated on two datasets:\nthe TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban\nsoundscapes, and the Silent Cities dataset, capturing quieter urban\nenvironments during the COVID-19 pandemic. Experimental results show that the\np-QIGA achieves accuracy comparable to state-of-the-art methods while\nexhibiting superior resilience to noise and limited training data, achieving up\nto 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and\noutperforming baseline methods by up to 2 dB with only 10% of the training\ndata. This research highlights the potential of p-QIGA to advance acoustic\nsignal processing in smart cities, particularly for noise pollution monitoring\nand acoustic surveillance.", "published": "2025-04-10 00:05:35", "link": "http://arxiv.org/abs/2504.07345v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The impact of economic policies on housing prices. Approximations and predictions in the UK, the US, France, and Switzerland from the 1980s to today", "abstract": "I show that house prices can be modeled using machine learning (kNN and\ntree-bagging) and a small dataset composed of macro-economic factors (MEF),\nincluding an inflation metric (CPI), US treasury rates (10-yr), Gross Domestic\nProduct (GDP), and portfolio size of central banks (ECB, FED). This set of\nparameters covers all the parties involved in a transaction (buyer, seller, and\nfinancing facility) while ignoring the intrinsic properties of each asset and\nencompassing local (inflation) and liquidity issues that may impede each\ntransaction composing a market. The model here takes the point of view of a\nreal estate trader who is interested in both the financing and the price of the\ntransaction. Machine Learning allows for the discrimination of two periods\nwithin the dataset. Unconventional policies of central banks may have allowed\nsome institutional investors to arbitrage between real estate returns and other\nbond markets (sovereign and corporate). Finally, to assess the models' relative\nperformances, I performed various sensitivity tests, which tend to constrain\nthe possibilities of each approach for each need. I also show that some models\ncan predict the evolution of prices over the next 4 quarters with uncertainties\nthat outperform existing index uncertainties.", "published": "2025-04-10 12:46:54", "link": "http://arxiv.org/abs/2505.09620v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
