{"title": "Continual Machine Reading Comprehension via Uncertainty-aware Fixed\n  Memory and Adversarial Domain Adaptation", "abstract": "Continual Machine Reading Comprehension aims to incrementally learn from a\ncontinuous data stream across time without access the previous seen data, which\nis crucial for the development of real-world MRC systems. However, it is a\ngreat challenge to learn a new domain incrementally without catastrophically\nforgetting previous knowledge. In this paper, MA-MRC, a continual MRC model\nwith uncertainty-aware fixed Memory and Adversarial domain adaptation, is\nproposed. In MA-MRC, a fixed size memory stores a small number of samples in\nprevious domain data along with an uncertainty-aware updating strategy when new\ndomain data arrives. For incremental learning, MA-MRC not only keeps a stable\nunderstanding by learning both memory and new domain data, but also makes full\nuse of the domain adaptation relationship between them by adversarial learning\nstrategy. The experimental results show that MA-MRC is superior to strong\nbaselines and has a substantial incremental learning ability without\ncatastrophically forgetting under two different continual MRC settings.", "published": "2022-08-10 08:40:53", "link": "http://arxiv.org/abs/2208.05217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in\n  Neural Machine Translation?", "abstract": "Neural Machine Translation (NMT) is an open vocabulary problem. As a result,\ndealing with the words not occurring during training (a.k.a. out-of-vocabulary\n(OOV) words) have long been a fundamental challenge for NMT systems. The\npredominant method to tackle this problem is Byte Pair Encoding (BPE) which\nsplits words, including OOV words, into sub-word segments. BPE has achieved\nimpressive results for a wide range of translation tasks in terms of automatic\nevaluation metrics. While it is often assumed that by using BPE, NMT systems\nare capable of handling OOV words, the effectiveness of BPE in translating OOV\nwords has not been explicitly measured. In this paper, we study to what extent\nBPE is successful in translating OOV words at the word-level. We analyze the\ntranslation quality of OOV words based on word type, number of segments,\ncross-attention weights, and the frequency of segment n-grams in the training\ndata. Our experiments show that while careful BPE settings seem to be fairly\nuseful in translating OOV words across datasets, a considerable percentage of\nOOV words are translated incorrectly. Furthermore, we highlight the slightly\nhigher effectiveness of BPE in translating OOV words for special cases, such as\nnamed-entities and when the languages involved are linguistically close to each\nother.", "published": "2022-08-10 08:57:13", "link": "http://arxiv.org/abs/2208.05225v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Proceedings End-to-End Compositional Models of Vector-Based Semantics", "abstract": "The workshop End-to-End Compositional Models of Vector-Based Semantics was\nheld at NUI Galway on 15 and 16 August 2022 as part of the 33rd European Summer\nSchool in Logic, Language and Information (ESSLLI 2022).\n  The workshop was sponsored by the research project 'A composition calculus\nfor vector-based semantic modelling with a localization for Dutch' (Dutch\nResearch Council 360-89-070, 2017-2022). The workshop program was made up of\ntwo parts, the first part reporting on the results of the aforementioned\nproject, the second part consisting of contributed papers on related\napproaches. The present volume collects the contributed papers and the\nabstracts of the invited talks.", "published": "2022-08-10 12:50:12", "link": "http://arxiv.org/abs/2208.05313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrasing, textual entailment, and semantic similarity above word\n  level", "abstract": "This dissertation explores the linguistic and computational aspects of the\nmeaning relations that can hold between two or more complex linguistic\nexpressions (phrases, clauses, sentences, paragraphs). In particular, it\nfocuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic\nSimilarity.\n  In Part I: \"Similarity at the Level of Words and Phrases\", I study the\nDistributional Hypothesis (DH) and explore several different methodologies for\nquantifying semantic similarity at the levels of words and short phrases.\n  In Part II: \"Paraphrase Typology and Paraphrase Identification\", I focus on\nthe meaning relation of paraphrasing and the empirical task of automated\nParaphrase Identification (PI).\n  In Part III: \"Paraphrasing, Textual Entailment, and Semantic Similarity\", I\npresent a novel direction in the research on textual meaning relations,\nresulting from joint research carried out on on paraphrasing, textual\nentailment, contradiction, and semantic similarity.", "published": "2022-08-10 15:07:49", "link": "http://arxiv.org/abs/2208.05387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Analysis about Building Cross-lingual Sememe Knowledge Base Based on\n  Deep Clustering Network", "abstract": "A sememe is defined as the minimum semantic unit of human languages. Sememe\nknowledge bases (KBs), which contain words annotated with sememes, have been\nsuccessfully applied to many NLP tasks, and we believe that by learning the\nsmallest unit of meaning, computers can more easily understand human language.\nHowever, Existing sememe KBs are built on only manual annotation, human\nannotations have personal understanding biases, and the meaning of vocabulary\nwill be constantly updated and changed with the times, and artificial methods\nare not always practical. To address the issue, we propose an unsupervised\nmethod based on a deep clustering network (DCN) to build a sememe KB, and you\ncan use any language to build a KB through this method. We first learn the\ndistributed representation of multilingual words, use MUSE to align them in a\nsingle vector space, learn the multi-layer meaning of each word through the\nself-attention mechanism, and use a DNC to cluster sememe features. Finally, we\ncompleted the prediction using only the 10-dimensional sememe space in English.\nWe found that the low-dimensional space can still retain the main feature of\nthe sememes.", "published": "2022-08-10 17:40:45", "link": "http://arxiv.org/abs/2208.05462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Retraining by Recycling Parameter-Efficient Prompts", "abstract": "Parameter-efficient methods are able to use a single frozen pre-trained large\nlanguage model (LLM) to perform many tasks by learning task-specific soft\nprompts that modulate model behavior when concatenated to the input text.\nHowever, these learned prompts are tightly coupled to a given frozen model --\nif the model is updated, corresponding new prompts need to be obtained. In this\nwork, we propose and investigate several approaches to \"Prompt Recycling'\"\nwhere a prompt trained on a source model is transformed to work with the new\ntarget model. Our methods do not rely on supervised pairs of prompts,\ntask-specific data, or training updates with the target model, which would be\njust as costly as re-tuning prompts with the target model from scratch. We show\nthat recycling between models is possible (our best settings are able to\nsuccessfully recycle $88.9\\%$ of prompts, producing a prompt that out-performs\nbaselines), but significant performance headroom remains, requiring improved\nrecycling techniques.", "published": "2022-08-10 22:10:53", "link": "http://arxiv.org/abs/2208.05577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TagRec++: Hierarchical Label Aware Attention Network for Question\n  Categorization", "abstract": "Online learning systems have multiple data repositories in the form of\ntranscripts, books and questions. To enable ease of access, such systems\norganize the content according to a well defined taxonomy of hierarchical\nnature (subject-chapter-topic). The task of categorizing inputs to the\nhierarchical labels is usually cast as a flat multi-class classification\nproblem. Such approaches ignore the semantic relatedness between the terms in\nthe input and the tokens in the hierarchical labels. Alternate approaches also\nsuffer from class imbalance when they only consider leaf level nodes as labels.\nTo tackle the issues, we formulate the task as a dense retrieval problem to\nretrieve the appropriate hierarchical labels for each content. In this paper,\nwe deal with categorizing questions. We model the hierarchical labels as a\ncomposition of their tokens and use an efficient cross-attention mechanism to\nfuse the information with the term representations of the content. We also\npropose an adaptive in-batch hard negative sampling approach which samples\nbetter negatives as the training progresses. We demonstrate that the proposed\napproach \\textit{TagRec++} outperforms existing state-of-the-art approaches on\nquestion datasets as measured by Recall@k. In addition, we demonstrate\nzero-shot capabilities of \\textit{TagRec++} and ability to adapt to label\nchanges.", "published": "2022-08-10 05:08:37", "link": "http://arxiv.org/abs/2208.05152v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Looking for a Needle in a Haystack: A Comprehensive Study of\n  Hallucinations in Neural Machine Translation", "abstract": "Although the problem of hallucinations in neural machine translation (NMT)\nhas received some attention, research on this highly pathological phenomenon\nlacks solid ground. Previous work has been limited in several ways: it often\nresorts to artificial settings where the problem is amplified, it disregards\nsome (common) types of hallucinations, and it does not validate adequacy of\ndetection heuristics. In this paper, we set foundations for the study of NMT\nhallucinations. First, we work in a natural setting, i.e., in-domain data\nwithout artificial noise neither in training nor in inference. Next, we\nannotate a dataset of over 3.4k sentences indicating different kinds of\ncritical errors and hallucinations. Then, we turn to detection methods and both\nrevisit methods used previously and propose using glass-box uncertainty-based\ndetectors. Overall, we show that for preventive settings, (i) previously used\nmethods are largely inadequate, (ii) sequence log-probability works best and\nperforms on par with reference-based methods. Finally, we propose\nDeHallucinator, a simple method for alleviating hallucinations at test time\nthat significantly reduces the hallucinatory rate. To ease future research, we\nrelease our annotated dataset for WMT18 German-English data, along with the\nmodel, training data, and code.", "published": "2022-08-10 12:44:13", "link": "http://arxiv.org/abs/2208.05309v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Quantum Natural Language Processing Approach to Pronoun Resolution", "abstract": "We use the Lambek Calculus with soft sub-exponential modalities to model and\nreason about discourse relations such as anaphora and ellipsis. A semantics for\nthis logic is obtained by using truncated Fock spaces, developed in our\nprevious work. We depict these semantic computations via a new string diagram.\nThe Fock Space semantics has the advantage that its terms are learnable from\nlarge corpora of data using machine learning and they can be experimented with\non mainstream natural language tasks. Further, and thanks to an existing\ntranslation from vector spaces to quantum circuits, we can also learn these\nterms on quantum computers and their simulators, such as the IBMQ range. We\nextend the existing translation to Fock spaces and develop quantum circuit\nsemantics for discourse relations. We then experiment with the IBMQ\nAerSimulations of these circuits in a definite pronoun resolution task, where\nthe highest accuracies were recorded for models when the anaphora was resolved.", "published": "2022-08-10 15:22:58", "link": "http://arxiv.org/abs/2208.05393v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Self-supervised Multi-modal Training from Uncurated Image and Reports\n  Enables Zero-shot Oversight Artificial Intelligence in Radiology", "abstract": "Oversight AI is an emerging concept in radiology where the AI forms a\nsymbiosis with radiologists by continuously supporting radiologists in their\ndecision-making. Recent advances in vision-language models sheds a light on the\nlong-standing problems of the oversight AI by the understanding both visual and\ntextual concepts and their semantic correspondences. However, there have been\nlimited successes in the application of vision-language models in the medical\ndomain, as the current vision-language models and learning strategies for\nphotographic images and captions call for the web-scale data corpus of image\nand text pairs which was not often feasible in the medical domain. To address\nthis, here we present a model dubbed Medical Cross-attention Vision-Language\nmodel (Medical X-VL), leveraging the key components to be tailored for the\nmedical domain. Our medical X-VL model is based on the following components:\nself-supervised uni-modal models in medical domain and fusion encoder to bridge\nthem, momentum distillation, sentence-wise contrastive learning for medical\nreports, and the sentence similarity-adjusted hard negative mining. We\nexperimentally demonstrated that our model enables various zero-shot tasks for\noversight AI, ranging from the zero-shot classification to zero-shot error\ncorrection. Our model outperformed the current state-of-the-art models in two\ndifferent medical image database, suggesting the novel clinical usage of our\noversight AI model for monitoring human errors. Our method was especially\nsuccessful in the data-limited setting, which is frequently encountered in the\nclinics, suggesting the potential widespread applicability in medical domain.", "published": "2022-08-10 04:35:58", "link": "http://arxiv.org/abs/2208.05140v4", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "CLEVR-Math: A Dataset for Compositional Language, Visual and\n  Mathematical Reasoning", "abstract": "We introduce CLEVR-Math, a multi-modal math word problems dataset consisting\nof simple math word problems involving addition/subtraction, represented partly\nby a textual description and partly by an image illustrating the scenario. The\ntext describes actions performed on the scene that is depicted in the image.\nSince the question posed may not be about the scene in the image, but about the\nstate of the scene before or after the actions are applied, the solver envision\nor imagine the state changes due to these actions. Solving these word problems\nrequires a combination of language, visual and mathematical reasoning. We apply\nstate-of-the-art neural and neuro-symbolic models for visual question answering\non CLEVR-Math and empirically evaluate their performances. Our results show how\nneither method generalise to chains of operations. We discuss the limitations\nof the two in addressing the task of multi-modal word problem solving.", "published": "2022-08-10 14:08:34", "link": "http://arxiv.org/abs/2208.05358v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "I.2.7; I.2.10; I.2.6; I.4.8; I.1.4"], "primary_category": "cs.LG"}
{"title": "Towards Cross-speaker Reading Style Transfer on Audiobook Dataset", "abstract": "Cross-speaker style transfer aims to extract the speech style of the given\nreference speech, which can be reproduced in the timbre of arbitrary target\nspeakers. Existing methods on this topic have explored utilizing\nutterance-level style labels to perform style transfer via either global or\nlocal scale style representations. However, audiobook datasets are typically\ncharacterized by both the local prosody and global genre, and are rarely\naccompanied by utterance-level style labels. Thus, properly transferring the\nreading style across different speakers remains a challenging task. This paper\naims to introduce a chunk-wise multi-scale cross-speaker style model to capture\nboth the global genre and the local prosody in audiobook speeches. Moreover, by\ndisentangling speaker timbre and style with the proposed switchable adversarial\nclassifiers, the extracted reading style is made adaptable to the timbre of\ndifferent speakers. Experiment results confirm that the model manages to\ntransfer a given reading style to new target speakers. With the support of\nlocal prosody and global genre type predictor, the potentiality of the proposed\nmethod in multi-speaker audiobook generation is further revealed.", "published": "2022-08-10 14:08:35", "link": "http://arxiv.org/abs/2208.05359v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-task Active Learning for Pre-trained Transformer-based Models", "abstract": "Multi-task learning, in which several tasks are jointly learned by a single\nmodel, allows NLP models to share information from multiple annotations and may\nfacilitate better predictions when the tasks are inter-related. This technique,\nhowever, requires annotating the same text with multiple annotation schemes\nwhich may be costly and laborious. Active learning (AL) has been demonstrated\nto optimize annotation processes by iteratively selecting unlabeled examples\nwhose annotation is most valuable for the NLP model. Yet, multi-task active\nlearning (MT-AL) has not been applied to state-of-the-art pre-trained\nTransformer-based NLP models. This paper aims to close this gap. We explore\nvarious multi-task selection criteria in three realistic multi-task scenarios,\nreflecting different relations between the participating tasks, and demonstrate\nthe effectiveness of multi-task compared to single-task selection. Our results\nsuggest that MT-AL can be effectively used in order to minimize annotation\nefforts for multi-task NLP models.", "published": "2022-08-10 14:54:13", "link": "http://arxiv.org/abs/2208.05379v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Moral Foundations Reddit Corpus", "abstract": "Moral framing and sentiment can affect a variety of online and offline\nbehaviors, including donation, pro-environmental action, political engagement,\nand even participation in violent protests. Various computational methods in\nNatural Language Processing (NLP) have been used to detect moral sentiment from\ntextual data, but in order to achieve better performances in such subjective\ntasks, large sets of hand-annotated training data are needed. Previous corpora\nannotated for moral sentiment have proven valuable, and have generated new\ninsights both within NLP and across the social sciences, but have been limited\nto Twitter. To facilitate improving our understanding of the role of moral\nrhetoric, we present the Moral Foundations Reddit Corpus, a collection of\n16,123 Reddit comments that have been curated from 12 distinct subreddits,\nhand-annotated by at least three trained annotators for 8 categories of moral\nsentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty,\nThin Morality, Implicit/Explicit Morality) based on the updated Moral\nFoundations Theory (MFT) framework. We use a range of methodologies to provide\nbaseline moral-sentiment classification results for this new corpus, e.g.,\ncross-domain classification and knowledge transfer.", "published": "2022-08-10 20:08:10", "link": "http://arxiv.org/abs/2208.05545v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR", "abstract": "It is common knowledge that the quantity and quality of the training data\nplay a significant role in the creation of a good machine learning model. In\nthis paper, we take it one step further and demonstrate that the way the\ntraining examples are arranged is also of crucial importance. Curriculum\nLearning is built on the observation that organized and structured assimilation\nof knowledge has the ability to enable faster training and better\ncomprehension. When humans learn to speak, they first try to utter basic phones\nand then gradually move towards more complex structures such as words and\nsentences. This methodology is known as Curriculum Learning, and we employ it\nin the context of Automatic Speech Recognition. We hypothesize that end-to-end\nmodels can achieve better performance when provided with an organized training\nset consisting of examples that exhibit an increasing level of difficulty (i.e.\na curriculum). To impose structure on the training set and to define the notion\nof an easy example, we explored multiple scoring functions that either use\nfeedback from an external neural network or incorporate feedback from the model\nitself. Empirical results show that with different curriculums we can balance\nthe training times and the network's performance.", "published": "2022-08-10 06:56:58", "link": "http://arxiv.org/abs/2208.05782v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "I.2.7; I.2.0"], "primary_category": "eess.AS"}
{"title": "Can Brain Signals Reveal Inner Alignment with Human Languages?", "abstract": "Brain Signals, such as Electroencephalography (EEG), and human languages have\nbeen widely explored independently for many downstream tasks, however, the\nconnection between them has not been well explored. In this study, we explore\nthe relationship and dependency between EEG and language. To study at the\nrepresentation level, we introduced \\textbf{MTAM}, a \\textbf{M}ultimodal\n\\textbf{T}ransformer \\textbf{A}lignment \\textbf{M}odel, to observe coordinated\nrepresentations between the two modalities. We used various relationship\nalignment-seeking techniques, such as Canonical Correlation Analysis and\nWasserstein Distance, as loss functions to transfigure features. On downstream\napplications, sentiment analysis and relation detection, we achieved new\nstate-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method\nachieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets\nfor sentiment analysis, and 7.4% on ZuCo for relation detection. In addition,\nwe provide interpretations of the performance improvement: (1) feature\ndistribution shows the effectiveness of the alignment module for discovering\nand encoding the relationship between EEG and language; (2) alignment weights\nshow the influence of different language semantics as well as EEG frequency\nfeatures; (3) brain topographical maps provide an intuitive demonstration of\nthe connectivity in the brain regions. Our code is available at\n\\url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.", "published": "2022-08-10 17:51:34", "link": "http://arxiv.org/abs/2208.06348v5", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Improving Hypernasality Estimation with Automatic Speech Recognition in\n  Cleft Palate Speech", "abstract": "Hypernasality is an abnormal resonance in human speech production, especially\nin patients with craniofacial anomalies such as cleft palate. In clinical\napplication, hypernasality estimation is crucial in cleft palate diagnosis, as\nits results determine the subsequent surgery and additional speech therapy.\nTherefore, designing an automatic hypernasality assessment method will\nfacilitate speech-language pathologists to make precise diagnoses. Existing\nmethods for hypernasality estimation only conduct acoustic analysis based on\nlow-resource cleft palate dataset, by using statistical or neural network-based\nfeatures. In this paper, we propose a novel approach that uses automatic speech\nrecognition model to improve hypernasality estimation. Specifically, we first\npre-train an encoder-decoder framework in an automatic speech recognition (ASR)\nobjective by using speech-to-text dataset, and then fine-tune ASR encoder on\nthe cleft palate dataset for hypernasality estimation. Benefiting from such\ndesign, our model for hypernasality estimation can enjoy the advantages of ASR\nmodel: 1) compared with low-resource cleft palate dataset, the ASR task usually\nincludes large-scale speech data in the general domain, which enables better\nmodel generalization; 2) the text annotations in ASR dataset guide model to\nextract better acoustic features. Experimental results on two cleft palate\ndatasets demonstrate that our method achieves superior performance compared\nwith previous approaches.", "published": "2022-08-10 03:15:57", "link": "http://arxiv.org/abs/2208.05122v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Preserving the beamforming effect for spatial cue-based pseudo-binaural\n  dereverberation of a single source", "abstract": "Reverberations are unavoidable in enclosures, resulting in reduced\nintelligibility for hearing impaired and non native listeners and even for the\nnormal hearing listeners in noisy circumstances. It also degrades the\nperformance of machine listening applications. In this paper, we propose a\nnovel approach of binaural dereverberation of a single speech source, using the\ndifferences in the interaural cues of the direct path signal and the\nreverberations. Two beamformers, spaced at an interaural distance, are used to\nextract the reverberations from the reverberant speech. The interaural cues\ngenerated by these reverberations and those generated by the direct path signal\nact as a two class dataset, used for the training of U-Net (a deep\nconvolutional neural network). After its training, the beamformers are removed\nand the trained U-Net along with the maximum likelihood estimation (MLE)\nalgorithm is used to discriminate between the direct path cues from the\nreverberation cues, when the system is exposed to the interaural spectrogram of\nthe reverberant speech signal. Our proposed model has outperformed the\nclassical signal processing dereverberation model weighted prediction error in\nterms of cepstral distance (CEP), frequency weighted segmental signal to noise\nratio (FWSEGSNR) and signal to reverberation modulation energy ratio (SRMR) by\n1.4 points, 8 dB and 0.6dB. It has achieved better performance than the deep\nlearning based dereverberation model by gaining 1.3 points improvement in CEP\nwith comparable FWSEGSNR, using training dataset which is almost 8 times\nsmaller than required for that model. The proposed model also sustained its\nperformance under relatively similar unseen acoustic conditions and at\npositions in the vicinity of its training position.", "published": "2022-08-10 07:07:05", "link": "http://arxiv.org/abs/2208.05184v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non-Contrastive Self-Supervised Learning of Utterance-Level Speech\n  Representations", "abstract": "Considering the abundance of unlabeled speech data and the high labeling\ncosts, unsupervised learning methods can be essential for better system\ndevelopment. One of the most successful methods is contrastive self-supervised\nmethods, which require negative sampling: sampling alternative samples to\ncontrast with the current sample (anchor). However, it is hard to ensure if all\nthe negative samples belong to classes different from the anchor class without\nlabels. This paper applies a non-contrastive self-supervised learning method on\nan unlabeled speech corpus to learn utterance-level embeddings. We used\nDIstillation with NO labels (DINO), proposed in computer vision, and adapted it\nto the speech domain. Unlike the contrastive methods, DINO does not require\nnegative sampling. These embeddings were evaluated on speaker verification and\nemotion recognition. In speaker verification, the unsupervised DINO embedding\nwith cosine scoring provided 4.38% EER on the VoxCeleb1 test trial. This\noutperforms the best contrastive self-supervised method by 40% relative in EER.\nAn iterative pseudo-labeling training pipeline, not requiring speaker labels,\nfurther improved the EER to 1.89%. In emotion recognition, the DINO embedding\nperformed 60.87, 79.21, and 56.98% in micro-f1 score on IEMOCAP, Crema-D, and\nMSP-Podcast, respectively. The results imply the generality of the DINO\nembedding to different speech applications.", "published": "2022-08-10 16:04:23", "link": "http://arxiv.org/abs/2208.05413v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Controlling Perceived Emotion in Symbolic Music Generation with Monte\n  Carlo Tree Search", "abstract": "This paper presents a new approach for controlling emotion in symbolic music\ngeneration with Monte Carlo Tree Search. We use Monte Carlo Tree Search as a\ndecoding mechanism to steer the probability distribution learned by a language\nmodel towards a given emotion. At every step of the decoding process, we use\nPredictor Upper Confidence for Trees (PUCT) to search for sequences that\nmaximize the average values of emotion and quality as given by an emotion\nclassifier and a discriminator, respectively. We use a language model as PUCT's\npolicy and a combination of the emotion classifier and the discriminator as its\nvalue function. To decode the next token in a piece of music, we sample from\nthe distribution of node visits created during the search. We evaluate the\nquality of the generated samples with respect to human-composed pieces using a\nset of objective metrics computed directly from the generated samples. We also\nperform a user study to evaluate how human subjects perceive the generated\nsamples' quality and emotion. We compare PUCT against Stochastic Bi-Objective\nBeam Search (SBBS) and Conditional Sampling (CS). Results suggest that PUCT\noutperforms SBBS and CS in almost all metrics of music quality and emotion.", "published": "2022-08-10 05:49:37", "link": "http://arxiv.org/abs/2208.05162v4", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information\n  Extraction from Speech", "abstract": "In recent studies, self-supervised pre-trained models tend to outperform\nsupervised pre-trained models in transfer learning. In particular,\nself-supervised learning (SSL) of utterance-level speech representation can be\nused in speech applications that require discriminative representation of\nconsistent attributes within an utterance: speaker, language, emotion, and age.\nExisting frame-level self-supervised speech representation, e.g., wav2vec, can\nbe used as utterance-level representation with pooling, but the models are\nusually large. There are also SSL techniques to learn utterance-level\nrepresentation. One of the most successful is a contrastive method, which\nrequires negative sampling: selecting alternative samples to contrast with the\ncurrent sample (anchor). However, this does not ensure that all the negative\nsamples belong to classes different from the anchor class without labels. This\npaper applies a non-contrastive self-supervised method to learn utterance-level\nembeddings. We adapted DIstillation with NO labels (DINO) from computer vision\nto speech. Unlike contrastive methods, DINO does not require negative sampling.\nWe compared DINO to x-vector trained in a supervised manner. When transferred\nto down-stream tasks (speaker verification, speech emotion recognition (SER),\nand Alzheimer's disease detection), DINO outperformed x-vector. We studied the\ninfluence of several aspects during transfer learning such as dividing the\nfine-tuning process into steps, chunk lengths, or augmentation. During\nfine-tuning, tuning the last affine layers first and then the whole network\nsurpassed fine-tuning all at once. Using shorter chunk lengths, although they\ngenerate more diverse inputs, did not necessarily improve performance, implying\nspeech segments at least with a specific length are required for better\nperformance per application. Augmentation was helpful in SER.", "published": "2022-08-10 16:56:39", "link": "http://arxiv.org/abs/2208.05445v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
