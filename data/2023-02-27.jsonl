{"title": "Elementwise Language Representation", "abstract": "We propose a new technique for computational language representation called\nelementwise embedding, in which a material (semantic unit) is abstracted into a\nhorizontal concatenation of lower-dimensional element (character) embeddings.\nWhile elements are always characters, materials are arbitrary levels of\nsemantic units so it generalizes to any type of tokenization. To focus only on\nthe important letters, the $n^{th}$ spellings of each semantic unit are aligned\nin $n^{th}$ attention heads, then concatenated back into original forms\ncreating unique embedding representations; they are jointly projected thereby\ndetermining own contextual importance. Technically, this framework is achieved\nby passing a sequence of materials, each consists of $v$ elements, to a\ntransformer having $h=v$ attention heads. As a pure embedding technique,\nelementwise embedding replaces the $w$-dimensional embedding table of a\ntransformer model with $256$ $c$-dimensional elements (each corresponding to\none of UTF-8 bytes) where $c=w/v$. Using this novel approach, we show that the\nstandard transformer architecture can be reused for all levels of language\nrepresentations and be able to process much longer sequences at the same\ntime-complexity without \"any\" architectural modification and additional\noverhead. BERT trained with elementwise embedding outperforms its subword\nequivalence (original implementation) in multilabel patent document\nclassification exhibiting superior robustness to domain-specificity and data\nimbalance, despite using $0.005\\%$ of embedding parameters. Experiments\ndemonstrate the generalizability of the proposed method by successfully\ntransferring these enhancements to differently architected transformers CANINE\nand ALBERT.", "published": "2023-02-27 02:15:56", "link": "http://arxiv.org/abs/2302.13475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategize Before Teaching: A Conversational Tutoring System with\n  Pedagogy Self-Distillation", "abstract": "Conversational tutoring systems (CTSs) aim to help students master\neducational material with natural language interaction in the form of a dialog.\nCTSs have become a key pillar in educational data mining research. A key\nchallenge in CTSs is to engage the student in the conversation while exposing\nthem to a diverse set of teaching strategies, akin to a human teacher, thereby,\nhelping them learn in the process. Different from previous work that generates\nresponses given the strategies as input, we propose to jointly predict teaching\nstrategies and generate tutor responses accordingly, which fits a more\nrealistic application scenario. We benchmark several competitive models on\nthree dialog tutoring datasets and propose a unified framework that combines\nteaching response generation and pedagogical strategy prediction, where a\nself-distillation mechanism is adopted to guide the teaching strategy learning\nand facilitate tutor response generation. Our experiments and analyses shed\nlight on how teaching strategies affect dialog tutoring.", "published": "2023-02-27 03:43:25", "link": "http://arxiv.org/abs/2302.13496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Action Dialog Policy Learning from Logged User Feedback", "abstract": "Multi-action dialog policy, which generates multiple atomic dialog actions\nper turn, has been widely applied in task-oriented dialog systems to provide\nexpressive and efficient system responses. Existing policy models usually\nimitate action combinations from the labeled multi-action dialog examples. Due\nto data limitations, they generalize poorly toward unseen dialog flows. While\nreinforcement learning-based methods are proposed to incorporate the service\nratings from real users and user simulators as external supervision signals,\nthey suffer from sparse and less credible dialog-level rewards. To cope with\nthis problem, we explore to improve multi-action dialog policy learning with\nexplicit and implicit turn-level user feedback received for historical\npredictions (i.e., logged user feedback) that are cost-efficient to collect and\nfaithful to real-world scenarios. The task is challenging since the logged user\nfeedback provides only partial label feedback limited to the particular\nhistorical dialog actions predicted by the agent. To fully exploit such\nfeedback information, we propose BanditMatch, which addresses the task from a\nfeedback-enhanced semi-supervised learning perspective with a hybrid objective\nof semi-supervised learning and bandit learning. BanditMatch integrates\npseudo-labeling methods to better explore the action space through constructing\nfull label feedback. Extensive experiments show that our BanditMatch\noutperforms the state-of-the-art methods by generating more concise and\ninformative responses. The source code and the appendix of this paper can be\nobtained from https://github.com/ShuoZhangXJTU/BanditMatch.", "published": "2023-02-27 04:01:28", "link": "http://arxiv.org/abs/2302.13505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Support Examples for In-Context Learning", "abstract": "Additionally, the strong dependency among in-context examples makes it an\nNP-hard combinatorial optimization problem and enumerating all permutations is\ninfeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this\nchallenge in two stages: First we filter the dataset to obtain informative\nin-context examples individually. Specifically, we propose a novel metric,\nInfoScore, to evaluate the example's in-context informativeness based on the\nlanguage model's feedback, and further propose a progressive filtering process\nto filter out uninformative examples. Then we propose diversity-guided example\nsearch which iteratively refines and evaluates the selected example\npermutations, to find examples that fully depict the task. The experimental\nresults show that LENS significantly outperforms a wide range of baselines.", "published": "2023-02-27 06:32:45", "link": "http://arxiv.org/abs/2302.13539v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "kNN-BOX: A Unified Framework for Nearest Neighbor Generation", "abstract": "Augmenting the base neural model with a token-level symbolic datastore is a\nnovel generation paradigm and has achieved promising results in machine\ntranslation (MT). In this paper, we introduce a unified framework kNN-BOX,\nwhich enables quick development and interactive analysis for this novel\nparadigm. kNN-BOX decomposes the datastore-augmentation approach into three\nmodules: datastore, retriever and combiner, thus putting diverse kNN generation\nmethods into a unified way. Currently, kNN-BOX has provided implementation of\nseven popular kNN-MT variants, covering research from performance enhancement\nto efficiency optimization. It is easy for users to reproduce these existing\nworks or customize their own models. Besides, users can interact with their kNN\ngeneration systems with kNN-BOX to better understand the underlying inference\nprocess in a visualized way. In the experiment section, we apply kNN-BOX for\nmachine translation and three other seq2seq generation tasks, namely, text\nsimplification, paraphrase generation and question generation. Experiment\nresults show that augmenting the base neural model with kNN-BOX leads to a\nlarge performance improvement in all these tasks. The code and document of\nkNN-BOX is available at https://github.com/NJUNLP/knn-box.", "published": "2023-02-27 08:21:14", "link": "http://arxiv.org/abs/2302.13574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisit Out-Of-Vocabulary Problem for Slot Filling: A Unified\n  Contrastive Frameword with Multi-level Data Augmentations", "abstract": "In real dialogue scenarios, the existing slot filling model, which tends to\nmemorize entity patterns, has a significantly reduced generalization facing\nOut-of-Vocabulary (OOV) problems. To address this issue, we propose an OOV\nrobust slot filling model based on multi-level data augmentations to solve the\nOOV problem from both word and slot perspectives. We present a unified\ncontrastive learning framework, which pull representations of the origin sample\nand augmentation samples together, to make the model resistant to OOV problems.\nWe evaluate the performance of the model from some specific slots and carefully\ndesign test data with OOV word perturbation to further demonstrate the\neffectiveness of OOV words. Experiments on two datasets show that our approach\noutperforms the previous sota methods in terms of both OOV slots and words.", "published": "2023-02-27 08:42:30", "link": "http://arxiv.org/abs/2302.13584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Prototypical Semantic Decoupling Method via Joint Contrastive Learning\n  for Few-Shot Name Entity Recognition", "abstract": "Few-shot named entity recognition (NER) aims at identifying named entities\nbased on only few labeled instances. Most existing prototype-based sequence\nlabeling models tend to memorize entity mentions which would be easily confused\nby close prototypes. In this paper, we proposed a Prototypical Semantic\nDecoupling method via joint Contrastive learning (PSDC) for few-shot NER.\nSpecifically, we decouple class-specific prototypes and contextual semantic\nprototypes by two masking strategies to lead the model to focus on two\ndifferent semantic information for inference. Besides, we further introduce\njoint contrastive learning objectives to better integrate two kinds of\ndecoupling information and prevent semantic collapse. Experimental results on\ntwo few-shot NER benchmarks demonstrate that PSDC consistently outperforms the\nprevious SOTA methods in terms of overall performance. Extensive analysis\nfurther validates the effectiveness and generalization of PSDC.", "published": "2023-02-27 09:20:00", "link": "http://arxiv.org/abs/2302.13610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Automatically Constructed Word Meaning Explanations", "abstract": "Preparing exact and comprehensive word meaning explanations is one of the key\nsteps in the process of monolingual dictionary writing. In standard\nmethodology, the explanations need an expert lexicographer who spends a\nsubstantial amount of time checking the consistency between the descriptive\ntext and corpus evidence. In the following text, we present a new tool that\nderives explanations automatically based on collective information from very\nlarge corpora, particularly on word sketches. We also propose a quantitative\nevaluation of the constructed explanations, concentrating on explanations of\nnouns. The methodology is to a certain extent language independent; however,\nthe presented verification is limited to Czech and English. We show that the\npresented approach allows to create explanations that contain data useful for\nunderstanding the word meaning in approximately 90% of cases. However, in many\ncases, the result requires post-editing to remove redundant information.", "published": "2023-02-27 09:47:55", "link": "http://arxiv.org/abs/2302.13625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Every Example Count: On the Stability and Utility of Self-Influence\n  for Learning from Noisy NLP Datasets", "abstract": "Increasingly larger datasets have become a standard ingredient to advancing\nthe state-of-the-art in NLP. However, data quality might have already become\nthe bottleneck to unlock further gains. Given the diversity and the sizes of\nmodern datasets, standard data filtering is not straight-forward to apply,\nbecause of the multifacetedness of the harmful data and elusiveness of\nfiltering rules that would generalize across multiple tasks. We study the\nfitness of task-agnostic self-influence scores of training examples for data\ncleaning, analyze their efficacy in capturing naturally occurring outliers, and\ninvestigate to what extent self-influence based data cleaning can improve\ndownstream performance in machine translation, question answering and text\nclassification, building up on recent approaches to self-influence calculation\nand automated curriculum learning.", "published": "2023-02-27 17:00:06", "link": "http://arxiv.org/abs/2302.13959v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMA: Open and Efficient Foundation Language Models", "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.", "published": "2023-02-27 17:11:15", "link": "http://arxiv.org/abs/2302.13971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diacritic Recognition Performance in Arabic ASR", "abstract": "We present an analysis of diacritic recognition performance in Arabic\nAutomatic Speech Recognition (ASR) systems. As most existing Arabic speech\ncorpora do not contain all diacritical marks, which represent short vowels and\nother phonetic information in Arabic script, current state-of-the-art ASR\nmodels do not produce full diacritization in their output. Automatic text-based\ndiacritization has previously been employed both as a pre-processing step to\ntrain diacritized ASR, or as a post-processing step to diacritize the resulting\nASR hypotheses. It is generally believed that input diacritization degrades ASR\nperformance, but no systematic evaluation of ASR diacritization performance,\nindependent of ASR performance, has been conducted to date. In this paper, we\nattempt to experimentally clarify whether input diacritiztation indeed degrades\nASR quality, and to compare the diacritic recognition performance against\ntext-based diacritization as a post-processing step. We start with pre-trained\nArabic ASR models and fine-tune them on transcribed speech data with different\ndiacritization conditions: manual, automatic, and no diacritization. We isolate\ndiacritic recognition performance from the overall ASR performance using\ncoverage and precision metrics. We find that ASR diacritization significantly\noutperforms text-based diacritization in post-processing, particularly when the\nASR model is fine-tuned with manually diacritized transcripts.", "published": "2023-02-27 18:27:42", "link": "http://arxiv.org/abs/2302.14022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TabGenie: A Toolkit for Table-to-Text Generation", "abstract": "Heterogenity of data-to-text generation datasets limits the research on\ndata-to-text generation systems. We present TabGenie - a toolkit which enables\nresearchers to explore, preprocess, and analyze a variety of data-to-text\ngeneration datasets through the unified framework of table-to-text generation.\nIn TabGenie, all the inputs are represented as tables with associated metadata.\nThe tables can be explored through the web interface, which also provides an\ninteractive mode for debugging table-to-text generation, facilitates\nside-by-side comparison of generated system outputs, and allows easy exports\nfor manual analysis. Furthermore, TabGenie is equipped with command line\nprocessing tools and Python bindings for unified dataset loading and\nprocessing. We release TabGenie as a PyPI package and provide its open-source\ncode and a live demo at https://github.com/kasnerz/tabgenie.", "published": "2023-02-27 22:05:46", "link": "http://arxiv.org/abs/2302.14169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fluid Transformers and Creative Analogies: Exploring Large Language\n  Models' Capacity for Augmenting Cross-Domain Analogical Creativity", "abstract": "Cross-domain analogical reasoning is a core creative ability that can be\nchallenging for humans. Recent work has shown some proofs-of concept of Large\nlanguage Models' (LLMs) ability to generate cross-domain analogies. However,\nthe reliability and potential usefulness of this capacity for augmenting human\ncreative work has received little systematic exploration. In this paper, we\nsystematically explore LLMs capacity to augment cross-domain analogical\nreasoning. Across three studies, we found: 1) LLM-generated cross-domain\nanalogies were frequently judged as helpful in the context of a problem\nreformulation task (median 4 out of 5 helpfulness rating), and frequently (~80%\nof cases) led to observable changes in problem formulations, and 2) there was\nan upper bound of 25% of outputs bring rated as potentially harmful, with a\nmajority due to potentially upsetting content, rather than biased or toxic\ncontent. These results demonstrate the potential utility -- and risks -- of\nLLMs for augmenting cross-domain analogical creativity.", "published": "2023-02-27 15:54:57", "link": "http://arxiv.org/abs/2302.12832v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading\n  Comprehension", "abstract": "The conversational machine reading comprehension (CMRC) task aims to answer\nquestions in conversations, which has been a hot research topic in recent years\nbecause of its wide applications. However, existing CMRC benchmarks in which\neach conversation is assigned a static passage are inconsistent with real\nscenarios. Thus, model's comprehension ability towards real scenarios are hard\nto evaluate reasonably. To this end, we propose the first Chinese CMRC\nbenchmark Orca and further provide zero-shot/few-shot settings to evaluate\nmodel's generalization ability towards diverse domains. We collect 831\nhot-topic driven conversations with 4,742 turns in total. Each turn of a\nconversation is assigned with a response-related passage, aiming to evaluate\nmodel's comprehension ability more reasonably. The topics of conversations are\ncollected from social media platform and cover 33 domains, trying to be\nconsistent with real scenarios. Importantly, answers in Orca are all\nwell-annotated natural responses rather than the specific spans or short phrase\nin previous datasets. Besides, we implement three strong baselines to tackle\nthe challenge in Orca. The results indicate the great challenge of our CMRC\nbenchmark. Our datatset and checkpoints are available at\nhttps://github.com/nuochenpku/Orca.", "published": "2023-02-27 09:40:41", "link": "http://arxiv.org/abs/2302.13619v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hulk: Graph Neural Networks for Optimizing Regionally Distributed\n  Computing Systems", "abstract": "Large deep learning models have shown great potential for delivering\nexceptional results in various applications. However, the training process can\nbe incredibly challenging due to the models' vast parameter sizes, often\nconsisting of hundreds of billions of parameters. Common distributed training\nmethods, such as data parallelism, tensor parallelism, and pipeline\nparallelism, demand significant data communication throughout the process,\nleading to prolonged wait times for some machines in physically distant\ndistributed systems. To address this issue, we propose a novel solution called\nHulk, which utilizes a modified graph neural network to optimize distributed\ncomputing systems. Hulk not only optimizes data communication efficiency\nbetween different countries or even different regions within the same city, but\nalso provides optimal distributed deployment of models in parallel. For\nexample, it can place certain layers on a machine in a specific region or pass\nspecific parameters of a model to a machine in a particular location. By using\nHulk in experiments, we were able to improve the time efficiency of training\nlarge deep learning models on distributed systems by more than 20\\%. Our open\nsource collection of unlabeled data:https://github.com/DLYuanGod/Hulk.", "published": "2023-02-27 13:06:58", "link": "http://arxiv.org/abs/2302.13741v2", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Let's have a chat! A Conversation with ChatGPT: Technology,\n  Applications, and Limitations", "abstract": "The emergence of an AI-powered chatbot that can generate human-like sentences\nand write coherent essays has caught the world's attention. This paper\ndiscusses the historical overview of chatbots and the technology behind Chat\nGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,\npotential applications of ChatGPT in various domains, including healthcare,\neducation, and research, are highlighted. Despite promising results, there are\nseveral privacy and ethical concerns surrounding ChatGPT. In addition, we\nhighlight some of the important limitations of the current version of ChatGPT.\nWe also ask ChatGPT to provide its point of view and present its responses to\nseveral questions we attempt to answer.", "published": "2023-02-27 14:26:29", "link": "http://arxiv.org/abs/2302.13817v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Argument Mining using BERT and Self-Attention based Embeddings", "abstract": "Argument mining automatically identifies and extracts the structure of\ninference and reasoning conveyed in natural language arguments. To the best of\nour knowledge, most of the state-of-the-art works in this field have focused on\nusing tree-like structures and linguistic modeling. But, these approaches are\nnot able to model more complex structures which are often found in online\nforums and real world argumentation structures. In this paper, a novel\nmethodology for argument mining is proposed which employs attention-based\nembeddings for link prediction to model the causational hierarchies in typical\nargument structures prevalent in online discourse.", "published": "2023-02-27 15:52:31", "link": "http://arxiv.org/abs/2302.13906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values\n  behind Arguments by Leveraging Their Definitions", "abstract": "We describe our experiments for SemEval-2023 Task 4 on the identification of\nhuman values behind arguments (ValueEval). Because human values are subjective\nconcepts which require precise definitions, we hypothesize that incorporating\nthe definitions of human values (in the form of annotation instructions and\nvalidated survey items) during model training can yield better prediction\nperformance. We explore this idea and show that our proposed models perform\nbetter than the challenge organizers' baselines, with improvements in macro F1\nscores of up to 18%.", "published": "2023-02-27 16:23:11", "link": "http://arxiv.org/abs/2302.13925v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Full Stack Optimization of Transformer Inference: a Survey", "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.", "published": "2023-02-27 18:18:13", "link": "http://arxiv.org/abs/2302.14017v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The ROOTS Search Tool: Data Transparency for LLMs", "abstract": "ROOTS is a 1.6TB multilingual text corpus developed for the training of\nBLOOM, currently the largest language model explicitly accompanied by\ncommensurate data governance efforts. In continuation of these efforts, we\npresent the ROOTS Search Tool: a search engine over the entire ROOTS corpus\noffering both fuzzy and exact search capabilities. ROOTS is the largest corpus\nto date that can be investigated this way. The ROOTS Search Tool is\nopen-sourced and available on Hugging Face Spaces. We describe our\nimplementation and the possible use cases of our tool.", "published": "2023-02-27 18:45:18", "link": "http://arxiv.org/abs/2302.14035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Is Not All You Need: Aligning Perception with Language Models", "abstract": "A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.", "published": "2023-02-27 18:55:27", "link": "http://arxiv.org/abs/2302.14045v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A low latency attention module for streaming self-supervised speech\n  representation learning", "abstract": "The transformer is a fundamental building block in deep learning, and the\nattention mechanism is the transformer's core component. Self-supervised speech\nrepresentation learning (SSRL) represents a popular use-case for the\ntransformer architecture. Due to transformers' acausal behavior, the use of\ntransformers for SSRL has been predominantly focused on acausal applications.\nHowever, several media processing problems, such as speech processing, require\nreal-time solutions. In this paper, we present an implementation of the\nattention module that enables training of SSRL architectures with low compute\nand memory requirements, while allowing real-time inference with low and fixed\nlatency. The attention module proposed in this paper includes two components,\nstreaming attention (SA) and low-latency streaming attention (LLSA). The SA\nrepresents our proposal for an efficient streaming SSRL implementation, while\nthe LLSA solves the latency build-up problem of other streaming attention\narchitectures, such as the masked acausal attention (MAA), guaranteeing a\nlatency equal to one layer even when multiple layers are stacked. We present a\ncomparative analysis between the vanilla attention, which we will refer here as\nacausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with\nautomatic speech recognition as downstream task. When training on\nlibrispeech-clean-100 and testing on librispeech-test-clean, our low-latency\nattention module has a word error rate (WER) of 5.84%, which represents a\nsignificant improvement over the MAA (WER = 13.82%). Our implementation also\nreduces the inference latency from 1.92 to 0.16 seconds. The proposed\nlow-latency module preserves many of the benefits of conventional acausal\ntransformers, but also enables latency characteristics that make it applicable\nto real-time streaming applications.", "published": "2023-02-27 00:44:22", "link": "http://arxiv.org/abs/2302.13451v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Changes in Commuter Behavior from COVID-19 Lockdowns in the Atlanta\n  Metropolitan Area", "abstract": "This paper analyzes the impact of COVID-19 related lockdowns in the Atlanta,\nGeorgia metropolitan area by examining commuter patterns in three periods:\nprior to, during, and after the pandemic lockdown. A cellular phone location\ndataset is utilized in a novel pipeline to infer the home and work locations of\nthousands of users from the Density-based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithm. The coordinates derived from the clustering are\nput through a reverse geocoding process from which word embeddings are\nextracted in order to categorize the industry of each work place based on the\nworkplace name and Point of Interest (POI) mapping. Frequencies of commute from\nhome locations to work locations are analyzed in and across all three time\nperiods. Public health and economic factors are discussed to explain potential\nreasons for the observed changes in commuter patterns.", "published": "2023-02-27 04:24:13", "link": "http://arxiv.org/abs/2302.13512v1", "categories": ["cs.LG", "cs.CL", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Duration-aware pause insertion using pre-trained language model for\n  multi-speaker text-to-speech", "abstract": "Pause insertion, also known as phrase break prediction and phrasing, is an\nessential part of TTS systems because proper pauses with natural duration\nsignificantly enhance the rhythm and intelligibility of synthetic speech.\nHowever, conventional phrasing models ignore various speakers' different styles\nof inserting silent pauses, which can degrade the performance of the model\ntrained on a multi-speaker speech corpus. To this end, we propose more powerful\npause insertion frameworks based on a pre-trained language model. Our approach\nuses bidirectional encoder representations from transformers (BERT) pre-trained\non a large-scale text corpus, injecting speaker embedding to capture various\nspeaker characteristics. We also leverage duration-aware pause insertion for\nmore natural multi-speaker TTS. We develop and evaluate two types of models.\nThe first improves conventional phrasing models on the position prediction of\nrespiratory pauses (RPs), i.e., silent pauses at word transitions without\npunctuation. It performs speaker-conditioned RP prediction considering\ncontextual information and is used to demonstrate the effect of speaker\ninformation on the prediction. The second model is further designed for\nphoneme-based TTS models and performs duration-aware pause insertion,\npredicting both RPs and punctuation-indicated pauses (PIPs) that are\ncategorized by duration. The evaluation results show that our models improve\nthe precision and recall of pause insertion and the rhythm of synthetic speech.", "published": "2023-02-27 10:40:41", "link": "http://arxiv.org/abs/2302.13652v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Auxiliary Tasks In Multimodal Fusion Of Wav2vec 2.0 And BERT For\n  Multimodal Emotion Recognition", "abstract": "The lack of data and the difficulty of multimodal fusion have always been\nchallenges for multimodal emotion recognition (MER). In this paper, we propose\nto use pretrained models as upstream network, wav2vec 2.0 for audio modality\nand BERT for text modality, and finetune them in downstream task of MER to cope\nwith the lack of data. For the difficulty of multimodal fusion, we use a\nK-layer multi-head attention mechanism as a downstream fusion module. Starting\nfrom the MER task itself, we design two auxiliary tasks to alleviate the\ninsufficient fusion between modalities and guide the network to capture and\nalign emotion-related features. Compared to the previous state-of-the-art\nmodels, we achieve a better performance by 78.42% Weighted Accuracy (WA) and\n79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.", "published": "2023-02-27 10:59:08", "link": "http://arxiv.org/abs/2302.13661v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MoLE : Mixture of Language Experts for Multi-Lingual Automatic Speech\n  Recognition", "abstract": "Multi-lingual speech recognition aims to distinguish linguistic expressions\nin different languages and integrate acoustic processing simultaneously. In\ncontrast, current multi-lingual speech recognition research follows a\nlanguage-aware paradigm, mainly targeted to improve recognition performance\nrather than discriminate language characteristics. In this paper, we present a\nmulti-lingual speech recognition network named\nMixture-of-Language-Expert(MoLE), which digests speech in a variety of\nlanguages. Specifically, MoLE analyzes linguistic expression from input speech\nin arbitrary languages, activating a language-specific expert with a\nlightweight language tokenizer. The tokenizer not only activates experts, but\nalso estimates the reliability of the activation. Based on the reliability, the\nactivated expert and the language-agnostic expert are aggregated to represent\nlanguage-conditioned embedding for efficient speech recognition. Our proposed\nmodel is evaluated in 5 languages scenario, and the experimental results show\nthat our structure is advantageous on multi-lingual recognition, especially for\nspeech in low-resource language.", "published": "2023-02-27 13:26:17", "link": "http://arxiv.org/abs/2302.13750v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural\n  Networks", "abstract": "As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations. Our code implementation is available at\nhttps://github.com/ridgerchu/SpikeGPT.", "published": "2023-02-27 16:43:04", "link": "http://arxiv.org/abs/2302.13939v5", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Inseq: An Interpretability Toolkit for Sequence Generation Models", "abstract": "Past work in natural language processing interpretability focused mainly on\npopular classification tasks while largely overlooking generation settings,\npartly due to a lack of dedicated tools. In this work, we introduce Inseq, a\nPython library to democratize access to interpretability analyses of sequence\ngeneration models. Inseq enables intuitive and optimized extraction of models'\ninternal information and feature importance scores for popular decoder-only and\nencoder-decoder Transformers architectures. We showcase its potential by\nadopting it to highlight gender biases in machine translation models and locate\nfactual knowledge inside GPT-2. Thanks to its extensible interface supporting\ncutting-edge techniques such as contrastive feature attribution, Inseq can\ndrive future advances in explainable natural language generation, centralizing\ngood practices and enabling fair and reproducible model evaluations.", "published": "2023-02-27 16:45:50", "link": "http://arxiv.org/abs/2302.13942v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Systematic Rectification of Language Models via Dead-end Analysis", "abstract": "With adversarial or otherwise normal prompts, existing large language models\n(LLM) can be pushed to generate toxic discourses. One way to reduce the risk of\nLLMs generating undesired discourses is to alter the training of the LLM. This\ncan be very restrictive due to demanding computation requirements. Other\nmethods rely on rule-based or prompt-based token elimination, which are limited\nas they dismiss future tokens and the overall meaning of the complete\ndiscourse. Here, we center detoxification on the probability that the finished\ndiscourse is ultimately considered toxic. That is, at each point, we advise\nagainst token selections proportional to how likely a finished text from this\npoint will be toxic. To this end, we formally extend the dead-end theory from\nthe recent reinforcement learning (RL) literature to also cover uncertain\noutcomes. Our approach, called rectification, utilizes a separate but\nsignificantly smaller model for detoxification, which can be applied to diverse\nLLMs as long as they share the same vocabulary. Importantly, our method does\nnot require access to the internal representations of the LLM, but only the\ntoken probability distribution at each decoding step. This is crucial as many\nLLMs today are hosted in servers and only accessible through APIs. When applied\nto various LLMs, including GPT-3, our approach significantly improves the\ngenerated discourse compared to the base LLMs and other techniques in terms of\nboth the overall language and detoxification performance.", "published": "2023-02-27 17:47:53", "link": "http://arxiv.org/abs/2302.14003v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Valence and Arousal in Text with Multilingual Pre-trained\n  Transformers", "abstract": "The analysis of emotions expressed in text has numerous applications. In\ncontrast to categorical analysis, focused on classifying emotions according to\na pre-defined set of common classes, dimensional approaches can offer a more\nnuanced way to distinguish between different emotions. Still, dimensional\nmethods have been less studied in the literature. Considering a valence-arousal\ndimensional space, this work assesses the use of pre-trained Transformers to\npredict these two dimensions on a continuous scale, with input texts from\nmultiple languages and domains. We specifically combined multiple annotated\ndatasets from previous studies, corresponding to either emotional lexica or\nshort text documents, and evaluated models of multiple sizes and trained under\ndifferent settings. Our results show that model size can have a significant\nimpact on the quality of predictions, and that by fine-tuning a large model we\ncan confidently predict valence and arousal in multiple languages. We make\navailable the code, models, and supporting data.", "published": "2023-02-27 18:25:19", "link": "http://arxiv.org/abs/2302.14021v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multimodal Speech Recognition for Language-Guided Embodied Agents", "abstract": "Benchmarks for language-guided embodied agents typically assume text-based\ninstructions, but deployed agents will encounter spoken instructions. While\nAutomatic Speech Recognition (ASR) models can bridge the input gap, erroneous\nASR transcripts can hurt the agents' ability to complete tasks. In this work,\nwe propose training a multimodal ASR model to reduce errors in transcribing\nspoken instructions by considering the accompanying visual context. We train\nour model on a dataset of spoken instructions, synthesized from the ALFRED task\ncompletion dataset, where we simulate acoustic noise by systematically masking\nspoken words. We find that utilizing visual observations facilitates masked\nword recovery, with multimodal ASR models recovering up to 30% more masked\nwords than unimodal baselines. We also find that a text-trained embodied agent\nsuccessfully completes tasks more often by following transcribed instructions\nfrom multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr", "published": "2023-02-27 18:41:48", "link": "http://arxiv.org/abs/2302.14030v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Text-only domain adaptation for end-to-end ASR using integrated\n  text-to-mel-spectrogram generator", "abstract": "We propose an end-to-end Automatic Speech Recognition (ASR) system that can\nbe trained on transcribed speech data, text-only data, or a mixture of both.\nThe proposed model uses an integrated auxiliary block for text-based training.\nThis block combines a non-autoregressive multi-speaker text-to-mel-spectrogram\ngenerator with a GAN-based enhancer to improve the spectrogram quality. The\nproposed system can generate a mel-spectrogram dynamically during training. It\ncan be used to adapt the ASR model to a new domain by using text-only data from\nthis domain. We demonstrate that the proposed training method significantly\nimproves ASR accuracy compared to the system trained on transcribed speech\nonly. It also surpasses cascade TTS systems with the vocoder in the adaptation\nquality and training speed.", "published": "2023-02-27 18:47:55", "link": "http://arxiv.org/abs/2302.14036v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning", "abstract": "In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.", "published": "2023-02-27 19:53:49", "link": "http://arxiv.org/abs/2302.14115v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Structured Pruning of Self-Supervised Pre-trained Models for Speech\n  Recognition and Understanding", "abstract": "Self-supervised speech representation learning (SSL) has shown to be\neffective in various downstream tasks, but SSL models are usually large and\nslow. Model compression techniques such as pruning aim to reduce the model size\nand computation without degradation in accuracy. Prior studies focus on the\npruning of Transformers; however, speech models not only utilize a stack of\nTransformer blocks, but also combine a frontend network based on multiple\nconvolutional layers for low-level feature representation learning. This\nfrontend has a small size but a heavy computational cost. In this work, we\npropose three task-specific structured pruning methods to deal with such\nheterogeneous networks. Experiments on LibriSpeech and SLURP show that the\nproposed method is more accurate than the original wav2vec2-base with 10% to\n30% less computation, and is able to reduce the computation by 40% to 50%\nwithout any degradation.", "published": "2023-02-27 20:39:54", "link": "http://arxiv.org/abs/2302.14132v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic\n  Speech Processing", "abstract": "Paralinguistic speech processing is important in addressing many issues, such\nas sentiment and neurocognitive disorder analyses. Recently, Transformer has\nachieved remarkable success in the natural language processing field and has\ndemonstrated its adaptation to speech. However, previous works on Transformer\nin the speech field have not incorporated the properties of speech, leaving the\nfull potential of Transformer unexplored. In this paper, we consider the\ncharacteristics of speech and propose a general structure-based framework,\ncalled SpeechFormer++, for paralinguistic speech processing. More concretely,\nfollowing the component relationship in the speech signal, we design a unit\nencoder to model the intra- and inter-unit information (i.e., frames, phones,\nand words) efficiently. According to the hierarchical relationship, we utilize\nmerging blocks to generate features at different granularities, which is\nconsistent with the structural pattern in the speech signal. Moreover, a word\nencoder is introduced to integrate word-grained features into each unit\nencoder, which effectively balances fine-grained and coarse-grained\ninformation. SpeechFormer++ is evaluated on the speech emotion recognition\n(IEMOCAP & MELD), depression classification (DAIC-WOZ) and Alzheimer's disease\ndetection (Pitt) tasks. The results show that SpeechFormer++ outperforms the\nstandard Transformer while greatly reducing the computational cost.\nFurthermore, it delivers superior results compared to the state-of-the-art\napproaches.", "published": "2023-02-27 11:48:54", "link": "http://arxiv.org/abs/2302.14638v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reward Design with Language Models", "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying\nhuman notions of desired behavior may be difficult via reward functions or\nrequire many expert demonstrations. Can we instead cheaply design rewards using\na natural language interface? This paper explores how to simplify reward design\nby prompting a large language model (LLM) such as GPT-3 as a proxy reward\nfunction, where the user provides a textual prompt containing a few examples\n(few-shot) or a description (zero-shot) of the desired behavior. Our approach\nleverages this proxy reward function in an RL framework. Specifically, users\nspecify a prompt once at the beginning of training. During training, the LLM\nevaluates an RL agent's behavior against the desired behavior described by the\nprompt and outputs a corresponding reward signal. The RL agent then uses this\nreward to update its behavior. We evaluate whether our approach can train\nagents aligned with user objectives in the Ultimatum Game, matrix games, and\nthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agents\ntrained with our framework are well-aligned with the user's objectives and\noutperform RL agents trained with reward functions learned via supervised\nlearning", "published": "2023-02-27 22:09:35", "link": "http://arxiv.org/abs/2303.00001v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TOT: Topology-Aware Optimal Transport For Multimodal Hate Detection", "abstract": "Multimodal hate detection, which aims to identify harmful content online such\nas memes, is crucial for building a wholesome internet environment. Previous\nwork has made enlightening exploration in detecting explicit hate remarks.\nHowever, most of their approaches neglect the analysis of implicit harm, which\nis particularly challenging as explicit text markers and demographic visual\ncues are often twisted or missing. The leveraged cross-modal attention\nmechanisms also suffer from the distributional modality gap and lack logical\ninterpretability. To address these semantic gaps issues, we propose TOT: a\ntopology-aware optimal transport framework to decipher the implicit harm in\nmemes scenario, which formulates the cross-modal aligning problem as solutions\nfor optimal transportation plans. Specifically, we leverage an optimal\ntransport kernel method to capture complementary information from multiple\nmodalities. The kernel embedding provides a non-linear transformation ability\nto reproduce a kernel Hilbert space (RKHS), which reflects significance for\neliminating the distributional modality gap. Moreover, we perceive the topology\ninformation based on aligned representations to conduct bipartite graph path\nreasoning. The newly achieved state-of-the-art performance on two publicly\navailable benchmark datasets, together with further visual analysis,\ndemonstrate the superiority of TOT in capturing implicit cross-modal alignment.", "published": "2023-02-27 06:58:19", "link": "http://arxiv.org/abs/2303.09314v2", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Explanations for Automatic Speech Recognition", "abstract": "We address quality assessment for neural network based ASR by providing\nexplanations that help increase our understanding of the system and ultimately\nhelp build trust in the system. Compared to simple classification labels,\nexplaining transcriptions is more challenging as judging their correctness is\nnot straightforward and transcriptions as a variable-length sequence is not\nhandled by existing interpretable machine learning models. We provide an\nexplanation for an ASR transcription as a subset of audio frames that is both a\nminimal and sufficient cause of the transcription. To do this, we adapt\nexisting explainable AI (XAI) techniques from image classification-Statistical\nFault Localisation(SFL) and Causal. Additionally, we use an adapted version of\nLocal Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in\nour experiments. We evaluate the quality of the explanations generated by the\nproposed techniques over three different ASR ,Google API, the baseline model of\nSphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.", "published": "2023-02-27 11:09:19", "link": "http://arxiv.org/abs/2302.14062v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Medical Speech-to-Text Accuracy with Vision-Language\n  Pre-training Model", "abstract": "Automatic Speech Recognition (ASR) is a technology that converts spoken words\ninto text, facilitating interaction between humans and machines. One of the\nmost common applications of ASR is Speech-To-Text (STT) technology, which\nsimplifies user workflows by transcribing spoken words into text. In the\nmedical field, STT has the potential to significantly reduce the workload of\nclinicians who rely on typists to transcribe their voice recordings. However,\ndeveloping an STT model for the medical domain is challenging due to the lack\nof sufficient speech and text datasets. To address this issue, we propose a\nmedical-domain text correction method that modifies the output text of a\ngeneral STT system using the Vision Language Pre-training (VLP) method. VLP\ncombines textual and visual information to correct text based on image\nknowledge. Our extensive experiments demonstrate that the proposed method\noffers quantitatively and clinically significant improvements in STT\nperformance in the medical field. We further show that multi-modal\nunderstanding of image and text information outperforms single-modal\nunderstanding using only text information.", "published": "2023-02-27 08:06:04", "link": "http://arxiv.org/abs/2303.00091v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Varianceflow: High-Quality and Controllable Text-to-Speech using\n  Variance Information via Normalizing Flow", "abstract": "There are two types of methods for non-autoregressive text-to-speech models\nto learn the one-to-many relationship between text and speech effectively. The\nfirst one is to use an advanced generative framework such as normalizing flow\n(NF). The second one is to use variance information such as pitch or energy\ntogether when generating speech. For the second type, it is also possible to\ncontrol the variance factors by adjusting the variance values provided to a\nmodel. In this paper, we propose a novel model called VarianceFlow combining\nthe advantages of the two types. By modeling the variance with NF, VarianceFlow\npredicts the variance information more precisely with improved speech quality.\nAlso, the objective function of NF makes the model use the variance information\nand the text in a disentangled manner resulting in more precise variance\ncontrol. In experiments, VarianceFlow shows superior performance over other\nstate-of-the-art TTS models both in terms of speech quality and\ncontrollability.", "published": "2023-02-27 01:12:19", "link": "http://arxiv.org/abs/2302.13458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "3D Neural Beamforming for Multi-channel Speech Separation Against\n  Location Uncertainty", "abstract": "Multi-channel speech separation using speaker's directional information has\ndemonstrated significant gains over blind speech separation. However, it has\ntwo limitations. First, substantial performance degradation is observed when\nthe coming directions of two sounds are close. Second, the result highly relies\non the precise estimation of the speaker's direction. To overcome these issues,\nthis paper proposes 3D features and an associated 3D neural beamformer for\nmulti-channel speech separation. Previous works in this area are extended in\ntwo important directions. First, the traditional 1D directional beam patterns\nare generalized to 3D. This enables the model to extract speech from any target\nregion in the 3D space. Thus, speakers with similar directions but different\nelevations or distances become separable. Second, to handle the speaker\nlocation uncertainty, previously proposed spatial feature is extended to a new\n3D region feature. The proposed 3D region feature and 3D neural beamformer are\nevaluated under an in-car scenario. Experimental results demonstrated that the\ncombination of 3D feature and 3D beamformer can achieve comparable performance\nto the separation model with ground truth speaker location as input.", "published": "2023-02-27 01:27:37", "link": "http://arxiv.org/abs/2302.13462v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VE-KWS: Visual Modality Enhanced End-to-End Keyword Spotting", "abstract": "The performance of the keyword spotting (KWS) system based on audio modality,\ncommonly measured in false alarms and false rejects, degrades significantly\nunder the far field and noisy conditions. Therefore, audio-visual keyword\nspotting, which leverages complementary relationships over multiple modalities,\nhas recently gained much attention. However, current studies mainly focus on\ncombining the exclusively learned representations of different modalities,\ninstead of exploring the modal relationships during each respective modeling.\nIn this paper, we propose a novel visual modality enhanced end-to-end KWS\nframework (VE-KWS), which fuses audio and visual modalities from two aspects.\nThe first one is utilizing the speaker location information obtained from the\nlip region in videos to assist the training of multi-channel audio beamformer.\nBy involving the beamformer as an audio enhancement module, the acoustic\ndistortions, caused by the far field or noisy environments, could be\nsignificantly suppressed. The other one is conducting cross-attention between\ndifferent modalities to capture the inter-modal relationships and help the\nrepresentation learning of each modality. Experiments on the MSIP challenge\ncorpus show that our proposed model achieves 2.79% false rejection rate and\n2.95% false alarm rate on the Eval set, resulting in a new SOTA performance\ncompared with the top-ranking systems in the ICASSP2022 MISP challenge.", "published": "2023-02-27 05:25:42", "link": "http://arxiv.org/abs/2302.13523v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DST: Deformable Speech Transformer for Emotion Recognition", "abstract": "Enabled by multi-head self-attention, Transformer has exhibited remarkable\nresults in speech emotion recognition (SER). Compared to the original full\nattention mechanism, window-based attention is more effective in learning\nfine-grained features while greatly reducing model redundancy. However,\nemotional cues are present in a multi-granularity manner such that the\npre-defined fixed window can severely degrade the model flexibility. In\naddition, it is difficult to obtain the optimal window settings manually. In\nthis paper, we propose a Deformable Speech Transformer, named DST, for SER\ntask. DST determines the usage of window sizes conditioned on input speech via\na light-weight decision network. Meanwhile, data-dependent offsets derived from\nacoustic features are utilized to adjust the positions of the attention\nwindows, allowing DST to adaptively discover and attend to the valuable\ninformation embedded in the speech. Extensive experiments on IEMOCAP and MELD\ndemonstrate the superiority of DST.", "published": "2023-02-27 12:52:23", "link": "http://arxiv.org/abs/2302.13729v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diagonal State Space Augmented Transformers for Speech Recognition", "abstract": "We improve on the popular conformer architecture by replacing the depthwise\ntemporal convolutions with diagonal state space (DSS) models. DSS is a recently\nintroduced variant of linear RNNs obtained by discretizing a linear dynamical\nsystem with a diagonal state transition matrix. DSS layers project the input\nsequence onto a space of orthogonal polynomials where the choice of basis\nfunctions, metric and support is controlled by the eigenvalues of the\ntransition matrix. We compare neural transducers with either conformer or our\nproposed DSS-augmented transformer (DSSformer) encoders on three public\ncorpora: Switchboard English conversational telephone speech 300 hours,\nSwitchboard+Fisher 2000 hours, and a spoken archive of holocaust survivor\ntestimonials called MALACH 176 hours. On Switchboard 300/2000 hours, we reach a\nsingle model performance of 8.9%/6.7% WER on the combined test set of the Hub5\n2000 evaluation, respectively, and on MALACH we improve the WER by 7% relative\nover the previous best published result. In addition, we present empirical\nevidence suggesting that DSS layers learn damped Fourier basis functions where\nthe attenuation coefficients are layer specific whereas the frequency\ncoefficients converge to almost identical linearly-spaced values across all\nlayers.", "published": "2023-02-27 20:08:36", "link": "http://arxiv.org/abs/2302.14120v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complex Clipping for Improved Generalization in Machine Learning", "abstract": "For many machine learning applications, a common input representation is a\nspectrogram. The underlying representation for a spectrogram is a short time\nFourier transform (STFT) which gives complex values. The spectrogram uses the\nmagnitude of these complex values, a commonly used detector. Modern machine\nlearning systems are commonly overparameterized, where possible\nill-conditioning problems are ameliorated by regularization. The common use of\nrectified linear unit (ReLU) activation functions between layers of a deep net\nhas been shown to help this regularization, improving system performance. We\nextend this idea of ReLU activation to detection for the complex STFT,\nproviding a simple-to-compute modified and regularized spectrogram, which\npotentially results in better behaved training. We then confirmed the benefit\nof this approach on a noisy acoustic data set used for a real-world\napplication. Generalization performance improved substantially. This approach\nmight benefit other applications which use time-frequency mappings, for\nacoustic, audio, and other applications.", "published": "2023-02-27 05:48:05", "link": "http://arxiv.org/abs/2302.13527v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Continuous descriptor-based control for deep audio synthesis", "abstract": "Despite significant advances in deep models for music generation, the use of\nthese techniques remains restricted to expert users. Before being democratized\namong musicians, generative models must first provide expressive control over\nthe generation, as this conditions the integration of deep generative models in\ncreative workflows. In this paper, we tackle this issue by introducing a deep\ngenerative audio model providing expressive and continuous descriptor-based\ncontrol, while remaining lightweight enough to be embedded in a hardware\nsynthesizer. We enforce the controllability of real-time generation by\nexplicitly removing salient musical features in the latent space using an\nadversarial confusion criterion. User-specified features are then reintroduced\nas additional conditioning information, allowing for continuous control of the\ngeneration, akin to a synthesizer knob. We assess the performance of our method\non a wide variety of sounds including instrumental, percussive and speech\nrecordings while providing both timbre and attributes transfer, allowing new\nways of generating sounds.", "published": "2023-02-27 06:40:11", "link": "http://arxiv.org/abs/2302.13542v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Predicting EEG Responses to Attended Speech via Deep Neural Networks for\n  Speech", "abstract": "Attending to the speech stream of interest in multi-talker environments can\nbe a challenging task, particularly for listeners with hearing impairment.\nResearch suggests that neural responses assessed with electroencephalography\n(EEG) are modulated by listener`s auditory attention, revealing selective\nneural tracking (NT) of the attended speech. NT methods mostly rely on\nhand-engineered acoustic and linguistic speech features to predict the neural\nresponse. Only recently, deep neural network (DNN) models without specific\nlinguistic information have been used to extract speech features for NT,\ndemonstrating that speech features in hierarchical DNN layers can predict\nneural responses throughout the auditory pathway. In this study, we go one step\nfurther to investigate the suitability of similar DNN models for speech to\npredict neural responses to competing speech observed in EEG. We recorded EEG\ndata using a 64-channel acquisition system from 17 listeners with normal\nhearing instructed to attend to one of two competing talkers. Our data revealed\nthat EEG responses are significantly better predicted by DNN-extracted speech\nfeatures than by hand-engineered acoustic features. Furthermore, analysis of\nhierarchical DNN layers showed that early layers yielded the highest\npredictions. Moreover, we found a significant increase in auditory attention\nclassification accuracies with the use of DNN-extracted speech features over\nthe use of hand-engineered acoustic features. These findings open a new avenue\nfor development of new NT measures to evaluate and further advance hearing\ntechnology.", "published": "2023-02-27 07:12:16", "link": "http://arxiv.org/abs/2302.13553v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "A Comparative Analysis Of Latent Regressor Losses For Singing Voice\n  Conversion", "abstract": "Previous research has shown that established techniques for spoken voice\nconversion (VC) do not perform as well when applied to singing voice conversion\n(SVC). We propose an alternative loss component in a loss function that is\notherwise well-established among VC tasks, which has been shown to improve our\nmodel's SVC performance. We first trained a singer identity embedding (SIE)\nnetwork on mel-spectrograms of singer recordings to produce singer-specific\nvariance encodings using contrastive learning. We subsequently trained a\nwell-known autoencoder framework (AutoVC) conditioned on these SIEs, and\nmeasured differences in SVC performance when using different latent regressor\nloss components. We found that using this loss w.r.t. SIEs leads to better\nperformance than w.r.t. bottleneck embeddings, where converted audio is more\nnatural and specific towards target singers. The inclusion of this loss\ncomponent has the advantage of explicitly forcing the network to reconstruct\nwith timbral similarity, and also negates the effect of poor disentanglement in\nAutoVC's bottleneck embeddings. We demonstrate peculiar diversity between\ncomputational and human evaluations on singer-converted audio clips, which\nhighlights the necessity of both. We also propose a pitch-matching mechanism\nbetween source and target singers to ensure these evaluations are not\ninfluenced by differences in pitch register.", "published": "2023-02-27 11:26:57", "link": "http://arxiv.org/abs/2302.13678v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech", "abstract": "The goal of this work is zero-shot text-to-speech synthesis, with speaking\nstyles and voices learnt from facial characteristics. Inspired by the natural\nfact that people can imagine the voice of someone when they look at his or her\nface, we introduce a face-styled diffusion text-to-speech (TTS) model within a\nunified framework learnt from visible attributes, called Face-TTS. This is the\nfirst time that face images are used as a condition to train a TTS model.\n  We jointly train cross-model biometrics and TTS models to preserve speaker\nidentity between face images and generated speech segments. We also propose a\nspeaker feature binding loss to enforce the similarity of the generated and the\nground truth speech segments in speaker embedding space. Since the biometric\ninformation is extracted directly from the face image, our method does not\nrequire extra fine-tuning steps to generate speech from unseen and unheard\nspeakers. We train and evaluate the model on the LRS3 dataset, an in-the-wild\naudio-visual corpus containing background noise and diverse speaking styles.\nThe project page is https://facetts.github.io.", "published": "2023-02-27 11:59:28", "link": "http://arxiv.org/abs/2302.13700v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Cross-modal Face- and Voice-style Transfer", "abstract": "Image-to-image translation and voice conversion enable the generation of a\nnew facial image and voice while maintaining some of the semantics such as a\npose in an image and linguistic content in audio, respectively. They can aid in\nthe content-creation process in many applications. However, as they are limited\nto the conversion within each modality, matching the impression of the\ngenerated face and voice remains an open question. We propose a cross-modal\nstyle transfer framework called XFaVoT that jointly learns four tasks: image\ntranslation and voice conversion tasks with audio or image guidance, which\nenables the generation of ``face that matches given voice\" and ``voice that\nmatches given face\", and intra-modality translation tasks with a single\nframework. Experimental results on multiple datasets show that XFaVoT achieves\ncross-modal style translation of image and voice, outperforming baselines in\nterms of quality, diversity, and face-voice correspondence.", "published": "2023-02-27 14:39:50", "link": "http://arxiv.org/abs/2302.13838v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Renaissance canons with asymmetric schemes", "abstract": "By a \"scheme\" of a musical canon, we mean the time and pitch displacement of\neach entering voice. When the time displacements are unequal, achieving\nconsonant sonorities is especially challenging. Using a first-species\ntheoretical model, we quantify the flexibility of schemes that Renaissance\ncomposers used or could have used. We craft an algorithm to compute this\nflexibility value precisely (finding in the process that it is an algebraic\ninteger). We find that Palestrina consistently selected some of the most\nflexible schemes, more so than his predecessors, but that he by no means\nexhausted all feasible schemes. To add support to the model, we present two new\ncompositions within the limits of the style utilizing unexplored canonic\nschemes.\n  In the Online Supplement (attached via Papers with Code), we provide MIDI\nrealizations of the musical examples and Sage code used in the numerical\ncomputations.", "published": "2023-02-27 21:41:06", "link": "http://arxiv.org/abs/2302.14160v2", "categories": ["math.NT", "cs.SD", "eess.AS", "math.CO", "00A65 (Primary), 68R10, 05C38, 90B10, 11R06 (Secondary)"], "primary_category": "math.NT"}
{"title": "HalluAudio: Hallucinating Frequency as Concepts for Few-Shot Audio\n  Classification", "abstract": "Few-shot audio classification is an emerging topic that attracts more and\nmore attention from the research community. Most existing work ignores the\nspecificity of the form of the audio spectrogram and focuses largely on the\nembedding space borrowed from image tasks, while in this work, we aim to take\nadvantage of this special audio format and propose a new method by\nhallucinating high-frequency and low-frequency parts as structured concepts.\nExtensive experiments on ESC-50 and our curated balanced Kaggle18 dataset show\nthe proposed method outperforms the baseline by a notable margin. The way that\nour method hallucinates high-frequency and low-frequency parts also enables its\ninterpretability and opens up new potentials for the few-shot audio\nclassification.", "published": "2023-02-27 23:56:31", "link": "http://arxiv.org/abs/2302.14204v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit", "abstract": "Data augmentations are known to improve robustness in speech-processing\ntasks. In this study, we summarize and compare different data augmentation\nstrategies using S3PRL toolkit. We explore how HuBERT and wav2vec perform using\ndifferent augmentation techniques (SpecAugment, Gaussian Noise, Speed\nPerturbation) for Phoneme Recognition (PR) and Automatic Speech Recognition\n(ASR) tasks. We evaluate model performance in terms of phoneme error rate (PER)\nand word error rate (WER). From the experiments, we observed that SpecAugment\nslightly improves the performance of HuBERT and wav2vec on the original\ndataset. Also, we show that models trained using the Gaussian Noise and Speed\nPerturbation dataset are more robust when tested with augmented test sets.", "published": "2023-02-27 20:46:36", "link": "http://arxiv.org/abs/2303.00510v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Visual Forced Alignment: Learning to Align Transcription with\n  Talking Face Video", "abstract": "Forced alignment refers to a technology that time-aligns a given\ntranscription with a corresponding speech. However, as the forced alignment\ntechnologies have developed using speech audio, they might fail in alignment\nwhen the input speech audio is noise-corrupted or is not accessible. We focus\non that there is another component that the speech can be inferred from, the\nspeech video (i.e., talking face video). Since the drawbacks of audio-based\nforced alignment can be complemented using the visual information when the\naudio signal is under poor condition, we try to develop a novel video-based\nforced alignment method. However, different from audio forced alignment, it is\nchallenging to develop a reliable visual forced alignment technology for the\nfollowing two reasons: 1) Visual Speech Recognition (VSR) has a much lower\nperformance compared to audio-based Automatic Speech Recognition (ASR), and 2)\nthe translation from text to video is not reliable, so the method typically\nused for building audio forced alignment cannot be utilized in developing\nvisual forced alignment. In order to alleviate these challenges, in this paper,\nwe propose a new method that is appropriate for visual forced alignment, namely\nDeep Visual Forced Alignment (DVFA). The proposed DVFA can align the input\ntranscription (i.e., sentence) with the talking face video without accessing\nthe speech audio. Moreover, by augmenting the alignment task with anomaly case\ndetection, DVFA can detect mismatches between the input transcription and the\ninput video while performing the alignment. Therefore, we can robustly align\nthe text with the talking face video even if there exist error words in the\ntext. Through extensive experiments, we show the effectiveness of the proposed\nDVFA not only in the alignment task but also in interpreting the outputs of VSR\nmodels.", "published": "2023-02-27 02:59:50", "link": "http://arxiv.org/abs/2303.08670v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
