{"title": "Live Blog Corpus for Summarization", "abstract": "Live blogs are an increasingly popular news format to cover breaking news and\nlive events in online journalism. Online news websites around the world are\nusing this medium to give their readers a minute by minute update on an event.\nGood summaries enhance the value of the live blogs for a reader but are often\nnot available. In this paper, we study a way of collecting corpora for\nautomatic live blog summarization. In an empirical evaluation using well-known\nstate-of-the-art summarization systems, we show that live blogs corpus poses\nnew challenges in the field of summarization. We make our tools publicly\navailable to reconstruct the corpus to encourage the research community and\nreplicate our results.", "published": "2018-02-27 13:51:30", "link": "http://arxiv.org/abs/1802.09884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Idiomatic and Literal Expressions Using Topic Models and\n  Intensity of Emotions", "abstract": "We describe an algorithm for automatic classification of idiomatic and\nliteral expressions. Our starting point is that words in a given text segment,\nsuch as a paragraph, that are highranking representatives of a common topic of\ndiscussion are less likely to be a part of an idiomatic expression. Our\nadditional hypothesis is that contexts in which idioms occur, typically, are\nmore affective and therefore, we incorporate a simple analysis of the intensity\nof the emotions expressed by the contexts. We investigate the bag of words\ntopic representation of one to three paragraphs containing an expression that\nshould be classified as idiomatic or literal (a target phrase). We extract\ntopics from paragraphs containing idioms and from paragraphs containing\nliterals using an unsupervised clustering method, Latent Dirichlet Allocation\n(LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of\nnon-compositionality, we assume that they usually present different semantics\nthan the words used in the local topic. We treat idioms as semantic outliers,\nand the identification of a semantic shift as outlier detection. Thus, this\ntopic representation allows us to differentiate idioms from literals using\nlocal semantic contexts. Our results are encouraging.", "published": "2018-02-27 15:20:43", "link": "http://arxiv.org/abs/1802.09961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Word-Character Approach to Abstractive Summarization", "abstract": "Automatic abstractive text summarization is an important and challenging\nresearch topic of natural language processing. Among many widely used\nlanguages, the Chinese language has a special property that a Chinese character\ncontains rich information comparable to a word. Existing Chinese text\nsummarization methods, either adopt totally character-based or word-based\nrepresentations, fail to fully exploit the information carried by both\nrepresentations. To accurately capture the essence of articles, we propose a\nhybrid word-character approach (HWC) which preserves the advantages of both\nword-based and character-based representations. We evaluate the advantage of\nthe proposed HWC approach by applying it to two existing methods, and discover\nthat it generates state-of-the-art performance with a margin of 24 ROUGE points\non a widely used dataset LCSTS. In addition, we find an issue contained in the\nLCSTS dataset and offer a script to remove overlapping pairs (a summary and a\nshort text) to create a clean dataset for the community. The proposed HWC\napproach also generates the best performance on the new, clean LCSTS dataset.", "published": "2018-02-27 15:31:11", "link": "http://arxiv.org/abs/1802.09968v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Text Summarization using Neural Networks", "abstract": "Text Summarization has been an extensively studied problem. Traditional\napproaches to text summarization rely heavily on feature engineering. In\ncontrast to this, we propose a fully data-driven approach using feedforward\nneural networks for single document summarization. We train and evaluate the\nmodel on standard DUC 2002 dataset which shows results comparable to the state\nof the art models. The proposed model is scalable and is able to produce the\nsummary of arbitrarily sized documents by breaking the original document into\nfixed sized parts and then feeding it recursively to the network.", "published": "2018-02-27 19:48:51", "link": "http://arxiv.org/abs/1802.10137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech Detection: A Solved Problem? The Challenging Case of Long\n  Tail on Twitter", "abstract": "In recent years, the increasing propagation of hate speech on social media\nand the urgent need for effective counter-measures have drawn significant\ninvestment from governments, companies, and researchers. A large number of\nmethods have been developed for automated hate speech detection online. This\naims to classify textual content into non-hate or hate speech, in which case\nthe method may also identify the targeting characteristics (i.e., types of\nhate, such as race, and religion) in the hate speech. However, we notice\nsignificant difference between the performance of the two (i.e., non-hate v.s.\nhate). In this work, we argue for a focus on the latter problem for practical\nreasons. We show that it is a much more challenging task, as our analysis of\nthe language in the typical datasets shows that hate speech lacks unique,\ndiscriminative features and therefore is found in the 'long tail' in a dataset\nthat is difficult to discover. We then propose Deep Neural Network structures\nserving as feature extractors that are particularly effective for capturing the\nsemantics of hate speech. Our methods are evaluated on the largest collection\nof hate speech datasets based on Twitter, and are shown to be able to\noutperform the best performing method by up to 5 percentage points in\nmacro-average F1, or 8 percentage points in the more challenging case of\nidentifying hateful content.", "published": "2018-02-27 20:49:51", "link": "http://arxiv.org/abs/1803.03662v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Neural Networks for Toxic Comment Classification", "abstract": "Flood of information is produced in a daily basis through the global Internet\nusage arising from the on-line interactive communications among users. While\nthis situation contributes significantly to the quality of human life,\nunfortunately it involves enormous dangers, since on-line texts with high\ntoxicity can cause personal attacks, on-line harassment and bullying behaviors.\nThis has triggered both industrial and research community in the last few years\nwhile there are several tries to identify an efficient model for on-line toxic\ncomment prediction. However, these steps are still in their infancy and new\napproaches and frameworks are required. On parallel, the data explosion that\nappears constantly, makes the construction of new machine learning\ncomputational tools for managing this information, an imperative need.\nThankfully advances in hardware, cloud computing and big data management allow\nthe development of Deep Learning approaches appearing very promising\nperformance so far. For text classification in particular the use of\nConvolutional Neural Networks (CNN) have recently been proposed approaching\ntext analytics in a modern manner emphasizing in the structure of words in a\ndocument. In this work, we employ this approach to discover toxic comments in a\nlarge pool of documents provided by a current Kaggle's competition regarding\nWikipedia's talk page edits. To justify this decision we choose to compare CNNs\nagainst the traditional bag-of-words approach for text analysis combined with a\nselection of algorithms proven to be very effective in text classification. The\nreported results provide enough evidence that CNN enhance toxic comment\nclassification reinforcing research interest towards this direction.", "published": "2018-02-27 15:11:28", "link": "http://arxiv.org/abs/1802.09957v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gaussian meta-embeddings for efficient scoring of a heavy-tailed PLDA\n  model", "abstract": "Embeddings in machine learning are low-dimensional representations of complex\ninput patterns, with the property that simple geometric operations like\nEuclidean distances and dot products can be used for classification and\ncomparison tasks. The proposed meta-embeddings are special embeddings that live\nin more general inner product spaces. They are designed to propagate\nuncertainty to the final output in speaker recognition and similar\napplications. The familiar Gaussian PLDA model (GPLDA) can be re-formulated as\nan extractor for Gaussian meta-embeddings (GMEs), such that likelihood ratio\nscores are given by Hilbert space inner products between Gaussian likelihood\nfunctions. GMEs extracted by the GPLDA model have fixed precisions and do not\npropagate uncertainty. We show that a generalization to heavy-tailed PLDA gives\nGMEs with variable precisions, which do propagate uncertainty. Experiments on\nNIST SRE 2010 and 2016 show that the proposed method applied to i-vectors\nwithout length normalization is up to 20% more accurate than GPLDA applied to\nlength-normalized ivectors.", "published": "2018-02-27 08:55:05", "link": "http://arxiv.org/abs/1802.09777v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over\n  Disparate Label Spaces", "abstract": "We combine multi-task learning and semi-supervised learning by inducing a\njoint embedding space between disparate label spaces and learning transfer\nfunctions between label embeddings, enabling us to jointly leverage unlabelled\ndata and auxiliary, annotated datasets. We evaluate our approach on a variety\nof sequence classification tasks with disparate label spaces. We outperform\nstrong single and multi-task baselines and achieve a new state-of-the-art for\ntopic-based sentiment analysis.", "published": "2018-02-27 14:38:43", "link": "http://arxiv.org/abs/1802.09913v2", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep factorization for speech signal", "abstract": "Various informative factors mixed in speech signals, leading to great\ndifficulty when decoding any of the factors. An intuitive idea is to factorize\neach speech frame into individual informative factors, though it turns out to\nbe highly difficult. Recently, we found that speaker traits, which were assumed\nto be long-term distributional properties, are actually short-time patterns,\nand can be learned by a carefully designed deep neural network (DNN). This\ndiscovery motivated a cascade deep factorization (CDF) framework that will be\npresented in this paper. The proposed framework infers speech factors in a\nsequential way, where factors previously inferred are used as conditional\nvariables when inferring other factors. We will show that this approach can\neffectively factorize speech signals, and using these factors, the original\nspeech spectrum can be recovered with a high accuracy. This factorization and\nreconstruction approach provides potential values for many speech processing\ntasks, e.g., speaker recognition and emotion recognition, as will be\ndemonstrated in the paper.", "published": "2018-02-27 12:45:16", "link": "http://arxiv.org/abs/1803.00886v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Extended Long Short-term Memory and Dependent Bidirectional Recurrent\n  Neural Network", "abstract": "In this work, we first analyze the memory behavior in three recurrent neural\nnetworks (RNN) cells; namely, the simple RNN (SRN), the long short-term memory\n(LSTM) and the gated recurrent unit (GRU), where the memory is defined as a\nfunction that maps previous elements in a sequence to the current output. Our\nstudy shows that all three of them suffer rapid memory decay. Then, to\nalleviate this effect, we introduce trainable scaling factors that act like an\nattention mechanism to adjust memory decay adaptively. The new design is called\nthe extended LSTM (ELSTM). Finally, to design a system that is robust to\nprevious erroneous predictions, we propose a dependent bidirectional recurrent\nneural network (DBRNN). Extensive experiments are conducted on different\nlanguage tasks to demonstrate the superiority of the proposed ELSTM and DBRNN\nsolutions. The ELTSM has achieved up to 30% increase in the labeled attachment\nscore (LAS) as compared to LSTM and GRU in the dependency parsing (DP) task.\nOur models also outperform other state-of-the-art models such as bi-attention\nand convolutional sequence to sequence (convseq2seq) by close to 10% in the\nLAS. The code is released as an open source\n(https://github.com/yuanhangsu/ELSTM-DBRNN)", "published": "2018-02-27 02:47:13", "link": "http://arxiv.org/abs/1803.01686v5", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Effect of Transducer Positioning in Active Noise Control", "abstract": "Research in traditional Active Noise Control(ANC) often abstracts acoustic\nchannels with band-limited filter coefficients. This is a limitation in\nexploring structural and positional aspects of ANC. As a solution to this, we\npropose the use of room acoustic models in ANC research. As a use case, we\ndemonstrate anti-noise source position optimization using room acoustics models\nin achieving better noise control. Using numerical simulations, we show that\nlevel of cancellation can be improved up to 7.34 dB. All the codes and results\nare available in the Github repository https://github.com/cksajil/ancram in the\nspirit of reproducible research.", "published": "2018-02-27 18:29:14", "link": "http://arxiv.org/abs/1802.10058v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interplay between musical practices and tuning in the marimba de chonta\n  music", "abstract": "In the Pacific Coast of Colombia there is a type of marimba called marimba de\nchonta, which provides the melodic and harmonic contour for traditional music\nwith characteristic chants and dances. The tunings of this marimba are based on\nthe voice of female singers and allows musical practices, as a transposition\nthat preserves relative distances between bars. Here we show that traditional\ntunings are consistent with isotonic scales, and that they have changed in the\nlast three decades due to the influence of Western music. Specifically, low\noctaves have changed into just octaves. Additionally, consonance properties of\nthis instrument include the occurrence of a broad minimum of dissonance that is\nused in the musical practices, while the narrow local peaks of dissonance are\navoided. We found that the main reason for this is the occurrence of\nuncertainties in the tunings with respect to the mathematical successions of\nisotonic scales. We conclude that in this music the emergence of tunings and\nmusical practices cannot be considered as separate issues. Consonance, timbre,\nand musical practices are entangled.", "published": "2018-02-27 21:03:37", "link": "http://arxiv.org/abs/1802.10162v1", "categories": ["cs.SD", "eess.AS", "91Cxx, 91Dxx, 91Exx, 91Fxx", "H.5.5; J.5"], "primary_category": "cs.SD"}
{"title": "Convolutional Neural Network Achieves Human-level Accuracy in Music\n  Genre Classification", "abstract": "Music genre classification is one example of content-based analysis of music\nsignals. Traditionally, human-engineered features were used to automatize this\ntask and 61% accuracy has been achieved in the 10-genre classification.\nHowever, it's still below the 70% accuracy that humans could achieve in the\nsame task. Here, we propose a new method that combines knowledge of human\nperception study in music genre classification and the neurophysiology of the\nauditory system. The method works by training a simple convolutional neural\nnetwork (CNN) to classify a short segment of the music signal. Then, the genre\nof a music is determined by splitting it into short segments and then combining\nCNN's predictions from all short segments. After training, this method achieves\nhuman-level (70%) accuracy and the filters learned in the CNN resemble the\nspectrotemporal receptive field (STRF) in the auditory system.", "published": "2018-02-27 03:08:55", "link": "http://arxiv.org/abs/1802.09697v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
