{"title": "Learning to Discover, Ground and Use Words with Segmental Neural\n  Language Models", "abstract": "We propose a segmental neural language model that combines the generalization\npower of neural networks with the ability to discover word-like units that are\nlatent in unsegmented character sequences. In contrast to previous segmentation\nmodels that treat word segmentation as an isolated task, our model unifies word\ndiscovery, learning how words fit together to form sentences, and, by\nconditioning the model on visual context, how words' meanings ground in\nrepresentations of non-linguistic modalities. Experiments show that the\nunconditional model learns predictive distributions better than character LSTM\nmodels, discovers words competitively with nonparametric Bayesian word\nsegmentation models, and that modeling language conditional on visual context\nimproves performance on both.", "published": "2018-11-23 03:39:27", "link": "http://arxiv.org/abs/1811.09353v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural language understanding for task oriented dialog in the\n  biomedical domain in a low resources context", "abstract": "In the biomedical domain, the lack of sharable datasets often limit the\npossibility of developing natural language processing systems, especially\ndialogue applications and natural language understanding models. To overcome\nthis issue, we explore data generation using templates and terminologies and\ndata augmentation approaches. Namely, we report our experiments using\nparaphrasing and word representations learned on a large EHR corpus with\nFasttext and ELMo, to learn a NLU model without any available dataset. We\nevaluate on a NLU task of natural language queries in EHRs divided in\nslot-filling and intent classification sub-tasks. On the slot-filling task, we\nobtain a F-score of 0.76 with the ELMo representation; and on the\nclassification task, a mean F-score of 0.71. Our results show that this method\ncould be used to develop a baseline system.", "published": "2018-11-23 10:20:02", "link": "http://arxiv.org/abs/1811.09417v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Neural Network for Sequence-to-Sequences Learning", "abstract": "In recent years, the sequence-to-sequence learning neural networks with\nattention mechanism have achieved great progress. However, there are still\nchallenges, especially for Neural Machine Translation (NMT), such as lower\ntranslation quality on long sentences. In this paper, we present a hierarchical\ndeep neural network architecture to improve the quality of long sentences\ntranslation. The proposed network embeds sequence-to-sequence neural networks\ninto a two-level category hierarchy by following the coarse-to-fine paradigm.\nLong sentences are input by splitting them into shorter sequences, which can be\nwell processed by the coarse category network as the long distance dependencies\nfor short sentences is able to be handled by network based on\nsequence-to-sequence neural network. Then they are concatenated and corrected\nby the fine category network. The experiments shows that our method can achieve\nsuperior results with higher BLEU(Bilingual Evaluation Understudy) scores,\nlower perplexity and better performance in imitating expression style and words\nusage than the traditional networks.", "published": "2018-11-23 17:40:30", "link": "http://arxiv.org/abs/1811.09575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Words Can Shift: Dynamically Adjusting Word Representations Using\n  Nonverbal Behaviors", "abstract": "Humans convey their intentions through the usage of both verbal and nonverbal\nbehaviors during face-to-face communication. Speaker intentions often vary\ndynamically depending on different nonverbal contexts, such as vocal patterns\nand facial expressions. As a result, when modeling human language, it is\nessential to not only consider the literal meaning of the words but also the\nnonverbal contexts in which these words appear. To better model human language,\nwe first model expressive nonverbal representations by analyzing the\nfine-grained visual and acoustic patterns that occur during word segments. In\naddition, we seek to capture the dynamic nature of nonverbal intents by\nshifting word representations based on the accompanying nonverbal behaviors. To\nthis end, we propose the Recurrent Attended Variation Embedding Network (RAVEN)\nthat models the fine-grained structure of nonverbal subword sequences and\ndynamically shifts word representations based on nonverbal cues. Our proposed\nmodel achieves competitive performance on two publicly available datasets for\nmultimodal sentiment analysis and emotion recognition. We also visualize the\nshifted word representations in different nonverbal contexts and summarize\ncommon patterns regarding multimodal variations of word representations.", "published": "2018-11-23 05:13:38", "link": "http://arxiv.org/abs/1811.09362v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine Grained Classification of Personal Data Entities", "abstract": "Entity Type Classification can be defined as the task of assigning category\nlabels to entity mentions in documents. While neural networks have recently\nimproved the classification of general entity mentions, pattern matching and\nother systems continue to be used for classifying personal data entities (e.g.\nclassifying an organization as a media company or a government institution for\nGDPR, and HIPAA compliance). We propose a neural model to expand the class of\npersonal data entities that can be classified at a fine grained level, using\nthe output of existing pattern matching systems as additional contextual\nfeatures. We introduce new resources, a personal data entities hierarchy with\n134 types, and two datasets from the Wikipedia pages of elected representatives\nand Enron emails. We hope these resource will aid research in the area of\npersonal data discovery, and to that effect, we provide baseline results on\nthese datasets, and compare our method with state of the art models on\nOntoNotes dataset.", "published": "2018-11-23 06:28:41", "link": "http://arxiv.org/abs/1811.09368v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explicit Interaction Model towards Text Classification", "abstract": "Text classification is one of the fundamental tasks in natural language\nprocessing. Recently, deep neural networks have achieved promising performance\nin the text classification task compared to shallow models. Despite of the\nsignificance of deep models, they ignore the fine-grained (matching signals\nbetween words and classes) classification clues since their classifications\nmainly rely on the text-level representations. To address this problem, we\nintroduce the interaction mechanism to incorporate word-level matching signals\ninto the text classification task. In particular, we design a novel framework,\nEXplicit interAction Model (dubbed as EXAM), equipped with the interaction\nmechanism. We justified the proposed approach on several benchmark datasets\nincluding both multi-label and multi-class text classification tasks. Extensive\nexperimental results demonstrate the superiority of the proposed method. As a\nbyproduct, we have released the codes and parameter settings to facilitate\nother researches.", "published": "2018-11-23 08:30:19", "link": "http://arxiv.org/abs/1811.09386v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning pronunciation from a foreign language in speech synthesis\n  networks", "abstract": "Although there are more than 6,500 languages in the world, the pronunciations\nof many phonemes sound similar across the languages. When people learn a\nforeign language, their pronunciation often reflects their native language's\ncharacteristics. This motivates us to investigate how the speech synthesis\nnetwork learns the pronunciation from datasets from different languages. In\nthis study, we are interested in analyzing and taking advantage of multilingual\nspeech synthesis network. First, we train the speech synthesis network\nbilingually in English and Korean and analyze how the network learns the\nrelations of phoneme pronunciation between the languages. Our experimental\nresult shows that the learned phoneme embedding vectors are located closer if\ntheir pronunciations are similar across the languages. Consequently, the\ntrained networks can synthesize the English speakers' Korean speech and vice\nversa. Using this result, we propose a training framework to utilize\ninformation from a different language. To be specific, we pre-train a speech\nsynthesis network using datasets from both high-resource language and\nlow-resource language, then we fine-tune the network using the low-resource\nlanguage dataset. Finally, we conducted more simulations on 10 different\nlanguages to show it is generally extendable to other languages.", "published": "2018-11-23 05:24:15", "link": "http://arxiv.org/abs/1811.09364v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Competency Questions and SPARQL-OWL Queries Dataset and Analysis", "abstract": "Competency Questions (CQs) are natural language questions outlining and\nconstraining the scope of knowledge represented by an ontology. Despite that\nCQs are a part of several ontology engineering methodologies, we have observed\nthat the actual publication of CQs for the available ontologies is very limited\nand even scarcer is the publication of their respective formalisations in terms\nof, e.g., SPARQL queries. This paper aims to contribute to addressing the\nengineering shortcomings of using CQs in ontology development, to facilitate\nwider use of CQs. In order to understand the relation between CQs and the\nqueries over the ontology to test the CQs on an ontology, we gather, analyse,\nand publicly release a set of 234 CQs and their translations to SPARQL-OWL for\nseveral ontologies in different domains developed by different groups. We\nanalysed the CQs in two principal ways. The first stage focused on a linguistic\nanalysis of the natural language text itself, i.e., a lexico-syntactic analysis\nwithout any presuppositions of ontology elements, and a subsequent step of\nsemantic analysis in order to find patterns. This increased diversity of CQ\nsources resulted in a 5-fold increase of hitherto published patterns, to 106\ndistinct CQ patterns, which have a limited subset of few patterns shared across\nthe CQ sets from the different ontologies. Next, we analysed the relation\nbetween the found CQ patterns and the 46 SPARQL-OWL query signatures, which\nrevealed that one CQ pattern may be realised by more than one SPARQL-OWL query\nsignature, and vice versa. We hope that our work will contribute to\nestablishing common practices, templates, automation, and user tools that will\nsupport CQ formulation, formalisation, execution, and general management.", "published": "2018-11-23 16:00:51", "link": "http://arxiv.org/abs/1811.09529v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "68T30"], "primary_category": "cs.AI"}
{"title": "Interpretable Convolutional Filters with SincNet", "abstract": "Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.", "published": "2018-11-23 23:13:09", "link": "http://arxiv.org/abs/1811.09725v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Application of Clinical Concept Embeddings for Heart Failure Prediction\n  in UK EHR data", "abstract": "Electronic health records (EHR) are increasingly being used for constructing\ndisease risk prediction models. Feature engineering in EHR data however is\nchallenging due to their highly dimensional and heterogeneous nature.\nLow-dimensional representations of EHR data can potentially mitigate these\nchallenges. In this paper, we use global vectors (GloVe) to learn word\nembeddings for diagnoses and procedures recorded using 13 million ontology\nterms across 2.7 million hospitalisations in national UK EHR. We demonstrate\nthe utility of these embeddings by evaluating their performance in identifying\npatients which are at higher risk of being hospitalised for congestive heart\nfailure. Our findings indicate that embeddings can enable the creation of\nrobust EHR-derived disease risk prediction models and address some the\nlimitations associated with manual clinical feature engineering.", "published": "2018-11-23 13:04:12", "link": "http://arxiv.org/abs/1811.11005v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Training Multi-Task Adversarial Network for Extracting Noise-Robust\n  Speaker Embedding", "abstract": "Under noisy environments, to achieve the robust performance of speaker\nrecognition is still a challenging task. Motivated by the promising performance\nof multi-task training in a variety of image processing tasks, we explore the\npotential of multi-task adversarial training for learning a noise-robust\nspeaker embedding. In this paper we present a novel framework which consists of\nthree components: an encoder that extracts noise-robust speaker embedding; a\nclassifier that classifies the speakers; a discriminator that discriminates the\nnoise type of the speaker embedding. Besides, we propose a training strategy\nusing the training accuracy as an indicator to stabilize the multi-class\nadversarial optimization process. We conduct our experiments on the English and\nMandarin corpus and the experimental results demonstrate that our proposed\nmulti-task adversarial training method could greatly outperform the other\nmethods without adversarial training in noisy environments. Furthermore,\nexperiments indicate that our method is also able to improve the speaker\nverification performance the clean condition.", "published": "2018-11-23 04:08:15", "link": "http://arxiv.org/abs/1811.09355v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
