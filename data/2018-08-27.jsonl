{"title": "Predicting Semantic Relations using Global Graph Properties", "abstract": "Semantic graphs, such as WordNet, are resources which curate natural language\non two distinguishable layers. On the local level, individual relations between\nsynsets (semantic building blocks) such as hypernymy and meronymy enhance our\nunderstanding of the words used to express their meanings. Globally, analysis\nof graph-theoretic properties of the entire net sheds light on the structure of\nhuman language as a whole. In this paper, we combine global and local\nproperties of semantic graphs through the framework of Max-Margin Markov Graph\nModels (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that\nscales to large multi-relational graphs. We demonstrate how such global\nmodeling improves performance on the local task of predicting semantic\nrelations between synsets, yielding new state-of-the-art results on the WN18RR\ndataset, a challenging version of WordNet link prediction in which \"easy\"\nreciprocal cases are removed. In addition, the M3GM model identifies\nmultirelational motifs that are characteristic of well-formed lexical semantic\nontologies.", "published": "2018-08-27 00:01:11", "link": "http://arxiv.org/abs/1808.08644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate Recognition of Chinese Clinical Named Entities with\n  Residual Dilated Convolutions", "abstract": "Clinical Named Entity Recognition (CNER) aims to identify and classify\nclinical terms such as diseases, symptoms, treatments, exams, and body parts in\nelectronic health records, which is a fundamental and crucial task for clinical\nand translation research. In recent years, deep learning methods have achieved\nsignificant success in CNER tasks. However, these methods depend greatly on\nRecurrent Neural Networks (RNNs), which maintain a vector of hidden activations\nthat are propagated through time, thus causing too much time to train models.\nIn this paper, we propose a Residual Dilated Convolutional Neural Network with\nConditional Random Field (RD-CNN-CRF) to solve it. Specifically, Chinese\ncharacters and dictionary features are first projected into dense vector\nrepresentations, then they are fed into the residual dilated convolutional\nneural network to capture contextual features. Finally, a conditional random\nfield is employed to capture dependencies between neighboring tags.\nComputational results on the CCKS-2017 Task 2 benchmark dataset show that our\nproposed RD-CNN-CRF method competes favorably with state-of-the-art RNN-based\nmethods both in terms of computational performance and training time.", "published": "2018-08-27 02:42:48", "link": "http://arxiv.org/abs/1808.08669v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep\n  Contextualized Word Representations", "abstract": "In this paper we describe our system designed for the WASSA 2018 Implicit\nEmotion Shared Task (IEST), which obtained 2$^{\\text{nd}}$ place out of 26\nteams with a test macro F1 score of $0.710$. The system is composed of a single\npre-trained ELMo layer for encoding words, a Bidirectional Long-Short Memory\nNetwork BiLSTM for enriching word representations with context, a max-pooling\noperation for creating sentence representations from said word vectors, and a\nDense Layer for projecting the sentence representations into label space. Our\nofficial submission was obtained by ensembling 6 of these models initialized\nwith different random seeds. The code for replicating this paper is available\nat https://github.com/jabalazs/implicit_emotion.", "published": "2018-08-27 02:57:42", "link": "http://arxiv.org/abs/1808.08672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "simNet: Stepwise Image-Topic Merging Network for Generating Detailed and\n  Comprehensive Image Captions", "abstract": "The encode-decoder framework has shown recent success in image captioning.\nVisual attention, which is good at detailedness, and semantic attention, which\nis good at comprehensiveness, have been separately proposed to ground the\ncaption on the image. In this paper, we propose the Stepwise Image-Topic\nMerging Network (simNet) that makes use of the two kinds of attention at the\nsame time. At each time step when generating the caption, the decoder\nadaptively merges the attentive information in the extracted topics and the\nimage according to the generated context, so that the visual information and\nthe semantic information can be effectively combined. The proposed approach is\nevaluated on two benchmark datasets and reaches the state-of-the-art\nperformances.(The code is available at https://github.com/lancopku/simNet)", "published": "2018-08-27 08:37:42", "link": "http://arxiv.org/abs/1808.08732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Attention-based Convolutional and Recurrent Neural Networks:\n  Success and Limitations in Machine Reading Comprehension", "abstract": "We propose a machine reading comprehension model based on the\ncompare-aggregate framework with two-staged attention that achieves\nstate-of-the-art results on the MovieQA question answering dataset. To\ninvestigate the limitations of our model as well as the behavioral difference\nbetween convolutional and recurrent neural networks, we generate adversarial\nexamples to confuse the model and compare to human performance. Furthermore, we\nassess the generalizability of our model by analyzing its differences to human\ninference,", "published": "2018-08-27 09:04:22", "link": "http://arxiv.org/abs/1808.08744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional\n  Neural Networks for Extreme Summarization", "abstract": "We introduce extreme summarization, a new single-document summarization task\nwhich does not favor extractive strategies and calls for an abstractive\nmodeling approach. The idea is to create a short, one-sentence news summary\nanswering the question \"What is the article about?\". We collect a real-world,\nlarge-scale dataset for this task by harvesting online articles from the\nBritish Broadcasting Corporation (BBC). We propose a novel abstractive model\nwhich is conditioned on the article's topics and based entirely on\nconvolutional neural networks. We demonstrate experimentally that this\narchitecture captures long-range dependencies in a document and recognizes\npertinent content, outperforming an oracle extractive system and\nstate-of-the-art abstractive approaches when evaluated automatically and by\nhumans.", "published": "2018-08-27 09:08:18", "link": "http://arxiv.org/abs/1808.08745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Cross-Lingual Word Embeddings by Meeting in the Middle", "abstract": "Cross-lingual word embeddings are becoming increasingly important in\nmultilingual NLP. Recently, it has been shown that these embeddings can be\neffectively learned by aligning two disjoint monolingual vector spaces through\nlinear transformations, using no more than a small bilingual dictionary as\nsupervision. In this work, we propose to apply an additional transformation\nafter the initial alignment step, which moves cross-lingual synonyms towards a\nmiddle point between them. By applying this transformation our aim is to obtain\na better cross-lingual integration of the vector spaces. In addition, and\nperhaps surprisingly, the monolingual spaces also improve by this\ntransformation. This is in contrast to the original alignment, which is\ntypically learned such that the structure of the monolingual spaces is\npreserved. Our experiments confirm that the resulting cross-lingual embeddings\noutperform state-of-the-art models in both monolingual and cross-lingual\nevaluation tasks.", "published": "2018-08-27 10:54:37", "link": "http://arxiv.org/abs/1808.08780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Auto-Encoder Matching Model for Learning Utterance-Level Semantic\n  Dependency in Dialogue Generation", "abstract": "Generating semantically coherent responses is still a major challenge in\ndialogue generation. Different from conventional text generation tasks, the\nmapping between inputs and responses in conversations is more complicated,\nwhich highly demands the understanding of utterance-level semantic dependency,\na relation between the whole meanings of inputs and outputs. To address this\nproblem, we propose an Auto-Encoder Matching (AEM) model to learn such\ndependency. The model contains two auto-encoders and one mapping module. The\nauto-encoders learn the semantic representations of inputs and responses, and\nthe mapping module learns to connect the utterance-level representations.\nExperimental results from automatic and human evaluations demonstrate that our\nmodel is capable of generating responses of high coherence and fluency compared\nto baseline models. The code is available at https://github.com/lancopku/AMM", "published": "2018-08-27 11:46:13", "link": "http://arxiv.org/abs/1808.08795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WiSeBE: Window-based Sentence Boundary Evaluation", "abstract": "Sentence Boundary Detection (SBD) has been a major research topic since\nAutomatic Speech Recognition transcripts have been used for further Natural\nLanguage Processing tasks like Part of Speech Tagging, Question Answering or\nAutomatic Summarization. But what about evaluation? Do standard evaluation\nmetrics like precision, recall, F-score or classification error; and more\nimportant, evaluating an automatic system against a unique reference is enough\nto conclude how well a SBD system is performing given the final application of\nthe transcript? In this paper we propose Window-based Sentence Boundary\nEvaluation (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary\nDetection systems based on multi-reference (dis)agreement. We evaluate and\ncompare the performance of different SBD systems over a set of Youtube\ntranscripts using WiSeBE and standard metrics. This double evaluation gives an\nunderstanding of how WiSeBE is a more reliable metric for the SBD task.", "published": "2018-08-27 14:02:58", "link": "http://arxiv.org/abs/1808.08850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine\n  Translation", "abstract": "In order to extract the best possible performance from asynchronous\nstochastic gradient descent one must increase the mini-batch size and scale the\nlearning rate accordingly. In order to achieve further speedup we introduce a\ntechnique that delays gradient updates effectively increasing the mini-batch\nsize. Unfortunately with the increase of mini-batch size we worsen the stale\ngradient problem in asynchronous stochastic gradient descent (SGD) which makes\nthe model convergence poor. We introduce local optimizers which mitigate the\nstale gradient problem and together with fine tuning our momentum we are able\nto train a shallow machine translation system 27% faster than an optimized\nbaseline with negligible penalty in BLEU.", "published": "2018-08-27 14:19:18", "link": "http://arxiv.org/abs/1808.08859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Sentiment Attitudes From Analytical Texts", "abstract": "In this paper we present the RuSentRel corpus including analytical texts in\nthe sphere of international relations. For each document we annotated\nsentiments from the author to mentioned named entities, and sentiments of\nrelations between mentioned entities. In the current experiments, we considered\nthe problem of extracting sentiment relations between entities for the whole\ndocuments as a three-class machine learning task. We experimented with\nconventional machine-learning methods (Naive Bayes, SVM, Random Forest).", "published": "2018-08-27 17:15:54", "link": "http://arxiv.org/abs/1808.08932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multilingual Word Embeddings", "abstract": "Multilingual Word Embeddings (MWEs) represent words from multiple languages\nin a single distributional vector space. Unsupervised MWE (UMWE) methods\nacquire multilingual embeddings without cross-lingual supervision, which is a\nsignificant advantage over traditional supervised approaches and opens many new\npossibilities for low-resource languages. Prior art for learning UMWEs,\nhowever, merely relies on a number of independently trained Unsupervised\nBilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These\nmethods fail to leverage the interdependencies that exist among many languages.\nTo address this shortcoming, we propose a fully unsupervised framework for\nlearning MWEs that directly exploits the relations between all language pairs.\nOur model substantially outperforms previous approaches in the experiments on\nmultilingual word translation and cross-lingual word similarity. In addition,\nour model even beats supervised approaches trained with cross-lingual\nresources.", "published": "2018-08-27 17:22:15", "link": "http://arxiv.org/abs/1808.08933v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Self-Attention? A Targeted Evaluation of Neural Machine Translation\n  Architectures", "abstract": "Recently, non-recurrent architectures (convolutional, self-attentional) have\noutperformed RNNs in neural machine translation. CNNs and self-attentional\nnetworks can connect distant words via shorter network paths than RNNs, and it\nhas been speculated that this improves their ability to model long-range\ndependencies. However, this theoretical argument has not been tested\nempirically, nor have alternative explanations for their strong performance\nbeen explored in-depth. We hypothesize that the strong performance of CNNs and\nself-attentional networks could also be due to their ability to extract\nsemantic features from the source text, and we evaluate RNNs, CNNs and\nself-attention networks on two tasks: subject-verb agreement (where capturing\nlong-range dependencies is required) and word sense disambiguation (where\nsemantic feature extraction is required). Our experimental results show that:\n1) self-attentional networks and CNNs do not outperform RNNs in modeling\nsubject-verb agreement over long distances; 2) self-attentional networks\nperform distinctly better than RNNs and CNNs on word sense disambiguation.", "published": "2018-08-27 17:51:27", "link": "http://arxiv.org/abs/1808.08946v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "abstract": "Contextual word representations derived from pre-trained bidirectional\nlanguage models (biLMs) have recently been shown to provide significant\nimprovements to the state of the art for a wide range of NLP tasks. However,\nmany questions remain as to how and why these models are so effective. In this\npaper, we present a detailed empirical study of how the choice of neural\narchitecture (e.g. LSTM, CNN, or self attention) influences both end task\naccuracy and qualitative properties of the representations that are learned. We\nshow there is a tradeoff between speed and accuracy, but all architectures\nlearn high quality contextual representations that outperform word embeddings\nfor four challenging NLP tasks. Additionally, all architectures learn\nrepresentations that vary with network depth, from exclusively morphological\nbased at the word embedding layer through local syntax based in the lower\ncontextual layers to longer range semantics such coreference at the upper\nlayers. Together, these results suggest that unsupervised biLMs, independent of\narchitecture, are learning much more about the structure of language than\npreviously appreciated.", "published": "2018-08-27 17:54:29", "link": "http://arxiv.org/abs/1808.08949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Margin Neural Language Model", "abstract": "We propose a large margin criterion for training neural language models.\nConventionally, neural language models are trained by minimizing perplexity\n(PPL) on grammatical sentences. However, we demonstrate that PPL may not be the\nbest metric to optimize in some tasks, and further propose a large margin\nformulation. The proposed method aims to enlarge the margin between the \"good\"\nand \"bad\" sentences in a task-specific sense. It is trained end-to-end and can\nbe widely applied to tasks that involve re-scoring of generated text. Compared\nwith minimum-PPL training, our method gains up to 1.1 WER reduction for speech\nrecognition and 1.0 BLEU increase for machine translation.", "published": "2018-08-27 18:31:33", "link": "http://arxiv.org/abs/1808.08987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back-Translation Sampling by Targeting Difficult Words in Neural Machine\n  Translation", "abstract": "Neural Machine Translation has achieved state-of-the-art performance for\nseveral language pairs using a combination of parallel and synthetic data.\nSynthetic data is often generated by back-translating sentences randomly\nsampled from monolingual data using a reverse translation model. While\nback-translation has been shown to be very effective in many cases, it is not\nentirely clear why. In this work, we explore different aspects of\nback-translation, and show that words with high prediction loss during training\nbenefit most from the addition of synthetic data. We introduce several\nvariations of sampling strategies targeting difficult-to-predict words using\nprediction losses and frequencies of words. In addition, we also target the\ncontexts of difficult words and sample sentences that are similar in context.\nExperimental results for the WMT news translation task show that our method\nimproves translation quality by up to 1.7 and 1.2 Bleu points over\nback-translation using random sampling for German-English and English-German,\nrespectively.", "published": "2018-08-27 19:27:01", "link": "http://arxiv.org/abs/1808.09006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation with Neural Variational Models", "abstract": "In this thesis, we explore the use of deep neural networks for generation of\nnatural language. Specifically, we implement two sequence-to-sequence neural\nvariational models - variational autoencoders (VAE) and variational\nencoder-decoders (VED). VAEs for text generation are difficult to train due to\nissues associated with the Kullback-Leibler (KL) divergence term of the loss\nfunction vanishing to zero. We successfully train VAEs by implementing\noptimization heuristics such as KL weight annealing and word dropout. We also\ndemonstrate the effectiveness of this continuous latent space through\nexperiments such as random sampling, linear interpolation and sampling from the\nneighborhood of the input. We argue that if VAEs are not designed\nappropriately, it may lead to bypassing connections which results in the latent\nspace being ignored during training. We show experimentally with the example of\ndecoder hidden state initialization that such bypassing connections degrade the\nVAE into a deterministic model, thereby reducing the diversity of generated\nsentences. We discover that the traditional attention mechanism used in\nsequence-to-sequence VED models serves as a bypassing connection, thereby\ndeteriorating the model's latent space. In order to circumvent this issue, we\npropose the variational attention mechanism where the attention context vector\nis modeled as a random variable that can be sampled from a distribution. We\nshow empirically using automatic evaluation metrics, namely entropy and\ndistinct measures, that our variational attention model generates more diverse\noutput sentences than the deterministic attention model. A qualitative analysis\nwith human evaluation study proves that our model simultaneously produces\nsentences that are of high quality and equally fluent as the ones generated by\nthe deterministic attention counterpart.", "published": "2018-08-27 19:40:53", "link": "http://arxiv.org/abs/1808.09012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pyramidal Recurrent Unit for Language Modeling", "abstract": "LSTMs are powerful tools for modeling contextual information, as evidenced by\ntheir success at the task of language modeling. However, modeling contexts in\nvery high dimensional space can lead to poor generalizability. We introduce the\nPyramidal Recurrent Unit (PRU), which enables learning representations in high\ndimensional space with more generalization power and fewer parameters. PRUs\nreplace the linear transformation in LSTMs with more sophisticated interactions\nincluding pyramidal and grouped linear transformations. This architecture gives\nstrong results on word-level language modeling while reducing the number of\nparameters significantly. In particular, PRU improves the perplexity of a\nrecent state-of-the-art language model Merity et al. (2018) by up to 1.3 points\nwhile learning 15-20% fewer parameters. For similar number of model parameters,\nPRU outperforms all previous RNN models that exploit different gating\nmechanisms and transformations. We provide a detailed examination of the PRU\nand its behavior on the language modeling tasks. Our code is open-source and\navailable at https://sacmehta.github.io/PRU/", "published": "2018-08-27 20:31:27", "link": "http://arxiv.org/abs/1808.09029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Targeted Syntactic Evaluation of Language Models", "abstract": "We present a dataset for evaluating the grammaticality of the predictions of\na language model. We automatically construct a large number of minimally\ndifferent pairs of English sentences, each consisting of a grammatical and an\nungrammatical sentence. The sentence pairs represent different variations of\nstructure-sensitive phenomena: subject-verb agreement, reflexive anaphora and\nnegative polarity items. We expect a language model to assign a higher\nprobability to the grammatical sentence than the ungrammatical one. In an\nexperiment using this data set, an LSTM language model performed poorly on many\nof the constructions. Multi-task training with a syntactic objective (CCG\nsupertagging) improved the LSTM's accuracy, but a large gap remained between\nits performance and the accuracy of human participants recruited online. This\nsuggests that there is considerable room for improvement over LSTMs in\ncapturing syntax in a language model.", "published": "2018-08-27 20:42:51", "link": "http://arxiv.org/abs/1808.09031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One-Shot Relational Learning for Knowledge Graphs", "abstract": "Knowledge graphs (KGs) are the key components of various natural language\nprocessing applications. To further expand KGs' coverage, previous studies on\nknowledge graph completion usually require a large number of training instances\nfor each relation. However, we observe that long-tail relations are actually\nmore common in KGs and those newly added relations often do not have many known\ntriples for training. In this work, we aim at predicting new facts under a\nchallenging setting where only one training instance is available. We propose a\none-shot relational learning framework, which utilizes the knowledge extracted\nby embedding models and learns a matching metric by considering both the\nlearned embeddings and one-hop graph structures. Empirically, our model yields\nconsiderable performance improvements over existing embedding models, and also\neliminates the need of re-training the embedding models when dealing with newly\nadded relations.", "published": "2018-08-27 21:42:56", "link": "http://arxiv.org/abs/1808.09040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Decomposition of Text Representation", "abstract": "In this paper, we present a method for adversarial decomposition of text\nrepresentation. This method can be used to decompose a representation of an\ninput sentence into several independent vectors, each of them responsible for a\nspecific aspect of the input sentence. We evaluate the proposed method on two\ncase studies: the conversion between different social registers and diachronic\nlanguage change. We show that the proposed method is capable of fine-grained\ncontrolled change of these aspects of the input sentence. It is also learning a\ncontinuous (rather than categorical) representation of the style of the\nsentence, which is more linguistically realistic. The model uses\nadversarial-motivational training and includes a special motivational loss,\nwhich acts opposite to the discriminator and encourages a better decomposition.\nFurthermore, we evaluate the obtained meaning embeddings on a downstream task\nof paraphrase detection and show that they significantly outperform the\nembeddings of a regular autoencoder.", "published": "2018-08-27 21:49:12", "link": "http://arxiv.org/abs/1808.09042v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter sharing between dependency parsers for related languages", "abstract": "Previous work has suggested that parameter sharing between transition-based\nneural dependency parsers for related languages can lead to better performance,\nbut there is no consensus on what parameters to share. We present an evaluation\nof 27 different parameter sharing strategies across 10 languages, representing\nfive pairs of related languages, each pair from a different language family. We\nfind that sharing transition classifier parameters always helps, whereas the\nusefulness of sharing word and/or character LSTM parameters varies. Based on\nthis result, we propose an architecture where the transition classifier is\nshared, and the sharing of word and character parameters is controlled by a\nparameter that can be tuned on validation data. This model is linguistically\nmotivated and obtains significant improvements over a monolingually trained\nbaseline. We also find that sharing transition classifier parameters helps when\ntraining a parser on unrelated language pairs, but we find that, in the case of\nunrelated languages, sharing too many parameters does not help.", "published": "2018-08-27 22:47:59", "link": "http://arxiv.org/abs/1808.09055v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation of the Interactions Between Pre-Trained Word\n  Embeddings, Character Models and POS Tags in Dependency Parsing", "abstract": "We provide a comprehensive analysis of the interactions between pre-trained\nword embeddings, character models and POS tags in a transition-based dependency\nparser. While previous studies have shown POS information to be less important\nin the presence of character models, we show that in fact there are complex\ninteractions between all three techniques. In isolation each produces large\nimprovements over a baseline system using randomly initialised word embeddings\nonly, but combining them quickly leads to diminishing returns. We categorise\nwords by frequency, POS tag and language in order to systematically investigate\nhow each of the techniques affects parsing quality. For many word categories,\napplying any two of the three techniques is almost as good as the full combined\nsystem. Character models tend to be more important for low-frequency open-class\nwords, especially in morphologically rich languages, while POS tags can help\ndisambiguate high-frequency function words. We also show that large character\nembedding sizes help even for languages with small character sets, especially\nin morphologically rich languages.", "published": "2018-08-27 23:11:47", "link": "http://arxiv.org/abs/1808.09060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Embeddings in NLI with Iterative Refinement Encoders", "abstract": "Sentence-level representations are necessary for various NLP tasks. Recurrent\nneural networks have proven to be very effective in learning distributed\nrepresentations and can be trained efficiently on natural language inference\ntasks. We build on top of one such model and propose a hierarchy of BiLSTM and\nmax pooling layers that implements an iterative refinement strategy and yields\nstate of the art results on the SciTail dataset as well as strong results for\nSNLI and MultiNLI. We can show that the sentence embeddings learned in this way\ncan be utilized in a wide variety of transfer learning tasks, outperforming\nInferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence\nembedding evaluation tasks. Furthermore, our model beats the InferSent model in\n8 out of 10 recently published SentEval probing tasks designed to evaluate\nsentence embeddings' ability to capture some of the important linguistic\nproperties of sentences.", "published": "2018-08-27 09:50:56", "link": "http://arxiv.org/abs/1808.08762v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Amobee at IEST 2018: Transfer Learning from Language Models", "abstract": "This paper describes the system developed at Amobee for the WASSA 2018\nimplicit emotions shared task (IEST). The goal of this task was to predict the\nemotion expressed by missing words in tweets without an explicit mention of\nthose words. We developed an ensemble system consisting of language models\ntogether with LSTM-based networks containing a CNN attention mechanism. Our\napproach represents a novel use of language models (specifically trained on a\nlarge Twitter dataset) to predict and classify emotions. Our system reached 1st\nplace with a macro $\\text{F}_1$ score of 0.7145.", "published": "2018-08-27 11:04:55", "link": "http://arxiv.org/abs/1808.08782v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Term Set Expansion based NLP Architect by Intel AI Lab", "abstract": "We present SetExpander, a corpus-based system for expanding a seed set of\nterms into amore complete set of terms that belong to the same semantic class.\nSetExpander implements an iterative end-to-end workflow. It enables users to\neasily select a seed set of terms, expand it, view the expanded set, validate\nit, re-expand the validated set and store it, thus simplifying the extraction\nof domain-specific fine-grained semantic classes.SetExpander has been used\nsuccessfully in real-life use cases including integration into an automated\nrecruitment system and an issues and defects resolution system. A video demo of\nSetExpander is available at\nhttps://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv (some images\nwere blurred for privacy reasons)", "published": "2018-08-27 12:19:07", "link": "http://arxiv.org/abs/1808.08953v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generating Text through Adversarial Training using Skip-Thought Vectors", "abstract": "GANs have been shown to perform exceedingly well on tasks pertaining to image\ngeneration and style transfer. In the field of language modelling, word\nembeddings such as GLoVe and word2vec are state-of-the-art methods for applying\nneural network models on textual data. Attempts have been made to utilize GANs\nwith word embeddings for text generation. This study presents an approach to\ntext generation using Skip-Thought sentence embeddings with GANs based on\ngradient penalty functions and f-measures. The proposed architecture aims to\nreproduce writing style in the generated text by modelling the way of\nexpression at a sentence level across all the works of an author. Extensive\nexperiments were run in different embedding settings on a variety of tasks\nincluding conditional text generation and language generation. The model\noutperforms baseline text generation networks across several automated\nevaluation metrics like BLEU-n, METEOR and ROUGE. Further, wide applicability\nand effectiveness in real life tasks are demonstrated through human judgement\nscores.", "published": "2018-08-27 06:51:07", "link": "http://arxiv.org/abs/1808.08703v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predefined Sparseness in Recurrent Sequence Models", "abstract": "Inducing sparseness while training neural networks has been shown to yield\nmodels with a lower memory footprint but similar effectiveness to dense models.\nHowever, sparseness is typically induced starting from a dense model, and thus\nthis advantage does not hold during training. We propose techniques to enforce\nsparseness upfront in recurrent sequence models for NLP applications, to also\nbenefit training. First, in language modeling, we show how to increase hidden\nstate sizes in recurrent layers without increasing the number of parameters,\nleading to more expressive models. Second, for sequence labeling, we show that\nword embeddings with predefined sparseness lead to similar performance as dense\nembeddings, at a fraction of the number of trainable parameters.", "published": "2018-08-27 07:55:41", "link": "http://arxiv.org/abs/1808.08720v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Multilingual Word Embeddings in Latent Metric Space: A\n  Geometric Approach", "abstract": "We propose a novel geometric approach for learning bilingual mappings given\nmonolingual embeddings and a bilingual dictionary. Our approach decouples\nlearning the transformation from the source language to the target language\ninto (a) learning rotations for language-specific embeddings to align them to a\ncommon space, and (b) learning a similarity metric in the common space to model\nsimilarities between the embeddings. We model the bilingual mapping problem as\nan optimization problem on smooth Riemannian manifolds. We show that our\napproach outperforms previous approaches on the bilingual lexicon induction and\ncross-lingual word similarity tasks. We also generalize our framework to\nrepresent multiple languages in a common latent space. In particular, the\nlatent space representations for several languages are learned jointly, given\nbilingual dictionaries for multiple language pairs. We illustrate the\neffectiveness of joint learning for multiple languages in zero-shot word\ntranslation setting. Our implementation is available at\nhttps://github.com/anoopkunchukuttan/geomm .", "published": "2018-08-27 10:37:16", "link": "http://arxiv.org/abs/1808.08773v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A strong baseline for question relevancy ranking", "abstract": "The best systems at the SemEval-16 and SemEval-17 community question\nanswering shared tasks -- a task that amounts to question relevancy ranking --\ninvolve complex pipelines and manual feature engineering. Despite this, many of\nthese still fail at beating the IR baseline, i.e., the rankings provided by\nGoogle's search engine. We present a strong baseline for question relevancy\nranking by training a simple multi-task feed forward network on a bag of 14\ndistance measures for the input question pair. This baseline model, which is\nfast to train and uses only language-independent features, outperforms the best\nshared task systems on the task of retrieving relevant previously asked\nquestions.", "published": "2018-08-27 13:19:49", "link": "http://arxiv.org/abs/1808.08836v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and\n  They Are Both Weakly Supervised", "abstract": "We present a neural framework for opinion summarization from online product\nreviews which is knowledge-lean and only requires light supervision (e.g., in\nthe form of product domain labels and user-provided ratings). Our method\ncombines two weakly supervised components to identify salient opinions and form\nextractive summaries from multiple reviews: an aspect extractor trained under a\nmulti-task objective, and a sentiment predictor based on multiple instance\nlearning. We introduce an opinion summarization dataset that includes a\ntraining set of product reviews from six diverse domains and human-annotated\ndevelopment and test sets with gold standard aspect annotations, salience\nlabels, and opinion summaries. Automatic evaluation shows significant\nimprovements over baselines, and a large-scale study indicates that our opinion\nsummaries are preferred by human judges according to multiple criteria.", "published": "2018-08-27 14:17:08", "link": "http://arxiv.org/abs/1808.08858v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot Transfer Learning for Semantic Parsing", "abstract": "While neural networks have shown impressive performance on large datasets,\napplying these models to tasks where little data is available remains a\nchallenging problem.\n  In this paper we propose to use feature transfer in a zero-shot experimental\nsetting on the task of semantic parsing.\n  We first introduce a new method for learning the shared space between\nmultiple domains based on the prediction of the domain label for each example.\n  Our experiments support the superiority of this method in a zero-shot\nexperimental setting in terms of accuracy metrics compared to state-of-the-art\ntechniques.\n  In the second part of this paper we study the impact of individual domains\nand examples on semantic parsing performance.\n  We use influence functions to this aim and investigate the sensitivity of\ndomain-label classification loss on each example.\n  Our findings reveal that cross-domain adversarial attacks identify useful\nexamples for training even from the domains the least similar to the target\ndomain. Augmenting our training data with these influential examples further\nboosts our accuracy at both the token and the sequence level.", "published": "2018-08-27 16:12:36", "link": "http://arxiv.org/abs/1808.09889v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An Adaptive Conversational Bot Framework", "abstract": "How can we enable users to heavily specify criteria for database queries in a\nuser-friendly way? This paper describes a general framework of a conversational\nbot that extracts meaningful information from user's sentences, that asks\nsubsequent questions to complete missing information, and that adjusts its\nquestions and information-extraction parameters for later conversations\ndepending on users' behavior. Additionally, we provide a comparison of existing\ntools and give novel techniques to implement such framework. Finally, we\nexemplify the framework with a bot to query movies in a database, whose code is\navailable for Microsoft employees.", "published": "2018-08-27 21:37:42", "link": "http://arxiv.org/abs/1808.09890v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Measuring the Volatility of the Political agenda in Public Opinion and\n  News Media", "abstract": "Recent election surprises, regime changes, and political shocks indicate that\npolitical agendas have become more fast-moving and volatile. The ability to\nmeasure the complex dynamics of agenda change and capture the nature and extent\nof volatility in political systems is therefore more crucial than ever before.\nThis study proposes a definition and operationalization of volatility that\ncombines insights from political science, communications, information theory,\nand computational techniques. The proposed measures of fractionalization and\nagenda change encompass the shifting salience of issues in the agenda as a\nwhole and allow the study of agendas across different domains. We evaluate\nthese metrics and compare them to other measures such as issue-level survival\nrates and the Pedersen Index, which uses public-opinion poll data to measure\npublic agendas, as well as traditional media content to measure media agendas\nin the UK and Germany. We show how these measures complement existing\napproaches and could be employed in future agenda-setting research.", "published": "2018-08-27 21:28:46", "link": "http://arxiv.org/abs/1808.09037v2", "categories": ["cs.CY", "cs.CL", "cs.IT", "math.IT", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "A neural attention model for speech command recognition", "abstract": "This paper introduces a convolutional recurrent network with attention for\nspeech command recognition. Attention models are powerful tools to improve\nperformance on natural language, image captioning and speech tasks. The\nproposed model establishes a new state-of-the-art accuracy of 94.1% on Google\nSpeech Commands dataset V1 and 94.5% on V2 (for the 20-commands recognition\ntask), while still keeping a small footprint of only 202K trainable parameters.\nResults are compared with previous convolutional implementations on 5 different\ntasks (20 commands recognition (V1 and V2), 12 commands recognition (V1), 35\nword recognition (V1) and left-right (V1)). We show detailed performance\nresults and demonstrate that the proposed attention mechanism not only improves\nperformance but also allows inspecting what regions of the audio were taken\ninto consideration by the network when outputting a given category.", "published": "2018-08-27 17:05:50", "link": "http://arxiv.org/abs/1808.08929v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Augmenting Bottleneck Features of Deep Neural Network Employing Motor\n  State for Speech Recognition at Humanoid Robots", "abstract": "As for the humanoid robots, the internal noise, which is generated by motors,\nfans and mechanical components when the robot is moving or shaking its body,\nseverely degrades the performance of the speech recognition accuracy. In this\npaper, a novel speech recognition system robust to ego-noise for humanoid\nrobots is proposed, in which on/off state of the motor is employed as auxiliary\ninformation for finding the relevant input features. For this, we consider the\nbottleneck features, which have been successfully applied to deep neural\nnetwork (DNN) based automatic speech recognition (ASR) system. When learning\nthe bottleneck features to catch, we first exploit the motor on/off state data\nas supplementary information in addition to the acoustic features as the input\nof the first deep neural network (DNN) for preliminary acoustic modeling. Then,\nthe second DNN for primary acoustic modeling employs both the bottleneck\nfeatures tossed from the first DNN and the acoustics features. When the\nproposed method is evaluated in terms of phoneme error rate (PER) on TIMIT\ndatabase, the experimental results show that achieve obvious improvement (11%\nrelative) is achieved by our algorithm over the conventional systems.", "published": "2018-08-27 06:41:47", "link": "http://arxiv.org/abs/1808.08702v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
