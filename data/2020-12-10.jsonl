{"title": "Infusing Finetuning with Semantic Dependencies", "abstract": "For natural language processing systems, two kinds of evidence support the\nuse of text representations from neural language models \"pretrained\" on large\nunannotated corpora: performance on application-inspired benchmarks (Peters et\nal., 2018, inter alia), and the emergence of syntactic abstractions in those\nrepresentations (Tenney et al., 2019, inter alia). On the other hand, the lack\nof grounded supervision calls into question how well these representations can\never capture meaning (Bender and Koller, 2020). We apply novel probes to recent\nlanguage models -- specifically focusing on predicate-argument structure as\noperationalized by semantic dependencies (Ivanova et al., 2012) -- and find\nthat, unlike syntax, semantics is not brought to the surface by today's\npretrained models. We then use convolutional graph encoders to explicitly\nincorporate semantic parses into task-specific finetuning, yielding benefits to\nnatural language understanding (NLU) tasks in the GLUE benchmark. This approach\ndemonstrates the potential for general-purpose (rather than task-specific)\nlinguistic supervision, above and beyond conventional pretraining and\nfinetuning. Several diagnostics help to localize the benefits of our approach.", "published": "2020-12-10 01:27:24", "link": "http://arxiv.org/abs/2012.05395v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rewriter-Evaluator Architecture for Neural Machine Translation", "abstract": "Encoder-decoder has been widely used in neural machine translation (NMT). A\nfew methods have been proposed to improve it with multiple passes of decoding.\nHowever, their full potential is limited by a lack of appropriate termination\npolicies. To address this issue, we present a novel architecture,\nRewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a\nsource sentence involves multiple passes. At every pass, the rewriter produces\na new translation to improve the past translation and the evaluator estimates\nthe translation quality to decide whether to terminate the rewriting process.\nWe also propose prioritized gradient descent (PGD) that facilitates training\nthe rewriter and the evaluator jointly. Though incurring multiple passes of\ndecoding, Rewriter-Evaluator with the proposed PGD method can be trained with a\nsimilar time to that of training encoder-decoder models. We apply the proposed\narchitecture to improve the general NMT models (e.g., Transformer). We conduct\nextensive experiments on two translation tasks, Chinese-English and\nEnglish-German, and show that the proposed architecture notably improves the\nperformances of NMT models and significantly outperforms previous baselines.", "published": "2020-12-10 02:21:34", "link": "http://arxiv.org/abs/2012.05414v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmenting Natural Language Sentences via Lexical Unit Analysis", "abstract": "In this work, we present Lexical Unit Analysis (LUA), a framework for general\nsequence segmentation tasks. Given a natural language sentence, LUA scores all\nthe valid segmentation candidates and utilizes dynamic programming (DP) to\nextract the maximum scoring one. LUA enjoys a number of appealing properties\nsuch as inherently guaranteeing the predicted segmentation to be valid and\nfacilitating globally optimal training and inference. Besides, the practical\ntime complexity of LUA can be reduced to linear time, which is very efficient.\nWe have conducted extensive experiments on 5 tasks, including syntactic\nchunking, named entity recognition (NER), slot filling, Chinese word\nsegmentation, and Chinese part-of-speech (POS) tagging, across 15 datasets. Our\nmodels have achieved the state-of-the-art performances on 13 of them. The\nresults also show that the F1 score of identifying long-length segments is\nnotably improved.", "published": "2020-12-10 02:31:52", "link": "http://arxiv.org/abs/2012.05418v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Analysis of Unlabeled Entity Problem in Named Entity\n  Recognition", "abstract": "In many scenarios, named entity recognition (NER) models severely suffer from\nunlabeled entity problem, where the entities of a sentence may not be fully\nannotated. Through empirical studies performed on synthetic datasets, we find\ntwo causes of performance degradation. One is the reduction of annotated\nentities and the other is treating unlabeled entities as negative instances.\nThe first cause has less impact than the second one and can be mitigated by\nadopting pretraining language models. The second cause seriously misguides a\nmodel in training and greatly affects its performances. Based on the above\nobservations, we propose a general approach, which can almost eliminate the\nmisguidance brought by unlabeled entities. The key idea is to use negative\nsampling that, to a large extent, avoids training NER models with unlabeled\nentities. Experiments on synthetic datasets and real-world datasets show that\nour model is robust to unlabeled entity problem and surpasses prior baselines.\nOn well-annotated datasets, our model is competitive with the state-of-the-art\nmethod.", "published": "2020-12-10 02:53:59", "link": "http://arxiv.org/abs/2012.05426v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models\n  for Other Languages", "abstract": "Large generative language models have been very successful for English, but\nother languages lag behind, in part due to data and computational limitations.\nWe propose a method that may overcome these problems by adapting existing\npre-trained models to new languages. Specifically, we describe the adaptation\nof English GPT-2 to Italian and Dutch by retraining lexical embeddings without\ntuning the Transformer layers. As a result, we obtain lexical embeddings for\nItalian and Dutch that are aligned with the original English lexical\nembeddings. Additionally, we scale up complexity by transforming relearned\nlexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This\nmethod minimises the amount of training and prevents losing information during\nadaptation that was learned by GPT-2. English GPT-2 models with relearned\nlexical embeddings can generate realistic sentences in Italian and Dutch.\nThough on average these sentences are still identifiable as artificial by\nhumans, they are assessed on par with sentences generated by a GPT-2 model\nfully trained from scratch.", "published": "2020-12-10 12:27:16", "link": "http://arxiv.org/abs/2012.05628v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Sense Language Modelling", "abstract": "The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.", "published": "2020-12-10 16:06:05", "link": "http://arxiv.org/abs/2012.05776v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Pair-Wise NMT for Indian Languages", "abstract": "In this paper, we address the task of improving pair-wise machine translation\nfor specific low resource Indian languages. Multilingual NMT models have\ndemonstrated a reasonable amount of effectiveness on resource-poor languages.\nIn this work, we show that the performance of these models can be significantly\nimproved upon by using back-translation through a filtered back-translation\nprocess and subsequent fine-tuning on the limited pair-wise language corpora.\nThe analysis in this paper suggests that this method can significantly improve\na multilingual model's performance over its baseline, yielding state-of-the-art\nresults for various Indian languages.", "published": "2020-12-10 16:22:36", "link": "http://arxiv.org/abs/2012.05786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Standardization of Colloquial Persian", "abstract": "The Iranian Persian language has two varieties: standard and colloquial. Most\nnatural language processing tools for Persian assume that the text is in\nstandard form: this assumption is wrong in many real applications especially\nweb content. This paper describes a simple and effective standardization\napproach based on sequence-to-sequence translation. We design an algorithm for\ngenerating artificial parallel colloquial-to-standard data for learning a\nsequence-to-sequence model. Moreover, we annotate a publicly available\nevaluation data consisting of 1912 sentences from a diverse set of domains. Our\nintrinsic evaluation shows a higher BLEU score of 62.8 versus 61.7 compared to\nan off-the-shelf rule-based standardization model in which the original text\nhas a BLEU score of 46.4. We also show that our model improves\nEnglish-to-Persian machine translation in scenarios for which the training data\nis from colloquial Persian with 1.4 absolute BLEU score difference in the\ndevelopment data, and 0.8 in the test data.", "published": "2020-12-10 18:39:26", "link": "http://arxiv.org/abs/2012.05879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Transfer Learning for QA Using Translation as Data\n  Augmentation", "abstract": "Prior work on multilingual question answering has mostly focused on using\nlarge multilingual pre-trained language models (LM) to perform zero-shot\nlanguage-wise learning: train a QA model on English and test on other\nlanguages. In this work, we explore strategies that improve cross-lingual\ntransfer by bringing the multilingual embeddings closer in the semantic space.\nOur first strategy augments the original English training data with machine\ntranslation-generated data. This results in a corpus of multilingual\nsilver-labeled QA pairs that is 14 times larger than the original training set.\nIn addition, we propose two novel strategies, language adversarial training and\nlanguage arbitration framework, which significantly improve the (zero-resource)\ncross-lingual transfer performance and result in LM embeddings that are less\nlanguage-variant. Empirically, we show that the proposed models outperform the\nprevious zero-shot baseline on the recently introduced multilingual MLQA and\nTyDiQA datasets.", "published": "2020-12-10 20:29:34", "link": "http://arxiv.org/abs/2012.05958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Let's Eat Grandma\": Does Punctuation Matter in Sentence Representation?", "abstract": "Neural network-based embeddings have been the mainstream approach for\ncreating a vector representation of the text to capture lexical and semantic\nsimilarities and dissimilarities. In general, existing encoding methods dismiss\nthe punctuation as insignificant information; consequently, they are routinely\ntreated as a predefined token/word or eliminated in the pre-processing phase.\nHowever, punctuation could play a significant role in the semantics of the\nsentences, as in \"Let's eat\\hl{,} grandma\" and \"Let's eat grandma\". We\nhypothesize that a punctuation-aware representation model would affect the\nperformance of the downstream tasks. Thereby, we propose a model-agnostic\nmethod that incorporates both syntactic and contextual information to improve\nthe performance of the sentiment classification task. We corroborate our\nfindings by conducting experiments on publicly available datasets and provide\ncase studies that our model generates representations with respect to the\npunctuation in the sentence.", "published": "2020-12-10 19:07:31", "link": "http://arxiv.org/abs/2101.03029v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Framework for Generating Annotated Social Media Corpora with\n  Demographics, Stance, Civility, and Topicality", "abstract": "In this paper we introduce a framework for annotating a social media text\ncorpora for various categories. Since, social media data is generated via\nindividuals, it is important to annotate the text for the individuals\ndemographic attributes to enable a socio-technical analysis of the corpora.\nFurthermore, when analyzing a large data-set we can often annotate a small\nsample of data and then train a prediction model using this sample to annotate\nthe full data for the relevant categories. We use a case study of a Facebook\ncomment corpora on student loan discussion which was annotated for gender,\nmilitary affiliation, age-group, political leaning, race, stance, topicalilty,\nneoliberlistic views and civility of the comment. We release three datasets of\nFacebook comments for further research at:\nhttps://github.com/socialmediaie/StudentDebtFbComments", "published": "2020-12-10 04:06:25", "link": "http://arxiv.org/abs/2012.05444v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Causal BERT : Language models for causality detection between events\n  expressed in text", "abstract": "Causality understanding between events is a critical natural language\nprocessing task that is helpful in many areas, including health care, business\nrisk management and finance. On close examination, one can find a huge amount\nof textual content both in the form of formal documents or in content arising\nfrom social media like Twitter, dedicated to communicating and exploring\nvarious types of causality in the real world. Recognizing these \"Cause-Effect\"\nrelationships between natural language events continues to remain a challenge\nsimply because it is often expressed implicitly. Implicit causality is hard to\ndetect through most of the techniques employed in literature and can also, at\ntimes be perceived as ambiguous or vague. Also, although well-known datasets do\nexist for this problem, the examples in them are limited in the range and\ncomplexity of the causal relationships they depict especially when related to\nimplicit relationships. Most of the contemporary methods are either based on\nlexico-semantic pattern matching or are feature-driven supervised methods.\nTherefore, as expected these methods are more geared towards handling explicit\ncausal relationships leading to limited coverage for implicit relationships and\nare hard to generalize. In this paper, we investigate the language model's\ncapabilities for causal association among events expressed in natural language\ntext using sentence context combined with event information, and by leveraging\nmasked event context with in-domain and out-of-domain data distribution. Our\nproposed methods achieve the state-of-art performance in three different data\ndistributions and can be leveraged for extraction of a causal diagram and/or\nbuilding a chain of events from unstructured text.", "published": "2020-12-10 04:59:12", "link": "http://arxiv.org/abs/2012.05453v2", "categories": ["cs.CL", "cs.AI", "I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "AI Driven Knowledge Extraction from Clinical Practice Guidelines:\n  Turning Research into Practice", "abstract": "Background and Objectives: Clinical Practice Guidelines (CPGs) represent the\nforemost methodology for sharing state-of-the-art research findings in the\nhealthcare domain with medical practitioners to limit practice variations,\nreduce clinical cost, improve the quality of care, and provide evidence based\ntreatment. However, extracting relevant knowledge from the plethora of CPGs is\nnot feasible for already burdened healthcare professionals, leading to large\ngaps between clinical findings and real practices. It is therefore imperative\nthat state-of-the-art Computing research, especially machine learning is used\nto provide artificial intelligence based solution for extracting the knowledge\nfrom CPGs and reducing the gap between healthcare research/guidelines and\npractice. Methods: This research presents a novel methodology for knowledge\nextraction from CPGs to reduce the gap and turn the latest research findings\ninto clinical practice. First, our system classifies the CPG sentences into\nfour classes such as condition-action, condition-consequences, action, and\nnot-applicable based on the information presented in a sentence. We use deep\nlearning with state-of-the-art word embedding, improved word vectors technique\nin classification process. Second, it identifies qualifier terms in the\nclassified sentences, which assist in recognizing the condition and action\nphrases in a sentence. Finally, the condition and action phrase are processed\nand transformed into plain rule If Condition(s) Then Action format. Results: We\nevaluate the methodology on three different domains guidelines including\nHypertension, Rhinosinusitis, and Asthma. The deep learning model classifies\nthe CPG sentences with an accuracy of 95%. While rule extraction was validated\nby user-centric approach, which achieved a Jaccard coefficient of 0.6, 0.7, and\n0.4 with three human experts extracted rules, respectively.", "published": "2020-12-10 07:23:02", "link": "http://arxiv.org/abs/2012.05489v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Event Correlation Filtering Method for Fake News Detection", "abstract": "Nowadays, social network platforms have been the prime source for people to\nexperience news and events due to their capacities to spread information\nrapidly, which inevitably provides a fertile ground for the dissemination of\nfake news. Thus, it is significant to detect fake news otherwise it could cause\npublic misleading and panic. Existing deep learning models have achieved great\nprogress to tackle the problem of fake news detection. However, training an\neffective deep learning model usually requires a large amount of labeled news,\nwhile it is expensive and time-consuming to provide sufficient labeled news in\nactual applications. To improve the detection performance of fake news, we take\nadvantage of the event correlations of news and propose an event correlation\nfiltering method (ECFM) for fake news detection, mainly consisting of the news\ncharacterizer, the pseudo label annotator, the event credibility updater, and\nthe news entropy selector. The news characterizer is responsible for extracting\ntextual features from news, which cooperates with the pseudo label annotator to\nassign pseudo labels for unlabeled news by fully exploiting the event\ncorrelations of news. In addition, the event credibility updater employs\nadaptive Kalman filter to weaken the credibility fluctuations of events. To\nfurther improve the detection performance, the news entropy selector\nautomatically discovers high-quality samples from pseudo labeled news by\nquantifying their news entropy. Finally, ECFM is proposed to integrate them to\ndetect fake news in an event correlation filtering manner. Extensive\nexperiments prove that the explainable introduction of the event correlations\nof news is beneficial to improve the detection performance of fake news.", "published": "2020-12-10 07:31:07", "link": "http://arxiv.org/abs/2012.05491v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Approches quantitatives de l'analyse des pr{\u00e9}dictions en traduction\n  automatique neuronale (TAN)", "abstract": "As part of a larger project on optimal learning conditions in neural machine\ntranslation, we investigate characteristic training phases of translation\nengines. All our experiments are carried out using OpenNMT-Py: the\npre-processing step is implemented using the Europarl training corpus and the\nINTERSECT corpus is used for validation. Longitudinal analyses of training\nphases suggest that the progression of translations is not always linear.\nFollowing the results of textometric explorations, we identify the importance\nof the phenomena related to chronological progression, in order to map\ndifferent processes at work in neural machine translation (NMT).", "published": "2020-12-10 09:31:59", "link": "http://arxiv.org/abs/2012.05541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Longitudinal Citation Prediction using Temporal Graph Neural Networks", "abstract": "Citation count prediction is the task of predicting the number of citations a\npaper has gained after a period of time. Prior work viewed this as a static\nprediction task. As papers and their citations evolve over time, considering\nthe dynamics of the number of citations a paper will receive would seem\nlogical. Here, we introduce the task of sequence citation prediction. The goal\nis to accurately predict the trajectory of the number of citations a scholarly\nwork receives over time. We propose to view papers as a structured network of\ncitations, allowing us to use topological information as a learning signal.\nAdditionally, we learn how this dynamic citation network changes over time and\nthe impact of paper meta-data such as authors, venues and abstracts. To\napproach the new task, we derive a dynamic citation network from Semantic\nScholar spanning over 42 years. We present a model which exploits topological\nand temporal information using graph convolution networks paired with sequence\nprediction, and compare it against multiple baselines, testing the importance\nof topological and temporal information and analyzing model performance. Our\nexperiments show that leveraging both the temporal and topological information\ngreatly increases the performance of predicting citation counts over time.", "published": "2020-12-10 15:25:16", "link": "http://arxiv.org/abs/2012.05742v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bew: Towards Answering Business-Entity-Related Web Questions", "abstract": "We present BewQA, a system specifically designed to answer a class of\nquestions that we call Bew questions. Bew questions are related to\nbusinesses/services such as restaurants, hotels, and movie theaters; for\nexample, \"Until what time is happy hour?\". These questions are challenging to\nanswer because the answers are found in open-domain Web, are present in short\nsentences without surrounding context, and are dynamic since the webpage\ninformation can be updated frequently. Under these conditions, existing QA\nsystems perform poorly. We present a practical approach, called BewQA, that can\nanswer Bew queries by mining a template of the business-related webpages and\nusing the template to guide the search. We show how we can extract the template\nautomatically by leveraging aggregator websites that aggregate information\nabout business entities in a domain (e.g., restaurants). We answer a given\nquestion by identifying the section from the extracted template that is most\nlikely to contain the answer. By doing so we can extract the answers even when\nthe answer span does not have sufficient context. Importantly, BewQA does not\nrequire any training. We crowdsource a new dataset of 1066 Bew questions and\nground-truth answers in the restaurant domain. Compared to state-of-the-art QA\nmodels, BewQA has a 27 percent point improvement in F1 score. Compared to a\ncommercial search engine, BewQA answered correctly 29% more Bew questions.", "published": "2020-12-10 16:46:55", "link": "http://arxiv.org/abs/2012.05818v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Neural Programming Interfaces", "abstract": "It is notoriously difficult to control the behavior of artificial neural\nnetworks such as generative neural language models. We recast the problem of\ncontrolling natural language generation as that of learning to interface with a\npretrained language model, just as Application Programming Interfaces (APIs)\ncontrol the behavior of programs by altering hyperparameters. In this new\nparadigm, a specialized neural network (called a Neural Programming Interface\nor NPI) learns to interface with a pretrained language model by manipulating\nthe hidden activations of the pretrained model to produce desired outputs.\nImportantly, no permanent changes are made to the weights of the original\nmodel, allowing us to re-purpose pretrained models for new tasks without\noverwriting any aspect of the language model. We also contribute a new data set\nconstruction algorithm and GAN-inspired loss function that allows us to train\nNPI models to control outputs of autoregressive transformers. In experiments\nagainst other state-of-the-art approaches, we demonstrate the efficacy of our\nmethods using OpenAI's GPT-2 model, successfully controlling noun selection,\ntopic aversion, offensive speech filtering, and other aspects of language while\nlargely maintaining the controlled model's fluency under deterministic\nsettings.", "published": "2020-12-10 21:17:04", "link": "http://arxiv.org/abs/2012.05983v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Practical Approach towards Causality Mining in Clinical Text using\n  Active Transfer Learning", "abstract": "Objective: Causality mining is an active research area, which requires the\napplication of state-of-the-art natural language processing techniques. In the\nhealthcare domain, medical experts create clinical text to overcome the\nlimitation of well-defined and schema driven information systems. The objective\nof this research work is to create a framework, which can convert clinical text\ninto causal knowledge. Methods: A practical approach based on term expansion,\nphrase generation, BERT based phrase embedding and semantic matching, semantic\nenrichment, expert verification, and model evolution has been used to construct\na comprehensive causality mining framework. This active transfer learning based\nframework along with its supplementary services, is able to extract and enrich,\ncausal relationships and their corresponding entities from clinical text.\nResults: The multi-model transfer learning technique when applied over multiple\niterations, gains performance improvements in terms of its accuracy and recall\nwhile keeping the precision constant. We also present a comparative analysis of\nthe presented techniques with their common alternatives, which demonstrate the\ncorrectness of our approach and its ability to capture most causal\nrelationships. Conclusion: The presented framework has provided cutting-edge\nresults in the healthcare domain. However, the framework can be tweaked to\nprovide causality detection in other domains, as well. Significance: The\npresented framework is generic enough to be utilized in any domain, healthcare\nservices can gain massive benefits due to the voluminous and various nature of\nits data. This causal knowledge extraction framework can be used to summarize\nclinical text, create personas, discover medical knowledge, and provide\nevidence to clinical decision making.", "published": "2020-12-10 06:51:13", "link": "http://arxiv.org/abs/2012.07563v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research Challenges in Designing Differentially Private Text Generation\n  Mechanisms", "abstract": "Accurately learning from user data while ensuring quantifiable privacy\nguarantees provides an opportunity to build better Machine Learning (ML) models\nwhile maintaining user trust. Recent literature has demonstrated the\napplicability of a generalized form of Differential Privacy to provide\nguarantees over text queries. Such mechanisms add privacy preserving noise to\nvectorial representations of text in high dimension and return a text based\nprojection of the noisy vectors. However, these mechanisms are sub-optimal in\ntheir trade-off between privacy and utility. This is due to factors such as a\nfixed global sensitivity which leads to too much noise added in dense spaces\nwhile simultaneously guaranteeing protection for sensitive outliers. In this\nproposal paper, we describe some challenges in balancing the tradeoff between\nprivacy and utility for these differentially private text mechanisms. At a high\nlevel, we provide two proposals: (1) a framework called LAC which defers some\nof the noise to a privacy amplification step and (2), an additional suite of\nthree different techniques for calibrating the noise based on the local region\naround a word. Our objective in this paper is not to evaluate a single solution\nbut to further the conversation on these challenges and chart pathways for\nbuilding better mechanisms.", "published": "2020-12-10 01:44:50", "link": "http://arxiv.org/abs/2012.05403v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech\n  Recognition", "abstract": "In this paper, we present a novel two-pass approach to unify streaming and\nnon-streaming end-to-end (E2E) speech recognition in a single model. Our model\nadopts the hybrid CTC/attention architecture, in which the conformer layers in\nthe encoder are modified. We propose a dynamic chunk-based attention strategy\nto allow arbitrary right context length. At inference time, the CTC decoder\ngenerates n-best hypotheses in a streaming way. The inference latency could be\neasily controlled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. This efficient\nrescoring process causes very little sentence-level latency. Our experiments on\nthe open 170-hour AISHELL-1 dataset show that, the proposed method can unify\nthe streaming and non-streaming model simply and efficiently. On the AISHELL-1\ntest set, our unified model achieves 5.60% relative character error rate (CER)\nreduction in non-streaming ASR compared to a standard non-streaming\ntransformer. The same model achieves 5.42% CER with 640ms latency in a\nstreaming ASR system.", "published": "2020-12-10 06:54:54", "link": "http://arxiv.org/abs/2012.05481v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direct multimodal few-shot learning of speech and images", "abstract": "We propose direct multimodal few-shot models that learn a shared embedding\nspace of spoken words and images from only a few paired examples. Imagine an\nagent is shown an image along with a spoken word describing the object in the\npicture, e.g. pen, book and eraser. After observing a few paired examples of\neach class, the model is asked to identify the \"book\" in a set of unseen\npictures. Previous work used a two-step indirect approach relying on learned\nunimodal representations: speech-speech and image-image comparisons are\nperformed across the support set of given speech-image pairs. We propose two\ndirect models which instead learn a single multimodal space where inputs from\ndifferent modalities are directly comparable: a multimodal triplet network\n(MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these\ndirect models, we mine speech-image pairs: the support set is used to pair up\nunlabelled in-domain speech and images. In a speech-to-image digit matching\ntask, direct models outperform indirect models, with the MTriplet achieving the\nbest multimodal five-shot accuracy. We show that the improvements are due to\nthe combination of unsupervised and transfer learning in the direct models, and\nthe absence of two-step compounding errors.", "published": "2020-12-10 14:06:57", "link": "http://arxiv.org/abs/2012.05680v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Recurrent Point Review Models", "abstract": "Deep neural network models represent the state-of-the-art methodologies for\nnatural language processing. Here we build on top of these methodologies to\nincorporate temporal information and model how to review data changes with\ntime. Specifically, we use the dynamic representations of recurrent point\nprocess models, which encode the history of how business or service reviews are\nreceived in time, to generate instantaneous language models with improved\nprediction capabilities. Simultaneously, our methodologies enhance the\npredictive power of our point process models by incorporating summarized review\ncontent representations. We provide recurrent network and temporal convolution\nsolutions for modeling the review content. We deploy our methodologies in the\ncontext of recommender systems, effectively characterizing the change in\npreference and taste of users as time evolves. Source code is available at [1].", "published": "2020-12-10 14:11:42", "link": "http://arxiv.org/abs/2012.05684v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generative Deep Learning Techniques for Password Generation", "abstract": "Password guessing approaches via deep learning have recently been\ninvestigated with significant breakthroughs in their ability to generate novel,\nrealistic password candidates. In the present work we study a broad collection\nof deep learning and probabilistic based models in the light of password\nguessing: attention-based deep neural networks, autoencoding mechanisms and\ngenerative adversarial networks. We provide novel generative deep-learning\nmodels in terms of variational autoencoders exhibiting state-of-art sampling\nperformance, yielding additional latent-space features such as interpolations\nand targeted sampling. Lastly, we perform a thorough empirical analysis in a\nunified controlled framework over well-known datasets (RockYou, LinkedIn,\nYouku, Zomato, Pwnd). Our results not only identify the most promising schemes\ndriven by deep neural networks, but also illustrate the strengths of each\napproach in terms of generation variability and sample uniqueness.", "published": "2020-12-10 14:11:45", "link": "http://arxiv.org/abs/2012.05685v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Sentiment Analysis Approach to the Prediction of Market Volatility", "abstract": "Prediction and quantification of future volatility and returns play an\nimportant role in financial modelling, both in portfolio optimization and risk\nmanagement. Natural language processing today allows to process news and social\nmedia comments to detect signals of investors' confidence. We have explored the\nrelationship between sentiment extracted from financial news and tweets and\nFTSE100 movements. We investigated the strength of the correlation between\nsentiment measures on a given day and market volatility and returns observed\nthe next day. The findings suggest that there is evidence of correlation\nbetween sentiment and stock market movements: the sentiment captured from news\nheadlines could be used as a signal to predict market returns; the same does\nnot apply for volatility. Also, in a surprising finding, for the sentiment\nfound in Twitter comments we obtained a correlation coefficient of -0.7, and\np-value below 0.05, which indicates a strong negative correlation between\npositive sentiment captured from the tweets on a given day and the volatility\nobserved the next day. We developed an accurate classifier for the prediction\nof market volatility in response to the arrival of new information by deploying\ntopic modelling, based on Latent Dirichlet Allocation, to extract feature\nvectors from a collection of tweets and financial news. The obtained features\nwere used as additional input to the classifier. Thanks to the combination of\nsentiment and topic modelling our classifier achieved a directional prediction\naccuracy for volatility of 63%.", "published": "2020-12-10 01:15:48", "link": "http://arxiv.org/abs/2012.05906v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "I.2.7; I.5.4; J.1"], "primary_category": "q-fin.ST"}
{"title": "Exploring Deep Neural Networks and Transfer Learning for Analyzing\n  Emotions in Tweets", "abstract": "In this paper, we present an experiment on using deep learning and transfer\nlearning techniques for emotion analysis in tweets and suggest a method to\ninterpret our deep learning models. The proposed approach for emotion analysis\ncombines a Long Short Term Memory (LSTM) network with a Convolutional Neural\nNetwork (CNN). Then we extend this approach for emotion intensity prediction\nusing transfer learning technique. Furthermore, we propose a technique to\nvisualize the importance of each word in a tweet to get a better understanding\nof the model. Experimentally, we show in our analysis that the proposed models\noutperform the state-of-the-art in emotion classification while maintaining\ncompetitive results in predicting emotion intensity.", "published": "2020-12-10 23:45:53", "link": "http://arxiv.org/abs/2012.06025v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Transfer Learning for Reliable Intelligence Identification on\n  Vietnamese SNSs (ReINTEL)", "abstract": "This paper proposed several transformer-based approaches for Reliable\nIntelligence Identification on Vietnamese social network sites at VLSP 2020\nevaluation campaign. We exploit both of monolingual and multilingual\npre-trained models. Besides, we utilize the ensemble method to improve the\nrobustness of different approaches. Our team achieved a score of 0.9378 at\nROC-AUC metric in the private test set which is competitive to other\nparticipants.", "published": "2020-12-10 15:43:50", "link": "http://arxiv.org/abs/2012.07557v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Argument Mining Driven Analysis of Peer-Reviews", "abstract": "Peer reviewing is a central process in modern research and essential for\nensuring high quality and reliability of published work. At the same time, it\nis a time-consuming process and increasing interest in emerging fields often\nresults in a high review workload, especially for senior researchers in this\narea. How to cope with this problem is an open question and it is vividly\ndiscussed across all major conferences. In this work, we propose an Argument\nMining based approach for the assistance of editors, meta-reviewers, and\nreviewers. We demonstrate that the decision process in the field of scientific\npublications is driven by arguments and automatic argument identification is\nhelpful in various use-cases. One of our findings is that arguments used in the\npeer-review process differ from arguments in other domains making the transfer\nof pre-trained models difficult. Therefore, we provide the community with a new\npeer-review dataset from different computer science conferences with annotated\narguments. In our extensive empirical evaluation, we show that Argument Mining\ncan be used to efficiently extract the most relevant parts from reviews, which\nare paramount for the publication decision. The process remains interpretable\nsince the extracted arguments can be highlighted in a review without detaching\nthem from their context.", "published": "2020-12-10 16:06:21", "link": "http://arxiv.org/abs/2012.07743v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Learning Multiple Sound Source 2D Localization", "abstract": "In this paper, we propose novel deep learning based algorithms for multiple\nsound source localization. Specifically, we aim to find the 2D Cartesian\ncoordinates of multiple sound sources in an enclosed environment by using\nmultiple microphone arrays. To this end, we use an encoding-decoding\narchitecture and propose two improvements on it to accomplish the task. In\naddition, we also propose two novel localization representations which increase\nthe accuracy. Lastly, new metrics are developed relying on resolution-based\nmultiple source association which enables us to evaluate and compare different\nlocalization approaches. We tested our method on both synthetic and real world\ndata. The results show that our method improves upon the previous baseline\napproach for this problem.", "published": "2020-12-10 08:51:16", "link": "http://arxiv.org/abs/2012.05515v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data-Efficient Framework for Real-world Multiple Sound Source 2D\n  Localization", "abstract": "Deep neural networks have recently led to promising results for the task of\nmultiple sound source localization. Yet, they require a lot of training data to\ncover a variety of acoustic conditions and microphone array layouts. One can\nleverage acoustic simulators to inexpensively generate labeled training data.\nHowever, models trained on synthetic data tend to perform poorly with\nreal-world recordings due to the domain mismatch. Moreover, learning for\ndifferent microphone array layouts makes the task more complicated due to the\ninfinite number of possible layouts. We propose to use adversarial learning\nmethods to close the gap between synthetic and real domains. Our novel\nensemble-discrimination method significantly improves the localization\nperformance without requiring any label from the real data. Furthermore, we\npropose a novel explicit transformation layer to be embedded in the\nlocalization architecture. It enables the model to be trained with data from\nspecific microphone array layouts while generalizing well to unseen layouts\nduring inference.", "published": "2020-12-10 09:22:52", "link": "http://arxiv.org/abs/2012.05533v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ensemble of Discriminators for Domain Adaptation in Multiple Sound\n  Source 2D Localization", "abstract": "This paper introduces an ensemble of discriminators that improves the\naccuracy of a domain adaptation technique for the localization of multiple\nsound sources. Recently, deep neural networks have led to promising results for\nthis task, yet they require a large amount of labeled data for training.\nRecording and labeling such datasets is very costly, especially because data\nneeds to be diverse enough to cover different acoustic conditions. In this\npaper, we leverage acoustic simulators to inexpensively generate labeled\ntraining samples. However, models trained on synthetic data tend to perform\npoorly with real-world recordings due to the domain mismatch. For this, we\nexplore two domain adaptation methods using adversarial learning for sound\nsource localization which use labeled synthetic data and unlabeled real data.\nWe propose a novel ensemble approach that combines discriminators applied at\ndifferent feature levels of the localization model. Experiments show that our\nensemble discrimination method significantly improves the localization\nperformance without requiring any label from the real data.", "published": "2020-12-10 09:17:29", "link": "http://arxiv.org/abs/2012.05908v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Neural Networks for COVID-19 Detection and Diagnosis using Images\n  and Acoustic-based Techniques: A Recent Review", "abstract": "The new coronavirus disease (COVID-19) has been declared a pandemic since\nMarch 2020 by the World Health Organization. It consists of an emerging viral\ninfection with respiratory tropism that could develop atypical pneumonia.\nExperts emphasize the importance of early detection of those who have the\nCOVID-19 virus. In this way, patients will be isolated from other people and\nthe spread of the virus can be prevented. For this reason, it has become an\narea of interest to develop early diagnosis and detection methods to ensure a\nrapid treatment process and prevent the virus from spreading. Since the\nstandard testing system is time-consuming and not available for everyone,\nalternative early-screening techniques have become an urgent need. In this\nstudy, the approaches used in the detection of COVID-19 based on deep learning\n(DL) algorithms, which have been popular in recent years, have been\ncomprehensively discussed. The advantages and disadvantages of different\napproaches used in literature are examined in detail. The Computed Tomography\nof the chest and X-ray images give a rich representation of the patient's lung\nthat is less time-consuming and allows an efficient viral pneumonia detection\nusing the DL algorithms. The first step is the pre-processing of these images\nto remove noise. Next, deep features are extracted using multiple types of deep\nmodels (pre-trained models, generative models, generic neural networks, etc.).\nFinally, the classification is performed using the obtained features to decide\nwhether the patient is infected by coronavirus or it is another lung disease.\nIn this study, we also give a brief review of the latest applications of cough\nanalysis to early screen the COVID-19, and human mobility estimation to limit\nits spread.", "published": "2020-12-10 19:52:12", "link": "http://arxiv.org/abs/2012.07655v4", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
