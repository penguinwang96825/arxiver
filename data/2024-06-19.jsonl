{"title": "Adaptive Curves for Optimally Efficient Market Making", "abstract": "Automated Market Makers (AMMs) are essential in Decentralized Finance (DeFi)\nas they match liquidity supply with demand. They function through liquidity\nproviders (LPs) who deposit assets into liquidity pools. However, the asset\ntrading prices in these pools often trail behind those in more dynamic,\ncentralized exchanges, leading to potential arbitrage losses for LPs. This\nissue is tackled by adapting market maker bonding curves to trader behavior,\nbased on the classical market microstructure model of Glosten and Milgrom. Our\napproach ensures a zero-profit condition for the market maker's prices. We\nderive the differential equation that an optimal adaptive curve should follow\nto minimize arbitrage losses while remaining competitive. Solutions to this\noptimality equation are obtained for standard Gaussian and Lognormal price\nmodels using Kalman filtering. A key feature of our method is its ability to\nestimate the external market price without relying on price or loss oracles. We\nalso provide an equivalent differential equation for the implied dynamics of\ncanonical static bonding curves and establish conditions for their optimality.\nOur algorithms demonstrate robustness to changing market conditions and\nadversarial perturbations, and we offer an on-chain implementation using\nUniswap v4 alongside off-chain AI co-processors.", "published": "2024-06-19 19:42:38", "link": "http://arxiv.org/abs/2406.13794v2", "categories": ["eess.SY", "cs.CE", "cs.SY", "q-fin.TR"], "primary_category": "eess.SY"}
{"title": "Learning to Generate Answers with Citations via Factual Consistency\n  Models", "abstract": "Large Language Models (LLMs) frequently hallucinate, impeding their\nreliability in mission-critical situations. One approach to address this issue\nis to provide citations to relevant sources alongside generated content,\nenhancing the verifiability of generations. However, citing passages accurately\nin answers remains a substantial challenge. This paper proposes a\nweakly-supervised fine-tuning method leveraging factual consistency models\n(FCMs). Our approach alternates between generating texts with citations and\nsupervised fine-tuning with FCM-filtered citation data. Focused learning is\nintegrated into the objective, directing the fine-tuning process to emphasise\nthe factual unit tokens, as measured by an FCM. Results on the ALCE few-shot\ncitation benchmark with various instruction-tuned LLMs demonstrate superior\nperformance compared to in-context learning, vanilla supervised fine-tuning,\nand state-of-the-art methods, with an average improvement of $34.1$, $15.5$,\nand $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfer\nsetting we show that the obtained citation generation ability robustly\ntransfers to unseen datasets. Notably, our citation improvements contribute to\nthe lowest factual error rate across baselines.", "published": "2024-06-19 00:40:19", "link": "http://arxiv.org/abs/2406.13124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Parts Are Greater Than Sums: Individual LLM Components Can\n  Outperform Full Models", "abstract": "This paper studies in-context learning by decomposing the output of large\nlanguage models into the individual contributions of attention heads and MLPs\n(components). We observe curious components: good-performing ones that\nindividually do well on a classification task, even when the model performs\npoorly; bad-performing ones that do much worse than chance; and label-biased\ncomponents that always predict the same label. We find that component\naccuracies are well-correlated across different demonstration sets and\nperturbations of prompt templates. Based on our findings, we propose component\nreweighting, which learns to linearly re-scale the component activations from a\nfew labeled examples. Given 24 labeled examples, our method improves by an\naverage of 6.0% accuracy points over 24-shot ICL across 8 tasks on Llama-2-7B.\nOverall, this paper both enriches our understanding of ICL and provides a\npractical method for improvement by examining model internals.", "published": "2024-06-19 00:48:44", "link": "http://arxiv.org/abs/2406.13131v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Diversity in Healthcare LLM Research: A Scientometric\n  Perspective", "abstract": "The deployment of large language models (LLMs) in healthcare has demonstrated\nsubstantial potential for enhancing clinical decision-making, administrative\nefficiency, and patient outcomes. However, the underrepresentation of diverse\ngroups in the development and application of these models can perpetuate\nbiases, leading to inequitable healthcare delivery. This paper presents a\ncomprehensive scientometric analysis of LLM research for healthcare, including\ndata from January 1, 2021, to July 1, 2024. By analyzing metadata from PubMed\nand Dimensions, including author affiliations, countries, and funding sources,\nwe assess the diversity of contributors to LLM research. Our findings highlight\nsignificant gender and geographic disparities, with a predominance of male\nauthors and contributions primarily from high-income countries (HICs). We\nintroduce a novel journal diversity index based on Gini diversity to measure\nthe inclusiveness of scientific publications. Our results underscore the\nnecessity for greater representation in order to ensure the equitable\napplication of LLMs in healthcare. We propose actionable strategies to enhance\ndiversity and inclusivity in artificial intelligence research, with the\nultimate goal of fostering a more inclusive and equitable future in healthcare\ninnovation.", "published": "2024-06-19 02:00:51", "link": "http://arxiv.org/abs/2406.13152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QRMeM: Unleash the Length Limitation through Question then Reflection\n  Memory Mechanism", "abstract": "While large language models (LLMs) have made notable advancements in natural\nlanguage processing, they continue to struggle with processing extensive text.\nMemory mechanism offers a flexible solution for managing long contexts,\nutilizing techniques such as compression, summarization, and structuring to\nfacilitate nuanced and efficient handling of large volumes of text. However,\nexisting techniques face challenges with static knowledge integration, leading\nto insufficient adaptation to task-specific needs and missing\nmulti-segmentation relationships, which hinders the dynamic reorganization and\nlogical combination of relevant segments during the response process. To\naddress these issues, we introduce a novel strategy, Question then Reflection\nMemory Mechanism (QRMeM), incorporating a dual-structured memory pool. This\npool synergizes static textual content with structured graph guidance,\nfostering a reflective trial-and-error approach for navigating and identifying\nrelevant segments. Our evaluation across multiple-choice questions (MCQ) and\nmulti-document question answering (Multi-doc QA) benchmarks showcases QRMeM\nenhanced performance compared to existing approaches.", "published": "2024-06-19 02:46:18", "link": "http://arxiv.org/abs/2406.13167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating and Extracting Relational Concepts in Large Language Models", "abstract": "Relational concepts are indeed foundational to the structure of knowledge\nrepresentation, as they facilitate the association between various entity\nconcepts, allowing us to express and comprehend complex world knowledge. By\nexpressing relational concepts in natural language prompts, people can\neffortlessly interact with large language models (LLMs) and recall desired\nfactual knowledge. However, the process of knowledge recall lacks\ninterpretability, and representations of relational concepts within LLMs remain\nunknown to us. In this paper, we identify hidden states that can express entity\nand relational concepts through causal mediation analysis in fact recall\nprocesses. Our finding reveals that at the last token position of the input\nprompt, there are hidden states that solely express the causal effects of\nrelational concepts. Based on this finding, we assume that these hidden states\ncan be treated as relational representations and we can successfully extract\nthem from LLMs. The experimental results demonstrate high credibility of the\nrelational representations: they can be flexibly transplanted into other fact\nrecall processes, and can also be used as robust entity connectors. Moreover,\nwe also show that the relational representations exhibit significant potential\nfor controllable fact recall through relation rewriting.", "published": "2024-06-19 03:29:51", "link": "http://arxiv.org/abs/2406.13184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIVE: Learnable In-Context Vector for Visual Question Answering", "abstract": "As language models continue to scale, Large Language Models (LLMs) have\nexhibited emerging capabilities in In-Context Learning (ICL), enabling them to\nsolve language tasks by prefixing a few in-context demonstrations (ICDs) as\ncontext. Inspired by these advancements, researchers have extended these\ntechniques to develop Large Multimodal Models (LMMs) with ICL capabilities.\nHowever, applying ICL usually faces two major challenges: 1) using more ICDs\nwill largely increase the inference time and 2) the performance is sensitive to\nthe selection of ICDs. These challenges are further exacerbated in LMMs due to\nthe integration of multiple data types and the combinational complexity of\nmultimodal ICDs. Recently, to address these challenges, some NLP studies\nintroduce non-learnable In-Context Vectors (ICVs) which extract useful task\ninformation from ICDs into a single vector and then insert it into the LLM to\nhelp solve the corresponding task. However, although useful in simple NLP\ntasks, these non-learnable methods fail to handle complex multimodal tasks like\nVisual Question Answering (VQA). In this study, we propose Learnable In-Context\nVEctor (LIVE) to distill essential task information from demonstrations,\nimproving ICL performance in LMMs. Experiments show that LIVE can significantly\nreduce computational costs while enhancing accuracy in VQA tasks compared to\ntraditional ICL and other non-learnable ICV methods. The code is available at\n\\url{https://github.com/ForJadeForest/LIVE-Learnable-In-Context-Vector}.", "published": "2024-06-19 03:33:45", "link": "http://arxiv.org/abs/2406.13185v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Translations via Matrix Completion", "abstract": "Bilingual Lexicon Induction is the task of learning word translations without\nbilingual parallel corpora. We model this task as a matrix completion problem,\nand present an effective and extendable framework for completing the matrix.\nThis method harnesses diverse bilingual and monolingual signals, each of which\nmay be incomplete or noisy. Our model achieves state-of-the-art performance for\nboth high and low resource languages.", "published": "2024-06-19 04:03:19", "link": "http://arxiv.org/abs/2406.13195v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Bridging Law and Data: Augmenting Reasoning via a Semi-Structured\n  Dataset with IRAC methodology", "abstract": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLEGALSEMI, a benchmark specifically curated for legal scenario analysis.\nLEGALSEMI comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework. In addition, LEGALSEMI is accompanied by a structured knowledge\ngraph (SKG). A series of experiments were conducted to assess the usefulness of\nLEGALSEMI for IRAC analysis. The experimental results demonstrate the\neffectiveness of incorporating the SKG for issue identification, rule\nretrieval, application and conclusion generation using four different LLMs.\nLEGALSEMI will be publicly available upon acceptance of this paper.", "published": "2024-06-19 04:59:09", "link": "http://arxiv.org/abs/2406.13217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Language Model Factuality via Activation-Based Confidence\n  Calibration and Guided Decoding", "abstract": "Calibrating language models (LMs) aligns their generation confidence with the\nactual likelihood of answer correctness, which can inform users about LMs'\nreliability and mitigate hallucinated content. However, prior calibration\nmethods, such as self-consistency-based and logit-based approaches, are either\nlimited in inference-time efficiency or fall short of providing informative\nsignals. Moreover, simply filtering out low-confidence responses reduces the\nLM's helpfulness when the answers are correct. Therefore, effectively using\ncalibration techniques to enhance an LM's factuality remains an unsolved\nchallenge. In this paper, we first propose an activation-based calibration\nmethod, ActCab, which trains a linear layer on top of the LM's last-layer\nactivations that can better capture the representations of knowledge. Built on\ntop of ActCab, we further propose CoDec, a confidence-guided decoding strategy\nto elicit truthful answers with high confidence from LMs. By evaluating on five\npopular QA benchmarks, ActCab achieves superior calibration performance than\nall competitive baselines, e.g., by reducing the average expected calibration\nerror (ECE) score by up to 39%. Further experiments on CoDec show consistent\nimprovements in several LMs' factuality on challenging QA datasets, such as\nTruthfulQA, highlighting the value of confidence signals in enhancing\nfactuality.", "published": "2024-06-19 05:33:34", "link": "http://arxiv.org/abs/2406.13230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning on a Budget: A Case Study in Token Classification", "abstract": "Few shot in-context learning (ICL) typically assumes access to large\nannotated training sets. However, in many real world scenarios, such as domain\nadaptation, there is only a limited budget to annotate a small number of\nsamples, with the goal of maximizing downstream performance. We study various\nmethods for selecting samples to annotate within a predefined budget, focusing\non token classification tasks, which are expensive to annotate and are\nrelatively less studied in ICL setups. Across various tasks, models, and\ndatasets, we observe that no method significantly outperforms the others, with\nmost yielding similar results, including random sample selection for\nannotation. Moreover, we demonstrate that a relatively small annotated sample\npool can achieve performance comparable to using the entire training set. We\nhope that future work adopts our realistic paradigm which takes annotation\nbudget into account.", "published": "2024-06-19 07:09:46", "link": "http://arxiv.org/abs/2406.13274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective", "abstract": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation.", "published": "2024-06-19 07:23:33", "link": "http://arxiv.org/abs/2406.13282v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "abstract": "In the Retrieval-Augmented Generation (RAG) system, advanced Large Language\nModels (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an\nunsupervised way, which re-rank documents based on the probability of\ngenerating the query given the content of a document. However, directly\nprompting LLMs to approximate QLMs inherently is biased, where the estimated\ndistribution might diverge from the actual document-specific distribution. In\nthis study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages\nBayesian decision theory to both quantify and mitigate this estimation bias.\nSpecifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the\nprobability of document generation, thereby harmonizing the optimization of\nquery and document generation probabilities under a unified risk minimization\nobjective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly\nenhances re-ranking, particularly in improving the Top-1 accuracy. It benefits\nthe QA tasks by achieving higher accuracy with fewer input documents.", "published": "2024-06-19 08:29:54", "link": "http://arxiv.org/abs/2406.13331v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How effective is Multi-source pivoting for Translation of Low Resource\n  Indian Languages?", "abstract": "Machine Translation (MT) between linguistically dissimilar languages is\nchallenging, especially due to the scarcity of parallel corpora. Prior works\nsuggest that pivoting through a high-resource language can help translation\ninto a related low-resource language. However, existing works tend to discard\nthe source sentence when pivoting. Taking the case of English to Indian\nlanguage MT, this paper explores the 'multi-source translation' approach with\npivoting, using both source and pivot sentences to improve translation. We\nconducted extensive experiments with various multi-source techniques for\ntranslating English to Konkani, Manipuri, Sanskrit, and Bodo, using Hindi,\nMarathi, and Bengali as pivot languages. We find that multi-source pivoting\nyields marginal improvements over the state-of-the-art, contrary to previous\nclaims, but these improvements can be enhanced with synthetic target language\ndata. We believe multi-source pivoting is a promising direction for\nLow-resource translation.", "published": "2024-06-19 08:31:52", "link": "http://arxiv.org/abs/2406.13332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Structural Generalization in Neural Machine Translation", "abstract": "Compositional generalization refers to the ability to generalize to novel\ncombinations of previously observed words and syntactic structures. Since it is\nregarded as a desired property of neural models, recent work has assessed\ncompositional generalization in machine translation as well as semantic\nparsing. However, previous evaluations with machine translation have focused\nmostly on lexical generalization (i.e., generalization to unseen combinations\nof known words). Thus, it remains unclear to what extent models can translate\nsentences that require structural generalization (i.e., generalization to\ndifferent sorts of syntactic structures). To address this question, we\nconstruct SGET, a machine translation dataset covering various types of\ncompositional generalization with control of words and sentence structures. We\nevaluate neural machine translation models on SGET and show that they struggle\nmore in structural generalization than in lexical generalization. We also find\ndifferent performance trends in semantic parsing and machine translation, which\nindicates the importance of evaluations across various tasks.", "published": "2024-06-19 09:09:11", "link": "http://arxiv.org/abs/2406.13363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALiiCE: Evaluating Positional Fine-grained Citation Generation", "abstract": "Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations.", "published": "2024-06-19 09:16:14", "link": "http://arxiv.org/abs/2406.13375v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration", "abstract": "Existing LLMs exhibit remarkable performance on various NLP tasks, but still\nstruggle with complex real-world tasks, even equipped with advanced strategies\nlike CoT and ReAct. In this work, we propose the CoAct framework, which\ntransfers the hierarchical planning and collaboration patterns in human society\nto LLM systems. Specifically, our CoAct framework involves two agents: (1) A\nglobal planning agent, to comprehend the problem scope, formulate macro-level\nplans and provide detailed sub-task descriptions to local execution agents,\nwhich serves as the initial rendition of a global plan. (2) A local execution\nagent, to operate within the multi-tier task execution structure, focusing on\ndetailed execution and implementation of specific tasks within the global plan.\nExperimental results on the WebArena benchmark show that CoAct can re-arrange\nthe process trajectory when facing failures, and achieves superior performance\nover baseline methods on long-horizon web tasks. Code is available at\nhttps://github.com/xmhou2002/CoAct.", "published": "2024-06-19 09:23:53", "link": "http://arxiv.org/abs/2406.13381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoreHopQA: More Than Multi-hop Reasoning", "abstract": "Most existing multi-hop datasets are extractive answer datasets, where the\nanswers to the questions can be extracted directly from the provided context.\nThis often leads models to use heuristics or shortcuts instead of performing\ntrue multi-hop reasoning. In this paper, we propose a new multi-hop dataset,\nMoreHopQA, which shifts from extractive to generative answers. Our dataset is\ncreated by utilizing three existing multi-hop datasets: HotpotQA,\n2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning,\nwe enhance the existing multi-hop questions by adding another layer of\nquestioning that involves one, two, or all three of the following types of\nreasoning: commonsense, arithmetic, and symbolic. Our dataset is created\nthrough a semi-automated process, resulting in a dataset with 1,118 samples\nthat have undergone human verification. We then use our dataset to evaluate\nfive different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and\n70B), and GPT-4. We also design various cases to analyze the reasoning steps in\nthe question-answering process. Our results show that models perform well on\ninitial multi-hop questions but struggle with our extended questions,\nindicating that our dataset is more challenging than previous ones. Our\nanalysis of question decomposition reveals that although models can correctly\nanswer questions, only a portion - 38.7% for GPT-4 and 33.4% for Llama3-70B -\nachieve perfect reasoning, where all corresponding sub-questions are answered\ncorrectly. Evaluation code and data are available at\nhttps://github.com/Alab-NII/morehopqa", "published": "2024-06-19 09:38:59", "link": "http://arxiv.org/abs/2406.13397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing via\n  Consistency-Enhanced Multi-Agent Collaboration", "abstract": "While fine-tuned large language models (LLMs) excel in generating\ngrammatically valid SQL in Text-to-SQL parsing, they often struggle to ensure\nsemantic accuracy in queries, leading to user confusion and diminished system\nusability. To tackle this challenge, we introduce SQLFixAgent, a new\nconsistency-enhanced multi-agent collaborative framework designed for detecting\nand repairing erroneous SQL. Our framework comprises a core agent, SQLRefiner,\nalongside two auxiliary agents: SQLReviewer and QueryCrafter. The SQLReviewer\nagent employs the rubber duck debugging method to identify potential semantic\nmismatches between SQL and user query. If the error is detected, the\nQueryCrafter agent generates multiple SQL as candidate repairs using a\nfine-tuned SQLTool. Subsequently, leveraging similar repair retrieval and\nfailure memory reflection, the SQLRefiner agent selects the most fitting SQL\nstatement from the candidates as the final repair. We evaluated our proposed\nframework on five Text-to-SQL benchmarks. The experimental results show that\nour method consistently enhances the performance of the baseline model,\nspecifically achieving an execution accuracy improvement of over 3% on the Bird\nbenchmark. Our framework also has a higher token efficiency compared to other\nadvanced methods, making it more competitive.", "published": "2024-06-19 09:57:19", "link": "http://arxiv.org/abs/2406.13408v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "abstract": "Large Language Models (LLMs) are increasingly relied upon to evaluate text\noutputs of other LLMs, thereby influencing leaderboards and development\ndecisions. However, concerns persist over the accuracy of these assessments and\nthe potential for misleading conclusions. In this work, we investigate the\neffectiveness of LLMs as evaluators for text generation tasks. We propose FBI,\na novel framework designed to examine the proficiency of Evaluator LLMs in\nassessing four critical abilities in other LLMs: factual accuracy, instruction\nfollowing, coherence in long-form writing, and reasoning proficiency. By\nintroducing targeted perturbations in answers generated by LLMs, that clearly\nimpact one of these key capabilities, we test whether an Evaluator LLM can\ndetect these quality drops. By creating a total of 2400 perturbed answers\ncovering 22 perturbation categories, we conduct a comprehensive study using\ndifferent evaluation strategies on five prominent LLMs commonly used as\nevaluators in the literature. Our findings reveal significant shortcomings in\ncurrent Evaluator LLMs, which failed to identify quality drops in over 50\\% of\ncases on average. Single-answer and pairwise evaluations demonstrated notable\nlimitations, whereas reference-based evaluations showed comparatively better\nperformance. These results underscore the unreliable nature of current\nEvaluator LLMs and advocate for cautious implementation in practical\napplications. Code and data are available at https://github.com/AI4Bharat/FBI.", "published": "2024-06-19 10:59:48", "link": "http://arxiv.org/abs/2406.13439v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual-Phase Accelerated Prompt Optimization", "abstract": "Gradient-free prompt optimization methods have made significant strides in\nenhancing the performance of closed-source Large Language Models (LLMs) across\na wide range of tasks. However, existing approaches make light of the\nimportance of high-quality prompt initialization and the identification of\neffective optimization directions, thus resulting in substantial optimization\nsteps to obtain satisfactory performance. In this light, we aim to accelerate\nprompt optimization process to tackle the challenge of low convergence rate. We\npropose a dual-phase approach which starts with generating high-quality initial\nprompts by adopting a well-designed meta-instruction to delve into\ntask-specific information, and iteratively optimize the prompts at the sentence\nlevel, leveraging previous tuning experience to expand prompt candidates and\naccept effective ones. Extensive experiments on eight datasets demonstrate the\neffectiveness of our proposed method, achieving a consistent accuracy gain over\nbaselines with less than five optimization steps.", "published": "2024-06-19 11:08:56", "link": "http://arxiv.org/abs/2406.13443v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "abstract": "The advent of transformers has fueled progress in machine translation. More\nrecently large language models (LLMs) have come to the spotlight thanks to\ntheir generality and strong performance in a wide range of language tasks,\nincluding translation. Here we show that open-source LLMs perform on par with\nor better than some state-of-the-art baselines in simultaneous machine\ntranslation (SiMT) tasks, zero-shot. We also demonstrate that injection of\nminimal background information, which is easy with an LLM, brings further\nperformance gains, especially on challenging technical subject-matter. This\nhighlights LLMs' potential for building next generation of massively\nmultilingual, context-aware and terminologically accurate SiMT systems that\nrequire no resource-intensive training or fine-tuning.", "published": "2024-06-19 11:57:42", "link": "http://arxiv.org/abs/2406.13476v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social\n  Media Data and Masked Language Models", "abstract": "Social biases such as gender or racial biases have been reported in language\nmodels (LMs), including Masked Language Models (MLMs). Given that MLMs are\ncontinuously trained with increasing amounts of additional data collected over\ntime, an important yet unanswered question is how the social biases encoded\nwith MLMs vary over time. In particular, the number of social media users\ncontinues to grow at an exponential rate, and it is a valid concern for the\nMLMs trained specifically on social media data whether their social biases (if\nany) would also amplify over time. To empirically analyse this problem, we use\na series of MLMs pretrained on chronologically ordered temporal snapshots of\ncorpora. Our analysis reveals that, although social biases are present in all\nMLMs, most types of social bias remain relatively stable over time (with a few\nexceptions). To further understand the mechanisms that influence social biases\nin MLMs, we analyse the temporal corpora used to train the MLMs. Our findings\nshow that some demographic groups, such as male, obtain higher preference over\nthe other, such as female on the training corpora constantly.", "published": "2024-06-19 13:45:21", "link": "http://arxiv.org/abs/2406.13556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexically Grounded Subword Segmentation", "abstract": "We present three innovations in tokenization and subword segmentation. First,\nwe propose to use unsupervised morphological analysis with Morfessor as\npre-tokenization. Second, we present an algebraic method for obtaining subword\nembeddings grounded in a word embedding space. Based on that, we design a novel\nsubword segmentation algorithm that uses the embeddings, ensuring that the\nprocedure considers lexical meaning. Third, we introduce an efficient\nsegmentation algorithm based on a subword bigram model that can be initialized\nwith the lexically aware segmentation method to avoid using Morfessor and large\nembedding tables at inference time. We evaluate the proposed approaches using\ntwo intrinsic metrics and measure their performance on two downstream tasks:\npart-of-speech tagging and machine translation. Our experiments show\nsignificant improvements in the morphological plausibility of the segmentation\nwhen evaluated using segmentation precision on morpheme boundaries and improved\nR\\'enyi efficiency in 8 languages. Although the proposed tokenization methods\ndo not have a large impact on automatic translation quality, we observe\nconsistent performance gains in the arguably more morphological task of\npart-of-speech tagging.", "published": "2024-06-19 13:48:19", "link": "http://arxiv.org/abs/2406.13560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Distractor Generation for Multiple-Choice Questions with\n  Retrieval Augmented Pretraining and Knowledge Graph Integration", "abstract": "In this paper, we tackle the task of distractor generation (DG) for\nmultiple-choice questions. Our study introduces two key designs. First, we\npropose \\textit{retrieval augmented pretraining}, which involves refining the\nlanguage model pretraining to align it more closely with the downstream task of\nDG. Second, we explore the integration of knowledge graphs to enhance the\nperformance of DG. Through experiments with benchmarking datasets, we show that\nour models significantly outperform the state-of-the-art results. Our\nbest-performing model advances the F1@3 score from 14.80 to 16.47 in MCQ\ndataset and from 15.92 to 16.50 in Sciq dataset.", "published": "2024-06-19 14:12:05", "link": "http://arxiv.org/abs/2406.13578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Former: Lightning-fast Compressing Context for Large Language\n  Model", "abstract": "With the rising popularity of Transformer-based large language models (LLMs),\nreducing their high inference costs has become a significant research focus.\nOne effective approach is to compress the long input contexts. Existing methods\ntypically leverage the self-attention mechanism of the LLM itself for context\ncompression. While these methods have achieved notable results, the compression\nprocess still involves quadratic time complexity, which limits their\napplicability. To mitigate this limitation, we propose the In-Context Former\n(IC-Former). Unlike previous methods, IC-Former does not depend on the target\nLLMs. Instead, it leverages the cross-attention mechanism and a small number of\nlearnable digest tokens to directly condense information from the contextual\nword embeddings. This approach significantly reduces inference time, which\nachieves linear growth in time complexity within the compression range.\nExperimental results indicate that our method requires only 1/32 of the\nfloating-point operations of the baseline during compression and improves\nprocessing speed by 68 to 112 times while achieving over 90% of the baseline\nperformance on evaluation metrics. Overall, our model effectively reduces\ncompression costs and makes real-time compression scenarios feasible.", "published": "2024-06-19 15:14:55", "link": "http://arxiv.org/abs/2406.13618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations", "abstract": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.", "published": "2024-06-19 15:28:29", "link": "http://arxiv.org/abs/2406.13632v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jailbreaking Large Language Models Through Alignment Vulnerabilities in\n  Out-of-Distribution Settings", "abstract": "Recently, Large Language Models (LLMs) have garnered significant attention\nfor their exceptional natural language processing capabilities. However,\nconcerns about their trustworthiness remain unresolved, particularly in\naddressing ``jailbreaking'' attacks on aligned LLMs. Previous research\npredominantly relies on scenarios involving white-box LLMs or specific, fixed\nprompt templates, which are often impractical and lack broad applicability. In\nthis paper, we introduce a straightforward and novel method called\nObscurePrompt for jailbreaking LLMs, inspired by the observed fragile\nalignments in Out-of-Distribution (OOD) data. Specifically, we first formulate\nthe decision boundary in the jailbreaking process and then explore how obscure\ntext affects LLM's ethical decision boundary. ObscurePrompt starts with\nconstructing a base prompt that integrates well-known jailbreaking techniques.\nPowerful LLMs are then utilized to obscure the original prompt through\niterative transformations, aiming to bolster the attack's robustness.\nComprehensive experiments show that our approach substantially improves upon\nprevious methods in terms of attack effectiveness, maintaining efficacy against\ntwo prevalent defense mechanisms.", "published": "2024-06-19 16:09:58", "link": "http://arxiv.org/abs/2406.13662v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented\n  Generation", "abstract": "Retrieval-augmented language models (RALMs) have shown strong performance and\nwide applicability in knowledge-intensive tasks. However, there are significant\ntrustworthiness concerns as RALMs are prone to generating unfaithful outputs,\nincluding baseless information or contradictions with the retrieved context.\nThis paper proposes SynCheck, a lightweight monitor that leverages fine-grained\ndecoding dynamics including sequence likelihood, uncertainty quantification,\ncontext influence, and semantic alignment to synchronously detect unfaithful\nsentences. By integrating efficiently measurable and complementary signals,\nSynCheck enables accurate and immediate feedback and intervention, achieving\n0.85 AUROC in detecting faithfulness errors across six long-form\nretrieval-augmented generation tasks, improving prior best method by 4%.\nLeveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding\nalgorithm guided by beam search for long-form retrieval-augmented generation.\nEmpirical results demonstrate that FOD outperforms traditional strategies such\nas abstention, reranking, or contrastive decoding significantly in terms of\nfaithfulness, achieving over 10% improvement across six datasets.", "published": "2024-06-19 16:42:57", "link": "http://arxiv.org/abs/2406.13692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of\n  Metaphorical Language", "abstract": "Machine Translation (MT) has developed rapidly since the release of Large\nLanguage Models and current MT evaluation is performed through comparison with\nreference human translations or by predicting quality scores from human-labeled\ndata. However, these mainstream evaluation methods mainly focus on fluency and\nfactual reliability, whilst paying little attention to figurative quality. In\nthis paper, we investigate the figurative quality of MT and propose a set of\nhuman evaluation metrics focused on the translation of figurative language. We\nadditionally present a multilingual parallel metaphor corpus generated by\npost-editing. Our evaluation protocol is designed to estimate four aspects of\nMT: Metaphorical Equivalence, Emotion, Authenticity, and Quality. In doing so,\nwe observe that translations of figurative expressions display different traits\nfrom literal ones.", "published": "2024-06-19 16:52:22", "link": "http://arxiv.org/abs/2406.13698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Open-Source and Proprietary LLMs for Machine Reading\n  Comprehension: A Practical Analysis for Industrial Applications", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in various Natural Language Processing (NLP) applications, such as\nsentiment analysis, content generation, and personalized recommendations.\nDespite their impressive capabilities, there remains a significant need for\nsystematic studies concerning the practical application of LLMs in industrial\nsettings, as well as the specific requirements and challenges related to their\ndeployment in these contexts. This need is particularly critical for Machine\nReading Comprehension (MCR), where factual, concise, and accurate responses are\nrequired. To date, most MCR rely on Small Language Models (SLMs) or Recurrent\nNeural Networks (RNNs) such as Long Short-Term Memory (LSTM). This trend is\nevident in the SQuAD2.0 rankings on the Papers with Code table. This article\npresents a comparative analysis between open-source LLMs and proprietary models\non this task, aiming to identify light and open-source alternatives that offer\ncomparable performance to proprietary models.", "published": "2024-06-19 17:11:51", "link": "http://arxiv.org/abs/2406.13713v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models along Dimensions of Language Variation:\n  A Systematik Invesdigatiom uv Cross-lingual Generalization", "abstract": "While large language models exhibit certain cross-lingual generalization\ncapabilities, they suffer from performance degradation (PD) on unseen\nclosely-related languages (CRLs) and dialects relative to their high-resource\nlanguage neighbour (HRLN). However, we currently lack a fundamental\nunderstanding of what kinds of linguistic distances contribute to PD, and to\nwhat extent. Furthermore, studies of cross-lingual generalization are\nconfounded by unknown quantities of CRL language traces in the training data,\nand by the frequent lack of availability of evaluation data in lower-resource\nrelated languages and dialects. To address these issues, we model phonological,\nmorphological, and lexical distance as Bayesian noise processes to synthesize\nartificial languages that are controllably distant from the HRLN. We analyse PD\nas a function of underlying noise parameters, offering insights on model\nrobustness to isolated and composed linguistic phenomena, and the impact of\ntask and HRL characteristics on PD. We calculate parameter posteriors on real\nCRL-HRLN pair data and show that they follow computed trends of artificial\nlanguages, demonstrating the viability of our noisers. Our framework offers a\ncheap solution for estimating task performance on an unseen CRL given HRLN\nperformance using its posteriors, as well as for diagnosing observed PD on a\nCRL in terms of its linguistic distances from its HRLN, and opens doors to\nprincipled methods of mitigating performance degradation.", "published": "2024-06-19 17:20:28", "link": "http://arxiv.org/abs/2406.13718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Reason in the Wild with Programs?", "abstract": "Large Language Models (LLMs) have shown superior capability to solve\nreasoning problems with programs. While being a promising direction, most of\nsuch frameworks are trained and evaluated in settings with a prior knowledge of\ntask requirements. However, as LLMs become more capable, it is necessary to\nassess their reasoning abilities in more realistic scenarios where many\nreal-world problems are open-ended with ambiguous scope, and often require\nmultiple formalisms to solve. To investigate this, we introduce the task of\nreasoning in the wild, where an LLM is tasked to solve a reasoning problem of\nunknown type by identifying the subproblems and their corresponding formalisms,\nand writing a program to solve each subproblem, guided by a tactic. We create a\nlarge tactic-guided trajectory dataset containing detailed solutions to a\ndiverse set of reasoning problems, ranging from well-defined single-form\nreasoning (e.g., math, logic), to ambiguous and hybrid ones (e.g., commonsense,\ncombined math and logic). This allows us to test various aspects of LLMs\nreasoning at the fine-grained level such as the selection and execution of\ntactics, and the tendency to take undesired shortcuts. In experiments, we\nhighlight that existing LLMs fail significantly on problems with ambiguous and\nmixed scope, revealing critical limitations and overfitting issues (e.g.\naccuracy on GSM8K drops by at least 50\\%). We further show the potential of\nfinetuning a local LLM on the tactic-guided trajectories in achieving better\nperformance. Project repo is available at\ngithub.com/gblackout/Reason-in-the-Wild", "published": "2024-06-19 18:26:19", "link": "http://arxiv.org/abs/2406.13764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FoRAG: Factuality-optimized Retrieval Augmented Generation for\n  Web-enhanced Long-form Question Answering", "abstract": "Retrieval Augmented Generation (RAG) has become prevalent in\nquestion-answering (QA) tasks due to its ability of utilizing search engine to\nenhance the quality of long-form question-answering (LFQA). Despite the\nemergence of various open source methods and web-enhanced commercial systems\nsuch as Bing Chat, two critical problems remain unsolved, i.e., the lack of\nfactuality and clear logic in the generated long-form answers. In this paper,\nwe remedy these issues via a systematic study on answer generation in\nweb-enhanced LFQA. Specifically, we first propose a novel outline-enhanced\ngenerator to achieve clear logic in the generation of multifaceted answers and\nconstruct two datasets accordingly. Then we propose a factuality optimization\nmethod based on a carefully designed doubly fine-grained RLHF framework, which\ncontains automatic evaluation and reward modeling in different levels of\ngranularity. Our generic framework comprises conventional fine-grained RLHF\nmethods as special cases. Extensive experiments verify the superiority of our\nproposed \\textit{Factuality-optimized RAG (FoRAG)} method on both English and\nChinese benchmarks. In particular, when applying our method to Llama2-7B-chat,\nthe derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly\nused metrics (i.e., coherence, helpfulness, and factuality), while the number\nof parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets\nand models are made publicly available for better reproducibility:\nhttps://huggingface.co/forag.", "published": "2024-06-19 19:06:36", "link": "http://arxiv.org/abs/2406.13779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs as Models for Analogical Reasoning", "abstract": "Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations.", "published": "2024-06-19 20:07:37", "link": "http://arxiv.org/abs/2406.13803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Framing Social Movements on Social Media: Unpacking Diagnostic,\n  Prognostic, and Motivational Strategies", "abstract": "Social media enables activists to directly communicate with the public and\nprovides a space for movement leaders, participants, bystanders, and opponents\nto collectively construct and contest narratives. Focusing on Twitter messages\nfrom social movements surrounding three issues in 2018-2019 (guns, immigration,\nand LGBTQ rights), we create a codebook, annotated dataset, and computational\nmodels to detect diagnostic (problem identification and attribution),\nprognostic (proposed solutions and tactics), and motivational (calls to action)\nframing strategies. We conduct an in-depth unsupervised linguistic analysis of\neach framing strategy, and uncover cross-movement similarities in associations\nbetween framing and linguistic features such as pronouns and deontic modal\nverbs. Finally, we compare framing strategies across issues and other social,\ncultural, and interactional contexts. For example, we show that diagnostic\nframing is more common in replies than original broadcast posts, and that\nsocial movement organizations focus much more on prognostic and motivational\nframing than journalists and ordinary citizens.", "published": "2024-06-19 20:31:34", "link": "http://arxiv.org/abs/2406.13820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning BERTs for Definition Extraction from Mathematical Text", "abstract": "In this paper, we fine-tuned three pre-trained BERT models on the task of\n\"definition extraction\" from mathematical English written in LaTeX. This is\npresented as a binary classification problem, where either a sentence contains\na definition of a mathematical term or it does not. We used two original data\nsets, \"Chicago\" and \"TAC,\" to fine-tune and test these models. We also tested\non WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the\nperformance of our models to theirs. We found that a high-performance\nSentence-BERT transformer model performed best based on overall accuracy,\nrecall, and precision metrics, achieving comparable results to the earlier\nmodels with less computational effort.", "published": "2024-06-19 20:47:23", "link": "http://arxiv.org/abs/2406.13827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuro-symbolic Training for Reasoning over Spatial Language", "abstract": "Spatial reasoning based on natural language expressions is essential for\neveryday human tasks. This reasoning ability is also crucial for machines to\ninteract with their environment in a human-like manner. However, recent\nresearch shows that even state-of-the-art language models struggle with spatial\nreasoning over text, especially when facing nesting spatial expressions. This\nis attributed to not achieving the right level of abstraction required for\ngeneralizability. To alleviate this issue, we propose training language models\nwith neuro-symbolic techniques that exploit the spatial logical rules as\nconstraints, providing additional supervision to improve spatial reasoning and\nquestion answering. Training language models to adhere to spatial reasoning\nrules guides them in making more effective and general abstractions for\ntransferring spatial knowledge to various domains. We evaluate our approach on\nexisting spatial question-answering benchmarks. Our results indicate the\neffectiveness of our proposed technique in improving language models in complex\nmulti-hop spatial reasoning over text.", "published": "2024-06-19 20:47:36", "link": "http://arxiv.org/abs/2406.13828v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributional reasoning in LLMs: Parallel reasoning processes in\n  multi-hop reasoning", "abstract": "Large language models (LLMs) have shown an impressive ability to perform\ntasks believed to require thought processes. When the model does not document\nan explicit thought process, it becomes difficult to understand the processes\noccurring within its hidden layers and to determine if these processes can be\nreferred to as reasoning. We introduce a novel and interpretable analysis of\ninternal multi-hop reasoning processes in LLMs. We demonstrate that the\nprediction process for compositional reasoning questions can be modeled using a\nsimple linear transformation between two semantic category spaces. We show that\nduring inference, the middle layers of the network generate highly\ninterpretable embeddings that represent a set of potential intermediate answers\nfor the multi-hop question. We use statistical analyses to show that a\ncorresponding subset of tokens is activated in the model's output, implying the\nexistence of parallel reasoning paths. These observations hold true even when\nthe model lacks the necessary knowledge to solve the task. Our findings can\nhelp uncover the strategies that LLMs use to solve reasoning tasks, offering\ninsights into the types of thought processes that can emerge from artificial\nintelligence. Finally, we also discuss the implication of cognitive modeling of\nthese results.", "published": "2024-06-19 21:36:40", "link": "http://arxiv.org/abs/2406.13858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptable Logical Control for Large Language Models", "abstract": "Despite the success of Large Language Models (LLMs) on various tasks\nfollowing human instructions, controlling model generation at inference time\nposes a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable\nframework that facilitates tractable and flexible control of LLM generation to\nreliably follow logical constraints. Ctrl-G combines any production-ready LLM\nwith a Hidden Markov Model, enabling LLM outputs to adhere to logical\nconstraints represented as deterministic finite automata. We show that Ctrl-G,\nwhen applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of\ninteractive text editing: specifically, for the task of generating text\ninsertions/continuations following logical constraints, Ctrl-G achieves over\n30% higher satisfaction rate in human evaluation compared to GPT4. When applied\nto medium-size language models (e.g., GPT2-large), Ctrl-G also beats its\ncounterparts for constrained generation by large margins on standard\nbenchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on\nthe Grade School Math benchmark to assist LLM reasoning, foreshadowing the\napplication of Ctrl-G, as well as other constrained generation approaches,\nbeyond traditional language generation tasks.", "published": "2024-06-19 23:47:59", "link": "http://arxiv.org/abs/2406.13892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Generative Large Language Models for Galician", "abstract": "Large language models (LLMs) have transformed natural language processing.\nYet, their predominantly English-centric training has led to biases and\nperformance disparities across languages. This imbalance marginalizes\nminoritized languages, making equitable access to NLP technologies more\ndifficult for languages with lower resources, such as Galician. We present the\nfirst two generative LLMs focused on Galician to bridge this gap. These models,\nfreely available as open-source resources, were trained using a GPT\narchitecture with 1.3B parameters on a corpus of 2.1B words. Leveraging\ncontinual pretraining, we adapt to Galician two existing LLMs trained on larger\ncorpora, thus mitigating the data constraints that would arise if the training\nwere performed from scratch. The models were evaluated using human judgments\nand task-based datasets from standardized benchmarks. These evaluations reveal\na promising performance, underscoring the importance of linguistic diversity in\ngenerative models.", "published": "2024-06-19 23:49:56", "link": "http://arxiv.org/abs/2406.13893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in\n  Sequence-Level Knowledge Distillation", "abstract": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing tasks, but deploying them remains computationally\nexpensive. Knowledge distillation (KD) is a promising solution, enabling the\ntransfer of capabilities from larger teacher LLMs to more compact student\nmodels. Particularly, sequence-level KD, which distills rationale-based\nreasoning processes instead of merely final outcomes, shows great potential in\nenhancing students' reasoning capabilities. However, current methods struggle\nwith sequence level KD under long-tailed data distributions, adversely\naffecting generalization on sparsely represented domains. We introduce the\nMulti-Stage Balanced Distillation (BalDistill) framework, which iteratively\nbalances training data within a fixed computational budget. By dynamically\nselecting representative head domain examples and synthesizing tail domain\nexamples, BalDistill achieves state-of-the-art performance across diverse\nlong-tailed datasets, enhancing both the efficiency and efficacy of the\ndistilled models.", "published": "2024-06-19 00:01:14", "link": "http://arxiv.org/abs/2406.13114v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Biased Because They Are Large Language Models", "abstract": "This position paper's primary goal is to provoke thoughtful discussion about\nthe relationship between bias and fundamental properties of large language\nmodels. I do this by seeking to convince the reader that harmful biases are an\ninevitable consequence arising from the design of any large language model as\nLLMs are currently formulated. To the extent that this is true, it suggests\nthat the problem of harmful bias cannot be properly addressed without a serious\nreconsideration of AI driven by LLMs, going back to the foundational\nassumptions underlying their design.", "published": "2024-06-19 01:08:03", "link": "http://arxiv.org/abs/2406.13138v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party\n  Dialogue Understanding of Conversation Systems", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversation systems, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nsystems often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, a conversation system is assigned the role of a\ncharacter from popular TV shows, requiring it to respond to spontaneous\nquestions using past dialogue information and to distinguish between known and\nunknown information. Key features of DialSim include assessing the system's\nability to respond within a reasonable time limit, handling long-term\nmulti-party dialogues, and evaluating performance under randomized questioning\nwith LongDialQA, a novel, high-quality question-answering dataset. Our\nexperiments using DialSim reveal the strengths and weaknesses of the latest\nconversation systems, offering valuable insights for future advancements in\nconversational AI. DialSim is available at https://dialsim.github.io/.", "published": "2024-06-19 01:37:10", "link": "http://arxiv.org/abs/2406.13144v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Amphista: Bi-directional Multi-head Decoding for Accelerating LLM\n  Inference", "abstract": "Large Language Models (LLMs) inherently use autoregressive decoding, which\nlacks parallelism in inference and results in significantly slow inference\nspeed. While methods such as Medusa constructs parallelized heads, they lack\nadequate information interaction across different prediction positions. To\novercome this limitation, we introduce Amphista, an enhanced speculative\ndecoding framework that builds upon Medusa. Specifically, Amphista models an\nAuto-embedding Block capable of parallel inference, incorporating\nbi-directional attention to enable interaction between different drafting\nheads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure\na seamless transition of semantic information from the target model's\nautoregressive inference to the drafting heads' non-autoregressive inference,\neffectively achieving paradigm shift and feature fusion. Experimental results\non Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista\nachieves substantial acceleration while maintaining generation quality. On\nMT-Bench, Amphista delivers up to 2.75$\\times$ speedup over vanilla\nautoregressive decoding and 1.40$\\times$ over Medusa on Vicuna 33B in\nwall-clock time.", "published": "2024-06-19 02:53:39", "link": "http://arxiv.org/abs/2406.13170v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Synthetic Context Generation for Question Generation", "abstract": "Despite rapid advancements in large language models (LLMs), QG remains a\nchallenging problem due to its complicated process, open-ended nature, and the\ndiverse settings in which question generation occurs. A common approach to\naddress these challenges involves fine-tuning smaller, custom models using\ndatasets containing background context, question, and answer. However,\nobtaining suitable domain-specific datasets with appropriate context is often\nmore difficult than acquiring question-answer pairs. In this paper, we\ninvestigate training QG models using synthetic contexts generated by LLMs from\nreadily available question-answer pairs. We conduct a comprehensive study to\nanswer critical research questions related to the performance of models trained\non synthetic contexts and their potential impact on QG research and\napplications. Our empirical results reveal: 1) contexts are essential for QG\ntasks, even if they are synthetic; 2) fine-tuning smaller language models has\nthe capability of achieving better performances as compared to prompting larger\nlanguage models; and 3) synthetic context and real context could achieve\ncomparable performances. These findings highlight the effectiveness of\nsynthetic contexts in QG and paves the way for future advancements in the\nfield.", "published": "2024-06-19 03:37:52", "link": "http://arxiv.org/abs/2406.13188v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark\n  Emphasizing Modality Consistency", "abstract": "Multimodal large language models (MLLMs) are prone to non-factual or outdated\nknowledge issues, which can manifest as misreading and misrecognition errors\ndue to the complexity of multimodal knowledge. Previous benchmarks have not\nsystematically analyzed the performance of editing methods in correcting these\ntwo error types. To better represent and correct these errors, we decompose\nmultimodal knowledge into its visual and textual components. Different error\ntypes correspond to different editing formats, which edit distinct parts of the\nmultimodal knowledge. We present MC-MKE, a fine-grained Multimodal Knowledge\nEditing benchmark emphasizing Modality Consistency. Our benchmark facilitates\nindependent correction of misreading and misrecognition errors by editing the\ncorresponding knowledge component. We evaluate four multimodal knowledge\nediting methods on MC-MKE, revealing their limitations, particularly in terms\nof modality consistency. Our work highlights the challenges posed by multimodal\nknowledge editing and motivates further research in developing effective\ntechniques for this task.", "published": "2024-06-19 05:15:21", "link": "http://arxiv.org/abs/2406.13219v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Data Contamination Can Cross Language Barriers", "abstract": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.", "published": "2024-06-19 05:53:27", "link": "http://arxiv.org/abs/2406.13236v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BeHonest: Benchmarking Honesty in Large Language Models", "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\npresent severe risks that intensify as these models approach superintelligent\nlevels. Enhancing honesty in LLMs addresses critical limitations and helps\nuncover latent capabilities that are not readily expressed. This underscores\nthe urgent need for reliable methods and benchmarks to effectively ensure and\nevaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We encourage the AI community to\nprioritize honesty alignment in these models, which can harness their full\npotential to benefit society while preventing them from causing harm through\ndeception or inconsistency. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.", "published": "2024-06-19 06:46:59", "link": "http://arxiv.org/abs/2406.13261v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large\n  Language Models", "abstract": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM\ndescribes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also\nhighlight the importance of the contextualization through examples of the above\nprocedure.", "published": "2024-06-19 08:48:05", "link": "http://arxiv.org/abs/2406.13342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Cross-Lingual Transfer via Progressive\n  Code-Switching", "abstract": "Code-switching is a data augmentation scheme mixing words from multiple\nlanguages into source lingual text. It has achieved considerable generalization\nperformance of cross-lingual transfer tasks by aligning cross-lingual\ncontextual word representations. However, uncontrolled and over-replaced\ncode-switching would augment dirty samples to model training. In other words,\nthe excessive code-switching text samples will negatively hurt the models'\ncross-lingual transferability. To this end, we propose a Progressive\nCode-Switching (PCS) method to gradually generate moderately difficult\ncode-switching examples for the model to discriminate from easy to hard. The\nidea is to incorporate progressively the preceding learned multilingual\nknowledge using easier code-switching data to guide model optimization on\nsucceeding harder code-switching data. Specifically, we first design a\ndifficulty measurer to measure the impact of replacing each word in a sentence\nbased on the word relevance score. Then a code-switcher generates the\ncode-switching data of increasing difficulty via a controllable temperature\nvariable. In addition, a training scheduler decides when to sample harder\ncode-switching data for model training. Experiments show our model achieves\nstate-of-the-art results on three different zero-shot cross-lingual transfer\ntasks across ten languages.", "published": "2024-06-19 09:06:24", "link": "http://arxiv.org/abs/2406.13361v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Factual Confidence of LLMs: on Reliability and Robustness of Current\n  Estimators", "abstract": "Large Language Models (LLMs) tend to be unreliable in the factuality of their\nanswers. To address this problem, NLP researchers have proposed a range of\ntechniques to estimate LLM's confidence over facts. However, due to the lack of\na systematic comparison, it is not clear how the different methods compare to\none another. To fill this gap, we present a survey and empirical comparison of\nestimators of factual confidence. We define an experimental framework allowing\nfor fair comparison, covering both fact-verification and question answering.\nOur experiments across a series of LLMs indicate that trained hidden-state\nprobes provide the most reliable confidence estimates, albeit at the expense of\nrequiring access to weights and training data. We also conduct a deeper\nassessment of factual confidence by measuring the consistency of model behavior\nunder meaning-preserving variations in the input. We find that the confidence\nof LLMs is often unstable across semantically equivalent inputs, suggesting\nthat there is much room for improvement of the stability of models' parametric\nknowledge. Our code is available at\n(https://github.com/amazon-science/factual-confidence-of-llms).", "published": "2024-06-19 10:11:37", "link": "http://arxiv.org/abs/2406.13415v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "abstract": "Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/", "published": "2024-06-19 11:09:16", "link": "http://arxiv.org/abs/2406.13444v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Mitigating Social Biases in Language Models through Unlearning", "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to\nthe widespread deployment of LMs. Numerous approaches revolve around data\npre-processing and fine-tuning of language models, tasks that can be both\ntime-consuming and computationally demanding. Consequently, there is a growing\ninterest in machine unlearning techniques given their capacity to induce the\nforgetting of undesired behaviors of the existing pre-trained or fine-tuned\nmodels with lower computational cost. In this work, we explore two unlearning\nmethods, (1) Partitioned Contrastive Gradient Unlearning (PCGU) applied on\ndecoder models and (2) Negation via Task Vector, to reduce social biases in\nstate-of-the-art and open-source LMs such as LLaMA-2 and OPT. We also implement\ndistributed PCGU for large models. It is empirically shown, through\nquantitative and qualitative analyses, that negation via Task Vector method\noutperforms PCGU in debiasing with minimum deterioration in performance and\nperplexity of the models. On LLaMA-27B, negation via Task Vector reduces the\nbias score by 11.8%", "published": "2024-06-19 13:38:34", "link": "http://arxiv.org/abs/2406.13551v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining United Nations General Assembly Debates", "abstract": "This project explores the application of Natural Language Processing (NLP)\ntechniques to analyse United Nations General Assembly (UNGA) speeches. Using\nNLP allows for the efficient processing and analysis of large volumes of\ntextual data, enabling the extraction of semantic patterns, sentiment analysis,\nand topic modelling. Our goal is to deliver a comprehensive dataset and a tool\n(interface with descriptive statistics and automatically extracted topics) from\nwhich political scientists can derive insights into international relations and\nhave the opportunity to have a nuanced understanding of global diplomatic\ndiscourse.", "published": "2024-06-19 13:43:27", "link": "http://arxiv.org/abs/2406.13553v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation", "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.", "published": "2024-06-19 13:44:56", "link": "http://arxiv.org/abs/2406.13555v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Psychological Counseling with Instruction-Tuned Large\n  Language Models", "abstract": "The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.", "published": "2024-06-19 15:13:07", "link": "http://arxiv.org/abs/2406.13617v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News\n  Headlines", "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.", "published": "2024-06-19 15:20:19", "link": "http://arxiv.org/abs/2406.13626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructRAG: Instructing Retrieval-Augmented Generation via\n  Self-Synthesized Rationales", "abstract": "Retrieval-augmented generation (RAG) has shown promising potential to enhance\nthe accuracy and factuality of language models (LMs). However, imperfect\nretrievers or noisy corpora can introduce misleading or even erroneous\ninformation to the retrieved contents, posing a significant challenge to the\ngeneration quality. Existing RAG methods typically address this challenge by\ndirectly predicting final answers despite potentially noisy inputs, resulting\nin an implicit denoising process that is difficult to interpret and verify. On\nthe other hand, the acquisition of explicit denoising supervision is often\ncostly, involving significant human efforts. In this work, we propose\nInstructRAG, where LMs explicitly learn the denoising process through\nself-synthesized rationales -- First, we instruct the LM to explain how the\nground-truth answer is derived from retrieved documents. Then, these rationales\ncan be used either as demonstrations for in-context learning of explicit\ndenoising or as supervised fine-tuning data to train the model. Compared to\nstandard RAG approaches, InstructRAG requires no additional supervision, allows\nfor easier verification of the predicted answers, and effectively improves\ngeneration accuracy. Experiments show InstructRAG consistently outperforms\nexisting RAG methods in both training-free and trainable scenarios, achieving a\nrelative improvement of 8.3% over the best baseline method on average across\nfive knowledge-intensive benchmarks. Extensive analysis indicates that\nInstructRAG scales well with increased numbers of retrieved documents and\nconsistently exhibits robust denoising ability even in out-of-domain datasets,\ndemonstrating strong generalizability.", "published": "2024-06-19 15:25:29", "link": "http://arxiv.org/abs/2406.13629v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Minimal Targeted Updates of Language Models with Targeted\n  Negative Training", "abstract": "Generative models of language exhibit impressive capabilities but still place\nnon-negligible probability mass over undesirable outputs. In this work, we\naddress the task of updating a model to avoid unwanted outputs while minimally\nchanging model behavior otherwise, a challenge we refer to as a minimal\ntargeted update. We first formalize the notion of a minimal targeted update and\npropose a method to achieve such updates using negative examples from a model's\ngenerations. Our proposed Targeted Negative Training (TNT) results in updates\nthat keep the new distribution close to the original, unlike existing losses\nfor negative signal which push down probability but do not control what the\nupdated distribution will be. In experiments, we demonstrate that TNT yields a\nbetter trade-off between reducing unwanted behavior and maintaining model\ngeneration behavior than baselines, paving the way towards a modeling paradigm\nbased on iterative training updates that constrain models from generating\nundesirable outputs while preserving their impressive capabilities.", "published": "2024-06-19 16:06:21", "link": "http://arxiv.org/abs/2406.13660v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora", "abstract": "Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies.", "published": "2024-06-19 16:30:58", "link": "http://arxiv.org/abs/2406.13677v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "BEACON: Balancing Convenience and Nutrition in Meals With Long-Term\n  Group Recommendations and Reasoning on Multimodal Recipes", "abstract": "A common, yet regular, decision made by people, whether healthy or with any\nhealth condition, is to decide what to have in meals like breakfast, lunch, and\ndinner, consisting of a combination of foods for appetizer, main course, side\ndishes, desserts, and beverages. However, often this decision is seen as a\ntrade-off between nutritious choices (e.g., low salt and sugar) or convenience\n(e.g., inexpensive, fast to prepare/obtain, taste better). In this preliminary\nwork, we present a data-driven approach for the novel meal recommendation\nproblem that can explore and balance choices for both considerations while also\nreasoning about a food's constituents and cooking process. Beyond the problem\nformulation, our contributions also include a goodness measure, a recipe\nconversion method from text to the recently introduced multimodal rich recipe\nrepresentation (R3) format, and learning methods using contextual bandits that\nshow promising results.", "published": "2024-06-19 17:14:41", "link": "http://arxiv.org/abs/2406.13714v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for\n  Few-shot Problems", "abstract": "Large Language Models (LLMs) have been observed to perform well on a wide\nrange of downstream tasks when fine-tuned on domain-specific data. However,\nsuch data may not be readily available in many applications, motivating\nzero-shot or few-shot approaches using domain-adjacent models. While several\nfine-tuned models for various tasks are available, finding an appropriate\ndomain-adjacent model for a given task is often not straight forward. In this\npaper, we study DAFT-E, a framework that utilizes an Ensemble of\nDomain-Adjacent Fine-Tuned Foundation Models for few-shot problems. We show\nthat for zero-shot problems, this ensembling method provides an accuracy\nperformance close to that of the single best model. With few-shot problems,\nthis performance improves further, at which point DEFT-E can outperform any\nsingle domain-adjacent model while requiring much less data for domain-specific\nfine-tuning.", "published": "2024-06-19 17:24:36", "link": "http://arxiv.org/abs/2406.13720v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learn and Unlearn in Multilingual LLMs", "abstract": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes.", "published": "2024-06-19 18:01:08", "link": "http://arxiv.org/abs/2406.13748v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Game of LLMs: Discovering Structural Constructs in Activities using\n  Large Language Models", "abstract": "Human Activity Recognition is a time-series analysis problem. A popular\nanalysis procedure used by the community assumes an optimal window length to\ndesign recognition pipelines. However, in the scenario of smart homes, where\nactivities are of varying duration and frequency, the assumption of a constant\nsized window does not hold. Additionally, previous works have shown these\nactivities to be made up of building blocks. We focus on identifying these\nunderlying building blocks--structural constructs, with the use of large\nlanguage models. Identifying these constructs can be beneficial especially in\nrecognizing short-duration and infrequent activities. We also propose the\ndevelopment of an activity recognition procedure that uses these building\nblocks to model activities, thus helping the downstream task of activity\nmonitoring in smart homes.", "published": "2024-06-19 19:02:44", "link": "http://arxiv.org/abs/2406.13777v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StackRAG Agent: Improving Developer Answers with Retrieval-Augmented\n  Generation", "abstract": "Developers spend much time finding information that is relevant to their\nquestions. Stack Overflow has been the leading resource, and with the advent of\nLarge Language Models (LLMs), generative models such as ChatGPT are used\nfrequently. However, there is a catch in using each one separately. Searching\nfor answers is time-consuming and tedious, as shown by the many tools developed\nby researchers to address this issue. On the other, using LLMs is not reliable,\nas they might produce irrelevant or unreliable answers (i.e., hallucination).\nIn this work, we present StackRAG, a retrieval-augmented Multiagent generation\ntool based on LLMs that combines the two worlds: aggregating the knowledge from\nSO to enhance the reliability of the generated answers. Initial evaluations\nshow that the generated answers are correct, accurate, relevant, and useful.", "published": "2024-06-19 21:07:35", "link": "http://arxiv.org/abs/2406.13840v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Text Serialization and Their Relationship with the Conventional\n  Paradigms of Tabular Machine Learning", "abstract": "Recent research has explored how Language Models (LMs) can be used for\nfeature representation and prediction in tabular machine learning tasks. This\ninvolves employing text serialization and supervised fine-tuning (SFT)\ntechniques. Despite the simplicity of these techniques, significant gaps remain\nin our understanding of the applicability and reliability of LMs in this\ncontext. Our study assesses how emerging LM technologies compare with\ntraditional paradigms in tabular machine learning and evaluates the feasibility\nof adopting similar approaches with these advanced technologies. At the data\nlevel, we investigate various methods of data representation and curation of\nserialized tabular data, exploring their impact on prediction performance. At\nthe classification level, we examine whether text serialization combined with\nLMs enhances performance on tabular datasets (e.g. class imbalance,\ndistribution shift, biases, and high dimensionality), and assess whether this\nmethod represents a state-of-the-art (SOTA) approach for addressing tabular\nmachine learning challenges. Our findings reveal current pre-trained models\nshould not replace conventional approaches.", "published": "2024-06-19 21:19:37", "link": "http://arxiv.org/abs/2406.13846v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Enhanced Large Language Models via Path Selection", "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various\nreal-world applications. However, they are known to generate factually\ninaccurate outputs, a.k.a. the hallucination problem. In recent years,\nincorporating external knowledge extracted from Knowledge Graphs (KGs) has\nbecome a promising strategy to improve the factual accuracy of LLM-generated\noutputs. Nevertheless, most existing explorations rely on LLMs themselves to\nperform KG knowledge extraction, which is highly inflexible as LLMs can only\nprovide binary judgment on whether a certain knowledge (e.g., a knowledge path\nin KG) should be used. In addition, LLMs tend to pick only knowledge with\ndirect semantic relationship with the input text, while potentially useful\nknowledge with indirect semantics can be ignored. In this work, we propose a\nprincipled framework KELP with three stages to handle the above problems.\nSpecifically, KELP is able to achieve finer granularity of flexible knowledge\nextraction by generating scores for knowledge paths with input texts via latent\nsemantic matching. Meanwhile, knowledge paths with indirect semantic\nrelationships with the input text can also be considered via trained encoding\nbetween the selected paths in KG and the input text. Experiments on real-world\ndatasets validate the effectiveness of KELP.", "published": "2024-06-19 21:45:20", "link": "http://arxiv.org/abs/2406.13862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Tagging System on Math Questions via LLMs with Flexible\n  Demonstration Retriever", "abstract": "Knowledge tagging for questions plays a crucial role in contemporary\nintelligent educational applications, including learning progress diagnosis,\npractice question recommendations, and course content organization.\nTraditionally, these annotations are always conducted by pedagogical experts,\nas the task requires not only a strong semantic understanding of both question\nstems and knowledge definitions but also deep insights into connecting\nquestion-solving logic with corresponding knowledge concepts. With the recent\nemergence of advanced text encoding algorithms, such as pre-trained language\nmodels, many researchers have developed automatic knowledge tagging systems\nbased on calculating the semantic similarity between the knowledge and question\nembeddings. In this paper, we explore automating the task using Large Language\nModels (LLMs), in response to the inability of prior encoding-based methods to\ndeal with the hard cases which involve strong domain knowledge and complicated\nconcept definitions. By showing the strong performance of zero- and few-shot\nresults over math questions knowledge tagging tasks, we demonstrate LLMs' great\npotential in conquering the challenges faced by prior methods. Furthermore, by\nproposing a reinforcement learning-based demonstration retriever, we\nsuccessfully exploit the great potential of different-sized LLMs in achieving\nbetter performance results while keeping the in-context demonstration usage\nefficiency high.", "published": "2024-06-19 23:30:01", "link": "http://arxiv.org/abs/2406.13885v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics\n  in the Real World", "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.", "published": "2024-06-19 23:44:25", "link": "http://arxiv.org/abs/2406.13890v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-View Empowered Structural Graph Wordification for Language Models", "abstract": "Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://github.com/Timothy914/Dr.E.", "published": "2024-06-19 16:43:56", "link": "http://arxiv.org/abs/2406.15504v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation", "abstract": "Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen\ntriplets (i.e., query triplets) for rare relations in KGs, given only several\ntriplets of these relations as references (i.e., support triplets). This task\nhas gained significant traction due to the widespread use of knowledge graphs\nin various natural language processing applications. Previous approaches have\nutilized meta-training methods and manually constructed meta-relation sets to\ntackle this task. Recent efforts have focused on edge-mask-based methods, which\nexploit the structure of the contextualized graphs of target triplets (i.e., a\nsubgraph containing relevant triplets in the KG). However, existing\nedge-mask-based methods have limitations in extracting insufficient information\nfrom KG and are highly influenced by spurious information in KG. To overcome\nthese challenges, we propose SAFER (Subgraph Adaptation for Few-shot Relational\nReasoning), a novel approach that effectively adapts the information in\ncontextualized graphs to various subgraphs generated from support and query\ntriplets to perform the prediction. Specifically, SAFER enables the extraction\nof more comprehensive information from support triplets while minimizing the\nimpact of spurious information when predicting query triplets. Experimental\nresults on three prevalent datasets demonstrate the superiority of our proposed\nframework SAFER.", "published": "2024-06-19 21:40:35", "link": "http://arxiv.org/abs/2406.15507v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language,\n  Vision, Speech, and Multimodal Proficiency", "abstract": "As large language models (LLMs) continue to advance, evaluating their\ncomprehensive capabilities becomes significant for their application in various\nfields. This research study comprehensively evaluates the language, vision,\nspeech, and multimodal capabilities of GPT-4o. The study employs standardized\nexam questions, reasoning tasks, and translation assessments to assess the\nmodel's language capability. Additionally, GPT-4o's vision and speech\ncapabilities are tested through image classification and object recognition\ntasks, as well as accent classification. The multimodal evaluation assesses the\nmodel's performance in integrating visual and linguistic data. Our findings\nreveal that GPT-4o demonstrates high accuracy and efficiency across multiple\ndomains in language and reasoning capabilities, excelling in tasks that require\nfew-shot learning. GPT-4o also provides notable improvements in multimodal\ntasks compared to its predecessors. However, the model shows variability and\nfaces limitations in handling complex and ambiguous inputs, particularly in\naudio and vision capabilities. This paper highlights the need for more\ncomprehensive benchmarks and robust evaluation frameworks, encompassing\nqualitative assessments involving human judgment as well as error analysis.\nFuture work should focus on expanding datasets, investigating prompt-based\nassessment, and enhancing few-shot learning techniques to test the model's\npractical applicability and performance in real-world scenarios.", "published": "2024-06-19 19:00:21", "link": "http://arxiv.org/abs/2407.09519v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our\napproach to tasks traditionally reliant on external tools like retrieval\nsystems or databases. Leveraging LCLMs' ability to natively ingest and process\nentire corpora of information offers numerous advantages. It enhances\nuser-friendliness by eliminating the need for specialized knowledge of tools,\nprovides robust end-to-end modeling that minimizes cascading errors in complex\npipelines, and allows for the application of sophisticated prompting techniques\nacross the entire system. To assess this paradigm shift, we introduce LOFT, a\nbenchmark of real-world tasks requiring context up to millions of tokens\ndesigned to evaluate LCLMs' performance on in-context retrieval and reasoning.\nOur findings reveal LCLMs' surprising ability to rival state-of-the-art\nretrieval and RAG systems, despite never having been explicitly trained for\nthese tasks. However, LCLMs still face challenges in areas like compositional\nreasoning that are required in SQL-like tasks. Notably, prompting strategies\nsignificantly influence performance, emphasizing the need for continued\nresearch as context lengths grow. Overall, LOFT provides a rigorous testing\nground for LCLMs, showcasing their potential to supplant existing paradigms and\ntackle novel tasks as model capabilities scale.", "published": "2024-06-19 00:28:58", "link": "http://arxiv.org/abs/2406.13121v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PathoLM: Identifying pathogenicity from the DNA sequence through the\n  Genome Foundation Model", "abstract": "Pathogen identification is pivotal in diagnosing, treating, and preventing\ndiseases, crucial for controlling infections and safeguarding public health.\nTraditional alignment-based methods, though widely used, are computationally\nintense and reliant on extensive reference databases, often failing to detect\nnovel pathogens due to their low sensitivity and specificity. Similarly,\nconventional machine learning techniques, while promising, require large\nannotated datasets and extensive feature engineering and are prone to\noverfitting. Addressing these challenges, we introduce PathoLM, a cutting-edge\npathogen language model optimized for the identification of pathogenicity in\nbacterial and viral sequences. Leveraging the strengths of pre-trained DNA\nmodels such as the Nucleotide Transformer, PathoLM requires minimal data for\nfine-tuning, thereby enhancing pathogen detection capabilities. It effectively\ncaptures a broader genomic context, significantly improving the identification\nof novel and divergent pathogens. We developed a comprehensive data set\ncomprising approximately 30 species of viruses and bacteria, including ESKAPEE\npathogens, seven notably virulent bacterial strains resistant to antibiotics.\nAdditionally, we curated a species classification dataset centered specifically\non the ESKAPEE group. In comparative assessments, PathoLM dramatically\noutperforms existing models like DciPatho, demonstrating robust zero-shot and\nfew-shot capabilities. Furthermore, we expanded PathoLM-Sp for ESKAPEE species\nclassification, where it showed superior performance compared to other advanced\ndeep learning methods, despite the complexities of the task.", "published": "2024-06-19 00:53:48", "link": "http://arxiv.org/abs/2406.13133v1", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "APPL: A Prompt Programming Language for Harmonious Integration of\n  Programs and Large Language Model Prompts", "abstract": "Large Language Models (LLMs) have become increasingly capable of handling\ndiverse tasks with the aid of well-crafted prompts and integration of external\ntools, but as task complexity rises, the workflow involving LLMs can be\ncomplicated and thus challenging to implement and maintain. To address this\nchallenge, we propose APPL, A Prompt Programming Language that acts as a bridge\nbetween computer programs and LLMs, allowing seamless embedding of prompts into\nPython functions, and vice versa. APPL provides an intuitive and Python-native\nsyntax, an efficient parallelized runtime with asynchronous semantics, and a\ntracing module supporting effective failure diagnosis and replaying without\nextra costs. We demonstrate that APPL programs are intuitive, concise, and\nefficient through three representative scenarios: Chain-of-Thought with\nself-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat.\nExperiments on three parallelizable workflows further show that APPL can\neffectively parallelize independent LLM calls, with a significant speedup ratio\nthat almost matches the estimation.", "published": "2024-06-19 02:29:59", "link": "http://arxiv.org/abs/2406.13161v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.AI"}
{"title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models", "abstract": "Discovering new materials can have significant scientific and technological\nimplications but remains a challenging problem today due to the enormity of the\nchemical space. Recent advances in machine learning have enabled data-driven\nmethods to rapidly screen or generate promising materials, but these methods\nstill depend heavily on very large quantities of training data and often lack\nthe flexibility and chemical understanding often desired in materials\ndiscovery. We introduce LLMatDesign, a novel language-based framework for\ninterpretable materials design powered by large language models (LLMs).\nLLMatDesign utilizes LLM agents to translate human instructions, apply\nmodifications to materials, and evaluate outcomes using provided tools. By\nincorporating self-reflection on its previous decisions, LLMatDesign adapts\nrapidly to new tasks and conditions in a zero-shot manner. A systematic\nevaluation of LLMatDesign on several materials design tasks, in silico,\nvalidates LLMatDesign's effectiveness in developing new materials with\nuser-defined target properties in the small data regime. Our framework\ndemonstrates the remarkable potential of autonomous LLM-guided materials\ndiscovery in the computational setting and towards self-driving laboratories in\nthe future.", "published": "2024-06-19 02:35:02", "link": "http://arxiv.org/abs/2406.13163v1", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Biomedical Visual Instruction Tuning with Clinician Preference Alignment", "abstract": "Recent advancements in multimodal foundation models have showcased impressive\ncapabilities in understanding and reasoning with visual and textual\ninformation. Adapting these foundation models trained for general usage to\nspecialized domains like biomedicine requires large-scale domain-specific\ninstruction datasets. While existing works have explored curating such datasets\nautomatically, the resultant datasets are not explicitly aligned with domain\nexpertise. In this work, we propose a data-centric framework, Biomedical Visual\nInstruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that\nincorporates clinician preferences into both stages of generating and selecting\ninstruction data for tuning biomedical multimodal foundation models. First,\nduring the generation stage, we prompt the GPT-4V generator with a diverse set\nof clinician-selected demonstrations for preference-aligned data candidate\ngeneration. Then, during the selection phase, we train a separate selection\nmodel, which explicitly distills clinician and policy-guided model preferences\ninto a rating function to select high-quality data for medical instruction\ntuning. Results show that the model tuned with the instruction-following data\nfrom our method demonstrates a significant improvement in open visual chat\n(18.5% relatively) and medical VQA (win rate up to 81.73%). Our\ninstruction-following data and models are available at BioMed-VITAL.github.io.", "published": "2024-06-19 03:07:33", "link": "http://arxiv.org/abs/2406.13173v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "68T50, 68T45, 68T37, 68T05, 68T07, 68T09,", "I.2.7; I.2.6; I.2.10"], "primary_category": "cs.CV"}
{"title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes", "abstract": "Multimodal Large Language Models (MLLMs) have seen growing adoption across\nvarious scientific disciplines. These advancements encourage the investigation\nof molecule-text modeling within synthetic chemistry, a field dedicated to\ndesigning and conducting chemical reactions to synthesize new compounds with\ndesired properties and applications. Current approaches, however, often neglect\nthe critical role of multiple molecule graph interaction in understanding\nchemical reactions, leading to suboptimal performance in synthetic chemistry\ntasks. This study introduces PRESTO(Progressive Pretraining Enhances Synthetic\nChemistry Outcomes), a new framework that bridges the molecule-text modality\ngap by integrating a comprehensive benchmark of pretraining strategies and\ndataset configurations. It progressively improves multimodal LLMs through\ncross-modal alignment and multi-graph understanding. Our extensive experiments\ndemonstrate that PRESTO offers competitive results in downstream synthetic\nchemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.", "published": "2024-06-19 03:59:46", "link": "http://arxiv.org/abs/2406.13193v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph"], "primary_category": "cs.LG"}
{"title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n  Filtering with LLM-Extracted Metadata", "abstract": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.", "published": "2024-06-19 04:53:48", "link": "http://arxiv.org/abs/2406.13213v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Probing the Emergence of Cross-lingual Alignment during LLM Training", "abstract": "Multilingual Large Language Models (LLMs) achieve remarkable levels of\nzero-shot cross-lingual transfer performance. We speculate that this is\npredicated on their ability to align languages without explicit supervision\nfrom parallel sentences. While representations of translationally equivalent\nsentences in different languages are known to be similar after convergence,\nhowever, it remains unclear how such cross-lingual alignment emerges during\npre-training of LLMs. Our study leverages intrinsic probing techniques, which\nidentify which subsets of neurons encode linguistic features, to correlate the\ndegree of cross-lingual neuron overlap with the zero-shot cross-lingual\ntransfer performance for a given model. In particular, we rely on checkpoints\nof BLOOM, a multilingual autoregressive LLM, across different training steps\nand model scales. We observe a high correlation between neuron overlap and\ndownstream performance, which supports our hypothesis on the conditions leading\nto effective cross-lingual transfer. Interestingly, we also detect a\ndegradation of both implicit alignment and multilingual abilities in certain\nphases of the pre-training process, providing new insights into the\nmultilingual pretraining dynamics.", "published": "2024-06-19 05:31:59", "link": "http://arxiv.org/abs/2406.13229v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and\n  Metrics for Open Domain Question Answering in the Era of Large Language\n  Models", "abstract": "Open Domain Question Answering (ODQA) within natural language processing\ninvolves building systems that answer factual questions using large-scale\nknowledge corpora. Recent advances stem from the confluence of several factors,\nsuch as large-scale training datasets, deep learning techniques, and the rise\nof large language models. High-quality datasets are used to train models on\nrealistic scenarios and enable the evaluation of the system on potentially\nunseen data. Standardized metrics facilitate comparisons between different ODQA\nsystems, allowing researchers to objectively track advancements in the field.\nOur study presents a thorough examination of the current landscape of ODQA\nbenchmarking by reviewing 52 datasets and 20 evaluation techniques across\ntextual and multimodal modalities. We introduce a novel taxonomy for ODQA\ndatasets that incorporates both the modality and difficulty of the question\ntypes. Additionally, we present a structured organization of ODQA evaluation\nmetrics along with a critical analysis of their inherent trade-offs. Our study\naims to empower researchers by providing a framework for the robust evaluation\nof modern question-answering systems. We conclude by identifying the current\nchallenges and outlining promising avenues for future research and development.", "published": "2024-06-19 05:43:02", "link": "http://arxiv.org/abs/2406.13232v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via\n  Multimodal LLMs", "abstract": "The ability to understand and reason about spatial relationships between\nobjects in images is an important component of visual reasoning. This skill\nrests on the ability to recognize and localize objects of interest and\ndetermine their spatial relation. Early vision and language models (VLMs) have\nbeen shown to struggle to recognize spatial relations. We extend the previously\nreleased What'sUp dataset and propose a novel comprehensive evaluation for\nspatial relationship understanding that highlights the strengths and weaknesses\nof 27 different models. In addition to the VLMs evaluated in What'sUp, our\nextensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary\nin their parameter sizes (ranging from 7B to 110B), training/instruction-tuning\nmethods, and visual resolution to benchmark their performances and scrutinize\nthe scaling laws in this task.", "published": "2024-06-19 06:15:26", "link": "http://arxiv.org/abs/2406.13246v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented\n  Generation", "abstract": "Retrieval augmented generation (RAG) has been applied in many scenarios to\naugment large language models (LLMs) with external documents provided by\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\ndifferences in their training objectives and architectures. This misalignment\nforces LLMs to passively accept the documents provided by the retrievers,\nleading to incomprehension in the generation process, where the LLMs are\nburdened with the task of distinguishing these documents using their inherent\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\nthis gap by incorporating Retrieval information into Retrieval Augmented\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\nretrieval-aware prompting strategy is designed to integrate retrieval\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\nwhere LLMs and retrievers are frozen. Extensive experiments across five\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\nthe generation process, thereby filling the semantic gap.", "published": "2024-06-19 06:19:48", "link": "http://arxiv.org/abs/2406.13249v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LangTopo: Aligning Language Descriptions of Graphs with Tokenized\n  Topological Modeling", "abstract": "Recently, large language models (LLMs) have been widely researched in the\nfield of graph machine learning due to their outstanding abilities in language\ncomprehension and learning. However, the significant gap between natural\nlanguage tasks and topological structure modeling poses a nonnegligible\nchallenge. Specifically, since natural language descriptions are not sufficient\nfor LLMs to understand and process graph-structured data, fine-tuned LLMs\nperform even worse than some traditional GNN models on graph tasks, lacking\ninherent modeling capabilities for graph structures. Existing research overly\nemphasizes LLMs' understanding of semantic information captured by external\nmodels, while inadequately exploring graph topological structure modeling,\nthereby overlooking the genuine capabilities that LLMs lack. Consequently, in\nthis paper, we introduce a new framework, LangTopo, which aligns graph\nstructure modeling with natural language understanding at the token level.\nLangTopo quantifies the graph structure modeling capabilities of GNNs and LLMs\nby constructing a codebook for the graph modality and performs consistency\nmaximization. This process aligns the text description of LLM with the\ntopological modeling of GNN, allowing LLM to learn the ability of GNN to\ncapture graph structures, enabling LLM to handle graph-structured data\nindependently. We demonstrate the effectiveness of our proposed method on\nmultiple datasets.", "published": "2024-06-19 06:20:22", "link": "http://arxiv.org/abs/2406.13250v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding\n  Datasets", "abstract": "In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic\nrepresentation describing the users' requests is key to a smooth interaction.\nIndeed, the system uses this representation to reason over a database and its\ndomain knowledge to choose its next action. The dialogue course thus depends on\nthe information provided by this semantic representation. While textual\ndatasets provide fine-grained semantic representations, spoken dialogue\ndatasets fall behind. This paper provides insights into automatic enhancement\nof spoken dialogue datasets' semantic representations. Our contributions are\nthree fold: (1) assess the relevance of Large Language Model fine-tuning, (2)\nevaluate the knowledge captured by the produced annotations and (3) highlight\nsemi-automatic annotation implications.", "published": "2024-06-19 06:59:57", "link": "http://arxiv.org/abs/2406.13269v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "eess.SP"], "primary_category": "cs.AI"}
{"title": "Enhancing Automated Audio Captioning via Large Language Models with\n  Optimized Audio Encoding", "abstract": "Automated audio captioning (AAC) is an audio-to-text task to describe audio\ncontents in natural language. Recently, the advancements in large language\nmodels (LLMs), with improvements in training approaches for audio encoders,\nhave opened up possibilities for improving AAC. Thus, we explore enhancing AAC\nfrom three aspects: 1) a pre-trained audio encoder via consistent ensemble\ndistillation (CED) is used to improve the effectivity of acoustic tokens, with\na querying transformer (Q-Former) bridging the modality gap to LLM and compress\nacoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B\nparameters as the decoder; 3) another pre-trained LLM corrects text errors\ncaused by insufficient training data and annotation ambiguities. Both the audio\nencoder and text decoder are optimized by low-rank adaptation (LoRA).\nExperiments show that each of these enhancements is effective. Our method\nobtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.", "published": "2024-06-19 07:09:46", "link": "http://arxiv.org/abs/2406.13275v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Medical Spoken Named Entity Recognition", "abstract": "Spoken Named Entity Recognition (NER) aims to extract named entities from\nspeech and categorise them into types like person, location, organization, etc.\nIn this work, we present VietMed-NER - the first spoken NER dataset in the\nmedical domain. To our knowledge, our Vietnamese real-world dataset is the\nlargest spoken NER dataset in the world regarding the number of entity types,\nfeaturing 18 distinct types. Furthermore, we present baseline results using\nvarious state-of-the-art pre-trained models: encoder-only and\nsequence-to-sequence; and conduct quantitative and qualitative error analysis.\nWe found that pre-trained multilingual models generally outperform monolingual\nmodels on reference text and ASR output and encoders outperform\nsequence-to-sequence models in NER tasks. By translating the transcripts, the\ndataset can also be utilised for text NER in the medical domain in other\nlanguages than Vietnamese. All code, data and models are publicly available:\nhttps://github.com/leduckhai/MultiMed/tree/master/VietMed-NER.", "published": "2024-06-19 08:39:09", "link": "http://arxiv.org/abs/2406.13337v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond\n  Words", "abstract": "Speech encompasses a wealth of information, including but not limited to\ncontent, paralinguistic, and environmental information. This comprehensive\nnature of speech significantly impacts communication and is crucial for\nhuman-computer interaction. Chat-Oriented Large Language Models (LLMs), known\nfor their general-purpose assistance capabilities, have evolved to handle\nmulti-modal inputs, including speech. Although these models can be adept at\nrecognizing and analyzing speech, they often fall short of generating\nappropriate responses. We argue that this is due to the lack of principles on\ntask definition and model development, which requires open-source datasets and\nmetrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a\nbenchmark dataset aimed at multidimensional evaluation of spoken dialogue\nunderstanding and generation. SD-Eval focuses on paralinguistic and\nenvironmental information and includes 7,303 utterances, amounting to 8.76\nhours of speech data. The data is aggregated from eight public datasets,\nrepresenting four perspectives: emotion, accent, age, and background sound. To\nassess the SD-Eval benchmark dataset, we implement three different models and\nconstruct a training set following a process similar to that of SD-Eval. The\ntraining set contains 1,052.72 hours of speech data and 724.4k utterances. We\nalso conduct a comprehensive evaluation using objective evaluation methods\n(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the\ngenerated responses. Models conditioned with paralinguistic and environmental\ninformation outperform their counterparts in both objective and subjective\nmeasures. Moreover, experiments demonstrate that LLM-based metrics show a\nhigher correlation with human evaluation compared to traditional metrics. We\nopen-source SD-Eval at https://github.com/amphionspace/SD-Eval.", "published": "2024-06-19 08:46:29", "link": "http://arxiv.org/abs/2406.13340v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Textual Unlearning Gives a False Sense of Unlearning", "abstract": "Language Models (LMs) are prone to ''memorizing'' training data, including\nsubstantial sensitive user information. To mitigate privacy risks and safeguard\nthe right to be forgotten, machine unlearning has emerged as a promising\napproach for enabling LMs to efficiently ''forget'' specific texts. However,\ndespite the good intentions, is textual unlearning really as effective and\nreliable as expected? To address the concern, we first propose Unlearning\nLikelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing\nmethod, and find that unlearned texts can still be detected with very high\nconfidence after unlearning. Further, we conduct an in-depth investigation on\nthe privacy risks of textual unlearning mechanisms in deployment and present\nthe Textual Unlearning Leakage Attack (TULA), along with its variants in both\nblack- and white-box scenarios. We show that textual unlearning mechanisms\ncould instead reveal more about the unlearned texts, exposing them to\nsignificant membership inference and data reconstruction risks. Our findings\nhighlight that existing textual unlearning actually gives a false sense of\nunlearning, underscoring the need for more robust and secure unlearning\nmechanisms.", "published": "2024-06-19 08:51:54", "link": "http://arxiv.org/abs/2406.13348v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Transferable speech-to-text large language model alignment module", "abstract": "By leveraging the power of Large Language Models(LLMs) and speech foundation\nmodels, state of the art speech-text bimodal works can achieve challenging\ntasks like spoken translation(ST) and question answering(SQA) altogether with\nmuch simpler architectures. In this paper, we utilize the capability of Whisper\nencoder and pre-trained Yi-6B. Empirical results reveal that modal alignment\ncan be achieved with one layer module and hundred hours of speech-text\nmultitask corpus. We further swap the Yi-6B with human preferences aligned\nversion of Yi-6B-Chat during inference, and discover that the alignment\ncapability is applicable as well. In addition, the alignment subspace revealed\nby singular value decomposition(SVD) also implies linear alignment subspace is\nsparse, which leaves the possibility to concatenate other features like\nvoice-print or video to expand modality.", "published": "2024-06-19 09:04:43", "link": "http://arxiv.org/abs/2406.13357v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models", "abstract": "Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.", "published": "2024-06-19 09:07:31", "link": "http://arxiv.org/abs/2406.13362v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Children's Speech Recognition through Discrete Token Enhancement", "abstract": "Children's speech recognition is considered a low-resource task mainly due to\nthe lack of publicly available data. There are several reasons for such data\nscarcity, including expensive data collection and annotation processes, and\ndata privacy, among others. Transforming speech signals into discrete tokens\nthat do not carry sensitive information but capture both linguistic and\nacoustic information could be a solution for privacy concerns. In this study,\nwe investigate the integration of discrete speech tokens into children's speech\nrecognition systems as input without significantly degrading the ASR\nperformance. Additionally, we explored single-view and multi-view strategies\nfor creating these discrete labels. Furthermore, we tested the models for\ngeneralization capabilities with unseen domain and nativity dataset. Results\nreveal that the discrete token ASR for children achieves nearly equivalent\nperformance with an approximate 83% reduction in parameters.", "published": "2024-06-19 10:45:12", "link": "http://arxiv.org/abs/2406.13431v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language\n  Models on Multilingual NLU Tasks", "abstract": "This paper explores the performance of encoder and decoder language models on\nmultilingual Natural Language Understanding (NLU) tasks, with a broad focus on\nGermanic languages. Building upon the ScandEval benchmark, initially restricted\nto evaluating encoder models, we extend the evaluation framework to include\ndecoder models. We introduce a method for evaluating decoder models on NLU\ntasks and apply it to the languages Danish, Swedish, Norwegian, Icelandic,\nFaroese, German, Dutch, and English. Through a series of experiments and\nanalyses, we also address research questions regarding the comparative\nperformance of encoder and decoder models, the impact of NLU task types, and\nthe variation across language resources. Our findings reveal that encoder\nmodels can achieve significantly better NLU performance than decoder models\ndespite having orders of magnitude fewer parameters. Additionally, we\ninvestigate the correlation between decoders and task performance via a UMAP\nanalysis, shedding light on the unique capabilities of decoder and encoder\nmodels. This study contributes to a deeper understanding of language model\nparadigms in NLU tasks and provides valuable insights for model selection and\nevaluation in multilingual settings.", "published": "2024-06-19 11:50:09", "link": "http://arxiv.org/abs/2406.13469v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ManWav: The First Manchu ASR Model", "abstract": "This study addresses the widening gap in Automatic Speech Recognition (ASR)\nresearch between high resource and extremely low resource languages, with a\nparticular focus on Manchu, a critically endangered language. Manchu\nexemplifies the challenges faced by marginalized linguistic communities in\naccessing state-of-the-art technologies. In a pioneering effort, we introduce\nthe first-ever Manchu ASR model ManWav, leveraging Wav2Vec2-XLSR-53. The\nresults of the first Manchu ASR is promising, especially when trained with our\naugmented data. Wav2Vec2-XLSR-53 fine-tuned with augmented data demonstrates a\n0.02 drop in CER and 0.13 drop in WER compared to the same base model\nfine-tuned with original data.", "published": "2024-06-19 12:47:34", "link": "http://arxiv.org/abs/2406.13502v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models", "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.", "published": "2024-06-19 13:29:53", "link": "http://arxiv.org/abs/2406.13542v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Visual Commonsense in Language Models via Multiple Image\n  Generation", "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.", "published": "2024-06-19 15:17:10", "link": "http://arxiv.org/abs/2406.13621v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation", "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.", "published": "2024-06-19 16:10:26", "link": "http://arxiv.org/abs/2406.13663v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Developing Story: Case Studies of Generative AI's Use in Journalism", "abstract": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context.", "published": "2024-06-19 16:58:32", "link": "http://arxiv.org/abs/2406.13706v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards\n  for Better Well-Being", "abstract": "Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.", "published": "2024-06-19 19:35:14", "link": "http://arxiv.org/abs/2406.13791v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge\n  Conflicts from Wikipedia", "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution to\nmitigate the limitations of large language models (LLMs), such as\nhallucinations and outdated information. However, it remains unclear how LLMs\nhandle knowledge conflicts arising from different augmented retrieved passages,\nespecially when these passages originate from the same source and have equal\ntrustworthiness. In this work, we conduct a comprehensive evaluation of\nLLM-generated answers to questions that have varying answers based on\ncontradictory passages from Wikipedia, a dataset widely regarded as a\nhigh-quality pre-training resource for most LLMs. Specifically, we introduce\nWikiContradict, a benchmark consisting of 253 high-quality, human-annotated\ninstances designed to assess LLM performance when augmented with retrieved\npassages containing real-world knowledge conflicts. We benchmark a diverse\nrange of both closed and open-source LLMs under different QA scenarios,\nincluding RAG with a single passage, and RAG with 2 contradictory passages.\nThrough rigorous human evaluations on a subset of WikiContradict instances\ninvolving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and\nlimitations of these models. For instance, when provided with two passages\ncontaining contradictory facts, all models struggle to generate answers that\naccurately reflect the conflicting nature of the context, especially for\nimplicit conflicts requiring reasoning. Since human evaluation is costly, we\nalso introduce an automated model that estimates LLM performance using a strong\nopen-source language model, achieving an F-score of 0.8. Using this automated\nmetric, we evaluate more than 1,500 answers from seven LLMs across all\nWikiContradict instances. To facilitate future work, we release WikiContradict\non: https://ibm.biz/wikicontradict.", "published": "2024-06-19 20:13:42", "link": "http://arxiv.org/abs/2406.13805v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video\n  Understanding", "abstract": "AI personal assistants deployed via robots or wearables require embodied\nunderstanding to collaborate with humans effectively. However, current\nVision-Language Models (VLMs) primarily focus on third-person view videos,\nneglecting the richness of egocentric perceptual experience. To address this\ngap, we propose three key contributions. First, we introduce the Egocentric\nVideo Understanding Dataset (EVUD) for training VLMs on video captioning and\nquestion answering tasks specific to egocentric videos. Second, we present\nAlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD.\nFinally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging\nbenchmark for embodied video question answering. Our model achieves\nstate-of-the-art performance, outperforming open-source models including strong\nSocratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform\nClaude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to\nGemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning.\nThis research paves the way for building efficient VLMs that can be deployed in\nrobots or wearables, leveraging embodied video understanding to collaborate\nseamlessly with humans in everyday tasks, contributing to the next generation\nof Embodied AI.", "published": "2024-06-19 20:14:14", "link": "http://arxiv.org/abs/2406.13807v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Joint vs Sequential Speaker-Role Detection and Automatic Speech\n  Recognition for Air-traffic Control", "abstract": "Utilizing air-traffic control (ATC) data for downstream natural-language\nprocessing tasks requires preprocessing steps. Key steps are the transcription\nof the data via automatic speech recognition (ASR) and speaker diarization,\nrespectively speaker role detection (SRD) to divide the transcripts into pilot\nand air-traffic controller (ATCO) transcripts. While traditional approaches\ntake on these tasks separately, we propose a transformer-based joint ASR-SRD\nsystem that solves both tasks jointly while relying on a standard ASR\narchitecture. We compare this joint system against two cascaded approaches for\nASR and SRD on multiple ATC datasets. Our study shows in which cases our joint\nsystem can outperform the two traditional approaches and in which cases the\nother architectures are preferable. We additionally evaluate how acoustic and\nlexical differences influence all architectures and show how to overcome them\nfor our joint architecture.", "published": "2024-06-19 21:11:01", "link": "http://arxiv.org/abs/2406.13842v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Combinatorial Reasoning: Selecting Reasons in Generative AI Pipelines\n  via Combinatorial Optimization", "abstract": "Recent Large Language Models (LLMs) have demonstrated impressive capabilities\nat tasks that require human intelligence and are a significant step towards\nhuman-like artificial intelligence (AI). Yet the performance of LLMs at\nreasoning tasks have been subpar and the reasoning capability of LLMs is a\nmatter of significant debate. While it has been shown that the choice of the\nprompting technique to the LLM can alter its performance on a multitude of\ntasks, including reasoning, the best performing techniques require human-made\nprompts with the knowledge of the tasks at hand. We introduce a framework for\nwhat we call Combinatorial Reasoning (CR), a fully-automated prompting method,\nwhere reasons are sampled from an LLM pipeline and mapped into a Quadratic\nUnconstrained Binary Optimization (QUBO) problem. The framework investigates\nwhether QUBO solutions can be profitably used to select a useful subset of the\nreasons to construct a Chain-of-Thought style prompt. We explore the\nacceleration of CR with specialized solvers. We also investigate the\nperformance of simpler zero-shot strategies such as linear majority rule or\nrandom selection of reasons. Our preliminary study indicates that coupling a\ncombinatorial solver to generative AI pipelines is an interesting avenue for AI\nreasoning and elucidates design principles for future CR methods.", "published": "2024-06-19 16:47:44", "link": "http://arxiv.org/abs/2407.00071v1", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "primary_category": "cs.AI"}
{"title": "GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual\n  Generation", "abstract": "While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.", "published": "2024-06-19 18:00:07", "link": "http://arxiv.org/abs/2406.13743v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal\n  Component Analysis", "abstract": "The remarkable success of transformers in sequence modeling tasks, spanning\nvarious applications in natural language processing and computer vision, is\nattributed to the critical role of self-attention. Similar to the development\nof most deep learning models, the construction of these attention mechanisms\nrelies on heuristics and experience. In our work, we derive self-attention from\nkernel principal component analysis (kernel PCA) and show that self-attention\nprojects its query vectors onto the principal component axes of its key matrix\nin a feature space. We then formulate the exact formula for the value matrix in\nself-attention, theoretically and empirically demonstrating that this value\nmatrix captures the eigenvectors of the Gram matrix of the key vectors in\nself-attention. Leveraging our kernel PCA framework, we propose Attention with\nRobust Principal Components (RPC-Attention), a novel class of robust attention\nthat is resilient to data contamination. We empirically demonstrate the\nadvantages of RPC-Attention over softmax attention on the ImageNet-1K object\nclassification, WikiText-103 language modeling, and ADE20K image segmentation\ntask.", "published": "2024-06-19 18:22:32", "link": "http://arxiv.org/abs/2406.13762v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Elliptical Attention", "abstract": "Pairwise dot-product self-attention is key to the success of transformers\nthat achieve state-of-the-art performance across a variety of applications in\nlanguage and vision. This dot-product self-attention computes attention weights\namong the input tokens using Euclidean distance, which makes the model prone to\nrepresentation collapse and vulnerable to contaminated samples. In this paper,\nwe propose using a Mahalanobis distance metric for computing the attention\nweights to stretch the underlying feature space in directions of high\ncontextual relevance. In particular, we define a hyper-ellipsoidal neighborhood\naround each query to increase the attention weights of the tokens lying in the\ncontextually important directions. We term this novel class of attention\nElliptical Attention. Our Elliptical Attention provides two benefits: 1)\nreducing representation collapse and 2) enhancing the model's robustness as\nElliptical Attention pays more attention to contextually relevant information\nrather than focusing on some small subset of informative features. We\nempirically demonstrate the advantages of Elliptical Attention over the\nbaseline dot-product attention and state-of-the-art attention methods on\nvarious practical tasks, including object classification, image segmentation,\nand language modeling across different data modalities.", "published": "2024-06-19 18:38:11", "link": "http://arxiv.org/abs/2406.13770v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Primal-Dual Framework for Transformers and Neural Networks", "abstract": "Self-attention is key to the remarkable success of transformers in sequence\nmodeling tasks including many applications in natural language processing and\ncomputer vision. Like neural network layers, these attention mechanisms are\noften developed by heuristics and experience. To provide a principled framework\nfor constructing attention layers in transformers, we show that the\nself-attention corresponds to the support vector expansion derived from a\nsupport vector regression problem, whose primal formulation has the form of a\nneural network layer. Using our framework, we derive popular attention layers\nused in practice and propose two new attentions: 1) the Batch Normalized\nAttention (Attention-BN) derived from the batch normalization layer and 2) the\nAttention with Scaled Head (Attention-SH) derived from using less training data\nto fit the SVR model. We empirically demonstrate the advantages of the\nAttention-BN and Attention-SH in reducing head redundancy, increasing the\nmodel's accuracy, and improving the model's efficiency in a variety of\npractical applications including image and time-series classification.", "published": "2024-06-19 19:11:22", "link": "http://arxiv.org/abs/2406.13781v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Audio Fingerprinting with Holographic Reduced Representations", "abstract": "This paper proposes an audio fingerprinting model with holographic reduced\nrepresentation (HRR). The proposed method reduces the number of stored\nfingerprints, whereas conventional neural audio fingerprinting requires many\nfingerprints for each audio track to achieve high accuracy and time resolution.\nWe utilize HRR to aggregate multiple fingerprints into a composite fingerprint\nvia circular convolution and summation, resulting in fewer fingerprints with\nthe same dimensional space as the original. Our search method efficiently finds\na combined fingerprint in which a query fingerprint exists. Using HRR's inverse\noperation, it can recover the relative position within a combined fingerprint,\nretaining the original time resolution. Experiments show that our method can\nreduce the number of fingerprints with modest accuracy degradation while\nmaintaining the time resolution, outperforming simple decimation and\nsummation-based aggregation methods.", "published": "2024-06-19 01:09:33", "link": "http://arxiv.org/abs/2406.13139v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CEC: A Noisy Label Detection Method for Speaker Recognition", "abstract": "Noisy labels are inevitable, even in well-annotated datasets. The detection\nof noisy labels is of significant importance to enhance the robustness of\nspeaker recognition models. In this paper, we propose a novel noisy label\ndetection approach based on two new statistical metrics: Continuous\nInconsistent Counting (CIC) and Total Inconsistent Counting (TIC). These\nmetrics are calculated through Cross-Epoch Counting (CEC) and correspond to the\nearly and late stages of training, respectively. Additionally, we categorize\nsamples based on their prediction results into three categories: inconsistent\nsamples, hard samples, and easy samples. During training, we gradually increase\nthe difficulty of hard samples to update model parameters, preventing noisy\nlabels from being overfitted. Compared to contrastive schemes, our approach not\nonly achieves the best performance in speaker verification but also excels in\nnoisy label detection.", "published": "2024-06-19 06:57:45", "link": "http://arxiv.org/abs/2406.13268v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pushing the Limit of Sound Event Detection with Multi-Dilated Frequency\n  Dynamic Convolution", "abstract": "Frequency dynamic convolution (FDY conv) has been a milestone in the sound\nevent detection (SED) field, but it involves a substantial increase in model\nsize due to multiple basis kernels. In this work, we propose partial frequency\ndynamic convolution (PFD conv), which concatenates outputs by conventional 2D\nconvolution and FDY conv as static and dynamic branches respectively. PFD-CRNN\nwith proportion of dynamic branch output as one eighth reduces 51.9% of\nparameters from FDY-CRNN while retaining the performance. Additionally, we\npropose multi-dilated frequency dynamic convolution (MDFD conv), which\nintegrates multiple dilated frequency dynamic convolution (DFD conv) branches\nwith different dilation size sets and a static branch within a single\nconvolution layer. Resulting best MDFD-CRNN with five non-dilated FDY Conv\nbranches, three differently dilated DFD Conv branches and a static branch\nachieved 3.17% improvement in polyphonic sound detection score (PSDS) over FDY\nconv without class-wise median filter. Application of sound event bounding box\nas post processing on best MDFD-CRNN achieved true PSDS1 of 0.485, which is the\nstate-of-the-art score in DESED dataset without external dataset or pretrained\nmodel. From the results of extensive ablation studies, we discovered that not\nonly multiple dynamic branches but also specific proportion of static branch\nhelps SED. In addition, non-dilated dynamic branches are necessary in addition\nto dilated dynamic branches in order to obtain optimal SED performance. The\nresults and discussions on ablation studies further enhance understanding and\nusability of FDY conv variants.", "published": "2024-06-19 08:02:02", "link": "http://arxiv.org/abs/2406.13312v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Domain-Incremental Learning Approach to Classify Acoustic Scenes\n  in All Locations", "abstract": "In this paper, we propose a method for online domain-incremental learning of\nacoustic scene classification from a sequence of different locations. Simply\ntraining a deep learning model on a sequence of different locations leads to\nforgetting of previously learned knowledge. In this work, we only correct the\nstatistics of the Batch Normalization layers of a model using a few samples to\nlearn the acoustic scenes from a new location without any excessive training.\nExperiments are performed on acoustic scenes from 11 different locations, with\nan initial task containing acoustic scenes from 6 locations and the remaining 5\nincremental tasks each representing the acoustic scenes from a different\nlocation. The proposed approach outperforms fine-tuning based methods and\nachieves an average accuracy of 48.8% after learning the last task in sequence\nwithout forgetting acoustic scenes from the previously learned locations.", "published": "2024-06-19 09:28:24", "link": "http://arxiv.org/abs/2406.13386v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diffusion-based Generative Modeling with Discriminative Guidance for\n  Streamable Speech Enhancement", "abstract": "Diffusion-based generative models (DGMs) have recently attracted attention in\nspeech enhancement research (SE) as previous works showed a remarkable\ngeneralization capability. However, DGMs are also computationally intensive, as\nthey usually require many iterations in the reverse diffusion process (RDP),\nmaking them impractical for streaming SE systems. In this paper, we propose to\nuse discriminative scores from discriminative models in the first steps of the\nRDP. These discriminative scores require only one forward pass with the\ndiscriminative model for multiple RDP steps, thus greatly reducing\ncomputations. This approach also allows for performance improvements. We show\nthat we can trade off between generative and discriminative capabilities as the\nnumber of steps with the discriminative score increases. Furthermore, we\npropose a novel streamable time-domain generative model with an algorithmic\nlatency of 50 ms, which has no significant performance degradation compared to\noffline models.", "published": "2024-06-19 11:51:37", "link": "http://arxiv.org/abs/2406.13471v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Global-Local Convolution with Spiking Neural Networks for\n  Energy-efficient Keyword Spotting", "abstract": "Thanks to Deep Neural Networks (DNNs), the accuracy of Keyword Spotting (KWS)\nhas made substantial progress. However, as KWS systems are usually implemented\non edge devices, energy efficiency becomes a critical requirement besides\nperformance. Here, we take advantage of spiking neural networks' energy\nefficiency and propose an end-to-end lightweight KWS model. The model consists\nof two innovative modules: 1) Global-Local Spiking Convolution (GLSC) module\nand 2) Bottleneck-PLIF module. Compared to the hand-crafted feature extraction\nmethods, the GLSC module achieves speech feature extraction that is sparser,\nmore energy-efficient, and yields better performance. The Bottleneck-PLIF\nmodule further processes the signals from GLSC with the aim to achieve higher\naccuracy with fewer parameters. Extensive experiments are conducted on the\nGoogle Speech Commands Dataset (V1 and V2). The results show our method\nachieves competitive performance among SNN-based KWS models with fewer\nparameters.", "published": "2024-06-19 03:19:25", "link": "http://arxiv.org/abs/2406.13179v1", "categories": ["cs.SD", "cs.AI", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Straight Through Gumbel Softmax Estimator based Bimodal Neural\n  Architecture Search for Audio-Visual Deepfake Detection", "abstract": "Deepfakes are a major security risk for biometric authentication. This\ntechnology creates realistic fake videos that can impersonate real people,\nfooling systems that rely on facial features and voice patterns for\nidentification. Existing multimodal deepfake detectors rely on conventional\nfusion methods, such as majority rule and ensemble voting, which often struggle\nto adapt to changing data characteristics and complex patterns. In this paper,\nwe introduce the Straight-through Gumbel-Softmax (STGS) framework, offering a\ncomprehensive approach to search multimodal fusion model architectures. Using a\ntwo-level search approach, the framework optimizes the network architecture,\nparameters, and performance. Initially, crucial features were efficiently\nidentified from backbone networks, whereas within the cell structure, a\nweighted fusion operation integrated information from various sources. An\narchitecture that maximizes the classification performance is derived by\nvarying parameters such as temperature and sampling time. The experimental\nresults on the FakeAVCeleb and SWAN-DF datasets demonstrated an impressive AUC\nvalue 94.4\\% achieved with minimal model parameters.", "published": "2024-06-19 09:26:22", "link": "http://arxiv.org/abs/2406.13384v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Explainable by-design Audio Segmentation through Non-Negative Matrix\n  Factorization and Probing", "abstract": "Audio segmentation is a key task for many speech technologies, most of which\nare based on neural networks, usually considered as black boxes, with\nhigh-level performances. However, in many domains, among which health or\nforensics, there is not only a need for good performance but also for\nexplanations about the output decision. Explanations derived directly from\nlatent representations need to satisfy \"good\" properties, such as\ninformativeness, compactness, or modularity, to be interpretable. In this\narticle, we propose an explainable-by-design audio segmentation model based on\nnon-negative matrix factorization (NMF) which is a good candidate for the\ndesign of interpretable representations. This paper shows that our model\nreaches good segmentation performances, and presents deep analyses of the\nlatent representation extracted from the non-negative matrix. The proposed\napproach opens new perspectives toward the evaluation of interpretable\nrepresentations according to \"good\" properties.", "published": "2024-06-19 09:26:33", "link": "http://arxiv.org/abs/2406.13385v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Voice Classification Of Autistic Subjects", "abstract": "Autism Spectrum Disorders (ASD) describe a heterogeneous set of conditions\nclassified as neurodevelopmental disorders. Although the mechanisms underlying\nASD are not yet fully understood, more recent literature focused on multiple\ngenetics and/or environmental risk factors. Heterogeneity of symptoms,\nespecially in milder forms of this condition, could be a challenge for the\nclinician. In this work, an automatic speech classification algorithm is\nproposed to characterize the prosodic elements that best distinguish autism, to\nsupport the traditional diagnosis. The performance of the proposed algorithm is\nevaluted by testing the classification algorithms on a dataset composed of\nrecorded speeches, collected among both autustic and non autistic subjects.", "published": "2024-06-19 11:50:58", "link": "http://arxiv.org/abs/2406.13470v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Automated Bioacoustic Monitoring for South African Bird Species on\n  Unlabeled Data", "abstract": "Analyses for biodiversity monitoring based on passive acoustic monitoring\n(PAM) recordings is time-consuming and challenged by the presence of background\nnoise in recordings. Existing models for sound event detection (SED) worked\nonly on certain avian species and the development of further models required\nlabeled data. The developed framework automatically extracted labeled data from\navailable platforms for selected avian species. The labeled data were embedded\ninto recordings, including environmental sounds and noise, and were used to\ntrain convolutional recurrent neural network (CRNN) models. The models were\nevaluated on unprocessed real world data recorded in urban KwaZulu-Natal\nhabitats. The Adapted SED-CRNN model reached a F1 score of 0.73, demonstrating\nits efficiency under noisy, real-world conditions. The proposed approach to\nautomatically extract labeled data for chosen avian species enables an easy\nadaption of PAM to other species and habitats for future conservation projects.", "published": "2024-06-19 14:14:24", "link": "http://arxiv.org/abs/2406.13579v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
