{"title": "O1 Replication Journey -- Part 3: Inference-time Scaling for Medical\n  Reasoning", "abstract": "Building upon our previous investigations of O1 replication (Part 1: Journey\nLearning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]),\nthis work explores the potential of inference-time scaling in large language\nmodels (LLMs) for medical reasoning tasks, ranging from diagnostic\ndecision-making to treatment planning. Through extensive experiments on medical\nbenchmarks of varying complexity (MedQA, Medbullets, and JAMA Clinical\nChallenges), our investigation reveals several key insights: (1) Increasing\ninference time does lead to improved performance. With a modest training set of\n500 samples, our model yields substantial performance improvements of 6%-11%.\n(2) Task complexity directly correlates with the required length of reasoning\nchains, confirming the necessity of extended thought processes for challenging\nproblems. (3) The differential diagnoses generated by our model adhere to the\nprinciples of the hypothetico-deductive method, producing a list of potential\nconditions that may explain a patient's symptoms and systematically narrowing\nthese possibilities by evaluating the evidence. These findings demonstrate the\npromising synergy between inference-time scaling and journey learning in\nadvancing LLMs' real-world clinical reasoning capabilities.", "published": "2025-01-11 07:10:23", "link": "http://arxiv.org/abs/2501.06458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Dialogue Knowledge Aggregation for Expressive\n  Conversational Speech Synthesis", "abstract": "Conversational speech synthesis (CSS) aims to take the current dialogue (CD)\nhistory as a reference to synthesize expressive speech that aligns with the\nconversational style. Unlike CD, stored dialogue (SD) contains preserved\ndialogue fragments from earlier stages of user-agent interaction, which include\nstyle expression knowledge relevant to scenarios similar to those in CD. Note\nthat this knowledge plays a significant role in enabling the agent to\nsynthesize expressive conversational speech that generates empathetic feedback.\nHowever, prior research has overlooked this aspect. To address this issue, we\npropose a novel Retrieval-Augmented Dialogue Knowledge Aggregation scheme for\nexpressive CSS, termed RADKA-CSS, which includes three main components: 1) To\neffectively retrieve dialogues from SD that are similar to CD in terms of both\nsemantic and style. First, we build a stored dialogue semantic-style database\n(SDSSD) which includes the text and audio samples. Then, we design a\nmulti-attribute retrieval scheme to match the dialogue semantic and style\nvectors of the CD with the stored dialogue semantic and style vectors in the\nSDSSD, retrieving the most similar dialogues. 2) To effectively utilize the\nstyle knowledge from CD and SD, we propose adopting the multi-granularity graph\nstructure to encode the dialogue and introducing a multi-source style knowledge\naggregation mechanism. 3) Finally, the aggregated style knowledge are fed into\nthe speech synthesizer to help the agent synthesize expressive speech that\naligns with the conversational style. We conducted a comprehensive and in-depth\nexperiment based on the DailyTalk dataset, which is a benchmarking dataset for\nthe CSS task.\n  Both objective and subjective evaluations demonstrate that RADKA-CSS\noutperforms baseline models in expressiveness rendering. Code and audio samples\ncan be found at: https://github.com/Coder-jzq/RADKA-CSS.", "published": "2025-01-11 07:43:18", "link": "http://arxiv.org/abs/2501.06467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Large Language Models for Improving Factuality in Legal\n  Question Answering", "abstract": "Hallucination, or the generation of incorrect or fabricated information,\nremains a critical challenge in large language models (LLMs), particularly in\nhigh-stake domains such as legal question answering (QA). In order to mitigate\nthe hallucination rate in legal QA, we first introduce a benchmark called\nLegalHalBench and three automatic metrics to evaluate the common hallucinations\nwhen LLMs answer legal questions. We then propose a hallucination mitigation\nmethod that integrates behavior cloning and a novel Hard Sample-aware Iterative\nDirect Preference Optimization (HIPO). We conduct extensive real-data\nexperiments to validate the effectiveness of our approach. Our results\ndemonstrate remarkable improvements in various metrics, including the newly\nproposed Non-Hallucinated Statute Rate, Statute Relevance Rate, Legal Claim\nTruthfulness, as well as traditional metrics such as METEOR, BERTScore,\nROUGE-L, and win rates.", "published": "2025-01-11 12:08:15", "link": "http://arxiv.org/abs/2501.06521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dispersion Measures as Predictors of Lexical Decision Time, Word\n  Familiarity, and Lexical Complexity", "abstract": "Various measures of dispersion have been proposed to paint a fuller picture\nof a word's distribution in a corpus, but only little has been done to validate\nthem externally. We evaluate a wide range of dispersion measures as predictors\nof lexical decision time, word familiarity, and lexical complexity in five\ndiverse languages. We find that the logarithm of range is not only a better\npredictor than log-frequency across all tasks and languages, but that it is\nalso the most powerful additional variable to log-frequency, consistently\noutperforming the more complex dispersion measures. We discuss the effects of\ncorpus part granularity and logarithmic transformation, shedding light on\ncontradictory results of previous studies.", "published": "2025-01-11 12:56:27", "link": "http://arxiv.org/abs/2501.06536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting", "abstract": "Information retrieval, specifically contract clause retrieval, is\nfoundational to contract drafting because lawyers rarely draft contracts from\nscratch; instead, they locate and revise the most relevant precedent. We\nintroduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval\nbenchmark for contract drafting fully annotated by experts. ACORD focuses on\ncomplex contract clauses such as Limitation of Liability, Indemnification,\nChange of Control, and Most Favored Nation. It includes 114 queries and over\n126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task\nis to find the most relevant precedent clauses to a query. The bi-encoder\nretriever paired with pointwise LLMs re-rankers shows promising results.\nHowever, substantial improvements are still needed to effectively manage the\ncomplex legal work typically undertaken by lawyers. As the first retrieval\nbenchmark for contract drafting annotated by experts, ACORD can serve as a\nvaluable IR benchmark for the NLP community.", "published": "2025-01-11 16:37:49", "link": "http://arxiv.org/abs/2501.06582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual use issues in the field of Natural Language Generation", "abstract": "This report documents the results of a recent survey in the SIGGEN community,\nfocusing on Dual Use issues in Natural Language Generation (NLG). SIGGEN is the\nSpecial Interest Group (SIG) of the Association for Computational Linguistics\n(ACL) for researchers working on NLG. The survey was prompted by the ACL\nexecutive board, which asked all SIGs to provide an overview of dual use issues\nwithin their respective subfields. The survey was sent out in October 2024 and\nthe results were processed in January 2025. With 23 respondents, the survey is\npresumably not representative of all SIGGEN members, but at least this document\noffers a helpful resource for future discussions.\n  This report is open to feedback from the SIGGEN community. Let me know if you\nhave any questions or comments!", "published": "2025-01-11 20:55:35", "link": "http://arxiv.org/abs/2501.06636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Down Semantic Leakage: Investigating Associative Bias in Smaller\n  Language Models", "abstract": "Semantic leakage is a phenomenon recently introduced by Gonen et al. (2024).\nIt refers to a situation in which associations learnt from the training data\nemerge in language model generations in an unexpected and sometimes undesired\nway. Prior work has focused on leakage in large language models (7B+\nparameters). In this study, I use Qwen2.5 model family to explore whether\nsmaller models, ranging from 500M to 7B parameters, demonstrate less semantic\nleakage due to their limited capacity for capturing complex associations.\nBuilding on the previous dataset from Gonen et al. (2024), I introduce a new\ndataset of color-focused prompts, categorized into specific types of semantic\nassociations, to systematically evaluate the models' performance. Results\nindicate that smaller models exhibit less semantic leakage overall, although\nthis trend is not strictly linear, with medium-sized models sometimes\nsurpassing larger ones in leaking behavior. The dataset, the model generations,\nand the evaluation code are publicly available at\nhttps://github.com/smilni/semantic_leakage_project.", "published": "2025-01-11 21:03:22", "link": "http://arxiv.org/abs/2501.06638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Feature Augmentation Improves Generalization Performance of\n  Language Models", "abstract": "Training and fine-tuning deep learning models, especially large language\nmodels (LLMs), on limited and imbalanced datasets poses substantial challenges.\nThese issues often result in poor generalization, where models overfit to\ndominant classes and underperform on minority classes, leading to biased\npredictions and reduced robustness in real-world applications. To overcome\nthese challenges, we propose augmenting features in the embedding space by\ngenerating synthetic samples using a range of techniques. By upsampling\nunderrepresented classes, this method improves model performance and alleviates\ndata imbalance. We validate the effectiveness of this approach across multiple\nopen-source text classification benchmarks, demonstrating its potential to\nenhance model robustness and generalization in imbalanced data scenarios.", "published": "2025-01-11 04:31:18", "link": "http://arxiv.org/abs/2501.06434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare", "abstract": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.", "published": "2025-01-11 07:35:51", "link": "http://arxiv.org/abs/2501.06465v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "First Token Probability Guided RAG for Telecom Question Answering", "abstract": "Large Language Models (LLMs) have garnered significant attention for their\nimpressive general-purpose capabilities. For applications requiring intricate\ndomain knowledge, Retrieval-Augmented Generation (RAG) has shown a distinct\nadvantage in incorporating domain-specific information into LLMs. However,\nexisting RAG research has not fully addressed the challenges of Multiple Choice\nQuestion Answering (MCQA) in telecommunications, particularly in terms of\nretrieval quality and mitigating hallucinations. To tackle these challenges, we\npropose a novel first token probability guided RAG framework. This framework\nleverages confidence scores to optimize key hyperparameters, such as chunk\nnumber and chunk window size, while dynamically adjusting the context. Our\nmethod starts by retrieving the most relevant chunks and generates a single\ntoken as the potential answer. The probabilities of all options are then\nnormalized to serve as confidence scores, which guide the dynamic adjustment of\nthe context. By iteratively optimizing the hyperparameters based on these\nconfidence scores, we can continuously improve RAG performance. We conducted\nexperiments to validate the effectiveness of our framework, demonstrating its\npotential to enhance accuracy in domain-specific MCQA tasks.", "published": "2025-01-11 07:47:31", "link": "http://arxiv.org/abs/2501.06468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sequential Classification of Aviation Safety Occurrences with Natural\n  Language Processing", "abstract": "Safety is a critical aspect of the air transport system given even slight\noperational anomalies can result in serious consequences. To reduce the chances\nof aviation safety occurrences, accidents and incidents are reported to\nestablish the root cause, propose safety recommendations etc. However, analysis\nnarratives of the pre-accident events are presented using human-understandable,\nraw, unstructured, text that a computer system cannot understand. The ability\nto classify and categorise safety occurrences from their textual narratives\nwould help aviation industry stakeholders make informed safety-critical\ndecisions. To classify and categorise safety occurrences, we applied natural\nlanguage processing (NLP) and AI (Artificial Intelligence) models to process\ntext narratives. The study aimed to answer the question. How well can the\ndamage level caused to the aircraft in a safety occurrence be inferred from the\ntext narrative using natural language processing. The classification\nperformance of various deep learning models including LSTM, BLSTM, GRU, sRNN,\nand combinations of these models including LSTM and GRU, BLSTM+GRU, sRNN and\nLSTM, sRNN and BLSTM, sRNN and GRU, sRNN and BLSTM and GRU, and sRNN and LSTM\nand GRU was evaluated on a set of 27,000 safety occurrence reports from the\nNTSB. The results of this study indicate that all models investigated performed\ncompetitively well recording an accuracy of over 87.9% which is well above the\nrandom guess of 25% for a four-class classification problem. Also, the models\nrecorded high precision, recall, and F1 scores above 80%, 88%, and 85%,\nrespectively. sRNN slightly outperformed other single models in terms of recall\n(90%) and accuracy (90%) while LSTM reported slightly better performance in\nterms of precision (87%).", "published": "2025-01-11 09:23:55", "link": "http://arxiv.org/abs/2501.06490v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing the Role of Context in Forecasting with Large Language Models", "abstract": "This study evaluates the forecasting performance of recent language models\n(LLMs) on binary forecasting questions. We first introduce a novel dataset of\nover 600 binary forecasting questions, augmented with related news articles and\ntheir concise question-related summaries. We then explore the impact of input\nprompts with varying level of context on forecasting performance. The results\nindicate that incorporating news articles significantly improves performance,\nwhile using few-shot examples leads to a decline in accuracy. We find that\nlarger models consistently outperform smaller models, highlighting the\npotential of LLMs in enhancing automated forecasting.", "published": "2025-01-11 10:11:19", "link": "http://arxiv.org/abs/2501.06496v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PASS: Presentation Automation for Slide Generation and Speech", "abstract": "In today's fast-paced world, effective presentations have become an essential\ntool for communication in both online and offline meetings. The crafting of a\ncompelling presentation requires significant time and effort, from gathering\nkey insights to designing slides that convey information clearly and concisely.\nHowever, despite the wealth of resources available, people often find\nthemselves manually extracting crucial points, analyzing data, and organizing\ncontent in a way that ensures clarity and impact. Furthermore, a successful\npresentation goes beyond just the slides; it demands rehearsal and the ability\nto weave a captivating narrative to fully engage the audience. Although there\nhas been some exploration of automating document-to-slide generation, existing\nresearch is largely centered on converting research papers. In addition,\nautomation of the delivery of these presentations has yet to be addressed. We\nintroduce PASS, a pipeline used to generate slides from general Word documents,\ngoing beyond just research papers, which also automates the oral delivery of\nthe generated slides. PASS analyzes user documents to create a dynamic,\nengaging presentation with an AI-generated voice. Additionally, we developed an\nLLM-based evaluation metric to assess our pipeline across three critical\ndimensions of presentations: relevance, coherence, and redundancy. The data and\ncodes are available at https://github.com/AggarwalTushar/PASS.", "published": "2025-01-11 10:22:04", "link": "http://arxiv.org/abs/2501.06497v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing and Deep Learning Models to Classify Phase\n  of Flight in Aviation Safety Occurrences", "abstract": "The air transport system recognizes the criticality of safety, as even minor\nanomalies can have severe consequences. Reporting accidents and incidents play\na vital role in identifying their causes and proposing safety recommendations.\nHowever, the narratives describing pre-accident events are presented in\nunstructured text that is not easily understood by computer systems.\nClassifying and categorizing safety occurrences based on these narratives can\nsupport informed decision-making by aviation industry stakeholders. In this\nstudy, researchers applied natural language processing (NLP) and artificial\nintelligence (AI) models to process text narratives to classify the flight\nphases of safety occurrences. The classification performance of two deep\nlearning models, ResNet and sRNN was evaluated, using an initial dataset of\n27,000 safety occurrence reports from the NTSB. The results demonstrated good\nperformance, with both models achieving an accuracy exceeding 68%, well above\nthe random guess rate of 14% for a seven-class classification problem. The\nmodels also exhibited high precision, recall, and F1 scores. The sRNN model\ngreatly outperformed the simplified ResNet model architecture used in this\nstudy. These findings indicate that NLP and deep learning models can infer the\nflight phase from raw text narratives, enabling effective analysis of safety\noccurrences.", "published": "2025-01-11 15:02:49", "link": "http://arxiv.org/abs/2501.06564v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChemAgent: Self-updating Library in Large Language Models Improves\n  Chemical Reasoning", "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand\nprecise calculations, where even minor errors can lead to cascading failures.\nFurthermore, large language models (LLMs) encounter difficulties handling\ndomain-specific formulas, executing reasoning steps accurately, and integrating\ncode effectively when tackling chemical reasoning tasks. To address these\nchallenges, we present ChemAgent, a novel framework designed to improve the\nperformance of LLMs through a dynamic, self-updating library. This library is\ndeveloped by decomposing chemical tasks into sub-tasks and compiling these\nsub-tasks into a structured collection that can be referenced for future\nqueries. Then, when presented with a new problem, ChemAgent retrieves and\nrefines pertinent information from the library, which we call memory,\nfacilitating effective task decomposition and the generation of solutions. Our\nmethod designs three types of memory and a library-enhanced reasoning\ncomponent, enabling LLMs to improve over time through experience. Experimental\nresults on four chemical reasoning datasets from SciBench demonstrate that\nChemAgent achieves performance gains of up to 46% (GPT-4), significantly\noutperforming existing methods. Our findings suggest substantial potential for\nfuture applications, including tasks such as drug discovery and materials\nscience. Our code can be found at https://github.com/gersteinlab/chemagent", "published": "2025-01-11 17:10:30", "link": "http://arxiv.org/abs/2501.06590v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings", "abstract": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B.\nAdditionally, we empirically reveals how FocalPO affects training on correct\nand incorrect sample groups, further underscoring its effectiveness.", "published": "2025-01-11 21:41:27", "link": "http://arxiv.org/abs/2501.06645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Magnitude of Categories of Texts Enriched by Language Models", "abstract": "The purpose of this article is twofold. Firstly, we use the next-token\nprobabilities given by a language model to explicitly define a\n$[0,1]$-enrichment of a category of texts in natural language, in the sense of\nBradley, Terilla, and Vlassopoulos. We consider explicitly the terminating\nconditions for text generation and determine when the enrichment itself can be\ninterpreted as a probability over texts. Secondly, we compute the M\\\"obius\nfunction and the magnitude of an associated generalized metric space\n$\\mathcal{M}$ of texts using a combinatorial version of these quantities\nrecently introduced by Vigneaux. The magnitude function $f(t)$ of $\\mathcal{M}$\nis a sum over texts $x$ (prompts) of the Tsallis $t$-entropies of the\nnext-token probability distributions $p(-|x)$ plus the cardinality of the\nmodel's possible outputs. The derivative of $f$ at $t=1$ recovers a sum of\nShannon entropies, which justifies seeing magnitude as a partition function.\nFollowing Leinster and Schulman, we also express the magnitude function of\n$\\mathcal M$ as an Euler characteristic of magnitude homology and provide an\nexplicit description of the zeroeth and first magnitude homology groups.", "published": "2025-01-11 23:28:50", "link": "http://arxiv.org/abs/2501.06662v1", "categories": ["math.CT", "cs.CL", "18D20, 68T50", "I.2.7; G.3"], "primary_category": "math.CT"}
{"title": "Tensor Product Attention Is All You Need", "abstract": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.", "published": "2025-01-11 03:37:10", "link": "http://arxiv.org/abs/2501.06425v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Recognition for Automatically Assessing Afrikaans and isiXhosa\n  Preschool Oral Narratives", "abstract": "We develop automatic speech recognition (ASR) systems for stories told by\nAfrikaans and isiXhosa preschool children. Oral narratives provide a way to\nassess children's language development before they learn to read. We consider a\nrange of prior child-speech ASR strategies to determine which is best suited to\nthis unique setting. Using Whisper and only 5 minutes of transcribed in-domain\nchild speech, we find that additional in-domain adult data (adult speech\nmatching the story domain) provides the biggest improvement, especially when\ncoupled with voice conversion. Semi-supervised learning also helps for both\nlanguages, while parameter-efficient fine-tuning helps on Afrikaans but not on\nisiXhosa (which is under-represented in the Whisper model). Few child-speech\nstudies look at non-English data, and even fewer at the preschool ages of 4 and\n5. Our work therefore represents a unique validation of a wide range of\nprevious child-speech ASR strategies in an under-explored setting.", "published": "2025-01-11 08:11:09", "link": "http://arxiv.org/abs/2501.06478v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey on Spoken Italian Datasets and Corpora", "abstract": "Spoken language datasets are vital for advancing linguistic research, Natural\nLanguage Processing, and speech technology. However, resources dedicated to\nItalian, a linguistically rich and diverse Romance language, remain\nunderexplored compared to major languages like English or Mandarin. This survey\nprovides a comprehensive analysis of 66 spoken Italian datasets, highlighting\ntheir characteristics, methodologies, and applications. The datasets are\ncategorized by speech type, source and context, and demographic and linguistic\nfeatures, with a focus on their utility in fields such as Automatic Speech\nRecognition, emotion detection, and education. Challenges related to dataset\nscarcity, representativeness, and accessibility are discussed alongside\nrecommendations for enhancing dataset creation and utilization. The full\ndataset inventory is publicly accessible via GitHub and archived on Zenodo,\nserving as a valuable resource for researchers and developers. By addressing\ncurrent gaps and proposing future directions, this work aims to support the\nadvancement of Italian speech technologies and linguistic research.", "published": "2025-01-11 14:33:57", "link": "http://arxiv.org/abs/2501.06557v2", "categories": ["cs.CL", "cs.AI", "cs.DL", "A.1; I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping", "abstract": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 29% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens. We release\nour code for training and inference for easier replication of experiments.", "published": "2025-01-11 17:06:30", "link": "http://arxiv.org/abs/2501.06589v4", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "EmoXpt: Analyzing Emotional Variances in Human Comments and\n  LLM-Generated Responses", "abstract": "The widespread adoption of generative AI has generated diverse opinions, with\nindividuals expressing both support and criticism of its applications. This\nstudy investigates the emotional dynamics surrounding generative AI by\nanalyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and\nLLMs. To further understand the emotional intelligence of ChatGPT, we examine\nits responses to selected tweets, highlighting differences in sentiment between\nhuman comments and LLM-generated responses. We introduce EmoXpt, a sentiment\nanalysis framework designed to assess both human perspectives on generative AI\nand the sentiment embedded in ChatGPT's responses. Unlike prior studies that\nfocus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional\nexpression of ChatGPT. Experimental results demonstrate that LLM-generated\nresponses are notably more efficient, cohesive, and consistently positive than\nhuman responses.", "published": "2025-01-11 17:45:13", "link": "http://arxiv.org/abs/2501.06597v1", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Ultra Memory-Efficient On-FPGA Training of Transformers via\n  Tensor-Compressed Optimization", "abstract": "Transformer models have achieved state-of-the-art performance across a wide\nrange of machine learning tasks. There is growing interest in training\ntransformers on resource-constrained edge devices due to considerations such as\nprivacy, domain adaptation, and on-device scientific machine learning. However,\nthe significant computational and memory demands required for transformer\ntraining often exceed the capabilities of an edge device. Leveraging low-rank\ntensor compression, this paper presents the first on-FPGA accelerator for\nend-to-end transformer training. On the algorithm side, we present a\nbi-directional contraction flow for tensorized transformer training,\nsignificantly reducing the computational FLOPS and intra-layer memory costs\ncompared to existing tensor operations. On the hardware side, we store all\nhighly compressed model parameters and gradient information on chip, creating\nan on-chip-memory-only framework for each stage in training. This reduces\noff-chip communication and minimizes latency and energy costs. Additionally, we\nimplement custom computing kernels for each training stage and employ\nintra-layer parallelism and pipe-lining to further enhance run-time and memory\nefficiency. Through experiments on transformer models within $36.7$ to $93.5$\nMB using FP-32 data formats on the ATIS dataset, our tensorized FPGA\naccelerator could conduct single-batch end-to-end training on the AMD Alevo U50\nFPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM.\nCompared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA\ntraining achieves a memory reduction of $30\\times$ to $51\\times$. Our FPGA\naccelerator also achieves up to $3.6\\times$ less energy cost per epoch compared\nwith tensor Transformer training on an NVIDIA RTX 3090 GPU.", "published": "2025-01-11 23:29:51", "link": "http://arxiv.org/abs/2501.06663v1", "categories": ["cs.LG", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The 1st SpeechWellness Challenge: Detecting Suicidal Risk Among\n  Adolescents", "abstract": "The 1st SpeechWellness Challenge (SW1) aims to advance methods for detecting\nsuicidal risk in adolescents using speech analysis techniques. Suicide among\nadolescents is a critical public health issue globally. Early detection of\nsuicidal tendencies can lead to timely intervention and potentially save lives.\nTraditional methods of assessment often rely on self-reporting or clinical\ninterviews, which may not always be accessible. The SW1 challenge addresses\nthis gap by exploring speech as a non-invasive and readily available indicator\nof mental health. We release the SW1 dataset which contains speech recordings\nfrom 600 adolescents aged 10-18 years. By focusing on speech generated from\nnatural tasks, the challenge seeks to uncover patterns and markers that\ncorrelate with suicidal risk.", "published": "2025-01-11 08:03:41", "link": "http://arxiv.org/abs/2501.06474v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-modal Speech Enhancement with Limited Electromyography Channels", "abstract": "Speech enhancement (SE) aims to improve the clarity, intelligibility, and\nquality of speech signals for various speech enabled applications. However,\nair-conducted (AC) speech is highly susceptible to ambient noise, particularly\nin low signal-to-noise ratio (SNR) and non-stationary noise environments.\nIncorporating multi-modal information has shown promise in enhancing speech in\nsuch challenging scenarios. Electromyography (EMG) signals, which capture\nmuscle activity during speech production, offer noise-resistant properties\nbeneficial for SE in adverse conditions. Most previous EMG-based SE methods\nrequired 35 EMG channels, limiting their practicality. To address this, we\npropose a novel method that considers only 8-channel EMG signals with acoustic\nsignals using a modified SEMamba network with added cross-modality modules. Our\nexperiments demonstrate substantial improvements in speech quality and\nintelligibility over traditional approaches, especially in extremely low SNR\nsettings. Notably, compared to the SE (AC) approach, our method achieves a\nsignificant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under\nmismatched conditions, highlighting its robustness.", "published": "2025-01-11 12:33:33", "link": "http://arxiv.org/abs/2501.06530v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation", "abstract": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.", "published": "2025-01-11 00:47:29", "link": "http://arxiv.org/abs/2501.06394v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in\n  Open-Set Condition", "abstract": "Current research in audio deepfake detection is gradually transitioning from\nbinary classification to multi-class tasks, referred as audio deepfake source\ntracing task. However, existing studies on source tracing consider only\nclosed-set scenarios and have not considered the challenges posed by open-set\nconditions. In this paper, we define the Neural Codec Source Tracing (NCST)\ntask, which is capable of performing open-set neural codec classification and\ninterpretable ALM detection. Specifically, we constructed the ST-Codecfake\ndataset for the NCST task, which includes bilingual audio samples generated by\n11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD)\ntest samples. Furthermore, we establish a comprehensive source tracing\nbenchmark to assess NCST models in open-set conditions. The experimental\nresults reveal that although the NCST models perform well in in-distribution\n(ID) classification and OOD detection, they lack robustness in classifying\nunseen real audio. The ST-codecfake dataset and code are available.", "published": "2025-01-11 11:15:58", "link": "http://arxiv.org/abs/2501.06514v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discrete Speech Unit Extraction via Independent Component Analysis", "abstract": "Self-supervised speech models (S3Ms) have become a common tool for the speech\nprocessing community, leveraging representations for downstream tasks.\nClustering S3M representations yields discrete speech units (DSUs), which serve\nas compact representations for speech signals. DSUs are typically obtained by\nk-means clustering. Using DSUs often leads to strong performance in various\ntasks, including automatic speech recognition (ASR). However, even with the\nhigh dimensionality and redundancy of S3M representations, preprocessing S3M\nrepresentations for better clustering remains unexplored, even though it can\naffect the quality of DSUs. In this paper, we investigate the potential of\nlinear preprocessing methods for extracting DSUs. We evaluate standardization,\nprincipal component analysis, whitening, and independent component analysis\n(ICA) on DSU-based ASR benchmarks and demonstrate their effectiveness as\npreprocessing for k-means. We also conduct extensive analyses of their\nbehavior, such as orthogonality or interpretability of individual components of\nICA.", "published": "2025-01-11 14:45:03", "link": "http://arxiv.org/abs/2501.06562v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
