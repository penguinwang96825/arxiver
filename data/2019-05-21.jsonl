{"title": "Adaptation and learning over networks under subspace constraints -- Part I: Stability Analysis", "abstract": "This paper considers optimization problems over networks where agents have individual objectives to meet, or individual parameter vectors to estimate, subject to subspace constraints that require the objectives across the network to lie in low-dimensional subspaces. This constrained formulation includes consensus optimization as a special case, and allows for more general task relatedness models such as smoothness. While such formulations can be solved via projected gradient descent, the resulting algorithm is not distributed. Starting from the centralized solution, we propose an iterative and distributed implementation of the projection step, which runs in parallel with the stochastic gradient descent update. We establish in this Part I of the work that, for small step-sizes $\u03bc$, the proposed distributed adaptive strategy leads to small estimation errors on the order of $\u03bc$. We examine in the accompanying Part II [2] the steady-state performance. The results will reveal explicitly the influence of the gradient noise, data characteristics, and subspace constraints, on the network performance. The results will also show that in the small step-size regime, the iterates generated by the distributed algorithm achieve the centralized steady-state performance.", "published": "2019-05-21 16:59:16", "link": "http://arxiv.org/abs/1905.08750v2", "categories": ["cs.MA", "eess.SP"], "primary_category": "cs.MA"}
