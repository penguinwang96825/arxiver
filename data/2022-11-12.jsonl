{"title": "Generating Textual Adversaries with Minimal Perturbation", "abstract": "Many word-level adversarial attack approaches for textual data have been\nproposed in recent studies. However, due to the massive search space consisting\nof combinations of candidate words, the existing approaches face the problem of\npreserving the semantics of texts when crafting adversarial counterparts. In\nthis paper, we develop a novel attack strategy to find adversarial texts with\nhigh similarity to the original texts while introducing minimal perturbation.\nThe rationale is that we expect the adversarial texts with small perturbation\ncan better preserve the semantic meaning of original texts. Experiments show\nthat, compared with state-of-the-art attack approaches, our approach achieves\nhigher success rates and lower perturbation rates in four benchmark datasets.", "published": "2022-11-12 04:46:07", "link": "http://arxiv.org/abs/2211.06571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConceptX: A Framework for Latent Concept Analysis", "abstract": "The opacity of deep neural networks remains a challenge in deploying\nsolutions where explanation is as important as precision. We present ConceptX,\na human-in-the-loop framework for interpreting and annotating latent\nrepresentational space in pre-trained Language Models (pLMs). We use an\nunsupervised method to discover concepts learned in these models and enable a\ngraphical interface for humans to generate explanations for the concepts. To\nfacilitate the process, we provide auto-annotations of the concepts (based on\ntraditional linguistic ontologies). Such annotations enable development of a\nlinguistic resource that directly represents latent concepts learned within\ndeep NLP models. These include not just traditional linguistic concepts, but\nalso task-specific or sensitive concepts (words grouped based on gender or\nreligious connotation) that helps the annotators to mark bias in the model. The\nframework consists of two parts (i) concept discovery and (ii) annotation\nplatform.", "published": "2022-11-12 11:31:09", "link": "http://arxiv.org/abs/2211.06642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLPeer: A Unified Resource for the Computational Study of Peer Review", "abstract": "Peer review constitutes a core component of scholarly publishing; yet it\ndemands substantial expertise and training, and is susceptible to errors and\nbiases. Various applications of NLP for peer reviewing assistance aim to\nsupport reviewers in this complex process, but the lack of clearly licensed\ndatasets and multi-domain corpora prevent the systematic study of NLP for peer\nreview. To remedy this, we introduce NLPeer -- the first ethically sourced\nmultidomain corpus of more than 5k papers and 11k review reports from five\ndifferent venues. In addition to the new datasets of paper drafts, camera-ready\nversions and peer reviews from the NLP community, we establish a unified data\nrepresentation and augment previous peer review datasets to include parsed and\nstructured paper representations, rich metadata and versioning information. We\ncomplement our resource with implementations and analysis of three reviewing\nassistance tasks, including a novel guided skimming task. Our work paves the\npath towards systematic, multi-faceted, evidence-based study of peer review in\nNLP and beyond. The data and code are publicly available.", "published": "2022-11-12 12:29:38", "link": "http://arxiv.org/abs/2211.06651v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Segmentation Ambiguity in Neural Linguistic Steganography", "abstract": "Previous studies on neural linguistic steganography, except Ueoka et al.\n(2021), overlook the fact that the sender must detokenize cover texts to avoid\narousing the eavesdropper's suspicion. In this paper, we demonstrate that\nsegmentation ambiguity indeed causes occasional decoding failures at the\nreceiver's side. With the near-ubiquity of subwords, this problem now affects\nany language. We propose simple tricks to overcome this problem, which are even\napplicable to languages without explicit word boundaries.", "published": "2022-11-12 13:42:49", "link": "http://arxiv.org/abs/2211.06662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language\n  Capabilities", "abstract": "In this work, we present a conceptually simple and effective method to train\na strong bilingual/multilingual multimodal representation model. Starting from\nthe pre-trained multimodal representation model CLIP released by OpenAI, we\naltered its text encoder with a pre-trained multilingual text encoder XLM-R,\nand aligned both languages and image representations by a two-stage training\nschema consisting of teacher learning and contrastive learning. We validate our\nmethod through evaluations of a wide range of tasks. We set new\nstate-of-the-art performances on a bunch of tasks including ImageNet-CN,\nFlicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder\nin CLIP for extended capabilities such as multilingual understanding. Our\nmodels and code are available at https://github.com/FlagAI-Open/FlagAI.", "published": "2022-11-12 14:48:55", "link": "http://arxiv.org/abs/2211.06679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collecting Interactive Multi-modal Datasets for Grounded Language\n  Understanding", "abstract": "Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.", "published": "2022-11-12 02:36:32", "link": "http://arxiv.org/abs/2211.06552v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic\n  Fusion Prompts", "abstract": "Multimodal sentiment analysis has gained significant attention due to the\nproliferation of multimodal content on social media. However, existing studies\nin this area rely heavily on large-scale supervised data, which is\ntime-consuming and labor-intensive to collect. Thus, there is a need to address\nthe challenge of few-shot multimodal sentiment analysis. To tackle this\nproblem, we propose a novel method called Multimodal Probabilistic Fusion\nPrompts (MultiPoint) that leverages diverse cues from different modalities for\nmultimodal sentiment detection in the few-shot scenario. Specifically, we start\nby introducing a Consistently Distributed Sampling approach called CDS, which\nensures that the few-shot dataset has the same category distribution as the\nfull dataset. Unlike previous approaches primarily using prompts based on the\ntext modality, we design unified multimodal prompts to reduce discrepancies\nbetween different modalities and dynamically incorporate multimodal\ndemonstrations into the context of each multimodal instance. To enhance the\nmodel's robustness, we introduce a probabilistic fusion method to fuse output\npredictions from multiple diverse prompts for each input. Our extensive\nexperiments on six datasets demonstrate the effectiveness of our approach.\nFirst, our method outperforms strong baselines in the multimodal few-shot\nsetting. Furthermore, under the same amount of data (1% of the full dataset),\nour CDS-based experimental results significantly outperform those based on\npreviously sampled datasets constructed from the same number of instances of\neach class.", "published": "2022-11-12 08:10:35", "link": "http://arxiv.org/abs/2211.06607v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "A unified one-shot prosody and speaker conversion system with\n  self-supervised discrete speech units", "abstract": "We present a unified system to realize one-shot voice conversion (VC) on the\npitch, rhythm, and speaker attributes. Existing works generally ignore the\ncorrelation between prosody and language content, leading to the degradation of\nnaturalness in converted speech. Additionally, the lack of proper language\nfeatures prevents these systems from accurately preserving language content\nafter conversion. To address these issues, we devise a cascaded modular system\nleveraging self-supervised discrete speech units as language representation.\nThese discrete units provide duration information essential for rhythm\nmodeling. Our system first extracts utterance-level prosody and speaker\nrepresentations from the raw waveform. Given the prosody representation, a\nprosody predictor estimates pitch, energy, and duration for each discrete unit\nin the utterance. A synthesizer further reconstructs speech based on the\npredicted prosody, speaker representation, and discrete units. Experiments show\nthat our system outperforms previous approaches in naturalness,\nintelligibility, speaker transferability, and prosody transferability. Code and\nsamples are publicly available.", "published": "2022-11-12 00:54:09", "link": "http://arxiv.org/abs/2211.06535v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lifelong and Continual Learning Dialogue Systems", "abstract": "Dialogue systems, commonly known as chatbots, have gained escalating\npopularity in recent times due to their wide-spread applications in carrying\nout chit-chat conversations with users and task-oriented dialogues to\naccomplish various user tasks. Existing chatbots are usually trained from\npre-collected and manually-labeled data and/or written with handcrafted rules.\nMany also use manually-compiled knowledge bases (KBs). Their ability to\nunderstand natural language is still limited, and they tend to produce many\nerrors resulting in poor user satisfaction. Typically, they need to be\nconstantly improved by engineers with more labeled data and more manually\ncompiled knowledge. This book introduces the new paradigm of lifelong learning\ndialogue systems to endow chatbots the ability to learn continually by\nthemselves through their own self-initiated interactions with their users and\nworking environments to improve themselves. As the systems chat more and more\nwith users or learn more and more from external sources, they become more and\nmore knowledgeable and better and better at conversing. The book presents the\nlatest developments and techniques for building such continual learning\ndialogue systems that continuously learn new language expressions and lexical\nand factual knowledge during conversation from users and off conversation from\nexternal sources, acquire new training examples during conversation, and learn\nconversational skills. Apart from these general topics, existing works on\ncontinual learning of some specific aspects of dialogue systems are also\nsurveyed. The book concludes with a discussion of open challenges for future\nresearch.", "published": "2022-11-12 02:39:41", "link": "http://arxiv.org/abs/2211.06553v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Robustness of DistilHuBERT to Unseen Noisy Conditions via\n  Data Augmentation, Curriculum Learning, and Multi-Task Enhancement", "abstract": "Self-supervised speech representation learning aims to extract meaningful\nfactors from the speech signal that can later be used across different\ndownstream tasks, such as speech and/or emotion recognition. Existing models,\nsuch as HuBERT, however, can be fairly large thus may not be suitable for edge\nspeech applications. Moreover, realistic applications typically involve speech\ncorrupted by noise and room reverberation, hence models need to provide\nrepresentations that are robust to such environmental factors. In this study,\nwe build on the so-called DistilHuBERT model, which distils HuBERT to a\nfraction of its original size, with three modifications, namely: (i) augment\nthe training data with noise and reverberation, while the student model needs\nto distill the clean representations from the teacher model; (ii) introduce a\ncurriculum learning approach where increasing levels of noise are introduced as\nthe model trains, thus helping with convergence and with the creation of more\nrobust representations; and (iii) introduce a multi-task learning approach\nwhere the model also reconstructs the clean waveform jointly with the\ndistillation task, thus also acting as an enhancement step to ensure additional\nenvironment robustness to the representation. Experiments on three SUPERB tasks\nshow the advantages of the proposed method not only relative to the original\nDistilHuBERT, but also to the original HuBERT, thus showing the advantages of\nthe proposed method for ``in the wild'' edge speech applications.", "published": "2022-11-12 03:50:22", "link": "http://arxiv.org/abs/2211.06562v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeltaNet:Conditional Medical Report Generation for COVID-19 Diagnosis", "abstract": "Fast screening and diagnosis are critical in COVID-19 patient treatment. In\naddition to the gold standard RT-PCR, radiological imaging like X-ray and CT\nalso works as an important means in patient screening and follow-up. However,\ndue to the excessive number of patients, writing reports becomes a heavy burden\nfor radiologists. To reduce the workload of radiologists, we propose DeltaNet\nto generate medical reports automatically. Different from typical image\ncaptioning approaches that generate reports with an encoder and a decoder,\nDeltaNet applies a conditional generation process. In particular, given a\nmedical image, DeltaNet employs three steps to generate a report: 1) first\nretrieving related medical reports, i.e., the historical reports from the same\nor similar patients; 2) then comparing retrieved images and current image to\nfind the differences; 3) finally generating a new report to accommodate\nidentified differences based on the conditional report. We evaluate DeltaNet on\na COVID-19 dataset, where DeltaNet outperforms state-of-the-art approaches.\nBesides COVID-19, the proposed DeltaNet can be applied to other diseases as\nwell. We validate its generalization capabilities on the public IU-Xray and\nMIMIC-CXR datasets for chest-related diseases. Code is available at\n\\url{https://github.com/LX-doctorAI1/DeltaNet}.", "published": "2022-11-12 07:41:03", "link": "http://arxiv.org/abs/2211.13229v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "A Self-Adjusting Fusion Representation Learning Model for Unaligned\n  Text-Audio Sequences", "abstract": "Inter-modal interaction plays an indispensable role in multimodal sentiment\nanalysis. Due to different modalities sequences are usually non-alignment, how\nto integrate relevant information of each modality to learn fusion\nrepresentations has been one of the central challenges in multimodal learning.\nIn this paper, a Self-Adjusting Fusion Representation Learning Model (SA-FRLM)\nis proposed to learn robust crossmodal fusion representations directly from the\nunaligned text and audio sequences. Different from previous works, our model\nnot only makes full use of the interaction between different modalities but\nalso maximizes the protection of the unimodal characteristics. Specifically, we\nfirst employ a crossmodal alignment module to project different modalities\nfeatures to the same dimension. The crossmodal collaboration attention is then\nadopted to model the inter-modal interaction between text and audio sequences\nand initialize the fusion representations. After that, as the core unit of the\nSA-FRLM, the crossmodal adjustment transformer is proposed to protect original\nunimodal characteristics. It can dynamically adapt the fusion representations\nby using single modal streams. We evaluate our approach on the public\nmultimodal sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experiment\nresults show that our model has significantly improved the performance of all\nthe metrics on the unaligned text-audio sequences.", "published": "2022-11-12 13:05:28", "link": "http://arxiv.org/abs/2212.11772v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Low Pass Filtering and Bandwidth Extension for Robust Anti-spoofing\n  Countermeasure Against Codec Variabilities", "abstract": "A reliable voice anti-spoofing countermeasure system needs to robustly\nprotect automatic speaker verification (ASV) systems in various kinds of\nspoofing scenarios. However, the performance of countermeasure systems could be\ndegraded by channel effects and codecs. In this paper, we show that using the\nlow-frequency subbands of signals as input can mitigate the negative impact\nintroduced by codecs on the countermeasure systems. To validate this, two types\nof low-pass filters with different cut-off frequencies are applied to\ncountermeasure systems, and the equal error rate (EER) is reduced by up to 25%\nrelatively. In addition, we propose a deep learning based bandwidth extension\napproach to further improve the detection accuracy. Recent studies show that\nthe error rate of countermeasure systems increase dramatically when the silence\npart is removed by Voice Activity Detection (VAD), our experimental results\nshow that the filtering and bandwidth extension approaches are also effective\nunder the codec condition when VAD is applied.", "published": "2022-11-12 02:04:00", "link": "http://arxiv.org/abs/2211.06546v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigations in Audio Captioning: Addressing Vocabulary Imbalance and\n  Evaluating Suitability of Language-Centric Performance Metrics", "abstract": "The analysis, processing, and extraction of meaningful information from\nsounds all around us is the subject of the broader area of audio analytics.\nAudio captioning is a recent addition to the domain of audio analytics, a\ncross-modal translation task that focuses on generating natural descriptions\nfrom sound events occurring in an audio stream. In this work, we identify and\nimprove on three main challenges in automated audio captioning: i) data\nscarcity, ii) imbalance or limitations in the audio captions vocabulary, and\niii) the proper performance evaluation metric that can best capture both\nauditory and semantic characteristics. We find that generally adopted loss\nfunctions can result in an unfair vocabulary imbalance during model training.\nWe propose two audio captioning augmentation methods that enrich the training\ndataset and the vocabulary size. We further underline the need for in-domain\npretraining by exploring the suitability of audio encoders that were previously\ntrained on different audio tasks. Finally, we systematically explore five\nperformance metrics borrowed from the image captioning domain and highlight\ntheir limitations for the audio domain.", "published": "2022-11-12 02:12:21", "link": "http://arxiv.org/abs/2211.06547v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion\n  and Keyword-to-Caption Augmentation", "abstract": "Contrastive learning has shown remarkable success in the field of multimodal\nrepresentation learning. In this paper, we propose a pipeline of contrastive\nlanguage-audio pretraining to develop an audio representation by combining\naudio data with natural language descriptions. To accomplish this target, we\nfirst release LAION-Audio-630K, a large collection of 633,526 audio-text pairs\nfrom different data sources. Second, we construct a contrastive language-audio\npretraining model by considering different audio encoders and text encoders. We\nincorporate the feature fusion mechanism and keyword-to-caption augmentation\ninto the model design to further enable the model to process audio inputs of\nvariable lengths and enhance the performance. Third, we perform comprehensive\nexperiments to evaluate our model across three tasks: text-to-audio retrieval,\nzero-shot audio classification, and supervised audio classification. The\nresults demonstrate that our model achieves superior performance in\ntext-to-audio retrieval task. In audio classification tasks, the model achieves\nstate-of-the-art performance in the zero-shot setting and is able to obtain\nperformance comparable to models' results in the non-zero-shot setting.\nLAION-Audio-630K and the proposed model are both available to the public.", "published": "2022-11-12 15:25:20", "link": "http://arxiv.org/abs/2211.06687v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Speaker and Wide-Band Simulated Conversations as Training Data for\n  End-to-End Neural Diarization", "abstract": "End-to-end diarization presents an attractive alternative to standard\ncascaded diarization systems because a single system can handle all aspects of\nthe task at once. Many flavors of end-to-end models have been proposed but all\nof them require (so far non-existing) large amounts of annotated data for\ntraining. The compromise solution consists in generating synthetic data and the\nrecently proposed simulated conversations (SC) have shown remarkable\nimprovements over the original simulated mixtures (SM). In this work, we create\nSC with multiple speakers per conversation and show that they allow for\nsubstantially better performance than SM, also reducing the dependence on a\nfine-tuning stage. We also create SC with wide-band public audio sources and\npresent an analysis on several evaluation sets. Together with this publication,\nwe release the recipes for generating such data and models trained on public\nsets as well as the implementation to efficiently handle multiple speakers per\nconversation and an auxiliary voice activity detection loss.", "published": "2022-11-12 21:32:06", "link": "http://arxiv.org/abs/2211.06750v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Speech Quality Assessment using Self-supervised Framewise\n  Embeddings", "abstract": "Automatic speech quality assessment is essential for audio researchers,\ndevelopers, speech and language pathologists, and system quality engineers. The\ncurrent state-of-the-art systems are based on framewise speech features\n(hand-engineered or learnable) combined with time dependency modeling. This\npaper proposes an efficient system with results comparable to the best\nperforming model in the ConferencingSpeech 2022 challenge. Our proposed system\nis characterized by a smaller number of parameters (40-60x), fewer FLOPS\n(100x), lower memory consumption (10-15x), and lower latency (30x). Speech\nquality practitioners can therefore iterate much faster, deploy the system on\nresource-limited hardware, and, overall, the proposed system contributes to\nsustainable machine learning. The paper also concludes that framewise\nembeddings outperform utterance-level embeddings and that multi-task training\nwith acoustic conditions modeling does not degrade speech quality prediction\nwhile providing better interpretation.", "published": "2022-11-12 11:57:08", "link": "http://arxiv.org/abs/2211.06646v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Phase Reconstruction via DNN-based Phase Differences Estimation", "abstract": "This paper presents a two-stage online phase reconstruction framework using\ncausal deep neural networks (DNNs). Phase reconstruction is a task of\nrecovering phase of the short-time Fourier transform (STFT) coefficients only\nfrom the corresponding magnitude. However, phase is sensitive to waveform\nshifts and not easy to estimate from the magnitude even with a DNN. To overcome\nthis problem, we propose to use DNNs for estimating differences of phase\nbetween adjacent time-frequency bins. We show that convolutional neural\nnetworks are suitable for phase difference estimation, according to the\ntheoretical relation between partial derivatives of STFT phase and magnitude.\nThe estimated phase differences are used for reconstructing phase by solving a\nweighted least squares problem in a frame-by-frame manner. In contrast to\nexisting DNN-based phase reconstruction methods, the proposed framework is\ncausal and does not require any iterative procedure. The experiments showed\nthat the proposed method outperforms existing online methods and a DNN-based\nmethod for phase reconstruction.", "published": "2022-11-12 20:45:51", "link": "http://arxiv.org/abs/2211.08246v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
