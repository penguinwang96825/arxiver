{"title": "Is ChatGPT A Good Keyphrase Generator? A Preliminary Study", "abstract": "The emergence of ChatGPT has recently garnered significant attention from the\ncomputational linguistics community. To demonstrate its capabilities as a\nkeyphrase generator, we conduct a preliminary evaluation of ChatGPT for the\nkeyphrase generation task. We evaluate its performance in various aspects,\nincluding keyphrase generation prompts, keyphrase generation diversity, and\nlong document understanding. Our evaluation is based on six benchmark datasets,\nand we adopt the prompt suggested by OpenAI while extending it to six candidate\nprompts. We find that ChatGPT performs exceptionally well on all six candidate\nprompts, with minor performance differences observed across the datasets. Based\non our findings, we conclude that ChatGPT has great potential for keyphrase\ngeneration. Moreover, we discover that ChatGPT still faces challenges when it\ncomes to generating absent keyphrases. Meanwhile, in the final section, we also\npresent some limitations and future expansions of this report.", "published": "2023-03-23 02:50:38", "link": "http://arxiv.org/abs/2303.13001v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Classification with Decoupled Representation", "abstract": "Retrieval augmented methods have shown promising results in various\nclassification tasks. However, existing methods focus on retrieving extra\ncontext to enrich the input, which is noise sensitive and non-expandable. In\nthis paper, following this line, we propose a $k$-nearest-neighbor (KNN) -based\nmethod for retrieval augmented classifications, which interpolates the\npredicted label distribution with retrieved instances' label distributions.\nDifferent from the standard KNN process, we propose a decoupling mechanism as\nwe find that shared representation for classification and retrieval hurts\nperformance and leads to training instability. We evaluate our method on a wide\nrange of classification datasets. Experimental results demonstrate the\neffectiveness and robustness of our proposed method. We also conduct extra\nexperiments to analyze the contributions of different components in our\nmodel.\\footnote{\\url{https://github.com/xnliang98/knn-cls-w-decoupling}}", "published": "2023-03-23 06:33:06", "link": "http://arxiv.org/abs/2303.13065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SwissBERT: The Multilingual Language Model for Switzerland", "abstract": "We present SwissBERT, a masked language model created specifically for\nprocessing Switzerland-related text. SwissBERT is a pre-trained model that we\nadapted to news articles written in the national languages of Switzerland --\nGerman, French, Italian, and Romansh. We evaluate SwissBERT on natural language\nunderstanding tasks related to Switzerland and find that it tends to outperform\nprevious models on these tasks, especially when processing contemporary news\nand/or Romansh Grischun. Since SwissBERT uses language adapters, it may be\nextended to Swiss German dialects in future work. The model and our open-source\ncode are publicly released at https://github.com/ZurichNLP/swissbert.", "published": "2023-03-23 14:44:47", "link": "http://arxiv.org/abs/2303.13310v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development and validation of a natural language processing algorithm to\n  pseudonymize documents in the context of a clinical data warehouse", "abstract": "The objective of this study is to address the critical issue of\nde-identification of clinical reports in order to allow access to data for\nresearch purposes, while ensuring patient privacy. The study highlights the\ndifficulties faced in sharing tools and resources in this domain and presents\nthe experience of the Greater Paris University Hospitals (AP-HP) in\nimplementing a systematic pseudonymization of text documents from its Clinical\nData Warehouse. We annotated a corpus of clinical documents according to 12\ntypes of identifying entities, and built a hybrid system, merging the results\nof a deep learning model as well as manual rules. Our results show an overall\nperformance of 0.99 of F1-score. We discuss implementation choices and present\nexperiments to better understand the effort involved in such a task, including\ndataset size, document types, language models, or rule addition. We share\nguidelines and code under a 3-Clause BSD license.", "published": "2023-03-23 17:17:46", "link": "http://arxiv.org/abs/2303.13451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mordecai 3: A Neural Geoparser and Event Geocoder", "abstract": "Mordecai3 is a new end-to-end text geoparser and event geolocation system.\nThe system performs toponym resolution using a new neural ranking model to\nresolve a place name extracted from a document to its entry in the Geonames\ngazetteer. It also performs event geocoding, the process of linking events\nreported in text with the place names where they are reported to occur, using\nan off-the-shelf question-answering model. The toponym resolution model is\ntrained on a diverse set of existing training data, along with several thousand\nnewly annotated examples. The paper describes the model, its training process,\nand performance comparisons with existing geoparsers. The system is available\nas an open source Python library, Mordecai 3, and replaces an earlier\ngeoparser, Mordecai v2, one of the most widely used text geoparsers (Halterman\n2017).", "published": "2023-03-23 21:10:04", "link": "http://arxiv.org/abs/2303.13675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain\n  Batch and Proxy Gradient Transfer", "abstract": "In Task Oriented Dialogue (TOD) system, detecting and inducing new intents\nare two main challenges to apply the system in the real world. In this paper,\nwe suggest the semantic multi-view model to resolve these two challenges: (1)\nSBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue\ndomain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized\nsemantic. MDB feeds diverse dialogue datasets to the model at once to tackle\nthe multi-domain problem by learning the multiple domain knowledge. We\nintroduce a novel method PGT, which employs the Siamese network to fine-tune\nthe model with a clustering method directly.Our model can learn how to cluster\ndialogue utterances by using PGT. Experimental results demonstrate that our\nmulti-view model with MDB and PGT significantly improves the Open Intent\nInduction performance compared to baseline systems.", "published": "2023-03-23 08:30:35", "link": "http://arxiv.org/abs/2303.13099v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fairness-guided Few-shot Prompting for Large Language Models", "abstract": "Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.", "published": "2023-03-23 12:28:25", "link": "http://arxiv.org/abs/2303.13217v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Sparse Retrievers and Rerankers using Adapters", "abstract": "Parameter-Efficient transfer learning with Adapters have been studied in\nNatural Language Processing (NLP) as an alternative to full fine-tuning.\nAdapters are memory-efficient and scale well with downstream tasks by training\nsmall bottle-neck layers added between transformer layers while keeping the\nlarge pretrained language model (PLMs) frozen. In spite of showing promising\nresults in NLP, these methods are under-explored in Information Retrieval.\nWhile previous studies have only experimented with dense retriever or in a\ncross lingual retrieval scenario, in this paper we aim to complete the picture\non the use of adapters in IR. First, we study adapters for SPLADE, a sparse\nretriever, for which adapters not only retain the efficiency and effectiveness\notherwise achieved by finetuning, but are memory-efficient and orders of\nmagnitude lighter to train. We observe that Adapters-SPLADE not only optimizes\njust 2\\% of training parameters, but outperforms fully fine-tuned counterpart\nand existing parameter-efficient dense IR models on IR benchmark datasets.\nSecondly, we address domain adaptation of neural retrieval thanks to adapters\non cross-domain BEIR datasets and TripClick. Finally, we also consider\nknowledge sharing between rerankers and first stage rankers. Overall, our study\ncomplete the examination of adapters for neural IR", "published": "2023-03-23 12:34:30", "link": "http://arxiv.org/abs/2303.13220v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Visual-Language Prompt Tuning with Knowledge-guided Context Optimization", "abstract": "Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n\\emph{i.e.,} achieves better performance with less training time.", "published": "2023-03-23 14:04:23", "link": "http://arxiv.org/abs/2303.13283v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly\n  Knowledge Graph", "abstract": "In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.", "published": "2023-03-23 15:29:21", "link": "http://arxiv.org/abs/2303.13351v3", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Compositional Zero-Shot Domain Transfer with Text-to-Text Models", "abstract": "Label scarcity is a bottleneck for improving task performance in specialised\ndomains. We propose a novel compositional transfer learning framework (DoT5 -\ndomain compositional zero-shot T5) for zero-shot domain transfer. Without\naccess to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of\nunlabelled in-domain free text) and task knowledge (from task training on more\nreadily available general-domain data) in a multi-task manner. To improve the\ntransferability of task training, we design a strategy named NLGU: we\nsimultaneously train NLG for in-domain label-to-data generation which enables\ndata augmentation for self-finetuning and NLU for label prediction. We evaluate\nDoT5 on the biomedical domain and the resource-lean subdomain of radiology,\nfocusing on NLI, text summarisation and embedding learning. DoT5 demonstrates\nthe effectiveness of compositional transfer learning through multi-task\nlearning. In particular, DoT5 outperforms the current SOTA in zero-shot\ntransfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with\nablations and a case study demonstrating its ability to solve challenging NLI\nexamples requiring in-domain expertise.", "published": "2023-03-23 15:58:41", "link": "http://arxiv.org/abs/2303.13386v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoBIT: A Contrastive Bi-directional Image-Text Generation Model", "abstract": "The field of vision and language has witnessed a proliferation of pre-trained\nfoundation models. Most existing methods are independently pre-trained with\ncontrastive objective like CLIP, image-to-text generative objective like PaLI,\nor text-to-image generative objective like Parti. However, the three objectives\ncan be pre-trained on the same data, image-text pairs, and intuitively they\ncomplement each other as contrasting provides global alignment capacity and\ngeneration grants fine-grained understanding. In this work, we present a\nContrastive Bi-directional Image-Text generation model (CoBIT), which attempts\nto unify the three pre-training objectives in one framework. Specifically,\nCoBIT employs a novel unicoder-decoder structure, consisting of an image\nunicoder, a text unicoder and a cross-modal decoder. The image/text unicoders\ncan switch between encoding and decoding in different tasks, enabling\nflexibility and shared knowledge that benefits both image-to-text and\ntext-to-image generations. CoBIT achieves superior performance in image\nunderstanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE)\nand text-based content creation, particularly in zero-shot scenarios. For\ninstance, 82.7% in zero-shot ImageNet classification, 9.37 FID score in\nzero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.", "published": "2023-03-23 17:24:31", "link": "http://arxiv.org/abs/2303.13455v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Return of the RNN: Residual Recurrent Networks for Invertible Sentence\n  Embeddings", "abstract": "This study presents a novel model for invertible sentence embeddings using a\nresidual recurrent network trained on an unsupervised encoding task. Rather\nthan the probabilistic outputs common to neural machine translation models, our\napproach employs a regression-based output layer to reconstruct the input\nsequence's word vectors. The model achieves high accuracy and fast training\nwith the ADAM optimizer, a significant finding given that RNNs typically\nrequire memory units, such as LSTMs, or second-order optimization methods. We\nincorporate residual connections and introduce a \"match drop\" technique, where\ngradients are calculated only for incorrect words. Our approach demonstrates\npotential for various natural language processing applications, particularly in\nneural network-based systems that require high-quality sentence embeddings.", "published": "2023-03-23 15:59:06", "link": "http://arxiv.org/abs/2303.13570v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting Multilingual Large Language Models to Generate Code-Mixed\n  Texts: The Case of South East Asian Languages", "abstract": "While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The recent proliferation of Large\nLanguage Models (LLMs) compels one to ask: how capable are these systems in\ngenerating code-mixed data? In this paper, we explore prompting multilingual\nLLMs in a zero-shot manner to generate code-mixed data for seven languages in\nSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,\nTamil, and Singlish. We find that publicly available multilingual\ninstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of\nproducing texts with phrases or clauses from different languages. ChatGPT\nexhibits inconsistent capabilities in generating code-mixed texts, wherein its\nperformance varies depending on the prompt template and language pairing. For\ninstance, ChatGPT generates fluent and natural Singlish texts (an English-based\ncreole spoken in Singapore), but for English-Tamil language pair, the system\nmostly produces grammatically incorrect or semantically meaningless utterances.\nFurthermore, it may erroneously introduce languages not specified in the\nprompt. Based on our investigation, existing multilingual LLMs exhibit a wide\nrange of proficiency in code-mixed data generation for SEA languages. As such,\nwe advise against using LLMs in this context without extensive human checks.", "published": "2023-03-23 18:16:30", "link": "http://arxiv.org/abs/2303.13592v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT for Shaping the Future of Dentistry: The Potential of\n  Multi-Modal Large Language Model", "abstract": "The ChatGPT, a lite and conversational variant of Generative Pretrained\nTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large\nLanguage Models (LLMs) with billions of parameters. LLMs have stirred up much\ninterest among researchers and practitioners in their impressive skills in\nnatural language processing tasks, which profoundly impact various fields. This\npaper mainly discusses the future applications of LLMs in dentistry. We\nintroduce two primary LLM deployment methods in dentistry, including automated\ndental diagnosis and cross-modal dental diagnosis, and examine their potential\napplications. Especially, equipped with a cross-modal encoder, a single LLM can\nmanage multi-source data and conduct advanced natural language reasoning to\nperform complex clinical operations. We also present cases to demonstrate the\npotential of a fully automatic Multi-Modal LLM AI system for dentistry clinical\napplication. While LLMs offer significant potential benefits, the challenges,\nsuch as data privacy, data quality, and model bias, need further study.\nOverall, LLMs have the potential to revolutionize dental diagnosis and\ntreatment, which indicates a promising avenue for clinical application and\nresearch in dentistry.", "published": "2023-03-23 15:34:26", "link": "http://arxiv.org/abs/2304.03086v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GesGPT: Speech Gesture Synthesis With Text Parsing from ChatGPT", "abstract": "Gesture synthesis has gained significant attention as a critical research\nfield, aiming to produce contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. In this letter, we propose GesGPT, a novel approach to\ngesture generation that leverages the semantic analysis capabilities of large\nlanguage models , such as ChatGPT. By capitalizing on the strengths of LLMs for\ntext analysis, we adopt a controlled approach to generate and integrate\nprofessional gestures and base gestures through a text parsing script,\nresulting in diverse and meaningful gestures. Firstly, our approach involves\nthe development of prompt principles that transform gesture generation into an\nintention classification problem using ChatGPT. We also conduct further\nanalysis on emphasis words and semantic words to aid in gesture generation.\nSubsequently, we construct a specialized gesture lexicon with multiple semantic\nannotations, decoupling the synthesis of gestures into professional gestures\nand base gestures. Finally, we merge the professional gestures with base\ngestures. Experimental results demonstrate that GesGPT effectively generates\ncontextually appropriate and expressive gestures.", "published": "2023-03-23 03:30:30", "link": "http://arxiv.org/abs/2303.13013v3", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SPeC: A Soft Prompt-Based Calibration on Performance Variability of\n  Large Language Model in Clinical Notes Summarization", "abstract": "Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.", "published": "2023-03-23 04:47:46", "link": "http://arxiv.org/abs/2303.13035v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Universal Transformer: block reusing with adaptor in Transformer\n  for automatic speech recognition", "abstract": "Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.", "published": "2023-03-23 06:54:37", "link": "http://arxiv.org/abs/2303.13072v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph\n  Question Answering", "abstract": "In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.", "published": "2023-03-23 14:06:26", "link": "http://arxiv.org/abs/2303.13284v3", "categories": ["cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense", "abstract": "The rise in malicious usage of large language models, such as fake content\ncreation and academic plagiarism, has motivated the development of approaches\nthat identify AI-generated text, including those based on watermarking or\noutlier detection. However, the robustness of these detection algorithms to\nparaphrases of AI-generated text remains unclear. To stress test these\ndetectors, we build a 11B parameter paraphrase generation model (DIPPER) that\ncan paraphrase paragraphs, condition on surrounding context, and control\nlexical diversity and content reordering. Using DIPPER to paraphrase text\ngenerated by three large language models (including GPT3.5-davinci-003)\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection\naccuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of\n1%), without appreciably modifying the input semantics.\n  To increase the robustness of AI-generated text detection to paraphrase\nattacks, we introduce a simple defense that relies on retrieving\nsemantically-similar generations and must be maintained by a language model API\nprovider. Given a candidate text, our algorithm searches a database of\nsequences previously generated by the API, looking for sequences that match the\ncandidate text within a certain threshold. We empirically verify our defense\nusing a database of 15M generations from a fine-tuned T5-XXL model and find\nthat it can detect 80% to 97% of paraphrased generations across different\nsettings while only classifying 1% of human-written sequences as AI-generated.\nWe open-source our models, code and data.", "published": "2023-03-23 16:29:27", "link": "http://arxiv.org/abs/2303.13408v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning and Verification of Task Structure in Instructional Videos", "abstract": "Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.", "published": "2023-03-23 17:59:54", "link": "http://arxiv.org/abs/2303.13519v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Unsupervised Speech Recognition with Diffusion GANs", "abstract": "We enhance the vanilla adversarial training method for unsupervised Automatic\nSpeech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance\nnoises of various intensities to the generator's output and unlabeled reference\ntext which are sampled from pretrained phoneme language models with a length\nconstraint, (2) asks diffusion timestep-dependent discriminators to separate\nthem, and (3) back-propagates the gradients to update the generator.\nWord/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for\ntest-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our\nenhancement strategies work effectively.", "published": "2023-03-23 02:54:00", "link": "http://arxiv.org/abs/2303.13559v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Deliberation-based Joint Acoustic and Text Decoder", "abstract": "We propose a new two-pass E2E speech recognition model that improves ASR\nperformance by training on a combination of paired data and unpaired text data.\nPreviously, the joint acoustic and text decoder (JATD) has shown promising\nresults through the use of text data during model training and the recently\nintroduced deliberation architecture has reduced recognition errors by\nleveraging first-pass decoding results. Our method, dubbed Deliberation-JATD,\ncombines the spelling correcting abilities of deliberation with JATD's use of\nunpaired text data to further improve performance. The proposed model produces\nsubstantial gains across multiple test sets, especially those focused on rare\nwords, where it reduces word error rate (WER) by between 12% and 22.5%\nrelative. This is done without increasing model size or requiring multi-stage\ntraining, making Deliberation-JATD an efficient candidate for on-device\napplications.", "published": "2023-03-23 18:02:23", "link": "http://arxiv.org/abs/2303.15293v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Judicial Intelligent Assistant System: Extracting Events from Divorce\n  Cases to Detect Disputes for the Judge", "abstract": "In formal procedure of civil cases, the textual materials provided by\ndifferent parties describe the development process of the cases. It is a\ndifficult but necessary task to extract the key information for the cases from\nthese textual materials and to clarify the dispute focus of related parties.\nCurrently, officers read the materials manually and use methods, such as\nkeyword searching and regular matching, to get the target information. These\napproaches are time-consuming and heavily depending on prior knowledge and\ncarefulness of the officers. To assist the officers to enhance working\nefficiency and accuracy, we propose an approach to detect disputes from divorce\ncases based on a two-round-labeling event extracting technique in this paper.\nWe implement the Judicial Intelligent Assistant (JIA) system according to the\nproposed approach to 1) automatically extract focus events from divorce case\nmaterials, 2) align events by identifying co-reference among them, and 3)\ndetect conflicts among events brought by the plaintiff and the defendant. With\nthe JIA system, it is convenient for judges to determine the disputed issues.\nExperimental results demonstrate that the proposed approach and system can\nobtain the focus of cases and detect conflicts more effectively and efficiently\ncomparing with existing method.", "published": "2023-03-23 08:58:49", "link": "http://arxiv.org/abs/2303.16751v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Simple Explanation for the Phase Transition in Large Language Models\n  with List Decoding", "abstract": "Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.", "published": "2023-03-23 09:00:07", "link": "http://arxiv.org/abs/2303.13112v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LMCodec: A Low Bitrate Speech Codec With Causal Transformer Models", "abstract": "We introduce LMCodec, a causal neural speech codec that provides high quality\naudio at very low bitrates. The backbone of the system is a causal\nconvolutional codec that encodes audio into a hierarchy of coarse-to-fine\ntokens using residual vector quantization. LMCodec trains a Transformer\nlanguage model to predict the fine tokens from the coarse ones in a generative\nfashion, allowing for the transmission of fewer codes. A second Transformer\npredicts the uncertainty of the next codes given the past transmitted codes,\nand is used to perform conditional entropy coding. A MUSHRA subjective test was\nconducted and shows that the quality is comparable to reference codecs at\nhigher bitrates. Example audio is available at\nhttps://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec.", "published": "2023-03-23 01:27:38", "link": "http://arxiv.org/abs/2303.12984v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Weighted Pressure and Mode Matching for Sound Field Reproduction:\n  Theoretical and Experimental Comparisons", "abstract": "Two sound field reproduction methods, weighted pressure matching and weighted\nmode matching, are theoretically and experimentally compared. The weighted\npressure and mode matching are a generalization of conventional pressure and\nmode matching, respectively. Both methods are derived by introducing a\nweighting matrix in the pressure and mode matching. The weighting matrix in the\nweighted pressure matching is defined on the basis of the kernel interpolation\nof the sound field from pressure at a discrete set of control points. In the\nweighted mode matching, the weighting matrix is defined by a regional\nintegration of spherical wavefunctions. It is theoretically shown that the\nweighted pressure matching is a special case of the weighted mode matching by\ninfinite-dimensional harmonic analysis for estimating expansion coefficients\nfrom pressure observations. The difference between the two methods are\ndiscussed through experiments.", "published": "2023-03-23 04:26:06", "link": "http://arxiv.org/abs/2303.13027v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pyramid Multi-branch Fusion DCNN with Multi-Head Self-Attention for\n  Mandarin Speech Recognition", "abstract": "As one of the major branches of automatic speech recognition, attention-based\nmodels greatly improves the feature representation ability of the model. In\nparticular, the multi-head mechanism is employed in the attention, hoping to\nlearn speech features of more aspects in different attention subspaces. For\nspeech recognition of complex languages, on the one hand, a small head size\nwill lead to an obvious shortage of learnable aspects. On the other hand, we\nneed to reduce the dimension of each subspace to keep the size of the overall\nfeature space unchanged when we increase the number of heads, which will\nsignificantly weaken the ability to represent the feature of each subspace.\nTherefore, this paper explores how to use a small attention subspace to\nrepresent complete speech features while ensuring many heads. In this work we\npropose a novel neural network architecture, namely, pyramid multi-branch\nfusion DCNN with multi-head self-attention. The proposed architecture is\ninspired by Dilated Convolution Neural Networks (DCNN), it uses multiple\nbranches with DCNN to extract the feature of the input speech under different\nreceptive fields. To reduce the number of parameters, every two branches are\nmerged until all the branches are merged into one. Thus, its shape is like a\npyramid rotated 90 degrees. We demonstrate that on Aishell-1, a widely used\nMandarin speech dataset, our model achieves a character error rate (CER) of\n6.45% on the test sets.", "published": "2023-03-23 13:18:54", "link": "http://arxiv.org/abs/2303.13243v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptive Endpointing with Deep Contextual Multi-armed Bandits", "abstract": "Current endpointing (EP) solutions learn in a supervised framework, which\ndoes not allow the model to incorporate feedback and improve in an online\nsetting. Also, it is a common practice to utilize costly grid-search to find\nthe best configuration for an endpointing model. In this paper, we aim to\nprovide a solution for adaptive endpointing by proposing an efficient method\nfor choosing an optimal endpointing configuration given utterance-level audio\nfeatures in an online setting, while avoiding hyperparameter grid-search. Our\nmethod does not require ground truth labels, and only uses online learning from\nreward signals without requiring annotated labels. Specifically, we propose a\ndeep contextual multi-armed bandit-based approach, which combines the\nrepresentational power of neural networks with the action exploration behavior\nof Thompson modeling algorithms. We compare our approach to several baselines,\nand show that our deep bandit models also succeed in reducing early cutoff\nerrors while maintaining low latency.", "published": "2023-03-23 16:28:26", "link": "http://arxiv.org/abs/2303.13407v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Better Together: Dialogue Separation and Voice Activity Detection for\n  Audio Personalization in TV", "abstract": "In TV services, dialogue level personalization is key to meeting user\npreferences and needs. When dialogue and background sounds are not separately\navailable from the production stage, Dialogue Separation (DS) can estimate them\nto enable personalization. DS was shown to provide clear benefits for the end\nuser. Still, the estimated signals are not perfect, and some leakage can be\nintroduced. This is undesired, especially during passages without dialogue. We\npropose to combine DS and Voice Activity Detection (VAD), both recently\nproposed for TV audio. When their combination suggests dialogue inactivity,\nbackground components leaking in the dialogue estimate are reassigned to the\nbackground estimate. A clear improvement of the audio quality is shown for\ndialogue-free signals, without performance drops when dialogue is active. A\npost-processed VAD estimate with improved detection accuracy is also generated.\nIt is concluded that DS and VAD can improve each other and are better used\ntogether.", "published": "2023-03-23 17:19:49", "link": "http://arxiv.org/abs/2303.13453v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention-based Speech Enhancement Using Human Quality Perception\n  Modelling", "abstract": "Perceptually-inspired objective functions such as the perceptual evaluation\nof speech quality (PESQ), signal-to-distortion ratio (SDR), and short-time\nobjective intelligibility (STOI), have recently been used to optimize\nperformance of deep-learning-based speech enhancement algorithms. These\nobjective functions, however, do not always strongly correlate with a\nlistener's assessment of perceptual quality, so optimizing with these measures\noften results in poorer performance in real-world scenarios. In this work, we\npropose an attention-based enhancement approach that uses learned speech\nembedding vectors from a mean-opinion score (MOS) prediction model and a speech\nenhancement module to jointly enhance noisy speech. The MOS prediction model\nestimates the perceptual MOS of speech quality, as assessed by human listeners,\ndirectly from the audio signal. The enhancement module also employs a quantized\nlanguage model that enforces spectral constraints for better speech realism and\nperformance. We train the model using real-world noisy speech data that has\nbeen captured in everyday environments and test it using unseen corpora. The\nresults show that our proposed approach significantly outperforms other\napproaches that are optimized with objective measures, where the predicted\nquality scores strongly correlate with human judgments.", "published": "2023-03-23 21:32:53", "link": "http://arxiv.org/abs/2303.13685v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale\n  Network and Self-Attention Mechanism", "abstract": "Instrument playing technique (IPT) is a key element of musical presentation.\nHowever, most of the existing works for IPT detection only concern monophonic\nmusic signals, yet little has been done to detect IPTs in polyphonic\ninstrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we\nformulate it as a frame-level multi-label classification problem and apply it\nto Guzheng, a Chinese plucked string instrument. We create a new dataset,\nGuzheng\\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT\nannotations of each note. Because different IPTs vary a lot in their lengths,\nwe propose a new method to solve this problem using multi-scale network and\nself-attention. The multi-scale network extracts features from different\nscales, and the self-attention mechanism applied to the feature maps at the\ncoarsest scale further enhances the long-range feature extraction. Our approach\noutperforms existing works by a large margin, indicating its effectiveness in\nIPT detection.", "published": "2023-03-23 13:52:42", "link": "http://arxiv.org/abs/2303.13272v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Egocentric Audio-Visual Object Localization", "abstract": "Humans naturally perceive surrounding scenes by unifying sound and sight in a\nfirst-person view. Likewise, machines are advanced to approach human\nintelligence by learning with multisensory inputs from an egocentric\nperspective. In this paper, we explore the challenging egocentric audio-visual\nobject localization task and observe that 1) egomotion commonly exists in\nfirst-person recordings, even within a short duration; 2) The out-of-view sound\ncomponents can be created while wearers shift their attention. To address the\nfirst problem, we propose a geometry-aware temporal aggregation module to\nhandle the egomotion explicitly. The effect of egomotion is mitigated by\nestimating the temporal geometry transformation and exploiting it to update\nvisual representations. Moreover, we propose a cascaded feature enhancement\nmodule to tackle the second issue. It improves cross-modal localization\nrobustness by disentangling visually-indicated audio representation. During\ntraining, we take advantage of the naturally available audio-visual temporal\nsynchronization as the ``free'' self-supervision to avoid costly labeling. We\nalso annotate and create the Epic Sounding Object dataset for evaluation\npurposes. Extensive experiments show that our method achieves state-of-the-art\nlocalization performance in egocentric videos and can be generalized to diverse\naudio-visual scenes.", "published": "2023-03-23 17:43:11", "link": "http://arxiv.org/abs/2303.13471v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and\n  Enhancement in Generative AI", "abstract": "Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.", "published": "2023-03-23 15:17:15", "link": "http://arxiv.org/abs/2303.13336v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
