{"title": "Determination of the Internet Anonymity Influence on the Level of\n  Aggression and Usage of Obscene Lexis", "abstract": "This article deals with the analysis of the semantic content of the anonymous\nRussian-speaking forum 2ch.hk, different verbal means of expressing of the\nemotional state of aggression are revealed for this site, and aggression is\nclassified by its directions. The lexis of different Russian-and English-\nspeaking anonymous forums (2ch.hk and iichan.hk, 4chan.org) and public\ncommunity \"MDK\" of the Russian-speaking social network VK is analyzed and\ncompared with the Open Corpus of the Russian language (Opencorpora.org and\nBrown corpus). The analysis shows that anonymity has no influence on the amount\nof invective items usage. The effectiveness of moderation was shown for\nanonymous forums. It was established that Russian obscene lexis was used to\nexpress the emotional state of aggression only in 60.4% of cases for 2ch.hk.\nThese preliminary results show that the Russian obscene lexis on the Internet\ndoes not have direct dependence on the emotional state of aggression.", "published": "2015-10-01 14:01:32", "link": "http://arxiv.org/abs/1510.00240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and\n  G\u00f3mez-Rodr\u00edguez (2015) on Dependency Length Minimization", "abstract": "We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and\nG\\'omez-Rodr\\'iguez, 2015) of our work on empirical evidence of dependency\nlength minimization across languages (Futrell et al., 2015). First, we\nacknowledge error in failing to acknowledge Liu (2008)'s previous work on\ncorpora of 20 languages with similar aims. A correction will appear in PNAS.\nNevertheless, we argue that our work provides novel, strong evidence for\ndependency length minimization as a universal quantitative property of\nlanguages, beyond this previous work, because it provides baselines which focus\non word order preferences. Second, we argue that our choices of baselines were\nappropriate because they control for alternative theories.", "published": "2015-10-01 22:09:34", "link": "http://arxiv.org/abs/1510.00436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RDF Knowledge Graph Visualization From a Knowledge Extraction System", "abstract": "In this paper, we present a system to visualize RDF knowledge graphs. These\ngraphs are obtained from a knowledge extraction system designed by\nGEOLSemantics. This extraction is performed using natural language processing\nand trigger detection. The user can visualize subgraphs by selecting some\nontology features like concepts or individuals. The system is also\nmultilingual, with the use of the annotated ontology in English, French, Arabic\nand Chinese.", "published": "2015-10-01 14:10:25", "link": "http://arxiv.org/abs/1510.00244v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Generative Model of Words and Relationships from Multiple Sources", "abstract": "Neural language models are a powerful tool to embed words into semantic\nvector spaces. However, learning such models generally relies on the\navailability of abundant and diverse training examples. In highly specialised\ndomains this requirement may not be met due to difficulties in obtaining a\nlarge corpus, or the limited range of expression in average use. Such domains\nmay encode prior knowledge about entities in a knowledge base or ontology. We\npropose a generative model which integrates evidence from diverse data sources,\nenabling the sharing of semantic information. We achieve this by generalising\nthe concept of co-occurrence from distributional semantics to include other\nrelationships between entities or words, which we model as affine\ntransformations on the embedding space. We demonstrate the effectiveness of\nthis approach by outperforming recent models on a link prediction task and\ndemonstrating its ability to profit from partially or fully unobserved data\ntraining labels. We further demonstrate the usefulness of learning from\ndifferent data sources with overlapping vocabularies.", "published": "2015-10-01 14:42:19", "link": "http://arxiv.org/abs/1510.00259v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Similarity of symbol frequency distributions with heavy tails", "abstract": "Quantifying the similarity between symbolic sequences is a traditional\nproblem in Information Theory which requires comparing the frequencies of\nsymbols in different sequences. In numerous modern applications, ranging from\nDNA over music to texts, the distribution of symbol frequencies is\ncharacterized by heavy-tailed distributions (e.g., Zipf's law). The large\nnumber of low-frequency symbols in these distributions poses major difficulties\nto the estimation of the similarity between sequences, e.g., they hinder an\naccurate finite-size estimation of entropies. Here we show analytically how the\nsystematic (bias) and statistical (fluctuations) errors in these estimations\ndepend on the sample size~$N$ and on the exponent~$\\gamma$ of the heavy-tailed\ndistribution. Our results are valid for the Shannon entropy $(\\alpha=1)$, its\ncorresponding similarity measures (e.g., the Jensen-Shanon divergence), and\nalso for measures based on the generalized entropy of order $\\alpha$. For small\n$\\alpha$'s, including $\\alpha=1$, the errors decay slower than the $1/N$-decay\nobserved in short-tailed distributions. For $\\alpha$ larger than a critical\nvalue $\\alpha^* = 1+1/\\gamma \\leq 2$, the $1/N$-decay is recovered. We show the\npractical significance of our results by quantifying the evolution of the\nEnglish language over the last two centuries using a complete $\\alpha$-spectrum\nof measures. We find that frequent words change more slowly than less frequent\nwords and that $\\alpha=2$ provides the most robust measure to quantify language\nchange.", "published": "2015-10-01 15:10:09", "link": "http://arxiv.org/abs/1510.00277v2", "categories": ["physics.soc-ph", "cs.CL", "physics.data-an"], "primary_category": "physics.soc-ph"}
