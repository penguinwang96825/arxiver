{"title": "Multitask Learning for Emotion and Personality Detection", "abstract": "In recent years, deep learning-based automated personality trait detection\nhas received a lot of attention, especially now, due to the massive digital\nfootprints of an individual. Moreover, many researchers have demonstrated that\nthere is a strong link between personality traits and emotions. In this paper,\nwe build on the known correlation between personality traits and emotional\nbehaviors, and propose a novel multitask learning framework, SoGMTL that\nsimultaneously predicts both of them. We also empirically evaluate and discuss\ndifferent information-sharing mechanisms between the two tasks. To ensure the\nhigh quality of the learning process, we adopt a MAML-like framework for model\noptimization. Our more computationally efficient CNN-based multitask model\nachieves the state-of-the-art performance across multiple famous personality\nand emotion datasets, even outperforming Language Model based models.", "published": "2021-01-07 03:09:55", "link": "http://arxiv.org/abs/2101.02346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking", "abstract": "Entity linking (EL) for the rapidly growing short text (e.g. search queries\nand news titles) is critical to industrial applications. Most existing\napproaches relying on adequate context for long text EL are not effective for\nthe concise and sparse short text. In this paper, we propose a novel framework\ncalled Multi-turn Multiple-choice Machine reading comprehension (M3}) to solve\nthe short text EL from a new perspective: a query is generated for each\nambiguous mention exploiting its surrounding context, and an option selection\nmodule is employed to identify the golden entity from candidates using the\nquery. In this way, M3 framework sufficiently interacts limited context with\ncandidate entities during the encoding process, as well as implicitly considers\nthe dissimilarities inside the candidate bunch in the selection stage. In\naddition, we design a two-stage verifier incorporated into M3 to address the\ncommonly existed unlinkable problem in short text. To further consider the\ntopical coherence and interdependence among referred entities, M3 leverages a\nmulti-turn fashion to deal with mentions in a sequence manner by retrospecting\nhistorical cues. Evaluation shows that our M3 framework achieves the\nstate-of-the-art performance on five Chinese and English datasets for the\nreal-world short text EL.", "published": "2021-01-07 06:17:15", "link": "http://arxiv.org/abs/2101.02394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ask2Transformers: Zero-Shot Domain labelling with Pre-trained Language\n  Models", "abstract": "In this paper we present a system that exploits different pre-trained\nLanguage Models for assigning domain labels to WordNet synsets without any kind\nof supervision. Furthermore, the system is not restricted to use a particular\nset of domain labels. We exploit the knowledge encoded within different\noff-the-shelf pre-trained Language Models and task formulations to infer the\ndomain label of a particular WordNet definition. The proposed zero-shot system\nachieves a new state-of-the-art on the English dataset used in the evaluation.", "published": "2021-01-07 18:04:39", "link": "http://arxiv.org/abs/2101.02661v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Detection Engine for Multilingual Texting on Mobile Devices", "abstract": "More than 2 billion mobile users worldwide type in multiple languages in the\nsoft keyboard. On a monolingual keyboard, 38% of falsely auto-corrected words\nare valid in another language. This can be easily avoided by detecting the\nlanguage of typed words and then validating it in its respective language.\nLanguage detection is a well-known problem in natural language processing. In\nthis paper, we present a fast, light-weight and accurate Language Detection\nEngine (LDE) for multilingual typing that dynamically adapts to user intended\nlanguage in real-time. We propose a novel approach where the fusion of\ncharacter N-gram model and logistic regression based selector model is used to\nidentify the language. Additionally, we present a unique method of reducing the\ninference time significantly by parameter reduction technique. We also discuss\nvarious optimizations fabricated across LDE to resolve ambiguity in input text\namong the languages with the same character pattern. Our method demonstrates an\naverage accuracy of 94.5% for Indian languages in Latin script and that of 98%\nfor European languages on the code-switched data. This model outperforms\nfastText by 60.39% and ML-Kit by 23.67% in F1 score for European languages. LDE\nis faster on mobile device with an average inference time of 25.91\nmicroseconds.", "published": "2021-01-07 16:49:47", "link": "http://arxiv.org/abs/2101.03963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Real-Time Optimized N-gram For Mobile Devices", "abstract": "With the increasing number of mobile devices, there has been continuous\nresearch on generating optimized Language Models (LMs) for soft keyboard. In\nspite of advances in this domain, building a single LM for low-end feature\nphones as well as high-end smartphones is still a pressing need. Hence, we\npropose a novel technique, Optimized N-gram (Op-Ngram), an end-to-end N-gram\npipeline that utilises mobile resources efficiently for faster Word Completion\n(WC) and Next Word Prediction (NWP). Op-Ngram applies Stupid Backoff and\npruning strategies to generate a light-weight model. The LM loading time on\nmobile is linear with respect to model size. We observed that Op-Ngram gives\n37% improvement in Language Model (LM)-ROM size, 76% in LM-RAM size, 88% in\nloading time and 89% in average suggestion time as compared to SORTED array\nvariant of BerkeleyLM. Moreover, our method shows significant performance\nimprovement over KenLM as well.", "published": "2021-01-07 14:51:26", "link": "http://arxiv.org/abs/2101.03967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying Transfer Learning for Improving Domain-Specific Search\n  Experience Using Query to Question Similarity", "abstract": "Search is one of the most common platforms used to seek information. However,\nusers mostly get overloaded with results whenever they use such a platform to\nresolve their queries. Nowadays, direct answers to queries are being provided\nas a part of the search experience. The question-answer (QA) retrieval process\nplays a significant role in enriching the search experience. Most off-the-shelf\nSemantic Textual Similarity models work fine for well-formed search queries,\nbut their performances degrade when applied to a domain-specific setting having\nincomplete or grammatically ill-formed search queries in prevalence. In this\npaper, we discuss a framework for calculating similarities between a given\ninput query and a set of predefined questions to retrieve the question which\nmatches to it the most. We have used it for the financial domain, but the\nframework is generalized for any domain-specific search engine and can be used\nin other domains as well. We use Siamese network [6] over Long Short-Term\nMemory (LSTM) [3] models to train a classifier which generates unnormalized and\nnormalized similarity scores for a given pair of questions. Moreover, for each\nof these question pairs, we calculate three other similarity scores: cosine\nsimilarity between their average word2vec embeddings [15], cosine similarity\nbetween their sentence embeddings [7] generated using RoBERTa [17] and their\ncustomized fuzzy-match score. Finally, we develop a metaclassifier using\nSupport Vector Machines [19] for combining these five scores to detect if a\ngiven pair of questions is similar. We benchmark our model's performance\nagainst existing State Of The Art (SOTA) models on Quora Question Pairs (QQP)\ndataset as well as a dataset specific to the financial domain.", "published": "2021-01-07 03:27:32", "link": "http://arxiv.org/abs/2101.02351v1", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "M\u00f6biusE: Knowledge Graph Embedding on M\u00f6bius Ring", "abstract": "In this work, we propose a novel Knowledge Graph Embedding (KGE) strategy,\ncalled M\\\"{o}biusE, in which the entities and relations are embedded to the\nsurface of a M\\\"{o}bius ring. The proposition of such a strategy is inspired by\nthe classic TorusE, in which the addition of two arbitrary elements is subject\nto a modulus operation. In this sense, TorusE naturally guarantees the critical\nboundedness of embedding vectors in KGE. However, the nonlinear property of\naddition operation on Torus ring is uniquely derived by the modulus operation,\nwhich in some extent restricts the expressiveness of TorusE. As a further\ngeneralization of TorusE, M\\\"{o}biusE also uses modulus operation to preserve\nthe closeness of addition operation on it, but the coordinates on M\\\"{o}bius\nring interacts with each other in the following way: {\\em \\color{red} any\nvector on the surface of a M\\\"{o}bius ring moves along its parametric trace\nwill goes to the right opposite direction after a cycle}. Hence, M\\\"{o}biusE\nassumes much more nonlinear representativeness than that of TorusE, and in turn\nit generates much more precise embedding results. In our experiments,\nM\\\"{o}biusE outperforms TorusE and other classic embedding strategies in\nseveral key indicators.", "published": "2021-01-07 03:35:06", "link": "http://arxiv.org/abs/2101.02352v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Homonym Identification using BERT -- Using a Clustering Approach", "abstract": "Homonym identification is important for WSD that require coarse-grained\npartitions of senses. The goal of this project is to determine whether\ncontextual information is sufficient for identifying a homonymous word. To\ncapture the context, BERT embeddings are used as opposed to Word2Vec, which\nconflates senses into one vector. SemCor is leveraged to retrieve the\nembeddings. Various clustering algorithms are applied to the embeddings.\nFinally, the embeddings are visualized in a lower-dimensional space to\nunderstand the feasibility of the clustering process.", "published": "2021-01-07 06:26:59", "link": "http://arxiv.org/abs/2101.02398v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplified DOM Trees for Transferable Attribute Extraction from the Web", "abstract": "There has been a steady need to precisely extract structured knowledge from\nthe web (i.e. HTML documents). Given a web page, extracting a structured object\nalong with various attributes of interest (e.g. price, publisher, author, and\ngenre for a book) can facilitate a variety of downstream applications such as\nlarge-scale knowledge base construction, e-commerce product search, and\npersonalized recommendation. Considering each web page is rendered from an HTML\nDOM tree, existing approaches formulate the problem as a DOM tree node tagging\ntask. However, they either rely on computationally expensive visual feature\nengineering or are incapable of modeling the relationship among the tree nodes.\nIn this paper, we propose a novel transferable method, Simplified DOM Trees for\nAttribute Extraction (SimpDOM), to tackle the problem by efficiently retrieving\nuseful context for each node by leveraging the tree structure. We study two\nchallenging experimental settings: (i) intra-vertical few-shot extraction, and\n(ii) cross-vertical fewshot extraction with out-of-domain knowledge, to\nevaluate our approach. Extensive experiments on the SWDE public dataset show\nthat SimpDOM outperforms the state-of-the-art (SOTA) method by 1.44% on the F1\nscore. We also find that utilizing knowledge from a different vertical\n(cross-vertical extraction) is surprisingly useful and helps beat the SOTA by a\nfurther 1.37%.", "published": "2021-01-07 07:41:55", "link": "http://arxiv.org/abs/2101.02415v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News\n  Detection in English", "abstract": "In this paper, we describe our system for the AAAI 2021 shared task of\nCOVID-19 Fake News Detection in English, where we achieved the 3rd position\nwith the weighted F1 score of 0.9859 on the test set. Specifically, we proposed\nan ensemble method of different pre-trained language models such as BERT,\nRoberta, Ernie, etc. with various training strategies including\nwarm-up,learning rate schedule and k-fold cross-validation. We also conduct an\nextensive analysis of the samples that are not correctly classified. The code\nis available\nat:https://github.com/archersama/3rd-solution-COVID19-Fake-News-Detection-in-English.", "published": "2021-01-07 04:01:13", "link": "http://arxiv.org/abs/2101.02359v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a Smart Data Processing and Storage Model", "abstract": "In several domains it is crucial to store and manipulate data whose origin\nneeds to be completely traceable to guarantee the consistency, trustworthiness\nand reliability on the data itself typically for ethical and legal reasons. It\nis also important to guarantee that such properties are also carried further\nwhen such data is composed and processed into new data. In this article we\npresent the main requirements and theorethical problems that arise by the\ndesign of a system supporting data with such capabilities. We present an\narchitecture for implementing a system as well as a prototype developed in\nPharo.", "published": "2021-01-07 12:52:11", "link": "http://arxiv.org/abs/2101.02522v1", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Off-Line Arabic Handwritten Words Segmentation using Morphological\n  Operators", "abstract": "The main aim of this study is the assessment and discussion of a model for\nhand-written Arabic through segmentation. The framework is proposed based on\nthree steps: pre-processing, segmentation, and evaluation. In the\npre-processing step, morphological operators are applied for Connecting Gaps\n(CGs) in written words. Gaps happen when pen lifting-off during writing,\nscanning documents, or while converting images to binary type. In the\nsegmentation step, first removed the small diacritics then bounded a connected\ncomponent to segment offline words. Huge data was utilized in the proposed\nmodel for applying a variety of handwriting styles so that to be more\ncompatible with real-life applications. Consequently, on the automatic\nevaluation stage, selected randomly 1,131 images from the IESK-ArDB database,\nand then segmented into sub-words. After small gaps been connected, the model\nperformance evaluation had been reached 88% against the standard ground truth\nof the database. The proposed model achieved the highest accuracy when compared\nwith the related works.", "published": "2021-01-07 23:38:53", "link": "http://arxiv.org/abs/2101.02797v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Attention-based multi-task learning for speech-enhancement and\n  speaker-identification in multi-speaker dialogue scenario", "abstract": "Multi-task learning (MTL) and attention mechanism have been proven to\neffectively extract robust acoustic features for various speech-related tasks\nin noisy environments. In this study, we propose an attention-based MTL (ATM)\napproach that integrates MTL and the attention-weighting mechanism to\nsimultaneously realize a multi-model learning structure that performs speech\nenhancement (SE) and speaker identification (SI). The proposed ATM system\nconsists of three parts: SE, SI, and attention-Net (AttNet). The SE part is\ncomposed of a long-short-term memory (LSTM) model, and a deep neural network\n(DNN) model is used to develop the SI and AttNet parts. The overall ATM system\nfirst extracts the representative features and then enhances the speech signals\nin LSTM-SE and specifies speaker identity in DNN-SI. The AttNet computes\nweights based on DNN-SI to prepare better representative features for LSTM-SE.\nWe tested the proposed ATM system on Taiwan Mandarin hearing in noise test\nsentences. The evaluation results confirmed that the proposed system can\neffectively enhance speech quality and intelligibility of a given noisy input.\nMoreover, the accuracy of the SI can also be notably improved by using the\nproposed ATM system.", "published": "2021-01-07 14:27:00", "link": "http://arxiv.org/abs/2101.02550v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Compound Word Transformer: Learning to Compose Full-Song Music over\n  Dynamic Directed Hypergraphs", "abstract": "To apply neural sequence models such as the Transformers to music generation\ntasks, one has to represent a piece of music by a sequence of tokens drawn from\na finite set of pre-defined vocabulary. Such a vocabulary usually involves\ntokens of various types. For example, to describe a musical note, one needs\nseparate tokens to indicate the note's pitch, duration, velocity (dynamics),\nand placement (onset time) along the time grid. While different types of tokens\nmay possess different properties, existing models usually treat them equally,\nin the same way as modeling words in natural languages. In this paper, we\npresent a conceptually different approach that explicitly takes into account\nthe type of the tokens, such as note types and metric types. And, we propose a\nnew Transformer decoder architecture that uses different feed-forward heads to\nmodel tokens of different types. With an expansion-compression trick, we\nconvert a piece of music to a sequence of compound words by grouping\nneighboring tokens, greatly reducing the length of the token sequences. We show\nthat the resulting model can be viewed as a learner over dynamic directed\nhypergraphs. And, we employ it to learn to compose expressive Pop piano music\nof full-song length (involving up to 10K individual tokens per song), both\nconditionally and unconditionally. Our experiment shows that, compared to\nstate-of-the-art models, the proposed model converges 5--10 times faster at\ntraining (i.e., within a day on a single GPU with 11 GB memory), and with\ncomparable quality in the generated music.", "published": "2021-01-07 06:57:34", "link": "http://arxiv.org/abs/2101.02402v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-2-End COVID-19 Detection from Breath & Cough Audio", "abstract": "Our main contributions are as follows: (I) We demonstrate the first attempt\nto diagnose COVID-19 using end-to-end deep learning from a crowd-sourced\ndataset of audio samples, achieving ROC-AUC of 0.846; (II) Our model, the\nCOVID-19 Identification ResNet, (CIdeR), has potential for rapid scalability,\nminimal cost and improving performance as more data becomes available. This\ncould enable regular COVID-19 testing at apopulation scale; (III) We introduce\na novel modelling strategy using a custom deep neural network to diagnose\nCOVID-19 from a joint breath and cough representation; (IV) We release our four\nstratified folds for cross parameter optimisation and validation on a standard\npublic corpus and details on the models for reproducibility and future\nreference.", "published": "2021-01-07 01:13:00", "link": "http://arxiv.org/abs/2102.08359v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T11", "I.2; I.5; J.3"], "primary_category": "cs.SD"}
