{"title": "Term Rewriting Based On Set Automaton Matching", "abstract": "In this article we investigate how a subterm pattern matching algorithm can\nbe exploited to implement efficient term rewriting procedures. From the\nleft-hand sides of the rewrite system we construct a set automaton, which can\nbe used to find all redexes in a term efficiently. We formally describe a\nprocedure that, given a rewrite strategy, interleaves pattern matching steps\nand rewriting steps and thus smoothly integrates redex discovery and subterm\nreplacement. We then present an efficient implementation that instantiates this\nprocedure with outermost rewriting, and present the results of some\nexperiments. Our implementation shows to be competitive with comparable tools.", "published": "2022-02-17 14:41:51", "link": "http://arxiv.org/abs/2202.08687v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models", "abstract": "With the increasing of model capacity brought by pre-trained language models,\nthere emerges boosting needs for more knowledgeable natural language processing\n(NLP) models with advanced functionalities including providing and making\nflexible use of encyclopedic and commonsense knowledge. The mere pre-trained\nlanguage models, however, lack the capacity of handling such\nknowledge-intensive NLP tasks alone. To address this challenge, large numbers\nof pre-trained language models augmented with external knowledge sources are\nproposed and in rapid development. In this paper, we aim to summarize the\ncurrent progress of pre-trained language model-based knowledge-enhanced models\n(PLMKEs) by dissecting their three vital elements: knowledge sources,\nknowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we\npresent the challenges of PLMKEs based on the discussion regarding the three\nelements and attempt to provide NLP practitioners with potential directions for\nfurther research.", "published": "2022-02-17 17:17:43", "link": "http://arxiv.org/abs/2202.08772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "cosFormer: Rethinking Softmax in Attention", "abstract": "Transformer has shown great successes in natural language processing,\ncomputer vision, and audio processing. As one of its core components, the\nsoftmax attention helps to capture long-range dependencies yet prohibits its\nscale-up due to the quadratic space and time complexity to the sequence length.\nKernel methods are often adopted to reduce the complexity by approximating the\nsoftmax operator. Nevertheless, due to the approximation errors, their\nperformances vary in different tasks/corpus and suffer crucial performance\ndrops when compared with the vanilla softmax attention. In this paper, we\npropose a linear transformer called cosFormer that can achieve comparable or\nbetter accuracy to the vanilla transformer in both casual and cross attentions.\ncosFormer is based on two key properties of softmax attention: i).\nnon-negativeness of the attention matrix; ii). a non-linear re-weighting scheme\nthat can concentrate the distribution of the attention matrix. As its linear\nsubstitute, cosFormer fulfills these properties with a linear operator and a\ncosine-based distance re-weighting mechanism. Extensive experiments on language\nmodeling and text understanding tasks demonstrate the effectiveness of our\nmethod. We further examine our method on long sequences and achieve\nstate-of-the-art performance on the Long-Range Arena benchmark. The source code\nis available at https://github.com/OpenNLPLab/cosFormer.", "published": "2022-02-17 17:53:48", "link": "http://arxiv.org/abs/2202.08791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Training for Back-Translation with Categorical\n  Reparameterization Trick", "abstract": "Back-translation (BT) is an effective semi-supervised learning framework in\nneural machine translation (NMT). A pre-trained NMT model translates\nmonolingual sentences and makes synthetic bilingual sentence pairs for the\ntraining of the other NMT model, and vice versa. Understanding the two NMT\nmodels as inference and generation models, respectively, the training method of\nvariational auto-encoder (VAE) was applied in previous works, which is a\nmainstream framework of generative models. However, the discrete property of\ntranslated sentences prevents gradient information from flowing between the two\nNMT models. In this paper, we propose the categorical reparameterization trick\n(CRT) that makes NMT models generate differentiable sentences so that the VAE's\ntraining framework can work in an end-to-end fashion. Our BT experiment\nconducted on a WMT benchmark dataset demonstrates the superiority of our\nproposed CRT compared to the Gumbel-softmax trick, which is a popular\nreparameterization method for categorical variable. Moreover, our experiments\nconducted on multiple WMT benchmark datasets demonstrate that our proposed\nend-to-end training framework is effective in terms of BLEU scores not only\ncompared to its counterpart baseline which is not trained in an end-to-end\nfashion, but also compared to other previous BT works. The code is available at\nthe web.", "published": "2022-02-17 06:31:03", "link": "http://arxiv.org/abs/2202.08465v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Evaluation Metrics for Paraphrase Generation", "abstract": "In this paper we revisit automatic metrics for paraphrase evaluation and\nobtain two findings that disobey conventional wisdom: (1) Reference-free\nmetrics achieve better performance than their reference-based counterparts. (2)\nMost commonly used metrics do not align well with human annotation. Underlying\nreasons behind the above findings are explored through additional experiments\nand in-depth analyses. Based on the experiments and analyses, we propose\nParaScore, a new evaluation metric for paraphrase generation. It possesses the\nmerits of reference-based and reference-free metrics and explicitly models\nlexical divergence. Experimental results demonstrate that ParaScore\nsignificantly outperforms existing metrics.", "published": "2022-02-17 07:18:54", "link": "http://arxiv.org/abs/2202.08479v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AISHELL-NER: Named Entity Recognition from Chinese Speech", "abstract": "Named Entity Recognition (NER) from speech is among Spoken Language\nUnderstanding (SLU) tasks, aiming to extract semantic information from the\nspeech signal. NER from speech is usually made through a two-step pipeline that\nconsists of (1) processing the audio using an Automatic Speech Recognition\n(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works\nhave shown the capability of the End-to-End (E2E) approach for NER from English\nand French speech, which is essentially entity-aware ASR. However, due to the\nmany homophones and polyphones that exist in Chinese, NER from Chinese speech\nis effectively a more challenging task. In this paper, we introduce a new\ndataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are\nconducted to explore the performance of several state-of-the-art methods. The\nresults demonstrate that the performance could be improved by combining\nentity-aware ASR and pretrained NER tagger, which can be easily applied to the\nmodern SLU pipeline. The dataset is publicly available at\ngithub.com/Alibaba-NLP/AISHELL-NER.", "published": "2022-02-17 09:18:48", "link": "http://arxiv.org/abs/2202.08533v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining On Alzheimer's Diseases Related Knowledge Graph to Identity\n  Potential AD-related Semantic Triples for Drug Repurposing", "abstract": "To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.", "published": "2022-02-17 15:33:27", "link": "http://arxiv.org/abs/2202.08712v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving English to Sinhala Neural Machine Translation using\n  Part-of-Speech Tag", "abstract": "The performance of Neural Machine Translation (NMT) depends significantly on\nthe size of the available parallel corpus. Due to this fact, low resource\nlanguage pairs demonstrate low translation performance compared to high\nresource language pairs. The translation quality further degrades when NMT is\nperformed for morphologically rich languages. Even though the web contains a\nlarge amount of information, most people in Sri Lanka are unable to read and\nunderstand English properly. Therefore, there is a huge requirement of\ntranslating English content to local languages to share information among\nlocals. Sinhala language is the primary language in Sri Lanka and building an\nNMT system that can produce quality English to Sinhala translations is\ndifficult due to the syntactic divergence between these two languages under low\nresource constraints. Thus, in this research, we explore effective methods of\nincorporating Part of Speech (POS) tags to the Transformer input embedding and\npositional encoding to further enhance the performance of the baseline English\nto Sinhala neural machine translation model.", "published": "2022-02-17 19:45:50", "link": "http://arxiv.org/abs/2202.08882v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models", "abstract": "Scale has opened new frontiers in natural language processing -- but at a\nhigh cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have\nbeen proposed as an energy efficient path to even larger and more capable\nlanguage models. But advancing the state-of-the-art across a broad set of\nnatural language tasks has been hindered by training instabilities and\nuncertain quality during fine-tuning. Our work focuses on these issues and acts\nas a design guide. We conclude by scaling a sparse model to 269B parameters,\nwith a computational cost comparable to a 32B dense encoder-decoder Transformer\n(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,\na sparse model achieves state-of-the-art performance in transfer learning,\nacross a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC\nChallenge), summarization (XSum, CNN-DM), closed book question answering\n(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,\nANLI R3).", "published": "2022-02-17 21:39:10", "link": "http://arxiv.org/abs/2202.08906v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discovering Fine-Grained Semantics in Knowledge Graph Relations", "abstract": "When it comes to comprehending and analyzing multi-relational data, the\nsemantics of relations are crucial. Polysemous relations between different\ntypes of entities, that represent multiple semantics, are common in real-world\nrelational datasets represented by knowledge graphs. For numerous use cases,\nsuch as entity type classification, question answering and knowledge graph\ncompletion, the correct semantic interpretation of these relations is\nnecessary. In this work, we provide a strategy for discovering the different\nsemantics associated with abstract relations and deriving many sub-relations\nwith fine-grained meaning. To do this, we leverage the types of the entities\nassociated with the relations and cluster the vector representations of\nentities and relations. The suggested method is able to automatically discover\nthe best number of sub-relations for a polysemous relation and determine their\nsemantic interpretation, according to our empirical evaluation.", "published": "2022-02-17 22:05:41", "link": "http://arxiv.org/abs/2202.08917v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Grammar-Based Grounded Lexicon Learning", "abstract": "We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist\napproach toward learning a compositional and grounded meaning representation of\nlanguage from grounded data, such as paired images and texts. At the core of\nG2L2 is a collection of lexicon entries, which map each word to a tuple of a\nsyntactic type and a neuro-symbolic semantic program. For example, the word\nshiny has a syntactic type of adjective; its neuro-symbolic semantic program\nhas the symbolic form {\\lambda}x. filter(x, SHINY), where the concept SHINY is\nassociated with a neural network embedding, which will be used to classify\nshiny objects. Given an input sentence, G2L2 first looks up the lexicon entries\nassociated with each token. It then derives the meaning of the sentence as an\nexecutable neuro-symbolic program by composing lexical meanings based on\nsyntax. The recovered meaning programs can be executed on grounded inputs. To\nfacilitate learning in an exponentially-growing compositional space, we\nintroduce a joint parsing and expected execution algorithm, which does local\nmarginalization over derivations to reduce the training time. We evaluate G2L2\non two domains: visual reasoning and language-driven navigation. Results show\nthat G2L2 can generalize from small amounts of data to novel compositions of\nwords.", "published": "2022-02-17 18:19:53", "link": "http://arxiv.org/abs/2202.08806v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Effects of Interactive AI Design on User Behavior: An Eye-tracking\n  Study of Fact-checking COVID-19 Claims", "abstract": "We conducted a lab-based eye-tracking study to investigate how the\ninteractivity of an AI-powered fact-checking system affects user interactions,\nsuch as dwell time, attention, and mental resources involved in using the\nsystem. A within-subject experiment was conducted, where participants used an\ninteractive and a non-interactive version of a mock AI fact-checking system and\nrated their perceived correctness of COVID-19 related claims. We collected\nweb-page interactions, eye-tracking data, and mental workload using NASA-TLX.\nWe found that the presence of the affordance of interactively manipulating the\nAI system's prediction parameters affected users' dwell times, and\neye-fixations on AOIs, but not mental workload. In the interactive system,\nparticipants spent the most time evaluating claims' correctness, followed by\nreading news. This promising result shows a positive role of interactivity in a\nmixed-initiative AI-powered system.", "published": "2022-02-17 21:08:57", "link": "http://arxiv.org/abs/2202.08901v2", "categories": ["cs.HC", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
{"title": "SGPT: GPT Sentence Embeddings for Semantic Search", "abstract": "Decoder transformers have continued increasing in scale reaching hundreds of\nbillions of parameters. Due to their scale the same decoder sets\nstate-of-the-art results on various language tasks via prompting or\nfine-tuning. Yet, these large foundation models remain unusable for the related\nfields of semantic search and sentence embeddings. This prevents possibly new\nstate-of-the-art results and forces organizations to train and maintain\nseparate models. To this end, we propose SGPT to use decoders for sentence\nembeddings and semantic search via prompting or fine-tuning. At 5.8 billion\nparameters SGPT improves on the previously best sentence embeddings by a margin\nof 7% and outperforms a concurrent method with 175 billion parameters as\nmeasured on the BEIR search benchmark. Code, models and result files are freely\navailable at https://github.com/Muennighoff/sgpt.", "published": "2022-02-17 21:35:56", "link": "http://arxiv.org/abs/2202.08904v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Intrinsic Exploration with Language Abstractions", "abstract": "Reinforcement learning (RL) agents are particularly hard to train when\nrewards are sparse. One common solution is to use intrinsic rewards to\nencourage agents to explore their environment. However, recent intrinsic\nexploration methods often use state-based novelty measures which reward\nlow-level exploration and may not scale to domains requiring more abstract\nskills. Instead, we explore natural language as a general medium for\nhighlighting relevant abstractions in an environment. Unlike previous work, we\nevaluate whether language can improve over existing exploration methods by\ndirectly extending (and comparing to) competitive intrinsic exploration\nbaselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These\nlanguage-based variants outperform their non-linguistic forms by 47-85% across\n13 challenging tasks from the MiniGrid and MiniHack environment suites.", "published": "2022-02-17 23:43:34", "link": "http://arxiv.org/abs/2202.08938v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on\n  YouTube", "abstract": "Over the last few years, YouTube Kids has emerged as one of the highly\ncompetitive alternatives to television for children's entertainment.\nConsequently, YouTube Kids' content should receive an additional level of\nscrutiny to ensure children's safety. While research on detecting offensive or\ninappropriate content for kids is gaining momentum, little or no current work\nexists that investigates to what extent AI applications can (accidentally)\nintroduce content that is inappropriate for kids.\n  In this paper, we present a novel (and troubling) finding that well-known\nautomatic speech recognition (ASR) systems may produce text content highly\ninappropriate for kids while transcribing YouTube Kids' videos. We dub this\nphenomenon as \\emph{inappropriate content hallucination}. Our analyses suggest\nthat such hallucinations are far from occasional, and the ASR systems often\nproduce them with high confidence. We release a first-of-its-kind data set of\naudios for which the existing state-of-the-art ASR systems hallucinate\ninappropriate content for kids. In addition, we demonstrate that some of these\nerrors can be fixed using language models.", "published": "2022-02-17 19:19:09", "link": "http://arxiv.org/abs/2203.04837v1", "categories": ["eess.AS", "cs.CL", "cs.CY"], "primary_category": "eess.AS"}
{"title": "When BERT Meets Quantum Temporal Convolution Learning for Text\n  Classification in Heterogeneous Computing", "abstract": "The rapid development of quantum computing has demonstrated many unique\ncharacteristics of quantum advantages, such as richer feature representation\nand more secured protection on model parameters. This work proposes a vertical\nfederated learning architecture based on variational quantum circuits to\ndemonstrate the competitive performance of a quantum-enhanced pre-trained BERT\nmodel for text classification. In particular, our proposed hybrid\nclassical-quantum model consists of a novel random quantum temporal convolution\n(QTC) learning framework replacing some layers in the BERT-based decoder. Our\nexperiments on intent classification show that our proposed BERT-QTC model\nattains competitive experimental results in the Snips and ATIS spoken language\ndatasets. Particularly, the BERT-QTC boosts the performance of the existing\nquantum circuit-based language model in two text classification datasets by\n1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasibly\ndeployed on both existing commercial-accessible quantum computation hardware\nand CPU-based interface for ensuring data isolation.", "published": "2022-02-17 09:55:21", "link": "http://arxiv.org/abs/2203.03550v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.NE", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Wearable SELD dataset: Dataset for sound event localization and\n  detection using wearable devices around head", "abstract": "Sound event localization and detection (SELD) is a combined task of\nidentifying the sound event and its direction. Deep neural networks (DNNs) are\nutilized to associate them with the sound signals observed by a microphone\narray. Although ambisonic microphones are popular in the literature of SELD,\nthey might limit the range of applications due to their predetermined geometry.\nSome applications (including those for pedestrians that perform SELD while\nwalking) require a wearable microphone array whose geometry can be designed to\nsuit the task. In this paper, for the development of such a wearable SELD, we\npropose a dataset named Wearable SELD dataset. It consists of data recorded by\n24 microphones placed on a head and torso simulators (HATS) with some\naccessories mimicking wearable devices (glasses, earphones, and headphones). We\nalso provide experimental results of SELD using the proposed dataset and\nSELDNet to investigate the effect of microphone configuration.", "published": "2022-02-17 06:08:27", "link": "http://arxiv.org/abs/2202.08458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic Event Detection with Classifier Chains", "abstract": "This paper proposes acoustic event detection (AED) with classifier chains, a\nnew classifier based on the probabilistic chain rule. The proposed AED with\nclassifier chains consists of a gated recurrent unit and performs iterative\nbinary detection of each event one by one. In each iteration, the event's\nactivity is estimated and used to condition the next output based on the\nprobabilistic chain rule to form classifier chains. Therefore, the proposed\nmethod can handle the interdependence among events upon classification, while\nthe conventional AED methods with multiple binary classifiers with a linear\nlayer and sigmoid function have placed an assumption of conditional\nindependence. In the experiments with a real-recording dataset, the proposed\nmethod demonstrates its superior AED performance to a relative 14.80%\nimprovement compared to a convolutional recurrent neural network baseline\nsystem with the multiple binary classifiers.", "published": "2022-02-17 06:46:48", "link": "http://arxiv.org/abs/2202.08470v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non-Autoregressive ASR with Self-Conditioned Folded Encoders", "abstract": "This paper proposes CTC-based non-autoregressive ASR with self-conditioned\nfolded encoders. The proposed method realizes non-autoregressive ASR with fewer\nparameters by folding the conventional stack of encoders into only two blocks;\nbase encoders and folded encoders. The base encoders convert the input audio\nfeatures into a neural representation suitable for recognition. This is\nfollowed by the folded encoders applied repeatedly for further refinement.\nApplying the CTC loss to the outputs of all encoders enforces the consistency\nof the input-output relationship. Thus, folded encoders learn to perform the\nsame operations as an encoder with deeper distinct layers. In experiments, we\ninvestigate how to set the number of layers and the number of iterations for\nthe base and folded encoders. The results show that the proposed method\nachieves a performance comparable to that of the conventional method using only\n38% as many parameters. Furthermore, it outperforms the conventional method\nwhen increasing the number of iterations.", "published": "2022-02-17 06:53:40", "link": "http://arxiv.org/abs/2202.08474v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Two-Stage U-Net for High-Fidelity Denoising of Historical Recordings", "abstract": "Enhancing the sound quality of historical music recordings is a long-standing\nproblem. This paper presents a novel denoising method based on a\nfully-convolutional deep neural network. A two-stage U-Net model architecture\nis designed to model and suppress the degradations with high fidelity. The\nmethod processes the time-frequency representation of audio, and is trained\nusing realistic noisy data to jointly remove hiss, clicks, thumps, and other\ncommon additive disturbances from old analog discs. The proposed model\noutperforms previous methods in both objective and subjective metrics. The\nresults of a formal blind listening test show that real gramophone recordings\ndenoised with this method have significantly better quality than the baseline\nmethods. This study shows the importance of realistic training data and the\npower of deep learning in audio restoration.", "published": "2022-02-17 15:14:38", "link": "http://arxiv.org/abs/2202.08702v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Chord-Conditioned Melody Harmonization with Controllable Harmonicity", "abstract": "Melody harmonization has long been closely associated with chorales composed\nby Johann Sebastian Bach. Previous works rarely emphasised chorale generation\nconditioned on chord progressions, and there has been a lack of focus on\nassistive compositional tools. In this paper, we first designed a music\nrepresentation that encoded chord symbols for chord conditioning, and then\nproposed DeepChoir, a melody harmonization system that can generate a four-part\nchorale for a given melody conditioned on a chord progression. With\ncontrollable harmonicity, users can control the extent of harmonicity for\ngenerated chorales. Experimental results reveal the effectiveness of the music\nrepresentation and the controllability of DeepChoir.", "published": "2022-02-17 02:59:36", "link": "http://arxiv.org/abs/2202.08423v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ADD 2022: the First Audio Deep Synthesis Detection Challenge", "abstract": "Audio deepfake detection is an emerging topic, which was included in the\nASVspoof 2021. However, the recent shared tasks have not covered many real-life\nand challenging scenarios. The first Audio Deep synthesis Detection challenge\n(ADD) was motivated to fill in the gap. The ADD 2022 includes three tracks:\nlow-quality fake audio detection (LF), partially fake audio detection (PF) and\naudio fake game (FG). The LF track focuses on dealing with bona fide and fully\nfake utterances with various real-world noises etc. The PF track aims to\ndistinguish the partially fake audio from the real. The FG track is a rivalry\ngame, which includes two tasks: an audio generation task and an audio fake\ndetection task. In this paper, we describe the datasets, evaluation metrics,\nand protocols. We also report major findings that reflect the recent advances\nin audio deepfake detection tasks.", "published": "2022-02-17 03:29:20", "link": "http://arxiv.org/abs/2202.08433v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MLP-ASR: Sequence-length agnostic all-MLP architectures for speech\n  recognition", "abstract": "We propose multi-layer perceptron (MLP)-based architectures suitable for\nvariable length input. MLP-based architectures, recently proposed for image\nclassification, can only be used for inputs of a fixed, pre-defined size.\nHowever, many types of data are naturally variable in length, for example,\nacoustic signals. We propose three approaches to extend MLP-based architectures\nfor use with sequences of arbitrary length. The first one uses a circular\nconvolution applied in the Fourier domain, the second applies a depthwise\nconvolution, and the final relies on a shift operation. We evaluate the\nproposed architectures on an automatic speech recognition task with the\nLibrispeech and Tedlium2 corpora. The best proposed MLP-based architectures\nimproves WER by 1.0 / 0.9%, 0.9 / 0.5% on Librispeech dev-clean/dev-other,\ntest-clean/test-other set, and 0.8 / 1.1% on Tedlium2 dev/test set using 86.4%\nthe size of self-attention-based architecture.", "published": "2022-02-17 06:06:09", "link": "http://arxiv.org/abs/2202.08456v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end Music Remastering System Using Self-supervised and\n  Adversarial Training", "abstract": "Mastering is an essential step in music production, but it is also a\nchallenging task that has to go through the hands of experienced audio\nengineers, where they adjust tone, space, and volume of a song. Remastering\nfollows the same technical process, in which the context lies in mastering a\nsong for the times. As these tasks have high entry barriers, we aim to lower\nthe barriers by proposing an end-to-end music remastering system that\ntransforms the mastering style of input audio to that of the target. The system\nis trained in a self-supervised manner, in which released pop songs were used\nfor training. We also anticipated the model to generate realistic audio\nreflecting the reference's mastering style by applying a pre-trained encoder\nand a projection discriminator. We validate our results with quantitative\nmetrics and a subjective listening test and show that the model generated\nsamples of mastering style similar to the target.", "published": "2022-02-17 08:50:12", "link": "http://arxiv.org/abs/2202.08520v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Channel Speech Denoising for Machine Ears", "abstract": "This work describes a speech denoising system for machine ears that aims to\nimprove speech intelligibility and the overall listening experience in noisy\nenvironments. We recorded approximately 100 hours of audio data with\nreverberation and moderate environmental noise using a pair of microphone\narrays placed around each of the two ears and then mixed sound recordings to\nsimulate adverse acoustic scenes. Then, we trained a multi-channel speech\ndenoising network (MCSDN) on the mixture of recordings. To improve the\ntraining, we employ an unsupervised method, complex angular central Gaussian\nmixture model (cACGMM), to acquire cleaner speech from noisy recordings to\nserve as the learning target. We propose a MCSDN-Beamforming-MCSDN framework in\nthe inference stage. The results of the subjective evaluation show that the\ncACGMM improves the training data, resulting in better noise reduction and user\npreference, and the entire system improves the intelligibility and listening\nexperience in noisy situations.", "published": "2022-02-17 17:59:30", "link": "http://arxiv.org/abs/2202.08793v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "RemixIT: Continual self-training of speech enhancement models via\n  bootstrapped remixing", "abstract": "We present RemixIT, a simple yet effective self-supervised method for\ntraining speech enhancement without the need of a single isolated in-domain\nspeech nor a noise waveform. Our approach overcomes limitations of previous\nmethods which make them dependent on clean in-domain target signals and thus,\nsensitive to any domain mismatch between train and test samples. RemixIT is\nbased on a continuous self-training scheme in which a pre-trained teacher model\non out-of-domain data infers estimated pseudo-target signals for in-domain\nmixtures. Then, by permuting the estimated clean and noise signals and remixing\nthem together, we generate a new set of bootstrapped mixtures and corresponding\npseudo-targets which are used to train the student network. Vice-versa, the\nteacher periodically refines its estimates using the updated parameters of the\nlatest student models. Experimental results on multiple speech enhancement\ndatasets and tasks not only show the superiority of our method over prior\napproaches but also showcase that RemixIT can be combined with any separation\nmodel as well as be applied towards any semi-supervised and unsupervised domain\nadaptation task. Our analysis, paired with empirical evidence, sheds light on\nthe inside functioning of our self-training scheme wherein the student model\nkeeps obtaining better performance while observing severely degraded\npseudo-targets.", "published": "2022-02-17 19:07:29", "link": "http://arxiv.org/abs/2202.08862v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Curriculum optimization for low-resource speech recognition", "abstract": "Modern end-to-end speech recognition models show astonishing results in\ntranscribing audio signals into written text. However, conventional data\nfeeding pipelines may be sub-optimal for low-resource speech recognition, which\nstill remains a challenging task. We propose an automated curriculum learning\napproach to optimize the sequence of training examples based on both the\nprogress of the model while training and prior knowledge about the difficulty\nof the training examples. We introduce a new difficulty measure called\ncompression ratio that can be used as a scoring function for raw audio in\nvarious noise conditions. The proposed method improves speech recognition Word\nError Rate performance by up to 33% relative over the baseline system", "published": "2022-02-17 19:47:50", "link": "http://arxiv.org/abs/2202.08883v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word Embeddings for Automatic Equalization in Audio Mixing", "abstract": "In recent years, machine learning has been widely adopted to automate the\naudio mixing process. Automatic mixing systems have been applied to various\naudio effects such as gain-adjustment, equalization, and reverberation. These\nsystems can be controlled through visual interfaces, providing audio examples,\nusing knobs, and semantic descriptors. Using semantic descriptors or textual\ninformation to control these systems is an effective way for artists to\ncommunicate their creative goals. In this paper, we explore the novel idea of\nusing word embeddings to represent semantic descriptors. Word embeddings are\ngenerally obtained by training neural networks on large corpora of written\ntext. These embeddings serve as the input layer of the neural network to create\na translation from words to EQ settings. Using this technique, the machine\nlearning model can also generate EQ settings for semantic descriptors that it\nhas not seen before. We compare the EQ settings of humans with the predictions\nof the neural network to evaluate the quality of predictions. The results\nshowed that the embedding layer enables the neural network to understand\nsemantic descriptors. We observed that the models with embedding layers perform\nbetter than those without embedding layers, but still not as good as human\nlabels.", "published": "2022-02-17 21:02:48", "link": "http://arxiv.org/abs/2202.08898v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.5.1; I.5.4"], "primary_category": "cs.SD"}
{"title": "Attributable-Watermarking of Speech Generative Models", "abstract": "Generative models are now capable of synthesizing images, speeches, and\nvideos that are hardly distinguishable from authentic contents. Such\ncapabilities cause concerns such as malicious impersonation and IP theft. This\npaper investigates a solution for model attribution, i.e., the classification\nof synthetic contents by their source models via watermarks embedded in the\ncontents. Building on past success of model attribution in the image domain, we\ndiscuss algorithmic improvements for generating user-end speech models that\nempirically achieve high attribution accuracy, while maintaining high\ngeneration quality. We show the trade off between attributability and\ngeneration quality under a variety of attacks on generated speech signals\nattempting to remove the watermarks, and the feasibility of learning robust\nwatermarks against these attacks.", "published": "2022-02-17 21:08:31", "link": "http://arxiv.org/abs/2202.08900v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Summary of the ComParE COVID-19 Challenges", "abstract": "The COVID-19 pandemic has caused massive humanitarian and economic damage.\nTeams of scientists from a broad range of disciplines have searched for methods\nto help governments and communities combat the disease. One avenue from the\nmachine learning field which has been explored is the prospect of a digital\nmass test which can detect COVID-19 from infected individuals' respiratory\nsounds. We present a summary of the results from the INTERSPEECH 2021\nComputational Paralinguistics Challenges: COVID-19 Cough, (CCS) and COVID-19\nSpeech, (CSS).", "published": "2022-02-17 18:50:20", "link": "http://arxiv.org/abs/2202.08981v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Iterative Phase Retrieval for Ptychography", "abstract": "One of the most prominent challenges in the field of diffractive imaging is\nthe phase retrieval (PR) problem: In order to reconstruct an object from its\ndiffraction pattern, the inverse Fourier transform must be computed. This is\nonly possible given the full complex-valued diffraction data, i.e. magnitude\nand phase. However, in diffractive imaging, generally only magnitudes can be\ndirectly measured while the phase needs to be estimated. In this work we\nspecifically consider ptychography, a sub-field of diffractive imaging, where\nobjects are reconstructed from multiple overlapping diffraction images. We\npropose an augmentation of existing iterative phase retrieval algorithms with a\nneural network designed for refining the result of each iteration. For this\npurpose we adapt and extend a recently proposed architecture from the speech\nprocessing field. Evaluation results show the proposed approach delivers\nimproved convergence rates in terms of both iteration count and algorithm\nruntime.", "published": "2022-02-17 09:13:35", "link": "http://arxiv.org/abs/2202.10573v1", "categories": ["eess.IV", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "eess.IV"}
{"title": "Recognizing Concepts and Recognizing Musical Themes. A Quantum Semantic\n  Analysis", "abstract": "How are abstract concepts and musical themes recognized on the basis of some\nprevious experience? It is interesting to compare the different behaviors of\nhuman and of artificial intelligences with respect to this problem. Generally,\na human mind that abstracts a concept (say, table) from a given set of known\nexamples creates a table-Gestalt: a kind of vague and out of focus image that\ndoes not fully correspond to a particular table with well determined features.\nA similar situation arises in the case of musical themes. Can the construction\nof a gestaltic pattern, which is so natural for human minds, be taught to an\nintelligent machine? This problem can be successfully discussed in the\nframework of a quantum approach to pattern recognition and to machine learning.\nThe basic idea is replacing classical data sets with quantum data sets, where\neither objects or musical themes can be formally represented as pieces of\nquantum information, involving the uncertainties and the ambiguities that\ncharacterize the quantum world. In this framework, the intuitive concept of\nGestalt can be simulated by the mathematical concept of positive centroid of a\ngiven quantum data set. Accordingly, the crucial problem \"how can we classify a\nnew object or a new musical theme (we have listened to) on the basis of a\nprevious experience?\" can be dealt with in terms of some special quantum\nsimilarity-relations. Although recognition procedures are different for human\nand for artificial intelligences, there is a common method of \"facing the\nproblems\" that seems to work in both cases.", "published": "2022-02-17 17:55:55", "link": "http://arxiv.org/abs/2202.10941v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "quant-ph"], "primary_category": "cs.LG"}
{"title": "A Study of Designing Compact Audio-Visual Wake Word Spotting System\n  Based on Iterative Fine-Tuning in Neural Network Pruning", "abstract": "Audio-only-based wake word spotting (WWS) is challenging under noisy\nconditions due to environmental interference in signal transmission. In this\npaper, we investigate on designing a compact audio-visual WWS system by\nutilizing visual information to alleviate the degradation. Specifically, in\norder to use visual information, we first encode the detected lips to\nfixed-size vectors with MobileNet and concatenate them with acoustic features\nfollowed by the fusion network for WWS. However, the audio-visual model based\non neural networks requires a large footprint and a high computational\ncomplexity. To meet the application requirements, we introduce a neural network\npruning strategy via the lottery ticket hypothesis in an iterative fine-tuning\nmanner (LTH-IF), to the single-modal and multi-modal models, respectively.\nTested on our in-house corpus for audio-visual WWS in a home TV scene, the\nproposed audio-visual system achieves significant performance improvements over\nthe single-modality (audio-only or video-only) system under different noisy\nconditions. Moreover, LTH-IF pruning can largely reduce the network parameters\nand computations with no degradation of WWS performance, leading to a potential\nproduct solution for the TV wake-up scenario.", "published": "2022-02-17 08:26:25", "link": "http://arxiv.org/abs/2202.08509v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Closed-model Adversarial Examples with Bayesian Neural\n  Modeling for Enhanced End-to-End Speech Recognition", "abstract": "In this work, we aim to enhance the system robustness of end-to-end automatic\nspeech recognition (ASR) against adversarially-noisy speech examples. We focus\non a rigorous and empirical \"closed-model adversarial robustness\" setting\n(e.g., on-device or cloud applications). The adversarial noise is only\ngenerated by closed-model optimization (e.g., evolutionary and zeroth-order\nestimation) without accessing gradient information of a targeted ASR model\ndirectly. We propose an advanced Bayesian neural network (BNN) based\nadversarial detector, which could model latent distributions against adaptive\nadversarial perturbation with divergence measurement. We further simulate\ndeployment scenarios of RNN Transducer, Conformer, and wav2vec-2.0 based ASR\nsystems with the proposed adversarial detection system. Leveraging the proposed\nBNN based detection system, we improve detection rate by +2.77 to +5.42%\n(relative +3.03 to +6.26%) and reduce the word error rate by 5.02 to 7.47% on\nLibriSpeech datasets compared to the current model enhancement methods against\nthe adversarial speech examples.", "published": "2022-02-17 09:17:58", "link": "http://arxiv.org/abs/2202.08532v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
