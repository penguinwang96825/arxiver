{"title": "Vietnamese Word Segmentation with SVM: Ambiguity Reduction and Suffix\n  Capture", "abstract": "In this paper, we approach Vietnamese word segmentation as a binary\nclassification by using the Support Vector Machine classifier. We inherit\nfeatures from prior works such as n-gram of syllables, n-gram of syllable\ntypes, and checking conjunction of adjacent syllables in the dictionary. We\npropose two novel ways to feature extraction, one to reduce the overlap\nambiguity and the other to increase the ability to predict unknown words\ncontaining suffixes. Different from UETsegmenter and RDRsegmenter, two\nstate-of-the-art Vietnamese word segmentation methods, we do not employ the\nlongest matching algorithm as an initial processing step or any post-processing\ntechnique. According to experimental results on benchmark Vietnamese datasets,\nour proposed method obtained a better F1-score than the prior state-of-the-art\nmethods UETsegmenter, and RDRsegmenter.", "published": "2020-06-14 05:19:46", "link": "http://arxiv.org/abs/2006.07804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinEst BERT and CroSloEngual BERT: less is more in multilingual models", "abstract": "Large pretrained masked language models have become state-of-the-art\nsolutions for many NLP problems. The research has been mostly focused on\nEnglish language, though. While massively multilingual models exist, studies\nhave shown that monolingual models produce much better results. We train two\ntrilingual BERT-like models, one for Finnish, Estonian, and English, the other\nfor Croatian, Slovenian, and English. We evaluate their performance on several\ndownstream tasks, NER, POS-tagging, and dependency parsing, using the\nmultilingual BERT and XLM-R as baselines. The newly created FinEst BERT and\nCroSloEngual BERT improve the results on all tasks in most monolingual and\ncross-lingual situations", "published": "2020-06-14 12:54:01", "link": "http://arxiv.org/abs/2006.07890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pot, kettle: Nonliteral titles aren't (natural) science", "abstract": "Researchers may be tempted to attract attention through poetic titles for\ntheir publications, but would this be mistaken in some fields? Whilst poetic\ntitles are known to be common in medicine, it is not clear whether the practice\nis widespread elsewhere. This article investigates the prevalence of poetic\nexpressions in journal article titles 1996-2019 in 3.3 million articles from\nall 27 Scopus broad fields. Expressions were identified by manually checking\nall phrases with at least 5 words that occurred at least 25 times, finding 149\nstock phrases, idioms, sayings, literary allusions, film names and song titles\nor lyrics. The expressions found are most common in the social sciences and the\nhumanities. They are also relatively common in medicine, but almost absent from\nengineering and the natural and formal sciences. The differences may reflect\nthe less hierarchical and more varied nature of the social sciences and\nhumanities, where interesting titles may attract an audience. In engineering,\nnatural science and formal science fields, authors should take extra care with\npoetic expressions, in case their choice is judged inappropriate. This includes\ninterdisciplinary research overlapping these areas. Conversely, reviewers of\ninterdisciplinary research involving the social sciences should be more\ntolerant of poetic license.", "published": "2020-06-14 09:32:13", "link": "http://arxiv.org/abs/2006.07849v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Examining the Role of Mood Patterns in Predicting Self-Reported\n  Depressive symptoms", "abstract": "Depression is the leading cause of disability worldwide. Initial efforts to\ndetect depression signals from social media posts have shown promising results.\nGiven the high internal validity, results from such analyses are potentially\nbeneficial to clinical judgment. The existing models for automatic detection of\ndepressive symptoms learn proxy diagnostic signals from social media data, such\nas help-seeking behavior for mental health or medication names. However, in\nreality, individuals with depression typically experience depressed mood, loss\nof pleasure nearly in all the activities, feeling of worthlessness or guilt,\nand diminished ability to think. Therefore, a lot of the proxy signals used in\nthese models lack the theoretical underpinnings for depressive symptoms. It is\nalso reported that social media posts from many patients in the clinical\nsetting do not contain these signals. Based on this research gap, we propose to\nmonitor a type of signal that is well-established as a class of symptoms in\naffective disorders -- mood. The mood is an experience of feeling that can last\nfor hours, days, or even weeks. In this work, we attempt to enrich current\ntechnology for detecting symptoms of potential depression by constructing a\n'mood profile' for social media users.", "published": "2020-06-14 12:48:43", "link": "http://arxiv.org/abs/2006.07887v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "UWSpeech: Speech to Speech Translation for Unwritten Languages", "abstract": "Existing speech to speech translation systems heavily rely on the text of\ntarget language: they usually translate source language either to target text\nand then synthesize target speech from text, or directly to target speech with\ntarget text for auxiliary training. However, those methods cannot be applied to\nunwritten target languages, which have no written text or phoneme available. In\nthis paper, we develop a translation system for unwritten languages, named as\nUWSpeech, which converts target unwritten speech into discrete tokens with a\nconverter, and then translates source-language speech into target discrete\ntokens with a translator, and finally synthesizes target speech from target\ndiscrete tokens with an inverter. We propose a method called XL-VAE, which\nenhances vector quantized variational autoencoder (VQ-VAE) with cross-lingual\n(XL) speech recognition, to train the converter and inverter of UWSpeech\njointly. Experiments on Fisher Spanish-English conversation translation dataset\nshow that UWSpeech outperforms direct translation and VQ-VAE baseline by about\n16 and 10 BLEU points respectively, which demonstrate the advantages and\npotentials of UWSpeech.", "published": "2020-06-14 15:22:12", "link": "http://arxiv.org/abs/2006.07926v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "FFR v1.1: Fon-French Neural Machine Translation", "abstract": "All over the world and especially in Africa, researchers are putting efforts\ninto building Neural Machine Translation (NMT) systems to help tackle the\nlanguage barriers in Africa, a continent of over 2000 different languages.\nHowever, the low-resourceness, diacritical, and tonal complexities of African\nlanguages are major issues being faced. The FFR project is a major step towards\ncreating a robust translation model from Fon, a very low-resource and tonal\nlanguage, to French, for research and public use. In this paper, we introduce\nFFR Dataset, a corpus of Fon-to-French translations, describe the diacritical\nencoding process, and introduce our FFR v1.1 model, trained on the dataset. The\ndataset and model are made publicly available at https://github.com/\nbonaventuredossou/ffr-v1, to promote collaboration and reproducibility.", "published": "2020-06-14 04:27:12", "link": "http://arxiv.org/abs/2006.09217v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Multimodal Behavioral Analytics for Automated Job Interview\n  Performance Assessment and Feedback", "abstract": "Behavioral cues play a significant part in human communication and cognitive\nperception. In most professional domains, employee recruitment policies are\nframed such that both professional skills and personality traits are adequately\nassessed. Hiring interviews are structured to evaluate expansively a potential\nemployee's suitability for the position - their professional qualifications,\ninterpersonal skills, ability to perform in critical and stressful situations,\nin the presence of time and resource constraints, etc. Therefore, candidates\nneed to be aware of their positive and negative attributes and be mindful of\nbehavioral cues that might have adverse effects on their success. We propose a\nmultimodal analytical framework that analyzes the candidate in an interview\nscenario and provides feedback for predefined labels such as engagement,\nspeaking rate, eye contact, etc. We perform a comprehensive analysis that\nincludes the interviewee's facial expressions, speech, and prosodic\ninformation, using the video, audio, and text transcripts obtained from the\nrecorded interview. We use these multimodal data sources to construct a\ncomposite representation, which is used for training machine learning\nclassifiers to predict the class labels. Such analysis is then used to provide\nconstructive feedback to the interviewee for their behavioral cues and body\nlanguage. Experimental validation showed that the proposed methodology achieved\npromising results.", "published": "2020-06-14 14:20:42", "link": "http://arxiv.org/abs/2006.07909v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Relational reasoning and generalization using non-symbolic neural\n  networks", "abstract": "The notion of equality (identity) is simple and ubiquitous, making it a key\ncase study for broader questions about the representations supporting abstract\nrelational reasoning. Previous work suggested that neural networks were not\nsuitable models of human relational reasoning because they could not represent\nmathematically identity, the most basic form of equality. We revisit this\nquestion. In our experiments, we assess out-of-sample generalization of\nequality using both arbitrary representations and representations that have\nbeen pretrained on separate tasks to imbue them with structure. We find neural\nnetworks are able to learn (1) basic equality (mathematical identity), (2)\nsequential equality problems (learning ABA-patterned sequences) with only\npositive training instances, and (3) a complex, hierarchical equality problem\nwith only basic equality training instances (\"zero-shot'\" generalization). In\nthe two latter cases, our models perform tasks proposed in previous work to\ndemarcate human-unique symbolic abilities. These results suggest that essential\naspects of symbolic reasoning can emerge from data-driven, non-symbolic\nlearning processes.", "published": "2020-06-14 18:25:42", "link": "http://arxiv.org/abs/2006.07968v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Continual General Chunking Problem and SyncMap", "abstract": "Humans possess an inherent ability to chunk sequences into their constituent\nparts. In fact, this ability is thought to bootstrap language skills and\nlearning of image patterns which might be a key to a more animal-like type of\nintelligence. Here, we propose a continual generalization of the chunking\nproblem (an unsupervised problem), encompassing fixed and probabilistic chunks,\ndiscovery of temporal and causal structures and their continual variations.\nAdditionally, we propose an algorithm called SyncMap that can learn and adapt\nto changes in the problem by creating a dynamic map which preserves the\ncorrelation between variables. Results of SyncMap suggest that the proposed\nalgorithm learn near optimal solutions, despite the presence of many types of\nstructures and their continual variation. When compared to Word2vec, PARSER and\nMRIL, SyncMap surpasses or ties with the best algorithm on $66\\%$ of the\nscenarios while being the second best in the remaining $34\\%$. SyncMap's\nmodel-free simple dynamics and the absence of loss functions reveal that,\nperhaps surprisingly, much can be done with self-organization alone. Code\navailable at https://github.com/zweifel/SyncMap.", "published": "2020-06-14 09:39:56", "link": "http://arxiv.org/abs/2006.07853v4", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.ET", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The JHU Multi-Microphone Multi-Speaker ASR System for the CHiME-6\n  Challenge", "abstract": "This paper summarizes the JHU team's efforts in tracks 1 and 2 of the CHiME-6\nchallenge for distant multi-microphone conversational speech diarization and\nrecognition in everyday home environments. We explore multi-array processing\ntechniques at each stage of the pipeline, such as multi-array guided source\nseparation (GSS) for enhancement and acoustic model training data, posterior\nfusion for speech activity detection, PLDA score fusion for diarization, and\nlattice combination for automatic speech recognition (ASR). We also report\nresults with different acoustic model architectures, and integrate other\ntechniques such as online multi-channel weighted prediction error (WPE)\ndereverberation and variational Bayes-hidden Markov model (VB-HMM) based\noverlap assignment to deal with reverberation and overlapping speakers,\nrespectively. As a result of these efforts, our ASR systems achieve a word\nerror rate of 40.5% and 67.5% on tracks 1 and 2, respectively, on the\nevaluation set. This is an improvement of 10.8% and 10.4% absolute, over the\nchallenge baselines for the respective tracks.", "published": "2020-06-14 13:24:09", "link": "http://arxiv.org/abs/2006.07898v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Solos: A Dataset for Audio-Visual Music Analysis", "abstract": "In this paper, we present a new dataset of music performance videos which can\nbe used for training machine learning methods for multiple tasks such as\naudio-visual blind source separation and localization, cross-modal\ncorrespondences, cross-modal generation and, in general, any audio-visual\nself-supervised task. These videos, gathered from YouTube, consist of solo\nmusical performances of 13 different instruments. Compared to previously\nproposed audio-visual datasets, Solos is cleaner since a big amount of its\nrecordings are auditions and manually checked recordings, ensuring there is no\nbackground noise nor effects added in the video post-processing. Besides, it\nis, up to the best of our knowledge, the only dataset that contains the whole\nset of instruments present in the URMP\\cite{URPM} dataset, a high-quality\ndataset of 44 audio-visual recordings of multi-instrument classical music\npieces with individual audio tracks. URMP was intented to be used for source\nseparation, thus, we evaluate the performance on the URMP dataset of two\ndifferent source-separation models trained on Solos. The dataset is publicly\navailable at https://juanfmontesinos.github.io/Solos/", "published": "2020-06-14 15:30:44", "link": "http://arxiv.org/abs/2006.07931v2", "categories": ["eess.AS", "cs.DB", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BatVision with GCC-PHAT Features for Better Sound to Vision Predictions", "abstract": "Inspired by sophisticated echolocation abilities found in nature, we train a\ngenerative adversarial network to predict plausible depth maps and grayscale\nlayouts from sound. To achieve this, our sound-to-vision model processes\nbinaural echo-returns from chirping sounds. We build upon previous work with\nBatVision that consists of a sound-to-vision model and a self-collected dataset\nusing our mobile robot and low-cost hardware. We improve on the previous model\nby introducing several changes to the model, which leads to a better depth and\ngrayscale estimation, and increased perceptual quality. Rather than using raw\nbinaural waveforms as input, we generate generalized cross-correlation (GCC)\nfeatures and use these as input instead. In addition, we change the model\ngenerator and base it on residual learning and use spectral normalization in\nthe discriminator. We compare and present both quantitative and qualitative\nimprovements over our previous BatVision model.", "published": "2020-06-14 19:49:58", "link": "http://arxiv.org/abs/2006.07995v1", "categories": ["cs.CV", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Deep Neural Network for Audio Classification with a Classifier\n  Attention Mechanism", "abstract": "Audio classification is considered as a challenging problem in pattern\nrecognition. Recently, many algorithms have been proposed using deep neural\nnetworks. In this paper, we introduce a new attention-based neural network\narchitecture called Classifier-Attention-Based Convolutional Neural Network\n(CAB-CNN). The algorithm uses a newly designed architecture consisting of a\nlist of simple classifiers and an attention mechanism as a classifier selector.\nThis design significantly reduces the number of parameters required by the\nclassifiers and thus their complexities. In this way, it becomes easier to\ntrain the classifiers and achieve a high and steady performance. Our claims are\ncorroborated by the experimental results. Compared to the state-of-the-art\nalgorithms, our algorithm achieves more than 10% improvements on all selected\ntest scores.", "published": "2020-06-14 21:29:44", "link": "http://arxiv.org/abs/2006.09815v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
