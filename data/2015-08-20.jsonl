{"title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "abstract": "Neural networks have been shown to improve performance across a range of\nnatural-language tasks. However, designing and training them can be\ncomplicated. Frequently, researchers resort to repeated experimentation to pick\noptimal settings. In this paper, we address the issue of choosing the correct\nnumber of units in hidden layers. We introduce a method for automatically\nadjusting network size by pruning out hidden units through $\\ell_{\\infty,1}$\nand $\\ell_{2,1}$ regularization. We apply this method to language modeling and\ndemonstrate its ability to correctly choose the number of hidden units while\nmaintaining perplexity. We also include these models in a machine translation\ndecoder and show that these smaller neural models maintain the significant\nimprovements of their unpruned versions.", "published": "2015-08-20 17:21:50", "link": "http://arxiv.org/abs/1508.05051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
