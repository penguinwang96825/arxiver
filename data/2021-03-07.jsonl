{"title": "MTLHealth: A Deep Learning System for Detecting Disturbing Content in\n  Student Essays", "abstract": "Essay submissions to standardized tests like the ACT occasionally include\nreferences to bullying, self-harm, violence, and other forms of disturbing\ncontent. Graders must take great care to identify cases like these and decide\nwhether to alert authorities on behalf of students who may be in danger. There\nis a growing need for robust computer systems to support human decision-makers\nby automatically flagging potential instances of disturbing content. This paper\ndescribes MTLHealth, a disturbing content detection pipeline built around\nrecent advances from computational linguistics, particularly pre-trained\nlanguage model Transformer networks.", "published": "2021-03-07 07:51:33", "link": "http://arxiv.org/abs/2103.04290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees", "abstract": "Pre-trained language models like BERT achieve superior performances in\nvarious NLP tasks without explicit consideration of syntactic information.\nMeanwhile, syntactic information has been proved to be crucial for the success\nof NLP applications. However, how to incorporate the syntax trees effectively\nand efficiently into pre-trained Transformers is still unsettled. In this\npaper, we address this problem by proposing a novel framework named\nSyntax-BERT. This framework works in a plug-and-play mode and is applicable to\nan arbitrary pre-trained checkpoint based on Transformer architecture.\nExperiments on various datasets of natural language understanding verify the\neffectiveness of syntax trees and achieve consistent improvement over multiple\npre-trained models, including BERT, RoBERTa, and T5.", "published": "2021-03-07 13:11:31", "link": "http://arxiv.org/abs/2103.04350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathetic BERT2BERT Conversational Model: Learning Arabic Language\n  Generation with Little Data", "abstract": "Enabling empathetic behavior in Arabic dialogue agents is an important aspect\nof building human-like conversational models. While Arabic Natural Language\nProcessing has seen significant advances in Natural Language Understanding\n(NLU) with language models such as AraBERT, Natural Language Generation (NLG)\nremains a challenge. The shortcomings of NLG encoder-decoder models are\nprimarily due to the lack of Arabic datasets suitable to train NLG models such\nas conversational agents. To overcome this issue, we propose a\ntransformer-based encoder-decoder initialized with AraBERT parameters. By\ninitializing the weights of the encoder and decoder with AraBERT pre-trained\nweights, our model was able to leverage knowledge transfer and boost\nperformance in response generation. To enable empathy in our conversational\nmodel, we train it using the ArabicEmpatheticDialogues dataset and achieve high\nperformance in empathetic response generation. Specifically, our model achieved\na low perplexity value of 17.0 and an increase in 5 BLEU points compared to the\nprevious state-of-the-art model. Also, our proposed model was rated highly by\n85 human evaluators, validating its high capability in exhibiting empathy while\ngenerating relevant and fluent responses in open-domain settings.", "published": "2021-03-07 13:23:51", "link": "http://arxiv.org/abs/2103.04353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Difficulty Classification of Arabic Sentences", "abstract": "In this paper, we present a Modern Standard Arabic (MSA) Sentence difficulty\nclassifier, which predicts the difficulty of sentences for language learners\nusing either the CEFR proficiency levels or the binary classification as simple\nor complex. We compare the use of sentence embeddings of different kinds\n(fastText, mBERT , XLM-R and Arabic-BERT), as well as traditional language\nfeatures such as POS tags, dependency trees, readability scores and frequency\nlists for language learners. Our best results have been achieved using\nfined-tuned Arabic-BERT. The accuracy of our 3-way CEFR classification is F-1\nof 0.80 and 0.75 for Arabic-Bert and XLM-R classification respectively and 0.71\nSpearman correlation for regression. Our binary difficulty classifier reaches\nF-1 0.94 and F-1 0.98 for sentence-pair semantic similarity classifier.", "published": "2021-03-07 16:02:04", "link": "http://arxiv.org/abs/2103.04386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text-to-SQL with Schema Dependency Learning", "abstract": "Text-to-SQL aims to map natural language questions to SQL queries. The\nsketch-based method combined with execution-guided (EG) decoding strategy has\nshown a strong performance on the WikiSQL benchmark. However, execution-guided\ndecoding relies on database execution, which significantly slows down the\ninference process and is hence unsatisfactory for many real-world applications.\nIn this paper, we present the Schema Dependency guided multi-task Text-to-SQL\nmodel (SDSQL) to guide the network to effectively capture the interactions\nbetween questions and schemas. The proposed model outperforms all existing\nmethods in both the settings with or without EG. We show the schema dependency\nlearning partially cover the benefit from EG and alleviates the need for it.\nSDSQL without EG significantly reduces time consumption during inference,\nsacrificing only a small amount of performance and provides more flexibility\nfor downstream applications.", "published": "2021-03-07 16:56:56", "link": "http://arxiv.org/abs/2103.04399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TypeShift: A User Interface for Visualizing the Typing Production\n  Process", "abstract": "TypeShift is a tool for visualizing linguistic patterns in the timing of\ntyping production. Language production is a complex process which draws on\nlinguistic, cognitive and motor skills. By visualizing holistic trends in the\ntyping process, TypeShift aims to elucidate the often noisy information signals\nthat are used to represent typing patterns, both at the word-level and\ncharacter-level. It accomplishes this by enabling a researcher to compare and\ncontrast specific linguistic phenomena, and compare an individual typing\nsession to multiple group averages. Finally, although TypeShift was originally\ndesigned for typing data, it can easy be adapted to accommodate speech data, as\nwell. A web demo is available at https://angoodkind.shinyapps.io/TypeShift/.\nThe source code can be accessed at https://github.com/angoodkind/TypeShift.", "published": "2021-03-07 00:59:31", "link": "http://arxiv.org/abs/2103.04222v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Orthogonal Attention: A Cloze-Style Approach to Negation Scope\n  Resolution", "abstract": "Negation Scope Resolution is an extensively researched problem, which is used\nto locate the words affected by a negation cue in a sentence. Recent works have\nshown that simply finetuning transformer-based architectures yield\nstate-of-the-art results on this task. In this work, we look at Negation Scope\nResolution as a Cloze-Style task, with the sentence as the Context and the cue\nwords as the Query. We also introduce a novel Cloze-Style Attention mechanism\ncalled Orthogonal Attention, which is inspired by Self Attention. First, we\npropose a framework for developing Orthogonal Attention variants, and then\npropose 4 Orthogonal Attention variants: OA-C, OA-CA, OA-EM, and OA-EMB. Using\nthese Orthogonal Attention layers on top of an XLNet backbone, we outperform\nthe finetuned XLNet state-of-the-art for Negation Scope Resolution, achieving\nthe best results to date on all 4 datasets we experiment with: BioScope\nAbstracts, BioScope Full Papers, SFU Review Corpus and the *sem 2012 Dataset\n(Sherlock).", "published": "2021-03-07 08:10:33", "link": "http://arxiv.org/abs/2103.04294v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Local word statistics affect reading times independently of surprisal", "abstract": "Surprisal theory has provided a unifying framework for understanding many\nphenomena in sentence processing (Hale, 2001; Levy, 2008a), positing that a\nword's conditional probability given all prior context fully determines\nprocessing difficulty. Problematically for this claim, one local statistic,\nword frequency, has also been shown to affect processing, even when conditional\nprobability given context is held constant. Here, we ask whether other local\nstatistics have a role in processing, or whether word frequency is a special\ncase. We present the first clear evidence that more complex local statistics,\nword bigram and trigram probability, also affect processing independently of\nsurprisal. These findings suggest a significant and independent role of local\nstatistics in processing. Further, it motivates research into new\ngeneralizations of surprisal that can also explain why local statistical\ninformation should have an outsized effect.", "published": "2021-03-07 22:18:46", "link": "http://arxiv.org/abs/2103.04469v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating the Unseen? Yoruba-English MT in Low-Resource,\n  Morphologically-Unmarked Settings", "abstract": "Translating between languages where certain features are marked\nmorphologically in one but absent or marked contextually in the other is an\nimportant test case for machine translation. When translating into English\nwhich marks (in)definiteness morphologically, from Yor\\`ub\\'a which uses bare\nnouns but marks these features contextually, ambiguities arise. In this work,\nwe perform fine-grained analysis on how an SMT system compares with two NMT\nsystems (BiLSTM and Transformer) when translating bare nouns in Yor\\`ub\\'a into\nEnglish. We investigate how the systems what extent they identify BNs,\ncorrectly translate them, and compare with human translation patterns. We also\nanalyze the type of errors each model makes and provide a linguistic\ndescription of these errors. We glean insights for evaluating model performance\nin low-resource settings. In translating bare nouns, our results show the\ntransformer model outperforms the SMT and BiLSTM models for 4 categories, the\nBiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the\nNMT models for 1 category.", "published": "2021-03-07 01:24:09", "link": "http://arxiv.org/abs/2103.04225v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Usability Evaluation for Online Professional Search in the Dutch\n  Archaeology Domain", "abstract": "This paper presents AGNES, the first information retrieval system for\narchaeological grey literature, allowing full-text search of these long\narchaeological documents.\n  This search system has a web interface that allows archaeology professionals\nand scholars to search through a collection of over 60,000 Dutch excavation\nreports, totalling 361 million words. We conducted a user study for the\nevaluation of AGNES's search interface, with a small but diverse user group.\nThe evaluation was done by screen capturing and a think aloud protocol,\ncombined with a user interface feedback questionnaire. The evaluation covered\nboth controlled use (completion of a pre-defined task) as well as free use\n(completion of a freely chosen task). The free use allows us to study the\ninformation needs of archaeologists, as well as their interactions with the\nsearch system. We conclude that: (1) the information needs of archaeologists\nare typically recall-oriented, often requiring a list of items as answer; (2)\nthe users prefer the use of free-text queries over metadata filters, confirming\nthe value of a free-text search system; (3) the compilation of a diverse user\ngroup contributed to the collection of diverse issues as feedback for improving\nthe system. We are currently refining AGNES's user interface and improving its\nprecision for archaeological entities, so that AGNES will help archaeologists\nto answer their research questions more effectively and efficiently, leading to\na more coherent narrative of the past.", "published": "2021-03-07 19:48:35", "link": "http://arxiv.org/abs/2103.04437v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "HTMD-Net: A Hybrid Masking-Denoising Approach to Time-Domain Monaural\n  Singing Voice Separation", "abstract": "The advent of deep learning has led to the prevalence of deep neural network\narchitectures for monaural music source separation, with end-to-end approaches\nthat operate directly on the waveform level increasingly receiving research\nattention. Among these approaches, transformation of the input mixture to a\nlearned latent space, and multiplicative application of a soft mask to the\nlatent mixture, achieves the best performance, but is prone to the introduction\nof artifacts to the source estimate. To alleviate this problem, in this paper\nwe propose a hybrid time-domain approach, termed the HTMD-Net, combining a\nlightweight masking component and a denoising module, based on skip\nconnections, in order to refine the source estimated by the masking procedure.\nEvaluation of our approach in the task of monaural singing voice separation in\nthe musdb18 dataset indicates that our proposed method achieves competitive\nperformance compared to methods based purely on masking when trained under the\nsame conditions, especially regarding the behavior during silent segments,\nwhile achieving higher computational efficiency.", "published": "2021-03-07 12:24:37", "link": "http://arxiv.org/abs/2103.04336v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Optimized Signal Processing Pipeline for Syllable Detection and\n  Speech Rate Estimation", "abstract": "Syllable detection is an important speech analysis task with applications in\nspeech rate estimation, word segmentation, and automatic prosody detection.\nBased on the well understood acoustic correlates of speech articulation, it has\nbeen realized by local peak picking on a frequency-weighted energy contour that\nrepresents vowel sonority. While several of the analysis parameters are set\nbased on known speech signal properties, the selection of the\nfrequency-weighting coefficients and peak-picking threshold typically involves\nheuristics, raising the possibility of data-based optimisation. In this work,\nwe consider the optimization of the parameters based on the direct minimization\nof naturally arising task-specific objective functions. The resulting\nnon-convex cost function is minimized using a population-based search algorithm\nto achieve a performance that exceeds previously published performance results\non the same corpus using a relatively low amount of labeled data. Further, the\noptimisation of system parameters on a different corpus is shown to result in\nan explainable change in the optimal values.", "published": "2021-03-07 13:06:21", "link": "http://arxiv.org/abs/2103.04346v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CNN-based Spoken Term Detection and Localization without Dynamic\n  Programming", "abstract": "In this paper, we propose a spoken term detection algorithm for simultaneous\nprediction and localization of in-vocabulary and out-of-vocabulary terms within\nan audio segment. The proposed algorithm infers whether a term was uttered\nwithin a given speech signal or not by predicting the word embeddings of\nvarious parts of the speech signal and comparing them to the word embedding of\nthe desired term. The algorithm utilizes an existing embedding space for this\ntask and does not need to train a task-specific embedding space. At inference\nthe algorithm simultaneously predicts all possible locations of the target term\nand does not need dynamic programming for optimal search. We evaluate our\nsystem on several spoken term detection tasks on read speech corpora.", "published": "2021-03-07 14:50:58", "link": "http://arxiv.org/abs/2103.05468v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
