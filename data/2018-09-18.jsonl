{"title": "Analysis of Bag-of-n-grams Representation's Properties Based on Textual\n  Reconstruction", "abstract": "Despite its simplicity, bag-of-n-grams sen- tence representation has been\nfound to excel in some NLP tasks. However, it has not re- ceived much attention\nin recent years and fur- ther analysis on its properties is necessary. We\npropose a framework to investigate the amount and type of information captured\nin a general- purposed bag-of-n-grams sentence represen- tation. We first use\nsentence reconstruction as a tool to obtain bag-of-n-grams representa- tion\nthat contains general information of the sentence. We then run prediction tasks\n(sen- tence length, word content, phrase content and word order) using the\nobtained representation to look into the specific type of information captured\nin the representation. Our analysis demonstrates that bag-of-n-grams\nrepresenta- tion does contain sentence structure level in- formation. However,\nincorporating n-grams with higher order n empirically helps little with\nencoding more information in general, except for phrase content information.", "published": "2018-09-18 02:09:56", "link": "http://arxiv.org/abs/1809.06502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Universal Sentence Representations with Mean-Max Attention\n  Autoencoder", "abstract": "In order to learn universal sentence representations, previous methods focus\non complex recurrent neural networks or supervised learning. In this paper, we\npropose a mean-max attention autoencoder (mean-max AAE) within the\nencoder-decoder framework. Our autoencoder rely entirely on the MultiHead\nself-attention mechanism to reconstruct the input sequence. In the encoding we\npropose a mean-max strategy that applies both mean and max pooling operations\nover the hidden vectors to capture diverse information of the input. To enable\nthe information to steer the reconstruction process dynamically, the decoder\nperforms attention over the mean-max representation. By training our model on a\nlarge collection of unlabelled data, we obtain high-quality representations of\nsentences. Experimental results on a broad range of 10 transfer tasks\ndemonstrate that our model outperforms the state-of-the-art unsupervised single\nmethods, including the classical skip-thoughts and the advanced\nskip-thoughts+LN model. Furthermore, compared with the traditional recurrent\nneural network, our mean-max AAE greatly reduce the training time.", "published": "2018-09-18 08:34:12", "link": "http://arxiv.org/abs/1809.06590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Attentional Encoder-Decoder Model and Bidirectional Beam\n  Search for Abstractive Summarization", "abstract": "Sequence generative models with RNN variants, such as LSTM, GRU, show\npromising performance on abstractive document summarization. However, they\nstill have some issues that limit their performance, especially while deal-ing\nwith long sequences. One of the issues is that, to the best of our knowledge,\nall current models employ a unidirectional decoder, which reasons only about\nthe past and still limited to retain future context while giving a prediction.\nThis makes these models suffer on their own by generating unbalanced outputs.\nMoreover, unidirec-tional attention-based document summarization can only\ncapture partial aspects of attentional regularities due to the inherited\nchallenges in document summarization. To this end, we propose an end-to-end\ntrainable bidirectional RNN model to tackle the aforementioned issues. The\nmodel has a bidirectional encoder-decoder architecture; in which the encoder\nand the decoder are bidirectional LSTMs. The forward decoder is initialized\nwith the last hidden state of the backward encoder while the backward decoder\nis initialized with the last hidden state of the for-ward encoder. In addition,\na bidirectional beam search mechanism is proposed as an approximate inference\nalgo-rithm for generating the output summaries from the bidi-rectional model.\nThis enables the model to reason about the past and future and to generate\nbalanced outputs as a result. Experimental results on CNN / Daily Mail dataset\nshow that the proposed model outperforms the current abstractive\nstate-of-the-art models by a considerable mar-gin.", "published": "2018-09-18 12:18:22", "link": "http://arxiv.org/abs/1809.06662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RumourEval 2019: Determining Rumour Veracity and Support for Rumours", "abstract": "This is the proposal for RumourEval-2019, which will run in early 2019 as\npart of that year's SemEval event. Since the first RumourEval shared task in\n2017, interest in automated claim validation has greatly increased, as the\ndangers of \"fake news\" have become a mainstream concern. Yet automated support\nfor rumour checking remains in its infancy. For this reason, it is important\nthat a shared task in this area continues to provide a focus for effort, which\nis likely to increase. We therefore propose a continuation in which the\nveracity of further rumours is determined, and as previously, supportive of\nthis goal, tweets discussing them are classified according to the stance they\ntake regarding the rumour. Scope is extended compared with the first\nRumourEval, in that the dataset is substantially expanded to include Reddit as\nwell as Twitter data, and additional languages are also included.", "published": "2018-09-18 13:12:21", "link": "http://arxiv.org/abs/1809.06683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation", "abstract": "In this paper, we empirically evaluate the utility of transfer and multi-task\nlearning on a challenging semantic classification task: semantic interpretation\nof noun--noun compounds. Through a comprehensive series of experiments and\nin-depth error analysis, we show that transfer learning via parameter\ninitialization and multi-task learning via parameter sharing can help a neural\nclassification model generalize over a highly skewed distribution of relations.\nFurther, we demonstrate how dual annotation with two distinct sets of relations\nover the same set of compounds can be exploited to improve the overall accuracy\nof a neural classifier and its F1 scores on the less frequent, but more\ndifficult relations.", "published": "2018-09-18 14:01:22", "link": "http://arxiv.org/abs/1809.06748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Conversations by Modeling,Filtering,and Optimizing for Coherence\n  and Diversity", "abstract": "We present three enhancements to existing encoder-decoder models for\nopen-domain conversational agents, aimed at effectively modeling coherence and\npromoting output diversity: (1) We introduce a measure of coherence as the\nGloVe embedding similarity between the dialogue context and the generated\nresponse, (2) we filter our training corpora based on the measure of coherence\nto obtain topically coherent and lexically diverse context-response pairs, (3)\nwe then train a response generator using a conditional variational autoencoder\nmodel that incorporates the measure of coherence as a latent variable and uses\na context gate to guarantee topical consistency with the context and promote\nlexical diversity. Experiments on the OpenSubtitles corpus show a substantial\nimprovement over competitive neural models in terms of BLEU score as well as\nmetrics of coherence and diversity.", "published": "2018-09-18 18:08:19", "link": "http://arxiv.org/abs/1809.06873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Moderation of Online Discussions via Interpretable Neural\n  Models", "abstract": "Growing amount of comments make online discussions difficult to moderate by\nhuman moderators only. Antisocial behavior is a common occurrence that often\ndiscourages other users from participating in discussion. We propose a neural\nnetwork based method that partially automates the moderation process. It\nconsists of two steps. First, we detect inappropriate comments for moderators\nto see. Second, we highlight inappropriate parts within these comments to make\nthe moderation faster. We evaluated our method on data from a major Slovak news\ndiscussion platform.", "published": "2018-09-18 19:45:25", "link": "http://arxiv.org/abs/1809.06906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Triad-based Neural Network for Coreference Resolution", "abstract": "We propose a triad-based neural network system that generates affinity scores\nbetween entity mentions for coreference resolution. The system simultaneously\naccepts three mentions as input, taking mutual dependency and logical\nconstraints of all three mentions into account, and thus makes more accurate\npredictions than the traditional pairwise approach. Depending on system\nchoices, the affinity scores can be further used in clustering or mention\nranking. Our experiments show that a standard hierarchical clustering using the\nscores produces state-of-art results with gold mentions on the English portion\nof CoNLL 2012 Shared Task. The model does not rely on many handcrafted features\nand is easy to train and use. The triads can also be easily extended to polyads\nof higher orders. To our knowledge, this is the first neural network system to\nmodel mutual dependency of more than two members at mention level.", "published": "2018-09-18 00:38:30", "link": "http://arxiv.org/abs/1809.06491v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automatic Judgment Prediction via Legal Reading Comprehension", "abstract": "Automatic judgment prediction aims to predict the judicial results based on\ncase materials. It has been studied for several decades mainly by lawyers and\njudges, considered as a novel and prospective application of artificial\nintelligence techniques in the legal field. Most existing methods follow the\ntext classification framework, which fails to model the complex interactions\namong complementary case materials. To address this issue, we formalize the\ntask as Legal Reading Comprehension according to the legal scenario. Following\nthe working protocol of human judges, LRC predicts the final judgment results\nbased on three types of information, including fact description, plaintiffs'\npleas, and law articles. Moreover, we propose a novel LRC model, AutoJudge,\nwhich captures the complex semantic interactions among facts, pleas, and laws.\nIn experiments, we construct a real-world civil case dataset for LRC.\nExperimental results on this dataset demonstrate that our model achieves\nsignificant improvement over state-of-the-art models. We will publish all\nsource codes and datasets of this work on \\urlgithub.com for further research.", "published": "2018-09-18 05:26:40", "link": "http://arxiv.org/abs/1809.06537v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "User Information Augmented Semantic Frame Parsing using Coarse-to-Fine\n  Neural Networks", "abstract": "Semantic frame parsing is a crucial component in spoken language\nunderstanding (SLU) to build spoken dialog systems. It has two main tasks:\nintent detection and slot filling. Although state-of-the-art approaches showed\ngood results, they require large annotated training data and long training\ntime. In this paper, we aim to alleviate these drawbacks for semantic frame\nparsing by utilizing the ubiquitous user information. We design a novel\ncoarse-to-fine deep neural network model to incorporate prior knowledge of user\ninformation intermediately to better and quickly train a semantic frame parser.\nDue to the lack of benchmark dataset with real user information, we synthesize\nthe simplest type of user information (location and time) on ATIS benchmark\ndata. The results show that our approach leverages such simple user information\nto outperform state-of-the-art approaches by 0.25% for intent detection and\n0.31% for slot filling using standard training data. When using smaller\ntraining data, the performance improvement on intent detection and slot filling\nreaches up to 1.35% and 1.20% respectively. We also show that our approach can\nachieve similar performance as state-of-the-art approaches by using less than\n80% annotated training data. Moreover, the training time to achieve the similar\nperformance is also reduced by over 60%.", "published": "2018-09-18 07:08:59", "link": "http://arxiv.org/abs/1809.06559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lung Cancer Concept Annotation from Spanish Clinical Narratives", "abstract": "Recent rapid increase in the generation of clinical data and rapid\ndevelopment of computational science make us able to extract new insights from\nmassive datasets in healthcare industry. Oncological clinical notes are\ncreating rich databases for documenting patients history and they potentially\ncontain lots of patterns that could help in better management of the disease.\nHowever, these patterns are locked within free text (unstructured) portions of\nclinical documents and consequence in limiting health professionals to extract\nuseful information from them and to finally perform Query and Answering (QA)\nprocess in an accurate way. The Information Extraction (IE) process requires\nNatural Language Processing (NLP) techniques to assign semantics to these\npatterns. Therefore, in this paper, we analyze the design of annotators for\nspecific lung cancer concepts that can be integrated over Apache Unstructured\nInformation Management Architecture (UIMA) framework. In addition, we explain\nthe details of generation and storage of annotation outcomes.", "published": "2018-09-18 10:55:03", "link": "http://arxiv.org/abs/1809.06639v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Talking to myself: self-dialogues as data for conversational agents", "abstract": "Conversational agents are gaining popularity with the increasing ubiquity of\nsmart devices. However, training agents in a data driven manner is challenging\ndue to a lack of suitable corpora. This paper presents a novel method for\ngathering topical, unstructured conversational data in an efficient way:\nself-dialogues through crowd-sourcing. Alongside this paper, we include a\ncorpus of 3.6 million words across 23 topics. We argue the utility of the\ncorpus by comparing self-dialogues with standard two-party conversations as\nwell as data from other corpora.", "published": "2018-09-18 11:09:49", "link": "http://arxiv.org/abs/1809.06641v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind Your POV: Convergence of Articles and Editors Towards Wikipedia's\n  Neutrality Norm", "abstract": "Wikipedia has a strong norm of writing in a 'neutral point of view' (NPOV).\nArticles that violate this norm are tagged, and editors are encouraged to make\ncorrections. But the impact of this tagging system has not been quantitatively\nmeasured. Does NPOV tagging help articles to converge to the desired style? Do\nNPOV corrections encourage editors to adopt this style? We study these\nquestions using a corpus of NPOV-tagged articles and a set of lexicons\nassociated with biased language. An interrupted time series analysis shows that\nafter an article is tagged for NPOV, there is a significant decrease in biased\nlanguage in the article, as measured by several lexicons. However, for\nindividual editors, NPOV corrections and talk page discussions yield no\nsignificant change in the usage of words in most of these lexicons, including\nWikipedia's own list of 'words to watch.' This suggests that NPOV tagging and\ndiscussion does improve content, but has less success enculturating editors to\nthe site's linguistic norms.", "published": "2018-09-18 22:03:48", "link": "http://arxiv.org/abs/1809.06951v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning with Sample Re-weighting for Machine Reading\n  Comprehension", "abstract": "We propose a multi-task learning framework to learn a joint Machine Reading\nComprehension (MRC) model that can be applied to a wide range of MRC tasks in\ndifferent domains. Inspired by recent ideas of data selection in machine\ntranslation, we develop a novel sample re-weighting scheme to assign\nsample-specific weights to the loss. Empirical study shows that our approach\ncan be applied to many existing MRC models. Combined with contextual\nrepresentations from pre-trained language models (such as ELMo), we achieve new\nstate-of-the-art results on a set of MRC benchmark datasets. We release our\ncode at https://github.com/xycforgithub/MultiTask-MRC.", "published": "2018-09-18 23:44:03", "link": "http://arxiv.org/abs/1809.06963v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FRAGE: Frequency-Agnostic Word Representation", "abstract": "Continuous word representation (aka word embedding) is a basic building block\nin many neural network-based models used in natural language processing tasks.\nAlthough it is widely accepted that words with similar semantics should be\nclose to each other in the embedding space, we find that word embeddings\nlearned in several tasks are biased towards word frequency: the embeddings of\nhigh-frequency and low-frequency words lie in different subregions of the\nembedding space, and the embedding of a rare word and a popular word can be far\nfrom each other even if they are semantically similar. This makes learned word\nembeddings ineffective, especially for rare words, and consequently limits the\nperformance of these neural network models. In this paper, we develop a neat,\nsimple yet effective way to learn \\emph{FRequency-AGnostic word Embedding}\n(FRAGE) using adversarial training. We conducted comprehensive studies on ten\ndatasets across four natural language processing tasks, including word\nsimilarity, language modeling, machine translation and text classification.\nResults show that with FRAGE, we achieve higher performance than the baselines\nin all tasks.", "published": "2018-09-18 13:31:22", "link": "http://arxiv.org/abs/1809.06858v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Language Identification with Deep Bottleneck Features", "abstract": "In this paper we proposed an end-to-end short utterances speech language\nidentification(SLD) approach based on a Long Short Term Memory (LSTM) neural\nnetwork which is special suitable for SLD application in intelligent vehicles.\nFeatures used for LSTM learning are generated by a transfer learning method.\nBottle-neck features of a deep neural network (DNN) which are trained for\nmandarin acoustic-phonetic classification are used for LSTM training. In order\nto improve the SLD accuracy of short utterances a phase vocoder based\ntime-scale modification(TSM) method is used to reduce and increase speech rated\nof the test utterance. By splicing the normal, speech rate reduced and\nincreased utterances, we can extend length of test utterances so as to improved\nimproved the performance of the SLD system. The experimental results on\nAP17-OLR database shows that the proposed methods can improve the performance\nof SLD, especially on short utterance with 1s and 3s durations.", "published": "2018-09-18 19:34:54", "link": "http://arxiv.org/abs/1809.08909v2", "categories": ["cs.CL", "cs.CV", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Advancing Multi-Accented LSTM-CTC Speech Recognition using a Domain\n  Specific Student-Teacher Learning Paradigm", "abstract": "Non-native speech causes automatic speech recognition systems to degrade in\nperformance. Past strategies to address this challenge have considered model\nadaptation, accent classification with a model selection, alternate\npronunciation lexicon, etc. In this study, we consider a recurrent neural\nnetwork (RNN) with connectionist temporal classification (CTC) cost function\ntrained on multi-accent English data including US (Native), Indian and Hispanic\naccents. We exploit dark knowledge from a model trained with the multi-accent\ndata to train student models under the guidance of both a teacher model and CTC\ncost of target transcription. We show that transferring knowledge from a single\nRNN-CTC trained model toward a student model, yields better performance than\nthe stand-alone teacher model. Since the outputs of different trained CTC\nmodels are not necessarily aligned, it is not possible to simply use an\nensemble of CTC teacher models. To address this problem, we train accent\nspecific models under the guidance of a single multi-accent teacher, which\nresults in having multiple aligned and trained CTC models. Furthermore, we\ntrain a student model under the supervision of the accent-specific teachers,\nresulting in an even further complementary model, which achieves +20.1%\nrelative Character Error Rate (CER) reduction compared to the baseline trained\nwithout any teacher. Having this effective multi-accent model, we can achieve\nfurther improvement for each accent by adapting the model to each accent. Using\nthe accent specific model's outputs to regularize the adapting process (i.e., a\nknowledge distillation version of Kullback-Leibler (KL) divergence) results in\neven superior performance compared to the conventional approach using general\nteacher models.", "published": "2018-09-18 17:05:51", "link": "http://arxiv.org/abs/1809.06833v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
