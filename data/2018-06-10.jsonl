{"title": "Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading\n  Comprehension Task", "abstract": "Current evaluation metrics to question answering based machine reading\ncomprehension (MRC) systems generally focus on the lexical overlap between the\ncandidate and reference answers, such as ROUGE and BLEU. However, bias may\nappear when these metrics are used for specific question types, especially\nquestions inquiring yes-no opinions and entity lists. In this paper, we make\nadaptations on the metrics to better correlate n-gram overlap with the human\njudgment for answers to these two question types. Statistical analysis proves\nthe effectiveness of our approach. Our adaptations may provide positive\nguidance for the development of real-scene MRC systems.", "published": "2018-06-10 03:50:10", "link": "http://arxiv.org/abs/1806.03578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Task-Specific Representation Learning for Text\n  Classification in Resource Poor Languages", "abstract": "Neural network models have shown promising results for text classification.\nHowever, these solutions are limited by their dependence on the availability of\nannotated data.\n  The prospect of leveraging resource-rich languages to enhance the text\nclassification of resource-poor languages is fascinating. The performance on\nresource-poor languages can significantly improve if the resource availability\nconstraints can be offset. To this end, we present a twin Bidirectional Long\nShort Term Memory (Bi-LSTM) network with shared parameters consolidated by a\ncontrastive loss function (based on a similarity metric). The model learns the\nrepresentation of resource-poor and resource-rich sentences in a common space\nby using the similarity between their assigned annotation tags. Hence, the\nmodel projects sentences with similar tags closer and those with different tags\nfarther from each other. We evaluated our model on the classification tasks of\nsentiment analysis and emoji prediction for resource-poor languages - Hindi and\nTelugu and resource-rich languages - English and Spanish. Our model\nsignificantly outperforms the state-of-the-art approaches in both the tasks\nacross all metrics.", "published": "2018-06-10 06:09:57", "link": "http://arxiv.org/abs/1806.03590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Acoustic Word Embeddings with Temporal Context for\n  Query-by-Example Speech Search", "abstract": "We propose to learn acoustic word embeddings with temporal context for\nquery-by-example (QbE) speech search. The temporal context includes the leading\nand trailing word sequences of a word. We assume that there exist spoken word\npairs in the training database. We pad the word pairs with their original\ntemporal context to form fixed-length speech segment pairs. We obtain the\nacoustic word embeddings through a deep convolutional neural network (CNN)\nwhich is trained on the speech segment pairs with a triplet loss. Shifting a\nfixed-length analysis window through the search content, we obtain a running\nsequence of embeddings. In this way, searching for the spoken query is\nequivalent to the matching of acoustic word embeddings. The experiments show\nthat our proposed acoustic word embeddings learned with temporal context are\neffective in QbE speech search. They outperform the state-of-the-art\nframe-level feature representations and reduce run-time computation since no\ndynamic time warping is required in QbE speech search. We also find that it is\nimportant to have sufficient speech segment pairs to train the deep CNN for\neffective acoustic word embeddings.", "published": "2018-06-10 09:40:08", "link": "http://arxiv.org/abs/1806.03621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciDTB: Discourse Dependency TreeBank for Scientific Abstracts", "abstract": "Annotation corpus for discourse relations benefits NLP tasks such as machine\ntranslation and question answering. In this paper, we present SciDTB, a\ndomain-specific discourse treebank annotated on scientific articles. Different\nfrom widely-used RST-DT and PDTB, SciDTB uses dependency trees to represent\ndiscourse structure, which is flexible and simplified to some extent but do not\nsacrifice structural integrity. We discuss the labeling framework, annotation\nworkflow and some statistics about SciDTB. Furthermore, our treebank is made as\na benchmark for evaluating discourse dependency parsers, on which we provide\nseveral baselines as fundamental work.", "published": "2018-06-10 13:00:15", "link": "http://arxiv.org/abs/1806.03653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Decoding and Training Methods for Simultaneous Translation\n  in Neural Machine Translation", "abstract": "We address the problem of simultaneous translation by modifying the Neural MT\ndecoder to operate with dynamically built encoder and attention. We propose a\ntunable agent which decides the best segmentation strategy for a user-defined\nBLEU loss and Average Proportion (AP) constraint. Our agent outperforms\npreviously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova,\n2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to\nNeural MT training to better match the incremental decoding framework.", "published": "2018-06-10 13:50:17", "link": "http://arxiv.org/abs/1806.03661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Learning for Chinese Zero pronoun Resolution", "abstract": "Deep neural network models for Chinese zero pronoun resolution learn semantic\ninformation for zero pronoun and candidate antecedents, but tend to be\nshort-sighted---they often make local decisions. They typically predict\ncoreference chains between the zero pronoun and one single candidate antecedent\none link at a time, while overlooking their long-term influence on future\ndecisions. Ideally, modeling useful information of preceding potential\nantecedents is critical when later predicting zero pronoun-candidate antecedent\npairs. In this study, we show how to integrate local and global decision-making\nby exploiting deep reinforcement learning models. With the help of the\nreinforcement learning agent, our model learns the policy of selecting\nantecedents in a sequential manner, where useful information provided by\nearlier predicted antecedents could be utilized for making later coreference\ndecisions. Experimental results on OntoNotes 5.0 dataset show that our\ntechnique surpasses the state-of-the-art models.", "published": "2018-06-10 19:29:03", "link": "http://arxiv.org/abs/1806.03711v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All-in-one: Multi-task Learning for Rumour Verification", "abstract": "Automatic resolution of rumours is a challenging task that can be broken down\ninto smaller components that make up a pipeline, including rumour detection,\nrumour tracking and stance classification, leading to the final outcome of\ndetermining the veracity of a rumour. In previous work, these steps in the\nprocess of rumour verification have been developed as separate components where\nthe output of one feeds into the next. We propose a multi-task learning\napproach that allows joint training of the main and auxiliary tasks, improving\nthe performance of rumour verification. We examine the connection between the\ndataset properties and the outcomes of the multi-task learning models used.", "published": "2018-06-10 19:46:17", "link": "http://arxiv.org/abs/1806.03713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Disambiguation of Syncretism in Inflected Lexicons", "abstract": "Lexical ambiguity makes it difficult to compute various useful statistics of\na corpus. A given word form might represent any of several morphological\nfeature bundles. One can, however, use unsupervised learning (as in EM) to fit\na model that probabilistically disambiguates word forms. We present such an\napproach, which employs a neural network to smoothly model a prior distribution\nover feature bundles (even rare ones). Although this basic model does not\nconsider a token's context, that very property allows it to operate on a simple\nlist of unigram type counts, partitioning each count among different analyses\nof that unigram. We discuss evaluation metrics for this novel task and report\nresults on 5 languages.", "published": "2018-06-10 23:19:07", "link": "http://arxiv.org/abs/1806.03740v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are All Languages Equally Hard to Language-Model?", "abstract": "For general modeling methods applied to diverse languages, a natural question\nis: how well should we expect our models to work on languages with differing\ntypological profiles? In this work, we develop an evaluation framework for fair\ncross-linguistic comparison of language models, using translated text so that\nall models are asked to predict approximately the same information. We then\nconduct a study on 21 languages, demonstrating that in some languages, the\ntextual expression of the information is harder to predict with both $n$-gram\nand LSTM language models. We show complex inflectional morphology to be a cause\nof performance differences among languages.", "published": "2018-06-10 23:24:33", "link": "http://arxiv.org/abs/1806.03743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Structured Variational Autoencoder for Contextual Morphological\n  Inflection", "abstract": "Statistical morphological inflectors are typically trained on fully\nsupervised, type-level data. One remaining open research question is the\nfollowing: How can we effectively exploit raw, token-level data to improve\ntheir performance? To this end, we introduce a novel generative latent-variable\nmodel for the semi-supervised learning of inflection generation. To enable\nposterior inference over the latent variables, we derive an efficient\nvariational inference procedure based on the wake-sleep algorithm. We\nexperiment on 23 languages, using the Universal Dependencies corpora in a\nsimulated low-resource setting, and find improvements of over 10% absolute\naccuracy in some cases.", "published": "2018-06-10 23:47:53", "link": "http://arxiv.org/abs/1806.03746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Knowledge is Needed to Solve the RTE5 Textual Entailment Challenge?", "abstract": "This document gives a knowledge-oriented analysis of about 20 interesting\nRecognizing Textual Entailment (RTE) examples, drawn from the 2005 RTE5\ncompetition test set. The analysis ignores shallow statistical matching\ntechniques between T and H, and rather asks: What would it take to reasonably\ninfer that T implies H? What world knowledge would be needed for this task?\nAlthough such knowledge-intensive techniques have not had much success in RTE\nevaluations, ultimately an intelligent system should be expected to know and\ndeploy this kind of world knowledge required to perform this kind of reasoning.\n  The selected examples are typically ones which our RTE system (called BLUE)\ngot wrong and ones which require world knowledge to answer. In particular, the\nanalysis covers cases where there was near-perfect lexical overlap between T\nand H, yet the entailment was NO, i.e., examples that most likely all current\nRTE systems will have got wrong. A nice example is #341 (page 26), that\nrequires inferring from \"a river floods\" that \"a river overflows its banks\".\nSeems it should be easy, right? Enjoy!", "published": "2018-06-10 00:33:47", "link": "http://arxiv.org/abs/1806.03561v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Disease Named Entity Extraction with Character-based BiLSTM+CRF\n  in Japanese Medical Text", "abstract": "We propose an 'end-to-end' character-based recurrent neural network that\nextracts disease named entities from a Japanese medical text and simultaneously\njudges its modality as either positive or negative; i.e., the mentioned disease\nor symptom is affirmed or negated. The motivation to adopt neural networks is\nto learn effective lexical and structural representation features for Entity\nRecognition and also for Positive/Negative classification from an annotated\ncorpora without explicitly providing any rule-based or manual feature sets. We\nconfirmed the superiority of our method over previous char-based CRF or SVM\nmethods in the results.", "published": "2018-06-10 12:34:00", "link": "http://arxiv.org/abs/1806.03648v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LexNLP: Natural language processing and information extraction for legal\n  and regulatory texts", "abstract": "LexNLP is an open source Python package focused on natural language\nprocessing and machine learning for legal and regulatory text. The package\nincludes functionality to (i) segment documents, (ii) identify key text such as\ntitles and section headings, (iii) extract over eighteen types of structured\ninformation like distances and dates, (iv) extract named entities such as\ncompanies and geopolitical entities, (v) transform text into features for model\ntraining, and (vi) build unsupervised and supervised models such as word\nembedding or tagging models. LexNLP includes pre-trained models based on\nthousands of unit tests drawn from real documents available from the SEC EDGAR\ndatabase as well as various judicial and regulatory proceedings. LexNLP is\ndesigned for use in both academic research and industrial applications, and is\ndistributed at https://github.com/LexPredict/lexpredict-lexnlp.", "published": "2018-06-10 16:55:40", "link": "http://arxiv.org/abs/1806.03688v1", "categories": ["cs.CL", "cs.IR", "stat.ML", "I.2.7; F.2.2; H.3.1; H.3.3; I.7"], "primary_category": "cs.CL"}
{"title": "Deconvolution-Based Global Decoding for Neural Machine Translation", "abstract": "A great proportion of sequence-to-sequence (Seq2Seq) models for Neural\nMachine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate\ntranslation word by word following a sequential order. As the studies of\nlinguistics have proved that language is not linear word sequence but sequence\nof complex structure, translation at each step should be conditioned on the\nwhole target-side context. To tackle the problem, we propose a new NMT model\nthat decodes the sequence with the guidance of its structural prediction of the\ncontext of the target sequence. Our model generates translation based on the\nstructural prediction of the target-side context so that the translation can be\nfreed from the bind of sequential order. Experimental results demonstrate that\nour model is more competitive compared with the state-of-the-art methods, and\nthe analysis reflects that our model is also robust to translating sentences of\ndifferent lengths and it also reduces repetition with the instruction from the\ntarget-side context for decoding.", "published": "2018-06-10 17:05:31", "link": "http://arxiv.org/abs/1806.03692v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
