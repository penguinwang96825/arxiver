{"title": "How essential are unstructured clinical narratives and information\n  fusion to clinical trial recruitment?", "abstract": "Electronic health records capture patient information using structured\ncontrolled vocabularies and unstructured narrative text. While structured data\ntypically encodes lab values, encounters and medication lists, unstructured\ndata captures the physician's interpretation of the patient's condition,\nprognosis, and response to therapeutic intervention. In this paper, we\ndemonstrate that information extraction from unstructured clinical narratives\nis essential to most clinical applications. We perform an empirical study to\nvalidate the argument and show that structured data alone is insufficient in\nresolving eligibility criteria for recruiting patients onto clinical trials for\nchronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is\nessential to solving 59% of the CLL trial criteria and 77% of the prostate\ncancer trial criteria. More specifically, for resolving eligibility criteria\nwith temporal constraints, we show the need for temporal reasoning and\ninformation integration with medical events within and across unstructured\nclinical narratives and structured data.", "published": "2015-02-13 16:28:40", "link": "http://arxiv.org/abs/1502.04049v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Linear Dynamical System Model for Text", "abstract": "Low dimensional representations of words allow accurate NLP models to be\ntrained on limited annotated data. While most representations ignore words'\nlocal context, a natural way to induce context-dependent representations is to\nperform inference in a probabilistic latent-variable sequence model. Given the\nrecent success of continuous vector space word representations, we provide such\nan inference procedure for continuous states, where words' representations are\ngiven by the posterior mean of a linear dynamical system. Here, efficient\ninference can be performed using Kalman filtering. Our learning algorithm is\nextremely scalable, operating on simple cooccurrence counts for both parameter\ninitialization using the method of moments and subsequent iterations of EM. In\nour experiments, we employ our inferred word embeddings as features in standard\ntagging tasks, obtaining significant accuracy improvements. Finally, the Kalman\nfilter updates can be seen as a linear recurrent neural network. We demonstrate\nthat using the parameters of our model to initialize a non-linear recurrent\nneural network language model reduces its training time by a day and yields\nlower perplexity.", "published": "2015-02-13 18:39:29", "link": "http://arxiv.org/abs/1502.04081v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
