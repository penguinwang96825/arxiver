{"title": "Improved Paraphrase Generation via Controllable Latent Diffusion", "abstract": "Paraphrase generation strives to generate high-quality and diverse\nexpressions of a given text, a domain where diffusion models excel. Though SOTA\ndiffusion generation reconciles generation quality and diversity, textual\ndiffusion suffers from a truncation issue that hinders efficiency and quality\ncontrol. In this work, we propose \\textit{L}atent \\textit{D}iffusion\n\\textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a\ncontrollable diffusion process given a learned latent space. LDP achieves\nsuperior generation efficiency compared to its diffusion counterparts. It can\nfacilitate only input segments to ensure paraphrase semantics, improving the\nresults without external features. Experiments show that LDP better reconciles\nparaphrase generation quality and diversity than baselines. Further analysis\nshows that our method is also helpful to other similar text generations and\ndomain adaptations", "published": "2024-04-13 09:24:32", "link": "http://arxiv.org/abs/2404.08938v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Cross-Document Event Coreference Resolution Using Linear\n  Semantic Transfer and Mixed-Modality Ensembles", "abstract": "Event coreference resolution (ECR) is the task of determining whether\ndistinct mentions of events within a multi-document corpus are actually linked\nto the same underlying occurrence. Images of the events can help facilitate\nresolution when language is ambiguous. Here, we propose a multimodal\ncross-document event coreference resolution method that integrates visual and\ntextual cues with a simple linear map between vision and language models. As\nexisting ECR benchmark datasets rarely provide images for all event mentions,\nwe augment the popular ECB+ dataset with event-centric images scraped from the\ninternet and generated using image diffusion models. We establish three methods\nthat incorporate images and text for coreference: 1) a standard fused model\nwith finetuning, 2) a novel linear mapping method without finetuning and 3) an\nensembling approach based on splitting mention pairs by semantic and\ndiscourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and\nAIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish\nan upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing\nassumptions used, and establish a novel baseline on AIDA Phase 1. Our results\ndemonstrate the utility of multimodal information in ECR for certain\nchallenging coreference problems, and highlight a need for more multimodal\nresources in the coreference resolution space.", "published": "2024-04-13 10:01:58", "link": "http://arxiv.org/abs/2404.08949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OOVs in the Spotlight: How to Inflect them?", "abstract": "We focus on morphological inflection in out-of-vocabulary (OOV) conditions,\nan under-researched subtask in which state-of-the-art systems usually are less\neffective. We developed three systems: a retrograde model and two\nsequence-to-sequence (seq2seq) models based on LSTM and Transformer. For\ntesting in OOV conditions, we automatically extracted a large dataset of nouns\nin the morphologically rich Czech language, with lemma-disjoint data splits,\nand we further manually annotated a real-world OOV dataset of neologisms. In\nthe standard OOV conditions, Transformer achieves the best results, with\nincreasing performance in ensemble with LSTM, the retrograde model and\nSIGMORPHON baselines. On the real-world OOV dataset of neologisms, the\nretrograde model outperforms all neural models. Finally, our seq2seq models\nachieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022\nshared task data in the OOV evaluation (feature overlap) in the large data\ncondition. We release the Czech OOV Inflection Dataset for rigorous evaluation\nin OOV conditions. Further, we release the inflection system with the seq2seq\nmodels as a ready-to-use Python library.", "published": "2024-04-13 11:40:06", "link": "http://arxiv.org/abs/2404.08974v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Labeled Morphological Segmentation with Semi-Markov Models", "abstract": "We present labeled morphological segmentation, an alternative view of\nmorphological processing that unifies several tasks. From an annotation\nstandpoint, we additionally introduce a new hierarchy of morphotactic tagsets.\nFinally, we develop \\modelname, a discriminative morphological segmentation\nsystem that, contrary to previous work, explicitly models morphotactics. We\nshow that \\textsc{chipmunk} yields improved performance on three tasks for all\nsix languages: (i) morphological segmentation, (ii) stemming and (iii)\nmorphological tag classification. On morphological segmentation, our method\nshows absolute improvements of 2--6 points $F_1$ over the baseline.", "published": "2024-04-13 12:51:53", "link": "http://arxiv.org/abs/2404.08997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiSplit++: Easy Data Refinement for Split and Rephrase", "abstract": "The task of Split and Rephrase, which splits a complex sentence into multiple\nsimple sentences with the same meaning, improves readability and enhances the\nperformance of downstream tasks in natural language processing (NLP). However,\nwhile Split and Rephrase can be improved using a text-to-text generation\napproach that applies encoder-decoder models fine-tuned with a large-scale\ndataset, it still suffers from hallucinations and under-splitting. To address\nthese issues, this paper presents a simple and strong data refinement approach.\nHere, we create WikiSplit++ by removing instances in WikiSplit where complex\nsentences do not entail at least one of the simpler sentences and reversing the\norder of reference simple sentences. Experimental results show that training\nwith WikiSplit++ leads to better performance than training with WikiSplit, even\nwith fewer training instances. In particular, our approach yields significant\ngains in the number of splits and the entailment ratio, a proxy for measuring\nhallucinations.", "published": "2024-04-13 13:07:32", "link": "http://arxiv.org/abs/2404.09002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models\n  with Sparse Mixture of Low-Rank Adapter Experts", "abstract": "Large language models like ChatGPT have shown substantial progress in natural\nlanguage understanding and generation, proving valuable across various\ndisciplines, including the medical field. Despite advancements, challenges\npersist due to the complexity and diversity inherent in medical tasks which\noften require multi-task learning capabilities. Previous approaches, although\nbeneficial, fall short in real-world applications because they necessitate\ntask-specific annotations at inference time, limiting broader generalization.\nThis paper introduces MING-MOE, a novel Mixture-of-Expert~(MOE)-based medical\nlarge language model designed to manage diverse and complex medical tasks\nwithout requiring task-specific annotations, thus enhancing its usability\nacross extensive datasets. MING-MOE employs a Mixture of Low-Rank Adaptation\n(MoLoRA) technique, allowing for efficient parameter usage by maintaining base\nmodel parameters static while adapting through a minimal set of trainable\nparameters. We demonstrate that MING-MOE achieves state-of-the-art (SOTA)\nperformance on over 20 medical tasks, illustrating a significant improvement\nover existing models. This approach not only extends the capabilities of\nmedical language models but also improves inference efficiency.", "published": "2024-04-13 15:28:52", "link": "http://arxiv.org/abs/2404.09027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large\n  Language Models for Behavioral Simulation", "abstract": "With the rapid advancement of large language models (LLMs) for handling\ncomplex language tasks, an increasing number of studies are employing LLMs as\nagents to emulate the sequential decision-making processes of humans often\nrepresented as Markov decision-making processes (MDPs). The actions in MDPs\nadhere to specific probability distributions and require iterative sampling.\nThis arouses curiosity regarding the capacity of LLM agents to comprehend\nprobability distributions, thereby guiding the agent's behavioral\ndecision-making through probabilistic sampling and generating behavioral\nsequences. To answer the above question, we divide the problem into two main\naspects: sequence simulation with known probability distribution and sequence\nsimulation with unknown probability distribution. Our analysis indicates that\nLLM agents can understand probabilities, but they struggle with probability\nsampling. Their ability to perform probabilistic sampling can be improved to\nsome extent by integrating coding tools, but this level of sampling precision\nstill makes it difficult to simulate human behavior as agents.", "published": "2024-04-13 16:59:28", "link": "http://arxiv.org/abs/2404.09043v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Evaluation of Semantic Textual Relatedness", "abstract": "The explosive growth of online content demands robust Natural Language\nProcessing (NLP) techniques that can capture nuanced meanings and cultural\ncontext across diverse languages. Semantic Textual Relatedness (STR) goes\nbeyond superficial word overlap, considering linguistic elements and\nnon-linguistic factors like topic, sentiment, and perspective. Despite its\npivotal role, prior NLP research has predominantly focused on English, limiting\nits applicability across languages. Addressing this gap, our paper dives into\ncapturing deeper connections between sentences beyond simple word overlap.\nGoing beyond English-centric NLP research, we explore STR in Marathi, Hindi,\nSpanish, and English, unlocking the potential for information retrieval,\nmachine translation, and more. Leveraging the SemEval-2024 shared task, we\nexplore various language models across three learning paradigms: supervised,\nunsupervised, and cross-lingual. Our comprehensive methodology gains promising\nresults, demonstrating the effectiveness of our approach. This work aims to not\nonly showcase our achievements but also inspire further research in\nmultilingual STR, particularly for low-resourced languages.", "published": "2024-04-13 17:16:03", "link": "http://arxiv.org/abs/2404.09047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM In-Context Recall is Prompt Dependent", "abstract": "The proliferation of Large Language Models (LLMs) highlights the critical\nimportance of conducting thorough evaluations to discern their comparative\nadvantages, limitations, and optimal use cases. Particularly important is\nassessing their capacity to accurately retrieve information included in a given\nprompt. A model's ability to do this significantly influences how effectively\nit can utilize contextual details, thus impacting its practical efficacy and\ndependability in real-world applications.\n  Our research analyzes the in-context recall performance of various LLMs using\nthe needle-in-a-haystack method. In this approach, a factoid (the \"needle\") is\nembedded within a block of filler text (the \"haystack\"), which the model is\nasked to retrieve. We assess the recall performance of each model across\nvarious haystack lengths and with varying needle placements to identify\nperformance patterns. This study demonstrates that an LLM's recall capability\nis not only contingent upon the prompt's content but also may be compromised by\nbiases in its training data. Conversely, adjustments to model architecture,\ntraining strategy, or fine-tuning can improve performance. Our analysis\nprovides insight into LLM behavior, offering direction for the development of\nmore effective applications of LLMs.", "published": "2024-04-13 01:13:59", "link": "http://arxiv.org/abs/2404.08865v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Enhancing Health Coaching Dialogue in Low-Resource Settings", "abstract": "Health coaching helps patients identify and accomplish lifestyle-related\ngoals, effectively improving the control of chronic diseases and mitigating\nmental health conditions. However, health coaching is cost-prohibitive due to\nits highly personalized and labor-intensive nature. In this paper, we propose\nto build a dialogue system that converses with the patients, helps them create\nand accomplish specific goals, and can address their emotions with empathy.\nHowever, building such a system is challenging since real-world health coaching\ndatasets are limited and empathy is subtle. Thus, we propose a modularized\nhealth coaching dialogue system with simplified NLU and NLG frameworks combined\nwith mechanism-conditioned empathetic response generation. Through automatic\nand human evaluation, we show that our system generates more empathetic,\nfluent, and coherent responses and outperforms the state-of-the-art in NLU\ntasks while requiring less annotation. We view our approach as a key step\ntowards building automated and more accessible health coaching systems.", "published": "2024-04-13 03:23:15", "link": "http://arxiv.org/abs/2404.08888v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RoNID: New Intent Discovery with Generated-Reliable Labels and\n  Cluster-friendly Representations", "abstract": "New Intent Discovery (NID) strives to identify known and reasonably deduce\nnovel intent groups in the open-world scenario. But current methods face issues\nwith inaccurate pseudo-labels and poor representation learning, creating a\nnegative feedback loop that degrades overall model performance, including\naccuracy and the adjusted rand index. To address the aforementioned challenges,\nwe propose a Robust New Intent Discovery (RoNID) framework optimized by an\nEM-style method, which focuses on constructing reliable pseudo-labels and\nobtaining cluster-friendly discriminative representations. RoNID comprises two\nmain modules: reliable pseudo-label generation module and cluster-friendly\nrepresentation learning module. Specifically, the pseudo-label generation\nmodule assigns reliable synthetic labels by solving an optimal transport\nproblem in the E-step, which effectively provides high-quality supervised\nsignals for the input of the cluster-friendly representation learning module.\nTo learn cluster-friendly representation with strong intra-cluster compactness\nand large inter-cluster separation, the representation learning module combines\nintra-cluster and inter-cluster contrastive learning in the M-step to feed more\ndiscriminative features into the generation module. RoNID can be performed\niteratively to ultimately yield a robust model with reliable pseudo-labels and\ncluster-friendly representations. Experimental results on multiple benchmarks\ndemonstrate our method brings substantial improvements over previous\nstate-of-the-art methods by a large margin of +1~+4 points.", "published": "2024-04-13 11:58:28", "link": "http://arxiv.org/abs/2404.08977v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic\n  Knowledge and Machine Reasoning", "abstract": "Intellecta dataset emerges as an innovative synthetic dataset, engineered to\nenhance the cognitive processing capabilities of contemporary language models.\nWith a composition of 11.53 billion tokens, integrating 8.01 billion tokens of\nsynthetic data with 3.52 billion tokens of rich textbook data, Intellecta is\ncrafted to foster advanced reasoning and comprehensive educational narrative\ngeneration. Leveraging the Mixtral-8x7B-Instruct-v0.1 model, the dataset\nfacilitates the generation of complex thought processes and detailed,\ntextbook-style explanations, thus enabling language models to engage in both\ncritical thinking and profound educational discourse. This hybrid dataset\nstands as a testament to the potential of synthetic data in pushing the\nboundaries of AI, offering a repository that is not only vast and varied but\nalso refined to align with ethical standards and intellectual rigor.", "published": "2024-04-13 06:11:25", "link": "http://arxiv.org/abs/2404.13065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Model as Simulated Patients for Clinical\n  Education", "abstract": "Simulated Patients (SPs) play a crucial role in clinical medical education by\nproviding realistic scenarios for student practice. However, the high cost of\ntraining and hiring qualified SPs, along with the heavy workload and potential\nrisks they face in consistently portraying actual patients, limit students'\naccess to this type of clinical training. Consequently, the integration of\ncomputer program-based simulated patients has emerged as a valuable educational\ntool in recent years. With the rapid development of Large Language Models\n(LLMs), their exceptional capabilities in conversational artificial\nintelligence and role-playing have been demonstrated, making them a feasible\noption for implementing Virtual Simulated Patient (VSP). In this paper, we\npresent an integrated model-agnostic framework called CureFun that harnesses\nthe potential of LLMs in clinical medical education. This framework facilitates\nnatural conversations between students and simulated patients, evaluates their\ndialogue, and provides suggestions to enhance students' clinical inquiry\nskills. Through comprehensive evaluations, our approach demonstrates more\nauthentic and professional SP-scenario dialogue flows compared to other\nLLM-based chatbots, thus proving its proficiency in simulating patients.\nAdditionally, leveraging CureFun's evaluation ability, we assess several\nmedical LLMs and discuss the possibilities and limitations of using LLMs as\nvirtual doctors from the perspective of their diagnostic abilities.", "published": "2024-04-13 06:36:32", "link": "http://arxiv.org/abs/2404.13066v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Speculative Decoding for Multimodal Large Language Models", "abstract": "Inference with Multimodal Large Language Models (MLLMs) is slow due to their\nlarge-language-model backbone which suffers from memory bandwidth bottleneck\nand generates tokens auto-regressively. In this paper, we explore the\napplication of speculative decoding to enhance the inference efficiency of\nMLLMs, specifically the LLaVA 7B model. We show that a language-only model can\nserve as a good draft model for speculative decoding with LLaVA 7B, bypassing\nthe need for image tokens and their associated processing components from the\ndraft model. Our experiments across three different tasks show that speculative\ndecoding can achieve a memory-bound speedup of up to 2.37$\\times$ using a 115M\nparameter language model that we trained from scratch. Additionally, we\nintroduce a compact LLaVA draft model incorporating an image adapter, which\nshows marginal performance gains in image captioning while maintaining\ncomparable results in other tasks.", "published": "2024-04-13 00:02:36", "link": "http://arxiv.org/abs/2404.08856v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aligning the Objective of LLM-based Program Repair", "abstract": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.", "published": "2024-04-13 02:36:40", "link": "http://arxiv.org/abs/2404.08877v5", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic\n  Comprehension", "abstract": "Large language models (LLMs) has experienced exponential growth, they\ndemonstrate remarkable performance across various tasks. Notwithstanding,\ncontemporary research primarily centers on enhancing the size and quality of\npretraining data, still utilizing the next token prediction task on\nautoregressive transformer model structure. The efficacy of this task in truly\nfacilitating the model's comprehension of code logic remains questionable, we\nspeculate that it still interprets code as mere text, while human emphasizes\nthe underlying logical knowledge. In order to prove it, we introduce a new\ntask, \"Logically Equivalent Code Selection,\" which necessitates the selection\nof logically equivalent code from a candidate set, given a query code. Our\nexperimental findings indicate that current LLMs underperform in this task,\nsince they understand code by unordered bag of keywords. To ameliorate their\nperformance, we propose an advanced pretraining task, \"Next Token Prediction+\".\nThis task aims to modify the sentence embedding distribution of the LLM without\nsacrificing its generative capabilities. Our experimental results reveal that\nfollowing this pretraining, both Code Llama and StarCoder, the prevalent code\ndomain pretraining models, display significant improvements on our logically\nequivalent code selection task and the code completion task.", "published": "2024-04-13 03:11:07", "link": "http://arxiv.org/abs/2404.08885v1", "categories": ["cs.PL", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "Introducing Super RAGs in Mistral 8x7B-v1", "abstract": "The relentless pursuit of enhancing Large Language Models (LLMs) has led to\nthe advent of Super Retrieval-Augmented Generation (Super RAGs), a novel\napproach designed to elevate the performance of LLMs by integrating external\nknowledge sources with minimal structural modifications. This paper presents\nthe integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,\nand examines the resultant improvements in accuracy, speed, and user\nsatisfaction. Our methodology uses a fine-tuned instruct model setup and a\ncache tuning fork system, ensuring efficient and relevant data retrieval. The\nevaluation, conducted over several epochs, demonstrates significant\nenhancements across all metrics. The findings suggest that Super RAGs can\neffectively augment LLMs, paving the way for more sophisticated and reliable AI\nsystems. This research contributes to the field by providing empirical evidence\nof the benefits of Super RAGs and offering insights into their potential\napplications.", "published": "2024-04-13 09:33:00", "link": "http://arxiv.org/abs/2404.08940v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning", "abstract": "Recently, pre-trained vision-language models (e.g., CLIP) have shown great\npotential in few-shot learning and attracted a lot of research interest.\nAlthough efforts have been made to improve few-shot ability of CLIP, key\nfactors on the effectiveness of existing methods have not been well studied,\nlimiting further exploration of CLIP's potential in few-shot learning. In this\npaper, we first introduce a unified formulation to analyze CLIP-based few-shot\nlearning methods from a perspective of logit bias, which encourages us to learn\nan effective logit bias for further improving performance of CLIP-based\nfew-shot learning methods. To this end, we disassemble three key components\ninvolved in computation of logit bias (i.e., logit features, logit predictor,\nand logit fusion) and empirically analyze the effect on performance of few-shot\nclassification. Based on analysis of key components, this paper proposes a\nnovel AMU-Tuning method to learn effective logit bias for CLIP-based few-shot\nclassification. Specifically, our AMU-Tuning predicts logit bias by exploiting\nthe appropriate $\\underline{\\textbf{A}}$uxiliary features, which are fed into\nan efficient feature-initialized linear classifier with\n$\\underline{\\textbf{M}}$ulti-branch training. Finally, an\n$\\underline{\\textbf{U}}$ncertainty-based fusion is developed to incorporate\nlogit bias into CLIP for few-shot classification. The experiments are conducted\non several widely used benchmarks, and the results show AMU-Tuning clearly\noutperforms its counterparts while achieving state-of-the-art performance of\nCLIP-based few-shot learning without bells and whistles.", "published": "2024-04-13 10:46:11", "link": "http://arxiv.org/abs/2404.08958v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Navigating the Landscape of Large Language Models: A Comprehensive\n  Review and Analysis of Paradigms and Fine-Tuning Strategies", "abstract": "With the surge of ChatGPT,the use of large models has significantly\nincreased,rapidly rising to prominence across the industry and sweeping across\nthe internet. This article is a comprehensive review of fine-tuning methods for\nlarge models. This paper investigates the latest technological advancements and\nthe application of advanced methods in aspects such as task-adaptive\nfine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge\ndistillation,multi-task learning,parameter-efficient fine-tuning,and dynamic\nfine-tuning.", "published": "2024-04-13 15:03:03", "link": "http://arxiv.org/abs/2404.09022v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adapting Mental Health Prediction Tasks for Cross-lingual Learning via\n  Meta-Training and In-context Learning with Large Language Model", "abstract": "Timely identification is essential for the efficient handling of mental\nhealth illnesses such as depression. However, the current research fails to\nadequately address the prediction of mental health conditions from social media\ndata in low-resource African languages like Swahili. This study introduces two\ndistinct approaches utilising model-agnostic meta-learning and leveraging large\nlanguage models (LLMs) to address this gap. Experiments are conducted on three\ndatasets translated to low-resource language and applied to four mental health\ntasks, which include stress, depression, depression severity and suicidal\nideation prediction. we first apply a meta-learning model with\nself-supervision, which results in improved model initialisation for rapid\nadaptation and cross-lingual transfer. The results show that our meta-trained\nmodel performs significantly better than standard fine-tuning methods,\noutperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\%\nover XLM-R and mBERT. In parallel, we use LLMs' in-context learning\ncapabilities to assess their performance accuracy across the Swahili mental\nhealth prediction tasks by analysing different cross-lingual prompting\napproaches. Our analysis showed that Swahili prompts performed better than\ncross-lingual prompts but less than English prompts. Our findings show that\nin-context learning can be achieved through cross-lingual transfer through\ncarefully crafted prompt templates with examples and instructions.", "published": "2024-04-13 17:11:35", "link": "http://arxiv.org/abs/2404.09045v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants", "abstract": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, the use of these tools can inadvertently expose the developer's\nproprietary code to the code assistant service provider during the development\nprocess. In this work, we propose a method to mitigate the risk of code leakage\nwhen using LLM-based code assistants. CodeCloak is a novel deep reinforcement\nlearning agent that manipulates the prompts before sending them to the code\nassistant service. CodeCloak aims to achieve the following two contradictory\ngoals: (i) minimizing code leakage, while (ii) preserving relevant and useful\nsuggestions for the developer. Our evaluation, employing StarCoder and Code\nLlama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness\non a diverse set of code repositories of varying sizes, as well as its\ntransferability across different models. We also designed a method for\nreconstructing the developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, to\nthoroughly analyze code leakage risks and evaluate the effectiveness of\nCodeCloak under practical development scenarios.", "published": "2024-04-13 19:30:58", "link": "http://arxiv.org/abs/2404.09066v3", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.CR"}
{"title": "CuriousLLM: Elevating Multi-Document Question Answering with\n  LLM-Enhanced Knowledge Graph Reasoning", "abstract": "Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.", "published": "2024-04-13 20:43:46", "link": "http://arxiv.org/abs/2404.09077v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic In-Domain Product Identification for Search Queries", "abstract": "Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to >25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a >50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.", "published": "2024-04-13 22:18:14", "link": "http://arxiv.org/abs/2404.09091v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal\n  Pre-Training Approach", "abstract": "In the contemporary era of widespread online recruitment, resume\nunderstanding has been widely acknowledged as a fundamental and crucial task,\nwhich aims to extract structured information from resume documents\nautomatically. Compared to the traditional rule-based approaches, the\nutilization of recently proposed pre-trained document understanding models can\ngreatly enhance the effectiveness of resume understanding. The present\napproaches have, however, disregarded the hierarchical relations within the\nstructured information presented in resumes, and have difficulty parsing\nresumes in an efficient manner. To this end, in this paper, we propose a novel\nmodel, namely ERU, to achieve efficient resume understanding. Specifically, we\nfirst introduce a layout-aware multi-modal fusion transformer for encoding the\nsegments in the resume with integrated textual, visual, and layout information.\nThen, we design three self-supervised tasks to pre-train this module via a\nlarge number of unlabeled resumes. Next, we fine-tune the model with a\nmulti-granularity sequence labeling task to extract structured information from\nresumes. Finally, extensive experiments on a real-world dataset clearly\ndemonstrate the effectiveness of ERU.", "published": "2024-04-13 14:31:24", "link": "http://arxiv.org/abs/2404.13067v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal\n  LLM", "abstract": "In e-commerce, accurately extracting product attribute values from multimodal\ndata is crucial for improving user experience and operational efficiency of\nretailers. However, previous approaches to multimodal attribute value\nextraction often struggle with implicit attribute values embedded in images or\ntext, rely heavily on extensive labeled data, and can easily confuse similar\nattribute values. To address these issues, we introduce EIVEN, a data- and\nparameter-efficient generative framework that pioneers the use of multimodal\nLLM for implicit attribute value extraction. EIVEN leverages the rich inherent\nknowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled\ndata. We also introduce a novel Learning-by-Comparison technique to reduce\nmodel confusion by enforcing attribute value comparison and difference\nidentification. Additionally, we construct initial open-source datasets for\nmultimodal implicit attribute value extraction. Our extensive experiments\nreveal that EIVEN significantly outperforms existing methods in extracting\nimplicit attribute values while requiring less labeled data.", "published": "2024-04-13 03:15:56", "link": "http://arxiv.org/abs/2404.08886v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Voice Attribute Editing with Text Prompt", "abstract": "Despite recent advancements in speech generation with text prompt providing\ncontrol over speech style, voice attributes in synthesized speech remain\nelusive and challenging to control. This paper introduces a novel task: voice\nattribute editing with text prompt, with the goal of making relative\nmodifications to voice attributes according to the actions described in the\ntext prompt. To solve this task, VoxEditor, an end-to-end generative model, is\nproposed. In VoxEditor, addressing the insufficiency of text prompt, a Residual\nMemory (ResMem) block is designed, that efficiently maps voice attributes and\nthese descriptors into the shared feature space. Additionally, the ResMem block\nis enhanced with a voice attribute degree prediction (VADP) block to align\nvoice attributes with corresponding descriptors, addressing the imprecision of\ntext prompt caused by non-quantitative descriptions of voice attributes. We\nalso establish the open-source VCTK-RVA dataset, which leads the way in manual\nannotations detailing voice characteristic differences among different\nspeakers. Extensive experiments demonstrate the effectiveness and\ngeneralizability of our proposed method in terms of both objective and\nsubjective metrics. The dataset and audio samples are available on the website.", "published": "2024-04-13 00:07:40", "link": "http://arxiv.org/abs/2404.08857v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tailors: New Music Timbre Visualizer to Entertain Music Through Imagery", "abstract": "In this paper, I have implemented a timbre visualization system called\nTailors. Through the experiment with 27 MIR users, Tailors was found to be\neffective in conveying timbral warmth, brightness, depth, shallowness,\nhardness, roughness, and sharpness features of music compared to the only music\ncondition and basic visualization. All scores of Tailors in the music imagery\nand music entertainment surveys were valued highest among the three conditions.\nMultiple linear regression analysis between timbre-imagery and\nimagery-entertainment shows significant and positive correlations. Coefficients\ncomparing results from Fisher Transformation show that Tailors made user's\nmusic entertainment better through improved music visual imagery. The\npost-survey result represents that Tailors ranked first for the best timbre\nexpression, music experience, and willingness to use it again. While some users\nfelt a burden in the eye, Tailors left the future work of the data-driven\napproach of the mapping rule of timbre visualization to gain consent from many\nusers. Furthermore, reducing timbre features to focus on features that Tailors\ncan express well was also discussed, with future work of Tailors in a more\nartistic way using the sense of space.", "published": "2024-04-13 02:18:05", "link": "http://arxiv.org/abs/2404.15181v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "J.5"], "primary_category": "cs.SD"}
