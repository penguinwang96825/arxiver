{"title": "Addressing Topic Leakage in Cross-Topic Evaluation for Authorship\n  Verification", "abstract": "Authorship verification (AV) aims to identify whether a pair of texts has the\nsame author. We address the challenge of evaluating AV models' robustness\nagainst topic shifts. The conventional evaluation assumes minimal topic overlap\nbetween training and test data. However, we argue that there can still be topic\nleakage in test data, causing misleading model performance and unstable\nrankings. To address this, we propose an evaluation method called\nHeterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset\nwith a heterogeneously distributed topic set. Our experimental results\ndemonstrate that HITS-sampled datasets yield a more stable ranking of models\nacross random seeds and evaluation splits. Our contributions include: 1. An\nanalysis of causes and effects of topic leakage. 2. A demonstration of the HITS\nin reducing the effects of topic leakage, and 3. The Robust Authorship\nVerification bENchmark (RAVEN) that allows topic shortcut test to uncover AV\nmodels' reliance on topic-specific features.", "published": "2024-07-27 04:16:11", "link": "http://arxiv.org/abs/2407.19164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Critical Period Effects in Language Acquisition through\n  Neural Language Models", "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second\nlanguage (L2) acquisition becomes harder after early childhood, and ceasing\nexposure to a first language (L1) after this period (but not before) typically\ndoes not lead to substantial loss of L1 proficiency. It is unknown whether\nthese CP effects result from innately determined brain maturation or as a\nstabilization of neural connections naturally induced by experience. In this\nstudy, we use language models (LMs) to test the extent to which these phenomena\nare peculiar to humans, or shared by a broader class of language learners. We\nvary the age of exposure by training LMs on language pairs in various\nexperimental conditions, and find that LMs, which lack any direct analog to\ninnate maturational stages, do not show CP effects when the age of exposure of\nL2 is delayed. Our results contradict the claim that CP effects are an\ninevitable result of statistical learning, and they are consistent with an\ninnate mechanism for CP effects. We show that we can reverse-engineer the CP by\nintroducing a regularizer partway through training to simulate a maturational\ndecrease in plasticity. All in all, our results suggest that L1 learning on its\nown may not be enough to induce a CP, and additional engineering is necessary\nto make language models more cognitively plausible.", "published": "2024-07-27 19:17:10", "link": "http://arxiv.org/abs/2407.19325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity\n  Measurement of Persian Social Networks Informal Texts", "abstract": "One fundamental task for NLP is to determine the similarity between two texts\nand evaluate the extent of their likeness. The previous methods for the Persian\nlanguage have low accuracy and are unable to comprehend the structure and\nmeaning of texts effectively. Additionally, these methods primarily focus on\nformal texts, but in real-world applications of text processing, there is a\nneed for robust methods that can handle colloquial texts. This requires\nalgorithms that consider the structure and significance of words based on\ncontext, rather than just the frequency of words. The lack of a proper dataset\nfor this task in the Persian language makes it important to develop such\nalgorithms and construct a dataset for Persian text. This paper introduces a\nnew transformer-based model to measure semantic similarity between Persian\ninformal short texts from social networks. In addition, a Persian dataset named\nFarSSiM has been constructed for this purpose, using real data from social\nnetworks and manually annotated and verified by a linguistic expert team. The\nproposed model involves training a large language model using the BERT\narchitecture from scratch. This model, called FarSSiBERT, is pre-trained on\napproximately 104 million Persian informal short texts from social networks,\nmaking it one of a kind in the Persian language. Moreover, a novel specialized\ninformal language tokenizer is provided that not only performs tokenization on\nformal texts well but also accurately identifies tokens that other Persian\ntokenizers are unable to recognize. It has been demonstrated that our proposed\nmodel outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and\nSpearman's coefficient criteria. Additionally, the pre-trained large language\nmodel has great potential for use in other NLP tasks on colloquial text and as\na tokenizer for less-known informal words.", "published": "2024-07-27 05:04:49", "link": "http://arxiv.org/abs/2407.19173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in\n  the Era of LLMs", "abstract": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders.", "published": "2024-07-27 08:00:27", "link": "http://arxiv.org/abs/2407.19200v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and\n  Implications", "abstract": "Understanding whether and to what extent large language models (LLMs) have\nmemorised training data has important implications for the reliability of their\noutput and the privacy of their training data. In order to cleanly measure and\ndisentangle memorisation from other phenomena (e.g. in-context learning), we\ncreate an experimental framework that is based on repeatedly exposing LLMs to\nrandom strings. Our framework allows us to better understand the dynamics,\ni.e., the behaviour of the model, when repeatedly exposing it to random\nstrings. Using our framework, we make several striking observations: (a) we\nfind consistent phases of the dynamics across families of models (Pythia, Phi\nand Llama2), (b) we identify factors that make some strings easier to memorise\nthan others, and (c) we identify the role of local prefixes and global context\nin memorisation. We also show that sequential exposition to different random\nstrings has a significant effect on memorisation. Our results, often\nsurprising, have significant downstream implications in the study and usage of\nLLMs.", "published": "2024-07-27 14:00:21", "link": "http://arxiv.org/abs/2407.19262v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Impact of LoRA Adapters for LLMs on Clinical NLP Classification\n  Under Data Limitations", "abstract": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to the domain gap and limited\ndata availability. This study investigates the effectiveness of various adapter\ntechniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a\nresource-constrained hospital environment. We experimented with four\nstructures-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN)-as final layers for clinical notes classification. We fine-tuned\nbiomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,\nalongside two Transformer-based models. Our extensive experimental results\nindicate that i) employing adapter structures does not yield significant\nimprovements in fine-tuning biomedical pre-trained LLMs, and ii) simpler\nTransformer-based models, trained from scratch, perform better under resource\nconstraints. Among the adapter structures, GRN demonstrated superior\nperformance with accuracy, precision, recall, and an F1 score of 0.88.\nMoreover, the total training time for LLMs exceeded 1000 hours, compared to\nunder 6 hours for simpler transformer-based models, highlighting that LLMs are\nmore suitable for environments with extensive computational resources and\nlarger datasets. Consequently, this study demonstrates that simpler\nTransformer-based models can be effectively trained from scratch, providing a\nviable solution for clinical NLP tasks in low-resource environments with\nlimited data availability. By identifying the GRN as the most effective adapter\nstructure, we offer a practical approach to enhance clinical note\nclassification without requiring extensive computational resources.", "published": "2024-07-27 16:48:03", "link": "http://arxiv.org/abs/2407.19299v1", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "IBMEA: Exploring Variational Information Bottleneck for Multi-modal\n  Entity Alignment", "abstract": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios.", "published": "2024-07-27 17:12:37", "link": "http://arxiv.org/abs/2407.19302v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning via Circular Convolution", "abstract": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large\nfoundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$\nto represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B}\n\\mathbf{A}$). This method reduces trainable parameters and mitigates heavy\nmemory consumption associated with full delta matrices by sequentially\nmultiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its\nsuccess, the intrinsic low-rank characteristic may limit its performance.\nAlthough several variants have been proposed to address this issue, they often\noverlook the crucial computational and memory efficiency brought by LoRA. In\nthis paper, we propose Circular Convolution Adaptation (C$^3$A), which not only\nachieves high-rank adaptation with enhanced performance but also excels in both\ncomputational power and memory utilization. Extensive experiments demonstrate\nthat C$^3$A consistently outperforms LoRA and its variants across various\nfine-tuning tasks.", "published": "2024-07-27 21:12:46", "link": "http://arxiv.org/abs/2407.19342v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inference-Time Selective Debiasing to Enhance Fairness in Text\n  Classification Models", "abstract": "We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories.", "published": "2024-07-27 21:56:23", "link": "http://arxiv.org/abs/2407.19345v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Polynomial Regression as a Task for Understanding In-context Learning\n  Through Finetuning and Alignment", "abstract": "Simple function classes have emerged as toy problems to better understand\nin-context-learning in transformer-based architectures used for large language\nmodels. But previously proposed simple function classes like linear regression\nor multi-layer-perceptrons lack the structure required to explore things like\nprompting and alignment within models capable of in-context-learning. We\npropose univariate polynomial regression as a function class that is just rich\nenough to study prompting and alignment, while allowing us to visualize and\nunderstand what is going on clearly.", "published": "2024-07-27 22:00:52", "link": "http://arxiv.org/abs/2407.19346v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Harmfully Manipulated Images Matter in Multimodal Misinformation\n  Detection", "abstract": "Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines.", "published": "2024-07-27 07:16:07", "link": "http://arxiv.org/abs/2407.19192v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Why Misinformation is Created? Detecting them by Integrating Intent\n  Features", "abstract": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.", "published": "2024-07-27 07:30:47", "link": "http://arxiv.org/abs/2407.19196v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards the Dynamics of a DNN Learning Symbolic Interactions", "abstract": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.", "published": "2024-07-27 07:34:49", "link": "http://arxiv.org/abs/2407.19198v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical\n  Care Medicine: A Scoping Review", "abstract": "With the rapid development of artificial intelligence (AI), large language\nmodels (LLMs) have shown strong capabilities in natural language understanding,\nreasoning, and generation, attracting amounts of research interest in applying\nLLMs to health and medicine. Critical care medicine (CCM) provides diagnosis\nand treatment for critically ill patients who often require intensive\nmonitoring and interventions in intensive care units (ICUs). Can LLMs be\napplied to CCM? Are LLMs just like stochastic parrots or ICU experts in\nassisting clinical decision-making? This scoping review aims to provide a\npanoramic portrait of the application of LLMs in CCM. Literature in seven\ndatabases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE\nXplore, and ACM Digital Library, were searched from January 1, 2019, to June\n10, 2024. Peer-reviewed journal and conference articles that discussed the\napplication of LLMs in critical care settings were included. From an initial\n619 articles, 24 were selected for final review. This review grouped\napplications of LLMs in CCM into three categories: clinical decision support,\nmedical documentation and reporting, and medical education and doctor-patient\ncommunication. LLMs have advantages in handling unstructured data and do not\nrequire manual feature engineering. Meanwhile, applying LLMs to CCM faces\nchallenges, including hallucinations, poor interpretability, bias and alignment\nchallenges, and privacy and ethics issues. Future research should enhance model\nreliability and interpretability, integrate up-to-date medical knowledge, and\nstrengthen privacy and ethical guidelines. As LLMs evolve, they could become\nkey tools in CCM to help improve patient outcomes and optimize healthcare\ndelivery. This study is the first review of LLMs in CCM, aiding researchers,\nclinicians, and policymakers to understand the current status and future\npotentials of LLMs in CCM.", "published": "2024-07-27 13:41:43", "link": "http://arxiv.org/abs/2407.19256v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LawLLM: Law Large Language Model for the US Legal System", "abstract": "In the rapidly evolving field of legal analytics, finding relevant cases and\naccurately predicting judicial outcomes are challenging because of the\ncomplexity of legal language, which often includes specialized terminology,\ncomplex syntax, and historical context. Moreover, the subtle distinctions\nbetween similar and precedent cases require a deep understanding of legal\nknowledge. Researchers often conflate these concepts, making it difficult to\ndevelop specialized techniques to effectively address these nuanced tasks. In\nthis paper, we introduce the Law Large Language Model (LawLLM), a multi-task\nmodel specifically designed for the US legal domain to address these\nchallenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case\nRecommendation (PCR), and Legal Judgment Prediction (LJP). By clearly\ndistinguishing between precedent and similar cases, we provide essential\nclarity, guiding future research in developing specialized strategies for these\ntasks. We propose customized data preprocessing techniques for each task that\ntransform raw legal data into a trainable format. Furthermore, we also use\ntechniques such as in-context learning (ICL) and advanced information retrieval\nmethods in LawLLM. The evaluation results demonstrate that LawLLM consistently\noutperforms existing baselines in both zero-shot and few-shot scenarios,\noffering unparalleled multi-task capabilities and filling critical gaps in the\nlegal domain.", "published": "2024-07-27 21:51:30", "link": "http://arxiv.org/abs/2407.21065v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment\n  of Bullying and Joking in Peer Interactions in Schools", "abstract": "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being.", "published": "2024-07-27 05:50:02", "link": "http://arxiv.org/abs/2408.01459v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "LocalValueBench: A Collaboratively Built and Extensible Benchmark for\n  Evaluating Localized Value Alignment and Ethical Safety in Large Language\n  Models", "abstract": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.", "published": "2024-07-27 05:55:42", "link": "http://arxiv.org/abs/2408.01460v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "RAVSS: Robust Audio-Visual Speech Separation in Multi-Speaker Scenarios\n  with Missing Visual Cues", "abstract": "While existing Audio-Visual Speech Separation (AVSS) methods primarily\nconcentrate on the audio-visual fusion strategy for two-speaker separation,\nthey demonstrate a severe performance drop in the multi-speaker separation\nscenarios. Typically, AVSS methods employ guiding videos to sequentially\nisolate individual speakers from the given audio mixture, resulting in notable\nmissing and noisy parts across various segments of the separated speech. In\nthis study, we propose a simultaneous multi-speaker separation framework that\ncan facilitate the concurrent separation of multiple speakers within a singular\nprocess. We introduce speaker-wise interactions to establish distinctions and\ncorrelations among speakers. Experimental results on the VoxCeleb2 and LRS3\ndatasets demonstrate that our method achieves state-of-the-art performance in\nseparating mixtures with 2, 3, 4, and 5 speakers, respectively. Additionally,\nour model can utilize speakers with complete audio-visual information to\nmitigate other visual-deficient speakers, thereby enhancing its resilience to\nmissing visual cues. We also conduct experiments where visual information for\nspecific speakers is entirely absent or visual frames are partially missing.\nThe results demonstrate that our model consistently outperforms others,\nexhibiting the smallest performance drop across all settings involving 2, 3, 4,\nand 5 speakers.", "published": "2024-07-27 09:56:23", "link": "http://arxiv.org/abs/2407.19224v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Few-shot Class Incremental Learning in Audio\n  Classification using Contrastive Representation", "abstract": "In machine learning applications, gradual data ingress is common, especially\nin audio processing where incremental learning is vital for real-time\nanalytics. Few-shot class-incremental learning addresses challenges arising\nfrom limited incoming data. Existing methods often integrate additional\ntrainable components or rely on a fixed embedding extractor post-training on\nbase sessions to mitigate concerns related to catastrophic forgetting and the\ndangers of model overfitting. However, using cross-entropy loss alone during\nbase session training is suboptimal for audio data. To address this, we propose\nincorporating supervised contrastive learning to refine the representation\nspace, enhancing discriminative power and leading to better generalization\nsince it facilitates seamless integration of incremental classes, upon arrival.\nExperimental results on NSynth and LibriSpeech datasets with 100 classes, as\nwell as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art\nperformance.", "published": "2024-07-27 14:16:25", "link": "http://arxiv.org/abs/2407.19265v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
