{"title": "Too Big to Fail: Larger Language Models are Disproportionately Resilient\n  to Induction of Dementia-Related Linguistic Anomalies", "abstract": "As artificial neural networks grow in complexity, understanding their inner\nworkings becomes increasingly challenging, which is particularly important in\nhealthcare applications. The intrinsic evaluation metrics of autoregressive\nneural language models (NLMs), perplexity (PPL), can reflect how \"surprised\" an\nNLM model is at novel input. PPL has been widely used to understand the\nbehavior of NLMs. Previous findings show that changes in PPL when masking\nattention layers in pre-trained transformer-based NLMs reflect linguistic\nanomalies associated with Alzheimer's disease dementia. Building upon this, we\nexplore a novel bidirectional attention head ablation method that exhibits\nproperties attributed to the concepts of cognitive and brain reserve in human\nbrain studies, which postulate that people with more neurons in the brain and\nmore efficient processing are more resilient to neurodegeneration. Our results\nshow that larger GPT-2 models require a disproportionately larger share of\nattention heads to be masked/ablated to display degradation of similar\nmagnitude to masking in smaller models. These results suggest that the\nattention mechanism in transformer models may present an analogue to the\nnotions of cognitive and brain reserve and could potentially be used to model\ncertain aspects of the progression of neurodegenerative disorders and aging.", "published": "2024-06-05 00:31:50", "link": "http://arxiv.org/abs/2406.02830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation", "abstract": "This research investigates the effect of prompt design on dialogue evaluation\nusing large language models (LLMs). While LLMs are increasingly used for\nscoring various inputs, creating effective prompts for dialogue evaluation\nremains challenging due to model sensitivity and subjectivity in dialogue\nassessments. Our study experimented with different prompt structures, altering\nthe sequence of output instructions and including explanatory reasons. We found\nthat the order of presenting reasons and scores significantly influences LLMs'\nscoring, with a \"reason-first\" approach yielding more comprehensive\nevaluations. This insight is crucial for enhancing the accuracy and consistency\nof LLM-based evaluations.", "published": "2024-06-05 02:25:10", "link": "http://arxiv.org/abs/2406.02863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Can Do Knowledge Tracing: Simple but Effective Method to\n  Integrate Language Model and Knowledge Tracing Task", "abstract": "Knowledge Tracing (KT) is a critical task in online learning for modeling\nstudent knowledge over time. Despite the success of deep learning-based KT\nmodels, which rely on sequences of numbers as data, most existing approaches\nfail to leverage the rich semantic information in the text of questions and\nconcepts. This paper proposes Language model-based Knowledge Tracing (LKT), a\nnovel framework that integrates pre-trained language models (PLMs) with KT\nmethods. By leveraging the power of language models to capture semantic\nrepresentations, LKT effectively incorporates textual information and\nsignificantly outperforms previous KT models on large benchmark datasets.\nMoreover, we demonstrate that LKT can effectively address the cold-start\nproblem in KT by leveraging the semantic knowledge captured by PLMs.\nInterpretability of LKT is enhanced compared to traditional KT models due to\nits use of text-rich data. We conducted the local interpretable model-agnostic\nexplanation technique and analysis of attention scores to interpret the model\nperformance further. Our work highlights the potential of integrating PLMs with\nKT and paves the way for future research in KT domain.", "published": "2024-06-05 03:26:59", "link": "http://arxiv.org/abs/2406.02893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S$^2$GSL: Incorporating Segment to Syntactic Enhanced Graph Structure\n  Learning for Aspect-based Sentiment Analysis", "abstract": "Previous graph-based approaches in Aspect based Sentiment Analysis(ABSA) have\ndemonstrated impressive performance by utilizing graph neural networks and\nattention mechanisms to learn structures of static dependency trees and dynamic\nlatent trees. However, incorporating both semantic and syntactic information\nsimultaneously within complex global structures can introduce irrelevant\ncontexts and syntactic dependencies during the process of graph structure\nlearning, potentially resulting in inaccurate predictions. In order to address\nthe issues above, we propose S$^2$GSL, incorporating Segment to Syntactic\nenhanced Graph Structure Learning for ABSA. Specifically,S$^2$GSL is featured\nwith a segment-aware semantic graph learning and a syntax-based latent graph\nlearning enabling the removal of irrelevant contexts and dependencies,\nrespectively. We further propose a self-adaptive aggregation network that\nfacilitates the fusion of two graph learning branches, thereby achieving\ncomplementarity across diverse structures. Experimental results on four\nbenchmarks demonstrate the effectiveness of our framework.", "published": "2024-06-05 03:44:35", "link": "http://arxiv.org/abs/2406.02902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Grounded Planning: Challenges and Benchmark Construction", "abstract": "The emergence of large language models (LLMs) has increasingly drawn\nattention to the use of LLMs for human-like planning. Existing work on\nLLM-based planning either focuses on leveraging the inherent language\ngeneration capabilities of LLMs to produce free-style plans, or employs\nreinforcement learning approaches to learn decision-making for a limited set of\nactions within restricted environments. However, both approaches exhibit\nsignificant discrepancies from the open and executable requirements in\nreal-world planning. In this paper, we propose a new planning task--open\ngrounded planning. The primary objective of open grounded planning is to ask\nthe model to generate an executable plan based on a variable action set,\nthereby ensuring the executability of the produced plan. To this end, we\nestablishes a benchmark for open grounded planning spanning a wide range of\ndomains. Then we test current state-of-the-art LLMs along with five planning\napproaches, revealing that existing LLMs and methods still struggle to address\nthe challenges posed by grounded planning in open domains. The outcomes of this\npaper define and establish a foundational dataset for open grounded planning,\nand shed light on the potential challenges and future directions of LLM-based\nplanning.", "published": "2024-06-05 03:46:52", "link": "http://arxiv.org/abs/2406.02903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving In-Context Learning with Prediction Feedback for Sentiment\n  Analysis", "abstract": "Large language models (LLMs) have achieved promising results in sentiment\nanalysis through the in-context learning (ICL) paradigm. However, their ability\nto distinguish subtle sentiments still remains a challenge. Inspired by the\nhuman ability to adjust understanding via feedback, this paper enhances ICL by\nincorporating prior predictions and feedback, aiming to rectify sentiment\nmisinterpretation of LLMs. Specifically, the proposed framework consists of\nthree steps: (1) acquiring prior predictions of LLMs, (2) devising predictive\nfeedback based on correctness, and (3) leveraging a feedback-driven prompt to\nrefine sentiment understanding. Experimental results across nine sentiment\nanalysis datasets demonstrate the superiority of our framework over\nconventional ICL methods, with an average F1 improvement of 5.95%.", "published": "2024-06-05 04:04:08", "link": "http://arxiv.org/abs/2406.02911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering\n  Medical Knowledge", "abstract": "Large language models (LLMs) have excelled across domains, also delivering\nnotable performance on the medical evaluation benchmarks, such as MedQA.\nHowever, there still exists a significant gap between the reported performance\nand the practical effectiveness in real-world medical scenarios. In this paper,\nwe aim to explore the causes of this gap by employing a multifaceted\nexamination schema to systematically probe the actual mastery of medical\nknowledge by current LLMs. Specifically, we develop a novel evaluation\nframework MultifacetEval to examine the degree and coverage of LLMs in encoding\nand mastering medical knowledge at multiple facets (comparison, rectification,\ndiscrimination, and verification) concurrently. Based on the MultifacetEval\nframework, we construct two multifaceted evaluation datasets: MultiDiseK (by\nproducing questions from a clinical disease knowledge base) and MultiMedQA (by\nrephrasing each question from a medical benchmark MedQA into multifaceted\nquestions). The experimental results on these multifaceted datasets demonstrate\nthat the extent of current LLMs in mastering medical knowledge is far below\ntheir performance on existing medical benchmarks, suggesting that they lack\ndepth, precision, and comprehensiveness in mastering medical knowledge.\nConsequently, current LLMs are not yet ready for application in real-world\nmedical tasks. The codes and datasets are available at\nhttps://github.com/THUMLP/MultifacetEval.", "published": "2024-06-05 04:15:07", "link": "http://arxiv.org/abs/2406.02919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Readability-guided Idiom-aware Sentence Simplification (RISS) for\n  Chinese", "abstract": "Chinese sentence simplification faces challenges due to the lack of\nlarge-scale labeled parallel corpora and the prevalence of idioms. To address\nthese challenges, we propose Readability-guided Idiom-aware Sentence\nSimplification (RISS), a novel framework that combines data augmentation\ntechniques with lexcial simplification. RISS introduces two key components: (1)\nReadability-guided Paraphrase Selection (RPS), a method for mining high-quality\nsentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances\nthe comprehension and simplification of idiomatic expressions. By integrating\nRPS and IAS using multi-stage and multi-task learning strategies, RISS\noutperforms previous state-of-the-art methods on two Chinese sentence\nsimplification datasets. Furthermore, RISS achieves additional improvements\nwhen fine-tuned on a small labeled dataset. Our approach demonstrates the\npotential for more effective and accessible Chinese text simplification.", "published": "2024-06-05 06:15:48", "link": "http://arxiv.org/abs/2406.02974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence\n  Models for Abstractive Radiology Report Summarization", "abstract": "Radiology report summarization is a crucial task that can help doctors\nquickly identify clinically significant findings without the need to review\ndetailed sections of reports. This study proposes RadBARTsum, a domain-specific\nand ontology facilitated adaptation of the BART model for abstractive radiology\nreport summarization. The approach involves two main steps: 1) re-training the\nBART model on a large corpus of radiology reports using a novel entity masking\nstrategy to improving biomedical domain knowledge learning, and 2) fine-tuning\nthe model for the summarization task using the Findings and Background sections\nto predict the Impression section. Experiments are conducted using different\nmasking strategies. Results show that the re-training process with domain\nknowledge facilitated masking improves performances consistently across various\nsettings. This work contributes a domain-specific generative language model for\nradiology report summarization and a method for utilising medical knowledge to\nrealise entity masking language model. The proposed approach demonstrates a\npromising direction of enhancing the efficiency of language models by deepening\nits understanding of clinical knowledge in radiology reports.", "published": "2024-06-05 08:43:11", "link": "http://arxiv.org/abs/2406.03062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent\n  Debate Framework", "abstract": "The advent of large language models (LLMs) has facilitated the development of\nnatural language text generation. It also poses unprecedented challenges, with\ncontent hallucination emerging as a significant concern. Existing solutions\noften involve expensive and complex interventions during the training process.\nMoreover, some approaches emphasize problem disassembly while neglecting the\ncrucial validation process, leading to performance degradation or limited\napplications. To overcome these limitations, we propose a Markov Chain-based\nmulti-agent debate verification framework to enhance hallucination detection\naccuracy in concise claims. Our method integrates the fact-checking process,\nincluding claim detection, evidence retrieval, and multi-agent verification. In\nthe verification stage, we deploy multiple agents through flexible Markov\nChain-based debates to validate individual claims, ensuring meticulous\nverification outcomes. Experimental results across three generative tasks\ndemonstrate that our approach achieves significant improvements over baselines.", "published": "2024-06-05 08:59:45", "link": "http://arxiv.org/abs/2406.03075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FragRel: Exploiting Fragment-level Relations in the External Memory of\n  Large Language Models", "abstract": "To process contexts with unlimited length using Large Language Models (LLMs),\nrecent studies explore hierarchically managing the long text. Only several text\nfragments are taken from the external memory and passed into the temporary\nworking memory, i.e., LLM's context window. However, existing approaches\nisolatedly handle the text fragments without considering their structural\nconnections, thereby suffering limited capability on texts with intensive\ninter-relations, e.g., coherent stories and code repositories. This work\nattempts to resolve this by exploiting the fragment-level relations in external\nmemory. First, we formulate the fragment-level relations and present several\ninstantiations for different text types. Next, we introduce a relation-aware\nfragment assessment criteria upon previous independent fragment assessment.\nFinally, we present the fragment-connected Hierarchical Memory based LLM. We\nvalidate the benefits of involving these relations on long story understanding,\nrepository-level code generation, and long-term chatting.", "published": "2024-06-05 09:31:37", "link": "http://arxiv.org/abs/2406.03092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Space Decomposition for Sentence Embedding", "abstract": "Determining sentence pair similarity is crucial for various NLP tasks. A\ncommon technique to address this is typically evaluated on a continuous\nsemantic textual similarity scale from 0 to 5. However, based on a linguistic\nobservation in STS annotation guidelines, we found that the score in the range\n[4,5] indicates an upper-range sample, while the rest are lower-range samples.\nThis necessitates a new approach to treating the upper-range and lower-range\nclasses separately. In this paper, we introduce a novel embedding space\ndecomposition method called MixSP utilizing a Mixture of Specialized\nProjectors, designed to distinguish and rank upper-range and lower-range\nsamples accurately. The experimental results demonstrate that MixSP decreased\nthe overlap representation between upper-range and lower-range classes\nsignificantly while outperforming competitors on STS and zero-shot benchmarks.", "published": "2024-06-05 10:20:10", "link": "http://arxiv.org/abs/2406.03125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Real-world Scenario: Imbalanced New Intent Discovery", "abstract": "New Intent Discovery (NID) aims at detecting known and previously undefined\ncategories of user intent by utilizing limited labeled and massive unlabeled\ndata. Most prior works often operate under the unrealistic assumption that the\ndistribution of both familiar and new intent classes is uniform, overlooking\nthe skewed and long-tailed distributions frequently encountered in real-world\nscenarios. To bridge the gap, our work introduces the imbalanced new intent\ndiscovery (i-NID) task, which seeks to identify familiar and novel intent\ncategories within long-tailed distributions. A new benchmark (ImbaNID-Bench)\ncomprised of three datasets is created to simulate the real-world long-tail\ndistributions. ImbaNID-Bench ranges from broad cross-domain to specific\nsingle-domain intent categories, providing a thorough representation of\npractical use cases. Besides, a robust baseline model ImbaNID is proposed to\nachieve cluster-friendly intent representations. It includes three stages:\nmodel pre-training, generation of reliable pseudo-labels, and robust\nrepresentation learning that strengthens the model performance to handle the\nintricacies of real-world data distributions. Our extensive experiments on\nprevious benchmarks and the newly established benchmark demonstrate the\nsuperior performance of ImbaNID in addressing the i-NID task, highlighting its\npotential as a powerful baseline for uncovering and categorizing user intents\nin imbalanced and long-tailed\ndistributions\\footnote{\\url{https://github.com/Zkdc/i-NID}}.", "published": "2024-06-05 10:22:27", "link": "http://arxiv.org/abs/2406.03127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StatBot.Swiss: Bilingual Open Data Exploration in Natural Language", "abstract": "The potential for improvements brought by Large Language Models (LLMs) in\nText-to-SQL systems is mostly assessed on monolingual English datasets.\nHowever, LLMs' performance for other languages remains vastly unexplored. In\nthis work, we release the StatBot.Swiss dataset, the first bilingual benchmark\nfor evaluating Text-to-SQL systems based on real-world applications. The\nStatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big\ndatabases with varying level of complexity for both English and German.\n  We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo\nand mixtral-8x7b-instruct for the Text-to-SQL translation task using an\nin-context learning approach. Our experimental analysis illustrates that\ncurrent LLMs struggle to generalize well in generating SQL queries on our novel\nbilingual dataset.", "published": "2024-06-05 12:03:19", "link": "http://arxiv.org/abs/2406.03170v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Missci: Reconstructing Fallacies in Misrepresented Science", "abstract": "Health-related misinformation on social networks can lead to poor\ndecision-making and real-world dangers. Such misinformation often misrepresents\nscientific publications and cites them as \"proof\" to gain perceived\ncredibility. To effectively counter such claims automatically, a system must\nexplain how the claim was falsely derived from the cited publication. Current\nmethods for automated fact-checking or fallacy detection neglect to assess the\n(mis)used evidence in relation to misinformation claims, which is required to\ndetect the mismatch between them. To address this gap, we introduce Missci, a\nnovel argumentation theoretical model for fallacious reasoning together with a\nnew dataset for real-world misinformation detection that misrepresents\nbiomedical publications. Unlike previous fallacy detection datasets, Missci (i)\nfocuses on implicit fallacies between the relevant content of the cited\npublication and the inaccurate claim, and (ii) requires models to verbalize the\nfallacious reasoning in addition to classifying it. We present Missci as a\ndataset to test the critical reasoning abilities of large language models\n(LLMs), that are required to reconstruct real-world fallacious arguments, in a\nzero-shot setting. We evaluate two representative LLMs and the impact of\ndifferent levels of detail about the fallacy classes provided to the LLM via\nprompts. Our experiments and human evaluation show promising results for GPT 4,\nwhile also demonstrating the difficulty of this task.", "published": "2024-06-05 12:11:10", "link": "http://arxiv.org/abs/2406.03181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Claim Extraction and Decontextualisation for\n  Fact-Checking", "abstract": "Selecting which claims to check is a time-consuming task for human\nfact-checkers, especially from documents consisting of multiple sentences and\ncontaining multiple claims. However, existing claim extraction approaches focus\nmore on identifying and extracting claims from individual sentences, e.g.,\nidentifying whether a sentence contains a claim or the exact boundaries of the\nclaim within a sentence. In this paper, we propose a method for document-level\nclaim extraction for fact-checking, which aims to extract check-worthy claims\nfrom documents and decontextualise them so that they can be understood out of\ncontext. Specifically, we first recast claim extraction as extractive\nsummarization in order to identify central sentences from documents, then\nrewrite them to include necessary context from the originating document through\nsentence decontextualisation. Evaluation with both automatic metrics and a\nfact-checking professional shows that our method is able to extract\ncheck-worthy claims from documents more accurately than previous work, while\nalso improving evidence retrieval.", "published": "2024-06-05 13:16:46", "link": "http://arxiv.org/abs/2406.03239v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement\n  Learning from Machine Feedback", "abstract": "Ensuring that online discussions are civil and productive is a major\nchallenge for social media platforms. Such platforms usually rely both on users\nand on automated detection tools to flag inappropriate arguments of other\nusers, which moderators then review. However, this kind of post-hoc moderation\nis expensive and time-consuming, and moderators are often overwhelmed by the\namount and severity of flagged content. Instead, a promising alternative is to\nprevent negative behavior during content creation. This paper studies how\ninappropriate language in arguments can be computationally mitigated. We\npropose a reinforcement learning-based rewriting approach that balances content\npreservation and appropriateness based on existing classifiers, prompting an\ninstruction-finetuned large language model (LLM) as our initial policy. Unlike\nrelated style transfer tasks, rewriting inappropriate arguments allows deleting\nand adding content permanently. It is therefore tackled on document level\nrather than sentence level. We evaluate different weighting schemes for the\nreward function in both absolute and relative human assessment studies.\nSystematic experiments on non-parallel data provide evidence that our approach\ncan mitigate the inappropriateness of arguments while largely preserving their\ncontent. It significantly outperforms competitive baselines, including few-shot\nlearning, prompting, and humans.", "published": "2024-06-05 15:18:08", "link": "http://arxiv.org/abs/2406.03363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Turkish Educational Quiz Generation Using Large Language\n  Models", "abstract": "Crafting quizzes from educational content is a pivotal activity that benefits\nboth teachers and students by reinforcing learning and evaluating\nunderstanding. In this study, we introduce a novel approach to generate quizzes\nfrom Turkish educational texts, marking a pioneering endeavor in educational\ntechnology specifically tailored to the Turkish educational context. We present\na specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive\ncollection of Turkish educational texts accompanied by multiple-choice and\nshort-answer quizzes. This research leverages the capabilities of Large\nLanguage Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo,\nLlama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz\nquestions and answers from the Turkish educational content. Our work delineates\nthe methodology for employing these LLMs in the context of Turkish educational\nmaterial, thereby opening new avenues for automated Turkish quiz generation.\nThe study not only demonstrates the efficacy of using such models for\ngenerating coherent and relevant quiz content but also sets a precedent for\nfuture research in the domain of automated educational content creation for\nlanguages other than English. The Turkish-Quiz-Instruct dataset is introduced\nas a valuable resource for researchers and practitioners aiming to explore the\nboundaries of educational technology and language-specific applications of LLMs\nin Turkish. By addressing the challenges of quiz generation in a non-English\ncontext specifically Turkish, this study contributes significantly to the field\nof Turkish educational technology, providing insights into the potential of\nleveraging LLMs for educational purposes across diverse linguistic landscapes.", "published": "2024-06-05 15:54:50", "link": "http://arxiv.org/abs/2406.03397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Synchronic Definitions and Semantic Relations to Classify Semantic\n  Change Types", "abstract": "There is abundant evidence of the fact that the way words change their\nmeaning can be classified in different types of change, highlighting the\nrelationship between the old and new meanings (among which generalization,\nspecialization and co-hyponymy transfer). In this paper, we present a way of\ndetecting these types of change by constructing a model that leverages\ninformation both from synchronic lexical relations and definitions of word\nmeanings. Specifically, we use synset definitions and hierarchy information\nfrom WordNet and test it on a digitized version of Blank's (1997) dataset of\nsemantic change types. Finally, we show how the sense relationships can improve\nmodels for both approximation of human judgments of semantic relatedness as\nwell as binary Lexical Semantic Change Detection.", "published": "2024-06-05 16:52:21", "link": "http://arxiv.org/abs/2406.03452v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization", "abstract": "The rapid proliferation of online content necessitates effective\nsummarization methods, among which dynamic aspect-based summarization stands\nout. Unlike its traditional counterpart, which assumes a fixed set of known\naspects, this approach adapts to the varied aspects of the input text. We\nintroduce a novel multi-objective learning framework employing a\nLongformer-Encoder-Decoder for this task. The framework optimizes aspect number\nprediction, minimizes disparity between generated and reference summaries for\neach aspect, and maximizes dissimilarity across aspect-specific summaries.\nExtensive experiments show our method significantly outperforms baselines on\nthree diverse datasets, largely due to the effective alignment of generated and\nreference aspect counts without sacrificing single-aspect summarization\nquality.", "published": "2024-06-05 17:32:28", "link": "http://arxiv.org/abs/2406.03479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ranking Manipulation for Conversational Search Engines", "abstract": "Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork.", "published": "2024-06-05 19:14:21", "link": "http://arxiv.org/abs/2406.03589v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TACT: Advancing Complex Aggregative Reasoning with Information\n  Extraction Tools", "abstract": "Large Language Models (LLMs) often do not perform well on queries that\nrequire the aggregation of information across texts. To better evaluate this\nsetting and facilitate modeling efforts, we introduce TACT - Text And\nCalculations through Tables, a dataset crafted to evaluate LLMs' reasoning and\ncomputational abilities using complex instructions. TACT contains challenging\ninstructions that demand stitching information scattered across one or more\ntexts, and performing complex integration on this information to generate the\nanswer. We construct this dataset by leveraging an existing dataset of texts\nand their associated tables. For each such tables, we formulate new queries,\nand gather their respective answers. We demonstrate that all contemporary LLMs\nperform poorly on this dataset, achieving an accuracy below 38%. To pinpoint\nthe difficulties and thoroughly dissect the problem, we analyze model\nperformance across three components: table-generation, Pandas\ncommand-generation, and execution. Unexpectedly, we discover that each\ncomponent presents substantial challenges for current LLMs. These insights lead\nus to propose a focused modeling framework, which we refer to as IE as a tool.\nSpecifically, we propose to add \"tools\" for each of the above steps, and\nimplement each such tool with few-shot prompting. This approach shows an\nimprovement over existing prompting techniques, offering a promising direction\nfor enhancing model capabilities in these tasks.", "published": "2024-06-05 20:32:56", "link": "http://arxiv.org/abs/2406.03618v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Harder Cross-document Event Coreference Resolution Datasets\n  using Metaphoric Paraphrasing", "abstract": "The most popular Cross-Document Event Coreference Resolution (CDEC) datasets\nfail to convey the true difficulty of the task, due to the lack of lexical\ndiversity between coreferring event triggers (words or phrases that refer to an\nevent). Furthermore, there is a dearth of event datasets for figurative\nlanguage, limiting a crucial avenue of research in event comprehension. We\naddress these two issues by introducing ECB+META, a lexically rich variant of\nEvent Coref Bank Plus (ECB+) for CDEC on symbolic and metaphoric language. We\nuse ChatGPT as a tool for the metaphoric transformation of sentences in the\ndocuments of ECB+, then tag the original event triggers in the transformed\nsentences in a semi-automated manner. In this way, we avoid the re-annotation\nof expensive coreference links. We present results that show existing methods\nthat work well on ECB+ struggle with ECB+META, thereby paving the way for CDEC\nresearch on a much more challenging dataset. Code/data:\nhttps://github.com/ahmeshaf/llms_coref", "published": "2024-06-05 17:26:45", "link": "http://arxiv.org/abs/2407.11988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Robustness in Doctor-Patient Conversation Summarization: An\n  Analysis of Out-of-Domain SOAP Notes", "abstract": "Summarizing medical conversations poses unique challenges due to the\nspecialized domain and the difficulty of collecting in-domain training data. In\nthis study, we investigate the performance of state-of-the-art doctor-patient\nconversation generative summarization models on the out-of-domain data. We\ndivide the summarization model of doctor-patient conversation into two\nconfigurations: (1) a general model, without specifying subjective (S),\nobjective (O), and assessment (A) and plan (P) notes; (2) a SOAP-oriented model\nthat generates a summary with SOAP sections. We analyzed the limitations and\nstrengths of the fine-tuning language model-based methods and GPTs on both\nconfigurations. We also conducted a Linguistic Inquiry and Word Count analysis\nto compare the SOAP notes from different datasets. The results exhibit a strong\ncorrelation for reference notes across different datasets, indicating that\nformat mismatch (i.e., discrepancies in word distribution) is not the main\ncause of performance decline on out-of-domain data. Lastly, a detailed analysis\nof SOAP notes is included to provide insights into missing information and\nhallucinations introduced by the models.", "published": "2024-06-05 00:11:20", "link": "http://arxiv.org/abs/2406.02826v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Minimum Bayes Risk Decoding using Low-Rank Matrix Completion\n  Algorithms", "abstract": "Minimum Bayes Risk (MBR) decoding is a powerful decoding strategy widely used\nfor text generation tasks, but its quadratic computational complexity limits\nits practical application. This paper presents a novel approach for\napproximating MBR decoding using matrix completion techniques, focusing on the\ntask of machine translation. We formulate MBR decoding as a matrix completion\nproblem, where the utility metric scores between candidate hypotheses and\npseudo-reference translations form a low-rank matrix. First, we empirically\nshow that the scores matrices indeed have a low-rank structure. Then, we\nexploit this by only computing a random subset of the scores and efficiently\nrecover the missing entries in the matrix by applying the Alternating Least\nSquares (ALS) algorithm, thereby enabling a fast approximation of the MBR\ndecoding process. Our experimental results on machine translation tasks\ndemonstrate that the proposed method requires 1/16 utility metric computations\ncompared to vanilla MBR decoding while achieving equal translation quality\nmeasured by COMET22 on the WMT22 dataset (en<>de and en<>ru). We also benchmark\nour method against other approximation methods and we show gains in quality\nwhen comparing to them.", "published": "2024-06-05 00:54:03", "link": "http://arxiv.org/abs/2406.02832v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Item-Language Model for Conversational Recommendation", "abstract": "Large-language Models (LLMs) have been extremely successful at tasks like\ncomplex dialogue understanding, reasoning and coding due to their emergent\nabilities. These emergent abilities have been extended with multi-modality to\ninclude image, audio, and video capabilities. Recommender systems, on the other\nhand, have been critical for information seeking and item discovery needs.\nRecently, there have been attempts to apply LLMs for recommendations. One\ndifficulty of current attempts is that the underlying LLM is usually not\ntrained on the recommender system data, which largely contains user interaction\nsignals and is often not publicly available. Another difficulty is user\ninteraction signals often have a different pattern from natural language text,\nand it is currently unclear if the LLM training setup can learn more\nnon-trivial knowledge from interaction signals compared with traditional\nrecommender system methods. Finally, it is difficult to train multiple LLMs for\ndifferent use-cases, and to retain the original language and reasoning\nabilities when learning from recommender system data. To address these three\nlimitations, we propose an Item-Language Model (ILM), which is composed of an\nitem encoder to produce text-aligned item representations that encode user\ninteraction signals, and a frozen LLM that can understand those item\nrepresentations with preserved pretrained knowledge. We conduct extensive\nexperiments which demonstrate both the importance of the language-alignment and\nof user interaction knowledge in the item encoder.", "published": "2024-06-05 01:35:50", "link": "http://arxiv.org/abs/2406.02844v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Xmodel-LM Technical Report", "abstract": "We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on around 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.", "published": "2024-06-05 02:12:06", "link": "http://arxiv.org/abs/2406.02856v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning\n  using Large Language Models", "abstract": "Numeral systems and units of measurement are two conjoined topics in\nactivities of human beings and have mutual effects with the languages\nexpressing them. Currently, the evaluation of Large Language Models (LLMs)\noften involves mathematical reasoning, yet little attention is given to how\nminor changes in numbers or units can drastically alter the complexity of\nproblems and the performance of LLMs. In this paper, we scrutinize existing\nLLMs on processing of numerals and units of measurement by constructing\ndatasets with perturbations. We first anatomize the reasoning of math word\nproblems to different sub-procedures like numeral conversions from language to\nnumbers and measurement conversions based on units. Then we further annotate\nmath word problems from ancient Chinese arithmetic works which are challenging\nin numerals and units of measurement. Experiments on perturbed datasets\ndemonstrate that LLMs still encounter difficulties in handling numeral and\nmeasurement conversions.", "published": "2024-06-05 02:26:14", "link": "http://arxiv.org/abs/2406.02864v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LCS: A Language Converter Strategy for Zero-Shot Neural Machine\n  Translation", "abstract": "Multilingual neural machine translation models generally distinguish\ntranslation directions by the language tag (LT) in front of the source or\ntarget sentences. However, current LT strategies cannot indicate the desired\ntarget language as expected on zero-shot translation, i.e., the off-target\nissue. Our analysis reveals that the indication of the target language is\nsensitive to the placement of the target LT. For example, when placing the\ntarget LT on the decoder side, the indication would rapidly degrade along with\ndecoding steps, while placing the target LT on the encoder side would lead to\ncopying or paraphrasing the source input. To address the above issues, we\npropose a simple yet effective strategy named Language Converter Strategy\n(LCS). By introducing the target language embedding into the top encoder\nlayers, LCS mitigates confusion in the encoder and ensures stable language\nindication for the decoder. Experimental results on MultiUN, TED, and OPUS-100\ndatasets demonstrate that LCS could significantly mitigate the off-target\nissue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile\noutperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on\nzero-shot translation, respectively.", "published": "2024-06-05 02:52:17", "link": "http://arxiv.org/abs/2406.02876v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Outdated Issue Aware Decoding for Reasoning Questions on Edited\n  Knowledge", "abstract": "Recently, Knowledge Editing has received increasing attention, since it could\nupdate the specific knowledge from outdated ones in pretrained models without\nre-training. However, as pointed out by recent studies, existing related\nmethods tend to merely memorize the superficial word composition of the edited\nknowledge, rather than truly learning and absorbing it. Consequently, on the\nreasoning questions, we discover that existing methods struggle to utilize the\nedited knowledge to reason the new answer, and tend to retain outdated\nresponses, which are generated by the original models utilizing original\nknowledge. Nevertheless, the outdated responses are unexpected for the correct\nanswers to reasoning questions, which we named as the outdated issue. To\nalleviate this issue, in this paper, we propose a simple yet effective decoding\nstrategy, i.e., outDated ISsue aware deCOding (DISCO), to enhance the\nperformance of edited models on reasoning questions. Specifically, we capture\nthe difference in the probability distribution between the original and edited\nmodels. Further, we amplify the difference of the token prediction in the\nedited model to alleviate the outdated issue, and thus enhance the model\nperformance w.r.t the edited knowledge. Experimental results suggest that\napplying DISCO could enhance edited models to reason, e.g., on reasoning\nquestions, DISCO outperforms the prior SOTA method by 12.99 F1 scores, and\nreduces the ratio of the outdated issue to 5.78% on the zsRE dataset.", "published": "2024-06-05 03:00:15", "link": "http://arxiv.org/abs/2406.02882v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PLaD: Preference-based Large Language Model Distillation with\n  Pseudo-Preference Pairs", "abstract": "Large Language Models (LLMs) have exhibited impressive capabilities in\nvarious tasks, yet their vast parameter sizes restrict their applicability in\nresource-constrained settings. Knowledge distillation (KD) offers a viable\nsolution by transferring expertise from large teacher models to compact student\nmodels. However, traditional KD techniques face specific challenges when\napplied to LLMs, including restricted access to LLM outputs, significant\nteacher-student capacity gaps, and the inherited mis-calibration issue. In this\nwork, we present PLaD, a novel preference-based LLM distillation framework.\nPLaD exploits the teacher-student capacity discrepancy to generate\npseudo-preference pairs where teacher outputs are preferred over student\noutputs. Then, PLaD leverages a ranking loss to re-calibrate student's\nestimation of sequence likelihood, which steers the student's focus towards\nunderstanding the relative quality of outputs instead of simply imitating the\nteacher. PLaD bypasses the need for access to teacher LLM's internal states,\ntackles the student's expressivity limitations, and mitigates the student\nmis-calibration issue. Through extensive experiments on two sequence generation\ntasks and with various LLMs, we demonstrate the effectiveness of our proposed\nPLaD framework.", "published": "2024-06-05 03:08:25", "link": "http://arxiv.org/abs/2406.02886v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Moment-Matching Distillation of Large Language Models", "abstract": "Knowledge distillation (KD) has been shown to be highly effective in guiding\na student model with a larger teacher model and achieving practical benefits in\nimproving the computational and memory efficiency for large language models\n(LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit\ndistribution distance between teacher and student probability predictions.\nInstead of optimizing these mandatory behaviour cloning objectives, we explore\nan imitation learning strategy for KD of LLMs. In particular, we minimize the\nimitation gap by matching the action-value moments of the teacher's behavior\nfrom both on- and off-policy perspectives. To achieve this action-value\nmoment-matching goal, we propose an adversarial training algorithm to jointly\nestimate the moment-matching distance and optimize the student policy to\nminimize it. Results from both task-agnostic instruction-following experiments\nand task-specific experiments demonstrate the effectiveness of our method and\nachieve new state-of-the-art performance.", "published": "2024-06-05 05:27:29", "link": "http://arxiv.org/abs/2406.02959v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of data inconsistency for multi-modal sentiment analysis", "abstract": "Emotion semantic inconsistency is an ubiquitous challenge in multi-modal\nsentiment analysis (MSA). MSA involves analyzing sentiment expressed across\nvarious modalities like text, audio, and videos. Each modality may convey\ndistinct aspects of sentiment, due to subtle and nuanced expression of human\nbeings, leading to inconsistency, which may hinder the prediction of artificial\nagents. In this work, we introduce a modality conflicting test set and assess\nthe performance of both traditional multi-modal sentiment analysis models and\nmulti-modal large language models (MLLMs). Our findings reveal significant\nperformance degradation across traditional models when confronted with\nsemantically conflicting data and point out the drawbacks of MLLMs when\nhandling multi-modal emotion analysis. Our research presents a new challenge\nand offer valuable insights for the future development of sentiment analysis\nsystems.", "published": "2024-06-05 07:11:56", "link": "http://arxiv.org/abs/2406.03004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling Selection Biases: Exploring Order and Token Sensitivity in\n  Large Language Models", "abstract": "In this paper, we investigate the phenomena of \"selection biases\" in Large\nLanguage Models (LLMs), focusing on problems where models are tasked with\nchoosing the optimal option from an ordered sequence. We delve into biases\nrelated to option order and token usage, which significantly impact LLMs'\ndecision-making processes. We also quantify the impact of these biases through\nan extensive empirical analysis across multiple models and tasks. Furthermore,\nwe propose mitigation strategies to enhance model performance. Our key\ncontributions are threefold: 1) Precisely quantifying the influence of option\norder and token on LLMs, 2) Developing strategies to mitigate the impact of\ntoken and order sensitivity to enhance robustness, and 3) Offering a detailed\nanalysis of sensitivity across models and tasks, which informs the creation of\nmore stable and reliable LLM applications for selection problems.", "published": "2024-06-05 07:16:51", "link": "http://arxiv.org/abs/2406.03009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Tarzan to Tolkien: Controlling the Language Proficiency Level of\n  LLMs for Content Generation", "abstract": "We study the problem of controlling the difficulty level of text generated by\nLarge Language Models (LLMs) for contexts where end-users are not fully\nproficient, such as language learners. Using a novel framework, we evaluate the\neffectiveness of several key approaches for this task, including few-shot\nprompting, supervised finetuning, and reinforcement learning (RL), utilising\nboth GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.\n  Our findings reveal a large performance gap between GPT-4 and the open source\nmodels when using prompt-based strategies. However, we show how to bridge this\ngap with a careful combination of finetuning and RL alignment. Our best model,\nCALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and\nother strategies, at only a fraction of the cost. We further validate the\nquality of our results through a small-scale human study.", "published": "2024-06-05 07:57:17", "link": "http://arxiv.org/abs/2406.03030v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud?", "abstract": "Recent advances in the field of large language models (LLMs), particularly\nthe ChatGPT family, have given rise to a powerful and versatile machine\ninterlocutor, packed with knowledge and challenging our understanding of\nlearning. This interlocutor is a double-edged sword: it can be harnessed for a\nwide variety of beneficial tasks, but it can also be used to cause harm. This\nstudy explores the complicated interaction between ChatGPT and the growing\nproblem of cryptocurrency fraud. Although ChatGPT is known for its adaptability\nand ethical considerations when used for harmful purposes, we highlight the\ndeep connection that may exist between ChatGPT and fraudulent actions in the\nvolatile cryptocurrency ecosystem. Based on our categorization of\ncryptocurrency frauds, we show how to influence outputs, bypass ethical terms,\nand achieve specific fraud goals by manipulating ChatGPT prompts. Furthermore,\nour findings emphasize the importance of realizing that ChatGPT could be a\nvaluable instructor even for novice fraudsters, as well as understanding and\nsafely deploying complex language models, particularly in the context of\ncryptocurrency frauds. Finally, our study underlines the importance of using\nLLMs responsibly and ethically in the digital currency sector, identifying\npotential risks and resolving ethical issues. It should be noted that our work\nis not intended to encourage and promote fraud, but rather to raise awareness\nof the risks of fraud associated with the use of ChatGPT.", "published": "2024-06-05 09:09:32", "link": "http://arxiv.org/abs/2406.03079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument\n  Summarisation and Evaluation", "abstract": "With the recent advances of large language models (LLMs), it is no longer\ninfeasible to build an automated debate system that helps people to synthesise\npersuasive arguments. Previous work attempted this task by integrating multiple\ncomponents. In our work, we introduce an argument mining dataset that captures\nthe end-to-end process of preparing an argumentative essay for a debate, which\ncovers the tasks of claim and evidence identification (Task 1 ED), evidence\nconvincingness ranking (Task 2 ECR), argumentative essay summarisation and\nhuman preference ranking (Task 3 ASR) and metric learning for automated\nevaluation of resulting essays, based on human feedback along argument quality\ndimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are\nfully annotated with the various properties supporting the aforementioned\ntasks. We evaluate multiple generative baselines for each of these tasks,\nincluding representative LLMs. We find, that while they show promising results\non individual tasks in our benchmark, their end-to-end performance on all four\ntasks in succession deteriorates significantly, both in automated measures as\nwell as in human-centred evaluation. This challenge presented by our proposed\ndataset motivates future research on end-to-end argument mining and\nsummarisation. The repository of this project is available at\nhttps://github.com/HaoBytes/ArgSum-Datatset", "published": "2024-06-05 11:15:45", "link": "http://arxiv.org/abs/2406.03151v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CSS: Contrastive Semantic Similarity for Uncertainty Quantification of\n  LLMs", "abstract": "Despite the impressive capability of large language models (LLMs), knowing\nwhen to trust their generations remains an open challenge. The recent\nliterature on uncertainty quantification of natural language generation (NLG)\nutilises a conventional natural language inference (NLI) classifier to measure\nthe semantic dispersion of LLMs responses. These studies employ logits of NLI\nclassifier for semantic clustering to estimate uncertainty. However, logits\nrepresent the probability of the predicted class and barely contain feature\ninformation for potential clustering. Alternatively, CLIP (Contrastive\nLanguage-Image Pre-training) performs impressively in extracting image-text\npair features and measuring their similarity. To extend its usability, we\npropose Contrastive Semantic Similarity, the CLIP-based feature extraction\nmodule to obtain similarity features for measuring uncertainty for text pairs.\nWe apply this method to selective NLG, which detects and rejects unreliable\ngenerations for better trustworthiness of LLMs. We conduct extensive\nexperiments with three LLMs on several benchmark question-answering datasets\nwith comprehensive evaluation metrics. Results show that our proposed method\nperforms better in estimating reliable responses of LLMs than comparable\nbaselines. Results show that our proposed method performs better in estimating\nreliable responses of LLMs than comparable baselines. The code are available at\n\\url{https://github.com/AoShuang92/css_uq_llms}.", "published": "2024-06-05 11:35:44", "link": "http://arxiv.org/abs/2406.03158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatLang-8: An LLM-Based Synthetic Data Generation Framework for\n  Grammatical Error Correction", "abstract": "We explore and improve the capabilities of LLMs to generate data for\ngrammatical error correction (GEC). When merely producing parallel sentences,\ntheir patterns are too simplistic to be valuable as a corpus. To address this\nissue, we propose an automated framework that includes a Subject Selector,\nGrammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a\nnew dataset for GEC tasks, named ChatLang-8, which encompasses eight types of\nsubject nouns and 23 types of grammar. It consists of 1 million pairs featuring\nhuman-like grammatical errors. Our experiments reveal that ChatLang-8 exhibits\na more uniform pattern composition compared to existing GEC datasets.\nFurthermore, we observe improved model performance when using ChatLang-8\ninstead of existing GEC datasets. The experimental results suggest that our\nframework and ChatLang-8 are valuable resources for enhancing ChatGPT's data\ngeneration capabilities.", "published": "2024-06-05 12:35:00", "link": "http://arxiv.org/abs/2406.03202v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linking Named Entities in Diderot's \\textit{Encyclop\u00e9die} to Wikidata", "abstract": "Diderot's \\textit{Encyclop\\'edie} is a reference work from XVIIIth century in\nEurope that aimed at collecting the knowledge of its era. \\textit{Wikipedia}\nhas the same ambition with a much greater scope. However, the lack of digital\nconnection between the two encyclopedias may hinder their comparison and the\nstudy of how knowledge has evolved. A key element of \\textit{Wikipedia} is\nWikidata that backs the articles with a graph of structured data. In this\npaper, we describe the annotation of more than 10,300 of the\n\\textit{Encyclop\\'edie} entries with Wikidata identifiers enabling us to\nconnect these entries to the graph. We considered geographic and human\nentities. The \\textit{Encyclop\\'edie} does not contain biographic entries as\nthey mostly appear as subentries of locations. We extracted all the geographic\nentries and we completely annotated all the entries containing a description of\nhuman entities. This represents more than 2,600 links referring to locations or\nhuman entities. In addition, we annotated more than 9,500 entries having a\ngeographic content only. We describe the annotation process as well as\napplication examples. This resource is available at\nhttps://github.com/pnugues/encyclopedie_1751", "published": "2024-06-05 13:00:04", "link": "http://arxiv.org/abs/2406.03221v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Error-preserving Automatic Speech Recognition of Young English Learners'\n  Language", "abstract": "One of the central skills that language learners need to practice is speaking\nthe language. Currently, students in school do not get enough speaking\nopportunities and lack conversational practice. Recent advances in speech\ntechnology and natural language processing allow for the creation of novel\ntools to practice their speaking skills. In this work, we tackle the first\ncomponent of such a pipeline, namely, the automated speech recognition module\n(ASR), which faces a number of challenges: first, state-of-the-art ASR models\nare often trained on adult read-aloud data by native speakers and do not\ntransfer well to young language learners' speech. Second, most ASR systems\ncontain a powerful language model, which smooths out errors made by the\nspeakers. To give corrective feedback, which is a crucial part of language\nlearning, the ASR systems in our setting need to preserve the errors made by\nthe language learners. In this work, we build an ASR system that satisfies\nthese requirements: it works on spontaneous speech by young language learners\nand preserves their errors. For this, we collected a corpus containing around\n85 hours of English audio spoken by learners in Switzerland from grades 4 to 6\non different language learning tasks, which we used to train an ASR model. Our\nexperiments show that our model benefits from direct fine-tuning on children's\nvoices and has a much higher error preservation rate than other models.", "published": "2024-06-05 13:15:37", "link": "http://arxiv.org/abs/2406.03235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Evaluators for Recommendation Explanations", "abstract": "The explainability of recommender systems has attracted significant attention\nin academia and industry. Many efforts have been made for explainable\nrecommendations, yet evaluating the quality of the explanations remains a\nchallenging and unresolved issue. In recent years, leveraging LLMs as\nevaluators presents a promising avenue in Natural Language Processing tasks\n(e.g., sentiment classification, information extraction), as they perform\nstrong capabilities in instruction following and common-sense reasoning.\nHowever, evaluating recommendation explanatory texts is different from these\nNLG tasks, as its criteria are related to human perceptions and are usually\nsubjective. In this paper, we investigate whether LLMs can serve as evaluators\nof recommendation explanations. To answer the question, we utilize real user\nfeedback on explanations given from previous work and additionally collect\nthird-party annotations and LLM evaluations. We design and apply a 3-level meta\nevaluation strategy to measure the correlation between evaluator labels and the\nground truth provided by users. Our experiments reveal that LLMs, such as GPT4,\ncan provide comparable evaluations with appropriate prompts and settings. We\nalso provide further insights into combining human labels with the LLM\nevaluation process and utilizing ensembles of multiple heterogeneous LLM\nevaluators to enhance the accuracy and stability of evaluations. Our study\nverifies that utilizing LLMs as evaluators can be an accurate, reproducible and\ncost-effective solution for evaluating recommendation explanation texts. Our\ncode is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.", "published": "2024-06-05 13:23:23", "link": "http://arxiv.org/abs/2406.03248v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions\n  of Large Language Models in Cooperation and Bargaining Games", "abstract": "Behavior study experiments are an important part of society modeling and\nunderstanding human interactions. In practice, many behavioral experiments\nencounter challenges related to internal and external validity,\nreproducibility, and social bias due to the complexity of social interactions\nand cooperation in human user studies. Recent advances in Large Language Models\n(LLMs) have provided researchers with a new promising tool for the simulation\nof human behavior. However, existing LLM-based simulations operate under the\nunproven hypothesis that LLM agents behave similarly to humans as well as\nignore a crucial factor in human decision-making: emotions.\n  In this paper, we introduce a novel methodology and the framework to study\nboth, the decision-making of LLMs and their alignment with human behavior under\nemotional states. Experiments with GPT-3.5 and GPT-4 on four games from two\ndifferent classes of behavioral game theory showed that emotions profoundly\nimpact the performance of LLMs, leading to the development of more optimal\nstrategies. While there is a strong alignment between the behavioral responses\nof GPT-3.5 and human participants, particularly evident in bargaining games,\nGPT-4 exhibits consistent behavior, ignoring induced emotions for rationality\ndecisions. Surprisingly, emotional prompting, particularly with `anger'\nemotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human\nemotional responses.", "published": "2024-06-05 14:08:54", "link": "http://arxiv.org/abs/2406.03299v1", "categories": ["cs.AI", "cs.CL", "I.2.7; J.4"], "primary_category": "cs.AI"}
{"title": "The Challenges of Evaluating LLM Applications: An Analysis of Automated,\n  Human, and LLM-Based Approaches", "abstract": "Chatbots have been an interesting application of natural language generation\nsince its inception. With novel transformer based Generative AI methods,\nbuilding chatbots have become trivial. Chatbots which are targeted at specific\ndomains for example medicine and psychology are implemented rapidly. This\nhowever, should not distract from the need to evaluate the chatbot responses.\nEspecially because the natural language generation community does not entirely\nagree upon how to effectively evaluate such applications. With this work we\ndiscuss the issue further with the increasingly popular LLM based evaluations\nand how they correlate with human evaluations. Additionally, we introduce a\ncomprehensive factored evaluation mechanism that can be utilized in conjunction\nwith both human and LLM-based evaluations. We present the results of an\nexperimental evaluation conducted using this scheme in one of our chatbot\nimplementations which consumed educational reports, and subsequently compare\nautomated, traditional human evaluation, factored human evaluation, and\nfactored LLM evaluation. Results show that factor based evaluation produces\nbetter insights on which aspects need to be improved in LLM applications and\nfurther strengthens the argument to use human evaluation in critical spaces\nwhere main functionality is not direct retrieval.", "published": "2024-06-05 14:55:10", "link": "http://arxiv.org/abs/2406.03339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models", "abstract": "Despite the widespread adoption of Large language models (LLMs), their\nremarkable capabilities remain limited to a few high-resource languages.\nAdditionally, many low-resource languages (\\eg African languages) are often\nevaluated only on basic text classification tasks due to the lack of\nappropriate or comprehensive benchmarks outside of high-resource languages. In\nthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for\n17 typologically-diverse low-resource African languages covering three tasks:\nnatural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and\nmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBench\nto evaluate zero-shot, few-shot, and translate-test settings~(where test sets\nare translated into English) across 10 open and six proprietary LLMs. Our\nevaluation reveals a significant performance gap between high-resource\nlanguages~(such as English and French) and low-resource African languages. We\nobserve a significant performance gap between open and proprietary models, with\nthe highest performing open model, Gemma 2 27B only at 63\\% of the\nbest-performing proprietary model GPT-4o performance. In addition, machine\ntranslating the test set to English before evaluation helped to close the gap\nfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1\n70B. These findings suggest that more efforts are needed to develop and adapt\nLLMs for African languages.", "published": "2024-06-05 15:23:08", "link": "http://arxiv.org/abs/2406.03368v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cycles of Thought: Measuring LLM Confidence through Stable Explanations", "abstract": "In many high-risk machine learning applications it is essential for a model\nto indicate when it is uncertain about a prediction. While large language\nmodels (LLMs) can reach and even surpass human-level accuracy on a variety of\nbenchmarks, their overconfidence in incorrect responses is still a\nwell-documented failure mode. Traditional methods for ML uncertainty\nquantification can be difficult to directly adapt to LLMs due to the\ncomputational cost of implementation and closed-source nature of many models. A\nvariety of black-box methods have recently been proposed, but these often rely\non heuristics such as self-verbalized confidence. We instead propose a\nframework for measuring an LLM's uncertainty with respect to the distribution\nof generated explanations for an answer. While utilizing explanations is not a\nnew idea in and of itself, by interpreting each possible model+explanation pair\nas a test-time classifier we can calculate a posterior answer distribution over\nthe most likely of these classifiers. We demonstrate how a specific instance of\nthis framework using explanation entailment as our classifier likelihood\nimproves confidence score metrics (in particular AURC and AUROC) over baselines\nacross five different datasets. We believe these results indicate that our\nframework is both a well-principled and effective way of quantifying\nuncertainty in LLMs.", "published": "2024-06-05 16:35:30", "link": "http://arxiv.org/abs/2406.03441v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are language models rational? The case of coherence norms and belief\n  revision", "abstract": "Do norms of rationality apply to machine learning models, in particular\nlanguage models? In this paper we investigate this question by focusing on a\nspecial subset of rational norms: coherence norms. We consider both logical\ncoherence norms as well as coherence norms tied to the strength of belief. To\nmake sense of the latter, we introduce the Minimal Assent Connection (MAC) and\npropose a new account of credence, which captures the strength of belief in\nlanguage models. This proposal uniformly assigns strength of belief simply on\nthe basis of model internal next token probabilities. We argue that rational\nnorms tied to coherence do apply to some language models, but not to others.\nThis issue is significant since rationality is closely tied to predicting and\nexplaining behavior, and thus it is connected to considerations about AI safety\nand alignment, as well as understanding model behavior more generally.", "published": "2024-06-05 16:36:21", "link": "http://arxiv.org/abs/2406.03442v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-trained Large Language Models Use Fourier Features to Compute\n  Addition", "abstract": "Pre-trained large language models (LLMs) exhibit impressive mathematical\nreasoning capabilities, yet how they compute basic arithmetic, such as\naddition, remains unclear. This paper shows that pre-trained LLMs add numbers\nusing Fourier features -- dimensions in the hidden state that represent numbers\nvia a set of features sparse in the frequency domain. Within the model, MLP and\nattention layers use Fourier features in complementary ways: MLP layers\nprimarily approximate the magnitude of the answer using low-frequency features,\nwhile attention layers primarily perform modular addition (e.g., computing\nwhether the answer is even or odd) using high-frequency features. Pre-training\nis crucial for this mechanism: models trained from scratch to add numbers only\nexploit low-frequency features, leading to lower accuracy. Introducing\npre-trained token embeddings to a randomly initialized model rescues its\nperformance. Overall, our analysis demonstrates that appropriate pre-trained\nrepresentations (e.g., Fourier features) can unlock the ability of Transformers\nto learn precise mechanisms for algorithmic tasks.", "published": "2024-06-05 16:40:53", "link": "http://arxiv.org/abs/2406.03445v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What is the Best Way for ChatGPT to Translate Poetry?", "abstract": "Machine translation (MT) has historically faced significant challenges when\napplied to literary works, particularly in the domain of poetry translation.\nThe advent of Large Language Models such as ChatGPT holds potential for\ninnovation in this field. This study examines ChatGPT's capabilities in\nEnglish-Chinese poetry translation tasks, utilizing targeted prompts and small\nsample scenarios to ascertain optimal performance. Despite promising outcomes,\nour analysis reveals persistent issues in the translations generated by ChatGPT\nthat warrant attention. To address these shortcomings, we propose an\nExplanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages\nmonolingual poetry explanation as a guiding information for the translation\nprocess. Furthermore, we refine existing evaluation criteria to better suit the\nnuances of modern poetry translation. We engaged a panel of professional poets\nfor assessments, complemented evaluations by using GPT-4. The results from both\nhuman and machine evaluations demonstrate that our EAPMT method outperforms\ntraditional translation methods of ChatGPT and the existing online systems.\nThis paper validates the efficacy of our method and contributes a novel\nperspective to machine-assisted literary translation.", "published": "2024-06-05 16:48:26", "link": "http://arxiv.org/abs/2406.03450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does your data spark joy? Performance gains from domain upsampling at\n  the end of training", "abstract": "Pretraining datasets for large language models (LLMs) have grown to trillions\nof tokens composed of large amounts of CommonCrawl (CC) web scrape along with\nsmaller, domain-specific datasets. It is expensive to understand the impact of\nthese domain-specific datasets on model capabilities as training at large FLOP\nscales is required to reveal significant changes to difficult and emergent\nbenchmarks. Given the increasing cost of experimenting with pretraining data,\nhow does one determine the optimal balance between the diversity in general web\nscrapes and the information density of domain specific data? In this work, we\nshow how to leverage the smaller domain specific datasets by upsampling them\nrelative to CC at the end of training to drive performance improvements on\ndifficult benchmarks. This simple technique allows us to improve up to 6.90 pp\non MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data\nmix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2\n(7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with\nablating the duration of domain upsampling from 5% to 30% of training and find\nthat 10% to 20% percent is optimal for navigating the tradeoff between general\nlanguage modeling capabilities and targeted benchmarks. We also use domain\nupsampling to characterize at scale the utility of individual datasets for\nimproving various benchmarks by removing them during this final phase of\ntraining. This tool opens up the ability to experiment with the impact of\ndifferent pretraining datasets at scale, but at an order of magnitude lower\ncost compared to full pretraining runs.", "published": "2024-06-05 17:29:15", "link": "http://arxiv.org/abs/2406.03476v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BIPED: Pedagogically Informed Tutoring System for ESL Education", "abstract": "Large Language Models (LLMs) have a great potential to serve as readily\navailable and cost-efficient Conversational Intelligent Tutoring Systems (CITS)\nfor teaching L2 learners of English. Existing CITS, however, are designed to\nteach only simple concepts or lack the pedagogical depth necessary to address\ndiverse learning strategies. To develop a more pedagogically informed CITS\ncapable of teaching complex concepts, we construct a BIlingual\nPEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human\nEnglish tutoring interactions. Through post-hoc analysis of the tutoring\ninteractions, we come up with a lexicon of dialogue acts (34 tutor acts and 9\nstudent acts), which we use to further annotate the collected dataset. Based on\na two-step framework of first predicting the appropriate tutor act then\ngenerating the corresponding response, we implemented two CITS models using\nGPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the\nimplemented models not only replicate the style of human teachers but also\nemploy diverse and contextually appropriate pedagogical strategies.", "published": "2024-06-05 17:49:24", "link": "http://arxiv.org/abs/2406.03486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing LLM Behavior in Dialogue Summarization: Unveiling\n  Circumstantial Hallucination Trends", "abstract": "Recent advancements in large language models (LLMs) have considerably\nadvanced the capabilities of summarization systems. However, they continue to\nface concerns about hallucinations. While prior work has evaluated LLMs\nextensively in news domains, most evaluation of dialogue summarization has\nfocused on BART-based models, leaving a gap in our understanding of their\nfaithfulness. Our work benchmarks the faithfulness of LLMs for dialogue\nsummarization, using human annotations and focusing on identifying and\ncategorizing span-level inconsistencies. Specifically, we focus on two\nprominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to\nwhat constitutes a hallucination: LLMs often generate plausible inferences,\nsupported by circumstantial evidence in the conversation, that lack direct\nevidence, a pattern that is less prevalent in older models. We propose a\nrefined taxonomy of errors, coining the category of \"Circumstantial Inference\"\nto bucket these LLM behaviors and release the dataset. Using our taxonomy, we\ncompare the behavioral differences between LLMs and older fine-tuned models.\nAdditionally, we systematically assess the efficacy of automatic error\ndetection methods on LLM summaries and find that they struggle to detect these\nnuanced errors. To address this, we introduce two prompt-based approaches for\nfine-grained error detection that outperform existing metrics, particularly for\nidentifying \"Circumstantial Inference.\"", "published": "2024-06-05 17:49:47", "link": "http://arxiv.org/abs/2406.03487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Retrieval Complexity in Question Answering Systems", "abstract": "In this paper, we investigate which questions are challenging for\nretrieval-based Question Answering (QA). We (i) propose retrieval complexity\n(RC), a novel metric conditioned on the completeness of retrieved documents,\nwhich measures the difficulty of answering questions, and (ii) propose an\nunsupervised pipeline to measure RC given an arbitrary retrieval system. Our\nproposed pipeline measures RC more accurately than alternative estimators,\nincluding LLMs, on six challenging QA benchmarks. Further investigation reveals\nthat RC scores strongly correlate with both QA performance and expert judgment\nacross five of the six studied benchmarks, indicating that RC is an effective\nmeasure of question difficulty. Subsequent categorization of high-RC questions\nshows that they span a broad set of question shapes, including multi-hop,\ncompositional, and temporal QA, indicating that RC scores can categorize a new\nsubset of complex questions. Our system can also have a major impact on\nretrieval-based systems by helping to identify more challenging questions on\nexisting datasets.", "published": "2024-06-05 19:30:52", "link": "http://arxiv.org/abs/2406.03592v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the\n  Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning", "abstract": "The integration of generative Large Language Models (LLMs) into various\napplications, including the legal domain, has been accelerated by their\nexpansive and versatile nature. However, when facing a legal case, users\nwithout a legal background often struggle to formulate professional queries and\nmay inadvertently overlook critical legal factors when presenting their case\nnarrative to LLMs. To address this issue, we propose the Diagnostic Legal Large\nLanguage Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions\nto collect additional case information and then provides high-quality feedback.\nD3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement\nLearning (PURL) algorithm, enabling the generation of critical questions and\nenhancing user-LLM interactions. Moreover, an integrated LLM-based stopping\ncriterion facilitates precise Court Views Generation (CVG). Our research also\nintroduces a new English-language CVG dataset based on the US case law\ndatabase, enriching the realm of LLM research and deployment with a vital\ndimension. D3LM surpasses classical LLMs by delivering outstanding performance\nand a remarkable user experience in the legal domain.", "published": "2024-06-05 19:47:35", "link": "http://arxiv.org/abs/2406.03600v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Free Self-Alignment Possible?", "abstract": "Aligning pretrained language models (LMs) often requires large-scale\npreference data and substantial computational resources. These costs become\neven more prohibitive for multi-objective or pluralistic alignment. Is this\ntruly necessary? Can we perform efficient alignment using only internal model\ncapabilities, and without additional training? To answer this question, we\npropose AlignEZ, a novel approach that leverages (1) self-generated preference\ndata and (2) representation editing to achieve cost-effective, efficient\nalignment. By operating directly on learned representations, AlignEZ\nindependently targets different behavioral aspects without the overhead of\ntraditional alignment methods. Our experiments reveal that this cost-efficient\nprocedure improves performance across diverse tasks: up to 19.9% on general\nalignment and 1.9% on challenging mathematical reasoning tasks, even when\nstarting from a strong base model. AlignEZ can also align models to multiple\nobjectives simultaneously, granting fine-grained control over multiple\npreference axes. Finally, we show that AlignEZ can accelerate more expensive\nalignment procedures--such as DPO--even under limited availability of\nground-truth preference data.", "published": "2024-06-05 22:35:17", "link": "http://arxiv.org/abs/2406.03642v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of Large Language Models in Detecting Fake News:\n  A Comparative Analysis", "abstract": "In an era increasingly influenced by artificial intelligence, the detection\nof fake news is crucial, especially in contexts like election seasons where\nmisinformation can have significant societal impacts. This study evaluates the\neffectiveness of various LLMs in identifying and filtering fake news content.\nUtilizing a comparative analysis approach, we tested four large LLMs -- GPT-4,\nClaude 3 Sonnet, Gemini Pro 1.0, and Mistral Large -- and two smaller LLMs --\nGemma 7B and Mistral 7B. By using fake news dataset samples from Kaggle, this\nresearch not only sheds light on the current capabilities and limitations of\nLLMs in fake news detection but also discusses the implications for developers\nand policymakers in enhancing AI-driven informational integrity.", "published": "2024-06-05 02:55:21", "link": "http://arxiv.org/abs/2406.06584v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bi-Chainer: Automated Large Language Models Reasoning with Bidirectional\n  Chaining", "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but\nstill face challenges in solving complex logical problems. Existing\nunidirectional chaining methods, such as forward chaining and backward\nchaining, suffer from issues like low prediction accuracy and efficiency. To\naddress these, we propose a bidirectional chaining method, Bi-Chainer, which\ndynamically switches to depth-first reasoning in the opposite reasoning\ndirection when it encounters multiple branching options within the current\ndirection. Thus, the intermediate reasoning results can be utilized as guidance\nto facilitate the reasoning process. We show that Bi-Chainer achieves sizable\naccuracy boots over unidirectional chaining frameworks on four challenging\nlogical reasoning datasets. Moreover, Bi-Chainer enhances the accuracy of\nintermediate proof steps and reduces the average number of inference calls,\nresulting in more efficient and accurate reasoning.", "published": "2024-06-05 08:15:38", "link": "http://arxiv.org/abs/2406.06586v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PatentEval: Understanding Errors in Patent Generation", "abstract": "In this work, we introduce a comprehensive error typology specifically\ndesigned for evaluating two distinct tasks in machine-generated patent texts:\nclaims-to-abstract generation, and the generation of the next claim given\nprevious ones. We have also developed a benchmark, PatentEval, for\nsystematically assessing language models in this context. Our study includes a\ncomparative analysis, annotated by humans, of various models. These range from\nthose specifically adapted during training for tasks within the patent domain\nto the latest general-purpose large language models (LLMs). Furthermore, we\nexplored and evaluated some metrics to approximate human judgments in patent\ntext evaluation, analyzing the extent to which these metrics align with expert\nassessments. These approaches provide valuable insights into the capabilities\nand limitations of current language models in the specialized field of patent\ntext generation.", "published": "2024-06-05 13:55:27", "link": "http://arxiv.org/abs/2406.06589v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are LLMs classical or nonmonotonic reasoners? Lessons from generics", "abstract": "Recent scholarship on reasoning in LLMs has supplied evidence of impressive\nperformance and flexible adaptation to machine generated or human feedback.\nNonmonotonic reasoning, crucial to human cognition for navigating the real\nworld, remains a challenging, yet understudied task. In this work, we study\nnonmonotonic reasoning capabilities of seven state-of-the-art LLMs in one\nabstract and one commonsense reasoning task featuring generics, such as 'Birds\nfly', and exceptions, 'Penguins don't fly' (see Fig. 1). While LLMs exhibit\nreasoning patterns in accordance with human nonmonotonic reasoning abilities,\nthey fail to maintain stable beliefs on truth conditions of generics at the\naddition of supporting examples ('Owls fly') or unrelated information ('Lions\nhave manes'). Our findings highlight pitfalls in attributing human reasoning\nbehaviours to LLMs, as well as assessing general capabilities, while consistent\nreasoning remains elusive.", "published": "2024-06-05 15:23:11", "link": "http://arxiv.org/abs/2406.06590v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improve Mathematical Reasoning in Language Models by Automated Process\n  Supervision", "abstract": "Complex multi-step reasoning tasks, such as solving mathematical problems or\ngenerating code, remain a significant hurdle for even the most advanced large\nlanguage models (LLMs). Verifying LLM outputs with an Outcome Reward Model\n(ORM) is a standard inference-time technique aimed at enhancing the reasoning\nperformance of LLMs. However, this still proves insufficient for reasoning\ntasks with a lengthy or multi-hop reasoning chain, where the intermediate\noutcomes are neither properly rewarded nor penalized. Process supervision\naddresses this limitation by assigning intermediate rewards during the\nreasoning process. To date, the methods used to collect process supervision\ndata have relied on either human annotation or per-step Monte Carlo estimation,\nboth prohibitively expensive to scale, thus hindering the broad application of\nthis technique. In response to this challenge, we propose a novel\ndivide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named\n\\textit{OmegaPRM} for the efficient collection of high-quality process\nsupervision data. This algorithm swiftly identifies the first error in the\nChain of Thought (CoT) with binary search and balances the positive and\nnegative examples, thereby ensuring both efficiency and quality. As a result,\nwe are able to collect over 1.5 million process supervision annotations to\ntrain Process Reward Models (PRMs). This fully automated process supervision\nalongside the weighted self-consistency algorithm is able to enhance LLMs' math\nreasoning performances. We improved the success rates of the instruction-tuned\nGemini Pro model from 51\\% to 69.4\\% on MATH500 and from 86.4\\% to 93.6\\% on\nGSM8K. Similarly, we boosted the success rates of Gemma2 27B from 42.3\\% to\n58.2\\% on MATH500 and from 74.0\\% to 92.2\\% on GSM8K. The entire process\noperates without any human intervention or supervision, making our method both\nfinancially and ...", "published": "2024-06-05 19:25:40", "link": "http://arxiv.org/abs/2406.06592v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HYDRA: Model Factorization Framework for Black-Box LLM Personalization", "abstract": "Personalization has emerged as a critical research area in modern intelligent\nsystems, focusing on mining users' behavioral history and adapting to their\npreferences for delivering tailored experiences. Despite the remarkable\nfew-shot capabilities exhibited by black-box large language models (LLMs), the\ninherent opacity of their model parameters presents significant challenges in\naligning the generated output with individual expectations. Existing solutions\nhave primarily focused on prompt design to incorporate user-specific profiles\nand behaviors; however, such approaches often struggle to generalize\neffectively due to their inability to capture shared knowledge among all users.\nTo address these challenges, we propose HYDRA, a model factorization framework\nthat captures both user-specific behavior patterns from historical data and\nshared general knowledge among all users to deliver personalized generation. In\norder to capture user-specific behavior patterns, we first train a reranker to\nprioritize the most useful information from top-retrieved relevant historical\nrecords. By combining the prioritized history with the corresponding query, we\ntrain an adapter to align the output with individual user-specific preferences,\neliminating the reliance on access to inherent model parameters of black-box\nLLMs. Both the reranker and the adapter can be decomposed into a base model\nwith multiple user-specific heads, resembling a hydra. The base model maintains\nshared knowledge across users, while the multiple personal heads capture\nuser-specific preferences. Experimental results demonstrate that HYDRA\noutperforms existing state-of-the-art prompt-based methods by an average\nrelative improvement of 9.01% across five diverse personalization tasks in the\nLaMP benchmark. Our implementation is available at\nhttps://github.com/night-chen/HYDRA.", "published": "2024-06-05 03:08:46", "link": "http://arxiv.org/abs/2406.02888v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment\n  Algorithms", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been crucial to the\nrecent success of Large Language Models (LLMs), however, it is often a complex\nand brittle process. In the classical RLHF framework, a reward model is first\ntrained to represent human preferences, which is in turn used by an online\nreinforcement learning (RL) algorithm to optimize the LLM. A prominent issue\nwith such methods is reward over-optimization or reward hacking, where\nperformance as measured by the learned proxy reward model increases, but true\nquality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like\nDirect Preference Optimization have emerged as alternatives to the classical\nRLHF pipeline by circumventing the reward modeling phase. However, although\nDAAs do not use a separate proxy reward model, they still commonly deteriorate\nfrom over-optimization. While the so-called reward hacking phenomenon is not\nwell-defined for DAAs, we still uncover similar trends: at higher KL budgets,\nDAA algorithms exhibit similar degradation patterns to their classic RLHF\ncounterparts. In particular, we find that DAA methods deteriorate not only\nacross a wide range of KL budgets but also often before even a single epoch of\nthe dataset is completed. Through extensive empirical experimentation, this\nwork formulates and formalizes the reward over-optimization or hacking problem\nfor DAAs and explores its consequences across objectives, training regimes, and\nmodel scales.", "published": "2024-06-05 03:41:37", "link": "http://arxiv.org/abs/2406.02900v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large\n  Language Models", "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face\ndeployment challenges due to their extensive size. Pruning methods drop a\nsubset of weights to accelerate, but many of them require retraining, which is\nprohibitively expensive and computationally demanding. Recently, post-training\npruning approaches introduced novel metrics, enabling the pruning of LLMs\nwithout retraining. However, these metrics require the involvement of human\nexperts and tedious trial and error. To efficiently identify superior pruning\nmetrics, we develop an automatic framework for searching symbolic pruning\nmetrics using genetic programming. In particular, we devise an elaborate search\nspace encompassing the existing pruning metrics to discover the potential\nsymbolic pruning metric. We propose an opposing operation simplification\nstrategy to increase the diversity of the population. In this way, Pruner-Zero\nallows auto-generation of symbolic pruning metrics. Based on the searched\nresults, we explore the correlation between pruning metrics and performance\nafter pruning and summarize some principles. Extensive experiments on LLaMA and\nLLaMA-2 on language modeling and zero-shot tasks demonstrate that our\nPruner-Zero obtains superior performance than SOTA post-training pruning\nmethods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.", "published": "2024-06-05 04:25:23", "link": "http://arxiv.org/abs/2406.02924v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Joint Beam Search Integrating CTC, Attention, and Transducer Decoders", "abstract": "End-to-end automatic speech recognition (E2E-ASR) can be classified by its\ndecoder architectures, such as connectionist temporal classification (CTC),\nrecurrent neural network transducer (RNN-T), attention-based encoder-decoder,\nand Mask-CTC models. Each decoder architecture has advantages and\ndisadvantages, leading practitioners to switch between these different models\ndepending on application requirements. Instead of building separate models, we\npropose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and\nMask-CTC) share the same encoder -- we refer to this as 4D modeling. The 4D\nmodel is trained jointly, which will bring model regularization and maximize\nthe model robustness thanks to their complementary properties. To efficiently\ntrain the 4D model, we introduce a two-stage training strategy that stabilizes\nthe joint training. In addition, we propose three novel joint beam search\nalgorithms by combining three decoders (CTC, RNN-T, and attention) to further\nimprove performance. These three beam search algorithms differ in which decoder\nis used as the primary decoder. We carefully evaluate the performance and\ncomputational tradeoffs associated with each algorithm. Experimental results\ndemonstrate that the jointly trained 4D model outperforms the E2E-ASR models\ntrained with only one individual decoder. Furthermore, we demonstrate that the\nproposed joint beam search algorithm outperforms the previously proposed\nCTC/attention decoding.", "published": "2024-06-05 05:18:20", "link": "http://arxiv.org/abs/2406.02950v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Docs2KG: Unified Knowledge Graph Construction from Heterogeneous\n  Documents Assisted by Large Language Models", "abstract": "Even for a conservative estimate, 80% of enterprise data reside in\nunstructured files, stored in data lakes that accommodate heterogeneous\nformats. Classical search engines can no longer meet information seeking needs,\nespecially when the task is to browse and explore for insight formulation. In\nother words, there are no obvious search keywords to use. Knowledge graphs, due\nto their natural visual appeals that reduce the human cognitive load, become\nthe winning candidate for heterogeneous data integration and knowledge\nrepresentation.\n  In this paper, we introduce Docs2KG, a novel framework designed to extract\nmultimodal information from diverse and heterogeneous unstructured documents,\nincluding emails, web pages, PDF files, and Excel files. Dynamically generates\na unified knowledge graph that represents the extracted key information,\nDocs2KG enables efficient querying and exploration of document data lakes.\nUnlike existing approaches that focus on domain-specific data sources or\npre-designed schemas, Docs2KG offers a flexible and extensible solution that\ncan adapt to various document structures and content types. The proposed\nframework unifies data processing supporting a multitude of downstream tasks\nwith improved domain interpretability. Docs2KG is publicly accessible at\nhttps://docs2kg.ai4wa.com, and a demonstration video is available at\nhttps://docs2kg.ai4wa.com/Video.", "published": "2024-06-05 05:35:59", "link": "http://arxiv.org/abs/2406.02962v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents", "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based\nintelligent agents have been developed to provide customized services with a\nset of user-defined tools. State-of-the-art methods for constructing LLM agents\nadopt trained LLMs and further fine-tune them on data for the agent task.\nHowever, we show that such methods are vulnerable to our proposed backdoor\nattacks named BadAgent on various agent tasks, where a backdoor can be embedded\nby fine-tuning on the backdoor data. At test time, the attacker can manipulate\nthe deployed LLM agents to execute harmful operations by showing the trigger in\nthe agent input or environment. To our surprise, our proposed attack methods\nare extremely robust even after fine-tuning on trustworthy data. Though\nbackdoor attacks have been studied extensively in natural language processing,\nto the best of our knowledge, we could be the first to study them on LLM agents\nthat are more dangerous due to the permission to use external tools. Our work\ndemonstrates the clear risk of constructing LLM agents based on untrusted LLMs\nor data. Our code is public at https://github.com/DPamK/BadAgent", "published": "2024-06-05 07:14:28", "link": "http://arxiv.org/abs/2406.03007v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and\n  Social Experiences", "abstract": "Recent advancements in foundation models (FMs) have unlocked new prospects in\nautonomous driving, yet the experimental settings of these studies are\npreliminary, over-simplified, and fail to capture the complexity of real-world\ndriving scenarios in human environments. It remains under-explored whether FM\nagents can handle long-horizon navigation tasks with free-from dialogue and\ndeal with unexpected situations caused by environmental dynamics or task\nchanges. To explore the capabilities and boundaries of FMs faced with the\nchallenges above, we introduce DriVLMe, a video-language-model-based agent to\nfacilitate natural and effective communication between humans and autonomous\nvehicles that perceive the environment and navigate. We develop DriVLMe from\nboth embodied experiences in a simulated environment and social experiences\nfrom real human dialogue. While DriVLMe demonstrates competitive performance in\nboth open-loop benchmarks and closed-loop human studies, we reveal several\nlimitations and challenges, including unacceptable inference time, imbalanced\ntraining data, limited visual understanding, challenges with multi-turn\ninteractions, simplified language generation from robotic experiences, and\ndifficulties in handling on-the-fly unexpected situations like environmental\ndynamics and task changes.", "published": "2024-06-05 07:14:44", "link": "http://arxiv.org/abs/2406.03008v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task\n  Learning", "abstract": "Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech\ntranslation) outputs target speech while receiving streaming speech inputs,\nwhich is critical for real-time communication. Beyond accomplishing translation\nbetween speech, Simul-S2ST requires a policy to control the model to generate\ncorresponding target speech at the opportune moment within speech inputs,\nthereby posing a double challenge of translation and policy. In this paper, we\npropose StreamSpeech, a direct Simul-S2ST model that jointly learns translation\nand simultaneous policy in a unified framework of multi-task learning. Adhering\nto a multi-task learning approach, StreamSpeech can perform offline and\nsimultaneous speech recognition, speech translation and speech synthesis via an\n\"All-in-One\" seamless model. Experiments on CVSS benchmark demonstrate that\nStreamSpeech achieves state-of-the-art performance in both offline S2ST and\nSimul-S2ST tasks. Besides, StreamSpeech is able to present high-quality\nintermediate results (i.e., ASR or translation results) during simultaneous\ntranslation process, offering a more comprehensive real-time communication\nexperience.", "published": "2024-06-05 08:24:22", "link": "http://arxiv.org/abs/2406.03049v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Distributional Associations vs In-Context Reasoning: A Study of\n  Feed-forward and Attention Layers", "abstract": "Large language models have been successful at tasks involving basic forms of\nin-context reasoning, such as generating coherent language, as well as storing\nvast amounts of knowledge. At the core of the Transformer architecture behind\nsuch models are feed-forward and attention layers, which are often associated\nto knowledge and reasoning, respectively. In this paper, we study this\ndistinction empirically and theoretically in a controlled synthetic setting\nwhere certain next-token predictions involve both distributional and in-context\ninformation. We find that feed-forward layers tend to learn simple\ndistributional associations such as bigrams, while attention layers focus on\nin-context reasoning. Our theoretical analysis identifies the noise in the\ngradients as a key factor behind this discrepancy. Finally, we illustrate how\nsimilar disparities emerge in pre-trained models through ablations on the\nPythia model family on simple reasoning tasks.", "published": "2024-06-05 08:51:08", "link": "http://arxiv.org/abs/2406.03068v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FusionBench: A Comprehensive Benchmark of Deep Model Fusion", "abstract": "Deep model fusion is an emerging technique that unifies the predictions or\nparameters of several deep neural networks into a single model in a\ncost-effective and data-efficient manner. This enables the unified model to\ntake advantage of the original models' strengths, potentially exceeding their\nperformance. Although a variety of deep model fusion techniques have been\nintroduced, their evaluations tend to be inconsistent and often inadequate to\nvalidate their effectiveness and robustness against distribution shifts. To\naddress this issue, we introduce FusionBench, which is the first comprehensive\nbenchmark dedicated to deep model fusion. FusionBench covers a wide range of\ntasks, including open-vocabulary image classification, text classification, and\ntext-to-text generation. Each category includes up to eight tasks with\ncorresponding task-specific models, featuring both full fine-tuning and LoRA\nfine-tuning, as well as models of different sizes, to ensure fair and balanced\ncomparisons of various multi-task model fusion techniques across different\ntasks, model scales, and fine-tuning strategies. We implement and evaluate a\nbroad spectrum of deep model fusion techniques. These techniques range from\nmodel ensemble methods, which combine the predictions to improve the overall\nperformance, to model merging, which integrates different models into a single\none, and model mixing methods, which upscale or recombine the components of the\noriginal models. FusionBench now contains 26 distinct tasks, 74 fine-tuned\nmodels, and 16 fusion techniques, and we are committed to consistently\nexpanding the benchmark with more tasks, models, and fusion techniques. In\naddition, we offer a well-documented set of resources and guidelines to aid\nresearchers in understanding and replicating the benchmark results. Homepage\nhttps://github.com/tanganke/fusion_bench", "published": "2024-06-05 13:54:28", "link": "http://arxiv.org/abs/2406.03280v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic\n  Bi-Spiking Mechanisms", "abstract": "Towards energy-efficient artificial intelligence similar to the human brain,\nthe bio-inspired spiking neural networks (SNNs) have advantages of biological\nplausibility, event-driven sparsity, and binary activation. Recently,\nlarge-scale language models exhibit promising generalization capability, making\nit a valuable issue to explore more general spike-driven models. However, the\nbinary spikes in existing SNNs fail to encode adequate semantic information,\nplacing technological challenges for generalization. This work proposes the\nfirst fully spiking mechanism for general language tasks, including both\ndiscriminative and generative ones. Different from previous spikes with {0,1}\nlevels, we propose a more general spike formulation with bi-directional,\nelastic amplitude, and elastic frequency encoding, while still maintaining the\naddition nature of SNNs. In a single time step, the spike is enhanced by\ndirection and amplitude information; in spike frequency, a strategy to control\nspike firing rate is well designed. We plug this elastic bi-spiking mechanism\nin language modeling, named SpikeLM. It is the first time to handle general\nlanguage tasks with fully spike-driven models, which achieve much higher\naccuracy than previously possible. SpikeLM also greatly bridges the performance\ngap between SNNs and ANNs in language modeling. Our code is available at\nhttps://github.com/Xingrun-Xing/SpikeLM.", "published": "2024-06-05 13:59:03", "link": "http://arxiv.org/abs/2406.03287v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead", "abstract": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.", "published": "2024-06-05 17:42:05", "link": "http://arxiv.org/abs/2406.03482v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Wings: Learning Multimodal LLMs without Text-only Forgetting", "abstract": "Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.", "published": "2024-06-05 17:59:40", "link": "http://arxiv.org/abs/2406.03496v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with\n  LLMs", "abstract": "Detecting anomalies in general ledger data is of utmost importance to ensure\ntrustworthiness of financial records. Financial audits increasingly rely on\nmachine learning (ML) algorithms to identify irregular or potentially\nfraudulent journal entries, each characterized by a varying number of\ntransactions. In machine learning, heterogeneity in feature dimensions adds\nsignificant complexity to data analysis. In this paper, we introduce a novel\napproach to anomaly detection in financial data using Large Language Models\n(LLMs) embeddings. To encode non-semantic categorical data from real-world\nfinancial records, we tested 3 pre-trained general purpose sentence-transformer\nmodels. For the downstream classification task, we implemented and evaluated 5\noptimized ML models including Logistic Regression, Random Forest, Gradient\nBoosting Machines, Support Vector Machines, and Neural Networks. Our\nexperiments demonstrate that LLMs contribute valuable information to anomaly\ndetection as our models outperform the baselines, in selected settings even by\na large margin. The findings further underscore the effectiveness of LLMs in\nenhancing anomaly detection in financial journal entries, particularly by\ntackling feature sparsity. We discuss a promising perspective on using LLM\nembeddings for non-semantic data in the financial context and beyond.", "published": "2024-06-05 20:19:09", "link": "http://arxiv.org/abs/2406.03614v1", "categories": ["cs.LG", "cs.CL", "q-fin.RM"], "primary_category": "cs.LG"}
{"title": "Style Mixture of Experts for Expressive Text-To-Speech Synthesis", "abstract": "Recent advances in style transfer text-to-speech (TTS) have improved the\nexpressiveness of synthesized speech. However, encoding stylistic information\n(e.g., timbre, emotion, and prosody) from diverse and unseen reference speech\nremains a challenge. This paper introduces StyleMoE, an approach that addresses\nthe issue of learning averaged style representations in the style encoder by\ncreating style experts that learn from subsets of data. The proposed method\nreplaces the style encoder in a TTS framework with a Mixture of Experts (MoE)\nlayer. The style experts specialize by learning from subsets of reference\nspeech routed to them by the gating network, enabling them to handle different\naspects of the style space. As a result, StyleMoE improves the style coverage\nof the style encoder for style transfer TTS. Our experiments, both objective\nand subjective, demonstrate improved style transfer for diverse and unseen\nreference speech. The proposed method enhances the performance of existing\nstate-of-the-art style transfer TTS models and represents the first study of\nstyle MoE in TTS.", "published": "2024-06-05 22:17:47", "link": "http://arxiv.org/abs/2406.03637v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Human-AI Perception Alignment in Sensory Experiences: Do LLMs\n  Understand Textile Hand?", "abstract": "Aligning large language models (LLMs) behaviour with human intent is critical\nfor future AI. An important yet often overlooked aspect of this alignment is\nthe perceptual alignment. Perceptual modalities like touch are more\nmultifaceted and nuanced compared to other sensory modalities such as vision.\nThis work investigates how well LLMs align with human touch experiences using\nthe \"textile hand\" task. We created a \"Guess What Textile\" interaction in which\nparticipants were given two textile samples -- a target and a reference -- to\nhandle. Without seeing them, participants described the differences between\nthem to the LLM. Using these descriptions, the LLM attempted to identify the\ntarget textile by assessing similarity within its high-dimensional embedding\nspace. Our results suggest that a degree of perceptual alignment exists,\nhowever varies significantly among different textile samples. For example, LLM\npredictions are well aligned for silk satin, but not for cotton denim.\nMoreover, participants didn't perceive their textile experiences closely\nmatched by the LLM predictions. This is only the first exploration into\nperceptual alignment around touch, exemplified through textile hand. We discuss\npossible sources of this alignment variance, and how better human-AI perceptual\nalignment can benefit future everyday tasks.", "published": "2024-06-05 08:46:07", "link": "http://arxiv.org/abs/2406.06587v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Assessing the Emergent Symbolic Reasoning Abilities of Llama Large\n  Language Models", "abstract": "Large Language Models (LLMs) achieve impressive performance in a wide range\nof tasks, even if they are often trained with the only objective of chatting\nfluently with users. Among other skills, LLMs show emergent abilities in\nmathematical reasoning benchmarks, which can be elicited with appropriate\nprompting methods. In this work, we systematically investigate the capabilities\nand limitations of popular open-source LLMs on different symbolic reasoning\ntasks. We evaluate three models of the Llama 2 family on two datasets that\nrequire solving mathematical formulas of varying degrees of difficulty. We test\na generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2\n(MAmmoTH and MetaMath) specifically designed to tackle mathematical problems.\nWe observe that both increasing the scale of the model and fine-tuning it on\nrelevant tasks lead to significant performance gains. Furthermore, using\nfine-grained evaluation measures, we find that such performance gains are\nmostly observed with mathematical formulas of low complexity, which\nnevertheless often remain challenging even for the largest fine-tuned models.", "published": "2024-06-05 12:22:43", "link": "http://arxiv.org/abs/2406.06588v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Large Language Models for Enhanced TNM\n  classification of Radiology Report in lung cancer staging", "abstract": "Background: Structured radiology reports remains underdeveloped due to\nlabor-intensive structuring and narrative-style reporting. Deep learning,\nparticularly large language models (LLMs) like GPT-3.5, offers promise in\nautomating the structuring of radiology reports in natural languages. However,\nalthough it has been reported that LLMs are less effective in languages other\nthan English, their radiological performance has not been extensively studied.\nPurpose: This study aimed to investigate the accuracy of TNM classification\nbased on radiology reports using GPT3.5-turbo (GPT3.5) and the utility of\nmultilingual LLMs in both Japanese and English. Material and Methods: Utilizing\nGPT3.5, we developed a system to automatically generate TNM classifications\nfrom chest CT reports for lung cancer and evaluate its performance. We\nstatistically analyzed the impact of providing full or partial TNM definitions\nin both languages using a Generalized Linear Mixed Model. Results: Highest\naccuracy was attained with full TNM definitions and radiology reports in\nEnglish (M = 94%, N = 80%, T = 47%, and ALL = 36%). Providing definitions for\neach of the T, N, and M factors statistically improved their respective\naccuracies (T: odds ratio (OR) = 2.35, p < 0.001; N: OR = 1.94, p < 0.01; M: OR\n= 2.50, p < 0.001). Japanese reports exhibited decreased N and M accuracies (N\naccuracy: OR = 0.74 and M accuracy: OR = 0.21). Conclusion: This study\nunderscores the potential of multilingual LLMs for automatic TNM classification\nin radiology reports. Even without additional model training, performance\nimprovements were evident with the provided TNM definitions, indicating LLMs'\nrelevance in radiology contexts.", "published": "2024-06-05 16:11:55", "link": "http://arxiv.org/abs/2406.06591v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Queue management for slo-oriented large language model serving", "abstract": "Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM.", "published": "2024-06-05 21:17:34", "link": "http://arxiv.org/abs/2407.00047v2", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "GPT-4's One-Dimensional Mapping of Morality: How the Accuracy of\n  Country-Estimates Depends on Moral Domain", "abstract": "Prior research demonstrates that Open AI's GPT models can predict variations\nin moral opinions between countries but that the accuracy tends to be\nsubstantially higher among high-income countries compared to low-income ones.\nThis study aims to replicate previous findings and advance the research by\nexamining how accuracy varies with different types of moral questions. Using\nresponses from the World Value Survey and the European Value Study, covering 18\nmoral issues across 63 countries, we calculated country-level mean scores for\neach moral issue and compared them with GPT-4's predictions. Confirming\nprevious findings, our results show that GPT-4 has greater predictive success\nin high-income than in low-income countries. However, our factor analysis\nreveals that GPT-4 bases its predictions primarily on a single dimension,\npresumably reflecting countries' degree of conservatism/liberalism. Conversely,\nthe real-world moral landscape appears to be two-dimensional, differentiating\nbetween personal-sexual and violent-dishonest issues. When moral issues are\ncategorized based on their moral domain, GPT-4's predictions are found to be\nremarkably accurate in the personal-sexual domain, across both high-income (r =\n.77) and low-income (r = .58) countries. Yet the predictive accuracy\nsignificantly drops in the violent-dishonest domain for both high-income (r =\n.30) and low-income (r = -.16) countries, indicating that GPT-4's\none-dimensional world-view does not fully capture the complexity of the moral\nlandscape. In sum, this study underscores the importance of not only\nconsidering country-specific characteristics to understand GPT-4's moral\nunderstanding, but also the characteristics of the moral issues at hand.", "published": "2024-06-05 12:58:45", "link": "http://arxiv.org/abs/2407.16886v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Text Injection for Neural Contextual Biasing", "abstract": "Neural contextual biasing effectively improves automatic speech recognition\n(ASR) for crucial phrases within a speaker's context, particularly those that\nare infrequent in the training data. This work proposes contextual text\ninjection (CTI) to enhance contextual ASR. CTI leverages not only the paired\nspeech-text data, but also a much larger corpus of unpaired text to optimize\nthe ASR model and its biasing component. Unpaired text is converted into\nspeech-like representations and used to guide the model's attention towards\nrelevant bias phrases. Moreover, we introduce a contextual text-injected (CTI)\nminimum word error rate (MWER) training, which minimizes the expected WER\ncaused by contextual biasing when unpaired text is injected into the model.\nExperiments show that CTI with 100 billion text sentences can achieve up to\n43.3% relative WER reduction from a strong neural biasing model. CTI-MWER\nprovides a further relative improvement of 23.5%.", "published": "2024-06-05 04:20:17", "link": "http://arxiv.org/abs/2406.02921v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech\n  Recognition", "abstract": "Synthetic data is widely used in speech recognition due to the availability\nof text-to-speech models, which facilitate adapting models to previously unseen\ntext domains. However, existing methods suffer in performance when they\nfine-tune an automatic speech recognition (ASR) model on synthetic data as they\nsuffer from the distributional shift commonly referred to as the\nsynthetic-to-real gap. In this paper, we find that task vector arithmetic is\neffective at mitigating this gap. Our proposed method, SYN2REAL task vector,\nshows an average improvement of 10.03\\% improvement in word error rate over\nbaselines on the SLURP dataset. Additionally, we show that an average of\nSYN2REAL task vectors, when we have real speeches from multiple different\ndomains, can further adapt the original ASR model to perform better on the\ntarget text domain.", "published": "2024-06-05 04:25:56", "link": "http://arxiv.org/abs/2406.02925v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Task-oriented Queries Benchmark (ToQB)", "abstract": "Task-oriented queries (e.g., one-shot queries to play videos, order food, or\ncall a taxi) are crucial for assessing the quality of virtual assistants,\nchatbots, and other large language model (LLM)-based services. However, a\nstandard benchmark for task-oriented queries is not yet available, as existing\nbenchmarks in the relevant NLP (Natural Language Processing) fields have\nprimarily focused on task-oriented dialogues. Thus, we present a new\nmethodology for efficiently generating the Task-oriented Queries Benchmark\n(ToQB) using existing task-oriented dialogue datasets and an LLM service. Our\nmethodology involves formulating the underlying NLP task to summarize the\noriginal intent of a speaker in each dialogue, detailing the key steps to\nperform the devised NLP task using an LLM service, and outlining a framework\nfor automating a major part of the benchmark generation process. Through a case\nstudy encompassing three domains (i.e., two single-task domains and one\nmulti-task domain), we demonstrate how to customize the LLM prompts (e.g.,\nomitting system utterances or speaker labels) for those three domains and\ncharacterize the generated task-oriented queries. The generated ToQB dataset is\nmade available to the public. We further discuss new domains that can be added\nto ToQB by community contributors and its practical applications.", "published": "2024-06-05 05:05:41", "link": "http://arxiv.org/abs/2406.02943v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC", "cs.NE"], "primary_category": "cs.IR"}
{"title": "PrE-Text: Training Language Models on Private Federated Data in the Age\n  of LLMs", "abstract": "On-device training is currently the most common approach for training machine\nlearning (ML) models on private, distributed user data. Despite this, on-device\ntraining has several drawbacks: (1) most user devices are too small to train\nlarge models on-device, (2) on-device training is communication- and\ncomputation-intensive, and (3) on-device training can be difficult to debug and\ndeploy. To address these problems, we propose Private Evolution-Text\n(PrE-Text), a method for generating differentially private (DP) synthetic\ntextual data. First, we show that across multiple datasets, training small\nmodels (models that fit on user devices) with PrE-Text synthetic data\noutperforms small models trained on-device under practical privacy regimes\n($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using\n9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and\n100$\\times$ less communication per round. Second, finetuning large models on\nPrE-Text's DP synthetic data improves large language model (LLM) performance on\nprivate data across the same range of privacy budgets. Altogether, these\nresults suggest that training on DP synthetic data can be a better option than\ntraining a model on-device on private distributed data. Code is available at\nhttps://github.com/houcharlie/PrE-Text.", "published": "2024-06-05 05:27:02", "link": "http://arxiv.org/abs/2406.02958v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture\n  of Large Language Models", "abstract": "We propose MoE-F - a formalized mechanism for combining $N$ pre-trained Large\nLanguage Models (LLMs) for online time-series prediction by adaptively\nforecasting the best weighting of LLM predictions at every time step. Our\nmechanism leverages the conditional information in each expert's running\nperformance to forecast the best combination of LLMs for predicting the time\nseries in its next step. Diverging from static (learned) Mixture of Experts\n(MoE) methods, our approach employs time-adaptive stochastic filtering\ntechniques to combine experts. By framing the expert selection problem as a\nfinite state-space, continuous-time Hidden Markov model (HMM), we can leverage\nthe Wohman-Shiryaev filter. Our approach first constructs N parallel filters\ncorresponding to each of the $N$ individual LLMs. Each filter proposes its best\ncombination of LLMs, given the information that they have access to.\nSubsequently, the N filter outputs are optimally aggregated to maximize their\nrobust predictive power, and this update is computed efficiently via a\nclosed-form expression, generating our ensemble predictor. Our contributions\nare: **(I)** the MoE-F plug-and-play filtering harness algorithm, **(II)**\ntheoretical optimality guarantees of the proposed filtering-based gating\nalgorithm (via optimality guarantees for its parallel Bayesian filtering and\nits robust aggregation steps), and **(III)** empirical evaluation and ablative\nresults using state-of-the-art foundational and MoE LLMs on a real-world\n__Financial Market Movement__ task where MoE-F attains a remarkable 17\\%\nabsolute and 48.5\\% relative F1 measure improvement over the next best\nperforming individual LLM expert predicting short-horizon market movement based\non streaming news. Further, we provide empirical evidence of substantial\nperformance gains in applying MoE-F over specialized models in the long-horizon\ntime-series forecasting domain.", "published": "2024-06-05 05:53:50", "link": "http://arxiv.org/abs/2406.02969v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-fin.CP", "q-fin.MF", "60J05, 60G35, 68T20, 68T42, 68T50", "I.2.6; I.2.7; G.3"], "primary_category": "cs.LG"}
{"title": "Once more Diarization: Improving meeting transcription systems through\n  segment-level speaker reassignment", "abstract": "Diarization is a crucial component in meeting transcription systems to ease\nthe challenges of speech enhancement and attribute the transcriptions to the\ncorrect speaker. Particularly in the presence of overlapping or noisy speech,\nthese systems have problems reliably assigning the correct speaker labels,\nleading to a significant amount of speaker confusion errors. We propose to add\nsegment-level speaker reassignment to address this issue. By revisiting, after\nspeech enhancement, the speaker attribution for each segment, speaker confusion\nerrors from the initial diarization stage are significantly reduced. Through\nexperiments across different system configurations and datasets, we further\ndemonstrate the effectiveness and applicability in various domains. Our results\nshow that segment-level speaker reassignment successfully rectifies at least\n40% of speaker confusion word errors, highlighting its potential for enhancing\ndiarization accuracy in meeting transcription systems.", "published": "2024-06-05 11:32:13", "link": "http://arxiv.org/abs/2406.03155v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CoLLAB: A Collaborative Approach for Multilingual Abuse Detection", "abstract": "In this study, we investigate representations from paralingual Pre-Trained\nmodel (PTM) for Audio Abuse Detection (AAD), which has not been explored for\nAAD. Our results demonstrate their superiority compared to other PTM\nrepresentations on the ADIMA benchmark. Furthermore, combining PTM\nrepresentations enhances AAD performance. Despite these improvements,\nchallenges with cross-lingual generalizability still remain, and certain\nlanguages require training in the same language. This demands individual models\nfor different languages, leading to scalability, maintenance, and resource\nallocation issues and hindering the practical deployment of AAD systems in\nlinguistically diverse real-world environments. To address this, we introduce\nCoLLAB, a novel framework that doesn't require training and allows seamless\nmerging of models trained in different languages through weight-averaging. This\nresults in a unified model with competitive AAD performance across multiple\nlanguages.", "published": "2024-06-05 12:43:33", "link": "http://arxiv.org/abs/2406.03205v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Reference Channel Selection by Multi-Channel Masking for End-to-End\n  Multi-Channel Speech Enhancement", "abstract": "In end-to-end multi-channel speech enhancement, the traditional approach of\ndesignating one microphone signal as the reference for processing may not\nalways yield optimal results. The limitation is particularly in scenarios with\nlarge distributed microphone arrays with varying speaker-to-microphone\ndistances or compact, highly directional microphone arrays where speaker or\nmicrophone positions change over time. Current mask-based methods often fix the\nreference channel during training, which makes it not possible to adaptively\nselect the reference channel for optimal performance. To address this problem,\nwe introduce an adaptive approach for selecting the optimal reference channel.\nOur method leverages a multi-channel masking-based scheme, where multiple\nmasked signals are combined to generate a single-channel output signal. This\nenhanced signal is then used for loss calculation, while the reference clean\nspeech is adjusted based on the highest scale-invariant signal-to-distortion\nratio (SI-SDR). The experimental results on the Spear challenge simulated\ndataset D4 demonstrate the superiority of our proposed method over the\nconventional approach of using a fixed reference channel with single-channel\nmasking.", "published": "2024-06-05 13:05:42", "link": "http://arxiv.org/abs/2406.03228v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "NeuRO: An Application for Code-Switched Autism Detection in Children", "abstract": "Code-switching is a common communication phenomenon where individuals\nalternate between two or more languages or linguistic styles within a single\nconversation. Autism Spectrum Disorder (ASD) is a developmental disorder posing\nchallenges in social interaction, communication, and repetitive behaviors.\nDetecting ASD in individuals with code-switch scenario presents unique\nchallenges. In this paper, we address this problem by building an application\nNeuRO which aims to detect potential signs of autism in code-switched\nconversations, facilitating early intervention and support for individuals with\nASD.", "published": "2024-06-05 13:38:18", "link": "http://arxiv.org/abs/2406.03514v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ConPCO: Preserving Phoneme Characteristics for Automatic Pronunciation\n  Assessment Leveraging Contrastive Ordinal Regularization", "abstract": "Automatic pronunciation assessment (APA) manages to evaluate the\npronunciation proficiency of a second language (L2) learner in a target\nlanguage. Existing efforts typically draw on regression models for proficiency\nscore prediction, where the models are trained to estimate target values\nwithout explicitly accounting for phoneme-awareness in the feature space. In\nthis paper, we propose a contrastive phonemic ordinal regularizer (ConPCO)\ntailored for regression-based APA models to generate more\nphoneme-discriminative features while considering the ordinal relationships\namong the regression targets. The proposed ConPCO first aligns the phoneme\nrepresentations of an APA model and textual embeddings of phonetic\ntranscriptions via contrastive learning. Afterward, the phoneme characteristics\nare retained by regulating the distances between inter- and intra-phoneme\ncategories in the feature space while allowing for the ordinal relationships\namong the output targets. We further design and develop a hierarchical APA\nmodel to evaluate the effectiveness of our method. Extensive experiments\nconducted on the speechocean762 benchmark dataset suggest the feasibility and\nefficacy of our approach in relation to some cutting-edge baselines.", "published": "2024-06-05 02:16:05", "link": "http://arxiv.org/abs/2406.02859v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "USM RNN-T model weights binarization", "abstract": "Large-scale universal speech models (USM) are already used in production.\nHowever, as the model size grows, the serving cost grows too. Serving cost of\nlarge models is dominated by model size that is why model size reduction is an\nimportant research topic. In this work we are focused on model size reduction\nusing weights only quantization. We present the weights binarization of USM\nRecurrent Neural Network Transducer (RNN-T) and show that its model size can be\nreduced by 15.9x times at cost of word error rate (WER) increase by only 1.9%\nin comparison to the float32 model. It makes it attractive for practical\napplications.", "published": "2024-06-05 03:08:37", "link": "http://arxiv.org/abs/2406.02887v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive\n  Modeling of Audio Discrete Codes", "abstract": "Prior works have demonstrated zero-shot text-to-speech by using a generative\nlanguage model on audio tokens obtained via a neural audio codec. It is still\nchallenging, however, to adapt them to low-latency scenarios. In this paper, we\npresent LiveSpeech - a fully autoregressive language model-based approach for\nzero-shot text-to-speech, enabling low-latency streaming of the output audio.\nTo allow multiple token prediction within a single decoding step, we propose\n(1) using adaptive codebook loss weights that consider codebook contribution in\neach frame and focus on hard instances, and (2) grouping codebooks and\nprocessing groups in parallel. Experiments show our proposed models achieve\ncompetitive results to state-of-the-art baselines in terms of content accuracy,\nspeaker similarity, audio quality, and inference speed while being suitable for\nlow-latency streaming applications.", "published": "2024-06-05 03:36:11", "link": "http://arxiv.org/abs/2406.02897v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robots Have Been Seen and Not Heard: Effects of Consequential Sounds on\n  Human-Perception of Robots", "abstract": "Robots make compulsory machine sounds, known as `consequential sounds', as\nthey move and operate. As robots become more prevalent in workplaces, homes and\npublic spaces, understanding how sounds produced by robots affect\nhuman-perceptions of these robots is becoming increasingly important to\ncreating positive human robot interactions (HRI). This paper presents the\nresults from 182 participants (858 trials) investigating how human-perception\nof robots is changed by consequential sounds. In a between-participants study,\nparticipants in the sound condition were shown 5 videos of different robots and\nasked their opinions on the robots and the sounds they made. This was compared\nto participants in the control condition who viewed silent videos.\nConsequential sounds correlated with significantly more negative perceptions of\nrobots, including increased negative `associated affects', feeling more\ndistracted, and being less willing to colocate in a shared environment with\nrobots.", "published": "2024-06-05 04:54:36", "link": "http://arxiv.org/abs/2406.02938v2", "categories": ["cs.RO", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with\n  Dual-Decoding Product-Quantized Variational Auto-Encoder", "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by\n``index collapse'', where only a small number of codewords are activated in\nlarge codebooks. This work proposes product-quantized (PQ) VAE with more\ncodebooks but fewer codewords to address this problem and build large-codebook\nspeech tokenizers. It encodes speech features into multiple VQ subspaces and\ncomposes them into codewords in a larger codebook. Besides, to utilize each VQ\nsubspace well, we also enhance PQ-VAE via a dual-decoding training strategy\nwith the encoding and quantized sequences. The experimental results demonstrate\nthat PQ-VAE addresses ``index collapse\" effectively, especially for larger\ncodebooks. The model with the proposed training strategy further improves\ncodebook perplexity and reconstruction quality, outperforming other\nmulti-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in\nlanguage-model-based TTS, supporting higher-quality speech generation with\nlarger codebooks.", "published": "2024-06-05 04:54:49", "link": "http://arxiv.org/abs/2406.02940v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dataset-Distillation Generative Model for Speech Emotion Recognition", "abstract": "Deep learning models for speech rely on large datasets, presenting\ncomputational challenges. Yet, performance hinges on training data size.\nDataset Distillation (DD) aims to learn a smaller dataset without much\nperformance degradation when training with it. DD has been investigated in\ncomputer vision but not yet in speech. This paper presents the first approach\nfor DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ\nGenerative Adversarial Networks (GANs) not to mimic real data but to distil key\ndiscriminative information of IEMOCAP that is useful for downstream training.\nThe GAN then replaces the original dataset and can sample custom synthetic\ndataset sizes. It performs comparably when following the original class\nimbalance but improves performance by 0.3% absolute UAR with balanced classes.\nIt also reduces dataset storage and accelerates downstream training by 95% in\nboth cases and reduces speaker information which could help for a privacy\napplication.", "published": "2024-06-05 05:38:46", "link": "http://arxiv.org/abs/2406.02963v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singing Voice Graph Modeling for SingFake Detection", "abstract": "Detecting singing voice deepfakes, or SingFake, involves determining the\nauthenticity and copyright of a singing voice. Existing models for speech\ndeepfake detection have struggled to adapt to unseen attacks in this unique\nsinging voice domain of human vocalization. To bridge the gap, we present a\ngroundbreaking SingGraph model. The model synergizes the capabilities of the\nMERT acoustic music understanding model for pitch and rhythm analysis with the\nwav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate\nfor using RawBoost and beat matching techniques grounded in music domain\nknowledge for singing voice augmentation, thereby enhancing SingFake detection\nperformance. Our proposed method achieves new state-of-the-art (SOTA) results\nwithin the SingFake dataset, surpassing the previous SOTA model across three\ndistinct scenarios: it improves EER relatively for seen singers by 13.2%, for\nunseen singers by 24.3%, and unseen singers using different codecs by 37.1%.", "published": "2024-06-05 10:02:56", "link": "http://arxiv.org/abs/2406.03111v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Frame-based Attention Interpretation Method for Relevant Acoustic\n  Feature Extraction in Long Speech Depression Detection", "abstract": "Speech-based depression detection tools could help early screening of\ndepression. Here, we address two issues that may hinder the clinical\npracticality of such tools: segment-level labelling noise and a lack of model\ninterpretability. We propose a speech-level Audio Spectrogram Transformer to\navoid segment-level labelling. We observe that the proposed model significantly\noutperforms a segment-level model, providing evidence for the presence of\nsegment-level labelling noise in audio modality and the advantage of\nlonger-duration speech analysis for depression detection. We introduce a\nframe-based attention interpretation method to extract acoustic features from\nprediction-relevant waveform signals for interpretation by clinicians. Through\ninterpretation, we observe that the proposed model identifies reduced loudness\nand F0 as relevant signals of depression, which aligns with the speech\ncharacteristics of depressed patients documented in clinical studies.", "published": "2024-06-05 10:47:00", "link": "http://arxiv.org/abs/2406.03138v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Fake Audio Detection via Deep Stable Learning", "abstract": "Although current fake audio detection approaches have achieved remarkable\nsuccess on specific datasets, they often fail when evaluated with datasets from\ndifferent distributions. Previous studies typically address distribution shift\nby focusing on using extra data or applying extra loss restrictions during\ntraining. However, these methods either require a substantial amount of data or\ncomplicate the training process. In this work, we propose a stable\nlearning-based training scheme that involves a Sample Weight Learning (SWL)\nmodule, addressing distribution shift by decorrelating all selected features\nvia learning weights from training samples. The proposed portable plug-in-like\nSWL is easy to apply to multiple base models and generalizes them without using\nextra data during training. Experiments conducted on the ASVspoof datasets\nclearly demonstrate the effectiveness of SWL in generalizing different models\nacross three evaluation datasets from different distributions.", "published": "2024-06-05 13:16:31", "link": "http://arxiv.org/abs/2406.03237v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake\n  Audio Detection", "abstract": "The generalization of Fake Audio Detection (FAD) is critical due to the\nemergence of new spoofing techniques. Traditional FAD methods often focus\nsolely on distinguishing between genuine and known spoofed audio. We propose a\nGenuine-Focused Learning (GFL) framework guided, aiming for highly generalized\nFAD, called GFL-FAD. This method incorporates a Counterfactual Reasoning\nEnhanced Representation (CRER) based on audio reconstruction using the Mask\nAutoEncoder (MAE) architecture to accurately model genuine audio features. To\nreduce the influence of spoofed audio during training, we introduce a genuine\naudio reconstruction loss, maintaining the focus on learning genuine data\nfeatures. In addition, content-related bottleneck (BN) features are extracted\nfrom the MAE to supplement the knowledge of the original audio. These BN\nfeatures are adaptively fused with CRER to further improve robustness. Our\nmethod achieves state-of-the-art performance with an EER of 0.25% on\nASVspoof2019 LA.", "published": "2024-06-05 13:22:09", "link": "http://arxiv.org/abs/2406.03247v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UrBAN: Urban Beehive Acoustics and PheNotyping Dataset", "abstract": "In this paper, we present a multimodal dataset obtained from a honey bee\ncolony in Montr\\'eal, Quebec, Canada, spanning the years of 2021 to 2022. This\napiary comprised 10 beehives, with microphones recording more than 2000 hours\nof high quality raw audio, and also sensors capturing temperature, and\nhumidity. Periodic hive inspections involved monitoring colony honey bee\npopulation changes, assessing queen-related conditions, and documenting overall\nhive health. Additionally, health metrics, such as Varroa mite infestation\nrates and winter mortality assessments were recorded, offering valuable\ninsights into factors affecting hive health status and resilience. In this\nstudy, we first outline the data collection process, sensor data description,\nand dataset structure. Furthermore, we demonstrate a practical application of\nthis dataset by extracting various features from the raw audio to predict\ncolony population using the number of frames of bees as a proxy.", "published": "2024-06-05 23:53:30", "link": "http://arxiv.org/abs/2406.03657v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection", "abstract": "With the rapid growth in deepfake video content, we require improved and\ngeneralizable methods to detect them. Most existing detection methods either\nuse uni-modal cues or rely on supervised training to capture the dissonance\nbetween the audio and visual modalities. While the former disregards the\naudio-visual correspondences entirely, the latter predominantly focuses on\ndiscerning audio-visual cues within the training corpus, thereby potentially\noverlooking correspondences that can help detect unseen deepfakes. We present\nAudio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method\nthat explicitly captures the correspondence between the audio and visual\nmodalities for improved deepfake detection. The first stage pursues\nrepresentation learning via self-supervision on real videos to capture the\nintrinsic audio-visual correspondences. To extract rich cross-modal\nrepresentations, we use contrastive learning and autoencoding objectives, and\nintroduce a novel audio-visual complementary masking and feature fusion\nstrategy. The learned representations are tuned in the second stage, where\ndeepfake classification is pursued via supervised learning on both real and\nfake videos. Extensive experiments and analysis suggest that our novel\nrepresentation learning paradigm is highly discriminative in nature. We report\n98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the\ncurrent audio-visual state-of-the-art by 14.9% and 9.9%, respectively.", "published": "2024-06-05 05:20:12", "link": "http://arxiv.org/abs/2406.02951v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding\n  using Contrastive Learning with Application to Room Shape Classification", "abstract": "This paper focuses on room fingerprinting, a task involving the analysis of\nan audio recording to determine the specific volume and shape of the room in\nwhich it was captured. While it is relatively straightforward to determine the\nbasic room parameters from the Room Impulse Responses (RIR), doing so from a\nspeech signal is a cumbersome task. To address this challenge, we introduce a\ndual-encoder architecture that facilitates the estimation of room parameters\ndirectly from speech utterances. During pre-training, one encoder receives the\nRIR while the other processes the reverberant speech signal. A contrastive loss\nfunction is employed to embed the speech and the acoustic response jointly. In\nthe fine-tuning stage, the specific classification task is trained. In the test\nphase, only the reverberant utterance is available, and its embedding is used\nfor the task of room shape classification. The proposed scheme is extensively\nevaluated using simulated acoustic environments.", "published": "2024-06-05 10:13:55", "link": "http://arxiv.org/abs/2406.03120v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm\n  with Real Emphasis and Fake Dispersion Strategy", "abstract": "With the proliferation of deepfake audio, there is an urgent need to\ninvestigate their attribution. Current source tracing methods can effectively\ndistinguish in-distribution (ID) categories. However, the rapid evolution of\ndeepfake algorithms poses a critical challenge in the accurate identification\nof out-of-distribution (OOD) novel deepfake algorithms. In this paper, we\npropose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake\nalgorithm recognition, demonstrating its effectiveness in discriminating ID\nsamples while identifying OOD samples. For effective OOD detection, we first\nexplore current post-hoc OOD methods and propose NSD, a novel OOD approach in\nidentifying novel deepfake algorithms through the similarity consideration of\nboth feature and logits scores. REFD achieves 86.83% F1-score as a single\nsystem in Audio Deepfake Detection Challenge 2023 Track3, showcasing its\nstate-of-the-art performance.", "published": "2024-06-05 13:16:55", "link": "http://arxiv.org/abs/2406.03240v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in\n  Meetings", "abstract": "Speaker Diarization (SD) aims at grouping speech segments that belong to the\nsame speaker. This task is required in many speech-processing applications,\nsuch as rich meeting transcription. In this context, distant microphone arrays\nusually capture the audio signal. Beamforming, i.e., spatial filtering, is a\ncommon practice to process multi-microphone audio data. However, it often\nrequires an explicit localization of the active source to steer the filter.\nThis paper proposes a self-attention-based algorithm to select the output of a\nbank of fixed spatial filters. This method serves as a feature extractor for\njoint Voice Activity (VAD) and Overlapped Speech Detection (OSD). The speaker\ndiarization is then inferred from the detected segments. The approach shows\nconvincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the\nAISHELL-4 dataset. The analysis of the self-attention weights demonstrates\ntheir explainability, as they correlate with the speaker's angular locations.", "published": "2024-06-05 13:28:28", "link": "http://arxiv.org/abs/2406.03251v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Microphone Speech Emotion Recognition using the Hierarchical\n  Token-semantic Audio Transformer Architecture", "abstract": "The performance of most emotion recognition systems degrades in real-life\nsituations ('in the wild' scenarios) where the audio is contaminated by\nreverberation. Our study explores new methods to alleviate the performance\ndegradation of SER algorithms and develop a more robust system for adverse\nconditions. We propose processing multi-microphone signals to address these\nchallenges and improve emotion classification accuracy. We adopt a\nstate-of-the-art transformer model, the HTS-AT, to handle multi-channel audio\ninputs. We evaluate two strategies: averaging mel-spectrograms across channels\nand summing patch-embedded representations. Our multi-microphone model achieves\nsuperior performance compared to single-channel baselines when tested on\nreal-world reverberant environments.", "published": "2024-06-05 13:50:59", "link": "http://arxiv.org/abs/2406.03272v3", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Enhancing CTC-based speech recognition with diverse modeling units", "abstract": "In recent years, the evolution of end-to-end (E2E) automatic speech\nrecognition (ASR) models has been remarkable, largely due to advances in deep\nlearning architectures like transformer. On top of E2E systems, researchers\nhave achieved substantial accuracy improvement by rescoring E2E model's N-best\nhypotheses with a phoneme-based model. This raises an interesting question\nabout where the improvements come from other than the system combination\neffect. We examine the underlying mechanisms driving these gains and propose an\nefficient joint training approach, where E2E models are trained jointly with\ndiverse modeling units. This methodology does not only align the strengths of\nboth phoneme and grapheme-based models but also reveals that using these\ndiverse modeling units in a synergistic way can significantly enhance model\naccuracy. Our findings offer new insights into the optimal integration of\nheterogeneous modeling units in the development of more robust and accurate ASR\nsystems.", "published": "2024-06-05 13:52:55", "link": "http://arxiv.org/abs/2406.03274v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Mamba: Bidirectional State Space Model for Audio Representation\n  Learning", "abstract": "Transformers have rapidly become the preferred choice for audio\nclassification, surpassing methods based on CNNs. However, Audio Spectrogram\nTransformers (ASTs) exhibit quadratic scaling due to self-attention. The\nremoval of this quadratic self-attention cost presents an appealing direction.\nRecently, state space models (SSMs), such as Mamba, have demonstrated potential\nin language and vision tasks in this regard. In this study, we explore whether\nreliance on self-attention is necessary for audio classification tasks. By\nintroducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based\nmodel for audio classification, we aim to address this question. We evaluate\nAuM on various audio datasets - comprising six different benchmarks - where it\nachieves comparable or better performance compared to well-established AST\nmodel.", "published": "2024-06-05 15:00:59", "link": "http://arxiv.org/abs/2406.03344v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech\n  Enhancement", "abstract": "To obtain improved speech enhancement models, researchers often focus on\nincreasing performance according to specific instrumental metrics. However,\nwhen the same metric is used in a loss function to optimize models, it may be\ndetrimental to aspects that the given metric does not see. The goal of this\npaper is to illustrate the risk of overfitting a speech enhancement model to\nthe metric used for evaluation. For this, we introduce enhancement models that\nexploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82\nPESQ on VB-DMD while scoring very poorly in a listening experiment. While the\nobtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on\nthe VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric,\nan isolated evaluation on the same metric may be misleading. Instead, other\nmetrics should be included in the evaluation and the resulting performance\npredictions should be confirmed by listening.", "published": "2024-06-05 17:07:39", "link": "http://arxiv.org/abs/2406.03460v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-based Clinical Depression Screening: An Empirical Study", "abstract": "This study investigates the utility of speech signals for AI-based depression\nscreening across varied interaction scenarios, including psychiatric\ninterviews, chatbot conversations, and text readings. Participants include\ndepressed patients recruited from the outpatient clinics of Peking University\nSixth Hospital and control group members from the community, all diagnosed by\npsychiatrists following standardized diagnostic protocols. We extracted\nacoustic and deep speech features from each participant's segmented recordings.\nClassifications were made using neural networks or SVMs, with aggregated clip\noutcomes determining final assessments. Our analysis across interaction\nscenarios, speech processing techniques, and feature types confirms speech as a\ncrucial marker for depression screening. Specifically, human-computer\ninteraction matches clinical interview efficacy, surpassing reading tasks.\nSegment duration and quantity significantly affect model performance, with deep\nspeech features substantially outperforming traditional acoustic features.", "published": "2024-06-05 09:43:54", "link": "http://arxiv.org/abs/2406.03510v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Harder or Different? Understanding Generalization of Audio Deepfake\n  Detection", "abstract": "Recent research has highlighted a key issue in speech deepfake detection:\nmodels trained on one set of deepfakes perform poorly on others. The question\narises: is this due to the continuously improving quality of Text-to-Speech\n(TTS) models, i.e., are newer DeepFakes just 'harder' to detect? Or, is it\nbecause deepfakes generated with one model are fundamentally different to those\ngenerated using another model? We answer this question by decomposing the\nperformance gap between in-domain and out-of-domain test data into 'hardness'\nand 'difference' components. Experiments performed using ASVspoof databases\nindicate that the hardness component is practically negligible, with the\nperformance gap being attributed primarily to the difference component. This\nhas direct implications for real-world deepfake detection, highlighting that\nmerely increasing model capacity, the currently-dominant research trend, may\nnot effectively address the generalization challenge.", "published": "2024-06-05 10:33:15", "link": "http://arxiv.org/abs/2406.03512v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
