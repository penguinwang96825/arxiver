{"title": "Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020", "abstract": "This overview paper describes the first shared task on fake news detection in\nUrdu language. The task was posed as a binary classification task, in which the\ngoal is to differentiate between real and fake news. We provided a dataset\ndivided into 900 annotated news articles for training and 400 news articles for\ntesting. The dataset contained news in five domains: (i) Health, (ii) Sports,\n(iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different\ncountries (India, China, Egypt, Germany, Pakistan, and the UK) registered for\nthe task. 9 teams submitted their experimental results. The participants used\nvarious machine learning methods ranging from feature-based traditional machine\nlearning to neural networks techniques. The best performing system achieved an\nF-score value of 0.90, showing that the BERT-based approach outperforms other\nmachine learning techniques", "published": "2022-07-25 03:41:32", "link": "http://arxiv.org/abs/2207.11893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Generation Meets Real People: Building a Social, Informative\n  Open-Domain Dialogue Agent", "abstract": "We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both\ninformative and conversational, our bot chats with users in an authentic,\nemotionally intelligent way. By integrating controlled neural generation with\nscaffolded, hand-written dialogue, we let both the user and bot take turns\ndriving the conversation, producing an engaging and socially fluent experience.\nDeployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge,\nChirpy Cardinal handled thousands of conversations per day, placing second out\nof nine bots with an average user rating of 3.58/5.", "published": "2022-07-25 09:57:23", "link": "http://arxiv.org/abs/2207.12021v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What makes you change your mind? An empirical investigation in online\n  group decision-making conversations", "abstract": "People leverage group discussions to collaborate in order to solve complex\ntasks, e.g. in project meetings or hiring panels. By doing so, they engage in a\nvariety of conversational strategies where they try to convince each other of\nthe best approach and ultimately reach a decision. In this work, we investigate\nmethods for detecting what makes someone change their mind. To this end, we\nleverage a recently introduced dataset containing group discussions of people\ncollaborating to solve a task. To find out what makes someone change their\nmind, we incorporate various techniques such as neural text classification and\nlanguage-agnostic change point detection. Evaluation of these methods shows\nthat while the task is not trivial, the best way to approach it is using a\nlanguage-aware model with learning-to-rank training. Finally, we examine the\ncues that the models develop as indicative of the cause of a change of mind.", "published": "2022-07-25 10:19:31", "link": "http://arxiv.org/abs/2207.12035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Semi-Supervised Task Oriented Dialog Systems by JSA Learning\n  of Discrete Latent Variable Models", "abstract": "Developing semi-supervised task-oriented dialog (TOD) systems by leveraging\nunlabeled dialog data has attracted increasing interests. For semi-supervised\nlearning of latent state TOD models, variational learning is often used, but\nsuffers from the annoying high-variance of the gradients propagated through\ndiscrete latent variables and the drawback of indirectly optimizing the target\nlog-likelihood. Recently, an alternative algorithm, called joint stochastic\napproximation (JSA), has emerged for learning discrete latent variable models\nwith impressive performances. In this paper, we propose to apply JSA to\nsemi-supervised learning of the latent state TOD models, which is referred to\nas JSA-TOD. To our knowledge, JSA-TOD represents the first work in developing\nJSA based semi-supervised learning of discrete latent variable conditional\nmodels for such long sequential generation problems like in TOD systems.\nExtensive experiments show that JSA-TOD significantly outperforms its\nvariational learning counterpart. Remarkably, semi-supervised JSA-TOD using 20%\nlabels performs close to the full-supervised baseline on MultiWOZ2.1.", "published": "2022-07-25 14:36:10", "link": "http://arxiv.org/abs/2207.12235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu", "abstract": "This paper gives the overview of the first shared task at FIRE 2020 on fake\nnews detection in the Urdu language. This is a binary classification task in\nwhich the goal is to identify fake news using a dataset composed of 900\nannotated news articles for training and 400 news articles for testing. The\ndataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz,\n(iv) Technology, and (v) Business. 42 teams from 6 different countries (India,\nChina, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams\nsubmitted their experimental results. The participants used various machine\nlearning methods ranging from feature-based traditional machine learning to\nneural network techniques. The best performing system achieved an F-score value\nof 0.90, showing that the BERT-based approach outperforms other machine\nlearning classifiers.", "published": "2022-07-25 03:46:51", "link": "http://arxiv.org/abs/2207.12406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Speaker Diarization that is Agnostic to Language,\n  Overlap-Aware, and Tuning Free", "abstract": "Podcasts are conversational in nature and speaker changes are frequent --\nrequiring speaker diarization for content understanding. We propose an\nunsupervised technique for speaker diarization without relying on\nlanguage-specific components. The algorithm is overlap-aware and does not\nrequire information about the number of speakers. Our approach shows 79%\nimprovement on purity scores (34% on F-score) against the Google Cloud Platform\nsolution on podcast data.", "published": "2022-07-25 20:07:14", "link": "http://arxiv.org/abs/2207.12504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit", "abstract": "Dialog system developers need high-quality data to train, fine-tune and\nassess their systems. They often use crowdsourcing for this since it provides\nlarge quantities of data from many workers. However, the data may not be of\nsufficiently good quality. This can be due to the way that the requester\npresents a task and how they interact with the workers. This paper introduces\nDialCrowd 2.0 to help requesters obtain higher quality data by, for example,\npresenting tasks more clearly and facilitating effective communication with\nworkers. DialCrowd 2.0 guides developers in creating improved Human\nIntelligence Tasks (HITs) and is directly applicable to the workflows used\ncurrently by developers and researchers.", "published": "2022-07-25 22:06:20", "link": "http://arxiv.org/abs/2207.12551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Innovations in Neural Data-to-text Generation: A Survey", "abstract": "The neural boom that has sparked natural language processing (NLP) research\nthrough the last decade has similarly led to significant innovations in\ndata-to-text generation (DTG). This survey offers a consolidated view into the\nneural DTG paradigm with a structured examination of the approaches, benchmark\ndatasets, and evaluation protocols. This survey draws boundaries separating DTG\nfrom the rest of the natural language generation (NLG) landscape, encompassing\nan up-to-date synthesis of the literature, and highlighting the stages of\ntechnological adoption from within and outside the greater NLG umbrella. With\nthis holistic view, we highlight promising avenues for DTG research that not\nonly focus on the design of linguistically capable systems but also systems\nthat exhibit fairness and accountability.", "published": "2022-07-25 23:21:48", "link": "http://arxiv.org/abs/2207.12571v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Bot Response Contradiction Detection via Utterance Rewriting", "abstract": "Though chatbots based on large neural models can often produce fluent\nresponses in open domain conversations, one salient error type is contradiction\nor inconsistency with the preceding conversation turns. Previous work has\ntreated contradiction detection in bot responses as a task similar to natural\nlanguage inference, e.g., detect the contradiction between a pair of bot\nutterances. However, utterances in conversations may contain co-references or\nellipsis, and using these utterances as is may not always be sufficient for\nidentifying contradictions. This work aims to improve the contradiction\ndetection via rewriting all bot utterances to restore antecedents and ellipsis.\nWe curated a new dataset for utterance rewriting and built a rewriting model on\nit. We empirically demonstrate that this model can produce satisfactory\nrewrites to make bot utterances more complete. Furthermore, using rewritten\nutterances improves contradiction detection performance significantly, e.g.,\nthe AUPR and joint accuracy scores (detecting contradiction along with\nevidence) increase by 6.5% and 4.5% (absolute increase), respectively.", "published": "2022-07-25 00:54:30", "link": "http://arxiv.org/abs/2207.11862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is GPT-3 all you need for Visual Question Answering in Cultural\n  Heritage?", "abstract": "The use of Deep Learning and Computer Vision in the Cultural Heritage domain\nis becoming highly relevant in the last few years with lots of applications\nabout audio smart guides, interactive museums and augmented reality. All these\ntechnologies require lots of data to work effectively and be useful for the\nuser. In the context of artworks, such data is annotated by experts in an\nexpensive and time consuming process. In particular, for each artwork, an image\nof the artwork and a description sheet have to be collected in order to perform\ncommon tasks like Visual Question Answering. In this paper we propose a method\nfor Visual Question Answering that allows to generate at runtime a description\nsheet that can be used for answering both visual and contextual questions about\nthe artwork, avoiding completely the image and the annotation process. For this\npurpose, we investigate on the use of GPT-3 for generating descriptions for\nartworks analyzing the quality of generated descriptions through captioning\nmetrics. Finally we evaluate the performance for Visual Question Answering and\ncaptioning tasks.", "published": "2022-07-25 12:12:46", "link": "http://arxiv.org/abs/2207.12101v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Post-processing Networks: Method for Optimizing Pipeline Task-oriented\n  Dialogue Systems using Reinforcement Learning", "abstract": "Many studies have proposed methods for optimizing the dialogue performance of\nan entire pipeline task-oriented dialogue system by jointly training modules in\nthe system using reinforcement learning. However, these methods are limited in\nthat they can only be applied to modules implemented using trainable\nneural-based methods. To solve this problem, we propose a method for optimizing\na pipeline system composed of modules implemented with arbitrary methods for\ndialogue performance. With our method, neural-based components called\npost-processing networks (PPNs) are installed inside such a system to\npost-process the output of each module. All PPNs are updated to improve the\noverall dialogue performance of the system by using reinforcement learning, not\nnecessitating each module to be differentiable. Through dialogue simulation and\nhuman evaluation on the MultiWOZ dataset, we show that our method can improve\nthe dialogue performance of pipeline systems consisting of various modules.", "published": "2022-07-25 13:22:40", "link": "http://arxiv.org/abs/2207.12185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning", "abstract": "Despite recent advances in natural language understanding and generation, and\ndecades of research on the development of conversational bots, building\nautomated agents that can carry on rich open-ended conversations with humans\n\"in the wild\" remains a formidable challenge. In this work we develop a\nreal-time, open-ended dialogue system that uses reinforcement learning (RL) to\npower a bot's conversational skill at scale. Our work pairs the succinct\nembedding of the conversation state generated using SOTA (supervised) language\nmodels with RL techniques that are particularly suited to a dynamic action\nspace that changes as the conversation progresses. Trained using crowd-sourced\ndata, our novel system is able to substantially exceeds the (strong) baseline\nsupervised model with respect to several metrics of interest in a live\nexperiment with real users of the Google Assistant.", "published": "2022-07-25 16:12:33", "link": "http://arxiv.org/abs/2208.02294v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning a Dual-Mode Speech Recognition Model via Self-Pruning", "abstract": "There is growing interest in unifying the streaming and full-context\nautomatic speech recognition (ASR) networks into a single end-to-end ASR model\nto simplify the model training and deployment for both use cases. While in\nreal-world ASR applications, the streaming ASR models typically operate under\nmore storage and computational constraints - e.g., on embedded devices - than\nany server-side full-context models. Motivated by the recent progress in\nOmni-sparsity supernet training, where multiple subnetworks are jointly\noptimized in one single model, this work aims to jointly learn a compact sparse\non-device streaming ASR model, and a large dense server non-streaming model, in\na single supernet. Next, we present that, performing supernet training on both\nwav2vec 2.0 self-supervised learning and supervised ASR fine-tuning can not\nonly substantially improve the large non-streaming model as shown in prior\nworks, and also be able to improve the compact sparse streaming model.", "published": "2022-07-25 05:03:13", "link": "http://arxiv.org/abs/2207.11906v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug\n  Labeling to Enhance Product-Specific Guidance Assessment", "abstract": "Product-specific guidances (PSGs) recommended by the United States Food and\nDrug Administration (FDA) are instrumental to promote and guide generic drug\nproduct development. To assess a PSG, the FDA assessor needs to take extensive\ntime and effort to manually retrieve supportive drug information of absorption,\ndistribution, metabolism, and excretion (ADME) from the reference listed drug\nlabeling. In this work, we leveraged the state-of-the-art pre-trained language\nmodels to automatically label the ADME paragraphs in the pharmacokinetics\nsection from the FDA-approved drug labeling to facilitate PSG assessment. We\napplied a transfer learning approach by fine-tuning the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model to develop\na novel application of ADME semantic labeling, which can automatically retrieve\nADME paragraphs from drug labeling instead of manual work. We demonstrated that\nfine-tuning the pre-trained BERT model can outperform the conventional machine\nlearning techniques, achieving up to 11.6% absolute F1 improvement. To our\nknowledge, we were the first to successfully apply BERT to solve the ADME\nsemantic labeling task. We further assessed the relative contribution of\npre-training and fine-tuning to the overall performance of the BERT model in\nthe ADME semantic labeling task using a series of analysis methods such as\nattention similarity and layer-based ablations. Our analysis revealed that the\ninformation learned via fine-tuning is focused on task-specific knowledge in\nthe top layers of the BERT, whereas the benefit from the pre-trained BERT model\nis from the bottom layers.", "published": "2022-07-25 17:43:36", "link": "http://arxiv.org/abs/2207.12376v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WinoGAViL: Gamified Association Benchmark to Challenge\n  Vision-and-Language Models", "abstract": "While vision-and-language models perform well on tasks such as visual\nquestion answering, they struggle when it comes to basic human commonsense\nreasoning skills. In this work, we introduce WinoGAViL: an online game of\nvision-and-language associations (e.g., between werewolves and a full moon),\nused as a dynamic evaluation benchmark. Inspired by the popular card game\nCodenames, a spymaster gives a textual cue related to several visual\ncandidates, and another player tries to identify them. Human players are\nrewarded for creating associations that are challenging for a rival AI model\nbut still solvable by other human players. We use the game to collect 3.5K\ninstances, finding that they are intuitive for humans (>90% Jaccard index) but\nchallenging for state-of-the-art AI models, where the best model (ViLT)\nachieves a score of 52%, succeeding mostly where the cue is visually salient.\nOur analysis as well as the feedback we collect from players indicate that the\ncollected associations require diverse reasoning skills, including general\nknowledge, common sense, abstraction, and more. We release the dataset, the\ncode and the interactive game, allowing future data collection that can be used\nto develop models with better association abilities.", "published": "2022-07-25 23:57:44", "link": "http://arxiv.org/abs/2207.12576v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unsupervised data selection for Speech Recognition with contrastive loss\n  ratios", "abstract": "This paper proposes an unsupervised data selection method by using a\nsubmodular function based on contrastive loss ratios of target and training\ndata sets. A model using a contrastive loss function is trained on both sets.\nThen the ratio of frame-level losses for each model is used by a submodular\nfunction. By using the submodular function, a training set for automatic speech\nrecognition matching the target data set is selected. Experiments show that\nmodels trained on the data sets selected by the proposed method outperform the\nselection method based on log-likelihoods produced by GMM-HMM models, in terms\nof word error rate (WER). When selecting a fixed amount, e.g. 10 hours of data,\nthe difference between the results of two methods on Tedtalks was 20.23% WER\nrelative. The method can also be used to select data with the aim of minimising\nnegative transfer, while maintaining or improving on performance of models\ntrained on the whole training set. Results show that the WER on the WSJCAM0\ndata set was reduced by 6.26% relative when selecting 85% from the whole data\nset.", "published": "2022-07-25 10:08:59", "link": "http://arxiv.org/abs/2207.12028v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transplantation of Conversational Speaking Style with Interjections in\n  Sequence-to-Sequence Speech Synthesis", "abstract": "Sequence-to-Sequence Text-to-Speech architectures that directly generate low\nlevel acoustic features from phonetic sequences are known to produce natural\nand expressive speech when provided with adequate amounts of training data.\nSuch systems can learn and transfer desired speaking styles from one seen\nspeaker to another (in multi-style multi-speaker settings), which is highly\ndesirable for creating scalable and customizable Human-Computer Interaction\nsystems. In this work we explore one-to-many style transfer from a dedicated\nsingle-speaker conversational corpus with style nuances and interjections. We\nelaborate on the corpus design and explore the feasibility of such style\ntransfer when assisted with Voice-Conversion-based data augmentation. In a set\nof subjective listening experiments, this approach resulted in high-fidelity\nstyle transfer with no quality degradation. However, a certain voice persona\nshift was observed, requiring further improvements in voice conversion.", "published": "2022-07-25 15:32:47", "link": "http://arxiv.org/abs/2207.12262v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ConceptBeam: Concept Driven Target Speech Extraction", "abstract": "We propose a novel framework for target speech extraction based on semantic\ninformation, called ConceptBeam. Target speech extraction means extracting the\nspeech of a target speaker in a mixture. Typical approaches have been\nexploiting properties of audio signals, such as harmonic structure and\ndirection of arrival. In contrast, ConceptBeam tackles the problem with\nsemantic clues. Specifically, we extract the speech of speakers speaking about\na concept, i.e., a topic of interest, using a concept specifier such as an\nimage or speech. Solving this novel problem would open the door to innovative\napplications such as listening systems that focus on a particular topic\ndiscussed in a conversation. Unlike keywords, concepts are abstract notions,\nmaking it challenging to directly represent a target concept. In our scheme, a\nconcept is encoded as a semantic embedding by mapping the concept specifier to\na shared embedding space. This modality-independent space can be built by means\nof deep metric learning using paired data consisting of images and their spoken\ncaptions. We use it to bridge modality-dependent information, i.e., the speech\nsegments in the mixture, and the specified, modality-independent concept. As a\nproof of our scheme, we performed experiments using a set of images associated\nwith spoken captions. That is, we generated speech mixtures from these spoken\ncaptions and used the images or speech signals as the concept specifiers. We\nthen extracted the target speech using the acoustic characteristics of the\nidentified segments. We compare ConceptBeam with two methods: one based on\nkeywords obtained from recognition systems and another based on sound source\nseparation. We show that ConceptBeam clearly outperforms the baseline methods\nand effectively extracts speech based on the semantic representation.", "published": "2022-07-25 08:06:07", "link": "http://arxiv.org/abs/2207.11964v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Label Uncertainty Modeling and Prediction for Speech Emotion Recognition\n  using t-Distributions", "abstract": "As different people perceive others' emotional expressions differently, their\nannotation in terms of arousal and valence are per se subjective. To address\nthis, these emotion annotations are typically collected by multiple annotators\nand averaged across annotators in order to obtain labels for arousal and\nvalence. However, besides the average, also the uncertainty of a label is of\ninterest, and should also be modeled and predicted for automatic emotion\nrecognition. In the literature, for simplicity, label uncertainty modeling is\ncommonly approached with a Gaussian assumption on the collected annotations.\nHowever, as the number of annotators is typically rather small due to resource\nconstraints, we argue that the Gaussian approach is a rather crude assumption.\nIn contrast, in this work we propose to model the label distribution using a\nStudent's t-distribution which allows us to account for the number of\nannotations available. With this model, we derive the corresponding\nKullback-Leibler divergence based loss function and use it to train an\nestimator for the distribution of emotion labels, from which the mean and\nuncertainty can be inferred. Through qualitative and quantitative analysis, we\nshow the benefits of the t-distribution over a Gaussian distribution. We\nvalidate our proposed method on the AVEC'16 dataset. Results reveal that our\nt-distribution based approach improves over the Gaussian approach with\nstate-of-the-art uncertainty modeling results in speech-based emotion\nrecognition, along with an optimal and even faster convergence.", "published": "2022-07-25 12:38:20", "link": "http://arxiv.org/abs/2207.12135v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
