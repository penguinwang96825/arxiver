{"title": "Cross-lingual Distillation for Text Classification", "abstract": "Cross-lingual text classification(CLTC) is the task of classifying documents\nwritten in different languages into the same taxonomy of categories. This paper\npresents a novel approach to CLTC that builds on model distillation, which\nadapts and extends a framework originally proposed for model compression. Using\nsoft probabilistic predictions for the documents in a label-rich language as\nthe (induced) supervisory labels in a parallel corpus of documents, we train\nclassifiers successfully for new languages in which labeled training data are\nnot available. An adversarial feature adaptation technique is also applied\nduring the model training to reduce distribution mismatch. We conducted\nexperiments on two benchmark CLTC datasets, treating English as the source\nlanguage and German, French, Japan and Chinese as the unlabeled target\nlanguages. The proposed approach had the advantageous or comparable performance\nof the other state-of-art methods.", "published": "2017-05-05 03:36:11", "link": "http://arxiv.org/abs/1705.02073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Argumentation Structures in Chinese Hotel Reviews", "abstract": "Argumentation mining aims at automatically extracting the premises-claim\ndiscourse structures in natural language texts. There is a great demand for\nargumentation corpora for customer reviews. However, due to the controversial\nnature of the argumentation annotation task, there exist very few large-scale\nargumentation corpora for customer reviews. In this work, we novelly use the\ncrowdsourcing technique to collect argumentation annotations in Chinese hotel\nreviews. As the first Chinese argumentation dataset, our corpus includes 4814\nargument component annotations and 411 argument relation annotations, and its\nannotations qualities are comparable to some widely used argumentation corpora\nin other languages.", "published": "2017-05-05 03:43:35", "link": "http://arxiv.org/abs/1705.02077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint RNN Model for Argument Component Boundary Detection", "abstract": "Argument Component Boundary Detection (ACBD) is an important sub-task in\nargumentation mining; it aims at identifying the word sequences that constitute\nargument components, and is usually considered as the first sub-task in the\nargumentation mining pipeline. Existing ACBD methods heavily depend on\ntask-specific knowledge, and require considerable human efforts on\nfeature-engineering. To tackle these problems, in this work, we formulate ACBD\nas a sequence labeling problem and propose a variety of Recurrent Neural\nNetwork (RNN) based methods, which do not use domain specific or handcrafted\nfeatures beyond the relative position of the sentence in the document. In\nparticular, we propose a novel joint RNN model that can predict whether\nsentences are argumentative or not, and use the predicted results to more\nprecisely detect the argument component boundaries. We evaluate our techniques\non two corpora from two different genres; results suggest that our joint RNN\nmodel obtain the state-of-the-art performance on both datasets.", "published": "2017-05-05 08:49:14", "link": "http://arxiv.org/abs/1705.02131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Speaker: an End-to-End Neural Speaker Embedding System", "abstract": "We present Deep Speaker, a neural speaker embedding system that maps\nutterances to a hypersphere where speaker similarity is measured by cosine\nsimilarity. The embeddings generated by Deep Speaker can be used for many\ntasks, including speaker identification, verification, and clustering. We\nexperiment with ResCNN and GRU architectures to extract the acoustic features,\nthen mean pool to produce utterance-level speaker embeddings, and train using\ntriplet loss based on cosine similarity. Experiments on three distinct datasets\nsuggest that Deep Speaker outperforms a DNN-based i-vector baseline. For\nexample, Deep Speaker reduces the verification equal error rate by 50%\n(relatively) and improves the identification accuracy by 60% (relatively) on a\ntext-independent dataset. We also present results that suggest adapting from a\nmodel trained with Mandarin can improve accuracy for English speaker\nrecognition.", "published": "2017-05-05 17:10:16", "link": "http://arxiv.org/abs/1705.02304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Morphological Chains for Agglutinative Languages", "abstract": "In this paper, we build morphological chains for agglutinative languages by\nusing a log-linear model for the morphological segmentation task. The model is\nbased on the unsupervised morphological segmentation system called\nMorphoChains. We extend MorphoChains log linear model by expanding the\ncandidate space recursively to cover more split points for agglutinative\nlanguages such as Turkish, whereas in the original model candidates are\ngenerated by considering only binary segmentation of each word. The results\nshow that we improve the state-of-art Turkish scores by 12% having a F-measure\nof 72% and we improve the English scores by 3% having a F-measure of 74%.\nEventually, the system outperforms both MorphoChains and other well-known\nunsupervised morphological segmentation systems. The results indicate that\ncandidate generation plays an important role in such an unsupervised log-linear\nmodel that is learned using contrastive estimation with negative samples.", "published": "2017-05-05 17:30:50", "link": "http://arxiv.org/abs/1705.02314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Learning of Universal Sentence Representations from Natural\n  Language Inference Data", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted. In this paper, we show how universal sentence representations trained\nusing the supervised data of the Stanford Natural Language Inference datasets\ncan consistently outperform unsupervised methods like SkipThought vectors on a\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\nobtain features, which can then be transferred to other tasks, our work tends\nto indicate the suitability of natural language inference for transfer learning\nto other NLP tasks. Our encoder is publicly available.", "published": "2017-05-05 18:54:39", "link": "http://arxiv.org/abs/1705.02364v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Attention: A Context-Aware Alignment Function for Machine\n  Reading", "abstract": "In this paper we propose a neural network model with a novel Sequential\nAttention layer that extends soft attention by assigning weights to words in an\ninput sequence in a way that takes into account not just how well that word\nmatches a query, but how well surrounding words match. We evaluate this\napproach on the task of reading comprehension (on the Who did What and CNN\ndatasets) and show that it dramatically improves a strong baseline--the\nStanford Reader--and is competitive with the state of the art.", "published": "2017-05-05 15:37:11", "link": "http://arxiv.org/abs/1705.02269v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases", "abstract": "The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC", "published": "2017-05-05 17:31:12", "link": "http://arxiv.org/abs/1705.02315v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for\n  Small-Footprint Keyword Spotting", "abstract": "We propose a max-pooling based loss function for training Long Short-Term\nMemory (LSTM) networks for small-footprint keyword spotting (KWS), with low\nCPU, memory, and latency requirements. The max-pooling loss training can be\nfurther guided by initializing with a cross-entropy loss trained network. A\nposterior smoothing based evaluation approach is employed to measure keyword\nspotting performance. Our experimental results show that LSTM models trained\nusing cross-entropy loss or max-pooling loss outperform a cross-entropy loss\ntrained baseline feed-forward Deep Neural Network (DNN). In addition,\nmax-pooling loss trained LSTM with randomly initialized network performs better\ncompared to cross-entropy loss trained LSTM. Finally, the max-pooling loss\ntrained LSTM initialized with a cross-entropy pre-trained network shows the\nbest performance, which yields $67.6\\%$ relative reduction compared to baseline\nfeed-forward DNN in Area Under the Curve (AUC) measure.", "published": "2017-05-05 22:36:04", "link": "http://arxiv.org/abs/1705.02411v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
