{"title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm", "abstract": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM", "published": "2025-09-15 17:53:11", "link": "http://arxiv.org/abs/2509.12190v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences", "abstract": "The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences.", "published": "2025-09-15 17:51:02", "link": "http://arxiv.org/abs/2509.12188v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models", "abstract": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.", "published": "2025-09-15 17:34:45", "link": "http://arxiv.org/abs/2509.12171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing", "abstract": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.", "published": "2025-09-15 17:31:15", "link": "http://arxiv.org/abs/2509.12168v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pun Unintended: LLMs and the Illusion of Humor Understanding", "abstract": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.", "published": "2025-09-15 17:22:30", "link": "http://arxiv.org/abs/2509.12158v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models", "abstract": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.", "published": "2025-09-15 16:57:25", "link": "http://arxiv.org/abs/2509.12132v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models", "abstract": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.", "published": "2025-09-15 16:53:41", "link": "http://arxiv.org/abs/2509.12130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models", "abstract": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.", "published": "2025-09-15 16:41:08", "link": "http://arxiv.org/abs/2509.12112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When marine radar target detection meets pretrained large language models", "abstract": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.", "published": "2025-09-15 16:38:13", "link": "http://arxiv.org/abs/2509.12110v1", "categories": ["eess.SP", "cs.CL", "cs.LG"], "primary_category": "eess.SP"}
{"title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "abstract": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.", "published": "2025-09-15 16:33:56", "link": "http://arxiv.org/abs/2509.12108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-domain SSL pre-training and streaming ASR", "abstract": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.", "published": "2025-09-15 16:25:43", "link": "http://arxiv.org/abs/2509.12101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities", "abstract": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.", "published": "2025-09-15 16:21:59", "link": "http://arxiv.org/abs/2509.12098v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SENSE models: an open source solution for multilingual and multimodal semantic-based tasks", "abstract": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),\nan open-source solution inspired by the SAMU-XLSR framework and conceptually\nsimilar to Meta AI's SONAR models. These approaches rely on a teacher-student\nframework to align a self-supervised speech encoder with the language-agnostic\ncontinuous representations of a text encoder at the utterance level. We\ndescribe how the original SAMU-XLSR method has been updated by selecting a\nstronger teacher text model and a better initial speech encoder. The source\ncode for training and using SENSE models has been integrated into the\nSpeechBrain toolkit, and the first SENSE model we trained has been publicly\nreleased. We report experimental results on multilingual and multimodal\nsemantic tasks, where our SENSE model achieves highly competitive performance.\nFinally, this study offers new insights into how semantics are captured in such\nsemantically aligned speech encoders.", "published": "2025-09-15 16:18:51", "link": "http://arxiv.org/abs/2509.12093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss", "abstract": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions.", "published": "2025-09-15 16:16:57", "link": "http://arxiv.org/abs/2509.12089v1", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect", "abstract": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.", "published": "2025-09-15 15:48:09", "link": "http://arxiv.org/abs/2509.12065v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval", "abstract": "Financial disclosures such as 10-K filings present challenging retrieval\nproblems due to their length, regulatory section hierarchy, and domain-specific\nlanguage, which standard retrieval-augmented generation (RAG) models underuse.\nWe introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a\nretrieval framework tailored to financial documents. FinGEAR combines a finance\nlexicon for Item-level guidance (FLAM), dual hierarchical indices for\nwithin-Item search (Summary Tree and Question Tree), and a two-stage\ncross-encoder reranker. This design aligns retrieval with disclosure structure\nand terminology, enabling fine-grained, query-aware context selection.\nEvaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR\ndelivers consistent gains in precision, recall, F1, and relevancy, improving F1\nby up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over\nprior tree-based systems, while also increasing downstream answer accuracy with\na fixed reader. By jointly modeling section hierarchy and domain lexicon\nsignals, FinGEAR improves retrieval fidelity and provides a practical\nfoundation for high-stakes financial analysis.", "published": "2025-09-15 15:25:26", "link": "http://arxiv.org/abs/2509.12042v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models", "abstract": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq.", "published": "2025-09-15 14:59:35", "link": "http://arxiv.org/abs/2509.12019v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles", "abstract": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.", "published": "2025-09-15 14:42:44", "link": "http://arxiv.org/abs/2509.11991v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Query-Focused Extractive Summarization for Sentiment Explanation", "abstract": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset.", "published": "2025-09-15 14:41:18", "link": "http://arxiv.org/abs/2509.11989v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lost in Embeddings: Information Loss in Vision-Language Models", "abstract": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.", "published": "2025-09-15 14:38:06", "link": "http://arxiv.org/abs/2509.11986v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MillStone: How Open-Minded Are LLMs?", "abstract": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.", "published": "2025-09-15 14:18:51", "link": "http://arxiv.org/abs/2509.11967v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models", "abstract": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.", "published": "2025-09-15 14:17:17", "link": "http://arxiv.org/abs/2509.11963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding", "abstract": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.", "published": "2025-09-15 14:16:51", "link": "http://arxiv.org/abs/2509.11961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Evaluate Medical AI", "abstract": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.", "published": "2025-09-15 14:01:22", "link": "http://arxiv.org/abs/2509.11941v1", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.AI"}
{"title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "abstract": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.", "published": "2025-09-15 13:37:35", "link": "http://arxiv.org/abs/2509.11921v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible", "abstract": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.", "published": "2025-09-15 13:33:32", "link": "http://arxiv.org/abs/2509.11915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "abstract": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.", "published": "2025-09-15 12:39:55", "link": "http://arxiv.org/abs/2509.11868v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "primary_category": "cs.CL"}
{"title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "abstract": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.", "published": "2025-09-15 12:35:14", "link": "http://arxiv.org/abs/2509.11860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The AI Memory Gap: Users Misremember What They Created With AI or Without", "abstract": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.", "published": "2025-09-15 12:31:00", "link": "http://arxiv.org/abs/2509.11851v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Collaborative Document Editing with Multiple Users and AI Agents", "abstract": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work.", "published": "2025-09-15 12:11:59", "link": "http://arxiv.org/abs/2509.11826v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection", "abstract": "In Semantic Change Detection (SCD), it is a common problem to obtain\nembeddings that are both interpretable and high-performing. However, improving\ninterpretability often leads to a loss in the SCD performance, and vice versa.\nTo address this problem, we propose SCDTour, a method that orders and merges\ninterpretable axes to alleviate the performance degradation of SCD. SCDTour\nconsiders both (a) semantic similarity between axes in the embedding space, as\nwell as (b) the degree to which each axis contributes to semantic change.\nExperimental results show that SCDTour preserves performance in semantic change\ndetection while maintaining high interpretability. Moreover, agglomerating the\nsorted axes produces a more refined set of word senses, which achieves\ncomparable or improved performance against the original full-dimensional\nembeddings in the SCD task. These findings demonstrate that SCDTour effectively\nbalances interpretability and SCD performance, enabling meaningful\ninterpretation of semantic shifts through a small number of refined axes.\nSource code is available at https://github.com/LivNLP/svp-tour .", "published": "2025-09-15 12:01:24", "link": "http://arxiv.org/abs/2509.11818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning", "abstract": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact.", "published": "2025-09-15 11:55:10", "link": "http://arxiv.org/abs/2509.11816v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges", "abstract": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.", "published": "2025-09-15 11:37:47", "link": "http://arxiv.org/abs/2509.11804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives", "abstract": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal", "published": "2025-09-15 11:34:46", "link": "http://arxiv.org/abs/2509.11803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries", "abstract": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.", "published": "2025-09-15 11:31:25", "link": "http://arxiv.org/abs/2509.11802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums", "abstract": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.", "published": "2025-09-15 10:58:41", "link": "http://arxiv.org/abs/2509.11777v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents", "abstract": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows.", "published": "2025-09-15 10:53:05", "link": "http://arxiv.org/abs/2509.11773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Room acoustics affect communicative success in hybrid meeting spaces: a pilot study", "abstract": "Since the COVID-19 pandemic in 2020, universities and companies have\nincreasingly integrated hybrid features into their meeting spaces, or even\ncreated dedicated rooms for this purpose. While the importance of a fast and\nstable internet connection is often prioritized, the acoustic design of seminar\nrooms is frequently overlooked. Poor acoustics, particularly excessive\nreverberation, can lead to issues such as misunderstandings, reduced speech\nintelligibility or cognitive and vocal fatigue. This pilot study investigates\nwhether room acoustic interventions in a seminar room at Graz University of\nTechnology support better communication in hybrid meetings. For this purpose,\nwe recorded two groups of persons twice, once before and once after improving\nthe acoustics of the room. Our findings -- despite not reaching statistical\nsignificance due to the small sample size - indicate clearly that our spatial\ninterventions improve communicative success in hybrid meetings. To make the\npaper accessible also for readers from the speech communication community, we\nexplain room acoustics background, relevant for the interpretation of our\nresults.", "published": "2025-09-15 09:09:33", "link": "http://arxiv.org/abs/2509.11709v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model", "abstract": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/", "published": "2025-09-15 09:01:39", "link": "http://arxiv.org/abs/2509.11698v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection", "abstract": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.", "published": "2025-09-15 08:38:08", "link": "http://arxiv.org/abs/2509.11687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs", "abstract": "Telecom domain 3GPP documents are replete with images containing sequence\ndiagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion\nof such images to machine-readable PlantUML (puml) formats. However, there is a\ngap in evaluation of such conversions - existing works do not compare puml\nscripts for various components. In this work, we propose performance metrics to\nmeasure the effectiveness of such conversions. A dataset of sequence diagrams\nfrom 3GPP documents is chosen to be representative of domain-specific actual\nscenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -\nagainst manually created ground truth representations. We use version control\ntools to capture differences and introduce standard performance metrics to\nmeasure accuracies along various components: participant identification,\nmessage flow accuracy, sequence ordering, and grouping construct preservation.\nWe demonstrate effectiveness of proposed metrics in quantifying conversion\nerrors across various components of puml scripts. The results show that nodes,\nedges and messages are accurately captured. However, we observe that VLMs do\nnot necessarily perform well on complex structures such as notes, box, groups.\nOur experiments and performance metrics indicates a need for better\nrepresentation of these components in training data for fine-tuned VLMs.", "published": "2025-09-15 08:08:41", "link": "http://arxiv.org/abs/2509.11667v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs", "abstract": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments.", "published": "2025-09-15 08:00:31", "link": "http://arxiv.org/abs/2509.11662v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "MALLM: Multi-Agent Large Language Models Framework", "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective\nintelligence by scaling test-time compute and leveraging expertise. Current\nframeworks for multi-agent debate are often designed towards tool use, lack\nintegrated evaluation, or provide limited configurability of agent personas,\nresponse generators, discussion paradigms, and decision protocols. We introduce\nMALLM (Multi-Agent Large Language Models), an open-source framework that\nenables systematic analysis of MAD components. MALLM offers more than 144\nunique configurations of MAD, including (1) agent personas (e.g., Expert,\nPersonality), (2) response generators (e.g., Critical, Reasoning), (3)\ndiscussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,\nVoting, Consensus). MALLM uses simple configuration files to define a debate.\nFurthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,\nWinoGrande) and provides an evaluation pipeline for easy comparison of MAD\nconfigurations. MALLM is tailored towards researchers and provides a window\ninto the heart of multi-agent debate, facilitating the understanding of its\ncomponents and their interplay.", "published": "2025-09-15 07:48:02", "link": "http://arxiv.org/abs/2509.11656v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "A.1; I.2.7"], "primary_category": "cs.MA"}
{"title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI", "abstract": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.", "published": "2025-09-15 07:35:35", "link": "http://arxiv.org/abs/2509.11648v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment", "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nPersonalized Image Aesthetic Assessment (PIAA) as a scalable alternative to\nexpert evaluations. However, their predictions may reflect subtle biases\ninfluenced by demographic factors such as gender, age, and education. In this\nwork, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two\ncomplementary dimensions: (1) stereotype bias, quantified by measuring\nvariations in aesthetic evaluations across demographic groups; and (2)\nalignment between model outputs and genuine human aesthetic preferences. Our\nbenchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and\nintroduces structured metrics (IFD, NRD, AAS) to assess both bias and\nalignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,\nClaude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).\nResults indicate that smaller models exhibit stronger stereotype biases,\nwhereas larger models align more closely with human preferences. Incorporating\nidentity information often exacerbates bias, particularly in emotional\njudgments. These findings underscore the importance of identity-aware\nevaluation frameworks in subjective vision-language tasks.", "published": "2025-09-15 06:25:39", "link": "http://arxiv.org/abs/2509.11620v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems", "abstract": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset", "published": "2025-09-15 06:23:36", "link": "http://arxiv.org/abs/2509.11619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification", "abstract": "Entity-level sentiment classification involves identifying the sentiment\npolarity linked to specific entities within text. This task poses several\nchallenges: effectively modeling the subtle and complex interactions between\nentities and their surrounding sentiment expressions; capturing dependencies\nthat may span across sentences; and ensuring consistent sentiment predictions\nfor multiple mentions of the same entity through coreference resolution.\nAdditionally, linguistic phenomena such as negation, ambiguity, and overlapping\nopinions further complicate the analysis. These complexities make entity-level\nsentiment classification a difficult problem, especially in real-world, noisy\ntextual data. To address these issues, we propose SpanEIT, a novel framework\nintegrating dynamic span interaction and graph-aware memory mechanisms for\nenhanced entity-sentiment relational modeling. SpanEIT builds span-based\nrepresentations for entities and candidate sentiment phrases, employs\nbidirectional attention for fine-grained interactions, and uses a graph\nattention network to capture syntactic and co-occurrence relations. A\ncoreference-aware memory module ensures entity-level consistency across\ndocuments. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT\noutperforms state-of-the-art transformer and hybrid baselines in accuracy and\nF1 scores. Ablation and interpretability analyses validate the effectiveness of\nour approach, underscoring its potential for fine-grained sentiment analysis in\napplications like social media monitoring and customer feedback analysis.", "published": "2025-09-15 05:47:57", "link": "http://arxiv.org/abs/2509.11604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study", "abstract": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice.", "published": "2025-09-15 05:18:17", "link": "http://arxiv.org/abs/2509.11591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain", "abstract": "Reasoning is essential for closed-domain QA systems in which procedural\ncorrectness and policy compliance are critical. While large language models\n(LLMs) have shown strong performance on many reasoning tasks, recent work\nreveals that their reasoning traces are often unfaithful - serving more as\nplausible justifications than as causally grounded derivations. Efforts to\ncombine LLMs with symbolic engines (e.g., Prover9, Z3) have improved\nreliability but remain limited to static forms of logic, struggling with\ndynamic, state-based reasoning such as multi-step progressions and conditional\ntransitions.\n  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a\nneuro-symbolic framework that integrates LLMs with model checking to support\nproperty verification. MCFR translates natural language into formal\nspecifications and verifies them over transition models. To support evaluation,\nwe introduce EduMC-QA, a benchmark dataset grounded in real academic\nprocedures. Our results show that MCFR improves reasoning faithfulness and\ninterpretability, offering a viable path toward verifiable QA in high-stakes\nclosed-domain applications. In addition to evaluating MCFR, we compare its\nperformance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to\ncontextualize its effectiveness.", "published": "2025-09-15 04:34:42", "link": "http://arxiv.org/abs/2509.11572v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges", "abstract": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.", "published": "2025-09-15 04:31:22", "link": "http://arxiv.org/abs/2509.11570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs", "abstract": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines.", "published": "2025-09-15 04:28:38", "link": "http://arxiv.org/abs/2509.11569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "abstract": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.", "published": "2025-09-15 03:32:50", "link": "http://arxiv.org/abs/2509.11552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HARP: Hallucination Detection via Reasoning Subspace Projection", "abstract": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their\nreliable use in critical decision-making. Although existing hallucination\ndetection methods have improved accuracy, they still struggle with\ndisentangling semantic and reasoning information and maintaining robustness. To\naddress these challenges, we propose HARP (Hallucination detection via\nreasoning subspace projection), a novel hallucination detection framework. HARP\nestablishes that the hidden state space of LLMs can be decomposed into a direct\nsum of a semantic subspace and a reasoning subspace, where the former encodes\nlinguistic expression and the latter captures internal reasoning processes.\nMoreover, we demonstrate that the Unembedding layer can disentangle these\nsubspaces, and by applying Singular Value Decomposition (SVD) to its\nparameters, the basis vectors spanning the semantic and reasoning subspaces are\nobtained. Finally, HARP projects hidden states onto the basis vectors of the\nreasoning subspace, and the resulting projections are then used as input\nfeatures for hallucination detection in LLMs. By using these projections, HARP\nreduces the dimension of the feature to approximately 5% of the original,\nfilters out most noise, and achieves enhanced robustness. Experiments across\nmultiple datasets show that HARP achieves state-of-the-art hallucination\ndetection performance; in particular, it achieves an AUROC of 92.8% on\nTriviaQA, outperforming the previous best method by 7.5%.", "published": "2025-09-15 03:02:33", "link": "http://arxiv.org/abs/2509.11536v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Distinctive Co-occurrence Characteristics of Antonymy", "abstract": "Antonymy has long received particular attention in lexical semantics.\nPrevious studies have shown that antonym pairs frequently co-occur in text,\nacross genres and parts of speech, more often than would be expected by chance.\nHowever, whether this co-occurrence pattern is distinctive of antonymy remains\nunclear, due to a lack of comparison with other semantic relations. This work\nfills the gap by comparing antonymy with three other relations across parts of\nspeech using robust co-occurrence metrics. We find that antonymy is distinctive\nin three respects: antonym pairs co-occur with high strength, in a preferred\nlinear order, and within short spans. All results are available online.", "published": "2025-09-15 02:58:14", "link": "http://arxiv.org/abs/2509.11534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation", "abstract": "BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable\nperformance in answering medical examinations. However, the extent to which\nthis high performance is transferable to medical questions in Spanish and from\na Latin American country remains unexplored. This knowledge is crucial as\nLLM-based medical applications gain traction in Latin America. AIMS: to build a\ndataset of questions from medical examinations taken by Peruvian physicians\npursuing specialty training; to fine-tune a LLM on this dataset; to evaluate\nand compare the performance in terms of accuracy between vanilla LLMs and the\nfine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice\nquestion-answering (MCQA) datasets containing 8,380 questions spanning 12\nmedical domains (2018-2025). We selected eight medical LLMs including\nmedgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific\nprompts to answer the questions appropriately. We employed parameter-efficient\nfine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it\nutilizing all questions except those from 2025 (test set). RESULTS:\nmedgemma-27b-text-it outperformed all other models, achieving a proportion of\ncorrect answers exceeding 90% in several instances. LLMs with <10 billion\nparameters exhibited <60% of correct answers, while some exams yielded results\n<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all\nLLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters\nacross various examinations. CONCLUSIONS: For medical AI application and\nresearch that require knowledge bases from Spanish-speaking countries and those\nexhibiting similar epidemiological profiles to Peru's, interested parties\nshould utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.", "published": "2025-09-15 02:07:26", "link": "http://arxiv.org/abs/2509.11517v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LVLMs are Bad at Overhearing Human Referential Communication", "abstract": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research.", "published": "2025-09-15 02:03:18", "link": "http://arxiv.org/abs/2509.11514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics", "abstract": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance.", "published": "2025-09-15 01:57:09", "link": "http://arxiv.org/abs/2509.11513v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification", "abstract": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results.", "published": "2025-09-15 01:25:37", "link": "http://arxiv.org/abs/2509.11498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization", "abstract": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization.", "published": "2025-09-15 01:19:49", "link": "http://arxiv.org/abs/2509.11496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims", "abstract": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.", "published": "2025-09-15 01:03:09", "link": "http://arxiv.org/abs/2509.11492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Relational Priming Improves Transformer in Multivariate Time Series", "abstract": "Standard attention mechanisms in transformers employ static token\nrepresentations that remain unchanged across all pair-wise computations in each\nlayer. This limits their representational alignment with the potentially\ndiverse relational dynamics of each token-pair interaction. While they excel in\ndomains with relatively homogeneous relationships, standard attention's static\nrelational learning struggles to capture the diverse, heterogeneous\ninter-channel dependencies of multivariate time series (MTS) data--where\ndifferent channel-pair interactions within a single system may be governed by\nentirely different physical laws or temporal dynamics. To better align the\nattention mechanism for such domain phenomena, we propose attention with\ndynamic relational priming (prime attention). Unlike standard attention where\neach token presents an identical representation across all of its pair-wise\ninteractions, prime attention tailors each token dynamically (or per\ninteraction) through learnable modulations to best capture the unique\nrelational dynamics of each token pair, optimizing each pair-wise interaction\nfor that specific relationship. This representational plasticity of prime\nattention enables effective extraction of relationship-specific information in\nMTS while maintaining the same asymptotic computational complexity as standard\nattention. Our results demonstrate that prime attention consistently\noutperforms standard attention across benchmarks, achieving up to 6.5\\%\nimprovement in forecasting accuracy. In addition, we find that prime attention\nachieves comparable or superior performance using up to 40\\% less sequence\nlength compared to standard attention, further demonstrating its superior\nrelational modeling capabilities.", "published": "2025-09-15 17:56:15", "link": "http://arxiv.org/abs/2509.12196v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Advancing Medical Artificial Intelligence Using a Century of Cases", "abstract": "BACKGROUND: For over a century, the New England Journal of Medicine\nClinicopathological Conferences (CPCs) have tested the reasoning of expert\nphysicians and, recently, artificial intelligence (AI). However, prior AI\nevaluations have focused on final diagnoses without addressing the multifaceted\nreasoning and presentation skills required of expert discussants.\n  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),\nwe conducted extensive physician annotation and automated processing to create\nCPC-Bench, a physician-validated benchmark spanning 10 text-based and\nmultimodal tasks, against which we evaluated leading large language models\n(LLMs). Then, we developed \"Dr. CaBot,\" an AI discussant designed to produce\nwritten and slide-based video presentations using only the case presentation,\nmodeling the role of the human expert in these cases.\n  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the\nfinal diagnosis first in 60% of cases and within the top ten in 84% of cases,\noutperforming a 20-physician baseline; next-test selection accuracy reached\n98%. Event-level physician annotations quantified AI diagnostic accuracy per\nunit of information. Performance was lower on literature search and image\ntasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image\nchallenges. In blinded comparisons of CaBot vs. human expert-generated text,\nphysicians misclassified the source of the differential in 46 of 62 (74%) of\ntrials, and scored CaBot more favorably across quality dimensions. To promote\nresearch, we are releasing CaBot and CPC-Bench.\n  CONCLUSIONS: LLMs exceed physician performance on complex text-based\ndifferential diagnosis and convincingly emulate expert medical presentations,\nbut image interpretation and literature retrieval remain weaker. CPC-Bench and\nCaBot may enable transparent and continued tracking of progress in medical AI.", "published": "2025-09-15 17:54:51", "link": "http://arxiv.org/abs/2509.12194v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "HoloGarment: 360\u00b0 Novel View Synthesis of In-the-Wild Garments", "abstract": "Novel view synthesis (NVS) of in-the-wild garments is a challenging task due\nsignificant occlusions, complex human poses, and cloth deformations. Prior\nmethods rely on synthetic 3D training data consisting of mostly unoccluded and\nstatic objects, leading to poor generalization on real-world clothing. In this\npaper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3\nimages or a continuous video of a person wearing a garment and generates\n360{\\deg} novel views of the garment in a canonical pose. Our key insight is to\nbridge the domain gap between real and synthetic data with a novel implicit\ntraining paradigm leveraging a combination of large-scale real video data and\nsmall-scale synthetic 3D data to optimize a shared garment embedding space.\nDuring inference, the shared embedding space further enables dynamic\nvideo-to-360{\\deg} NVS through the construction of a garment \"atlas\"\nrepresentation by finetuning a garment embedding on a specific real-world\nvideo. The atlas captures garment-specific geometry and texture across all\nviewpoints, independent of body pose or motion. Extensive experiments show that\nHoloGarment achieves state-of-the-art performance on NVS of in-the-wild\ngarments from images and videos. Notably, our method robustly handles\nchallenging real-world artifacts -- such as wrinkling, pose variation, and\nocclusion -- while maintaining photorealism, view consistency, fine texture\ndetails, and accurate geometry. Visit our project page for additional results:\nhttps://johannakarras.github.io/HoloGarment", "published": "2025-09-15 17:50:57", "link": "http://arxiv.org/abs/2509.12187v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation", "abstract": "Current AI alignment through RLHF follows a single directional paradigm that\nAI conforms to human preferences while treating human cognition as fixed. We\npropose a shift to co-alignment through Bidirectional Cognitive Alignment\n(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,\nrepresentation mapping, and KL-budget constraints for controlled co-evolution.\nIn collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,\nwith 230% better mutual adaptation and 332% better protocol convergence.\nEmergent protocols outperformed handcrafted ones by 84%, while bidirectional\nadaptation unexpectedly improved safety (+23% out-of-distribution robustness).\nThe 46% synergy improvement demonstrates optimal collaboration exists at the\nintersection, not union, of human and AI capabilities, validating the shift\nfrom single-directional to co-alignment paradigms.", "published": "2025-09-15 17:41:16", "link": "http://arxiv.org/abs/2509.12179v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Approaches to Analysis and Design of AI-Based Autonomous Vehicles", "abstract": "Artificial intelligence (AI) models are becoming key components in an\nautonomous vehicle (AV), especially in handling complicated perception tasks.\nHowever, closing the loop through AI-based feedback may pose significant risks\non reliability of autonomous driving due to very limited understanding about\nthe mechanism of AI-driven perception processes. To overcome it, this paper\naims to develop tools for modeling, analysis, and synthesis for a class of\nAI-based AV; in particular, their closed-loop properties, e.g., stability,\nrobustness, and performance, are rigorously studied in the statistical sense.\nFirst, we provide a novel modeling means for the AI-driven perception processes\nby looking at their error characteristics. Specifically, three fundamental\nAI-induced perception uncertainties are recognized and modeled by Markov\nchains, Gaussian processes, and bounded disturbances, respectively. By means of\nthat, the closed-loop stochastic stability (SS) is established in the sense of\nmean square, and then, an SS control synthesis method is presented within the\nframework of linear matrix inequalities (LMIs). Besides the SS properties, the\nrobustness and performance of AI-based AVs are discussed in terms of a\nstochastic guaranteed cost, and criteria are given to test the robustness level\nof an AV when in the presence of AI-induced uncertainties. Furthermore, the\nstochastic optimal guaranteed cost control is investigated, and an efficient\ndesign procedure is developed innovatively based on LMI techniques and convex\noptimization. Finally, to illustrate the effectiveness, the developed results\nare applied to an example of car following control, along with extensive\nsimulation.", "published": "2025-09-15 17:32:29", "link": "http://arxiv.org/abs/2509.12169v1", "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "eess.SY"}
{"title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression", "abstract": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder.", "published": "2025-09-15 17:23:46", "link": "http://arxiv.org/abs/2509.12159v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference", "abstract": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions.", "published": "2025-09-15 17:17:26", "link": "http://arxiv.org/abs/2509.12152v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Multi Anatomy X-Ray Foundation Model", "abstract": "X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation\nmodels are limited to chest anatomy and fail to generalize across broader\nclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray\nfoundation model using self-supervised learning on a large, private dataset of\n1.15 million images spanning diverse anatomical regions and evaluated across 12\ndatasets and 20 downstream tasks, including classification, retrieval,\nsegmentation, localization, visual grounding, and report generation. XR-0\nachieves state-of-the-art performance on most multi-anatomy tasks and remains\ncompetitive on chest-specific benchmarks. Our results demonstrate that\nanatomical diversity and supervision are critical for building robust,\ngeneral-purpose medical vision models, paving the way for scalable and\nadaptable AI systems in radiology.", "published": "2025-09-15 17:12:26", "link": "http://arxiv.org/abs/2509.12146v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data", "abstract": "Major depressive disorder (MDD) is a prevalent mental health condition that\nnegatively impacts both individual well-being and global public health.\nAutomated detection of MDD using structural magnetic resonance imaging (sMRI)\nand deep learning (DL) methods holds increasing promise for improving\ndiagnostic accuracy and enabling early intervention. Most existing methods\nemploy either voxel-level features or handcrafted regional representations\nbuilt from predefined brain atlases, limiting their ability to capture complex\nbrain patterns. This paper develops a unified pipeline that utilizes Vision\nTransformers (ViTs) for extracting 3D region embeddings from sMRI data and\nGraph Neural Network (GNN) for classification. We explore two strategies for\ndefining regions: (1) an atlas-based approach using predefined structural and\nfunctional brain atlases, and (2) an cube-based method by which ViTs are\ntrained directly to identify regions from uniformly extracted 3D patches.\nFurther, cosine similarity graphs are generated to model interregional\nrelationships, and guide GNN-based classification. Extensive experiments were\nconducted using the REST-meta-MDD dataset to demonstrate the effectiveness of\nour model. With stratified 10-fold cross-validation, the best model obtained\n78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and\n78.98% F1-score. Further, atlas-based models consistently outperformed the\ncube-based approach, highlighting the importance of using domain-specific\nanatomical priors for MDD detection.", "published": "2025-09-15 17:10:39", "link": "http://arxiv.org/abs/2509.12143v1", "categories": ["cs.CV", "cs.AI", "62P10, 68T07, 92B20", "I.2.6; J.3"], "primary_category": "cs.CV"}
{"title": "Control Analysis and Design for Autonomous Vehicles Subject to Imperfect AI-Based Perception", "abstract": "Safety is a critical concern in autonomous vehicle (AV) systems, especially\nwhen AI-based sensing and perception modules are involved. However, due to the\nblack box nature of AI algorithms, it makes closed-loop analysis and synthesis\nparticularly challenging, for example, establishing closed-loop stability and\nensuring performance, while they are fundamental to AV safety. To approach this\ndifficulty, this paper aims to develop new modeling, analysis, and synthesis\ntools for AI-based AVs. Inspired by recent developments in perception error\nmodels (PEMs), the focus is shifted from directly modeling AI-based perception\nprocesses to characterizing the perception errors they produce. Two key classes\nof AI-induced perception errors are considered: misdetection and measurement\nnoise. These error patterns are modeled using continuous-time Markov chains and\nWiener processes, respectively. By means of that, a PEM-augmented driving model\nis proposed, with which we are able to establish the closed-loop stability for\na class of AI-driven AV systems via stochastic calculus. Furthermore, a\nperformance-guaranteed output feedback control synthesis method is presented,\nwhich ensures both stability and satisfactory performance. The method is\nformulated as a convex optimization problem, allowing for efficient numerical\nsolutions. The results are then applied to an adaptive cruise control (ACC)\nscenario, demonstrating their effectiveness and robustness despite the\ncorrupted and misleading perception.", "published": "2025-09-15 17:03:21", "link": "http://arxiv.org/abs/2509.12137v1", "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "eess.SY"}
{"title": "$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning", "abstract": "Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)\ntypically employ a policy update that responds to the current strategies of\nother agents. While being straightforward, this approach does not account for\nthe updates of other agents at the same update step, resulting in\nmiscoordination. In this paper, we introduce the $K$-Level Policy Gradient\n(KPG), a method that recursively updates each agent against the updated\npolicies of other agents, speeding up the discovery of effective coordinated\npolicies. We theoretically prove that KPG with finite iterates achieves\nmonotonic convergence to a local Nash equilibrium under certain conditions. We\nprovide principled implementations of KPG by applying it to the deep MARL\nalgorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior\nperformance over existing deep MARL algorithms in StarCraft II and multi-agent\nMuJoCo.", "published": "2025-09-15 16:42:56", "link": "http://arxiv.org/abs/2509.12117v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice", "abstract": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning.", "published": "2025-09-15 16:33:37", "link": "http://arxiv.org/abs/2509.12107v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference", "abstract": "The integration of Large Language Models (LLMs) into legal practice raises\npressing concerns about judicial fairness, particularly due to the nature of\ntheir \"black-box\" processes. This study introduces JustEva, a comprehensive,\nopen-source evaluation toolkit designed to measure LLM fairness in legal tasks.\nJustEva features several advantages: (1) a structured label system covering 65\nextra-legal factors; (2) three core fairness metrics - inconsistency, bias, and\nimbalanced inaccuracy; (3) robust statistical inference methods; and (4)\ninformative visualizations. The toolkit supports two types of experiments,\nenabling a complete evaluation workflow: (1) generating structured outputs from\nLLMs using a provided dataset, and (2) conducting statistical analysis and\ninference on LLMs' outputs through regression and other statistical methods.\nEmpirical application of JustEva reveals significant fairness deficiencies in\ncurrent LLMs, highlighting the lack of fair and trustworthy LLM legal tools.\nJustEva offers a convenient tool and methodological foundation for evaluating\nand improving algorithmic fairness in the legal domain.", "published": "2025-09-15 16:31:26", "link": "http://arxiv.org/abs/2509.12104v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists", "abstract": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability.", "published": "2025-09-15 16:26:13", "link": "http://arxiv.org/abs/2509.12102v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants", "abstract": "Engineering models created in Model-Based Systems Engineering (MBSE)\nenvironments contain detailed information about system structure and behavior.\nHowever, they typically lack symbolic planning semantics such as preconditions,\neffects, and constraints related to resource availability and timing. This\nlimits their ability to evaluate whether a given system variant can fulfill\nspecific tasks and how efficiently it performs compared to alternatives.\n  To address this gap, this paper presents a model-driven method that enables\nthe specification and automated generation of symbolic planning artifacts\nwithin SysML-based engineering models. A dedicated SysML profile introduces\nreusable stereotypes for core planning constructs. These are integrated into\nexisting model structures and processed by an algorithm that generates a valid\ndomain file and a corresponding problem file in Planning Domain Definition\nLanguage (PDDL). In contrast to previous approaches that rely on manual\ntransformations or external capability models, the method supports native\nintegration and maintains consistency between engineering and planning\nartifacts.\n  The applicability of the method is demonstrated through a case study from\naircraft assembly. The example illustrates how existing engineering models are\nenriched with planning semantics and how the proposed workflow is applied to\ngenerate consistent planning artifacts from these models. The generated\nplanning artifacts enable the validation of system variants through AI\nplanning.", "published": "2025-09-15 16:18:08", "link": "http://arxiv.org/abs/2509.12091v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors", "abstract": "This paper proposes deception as a mechanism for out-of-distribution (OOD)\ngeneralization: by learning data representations that make training data appear\nindependent and identically distributed (iid) to an observer, we can identify\nstable features that eliminate spurious correlations and generalize to unseen\ndomains. We refer to this principle as deceptive risk minimization (DRM) and\ninstantiate it with a practical differentiable objective that simultaneously\nlearns features that eliminate distribution shifts from the perspective of a\ndetector based on conformal martingales while minimizing a task-specific loss.\nIn contrast to domain adaptation or prior invariant representation learning\nmethods, DRM does not require access to test data or a partitioning of training\ndata into a finite number of data-generating domains. We demonstrate the\nefficacy of DRM on numerical experiments with concept shift and a simulated\nimitation learning setting with covariate shift in environments that a robot is\ndeployed in.", "published": "2025-09-15 16:11:55", "link": "http://arxiv.org/abs/2509.12081v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "cs.LG"}
{"title": "A Time-Series Foundation Model by Universal Delay Embedding", "abstract": "This study introduces Universal Delay Embedding (UDE), a pretrained\nfoundation model designed to revolutionize time-series forecasting through\nprincipled integration of delay embedding representation and Koopman operator\nprediction. Leveraging Takens' embedding theorem, UDE as a dynamical\nrepresentation of observed data constructs two-dimensional subspace patches\nfrom Hankel matrices, theoretically preserving dynamical and topological\nproperties of underlying dynamical systems. Such patches are viewed as images,\nwhich can be efficiently processed by exploiting advanced deep learning\ntechnologies. Computationally, these patches further serve as tokens for\nlearning a self-attention encoder, thus enabling accurate prediction of\nnonlinear time-series by a finite-dimensional Koopman operator in a linear\nmanner in a latent space. Extensive evaluations across various benchmarks and\nreal-world climate datasets demonstrate over 20% average reduction in mean\nsquared error versus state-of-the-art foundation models, alongside superior\ngeneralization in fine-tuning scenarios. In particular, the learned dynamical\nrepresentations and Koopman operator prediction forms from the patches exhibit\nexceptional interpretability, with consistent identification of topologically\ninformative subspaces and robust encoding of domain-invariant dynamics,\nestablishing UDE as a scalable, interpretable framework for universal\ntime-series modeling and forecasting with broad scientific and industrial\napplicability.", "published": "2025-09-15 16:11:49", "link": "http://arxiv.org/abs/2509.12080v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning", "abstract": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic\nweed that threatens tomato production by extracting nutrients from the host. We\ninvestigate early detection using leaf-level spectral reflectance (400-2500 nm)\nand ensemble machine learning. In a field experiment in Woodland, California,\nwe tracked 300 tomato plants across growth stages defined by growing degree\ndays (GDD). Leaf reflectance was acquired with a portable spectrometer and\npreprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,\ncorrelation-based band reduction). Clear class differences were observed near\n1500 nm and 2000 nm water absorption features, consistent with reduced leaf\nwater content in infected plants at early stages. An ensemble combining Random\nForest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at\n585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy\ndeclined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and\nweed interference. Despite the small number of infected plants and\nenvironmental confounders, results show that proximal sensing with ensemble\nlearning enables timely detection of broomrape before canopy symptoms are\nvisible, supporting targeted interventions and reduced yield losses.", "published": "2025-09-15 16:00:32", "link": "http://arxiv.org/abs/2509.12074v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP", "68T07, 68T45, 68U10", "I.5.4; I.4.6; I.2.6"], "primary_category": "cs.LG"}
{"title": "U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT", "abstract": "Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in\ndentistry, providing volumetric information about the anatomical structures of\njaws and teeth. Accurate segmentation of these anatomies is critical for\nclinical applications such as diagnosis and surgical planning, but remains\ntime-consuming and challenging. In this paper, we present U-Mamba2, a new\nneural network architecture designed for multi-anatomy CBCT segmentation in the\ncontext of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state\nspace models into the U-Net architecture, enforcing stronger structural\nconstraints for higher efficiency without compromising performance. In\naddition, we integrate interactive click prompts with cross-attention blocks,\npre-train U-Mamba2 using self-supervised learning, and incorporate dental\ndomain knowledge into the model design to address key challenges of dental\nanatomy segmentation in CBCT. Extensive experiments, including independent\ntests, demonstrate that U-Mamba2 is both effective and efficient, securing top\n3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2\nachieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with\nan average inference time of XX (TBC during the ODIN workshop). In Task 2,\nU-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out\ntest data. The code is publicly available at\nhttps://github.com/zhiqin1998/UMamba2.", "published": "2025-09-15 15:52:43", "link": "http://arxiv.org/abs/2509.12069v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) are susceptible to the implicit\nreasoning risk, wherein innocuous unimodal inputs synergistically assemble into\nrisky multimodal data that produce harmful outputs. We attribute this\nvulnerability to the difficulty of MLLMs maintaining safety alignment through\nlong-chain reasoning. To address this issue, we introduce\nSafe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring\ninterpretable reasoning paths tailored for such a cross-modal challenge. A\nnovel training framework, Safety-aware Reasoning Path Optimization (SRPO), is\nalso designed based on the SSUI dataset to align the MLLM's internal reasoning\nprocess with human safety values. Experimental results show that our\nSRPO-trained models achieve state-of-the-art results on key safety benchmarks,\nincluding the proposed Reasoning Path Benchmark (RSBench), significantly\noutperforming both open-source and top-tier commercial MLLMs.", "published": "2025-09-15 15:40:58", "link": "http://arxiv.org/abs/2509.12060v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications", "abstract": "Modern tensor applications, especially foundation models and generative AI\napplications require multiple input modalities (both vision and language),\nwhich increases the demand for flexible accelerator architecture. Existing\nframeworks suffer from the trade-off between design flexibility and\nproductivity of RTL generation: either limited to very few hand-written\ntemplates or cannot automatically generate the RTL. To address this challenge,\nwe propose the LEGO framework, which targets tensor applications and\nautomatically generates spatial architecture design and outputs synthesizable\nRTL code without handwritten RTL design templates. Leveraging the\naffine-transformation-based architecture representation, LEGO front end finds\ninterconnections between function units, synthesizes the memory system, and\nfuses different spatial dataflow designs based on data reuse analysis. LEGO\nback end then translates the hardware in a primitive-level graph to perform\nlower-level optimizations, and applies a set of linear-programming algorithms\nto optimally insert pipeline registers and reduce the overhead of unused logic\nwhen switching spatial dataflows. Our evaluation demonstrates that LEGO can\nachieve 3.2x speedup and 2.4x energy efficiency compared to previous work\nGemmini, and can generate one architecture for diverse modern foundation models\nin generative AI applications.", "published": "2025-09-15 15:36:18", "link": "http://arxiv.org/abs/2509.12053v1", "categories": ["cs.AR", "cs.AI", "cs.LG"], "primary_category": "cs.AR"}
{"title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents", "abstract": "Although browser-using agents (BUAs) show promise for web tasks and\nautomation, most BUAs terminate after executing a single instruction, failing\nto support users' complex, nonlinear browsing with ambiguous goals, iterative\ndecision-making, and changing contexts. We present a human-in-the-loop (HITL)\nconceptual framework informed by theories of human web browsing behavior. The\nframework centers on an iterative loop in which the BUA proactively proposes\nnext actions and the user steers the browsing process through feedback. It also\ndistinguishes between exploration and exploitation actions, enabling users to\ncontrol the breadth and depth of their browsing. Consequently, the framework\naims to reduce users' physical and cognitive effort while preserving users'\ntraditional browsing mental model and supporting users in achieving\nsatisfactory outcomes. We illustrate how the framework operates with\nhypothetical use cases and discuss the shift from manual browsing to\ninteraction-driven browsing. We contribute a theoretically informed conceptual\nframework for BUAs.", "published": "2025-09-15 15:31:53", "link": "http://arxiv.org/abs/2509.12049v1", "categories": ["cs.HC", "cs.AI", "cs.MA"], "primary_category": "cs.HC"}
{"title": "A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset", "abstract": "Animal behavior analysis plays a crucial role in understanding animal\nwelfare, health status, and productivity in agricultural settings. However,\ntraditional manual observation methods are time-consuming, subjective, and\nlimited in scalability. We present a modular pipeline that leverages\nopen-sourced state-of-the-art computer vision techniques to automate animal\nbehavior analysis in a group housing environment. Our approach combines\nstate-of-the-art models for zero-shot object detection, motion-aware tracking\nand segmentation, and advanced feature extraction using vision transformers for\nrobust behavior recognition. The pipeline addresses challenges including animal\nocclusions and group housing scenarios as demonstrated in indoor pig\nmonitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset\nfor multiple behavioral tasks. Our temporal model achieved 94.2% overall\naccuracy, representing a 21.2 percentage point improvement over existing\nmethods. The pipeline demonstrated robust tracking capabilities with 93.3%\nidentity preservation score and 89.3% object detection precision. The modular\ndesign suggests potential for adaptation to other contexts, though further\nvalidation across species would be required. The open-source implementation\nprovides a scalable solution for behavior monitoring, contributing to precision\npig farming and welfare assessment through automated, objective, and continuous\nanalysis.", "published": "2025-09-15 15:31:12", "link": "http://arxiv.org/abs/2509.12047v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking", "abstract": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.", "published": "2025-09-15 15:27:29", "link": "http://arxiv.org/abs/2509.12046v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing", "abstract": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task\nthat adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)\ndomain, remains underexplored due to the absence of a unified evaluation\nbenchmark and the domain gap between natural and RS images. To bridge these\ngaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench})\nbased on widely-used RS segmentation datasets, enabling consistent evaluation\nacross methods. Using this benchmark, we comprehensively evaluate several\nrepresentative OVS/OVRSIS models and reveal their limitations when directly\napplied to remote sensing scenarios. Building on these insights, we propose\n\\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for\nremote sensing. RSKT-Seg integrates three key components: (1) a\nMulti-Directional Cost Map Aggregation (RS-CMA) module that captures\nrotation-invariant visual cues by computing vision-language cosine similarities\nacross multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)\ntransformer, which jointly models spatial and semantic dependencies with a\nlightweight dimensionality reduction strategy; and (3) a Remote Sensing\nKnowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and\nfacilitates domain adaptation via enhanced upsampling. Extensive experiments on\nthe benchmark show that RSKT-Seg consistently outperforms strong OVS baselines\nby +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through\nefficient aggregation. Our code is\n\\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.", "published": "2025-09-15 15:24:49", "link": "http://arxiv.org/abs/2509.12040v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review", "abstract": "In high-stakes disaster scenarios, timely and informed decision-making is\ncritical yet often challenged by uncertainty, dynamic environments, and limited\nresources. This paper presents a systematic review of Human-AI collaboration\npatterns that support decision-making across all disaster management phases.\nDrawing from 51 peer-reviewed studies, we identify four major categories:\nHuman-AI Decision Support Systems, Task and Resource Coordination, Trust and\nTransparency, and Simulation and Training. Within these, we analyze\nsub-patterns such as cognitive-augmented intelligence, multi-agent\ncoordination, explainable AI, and virtual training environments. Our review\nhighlights how AI systems may enhance situational awareness, improves response\nefficiency, and support complex decision-making, while also surfacing critical\nlimitations in scalability, interpretability, and system interoperability. We\nconclude by outlining key challenges and future research directions,\nemphasizing the need for adaptive, trustworthy, and context-aware Human-AI\nsystems to improve disaster resilience and equitable recovery outcomes.", "published": "2025-09-15 15:18:49", "link": "http://arxiv.org/abs/2509.12034v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Imitation Learning as Return Distribution Matching", "abstract": "We study the problem of training a risk-sensitive reinforcement learning (RL)\nagent through imitation learning (IL). Unlike standard IL, our goal is not only\nto train an agent that matches the expert's expected return (i.e., its average\nperformance) but also its risk attitude (i.e., other features of the return\ndistribution, such as variance). We propose a general formulation of the\nrisk-sensitive IL problem in which the objective is to match the expert's\nreturn distribution in Wasserstein distance. We focus on the tabular setting\nand assume the expert's reward is known. After demonstrating the limited\nexpressivity of Markovian policies for this task, we introduce an efficient and\nsufficiently expressive subclass of non-Markovian policies tailored to it.\nBuilding on this subclass, we develop two provably efficient algorithms, RS-BC\nand RS-KT, for solving the problem when the transition model is unknown and\nknown, respectively. We show that RS-KT achieves substantially lower sample\ncomplexity than RS-BC by exploiting dynamics information. We further\ndemonstrate the sample efficiency of return distribution matching in the\nsetting where the expert's reward is unknown by designing an oracle-based\nvariant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and\nRS-BC with numerical simulations, highlighting both their sample efficiency and\nthe advantages of non-Markovian policies over standard sample-efficient IL\nalgorithms.", "published": "2025-09-15 15:08:04", "link": "http://arxiv.org/abs/2509.12026v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids", "abstract": "We study the problem of generalizing an expert agent's behavior, provided\nthrough demonstrations, to new environments and/or additional constraints.\nInverse Reinforcement Learning (IRL) offers a promising solution by seeking to\nrecover the expert's underlying reward function, which, if used for planning in\nthe new settings, would reproduce the desired behavior. However, IRL is\ninherently ill-posed: multiple reward functions, forming the so-called feasible\nset, can explain the same observed behavior. Since these rewards may induce\ndifferent policies in the new setting, in the absence of additional\ninformation, a decision criterion is needed to select which policy to deploy.\nIn this paper, we propose a novel, principled criterion that selects the\n\"average\" policy among those induced by the rewards in a certain bounded subset\nof the feasible set. Remarkably, we show that this policy can be obtained by\nplanning with the reward centroid of that subset, for which we derive a\nclosed-form expression. We then present a provably efficient algorithm for\nestimating this centroid using an offline dataset of expert demonstrations\nonly. Finally, we conduct numerical simulations that illustrate the\nrelationship between the expert's behavior and the behavior produced by our\nmethod.", "published": "2025-09-15 14:53:54", "link": "http://arxiv.org/abs/2509.12010v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Poison to Detect: Detection of Targeted Overfitting in Federated Learning", "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralised clients while keeping local data private, making it a widely\nadopted privacy-enhancing technology (PET). Despite its privacy benefits, FL\nremains vulnerable to privacy attacks, including those targeting specific\nclients. In this paper, we study an underexplored threat where a dishonest\norchestrator intentionally manipulates the aggregation process to induce\ntargeted overfitting in the local models of specific clients. Whereas many\nstudies in this area predominantly focus on reducing the amount of information\nleakage during training, we focus on enabling an early client-side detection of\ntargeted overfitting, thereby allowing clients to disengage before significant\nharm occurs. In line with this, we propose three detection techniques - (a)\nlabel flipping, (b) backdoor trigger injection, and (c) model fingerprinting -\nthat enable clients to verify the integrity of the global aggregation. We\nevaluated our methods on multiple datasets under different attack scenarios.\nOur results show that the three methods reliably detect targeted overfitting\ninduced by the orchestrator, but they differ in terms of computational\ncomplexity, detection latency, and false-positive rates.", "published": "2025-09-15 14:23:39", "link": "http://arxiv.org/abs/2509.11974v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "MusicSwarm: Biologically Inspired Intelligence for Music Composition", "abstract": "We show that coherent, long-form musical composition can emerge from a\ndecentralized swarm of identical, frozen foundation models that coordinate via\nstigmergic, peer-to-peer signals, without any weight updates. We compare a\ncentralized multi-agent system with a global critic to a fully decentralized\nswarm in which bar-wise agents sense and deposit harmonic, rhythmic, and\nstructural cues, adapt short-term memory, and reach consensus. Across symbolic,\naudio, and graph-theoretic analyses, the swarm yields superior quality while\ndelivering greater diversity and structural variety and leads across creativity\nmetrics. The dynamics contract toward a stable configuration of complementary\nroles, and self-similarity networks reveal a small-world architecture with\nefficient long-range connectivity and specialized bridging motifs, clarifying\nhow local novelties consolidate into global musical form. By shifting\nspecialization from parameter updates to interaction rules, shared memory, and\ndynamic consensus, MusicSwarm provides a compute- and data-efficient route to\nlong-horizon creative structure that is immediately transferable beyond music\nto collaborative writing, design, and scientific discovery.", "published": "2025-09-15 14:23:09", "link": "http://arxiv.org/abs/2509.11973v1", "categories": ["cs.AI", "cs.MM", "cs.SD"], "primary_category": "cs.AI"}
{"title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study", "abstract": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies.", "published": "2025-09-15 14:22:08", "link": "http://arxiv.org/abs/2509.11971v1", "categories": ["cs.RO", "cs.AI", "cs.CR"], "primary_category": "cs.RO"}
{"title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students", "abstract": "This project addresses a critical pedagogical need: offering students\ncontinuous, on-demand academic assistance beyond conventional reception hours.\nI present a domain-specific Retrieval-Augmented Generation (RAG) system powered\nby a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The\nassistant enhances learning by delivering real-time, personalized responses\naligned with the \"Introduction to Parallel Processing\" course materials. GPU\nacceleration significantly improves inference latency, enabling practical\ndeployment on consumer hardware. This approach demonstrates how consumer GPUs\ncan enable affordable, private, and effective AI tutoring for HPC education.", "published": "2025-09-15 14:06:09", "link": "http://arxiv.org/abs/2509.11947v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare", "abstract": "Healthcare and medicine are multimodal disciplines that deal with multimodal\ndata for reasoning and diagnosing multiple diseases. Although some multimodal\nreasoning models have emerged for reasoning complex tasks in scientific\ndomains, their applications in the healthcare domain remain limited and fall\nshort in correct reasoning for diagnosis. To address the challenges of\nmultimodal medical reasoning for correct diagnosis and assist the healthcare\nprofessionals, a novel temporal graph-based reasoning process modelled through\na directed graph has been proposed in the current work. It helps in\naccommodating dynamic changes in reasons through backtracking, refining the\nreasoning content, and creating new or deleting existing reasons to reach the\nbest recommendation or answer. Again, consideration of multimodal data at\ndifferent time points can enable tracking and analysis of patient health and\ndisease progression. Moreover, the proposed multi-agent temporal reasoning\nframework provides task distributions and a cross-validation mechanism to\nfurther enhance the accuracy of reasoning outputs. A few basic experiments and\nanalysis results justify the novelty and practical utility of the proposed\npreliminary approach.", "published": "2025-09-15 14:03:19", "link": "http://arxiv.org/abs/2509.11944v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics", "abstract": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.", "published": "2025-09-15 14:03:06", "link": "http://arxiv.org/abs/2509.11943v1", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.MA"], "primary_category": "cs.AI"}
{"title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems", "abstract": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality.", "published": "2025-09-15 14:02:29", "link": "http://arxiv.org/abs/2509.11942v1", "categories": ["cs.SE", "cs.AI", "cs.HC"], "primary_category": "cs.SE"}
{"title": "Neuromorphic Intelligence", "abstract": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which depend on massive computational and\nenergy resources, neuromorphic systems exploit brain-inspired principles of\ncomputation to achieve orders of magnitude greater energy efficiency. By\ndrawing on insights from artificial intelligence, neuroscience, physics,\nchemistry, and materials science, neuromorphic computing promises to deliver\nintelligent systems that are sustainable, transparent, and widely accessible. A\ncentral challenge, however, is to identify a unifying theoretical framework\ncapable of bridging these diverse disciplines. We argue that dynamical systems\ntheory provides such a foundation. Rooted in differential calculus, it offers a\nprincipled language for modeling inference, learning, and control in both\nnatural and artificial substrates. Within this framework, noise can be\nharnessed as a resource for learning, while differential genetic programming\nenables the discovery of dynamical systems that implement adaptive behaviors.\nEmbracing this perspective paves the way toward emergent neuromorphic\nintelligence, where intelligent behavior arises from the dynamics of physical\nsubstrates, advancing both the science and sustainability of AI.", "published": "2025-09-15 13:59:42", "link": "http://arxiv.org/abs/2509.11940v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "MMORE: Massive Multimodal Open RAG & Extraction", "abstract": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore.", "published": "2025-09-15 13:56:06", "link": "http://arxiv.org/abs/2509.11937v1", "categories": ["cs.SE", "cs.AI", "D.2.0; E.m"], "primary_category": "cs.SE"}
{"title": "BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning", "abstract": "Reinforcement learning (RL) has proven effective for AI-based building energy\nmanagement. However, there is a lack of flexible framework to implement RL\nacross various control problems in building energy management. To address this\ngap, we propose BuildingGym, an open-source tool designed as a\nresearch-friendly and flexible framework for training RL control strategies for\ncommon challenges in building energy management. BuildingGym integrates\nEnergyPlus as its core simulator, making it suitable for both system-level and\nroom-level control. Additionally, BuildingGym is able to accept external\nsignals as control inputs instead of taking the building as a stand-alone\nentity. This feature makes BuildingGym applicable for more flexible\nenvironments, e.g. smart grid and EVs community. The tool provides several\nbuilt-in RL algorithms for control strategy training, simplifying the process\nfor building managers to obtain optimal control strategies. Users can achieve\nthis by following a few straightforward steps to configure BuildingGym for\noptimization control for common problems in the building energy management\nfield. Moreover, AI specialists can easily implement and test state-of-the-art\ncontrol algorithms within the platform. BuildingGym bridges the gap between\nbuilding managers and AI specialists by allowing for the easy configuration and\nreplacement of RL algorithms, simulators, and control environments or problems.\nWith BuildingGym, we efficiently set up training tasks for cooling load\nmanagement, targeting both constant and dynamic cooling load management. The\nbuilt-in algorithms demonstrated strong performance across both tasks,\nhighlighting the effectiveness of BuildingGym in optimizing cooling strategies.", "published": "2025-09-15 13:37:48", "link": "http://arxiv.org/abs/2509.11922v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models", "abstract": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex\nmodels that process real-time omnimodal streams. EgoMem enables real-time\nmodels to recognize multiple users directly from raw audiovisual streams, to\nprovide personalized response, and to maintain long-term knowledge of users'\nfacts, preferences, and social relationships extracted from audiovisual\nhistory. EgoMem operates with three asynchronous processes: (i) a retrieval\nprocess that dynamically identifies user via face and voice, and gathers\nrelevant context from a long-term memory; (ii) an omnimodal dialog process that\ngenerates personalized audio responses based on the retrieved context; and\n(iii) a memory management process that automatically detects dialog boundaries\nfrom omnimodal streams, and extracts necessary information to update the\nlong-term memory. Unlike existing memory agents for LLMs, EgoMem relies\nentirely on raw audiovisual streams, making it especially suitable for\nlifelong, real-time, and embodied scenarios. Experimental results demonstrate\nthat EgoMem's retrieval and memory management modules achieve over 95% accuracy\non the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,\nthe system achieves fact-consistency scores above 87% in real-time personalized\ndialogs, establishing a strong baseline for future research.", "published": "2025-09-15 13:33:29", "link": "http://arxiv.org/abs/2509.11914v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Integrating Prior Observations for Incremental 3D Scene Graph Prediction", "abstract": "3D semantic scene graphs (3DSSG) provide compact structured representations\nof environments by explicitly modeling objects, attributes, and relationships.\nWhile 3DSSGs have shown promise in robotics and embodied AI, many existing\nmethods rely mainly on sensor data, not integrating further information from\nsemantically rich environments. Additionally, most methods assume access to\ncomplete scene reconstructions, limiting their applicability in real-world,\nincremental settings. This paper introduces a novel heterogeneous graph model\nfor incremental 3DSSG prediction that integrates additional, multi-modal\ninformation, such as prior observations, directly into the message-passing\nprocess. Utilizing multiple layers, the model flexibly incorporates global and\nlocal scene representations without requiring specialized modules or full scene\nreconstructions. We evaluate our approach on the 3DSSG dataset, showing that\nGNNs enriched with multi-modal information such as semantic embeddings (e.g.,\nCLIP) and prior observations offer a scalable and generalizable solution for\ncomplex, real-world environments. The full source code of the presented\narchitecture will be made available at\nhttps://github.com/m4renz/incremental-scene-graph-prediction.", "published": "2025-09-15 13:10:34", "link": "http://arxiv.org/abs/2509.11895v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning", "abstract": "This paper introduces a novel application of Supervised Contrastive Learning\n(SupCon) to Imitation Learning (IL), with a focus on learning more effective\nstate representations for agents in video game environments. The goal is to\nobtain latent representations of the observations that capture better the\naction-relevant factors, thereby modeling better the cause-effect relationship\nfrom the observations that are mapped to the actions performed by the\ndemonstrator, for example, the player jumps whenever an obstacle appears ahead.\nWe propose an approach to integrate the SupCon loss with continuous output\nspaces, enabling SupCon to operate without constraints regarding the type of\nactions of the environment. Experiments on the 3D games Astro Bot and Returnal,\nand multiple 2D Atari games show improved representation quality, faster\nlearning convergence, and better generalization compared to baseline models\ntrained only with supervised action prediction loss functions.", "published": "2025-09-15 13:00:29", "link": "http://arxiv.org/abs/2509.11880v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer", "abstract": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies.", "published": "2025-09-15 12:39:15", "link": "http://arxiv.org/abs/2509.11865v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Bridging Vision Language Models and Symbolic Grounding for Video Question Answering", "abstract": "Video Question Answering (VQA) requires models to reason over spatial,\ntemporal, and causal cues in videos. Recent vision language models (VLMs)\nachieve strong results but often rely on shallow correlations, leading to weak\ntemporal grounding and limited interpretability. We study symbolic scene graphs\n(SGs) as intermediate grounding signals for VQA. SGs provide structured\nobject-relation representations that complement VLMs holistic reasoning. We\nintroduce SG-VLM, a modular framework that integrates frozen VLMs with scene\ngraph grounding via prompting and visual localization. Across three benchmarks\n(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM\nimproves causal and temporal reasoning and outperforms prior baselines, though\ngains over strong VLMs are limited. These findings highlight both the promise\nand current limitations of symbolic grounding, and offer guidance for future\nhybrid VLM-symbolic approaches in video understanding.", "published": "2025-09-15 12:35:56", "link": "http://arxiv.org/abs/2509.11862v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network", "abstract": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github.", "published": "2025-09-15 12:25:25", "link": "http://arxiv.org/abs/2509.11838v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio", "abstract": "Online AI platforms for creating music from text prompts (AI music), such as\nSuno and Udio, are now being used by hundreds of thousands of users. Some AI\nmusic is appearing in advertising, and even charting, in multiple countries.\nHow are these platforms being used? What subjects are inspiring their users?\nThis article answers these questions for Suno and Udio using a large collection\nof songs generated by users of these platforms from May to October 2024. Using\na combination of state-of-the-art text embedding models, dimensionality\nreduction and clustering methods, we analyze the prompts, tags and lyrics, and\nautomatically annotate and display the processed data in interactive plots. Our\nresults reveal prominent themes in lyrics, language preference, prompting\nstrategies, as well as peculiar attempts at steering models through the use of\nmetatags. To promote the musicological study of the developing cultural\npractice of AI-generated music we share our code and resources.", "published": "2025-09-15 12:10:50", "link": "http://arxiv.org/abs/2509.11824v1", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "cs.IR"}
{"title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models", "abstract": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.", "published": "2025-09-15 11:53:56", "link": "http://arxiv.org/abs/2509.11815v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference", "abstract": "Trajectory data has become a key resource for automated map in-ference due to\nits low cost, broad coverage, and continuous availability. However, uneven\ntrajectory density often leads to frag-mented roads in sparse areas and\nredundant segments in dense regions, posing significant challenges for existing\nmethods. To address these issues, we propose DGMap, a dual-decoding framework\nwith global context awareness, featuring Multi-scale Grid Encoding,\nMask-enhanced Keypoint Extraction, and Global Context-aware Relation\nPrediction. By integrating global semantic context with local geometric\nfeatures, DGMap improves keypoint detection accuracy to reduce road\nfragmentation in sparse-trajectory areas. Additionally, the Global\nContext-aware Relation Prediction module suppresses false connections in\ndense-trajectory regions by modeling long-range trajectory patterns.\nExperimental results on three real-world datasets show that DGMap outperforms\nstate-of-the-art methods by 5% in APLS, with notable performance gains on\ntrajectory data from the Didi Chuxing platform", "published": "2025-09-15 09:31:38", "link": "http://arxiv.org/abs/2509.11731v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Microsurgical Instrument Segmentation for Robot-Assisted Surgery", "abstract": "Accurate segmentation of thin structures is critical for microsurgical scene\nunderstanding but remains challenging due to resolution loss, low contrast, and\nclass imbalance. We propose Microsurgery Instrument Segmentation for Robotic\nAssistance(MISRA), a segmentation framework that augments RGB input with\nluminance channels, integrates skip attention to preserve elongated features,\nand employs an Iterative Feedback Module(IFM) for continuity restoration across\nmultiple passes. In addition, we introduce a dedicated microsurgical dataset\nwith fine-grained annotations of surgical instruments including thin objects,\nproviding a benchmark for robust evaluation Dataset available at\nhttps://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate\nthat MISRA achieves competitive performance, improving the mean class IoU by\n5.37% over competing methods, while delivering more stable predictions at\ninstrument contacts and overlaps. These results position MISRA as a promising\nstep toward reliable scene parsing for computer-assisted and robotic\nmicrosurgery.", "published": "2025-09-15 09:29:27", "link": "http://arxiv.org/abs/2509.11727v1", "categories": ["cs.CV", "cs.AI", "I.4.6; I.4.8"], "primary_category": "cs.CV"}
{"title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction", "abstract": "Multi-agent trajectory prediction in autonomous driving requires a\ncomprehensive understanding of complex social dynamics. Existing methods,\nhowever, often struggle to capture the full richness of these dynamics,\nparticularly the co-existence of multi-scale interactions and the diverse\nbehaviors of heterogeneous agents. To address these challenges, this paper\nintroduces HeLoFusion, an efficient and scalable encoder for modeling\nheterogeneous and multi-scale agent interactions. Instead of relying on global\ncontext, HeLoFusion constructs local, multi-scale graphs centered on each\nagent, allowing it to effectively model both direct pairwise dependencies and\ncomplex group-wise interactions (\\textit{e.g.}, platooning vehicles or\npedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of\nagent heterogeneity through an aggregation-decomposition message-passing scheme\nand type-specific feature networks, enabling it to learn nuanced,\ntype-dependent interaction patterns. This locality-focused approach enables a\nprincipled representation of multi-level social context, yielding powerful and\nexpressive agent embeddings. On the challenging Waymo Open Motion Dataset,\nHeLoFusion achieves state-of-the-art performance, setting new benchmarks for\nkey metrics including Soft mAP and minADE. Our work demonstrates that a\nlocality-grounded architecture, which explicitly models multi-scale and\nheterogeneous interactions, is a highly effective strategy for advancing motion\nforecasting.", "published": "2025-09-15 09:19:41", "link": "http://arxiv.org/abs/2509.11719v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models", "abstract": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.", "published": "2025-09-15 08:38:01", "link": "http://arxiv.org/abs/2509.11686v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering", "abstract": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads.", "published": "2025-09-15 08:02:55", "link": "http://arxiv.org/abs/2509.11663v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition", "abstract": "Intelligent tableware cleaning is a critical application in food safety and\nsmart homes, but existing methods are limited by coarse-grained classification\nand scarcity of few-shot data, making it difficult to meet industrialization\nrequirements. We propose DTGen, a few-shot data augmentation scheme based on\ngenerative diffusion models, specifically designed for fine-grained dirty\ntableware recognition. DTGen achieves efficient domain specialization through\nLoRA, generates diverse dirty images via structured prompts, and ensures data\nquality through CLIP-based cross-modal filtering. Under extremely limited real\nfew-shot conditions, DTGen can synthesize virtually unlimited high-quality\nsamples, significantly improving classifier performance and supporting\nfine-grained dirty tableware recognition. We further elaborate on lightweight\ndeployment strategies, promising to transfer DTGen's benefits to embedded\ndishwashers and integrate with cleaning programs to intelligently regulate\nenergy consumption and detergent usage. Research results demonstrate that DTGen\nnot only validates the value of generative AI in few-shot industrial vision but\nalso provides a feasible deployment path for automated tableware cleaning and\nfood safety monitoring.", "published": "2025-09-15 07:59:34", "link": "http://arxiv.org/abs/2509.11661v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework", "abstract": "This study presents the first comprehensive evaluation of Multimodal Large\nLanguage Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)\nself-management. We constructed a database of approximately 3,000\nanteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a\n`Divide and Conquer' framework consisting of a visual question-answering task,\na domain knowledge assessment task, and a patient education counseling\nassessment task. Our investigation revealed limitations of MLLMs' ability in\ninterpreting complex spinal radiographs and comprehending AIS care knowledge.\nTo address these, we pioneered enhancing MLLMs with spinal keypoint prompting\nand compiled an AIS knowledge base for retrieval augmented generation (RAG),\nrespectively. Results showed varying effectiveness of visual prompting across\ndifferent architectures, while RAG substantially improved models' performances\non the knowledge assessment task. Our findings indicate current MLLMs are far\nfrom capable in realizing personalized assistant in AIS care. The greatest\nchallenge lies in their abilities to obtain accurate detections of spinal\ndeformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).", "published": "2025-09-15 07:34:12", "link": "http://arxiv.org/abs/2509.11645v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications", "abstract": "With the emergence of diverse and massive data in the upcoming\nsixth-generation (6G) networks, the task-agnostic semantic communication system\nis regarded to provide robust intelligent services. In this paper, we propose a\ntask-agnostic learnable weighted-knowledge base semantic communication (TALSC)\nframework for robust image transmission to address the real-world heterogeneous\ndata bias in KB, including label flipping noise and class imbalance. The TALSC\nframework incorporates a sample confidence module (SCM) as meta-learner and the\nsemantic coding networks as learners. The learners are updated based on the\nempirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,\nthe meta-learner evaluates the significance of samples according to the task\nloss feedback, and adjusts the update strategy of learners to enhance the\nrobustness in semantic recovery for unknown tasks. To strike a balance between\nSCM parameters and precision of significance evaluation, we design an SCM-grid\nextension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)\nwithin SCM, which leverages the concept of spline refinement in KAN and enables\nscalable SCM with customizable granularity without retraining. Simulations\ndemonstrate that the TALSC framework effectively mitigates the effects of\nflipping noise and class imbalance in task-agnostic image semantic\ncommunication, achieving at least 12% higher semantic recovery accuracy (SRA)\nand multi-scale structural similarity (MS-SSIM) compared to state-of-the-art\nmethods.", "published": "2025-09-15 07:10:21", "link": "http://arxiv.org/abs/2509.11636v1", "categories": ["cs.IT", "cs.AI", "cs.NI", "math.IT"], "primary_category": "cs.IT"}
{"title": "Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check", "abstract": "As large language models (LLMs) continue to advance in capabilities, ensuring\ntheir safety against jailbreak attacks remains a critical challenge. In this\npaper, we introduce a novel safety alignment approach called Answer-Then-Check,\nwhich enhances LLM robustness against malicious prompts by applying thinking\nability to mitigate jailbreaking problems before producing a final answer to\nthe user. Our method enables models to directly answer the question in their\nthought and then critically evaluate its safety before deciding whether to\nprovide it. To implement this approach, we construct the Reasoned Safety\nAlignment (ReSA) dataset, comprising 80K examples that teach models to reason\nthrough direct responses and then analyze their safety. Experimental results\ndemonstrate that our approach achieves the Pareto frontier with superior safety\ncapability while decreasing over-refusal rates on over-refusal benchmarks.\nNotably, the model fine-tuned with ReSA maintains general reasoning\ncapabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our\nmethod equips models with the ability to perform safe completion. Unlike\npost-hoc methods that can only reject harmful queries, our model can provide\nhelpful and safe alternative responses for sensitive topics (e.g., self-harm).\nFurthermore, we discover that training on a small subset of just 500 examples\ncan achieve comparable performance to using the full dataset, suggesting that\nsafety alignment may require less data than previously assumed.", "published": "2025-09-15 06:47:35", "link": "http://arxiv.org/abs/2509.11629v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching", "abstract": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}", "published": "2025-09-15 06:46:22", "link": "http://arxiv.org/abs/2509.11628v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "abstract": "Recent advancements in Large Language Models (LLMs) has lead to the\ndevelopment of agents capable of complex reasoning and interaction with\nexternal tools. In enterprise contexts, the effective use of such tools that\nare often enabled by application programming interfaces (APIs), is hindered by\npoor documentation, complex input or output schema, and large number of\noperations. These challenges make tool selection difficult and reduce the\naccuracy of payload formation by up to 25%. We propose ACE, an automated tool\ncreation and enrichment framework that transforms enterprise APIs into\nLLM-compatible tools. ACE, (i) generates enriched tool specifications with\nparameter descriptions and examples to improve selection and invocation\naccuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters\nrelevant tools at runtime, reducing prompt complexity while maintaining\nscalability. We validate our framework on both proprietary and open-source APIs\nand demonstrate its integration with agentic frameworks. To the best of our\nknowledge, ACE is the first end-to-end framework that automates the creation,\nenrichment, and dynamic selection of enterprise API tools for LLM agents.", "published": "2025-09-15 06:41:54", "link": "http://arxiv.org/abs/2509.11626v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Inducing Uncertainty for Test-Time Privacy", "abstract": "Unlearning is the predominant method for removing the influence of data in\nmachine learning models. However, even after unlearning, models often continue\nto produce the same predictions on the unlearned data with high confidence.\nThis persistent behavior can be exploited by adversaries using confident model\npredictions on incorrect or obsolete data to harm users. We call this threat\nmodel, which unlearning fails to protect against, *test-time privacy*. In\nparticular, an adversary with full model access can bypass any naive defenses\nwhich ensure test-time privacy. To address this threat, we introduce an\nalgorithm which perturbs model weights to induce maximal uncertainty on\nprotected instances while preserving accuracy on the rest of the instances. Our\ncore algorithm is based on finetuning with a Pareto optimal objective that\nexplicitly balances test-time privacy against utility. We also provide a\ncertifiable approximation algorithm which achieves $(\\varepsilon, \\delta)$\nguarantees without convexity assumptions. We then prove a tight, non-vacuous\nbound that characterizes the privacy-utility tradeoff that our algorithms\nincur. Empirically, our method obtains $>3\\times$ stronger uncertainty than\npretraining with $<0.2\\%$ drops in accuracy on various image recognition\nbenchmarks. Altogether, this framework provides a tool to guarantee additional\nprotection to end users.", "published": "2025-09-15 06:38:57", "link": "http://arxiv.org/abs/2509.11625v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification", "abstract": "Effective network state classification is a primary task for ensuring network\nsecurity and optimizing performance. Existing deep learning models have shown\nconsiderable progress in this area. Some methods excel at analyzing the complex\ntemporal periodicities found in traffic data, while graph-based approaches are\nadept at modeling the dynamic dependencies between different variables.\nHowever, a key trade-off remains, as these methods struggle to capture both\ncharacteristics simultaneously. Models focused on temporal patterns often\noverlook crucial variable dependencies, whereas those centered on dependencies\nmay fail to capture fine-grained temporal details. To address this trade-off,\nwe introduce DAPNet, a framework based on a Mixture-of-Experts architecture.\nDAPNet integrates three specialized networks for periodic analysis, dynamic\ncross-variable correlation modeling, and hybrid temporal feature extraction. A\nlearnable gating network dynamically assigns weights to experts based on the\ninput sample and computes a weighted fusion of their outputs. Furthermore, a\nhybrid regularization loss function ensures stable training and addresses the\ncommon issue of class imbalance. Extensive experiments on two large-scale\nnetwork intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher\naccuracy for its target application. The generalizability of the architectural\ndesign is evaluated across ten public UEA benchmark datasets, positioning\nDAPNet as a specialized framework for network state classification.", "published": "2025-09-15 05:32:32", "link": "http://arxiv.org/abs/2509.11601v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions", "abstract": "Anti-money laundering (AML) research is constrained by the lack of publicly\nshareable, regulation-aligned transaction datasets. We present AMLNet, a\nknowledge-based multi-agent framework with two coordinated units: a\nregulation-aware transaction generator and an ensemble detection pipeline. The\ngenerator produces 1,090,173 synthetic transactions (approximately 0.16\\%\nlaundering-positive) spanning core laundering phases (placement, layering,\nintegration) and advanced typologies (e.g., structuring, adaptive threshold\nbehavior). Regulatory alignment reaches 75\\% based on AUSTRAC rule coverage\n(Section 4.2), while a composite technical fidelity score of 0.75 summarizes\ntemporal, structural, and behavioral realism components (Section 4.4). The\ndetection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the\ninternal test partitions of AMLNet and adapts to the external SynthAML dataset,\nindicating architectural generalizability across different synthetic generation\nparadigms. We provide multi-dimensional evaluation (regulatory, temporal,\nnetwork, behavioral) and release the dataset (Version 1.0,\nhttps://doi.org/10.5281/zenodo.16736515), to advance reproducible and\nregulation-conscious AML experimentation.", "published": "2025-09-15 05:25:46", "link": "http://arxiv.org/abs/2509.11595v1", "categories": ["cs.AI", "cs.CE", "cs.CR", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning", "abstract": "GBPP is a fast learning based scorer that selects a robot base pose for\ngrasping from a single RGB-D snapshot. The method uses a two stage curriculum:\n(1) a simple distance-visibility rule auto-labels a large dataset at low cost;\nand (2) a smaller set of high fidelity simulation trials refines the model to\nmatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLP\nscores dense grids of candidate poses, enabling rapid online selection without\nfull task-and-motion optimization. In simulation and on a real mobile\nmanipulator, GBPP outperforms proximity and geometry only baselines, choosing\nsafer and more reachable stances and degrading gracefully when wrong. The\nresults offer a practical recipe for data efficient, geometry aware base\nplacement: use inexpensive heuristics for coverage, then calibrate with\ntargeted simulation.", "published": "2025-09-15 05:25:40", "link": "http://arxiv.org/abs/2509.11594v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification", "abstract": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to\nlearn modality-invariant image features from unlabeled cross-modal person\ndatasets by reducing the modality gap while minimizing reliance on costly\nmanual annotations. Existing methods typically address USVI-ReID using\ncluster-based contrastive learning, which represents a person by a single\ncluster center. However, they primarily focus on the commonality of images\nwithin each cluster while neglecting the finer-grained differences among them.\nTo address the limitation, we propose a Hierarchical Identity Learning (HIL)\nframework. Since each cluster may contain several smaller sub-clusters that\nreflect fine-grained variations among images, we generate multiple memories for\neach existing coarse-grained cluster via a secondary clustering. Additionally,\nwe propose Multi-Center Contrastive Learning (MCCL) to refine representations\nfor enhancing intra-modal clustering and minimizing cross-modal discrepancies.\nTo further improve cross-modal matching quality, we design a Bidirectional\nReverse Selection Transmission (BRST) mechanism, which establishes reliable\ncross-modal correspondences by performing bidirectional matching of\npseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB\ndatasets demonstrate that the proposed method outperforms existing approaches.\nThe source code is available at: https://github.com/haonanshi0125/HIL.", "published": "2025-09-15 05:10:43", "link": "http://arxiv.org/abs/2509.11587v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models", "abstract": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.", "published": "2025-09-15 04:39:50", "link": "http://arxiv.org/abs/2509.11575v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Dstack: A Zero Trust Framework for Confidential Containers", "abstract": "Web3 applications require execution platforms that maintain confidentiality\nand integrity without relying on centralized trust authorities. While Trusted\nExecution Environments (TEEs) offer promising capabilities for confidential\ncomputing, current implementations face significant limitations when applied to\nWeb3 contexts, particularly in security reliability, censorship resistance, and\nvendor independence.\n  This paper presents dstack, a comprehensive framework that transforms raw TEE\ntechnology into a true Zero Trust platform. We introduce three key innovations:\n(1) Portable Confidential Containers that enable seamless workload migration\nacross heterogeneous TEE environments while maintaining security guarantees,\n(2) Decentralized Code Management that leverages smart contracts for\ntransparent governance of TEE applications, and (3) Verifiable Domain\nManagement that ensures secure and verifiable application identity without\ncentralized authorities.\n  These innovations are implemented through three core components: dstack-OS,\ndstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both\nthe performance advantages of VM-level TEE solutions and the trustless\nguarantees required by Web3 applications. Our evaluation shows that dstack\nprovides comprehensive security guarantees while maintaining practical\nusability for real-world applications.", "published": "2025-09-15 03:36:27", "link": "http://arxiv.org/abs/2509.11555v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Task Decoding based on Eye Movements using Synthetic Data Augmentation", "abstract": "Machine learning has been extensively used in various applications related to\neye-tracking research. Understanding eye movement is one of the most\nsignificant subsets of eye-tracking research that reveals the scanning pattern\nof an individual. Researchers have thoroughly analyzed eye movement data to\nunderstand various eye-tracking applications, such as attention mechanisms,\nnavigational behavior, task understanding, etc. The outcome of traditional\nmachine learning algorithms used for decoding tasks based on eye movement data\nhas received a mixed reaction to Yarbus' claim that it is possible to decode\nthe observer's task from their eye movements. In this paper, to support the\nhypothesis by Yarbus, we are decoding tasks categories while generating\nsynthetic data samples using well-known Synthetic Data Generators CTGAN and its\nvariations such as CopulaGAN and Gretel AI Synthetic Data generators on\navailable data from an in-person user study. Our results show that augmenting\nmore eye movement data combined with additional synthetically generated\nimproves classification accuracy even with traditional machine learning\nalgorithms. We see a significant improvement in task decoding accuracy from\n28.1% using Random Forest to 82% using Inception Time when five times more data\nis added in addition to the 320 real eye movement dataset sample. Our proposed\nframework outperforms all the available studies on this dataset because of the\nuse of additional synthetic datasets. We validated our claim with various\nalgorithms and combinations of real and synthetic data to show how decoding\naccuracy increases with the increase in the augmentation of generated data to\nreal data.", "published": "2025-09-15 03:28:21", "link": "http://arxiv.org/abs/2509.11547v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning", "abstract": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.", "published": "2025-09-15 03:24:08", "link": "http://arxiv.org/abs/2509.11543v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Know What You Don't Know: Selective Prediction for Early Exit DNNs", "abstract": "Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the\nbottlenecks in deploying them in critical applications like sensitive tasks.\nEarly Exit (EE) DNNs overcome the latency issues by allowing samples to exit\nfrom intermediary layers if they attain `high' confidence scores on the\npredicted class. However, the DNNs are known to exhibit overconfidence, which\ncan lead to many samples exiting early and render EE strategies untrustworthy.\nWe use Selective Prediction (SP) to overcome this issue by checking the\n`hardness' of the samples rather than just relying on the confidence score\nalone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs)\nat each layer to check the hardness of samples before performing EEs.\nSpecifically, the DCs identify if a sample is hard to predict at an\nintermediary layer, leading to hallucination, and defer it to an expert. Early\ndetection of hard samples for inference prevents the wastage of computational\nresources and improves trust by deferring the hard samples to the expert. We\ndemonstrate that EE aided with SP improves both accuracy and latency. Our\nmethod minimizes the risk of wrong prediction by $50\\%$ with a speedup of\n$2.05\\times$ as compared to the final layer. The anonymized source code is\navailable at https://github.com/Div290/SPEED", "published": "2025-09-15 02:19:09", "link": "http://arxiv.org/abs/2509.11520v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Machine Learning-Driven Predictive Resource Management in Complex Science Workflows", "abstract": "The collaborative efforts of large communities in science experiments, often\ncomprising thousands of global members, reflect a monumental commitment to\nexploration and discovery. Recently, advanced and complex data processing has\ngained increasing importance in science experiments. Data processing workflows\ntypically consist of multiple intricate steps, and the precise specification of\nresource requirements is crucial for each step to allocate optimal resources\nfor effective processing. Estimating resource requirements in advance is\nchallenging due to a wide range of analysis scenarios, varying skill levels\namong community members, and the continuously increasing spectrum of computing\noptions. One practical approach to mitigate these challenges involves initially\nprocessing a subset of each step to measure precise resource utilization from\nactual processing profiles before completing the entire step. While this\ntwo-staged approach enables processing on optimal resources for most of the\nworkflow, it has drawbacks such as initial inaccuracies leading to potential\nfailures and suboptimal resource usage, along with overhead from waiting for\ninitial processing completion, which is critical for fast-turnaround analyses.\nIn this context, our study introduces a novel pipeline of machine learning\nmodels within a comprehensive workflow management system, the Production and\nDistributed Analysis (PanDA) system. These models employ advanced machine\nlearning techniques to predict key resource requirements, overcoming challenges\nposed by limited upfront knowledge of characteristics at each step. Accurate\nforecasts of resource requirements enable informed and proactive\ndecision-making in workflow management, enhancing the efficiency of handling\ndiverse, complex workflows across heterogeneous resources.", "published": "2025-09-15 01:53:30", "link": "http://arxiv.org/abs/2509.11512v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "68T05, 68M14, 68W10"], "primary_category": "cs.DC"}
{"title": "MedicalOS: An LLM Agent based Operating System for Digital Healthcare", "abstract": "Decades' advances in digital health technologies, such as electronic health\nrecords, have largely streamlined routine clinical processes. Yet, most these\nsystems are still hard to learn and use: Clinicians often face the burden of\nmanaging multiple tools, repeating manual actions for each patient, navigating\ncomplicated UI trees to locate functions, and spending significant time on\nadministration instead of caring for patients. The recent rise of large\nlanguage model (LLM) based agents demonstrates exceptional capability in coding\nand computer operation, revealing the potential for humans to interact with\noperating systems and software not by direct manipulation, but by instructing\nagents through natural language. This shift highlights the need for an\nabstraction layer, an agent-computer interface, that translates human language\ninto machine-executable commands. In digital healthcare, however, requires a\nmore domain-specific abstractions that strictly follow trusted clinical\nguidelines and procedural standards to ensure safety, transparency, and\ncompliance. To address this need, we present \\textbf{MedicalOS}, a unified\nagent-based operational system designed as such a domain-specific abstract\nlayer for healthcare. It translates human instructions into pre-defined digital\nhealthcare commands, such as patient inquiry, history retrieval, exam\nmanagement, report generation, referrals, treatment planning, that we wrapped\nas off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,\nLinux). We empirically validate MedicalOS on 214 patient cases across 22\nspecialties, demonstrating high diagnostic accuracy and confidence, clinically\nsound examination requests, and consistent generation of structured reports and\nmedication recommendations. These results highlight MedicalOS as a trustworthy\nand scalable foundation for advancing workflow automation in clinical practice.", "published": "2025-09-15 01:43:20", "link": "http://arxiv.org/abs/2509.11507v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "RAPTOR: A Foundation Policy for Quadrotor Control", "abstract": "Humans are remarkably data-efficient when adapting to new unseen conditions,\nlike driving a new car. In contrast, modern robotic control systems, like\nneural network policies trained using Reinforcement Learning (RL), are highly\nspecialized for single environments. Because of this overfitting, they are\nknown to break down even under small differences like the Simulation-to-Reality\n(Sim2Real) gap and require system identification and retraining for even\nminimal changes to the system. In this work, we present RAPTOR, a method for\ntraining a highly adaptive foundation policy for quadrotor control. Our method\nenables training a single, end-to-end neural-network policy to control a wide\nvariety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg\nthat also differ in motor type (brushed vs. brushless), frame type (soft vs.\nrigid), propeller type (2/3/4-blade), and flight controller\n(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy\nwith only 2084 parameters is sufficient for zero-shot adaptation to a wide\nvariety of platforms. The adaptation through In-Context Learning is made\npossible by using a recurrence in the hidden layer. The policy is trained\nthrough a novel Meta-Imitation Learning algorithm, where we sample 1000\nquadrotors and train a teacher policy for each of them using Reinforcement\nLearning. Subsequently, the 1000 teachers are distilled into a single, adaptive\nstudent policy. We find that within milliseconds, the resulting foundation\npolicy adapts zero-shot to unseen quadrotors. We extensively test the\ncapabilities of the foundation policy under numerous conditions (trajectory\ntracking, indoor/outdoor, wind disturbance, poking, different propellers).", "published": "2025-09-15 00:05:40", "link": "http://arxiv.org/abs/2509.11481v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs", "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist\npolicies for robotic control, yet their performance scaling across model\narchitectures and hardware platforms, as well as their associated power\nbudgets, remain poorly understood. This work presents an evaluation of five\nrepresentative VLA models -- spanning state-of-the-art baselines and two newly\nproposed architectures -- targeting edge and datacenter GPU platforms. Using\nthe LIBERO benchmark, we measure accuracy alongside system-level metrics,\nincluding latency, throughput, and peak memory usage, under varying edge power\nconstraints and high-performance datacenter GPU configurations. Our results\nidentify distinct scaling trends: (1) architectural choices, such as action\ntokenization and model backbone size, strongly influence throughput and memory\nfootprint; (2) power-constrained edge devices exhibit non-linear performance\ndegradation, with some configurations matching or exceeding older datacenter\nGPUs; and (3) high-throughput variants can be achieved without significant\naccuracy loss. These findings provide actionable insights when selecting and\noptimizing VLAs across a range of deployment constraints. Our work challenges\ncurrent assumptions about the superiority of datacenter hardware for robotic\ninference.", "published": "2025-09-15 00:00:37", "link": "http://arxiv.org/abs/2509.11480v1", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Character-Centric Understanding of Animated Movies", "abstract": "Animated movies are captivating for their unique character designs and\nimaginative storytelling, yet they pose significant challenges for existing\nrecognition systems. Unlike the consistent visual patterns detected by\nconventional face recognition methods, animated characters exhibit extreme\ndiversity in their appearance, motion, and deformation. In this work, we\npropose an audio-visual pipeline to enable automatic and robust animated\ncharacter recognition, and thereby enhance character-centric understanding of\nanimated movies. Central to our approach is the automatic construction of an\naudio-visual character bank from online sources. This bank contains both visual\nexemplars and voice (audio) samples for each character, enabling subsequent\nmulti-modal character recognition despite long-tailed appearance distributions.\nBuilding on accurate character recognition, we explore two downstream\napplications: Audio Description (AD) generation for visually impaired\naudiences, and character-aware subtitling for the hearing impaired. To support\nresearch in this domain, we introduce CMD-AM, a new dataset of 75 animated\nmovies with comprehensive annotations. Our character-centric pipeline\ndemonstrates significant improvements in both accessibility and narrative\ncomprehension for animated content over prior face-detection-based approaches.\nFor the code and dataset, visit\nhttps://www.robots.ox.ac.uk/~vgg/research/animated_ad/.", "published": "2025-09-15 17:59:51", "link": "http://arxiv.org/abs/2509.12204v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence", "abstract": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.", "published": "2025-09-15 17:59:47", "link": "http://arxiv.org/abs/2509.12203v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling", "abstract": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.", "published": "2025-09-15 17:59:19", "link": "http://arxiv.org/abs/2509.12201v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review", "abstract": "In this paper, we present a comprehensive review of 3D human pose estimation\nand human mesh recovery from in-the-wild LiDAR point clouds. We compare\nexisting approaches across several key dimensions, and propose a structured\ntaxonomy to classify these methods. Following this taxonomy, we analyze each\nmethod's strengths, limitations, and design choices. In addition, (i) we\nperform a quantitative comparison of the three most widely used datasets,\ndetailing their characteristics; (ii) we compile unified definitions of all\nevaluation metrics; and (iii) we establish benchmark tables for both tasks on\nthese datasets to enable fair comparisons and promote progress in the field. We\nalso outline open challenges and research directions critical for advancing\nLiDAR-based 3D human understanding. Moreover, we maintain an accompanying\nwebpage that organizes papers according to our taxonomy and continuously update\nit with new studies:\nhttps://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR", "published": "2025-09-15 17:56:33", "link": "http://arxiv.org/abs/2509.12197v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Domain-Adaptive Pretraining Improves Primate Behavior Recognition", "abstract": "Computer vision for animal behavior offers promising tools to aid research in\necology, cognition, and to support conservation efforts. Video camera traps\nallow for large-scale data collection, but high labeling costs remain a\nbottleneck to creating large-scale datasets. We thus need data-efficient\nlearning approaches. In this work, we show that we can utilize self-supervised\nlearning to considerably improve action recognition on primate behavior. On two\ndatasets of great ape behavior (PanAf and ChimpACT), we outperform published\nstate-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.\nmAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and\napplying domain-adaptive pretraining (DAP), i.e. continuing the pretraining\nwith in-domain data. We show that most of the performance gain stems from the\nDAP. Our method promises great potential for improving the recognition of\nanimal behavior, as DAP does not require labeled samples. Code is available at\nhttps://github.com/ecker-lab/dap-behavior", "published": "2025-09-15 17:54:20", "link": "http://arxiv.org/abs/2509.12193v1", "categories": ["cs.CV", "I.4.8; I.2.10; I.5"], "primary_category": "cs.CV"}
{"title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury", "abstract": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for\nfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose\nRadiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic\nBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency of\nthis approach, we compare LoRA with traditional full fine-tuning and\ninference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3\nand 75 mm3), centered at the treatment isocenter, in addition to different\nadaptation techniques for adapting the 2D LVMs for 3D data were used to\ndetermine the sensitivity of the models to spatial context. Experimental\nresults show that LoRA achieves comparable or superior performance to\ntraditional fine-tuning while significantly reducing computational costs and\ntraining times by requiring fewer trainable parameters.", "published": "2025-09-15 17:21:22", "link": "http://arxiv.org/abs/2509.12155v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Open-ended Hierarchical Streaming Video Understanding with Vision Language Models", "abstract": "We introduce Hierarchical Streaming Video Understanding, a task that combines\nonline temporal action localization with free-form description generation.\nGiven the scarcity of datasets with hierarchical and fine-grained temporal\nannotations, we demonstrate that LLMs can effectively group atomic actions into\nhigher-level events, enriching existing datasets. We then propose OpenHOUSE\n(Open-ended Hierarchical Online Understanding System for Events), which extends\nstreaming action perception beyond action classification. OpenHOUSE features a\nspecialized streaming module that accurately detects boundaries between closely\nadjacent actions, nearly doubling the performance of direct extensions of\nexisting methods. We envision the future of streaming action perception in the\nintegration of powerful generative models, with OpenHOUSE representing a key\nstep in that direction.", "published": "2025-09-15 17:11:06", "link": "http://arxiv.org/abs/2509.12145v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RailSafeNet: Visual Scene Understanding for Tram Safety", "abstract": "Tram-human interaction safety is an important challenge, given that trams\nfrequently operate in densely populated areas, where collisions can range from\nminor injuries to fatal outcomes. This paper addresses the issue from the\nperspective of designing a solution leveraging digital image processing, deep\nlearning, and artificial intelligence to improve the safety of pedestrians,\ndrivers, cyclists, pets, and tram passengers. We present RailSafeNet, a\nreal-time framework that fuses semantic segmentation, object detection and a\nrule-based Distance Assessor to highlight track intrusions. Using only\nmonocular video, the system identifies rails, localises nearby objects and\nclassifies their risk by comparing projected distances with the standard 1435mm\nrail gauge. Experiments on the diverse RailSem19 dataset show that a\nclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),\nwhile a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated\nat an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore\ndelivers accurate, annotation-light scene understanding that can warn drivers\nbefore dangerous situations escalate. Code available at\nhttps://github.com/oValach/RailSafeNet.", "published": "2025-09-15 16:51:21", "link": "http://arxiv.org/abs/2509.12125v1", "categories": ["cs.CV", "68T45 (Primary), 68T07", "I.4.8"], "primary_category": "cs.CV"}
{"title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation", "abstract": "Few-shot semantic segmentation has recently attracted great attention. The\ngoal is to develop a model capable of segmenting unseen classes using only a\nfew annotated samples. Most existing approaches adapt a pre-trained model by\ntraining from scratch an additional module. Achieving optimal performance with\nthese approaches requires extensive training on large-scale datasets. The\nSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image and\nvideo segmentation with a modular design. In this paper, we propose a Few-Shot\nsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities\nare directly repurposed for the few-shot task. Moreover, we apply a Low-Rank\nAdaptation (LoRA) to the original modules in order to handle the diverse images\ntypically found in standard datasets, unlike the temporally connected frames\nused in SAM2's pre-training. With this approach, only a small number of\nparameters is meta-trained, which effectively adapts SAM2 while benefiting from\nits impressive segmentation performance. Our method supports any K-shot\nconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and\nFSS-1000 datasets, achieving remarkable results and demonstrating excellent\ncomputational efficiency during inference. Code is available at\nhttps://github.com/fornib/FS-SAM2", "published": "2025-09-15 16:32:31", "link": "http://arxiv.org/abs/2509.12105v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI", "abstract": "Reconstructing cardiac motion from cine CMR sequences is critical for\ndiagnosis, prediction, and intervention. Existing methods rely on complete CMR\nstacks to infer full heart motion, limiting their utility in intra-procedural\nscenarios where only sparse observations are available. We present TetHeart,\nthe first end-to-end framework that unifies full 4D multi-structure heart mesh\nrecovery from both offline full-stack acquisitions and intra-procedural\nsparse-slice observations. Our method leverages deep deformable tetrahedra, an\nexplicit-implicit hybrid representation, to capture shape and motion in a\ncoherent space shared across cardiac structures. It is initialized from\nhigh-quality pre-procedural or offline-acquired full stacks to build detailed,\npatient-specific heart meshes, which can then be updated using whatever slices\nare available, from full stacks down to a single slice. We further incorporate\nseveral key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D\nfeature assembly that dynamically integrates information from arbitrary numbers\nof slices at any position, combined with a distillation strategy from\nfull-slice to sparse-slice settings to ensure accurate reconstruction under\nextreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme\nrequiring only keyframe (e.g., ED and ES) annotations. Trained and validated on\nthree large public datasets and externally evaluated zero-shot on additional\nprivate interventional and public CMR datasets, TetHeart achieves\nstate-of-the-art accuracy and strong generalization in both pre- and\nintra-procedural settings.", "published": "2025-09-15 16:17:45", "link": "http://arxiv.org/abs/2509.12090v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Progressive Flow-inspired Unfolding for Spectral Compressive Imaging", "abstract": "Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral\nimage (HSI) from a single 2D compressed measurement, which is a highly\nchallenging reconstruction task. Recent deep unfolding networks (DUNs),\nempowered by explicit data-fidelity updates and implicit deep denoisers, have\nachieved the state of the art in CASSI reconstruction. However, existing\nunfolding approaches suffer from uncontrollable reconstruction trajectories,\nleading to abrupt quality jumps and non-gradual refinement across stages.\nInspired by diffusion trajectories and flow matching, we propose a novel\ntrajectory-controllable unfolding framework that enforces smooth, continuous\noptimization paths from noisy initial estimates to high-quality\nreconstructions. To achieve computational efficiency, we design an efficient\nspatial-spectral Transformer tailored for hyperspectral reconstruction, along\nwith a frequency-domain fusion module to gurantee feature consistency.\nExperiments on simulation and real data demonstrate that our method achieves\nbetter reconstruction quality and efficiency than prior state-of-the-art\napproaches.", "published": "2025-09-15 16:10:50", "link": "http://arxiv.org/abs/2509.12079v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data", "abstract": "The fine-grained surface reconstruction of different organs from 3D medical\nimaging can provide advanced diagnostic support and improved surgical planning.\nHowever, the representation of the organs is often limited by the resolution,\nwith a detailed higher resolution requiring more memory and computing\nfootprint. Implicit representations of objects have been proposed to alleviate\nthis problem in general computer vision by providing compact and differentiable\nfunctions to represent the 3D object shapes. However, architectural and\ndata-related differences prevent the direct application of these methods to\nmedical images. This work introduces ImplMORe, an end-to-end deep learning\nmethod using implicit surface representations for multi-organ reconstruction\nfrom 3D medical images. ImplMORe incorporates local features using a 3D CNN\nencoder and performs multi-scale interpolation to learn the features in the\ncontinuous domain using occupancy functions. We apply our method for single and\nmultiple organ reconstructions using the totalsegmentator dataset. By\nleveraging the continuous nature of occupancy functions, our approach\noutperforms the discrete explicit representation based surface reconstruction\napproaches, providing fine-grained surface details of the organ at a resolution\nhigher than the given input image. The source code will be made publicly\navailable at: https://github.com/CAMMA-public/ImplMORe", "published": "2025-09-15 15:52:20", "link": "http://arxiv.org/abs/2509.12068v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation", "abstract": "Fetal motion is a critical indicator of neurological development and\nintrauterine health, yet its quantification remains challenging, particularly\nat earlier gestational ages (GA). Current methods track fetal motion by\npredicting the location of annotated landmarks on 3D echo planar imaging (EPI)\ntime-series, primarily in third-trimester fetuses. The predicted landmarks\nenable simplification of the fetal body for downstream analysis. While these\nmethods perform well within their training age distribution, they consistently\nfail to generalize to early GAs due to significant anatomical changes in both\nmother and fetus across gestation, as well as the difficulty of obtaining\nannotated early GA EPI data. In this work, we develop a cross-population data\naugmentation framework that enables pose estimation models to robustly\ngeneralize to younger GA clinical cohorts using only annotated images from\nolder GA cohorts. Specifically, we introduce a fetal-specific augmentation\nstrategy that simulates the distinct intrauterine environment and fetal\npositioning of early GAs. Our experiments find that cross-population\naugmentation yields reduced variability and significant improvements across\nboth older GA and challenging early GA cases. By enabling more reliable pose\nestimation across gestation, our work potentially facilitates early clinical\ndetection and intervention in challenging 4D fetal imaging settings. Code is\navailable at https://github.com/sebodiaz/cross-population-pose.", "published": "2025-09-15 15:42:28", "link": "http://arxiv.org/abs/2509.12062v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective", "abstract": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.", "published": "2025-09-15 15:34:02", "link": "http://arxiv.org/abs/2509.12052v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration", "abstract": "This work presents Robust Representation Learning via Adaptive Mask (RAM++),\na two-stage framework for all-in-one image restoration. RAM++ integrates\nhigh-level semantic understanding with low-level texture generation to achieve\ncontent-oriented robust restoration. It addresses the limitations of existing\ndegradation-oriented methods in extreme scenarios (e.g., degradations strongly\ncoupled with image structures). RAM++ also mitigates common challenges such as\nunbalanced performance across tasks, overfitting to seen degradations, and weak\ngeneralization to unseen ones through three key designs: 1) Adaptive\nSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level\nmasks to semantically rich and textured regions. This design enables the\nnetwork to learn both generative priors and image content priors from various\ndegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning\nstrategy that adjusts the layers with higher contributions to bridge the\nintegrity gap between masked pretraining and full-image fine-tuning while\nretaining learned priors. 3) Robust Feature Regularization (RFR): a strategy\nthat leverages DINOv2's semantically consistent and degradation-invariant\nrepresentations, together with efficient feature fusion, to achieve faithful\nand semantically coherent restoration. With these designs, RAM++ achieves\nrobust, well-balanced, and state-of-the-art performance across seen, unseen,\nextreme, and mixed degradations. Our code and model will be released at\nhttps://github.com/DragonisCV/RAM", "published": "2025-09-15 15:24:15", "link": "http://arxiv.org/abs/2509.12039v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness", "abstract": "Diffusion models have achieved unprecedented success in image generation but\npose increasing risks in terms of privacy, fairness, and security. A growing\ndemand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW\ncontent, private individuals, artistic styles) from these models while\npreserving their overall generative capabilities. We introduce \\textbf{SCORE}\n(Secure and Concept-Oriented Robust Erasure), a novel framework for robust\nconcept removal in diffusion models. SCORE formulates concept erasure as an\n\\emph{adversarial independence} problem, theoretically guaranteeing that the\nmodel's outputs become statistically independent of the erased concept. Unlike\nprior heuristic methods, SCORE minimizes the mutual information between a\ntarget concept and generated outputs, yielding provable erasure guarantees. We\nprovide formal proofs establishing convergence properties and derive upper\nbounds on residual concept leakage. Empirically, we evaluate SCORE on Stable\nDiffusion and FLUX across four challenging benchmarks: object erasure, NSFW\nremoval, celebrity face suppression, and artistic style unlearning. SCORE\nconsistently outperforms state-of-the-art methods including EraseAnything, ANT,\nMACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy\nwhile maintaining comparable or superior image quality. By integrating\nadversarial optimization, trajectory consistency, and saliency-driven\nfine-tuning, SCORE sets a new standard for secure and robust concept erasure in\ndiffusion models.", "published": "2025-09-15 15:05:50", "link": "http://arxiv.org/abs/2509.12024v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning", "abstract": "A healthy smile plays a significant role in functional as well as esthetic\nconsiderations, improving confidence. It is difficult for dental professionals\nto strike a balance between esthetic requirements and functional requirements.\nTraditional smile design has had heavy reliance on dentist expertise and used\nplaster models and hand drawings, raising questions about the outcome for\npatients. Digital technology, led by Dr. Christian Coachman in 2007, allows\nphotographic and videographic assessments, enabling improved intercommunication\namong specialists and patients. Advances in artificial intelligence (AI) and\nbig data have supported analysis of facial features and development of\npersonalized smile designs in the last few years. Outputs are, however,\nsusceptible to practitioner bias or limitations of training data, and may be\nsuboptimal for individual users. The study presented here suggests a\ncomprehensive system integrating AI, big data, and recognition technologies to\nautomate the smile design process so that both experienced and inexperienced\ndentists can generate pleasing aesthetics with ease. The system has a Facial\nFeature Extraction Module and an Image Generation Module, serving diverse\npractitioner and patient needs. User data can be incorporated in future\nresearch for design optimization and testing of virtual and augmented reality\nfor real-time previewing. Data gathered can also be employed in aesthetic\npreference analyses, which can enhance our knowledge of smile design in dental\npractice.", "published": "2025-09-15 14:49:27", "link": "http://arxiv.org/abs/2509.12001v1", "categories": ["eess.IV", "cs.CV", "I.2.6; I.2.10; J.3"], "primary_category": "eess.IV"}
{"title": "Learning to Generate 4D LiDAR Sequences", "abstract": "While generative world models have advanced video and occupancy-based data\nsynthesis, LiDAR generation remains underexplored despite its importance for\naccurate 3D perception. Extending generation to 4D LiDAR data introduces\nchallenges in controllability, temporal stability, and evaluation. We present\nLiDARCrafter, a unified framework that converts free-form language into\neditable LiDAR sequences. Instructions are parsed into ego-centric scene\ngraphs, which a tri-branch diffusion model transforms into object layouts,\ntrajectories, and shapes. A range-image diffusion model generates the initial\nscan, and an autoregressive module extends it into a temporally coherent\nsequence. The explicit layout design further supports object-level editing,\nsuch as insertion or relocation. To enable fair assessment, we provide\nEvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On\nnuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and\ntemporal consistency, offering a foundation for LiDAR-based simulation and data\naugmentation.", "published": "2025-09-15 14:14:48", "link": "http://arxiv.org/abs/2509.11959v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation", "abstract": "Accurate land cover classification from satellite imagery is crucial in\nenvironmental monitoring and sustainable resource management. However, it\nremains challenging due to the complexity of natural landscapes, the visual\nsimilarity between classes, and the significant class imbalance in the\navailable datasets. To address these issues, we propose a dual encoder\narchitecture that independently extracts modality-specific features from\noptical and Synthetic Aperture Radar (SAR) imagery, which are then fused using\na cross-modality attention-fusion module named Cross-modality Land cover\nsegmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations\n(CLAIRE). This fusion mechanism highlights complementary spatial and textural\nfeatures, enabling the network to better capture detailed and diverse land\ncover patterns. We incorporate a hybrid loss function that utilizes Weighted\nFocal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address\nclass imbalance and improve segmentation performance across underrepresented\ncategories. Our model achieves competitive performance across multiple\nbenchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall\nAccuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with\na mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and\nremarkable robustness under cloud-obstructed conditions, achieving an mIoU of\n86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce\na metric-driven reasoning module generated by a Small Language Model (Phi-3),\nwhich generates expert-level, sample-specific justifications for model\npredictions, thereby enhancing transparency and interpretability.", "published": "2025-09-15 14:10:52", "link": "http://arxiv.org/abs/2509.11952v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360\u00b0 Videos", "abstract": "The recent success of immersive applications is pushing the research\ncommunity to define new approaches to process 360{\\deg} images and videos and\noptimize their transmission. Among these, saliency estimation provides a\npowerful tool that can be used to identify visually relevant areas and,\nconsequently, adapt processing algorithms. Although saliency estimation has\nbeen widely investigated for 2D content, very few algorithms have been proposed\nfor 360{\\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,\na saliency detection model for 360{\\deg} videos that leverages a Generative\nAdversarial Network with spherical convolutions. Extensive experiments were\nconducted using a public 360{\\deg} video saliency dataset, and the results\ndemonstrate that Sphere-GAN outperforms state-of-the-art models in accurately\npredicting saliency maps.", "published": "2025-09-15 14:07:33", "link": "http://arxiv.org/abs/2509.11948v1", "categories": ["cs.CV", "cs.MM", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization", "abstract": "Conventional deep neural nets (DNNs) initialize network parameters at random\nand then optimize each one via stochastic gradient descent (SGD), resulting in\nsubstantial risk of poor-performing local minima.Focusing on the image\ninterpolation problem and leveraging a recent theorem that maps a\n(pseudo-)linear interpolator {\\Theta} to a directed graph filter that is a\nsolution to a MAP problem regularized with a graph shift variation (GSV) prior,\nwe first initialize a directed graph adjacency matrix A based on a known\ninterpolator {\\Theta}, establishing a baseline performance.Then, towards\nfurther gain, we learn perturbation matrices P and P(2) from data to augment A,\nwhose restoration effects are implemented via Douglas-Rachford (DR) iterations,\nwhich we unroll into a lightweight interpretable neural net.Experimental\nresults demonstrate state-of-the-art image interpolation results, while\ndrastically reducing network parameters.", "published": "2025-09-15 13:43:55", "link": "http://arxiv.org/abs/2509.11926v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI", "abstract": "Multimodal learning has attracted much attention in recent years due to its\nability to effectively utilize data features from a variety of different\nmodalities. Diagnosing the vulnerability of atherosclerotic plaques directly\nfrom carotid 3D MRI images is relatively challenging for both radiologists and\nconventional 3D vision networks. In clinical practice, radiologists assess\npatient conditions using a multimodal approach that incorporates various\nimaging modalities and domain-specific expertise, paving the way for the\ncreation of multimodal diagnostic networks. In this paper, we have developed an\neffective strategy to leverage radiologists' domain knowledge to automate the\ndiagnosis of carotid plaque vulnerability through Variation inference and\nMultimodal knowledge Distillation (VMD). This method excels in harnessing\ncross-modality prior knowledge from limited image annotations and radiology\nreports within training data, thereby enhancing the diagnostic network's\naccuracy for unannotated 3D MRI images. We conducted in-depth experiments on\nthe dataset collected in-house and verified the effectiveness of the VMD\nstrategy we proposed.", "published": "2025-09-15 13:38:35", "link": "http://arxiv.org/abs/2509.11924v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition", "abstract": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity.", "published": "2025-09-15 13:33:54", "link": "http://arxiv.org/abs/2509.11916v1", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "primary_category": "cs.CV"}
{"title": "Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection", "abstract": "The ability to detect out-of-distribution data is essential not only for\nensuring robustness against unknown or unexpected input data but also for\nimproving the generalization performance of the model. Among various\nout-of-distribution detection methods, Outlier Exposure and Mixture Outlier\nExposure are promising approaches that enhance out-of-distribution detection\nperformance by exposing the outlier data during training. However, even with\nthese sophisticated techniques, it remains challenging for models to learn the\nrelationships between classes effectively and to distinguish data sampling from\nin-distribution and out-of-distribution clearly. Therefore, we focus on the\nlogit space, where the properties between class-wise distributions are\ndistinctly separated from those in the input or feature spaces. Specifically,\nwe propose a linear interpolation technique in the logit space that mixes\nin-distribution and out-of-distribution data to facilitate smoothing logits\nbetween classes and improve the out-of-distribution detection performance,\nparticularly for out-of-distribution data that lie close to the in-distribution\ndata. Additionally, we enforce consistency between the logits obtained through\nmixing in the logit space and those generated via mixing in the input space.\nOur experiments demonstrate that our logit-space mixing technique reduces the\nabrupt fluctuations in the model outputs near the decision boundaries,\nresulting in smoother and more reliable separation between in-distribution and\nout-of-distribution data. Furthermore, we evaluate the effectiveness of the\nproposed method on a fine-grained out-of-distribution detection task.", "published": "2025-09-15 13:08:02", "link": "http://arxiv.org/abs/2509.11892v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation", "abstract": "Monocular depth estimation in bronchoscopy can significantly improve\nreal-time navigation accuracy and enhance the safety of interventions in\ncomplex, branching airways. Recent advances in depth foundation models have\nshown promise for endoscopic scenarios, yet these models often lack anatomical\nawareness in bronchoscopy, overfitting to local textures rather than capturing\nthe global airway structure, particularly under ambiguous depth cues and poor\nlighting. To address this, we propose Brea-Depth, a novel framework that\nintegrates airway-specific geometric priors into foundation model adaptation\nfor bronchoscopic depth estimation. Our method introduces a depth-aware\nCycleGAN, refining the translation between real bronchoscopic images and airway\ngeometries from anatomical data, effectively bridging the domain gap. In\naddition, we introduce an airway structure awareness loss to enforce depth\nconsistency within the airway lumen while preserving smooth transitions and\nstructural integrity. By incorporating anatomical priors, Brea-Depth enhances\nmodel generalization and yields more robust, accurate 3D airway\nreconstructions. To assess anatomical realism, we introduce Airway Depth\nStructure Evaluation, a new metric for structural consistency. We validate\nBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic\ndataset, where it outperforms existing methods in anatomical depth\npreservation.", "published": "2025-09-15 13:02:42", "link": "http://arxiv.org/abs/2509.11885v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection", "abstract": "This paper introduces a new Segment Anything Model (SAM) that leverages\nreverse parameter configuration and test-time training to enhance its\nperformance on Camouflaged Object Detection (COD), named SAM-TTT. While most\nexisting SAM-based COD models primarily focus on enhancing SAM by extracting\nfavorable features and amplifying its advantageous parameters, a crucial gap is\nidentified: insufficient attention to adverse parameters that impair SAM's\nsemantic understanding in downstream tasks. To tackle this issue, the Reverse\nSAM Parameter Configuration Module is proposed to effectively mitigate the\ninfluence of adverse parameters in a train-free manner by configuring SAM's\nparameters. Building on this foundation, the T-Visioner Module is unveiled to\nstrengthen advantageous parameters by integrating Test-Time Training layers,\noriginally developed for language tasks, into vision tasks. Test-Time Training\nlayers represent a new class of sequence modeling layers characterized by\nlinear complexity and an expressive hidden state. By integrating two modules,\nSAM-TTT simultaneously suppresses adverse parameters while reinforcing\nadvantageous ones, significantly improving SAM's semantic understanding in COD\ntask. Our experimental results on various COD benchmarks demonstrate that the\nproposed approach achieves state-of-the-art performance, setting a new\nbenchmark in the field. The code will be available at\nhttps://github.com/guobaoxiao/SAM-TTT.", "published": "2025-09-15 13:02:27", "link": "http://arxiv.org/abs/2509.11884v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation", "abstract": "Poetry is an expressive form of art that invites multiple interpretations, as\nreaders often bring their own emotions, experiences, and cultural backgrounds\ninto their understanding of a poem. Recognizing this, we aim to generate images\nfor poems and improve these images in a zero-shot setting, enabling audiences\nto modify images as per their requirements. To achieve this, we introduce a\nnovel Weighted Prompt Manipulation (WPM) technique, which systematically\nmodifies attention weights and text embeddings within diffusion models. By\ndynamically adjusting the importance of specific words, WPM enhances or\nsuppresses their influence in the final generated image, leading to\nsemantically richer and more contextually accurate visualizations. Our approach\nexploits diffusion models and large language models (LLMs) such as GPT in\nconjunction with existing poetry datasets, ensuring a comprehensive and\nstructured methodology for improved image generation in the literary domain. To\nthe best of our knowledge, this is the first attempt at integrating weighted\nprompt manipulation for enhancing imagery in poetic language.", "published": "2025-09-15 12:58:38", "link": "http://arxiv.org/abs/2509.11878v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods", "abstract": "Precision livestock farming requires advanced monitoring tools to meet the\nincreasing management needs of the industry. Computer vision systems capable of\nlong-term multi-animal tracking (MAT) are essential for continuous behavioral\nmonitoring in livestock production. MAT, a specialized subset of multi-object\ntracking (MOT), shares many challenges with MOT, but also faces domain-specific\nissues including frequent animal occlusion, highly similar appearances among\nanimals, erratic motion patterns, and a wide range of behavior types.\n  While some existing MAT tools are user-friendly and widely adopted, they\noften underperform compared to state-of-the-art MOT methods, which can result\nin inaccurate downstream tasks such as behavior analysis, health state\nestimation, and related applications. In this study, we benchmarked both MAT\nand MOT approaches for long-term tracking of pigs. We compared tools such as\nDeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,\ncross-input consistency, and newer approaches like Track-Anything and\nPromptTrack.\n  All methods were evaluated on a 10-minute pig tracking dataset. Our results\ndemonstrate that, overall, MOT approaches outperform traditional MAT tools,\neven for long-term tracking scenarios. These findings highlight the potential\nof recent MOT techniques to enhance the accuracy and reliability of automated\nlivestock tracking.", "published": "2025-09-15 12:52:31", "link": "http://arxiv.org/abs/2509.11873v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding", "abstract": "Recent advancements in large video models (LVMs) have significantly enhance\nvideo understanding. However, these models continue to suffer from\nhallucinations, producing content that conflicts with input videos. To address\nthis issue, we propose Dr.V, a hierarchical framework covering perceptive,\ntemporal, and cognitive levels to diagnose video hallucination by fine-grained\nspatial-temporal grounding. Dr.V comprises of two key components: a benchmark\ndataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes\n10k instances drawn from 4,974 videos spanning diverse tasks, each enriched\nwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in\nLVMs by systematically applying fine-grained spatial-temporal grounding at the\nperceptive and temporal levels, followed by cognitive level reasoning. This\nstep-by-step pipeline mirrors human-like video comprehension and effectively\nidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is\neffective in diagnosing hallucination while enhancing interpretability and\nreliability, offering a practical blueprint for robust video understanding in\nreal-world scenarios. All our data and code are available at\nhttps://github.com/Eurekaleo/Dr.V.", "published": "2025-09-15 12:39:19", "link": "http://arxiv.org/abs/2509.11866v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting", "abstract": "Sparse-view synthesis remains a challenging problem due to the difficulty of\nrecovering accurate geometry and appearance from limited observations. While\nrecent advances in 3D Gaussian Splatting (3DGS) have enabled real-time\nrendering with competitive quality, existing pipelines often rely on\nStructure-from-Motion (SfM) for camera pose estimation, an approach that\nstruggles in genuinely sparse-view settings. Moreover, several SfM-free methods\nreplace SfM with multi-view stereo (MVS) models, but generate massive numbers\nof 3D Gaussians by back-projecting every pixel into 3D space, leading to high\nmemory costs. We propose Segmentation-Driven Initialization for Gaussian\nSplatting (SDI-GS), a method that mitigates inefficiency by leveraging\nregion-based segmentation to identify and retain only structurally significant\nregions. This enables selective downsampling of the dense point cloud,\npreserving scene fidelity while substantially reducing Gaussian count.\nExperiments across diverse benchmarks show that SDI-GS reduces Gaussian count\nby up to 50% and achieves comparable or superior rendering quality in PSNR and\nSSIM, with only marginal degradation in LPIPS. It further enables faster\ntraining and lower memory footprint, advancing the practicality of 3DGS for\nconstrained-view scenarios.", "published": "2025-09-15 12:31:33", "link": "http://arxiv.org/abs/2509.11853v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation", "abstract": "Generative vision-language models (VLMs) exhibit strong high-level image\nunderstanding but lack spatially dense alignment between vision and language\nmodalities, as our findings indicate. Orthogonal to advancements in generative\nVLMs, another line of research has focused on representation learning for\nvision-language alignment, targeting zero-shot inference for dense tasks like\nsegmentation. In this work, we bridge these two directions by densely aligning\nimages with synthetic descriptions generated by VLMs. Synthetic captions are\ninexpensive, scalable, and easy to generate, making them an excellent source of\nhigh-level semantic understanding for dense alignment methods. Empirically, our\napproach outperforms prior work on standard zero-shot open-vocabulary\nsegmentation benchmarks/datasets, while also being more data-efficient.", "published": "2025-09-15 12:26:47", "link": "http://arxiv.org/abs/2509.11840v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning", "abstract": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory. For more details, please\nrefer to https://jiachengliu3.github.io/TrajBooster.", "published": "2025-09-15 12:25:39", "link": "http://arxiv.org/abs/2509.11839v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "FedDAF: Federated Domain Adaptation Using Model Functional Distance", "abstract": "Federated Domain Adaptation (FDA) is a federated learning (FL) approach that\nimproves model performance at the target client by collaborating with source\nclients while preserving data privacy. FDA faces two primary challenges: domain\nshifts between source and target data and limited labeled data at the target.\nMost existing FDA methods focus on domain shifts, assuming ample target data,\nyet often neglect the combined challenges of both domain shifts and data\nscarcity. Moreover, approaches that address both challenges fail to prioritize\nsharing relevant information from source clients according to the target's\nobjective. In this paper, we propose FedDAF, a novel approach addressing both\nchallenges in FDA. FedDAF uses similarity-based aggregation of the global\nsource model and target model by calculating model functional distance from\ntheir mean gradient fields computed on target data. This enables effective\nmodel aggregation based on the target objective, constructed using target data,\neven with limited data. While computing model functional distance between these\ntwo models, FedDAF computes the angle between their mean gradient fields and\nthen normalizes with the Gompertz function. To construct the global source\nmodel, all the local source models are aggregated using simple average in the\nserver. Experiments on real-world datasets demonstrate FedDAF's superiority\nover existing FL, PFL, and FDA methods in terms of achieving better test\naccuracy.", "published": "2025-09-15 12:03:38", "link": "http://arxiv.org/abs/2509.11819v1", "categories": ["cs.LG", "cs.CV", "68W15, 68T05, 90C25", "I.2.6; I.5.1; C.2.4"], "primary_category": "cs.LG"}
{"title": "MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation", "abstract": "Infrared-visible image fusion methods aim at generating fused images with\ngood visual quality and also facilitate the performance of high-level tasks.\nIndeed, existing semantic-driven methods have considered semantic information\ninjection for downstream applications. However, none of them investigates the\npotential for reciprocal promotion between pixel-wise image fusion and\ncross-modal feature fusion perception tasks from a macroscopic task-level\nperspective. To address this limitation, we propose a unified network for image\nfusion and semantic segmentation. MAFS is a parallel structure, containing a\nfusion sub-network and a segmentation sub-network. On the one hand, We devise a\nheterogeneous feature fusion strategy to enhance semantic-aware capabilities\nfor image fusion. On the other hand, by cascading the fusion sub-network and a\nsegmentation backbone, segmentation-related knowledge is transferred to promote\nfeature-level fusion-based segmentation. Within the framework, we design a\nnovel multi-stage Transformer decoder to aggregate fine-grained multi-scale\nfused features efficiently. Additionally, a dynamic factor based on the max-min\nfairness allocation principle is introduced to generate adaptive weights of two\ntasks and guarantee smooth training in a multi-task manner. Extensive\nexperiments demonstrate that our approach achieves competitive results compared\nwith state-of-the-art methods. The code is available at\nhttps://github.com/Abraham-Einstein/MAFS/.", "published": "2025-09-15 11:55:55", "link": "http://arxiv.org/abs/2509.11817v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio", "abstract": "Retinal vessel segmentation is critical for the early diagnosis of\nvision-threatening and systemic diseases, especially in real-world clinical\nsettings with limited computational resources. Although significant\nimprovements have been made in deep learning-based segmentation methods,\ncurrent models still face challenges in extracting tiny vessels and suffer from\nhigh computational costs. In this study, we present LFRA-Net by incorporating\nfocal modulation attention at the encoder-decoder bottleneck and region-aware\nattention in the selective skip connections. LFRA-Net is a lightweight network\noptimized for precise and effective retinal vascular segmentation. It enhances\nfeature representation and regional focus by efficiently capturing local and\nglobal dependencies. LFRA-Net outperformed many state-of-the-art models while\nmaintaining lightweight characteristics with only 0.17 million parameters, 0.66\nMB memory size, and 10.50 GFLOPs. We validated it on three publicly available\ndatasets: DRIVE, STARE, and CHASE\\_DB. It performed better in terms of Dice\nscore (84.28\\%, 88.44\\%, and 85.50\\%) and Jaccard index (72.86\\%, 79.31\\%, and\n74.70\\%) on the DRIVE, STARE, and CHASE\\_DB datasets, respectively. LFRA-Net\nprovides an ideal ratio between segmentation accuracy and computational cost\ncompared to existing deep learning methods, which makes it suitable for\nreal-time clinical applications in areas with limited resources. The code can\nbe found at https://github.com/Mehwish4593/LFRA-Net.", "published": "2025-09-15 11:47:51", "link": "http://arxiv.org/abs/2509.11811v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics", "abstract": "Computer-aided diagnosis systems must make critical decisions from medical\nimages that are often noisy, ambiguous, or conflicting, yet today's models are\ntrained on overly simplistic labels that ignore diagnostic uncertainty. One-hot\nlabels erase inter-rater variability and force models to make overconfident\npredictions, especially when faced with incomplete or artifact-laden inputs. We\naddress this gap by introducing a novel framework that brings uncertainty back\ninto the label space. Our method leverages neural network training dynamics\n(NNTD) to assess the inherent difficulty of each training sample. By\naggregating and calibrating model predictions during training, we generate\nuncertainty-aware pseudo-labels that reflect the ambiguity encountered during\nlearning. This label augmentation approach is architecture-agnostic and can be\napplied to any supervised learning pipeline to enhance uncertainty estimation\nand robustness. We validate our approach on a challenging echocardiography\nclassification benchmark, demonstrating superior performance over specialized\nbaselines in calibration, selective classification, and multi-view fusion.", "published": "2025-09-15 11:30:12", "link": "http://arxiv.org/abs/2509.11800v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning", "abstract": "Video Question Answering (VideoQA) based on Large Language Models (LLMs) has\nshown potential in general video understanding but faces significant challenges\nwhen applied to the inherently complex domain of sports videos. In this work,\nwe propose FineQuest, the first training-free framework that leverages\ndual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for\nstraightforward sports queries and ii) Deliberative Reasoning for more complex\nones. To bridge the knowledge gap between general-purpose models and\ndomain-specific sports understanding, FineQuest incorporates SSGraph, a\nmultimodal sports knowledge scene graph spanning nine sports, which encodes\nboth visual instances and domain-specific terminology to enhance reasoning\naccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA\nand Diving-QA, derived from the FineGym and FineDiving datasets, enabling\ndiverse and comprehensive evaluation. FineQuest achieves state-of-the-art\nperformance on these benchmarks as well as the existing SPORTU dataset, while\nmaintains strong general VideoQA capabilities.", "published": "2025-09-15 11:27:23", "link": "http://arxiv.org/abs/2509.11796v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation", "abstract": "Retinal vessel segmentation is essential for early diagnosis of diseases such\nas diabetic retinopathy, hypertension, and neurodegenerative disorders.\nAlthough SA-UNet introduces spatial attention in the bottleneck, it underuses\nattention in skip connections and does not address the severe\nforeground-background imbalance. We propose SA-UNetv2, a lightweight model that\ninjects cross-scale spatial attention into all skip connections to strengthen\nmulti-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)\nplus Matthews Correlation Coefficient (MCC) loss to improve robustness to class\nimbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves\nstate-of-the-art performance with only 1.2MB memory and 0.26M parameters (less\nthan 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,\ndemonstrating strong efficiency and deployability in resource-constrained,\nCPU-only settings.", "published": "2025-09-15 10:53:28", "link": "http://arxiv.org/abs/2509.11774v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization", "abstract": "Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to\noperate reliably in dynamic environments. MOT ensures consistent object\nidentity assignment and precise spatial delineation. Recent advances in\nfoundation models, such as SAM2, have demonstrated strong zero-shot\ngeneralization for video segmentation, but their direct application to MOTS\n(MOT+Segmentation) remains limited by insufficient identity management and\nmemory efficiency. This work introduces Seg2Track-SAM2, a framework that\nintegrates pre-trained object detectors with SAM2 and a novel Seg2Track module\nto address track initialization, track management, and reinforcement. The\nproposed approach requires no fine-tuning and remains detector-agnostic.\nExperimental results on KITTI MOT and KITTI MOTS benchmarks show that\nSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth\noverall in both car and pedestrian classes on KITTI MOTS, while establishing a\nnew benchmark in association accuracy (AssA). Furthermore, a sliding-window\nmemory strategy reduces memory usage by up to 75% with negligible performance\ndegradation, supporting deployment under resource constraints. These results\nconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot\ntracking, enhanced identity preservation, and efficient memory utilization. The\ncode is available at https://github.com/hcmr-lab/Seg2Track-SAM2", "published": "2025-09-15 10:52:27", "link": "http://arxiv.org/abs/2509.11772v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images", "abstract": "Reconstructing 3D face from a single unconstrained image remains a\nchallenging problem due to diverse conditions in unconstrained environments.\nRecently, learning-based methods have achieved notable results by effectively\ncapturing complex facial structures and details across varying conditions.\nConsequently, many existing approaches employ projection-based losses between\ngenerated and input images to constrain model training. However, learning-based\nmethods for 3D face reconstruction typically require substantial amounts of 3D\nfacial data, which is difficult and costly to obtain. Consequently, to reduce\nreliance on labeled 3D face datasets, many existing approaches employ\nprojection-based losses between generated and input images to constrain model\ntraining. Nonetheless, despite these advancements, existing approaches\nfrequently struggle to capture detailed and multi-scale features under diverse\nfacial attributes and conditions, leading to incomplete or less accurate\nreconstructions. In this paper, we propose a Multi-Scale Feature Fusion with\nMulti-Attribute (MSMA) framework for 3D face reconstruction from unconstrained\nimages. Our method integrates multi-scale feature fusion with a focus on\nmulti-attribute learning and leverages a large-kernel attention module to\nenhance the precision of feature extraction across scales, enabling accurate 3D\nfacial parameter estimation from a single 2D image. Comprehensive experiments\non the MICC Florence, Facewarehouse and custom-collect datasets demonstrate\nthat our approach achieves results on par with current state-of-the-art\nmethods, and in some instances, surpasses SOTA performance across challenging\nconditions.", "published": "2025-09-15 10:30:08", "link": "http://arxiv.org/abs/2509.11763v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications", "abstract": "Artificial intelligence (AI) that can effectively learn ultrasound\nrepresentations by integrating multi-source data holds significant promise for\nadvancing clinical care. However, the scarcity of large labeled datasets in\nreal-world clinical environments and the limited generalizability of\ntask-specific models have hindered the development of generalizable clinical AI\nmodels for ultrasound applications. In this study, we present EchoCare, a novel\nultrasound foundation model for generalist clinical use, developed via\nself-supervised learning on our curated, publicly available, large-scale\ndataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,\nsourced from over 23 countries across 5 continents and acquired via a diverse\nrange of distinct imaging devices, thus encompassing global cohorts that are\nmulti-center, multi-device, and multi-ethnic. Unlike prior studies that adopt\noff-the-shelf vision foundation model architectures, we introduce a\nhierarchical classifier into EchoCare to enable joint learning of pixel-level\nand representation-level features, capturing both global anatomical contexts\nand local ultrasound characteristics. With minimal training, EchoCare\noutperforms state-of-the-art comparison models across 10 representative\nultrasound benchmarks of varying diagnostic difficulties, spanning disease\ndiagnosis, lesion segmentation, organ detection, landmark prediction,\nquantitative regression, imaging enhancement and report generation. The code\nand pretrained model are publicly released, rendering EchoCare accessible for\nfine-tuning and local adaptation, supporting extensibility to additional\napplications. EchoCare provides a fully open and generalizable foundation model\nto boost the development of AI technologies for diverse clinical ultrasound\napplications.", "published": "2025-09-15 10:05:31", "link": "http://arxiv.org/abs/2509.11752v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DRAG: Data Reconstruction Attack using Guided Diffusion", "abstract": "With the rise of large foundation models, split inference (SI) has emerged as\na popular computational paradigm for deploying models across lightweight edge\ndevices and cloud servers, addressing data privacy and computational cost\nconcerns. However, most existing data reconstruction attacks have focused on\nsmaller CNN classification models, leaving the privacy risks of foundation\nmodels in SI settings largely unexplored. To address this gap, we propose a\nnovel data reconstruction attack based on guided diffusion, which leverages the\nrich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on\na large-scale dataset. Our method performs iterative reconstruction on the\nLDM's learned image prior, effectively generating high-fidelity images\nresembling the original data from their intermediate representations (IR).\nExtensive experiments demonstrate that our approach significantly outperforms\nstate-of-the-art methods, both qualitatively and quantitatively, in\nreconstructing data from deep-layer IRs of the vision foundation model. The\nresults highlight the urgent need for more robust privacy protection mechanisms\nfor large models in SI scenarios. Code is available at:\nhttps://github.com/ntuaislab/DRAG.", "published": "2025-09-15 09:26:19", "link": "http://arxiv.org/abs/2509.11724v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Advanced Layout Analysis Models for Docling", "abstract": "This technical report documents the development of novel Layout Analysis\nmodels integrated into the Docling document-conversion pipeline. We trained\nseveral state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and\nDFINE architectures on a heterogeneous corpus of 150,000 documents (both openly\navailable and proprietary). Post-processing steps were applied to the raw\ndetections to make them more applicable to the document conversion task. We\nevaluated the effectiveness of the layout analysis on various document\nbenchmarks using different methodologies while also measuring the runtime\nperformance across different environments (CPU, Nvidia and Apple GPUs). We\nintroduce five new document layout models achieving 20.6% - 23.9% mAP\nimprovement over Docling's previous baseline, with comparable or better\nruntime. Our best model, \"heron-101\", attains 78% mAP with 28 ms/image\ninference time on a single NVIDIA A100 GPU. Extensive quantitative and\nqualitative experiments establish best practices for training, evaluating, and\ndeploying document-layout detectors, providing actionable guidance for the\ndocument conversion community. All trained checkpoints, code, and documentation\nare released under a permissive license on HuggingFace.", "published": "2025-09-15 09:20:11", "link": "http://arxiv.org/abs/2509.11720v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "The Quest for Universal Master Key Filters in DS-CNNs", "abstract": "A recent study has proposed the \"Master Key Filters Hypothesis\" for\nconvolutional neural network filters. This paper extends this hypothesis by\nradically constraining its scope to a single set of just 8 universal filters\nthat depthwise separable convolutional networks inherently converge to. While\nconventional DS-CNNs employ thousands of distinct trained filters, our analysis\nreveals these filters are predominantly linear shifts (ax+b) of our discovered\nuniversal set. Through systematic unsupervised search, we extracted these\nfundamental patterns across different architectures and datasets. Remarkably,\nnetworks initialized with these 8 unique frozen filters achieve over 80%\nImageNet accuracy, and even outperform models with thousands of trainable\nparameters when applied to smaller datasets. The identified master key filters\nclosely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,\nstructures that are not only fundamental to classical image processing but also\nstrikingly similar to receptive fields in mammalian visual systems. Our\nfindings provide compelling evidence that depthwise convolutional layers\nnaturally gravitate toward this fundamental set of spatial operators regardless\nof task or architecture. This work offers new insights for understanding\ngeneralization and transfer learning through the universal language of these\nmaster key filters.", "published": "2025-09-15 09:10:13", "link": "http://arxiv.org/abs/2509.11711v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation", "abstract": "Uncertainty estimation is critical for reliable medical image segmentation,\nparticularly in retinal vessel analysis, where accurate predictions are\nessential for diagnostic applications. Deep Ensembles, where multiple networks\nare trained individually, are widely used to improve medical image segmentation\nperformance. However, training and testing costs increase with the number of\nensembles. In this work, we propose Ensemble Distillation as a robust\nalternative to commonly used uncertainty estimation techniques by distilling\nthe knowledge of multiple ensemble models into a single model. Through\nextensive experiments on the DRIVE and FIVES datasets, we demonstrate that\nEnsemble Distillation achieves comparable performance via calibration and\nsegmentation metrics, while significantly reducing computational complexity.\nThese findings suggest that Ensemble distillation provides an efficient and\nreliable approach for uncertainty estimation in the segmentation of the retinal\nvessels, making it a promising tool for medical imaging applications.", "published": "2025-09-15 08:41:19", "link": "http://arxiv.org/abs/2509.11689v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects", "abstract": "Object 6DoF (6D) pose estimation is essential for robotic perception,\nespecially in industrial settings. It enables robots to interact with the\nenvironment and manipulate objects. However, existing benchmarks on object 6D\npose estimation primarily use everyday objects with rich textures and\nlow-reflectivity, limiting model generalization to industrial scenarios where\nobjects are often metallic, texture-less, and highly reflective. To address\nthis gap, we propose a novel dataset and benchmark namely \\textit{Industrial\nMetallic Dataset (IMD)}, tailored for industrial applications. Our dataset\ncomprises 45 true-to-scale industrial components, captured with an RGB-D camera\nunder natural indoor lighting and varied object arrangements to replicate\nreal-world conditions. The benchmark supports three tasks, including video\nobject segmentation, 6D pose tracking, and one-shot 6D pose estimation. We\nevaluate existing state-of-the-art models, including XMem and SAM2 for\nsegmentation, and BundleTrack and BundleSDF for pose estimation, to assess\nmodel performance in industrial contexts. Evaluation results show that our\nindustrial dataset is more challenging than existing household object datasets.\nThis benchmark provides the baseline for developing and comparing segmentation\nand pose estimation algorithms that better generalize to industrial robotics\nscenarios.", "published": "2025-09-15 08:28:15", "link": "http://arxiv.org/abs/2509.11680v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps", "abstract": "Paper maps remain widely used for hiking and sightseeing because they contain\ncurated trails and locally relevant annotations that are often missing from\ndigital navigation applications such as Google Maps. We propose a pipeline to\nextract navigable trails from scanned maps, enabling their use in GPS-based\nnavigation. Our method combines georeferencing, U-Net-based binary\nsegmentation, graph construction, and an iterative refinement procedure using a\nrouting engine. We evaluate the full end-to-end pipeline as well as individual\ncomponents, showing that the approach can robustly recover trail networks from\ndiverse map styles and generate GPS routes suitable for practical use.", "published": "2025-09-15 08:16:12", "link": "http://arxiv.org/abs/2509.11674v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba", "abstract": "OCTA is a crucial non-invasive imaging technique for diagnosing and\nmonitoring retinal diseases like diabetic retinopathy, age-related macular\ndegeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)\nsegmentation offer insufficient accuracy. To address this, we propose RVMamba,\na novel architecture integrating multiple feature extraction modules with the\nMamba state-space model. Moreover, existing joint segmentation models for OCTA\ndata exhibit performance imbalance between different tasks. To simultaneously\nimprove the segmentation of the foveal avascular zone (FAZ) and mitigate this\nimbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.\nExperimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba\noutperforms existing models across evaluation metrics.The code is available at\nhttps://github.com/lc-sfis/Joint-OCTAMamba.", "published": "2025-09-15 07:36:21", "link": "http://arxiv.org/abs/2509.11649v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration", "abstract": "Existing all-in-one image restoration approaches, which aim to handle\nmultiple weather degradations within a single framework, are predominantly\ntrained and evaluated using mixed single-weather synthetic datasets. However,\nthese datasets often differ significantly in resolution, style, and domain\ncharacteristics, leading to substantial domain gaps that hinder the development\nand fair evaluation of unified models. Furthermore, the lack of a large-scale,\nreal-world all-in-one weather restoration dataset remains a critical bottleneck\nin advancing this field. To address these limitations, we present a real-world\nall-in-one adverse weather image restoration benchmark dataset, which contains\nimage pairs captured under various weather conditions, including rain, snow,\nand haze, as well as diverse outdoor scenes and illumination settings. The\nresulting dataset provides precisely aligned degraded and clean images,\nenabling supervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of task-specific, task-general, and\nall-in-one restoration methods on our dataset. Our dataset offers a valuable\nfoundation for advancing robust and practical all-in-one image restoration in\nreal-world scenarios. The dataset has been publicly released and is available\nat https://github.com/guanqiyuan/WeatherBench.", "published": "2025-09-15 07:24:29", "link": "http://arxiv.org/abs/2509.11642v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed", "abstract": "Diffusion models have shown promising results in free-form inpainting. Recent\nstudies based on refined diffusion samplers or novel architectural designs led\nto realistic results and high data consistency. However, random initialization\nseed (noise) adopted in vanilla diffusion process may introduce mismatched\nsemantic information in masked regions, leading to biased inpainting results,\ne.g., low consistency and low coherence with the other unmasked area. To\naddress this issue, we propose the Initial Seed refined Diffusion Model\n(IS-Diff), a completely training-free approach incorporating distributional\nharmonious seeds to produce harmonious results. Specifically, IS-Diff employs\ninitial seeds sampled from unmasked areas to imitate the masked data\ndistribution, thereby setting a promising direction for the diffusion\nprocedure. Moreover, a dynamic selective refinement mechanism is proposed to\ndetect severe unharmonious inpaintings in intermediate latent and adjust the\nstrength of our initialization prior dynamically. We validate our method on\nboth standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,\nand Places2 datasets, demonstrating its effectiveness across all metrics\ncompared to state-of-the-art inpainting methods.", "published": "2025-09-15 07:16:03", "link": "http://arxiv.org/abs/2509.11638v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting", "abstract": "We propose a novel 3D deepfake generation framework based on 3D Gaussian\nSplatting that enables realistic, identity-preserving face swapping and\nreenactment in a fully controllable 3D space. Compared to conventional 2D\ndeepfake approaches that suffer from geometric inconsistencies and limited\ngeneralization to novel view, our method combines a parametric head model with\ndynamic Gaussian representations to support multi-view consistent rendering,\nprecise expression control, and seamless background integration. To address\nediting challenges in point-based representations, we explicitly separate the\nhead and background Gaussians and use pre-trained 2D guidance to optimize the\nfacial region across views. We further introduce a repair module to enhance\nvisual consistency under extreme poses and expressions. Experiments on\nNeRSemble and additional evaluation videos demonstrate that our method achieves\ncomparable performance to state-of-the-art 2D approaches in identity\npreservation, as well as pose and expression consistency, while significantly\noutperforming them in multi-view rendering quality and 3D consistency. Our\napproach bridges the gap between 3D modeling and deepfake synthesis, enabling\nnew directions for scene-aware, controllable, and immersive visual forgeries,\nrevealing the threat that emerging 3D Gaussian Splatting technique could be\nused for manipulation attacks.", "published": "2025-09-15 06:34:17", "link": "http://arxiv.org/abs/2509.11624v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection", "abstract": "Video Anomaly Detection (VAD) is critical for surveillance and public safety.\nHowever, existing benchmarks are limited to either frame-level or video-level\ntasks, restricting a holistic view of model generalization. This work first\nintroduces a softmax-based frame allocation strategy that prioritizes\nanomaly-dense segments while maintaining full-video coverage, enabling balanced\nsampling across temporal scales. Building on this process, we construct two\ncomplementary benchmarks. The image-based benchmark evaluates frame-level\nreasoning with representative frames, while the video-based benchmark extends\nto temporally localized segments and incorporates an abnormality scoring\ntask.Experiments on UCF-Crime demonstrate improvements at both the frame and\nvideo levels, and ablation studies confirm clear advantages of anomaly-focused\nsampling over uniform and random baselines.", "published": "2025-09-15 05:48:22", "link": "http://arxiv.org/abs/2509.11605v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework", "abstract": "Despite the remarkable success of Self-Supervised Learning (SSL), its\ngeneralization is fundamentally hindered by Shortcut Learning, where models\nexploit superficial features like texture instead of intrinsic structure. We\nexperimentally verify this flaw within the generative paradigm (e.g., MAE) and\nargue it is a systemic issue also affecting discriminative methods, identifying\nit as the root cause of their failure on unseen domains. While existing methods\noften tackle this at a surface level by aligning or separating domain-specific\nfeatures, they fail to alter the underlying learning mechanism that fosters\nshortcut dependency. To address this at its core, we propose HyGDL (Hybrid\nGenerative-Discriminative Learning Framework), a hybrid framework that achieves\nexplicit content-style disentanglement. Our approach is guided by the\nInvariance Pre-training Principle: forcing a model to learn an invariant\nessence by systematically varying a bias (e.g., style) at the input while\nkeeping the supervision signal constant. HyGDL operates on a single encoder and\nanalytically defines style as the component of a representation that is\northogonal to its style-invariant content, derived via vector projection.", "published": "2025-09-15 05:28:32", "link": "http://arxiv.org/abs/2509.11598v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment", "abstract": "With the rapid advancement of video generation models such as Sora, video\nquality assessment (VQA) is becoming increasingly crucial for selecting\nhigh-quality videos from large-scale datasets used in pre-training. Traditional\nVQA methods, typically producing single numerical scores, often lack\ncomprehensiveness and interpretability. To address these challenges, we\nintroduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over\n68,000 carefully annotated videos, covering seven essential quality dimensions:\noverall aesthetics, camera movement, dynamic degree, texture detail,\ncomposition, visual quality, and factual consistency. Each annotation includes\ndetailed chain-of-thought reasoning to facilitate interpretability and\ncomprehensive understanding. Extensive experiments demonstrate that MVQA-68K\nsignificantly enhances the performance of various multimodal large language\nmodels (MLLMs) on the VQA task, achieving state-of-the-art results not only on\nour internal test set (Fig.1) but also on public benchmarks including\nLSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning\nprocess during VQA training substantially boosts the zero-shot generalization.\nCode and dataset will be available at github:\nhttps://github.com/Controller01-ai/MVQA-68K", "published": "2025-09-15 05:16:54", "link": "http://arxiv.org/abs/2509.11589v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Optimizing Class Distributions for Bias-Aware Multi-Class Learning", "abstract": "We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an\niterative, data-centric framework that identifies Pareto optimized class\ndistributions for multi-class image classification. BiCDO enables performance\nprioritization for specific classes, which is useful in safety-critical\nscenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,\nBiCDO determines the optimal number of images per class to enhance reliability\nand minimize bias and variance in the objective function. BiCDO can be\nincorporated into existing training pipelines with minimal code changes and\nsupports any labelled multi-class dataset. We have validated BiCDO using\nEfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,\ndemonstrating improved, balanced model performance through optimized data\ndistribution.", "published": "2025-09-15 05:13:21", "link": "http://arxiv.org/abs/2509.11588v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps", "abstract": "While recent Gaussian-based SLAM methods achieve photorealistic\nreconstruction from RGB-D data, their computational performance remains a\ncritical bottleneck. State-of-the-art techniques operate at less than 20 fps,\nsignificantly lagging behind geometry-centric approaches like KinectFusion\n(hundreds of fps). This limitation stems from the heavy computational burden:\nmodeling scenes requires numerous Gaussians and complex iterative optimization\nto fit RGB-D data, where insufficient Gaussian counts or optimization\niterations cause severe quality degradation. To address this, we propose a\nGaussian-SDF hybrid representation, combining a colorized Signed Distance Field\n(SDF) for smooth geometry and appearance with 3D Gaussians to capture\nunderrepresented details. The SDF is efficiently constructed via RGB-D fusion\n(as in geometry-centric methods), while Gaussians undergo iterative\noptimization. Our representation enables drastic Gaussian reduction (50% fewer)\nby avoiding full-scene Gaussian modeling, and efficient Gaussian optimization\n(75% fewer iterations) through targeted appearance refinement. Building upon\nthis representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time\n3D reconstruction system achieving over 150 fps on real-world Azure Kinect\nsequences -- delivering an order-of-magnitude speedup over state-of-the-art\ntechniques while maintaining comparable reconstruction quality. We will release\nthe source code and data to facilitate future research.", "published": "2025-09-15 04:37:32", "link": "http://arxiv.org/abs/2509.11574v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "How Auxiliary Reasoning Unleashes GUI Grounding in VLMs", "abstract": "Graphical user interface (GUI) grounding is a fundamental task for building\nGUI agents. However, general vision-language models (VLMs) struggle with this\ntask due to a lack of specific optimization. We identify a key gap in this\npaper: while VLMs exhibit significant latent grounding potential, as\ndemonstrated by their performance measured by Pointing Game, they underperform\nwhen tasked with outputting explicit coordinates. To address this discrepancy,\nand bypass the high data and annotation costs of current fine-tuning\napproaches, we propose three zero-shot auxiliary reasoning methods. By\nproviding explicit spatial cues such as axes, grids and labeled intersections\nas part of the input image, these methods enable VLMs to articulate their\nimplicit spatial understanding capabilities. We evaluate these methods on four\nGUI grounding benchmarks across seven open-source and proprietary VLMs. The\nevaluation results demonstrate that the proposed methods substantially improve\nthe performance of GUI grounding.", "published": "2025-09-15 03:28:29", "link": "http://arxiv.org/abs/2509.11548v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection", "abstract": "Camouflaged object detection (COD) aims to segment objects that blend into\ntheir surroundings. However, most existing studies overlook the semantic\ndifferences among textual prompts of different targets as well as fine-grained\nfrequency features. In this work, we propose a novel Semantic and Frequency\nGuided Network (SFGNet), which incorporates semantic prompts and\nfrequency-domain features to capture camouflaged objects and improve boundary\nperception. We further design Multi-Band Fourier Module(MBFM) to enhance the\nability of the network in handling complex backgrounds and blurred boundaries.\nIn addition, we design an Interactive Structure Enhancement Block (ISEB) to\nensure structural integrity and boundary details in the predictions. Extensive\nexperiments conducted on three COD benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches. The core code of\nthe model is available at the following link:\nhttps://github.com/winter794444/SFGNetICASSP2026.", "published": "2025-09-15 03:15:31", "link": "http://arxiv.org/abs/2509.11539v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis", "abstract": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.", "published": "2025-09-15 02:31:33", "link": "http://arxiv.org/abs/2509.11526v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Geometric Analysis of Magnetic Labyrinthine Stripe Evolution via U-Net Segmentation", "abstract": "Labyrinthine stripe patterns are common in many physical systems, yet their\nlack of long-range order makes quantitative characterization challenging. We\ninvestigate the evolution of such patterns in bismuth-doped yttrium iron garnet\n(Bi:YIG) films subjected to a magnetic field annealing protocol. A U-Net deep\nlearning model, trained with synthetic degradations including additive white\nGaussian and Simplex noise, enables robust segmentation of experimental\nmagneto-optical images despite noise and occlusions. Building on this\nsegmentation, we develop a geometric analysis pipeline based on\nskeletonization, graph mapping, and spline fitting, which quantifies local\nstripe propagation through length and curvature measurements. Applying this\nframework to 444 images from 12 annealing protocol trials, we analyze the\ntransition from the \"quenched\" state to a more parallel and coherent \"annealed\"\nstate, and identify two distinct evolution modes (Type A and Type B) linked to\nfield polarity. Our results provide a quantitative analysis of geometric and\ntopological properties in magnetic stripe patterns and offer new insights into\ntheir local structural evolution, and establish a general tool for analyzing\ncomplex labyrinthine systems.", "published": "2025-09-15 00:23:23", "link": "http://arxiv.org/abs/2509.11485v1", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm", "abstract": "Decision trees are a ubiquitous model for classification and regression tasks\ndue to their interpretability and efficiency. However, solving the optimal\ndecision tree (ODT) problem remains a challenging combinatorial optimization\ntask. Even for the simplest splitting rules--axis-parallel hyperplanes--it is\nNP-hard to optimize. In Part I of this series, we rigorously defined the proper\ndecision tree model through four axioms and, based on these, introduced four\nformal definitions of the ODT problem. From these definitions, we derived four\ngeneric algorithms capable of solving ODT problems for arbitrary decision trees\nsatisfying the axioms. We also analyzed the combinatorial geometric properties\nof hypersurfaces, showing that decision trees defined by polynomial\nhypersurface splitting rules satisfy the proper axioms that we proposed.\n  In this second paper (Part II) of this two-part series, building on the\nalgorithmic and geometric foundations established in Part I, we introduce the\nfirst hypersurface decision tree (HODT) algorithm. To the best of our\nknowledge, existing optimal decision tree methods are, to date, limited to\nhyperplane splitting rules--a special case of hypersurfaces--and rely on\ngeneral-purpose solvers. In contrast, our HODT algorithm addresses the general\nhypersurface decision tree model without requiring external solvers.\n  Using synthetic datasets generated from ground-truth hyperplane decision\ntrees, we vary tree size, data size, dimensionality, and label and feature\nnoise. Results showing that our algorithm recovers the ground truth more\naccurately than axis-parallel trees and exhibits greater robustness to noise.\nWe also analyzed generalization performance across 30 real-world datasets,\nshowing that HODT can achieve up to 30% higher accuracy than the\nstate-of-the-art optimal axis-parallel decision tree algorithm when tree\ncomplexity is properly controlled.", "published": "2025-09-15 15:38:44", "link": "http://arxiv.org/abs/2509.12057v1", "categories": ["cs.LG", "cs.DM", "cs.DS"], "primary_category": "cs.LG"}
{"title": "Statistical Model Checking Beyond Means: Quantiles, CVaR, and the DKW Inequality (extended version)", "abstract": "Statistical model checking (SMC) randomly samples probabilistic models to\napproximate quantities of interest with statistical error guarantees. It is\ntraditionally used to estimate probabilities and expected rewards, i.e. means\nof different random variables on paths. In this paper, we develop methods using\nthe Dvoretzky-Kiefer-Wolfowitz-Massart inequality (DKW) to extend SMC beyond\nmeans to compute quantities such as quantiles, conditional value-at-risk, and\nentropic risk. The DKW provides confidence bounds on the random variable's\nentire cumulative distribution function, a much more versatile guarantee\ncompared to the statistical methods prevalent in SMC today. We have implemented\nsupport for computing new quantities via the DKW in the 'modes' simulator of\nthe Modest Toolset. We highlight the implementation and its versatility on\nbenchmarks from the quantitative verification literature.", "published": "2025-09-15 12:34:33", "link": "http://arxiv.org/abs/2509.11859v1", "categories": ["stat.ME", "cs.DM", "cs.LO"], "primary_category": "stat.ME"}
{"title": "Agglomeration based influential node ranking in path-type networks", "abstract": "Identification of vital nodes contributes to the research of network\nrobustness and vulnerability. The most influential nodes are effective in\nmaximizing the speed and accelerating the information propagation in complex\nnetworks. Identifying and ranking the most influential nodes in complex\nnetworks has not only theoretical but also practical significance in network\nanalysis since these nodes have a critical influence on the structure and\nfunction of complex networks. This paper is devoted to the evaluating the\nimportance of nodes and ranking influential nodes in paths and path-type\nnetworks such as comets, double comets, and lollipop networks by network\nagglomeration based node contraction method.", "published": "2025-09-15 07:56:04", "link": "http://arxiv.org/abs/2509.11659v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation", "abstract": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.", "published": "2025-09-15 16:14:05", "link": "http://arxiv.org/abs/2509.12086v1", "categories": ["cs.DB", "cs.DS", "cs.IR"], "primary_category": "cs.DB"}
{"title": "AEFS: Adaptive Early Feature Selection for Deep Recommender Systems", "abstract": "Feature selection has emerged as a crucial technique in refining recommender\nsystems. Recent advancements leveraging Automated Machine Learning (AutoML) has\ndrawn significant attention, particularly in two main categories: early feature\nselection and late feature selection, differentiated by whether the selection\noccurs before or after the embedding layer. The early feature selection selects\na fixed subset of features and retrains the model, while the late feature\nselection, known as adaptive feature selection, dynamically adjusts feature\nchoices for each data instance, recognizing the variability in feature\nsignificance. Although adaptive feature selection has shown remarkable\nimprovements in performance, its main drawback lies in its post-embedding layer\nfeature selection. This process often becomes cumbersome and inefficient in\nlarge-scale recommender systems with billions of ID-type features, leading to a\nhighly sparse and parameter-heavy embedding layer. To overcome this, we\nintroduce Adaptive Early Feature Selection (AEFS), a very simple method that\nnot only adaptively selects informative features for each instance, but also\nsignificantly reduces the activated parameters of the embedding layer. AEFS\nemploys a dual-model architecture, encompassing an auxiliary model dedicated to\nfeature selection and a main model responsible for prediction. To ensure\neffective alignment between these two models, we incorporate two collaborative\ntraining loss constraints. Our extensive experiments on three benchmark\ndatasets validate the efficiency and effectiveness of our approach. Notably,\nAEFS matches the performance of current state-of-theart Adaptive Late Feature\nSelection methods while achieving a significant reduction of 37. 5% in the\nactivated parameters of the embedding layer. AEFS is open-source at\nhttps://github. com/fly-dragon211/AEFS .", "published": "2025-09-15 16:04:24", "link": "http://arxiv.org/abs/2509.12076v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Results of the 2025 Video Browser Showdown", "abstract": "This report presents the results of the 14th Video Browser Showdown, held at\nthe 2025 International Conference on Multimedia Modeling on the 8th of January\n2025 in Nara, Japan.", "published": "2025-09-15 14:48:21", "link": "http://arxiv.org/abs/2509.12000v1", "categories": ["cs.MM", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation", "abstract": "Fine-tuning large language models (LLMs) for recommendation in a generative\nmanner has delivered promising results, but encounters significant inference\noverhead due to autoregressive decoding in the language space. This work\nexplores bypassing language-space decoding by directly matching candidate items\nwith the LLM's internal thought representations in the latent space,\neliminating the time-consuming autoregressive process to reduce computational\ncosts. Towards this, we introduce Light Latent-space Decoding (L2D), an\neffective and efficient latent-space decoding method. L2D represents\nuser-preferred items by using the hidden states of test sequences reflecting\nthe LLM's internal thought, and obtains candidate item representations from the\nhidden states of training sequences labeled with the corresponding candidate\nitems. It then matches the two types of representations to decode items,\nachieving latent-space decoding. In this way, it enables efficient decoding\nwithout altering the LLM's generative tuning paradigm, thereby preserving\nperformance. Extensive empirical results demonstrate that L2D is more than 10x\nfaster than language-space decoding while maintaining or enhancing performance.", "published": "2025-09-15 02:30:35", "link": "http://arxiv.org/abs/2509.11524v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Secure Semantic Communication over Wiretap Channels: Rate-Distortion-Equivocation Tradeoff", "abstract": "This paper investigates an information-theoretic model of secure\nsemantic-aware communication. For this purpose, we consider the lossy joint\nsource-channel coding (JSCC) of a memoryless semantic source transmitted over a\nmemoryless wiretap channel. The source consists of two correlated parts that\nrepresent semantic and observed aspects of the information. Our model assumes\nseparate fidelity and secrecy constraints on each source component and, in\naddition, encompasses two cases for the source output, in order to evaluate the\nperformance gains if the encoder has an extended access to the source.\nSpecifically, in Case 1, the encoder has direct access only to the samples from\na single (observed) source component, while in Case 2 it has additional direct\naccess to the samples of the underlaying semantic information. We derive\nsingle-letter converse and achievability bounds on the\nrate-distortion-equivocation region. The converse bound explicitly contains\nrate-distortion functions, making it easy to evaluate, especially for some\ncommon distributions. The proposed achievability coding scheme involves novel\nstochastic superposition coding with two private parts to enable analysis of\nthe equivocation for each source component, separately. Our results generalise\nsome of the previously established source and source-channel coding problems.\nThe general results are further specialised to Gaussian and Bernoulli sources\ntransmitted over Gaussian and binary wiretap channels, respectively. The\nnumerical evaluations illustrate the derived bounds for these distributions.", "published": "2025-09-15 17:07:29", "link": "http://arxiv.org/abs/2509.12142v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Energy Efficiency Maximization for Movable Antenna-Enhanced MIMO Downlink System Based on S-CSI", "abstract": "This paper presents an innovative movable antenna (MA)-enhanced multi-user\nmultiple-input multiple-output (MIMO) downlink system. We aim to maximize the\nenergy efficiency (EE) under statistical channel state information (S-CSI)\nthrough a joint optimization of the precoding matrix and the antenna position\nvectors (APVs). To solve the resulting stochastic problem, we first resort to\ndeterministic equivalent (DE) tecnology to formulate the deterministic\nminorizing function of the system EE and the deterministic function of each\nuser terminal (UT)'s average achievable rate w.r.t. the transmit variables\n(i.e., the precoding matrix and the transmit APV) and the corresponding receive\nAPV, respectively. Then, we propose an alternating optimization (AO) algorithm\nto alternatively optimize the transmit variables and the receive APVs to\nmaximize the formulated deterministic objective functions, respectively.\nFinally, the above AO algorithm is tailored for the single-user scenario. Our\nnumerical results reveal that, the proposed MA-enhanced system can\nsignificantly improve the system EE compared to several benchmark schemes based\non the S-CSI and the optimal performance can be achieved with a finite size of\nmovement regions for MAs.", "published": "2025-09-15 15:23:15", "link": "http://arxiv.org/abs/2509.12036v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy Constraints", "abstract": "In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road\nSide Units (RSUs) intend to gather location data from the vehicles to offer\nvarious location-based services. Although vehicles use the Global Positioning\nSystem (GPS) for navigation, they may refrain from sharing their exact GPS\ncoordinates to the RSUs due to privacy considerations. Thus, to address the\nlocalization expectations of the RSUs and the privacy concerns of the vehicles,\nwe introduce a relaxed-privacy model wherein the vehicles share their partial\nlocation information in order to avail the location-based services. To\nimplement this notion of relaxed-privacy, we propose a low-latency protocol for\nspatial-provenance recovery, wherein vehicles use correlated linear Bloom\nfilters to embed their position information. Our proposed spatial-provenance\nrecovery process takes into account the resolution of localization, the\nunderlying ad hoc protocol, and the coverage range of the wireless technology\nused by the vehicles. Through a rigorous theoretical analysis, we present\nextensive analysis on the underlying trade-off between relaxed-privacy and the\ncommunication-overhead of the protocol. Finally, using a wireless testbed, we\nshow that our proposed method requires a few bits in the packet header to\nprovide security features such as localizing a low-power jammer executing a\ndenial-of-service attack.", "published": "2025-09-15 10:28:52", "link": "http://arxiv.org/abs/2509.11761v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Permutation decoding of first-order Generalized Reed-Muller codes", "abstract": "In [4] we describe a variation of the classical permutation decoding\nalgorithm that can be applied to any binary affine-invariant code; in\nparticular, it can be applied to first-order Reed-Muller codes successfully. In\nthis paper we study how to implement it for the family of first-order\nGeneralized Reed-Muller codes. Then, we give examples which show that we\nimprove the number of errors we can correct in comparison with the known\nresults for this family of codes. Finally, we deal, from a probabilistic point\nof view, with the problem of determining when the algorithm only needs to use a\nsmaller PD-like set.", "published": "2025-09-15 10:21:25", "link": "http://arxiv.org/abs/2509.11757v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Reflexive Partitions Induced by Rank Support and Non-Reflexive Partitions Induced by Rank Weight", "abstract": "In this paper, we study partitions of finite modules induced by rank support\nand rank weight. First, we show that partitions induced by rank support are\nmutually dual with respect to suitable non-degenerate pairings, and hence are\nreflexive; moreover, we compute the associated generalized Krawtchouk matrices.\nSimilar results are established for partitions induced by isomorphic relation\nof rank support. These results generalize counterpart results established for\nrow space partitions and rank partitions of matrix spaces over finite fields.\nNext, we show that partitions of free modules over a finite chain ring $R$\ninduced by rank weight are non-reflexive provided that $R$ is not a field;\nmoreover, we characterize the dual partitions explicitly. As a corollary, we\nshow that rank partitions of matrix spaces over $R$ are reflexive if and only\nif $R$ is a field; moreover, two matrices belong to the same member of the dual\npartition if and only if their transposes are equivalent. In particular, we\nshow that opposite to matrices over finite fields, rank metric does not induce\nan association scheme provided that $R$ is not a field, which further settles\nan open question proposed by Blanco-Chac\\'{o}n, Boix, Greferath and Hieta-Aho\nin \\cite{2}.", "published": "2025-09-15 08:30:10", "link": "http://arxiv.org/abs/2509.11681v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Active Sequential Hypothesis Testing with Non-Homogeneous Costs", "abstract": "We study the Non-Homogeneous Sequential Hypothesis Testing (NHSHT), where a\nsingle active Decision-Maker (DM) selects actions with heterogeneous positive\ncosts to identify the true hypothesis under an average error constraint\n\\(\\delta\\), while minimizing expected total cost paid. Under standard\narguments, we show that the objective decomposes into the product of the mean\nnumber of samples and the mean per-action cost induced by the policy. This\nleads to a key design principle: one should optimize the ratio of expectations\n(expected information gain per expected cost) rather than the expectation of\nper-step information-per-cost (\"bit-per-buck\"), which can be suboptimal. We\nadapt the Chernoff scheme to NHSHT, preserving its classical \\(\\log 1/\\delta\\)\nscaling. In simulations, the adapted scheme reduces mean cost by up to 50\\%\nrelative to the classic Chernoff policy and by up to 90\\% relative to the naive\nbit-per-buck heuristic.", "published": "2025-09-15 06:55:04", "link": "http://arxiv.org/abs/2509.11632v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dynamic Length FSK Waveforms for Joint Communications and Radar", "abstract": "Motivated by the constant modulus property of frequency shift keying (FSK)\nbased waveforms and the stabilisation of its radar performance with an increase\nin the number of subpulses, in this paper an FSK-based dynamic subpulse number\njoint communications and radar waveform design is proposed. From a\ncommunications point of view, the system operates based on traditional FSK\nmodulation. From a sensing point of view, although the subpulses are\ncontinuously generated and transmitted, radar waveforms are dynamically formed\nby monitoring the flatness of the spectrum which in turn guarantees the\naccuracy of the delay estimation. Other constraints on the waveform length are\nused to ensure satisfactory values of the root mean square time duration,\nambiguity function sidelobe levels and prevent overly long waveforms. To\nprovide an estimation of the probability of generating extremely long\nwaveforms, the distribution of the number of subpulses is approximated using a\nBrownian motion process and an existing result on its one-sided exit density.\nNumerical examples are provided to evaluate the accuracy of the approximate\ndistribution, as well as the ambiguity function sidelobe levels and the delay\nand Doppler shift estimation performance of the transmitted waveforms.", "published": "2025-09-15 01:28:58", "link": "http://arxiv.org/abs/2509.11500v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "The Morgan-Pitman Test of Equality of Variances and its Application to Machine Learning Model Evaluation and Selection", "abstract": "Model selection in non-linear models often prioritizes performance metrics\nover statistical tests, limiting the ability to account for sampling\nvariability. We propose the use of a statistical test to assess the equality of\nvariances in forecasting errors. The test builds upon the classic Morgan-Pitman\napproach, incorporating enhancements to ensure robustness against data with\nheavy-tailed distributions or outliers with high variance, plus a strategy to\nmake residuals from machine learning models statistically independent. Through\na series of simulations and real-world data applications, we demonstrate the\ntest's effectiveness and practical utility, offering a reliable tool for model\nevaluation and selection in diverse contexts.", "published": "2025-09-15 17:47:38", "link": "http://arxiv.org/abs/2509.12185v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "All that structure matches does not glitter", "abstract": "Generative models for materials, especially inorganic crystals, hold\npotential to transform the theoretical prediction of novel compounds and\nstructures. Advancement in this field depends critically on robust benchmarks\nand minimal, information-rich datasets that enable meaningful model evaluation.\nThis paper critically examines common datasets and reported metrics for a\ncrystal structure prediction task$\\unicode{x2014}$generating the most likely\nstructures given the chemical composition of a material. We focus on three key\nissues: First, materials datasets should contain unique crystal structures; for\nexample, we show that the widely-utilized carbon-24 dataset only contains\n$\\approx$40% unique structures. Second, materials datasets should not be split\nrandomly if polymorphs of many different compositions are numerous, which we\nfind to be the case for the perov-5 dataset. Third, benchmarks can mislead if\nused uncritically, e.g., reporting a match rate metric without considering the\nstructural variety exhibited by identical building blocks. To address these\noft-overlooked issues, we introduce several fixes. We provide revised versions\nof the carbon-24 dataset: one with duplicates removed, one deduplicated and\nsplit by number of atoms $N$, and two containing only identical structures but\nwith different unit cells. We also propose a new split for the perov-5 dataset\nwhich ensures polymorphs are grouped within each split subset, setting a more\nsensible standard for benchmarking model performance. Finally, we present METRe\nand cRMSE, new model evaluation metrics that can correct existing issues with\nthe match rate metric.", "published": "2025-09-15 17:41:16", "link": "http://arxiv.org/abs/2509.12178v1", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "cs.LG"}
{"title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning", "abstract": "Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation.", "published": "2025-09-15 17:40:19", "link": "http://arxiv.org/abs/2509.12176v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "MMM: Clustering Multivariate Longitudinal Mixed-type Data", "abstract": "Multivariate longitudinal data of mixed-type are increasingly collected in\nmany science domains. However, algorithms to cluster this kind of data remain\nscarce, due to the challenge to simultaneously model the within- and\nbetween-time dependence structures for multivariate data of mixed kind. We\nintroduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a\nthree-way structure and assuming that the non-continuous variables are\nobservations of underlying latent continuous variables, the model relies on a\nmixture of matrix-variate normal distributions to perform clustering in the\nlatent dimension. The MMM model is thus able to handle continuous, ordinal,\nbinary, nominal and count data and to concurrently model the heterogeneity, the\nassociation among the responses and the temporal dependence structure in a\nparsimonious way and without assuming conditional independence. The inference\nis carried out through an MCMC-EM algorithm, which is detailed. An evaluation\nof the model through synthetic data shows its inference abilities. A real-world\napplication on financial data is presented.", "published": "2025-09-15 17:30:31", "link": "http://arxiv.org/abs/2509.12166v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Learning Neural Networks by Neuron Pursuit", "abstract": "The first part of this paper studies the evolution of gradient flow for\nhomogeneous neural networks near a class of saddle points exhibiting a sparsity\nstructure. The choice of these saddle points is motivated from previous works\non homogeneous networks, which identified the first saddle point encountered by\ngradient flow after escaping the origin. It is shown here that, when\ninitialized sufficiently close to such saddle points, gradient flow remains\nnear the saddle point for a sufficiently long time, during which the set of\nweights with small norm remain small but converge in direction. Furthermore,\nimportant empirical observations are made on the behavior of gradient descent\nafter escaping these saddle points. The second part of the paper, motivated by\nthese results, introduces a greedy algorithm to train deep neural networks\ncalled Neuron Pursuit (NP). It is an iterative procedure which alternates\nbetween expanding the network by adding neuron(s) with carefully chosen\nweights, and minimizing the training loss using this augmented network. The\nefficacy of the proposed algorithm is validated using numerical experiments.", "published": "2025-09-15 17:18:35", "link": "http://arxiv.org/abs/2509.12154v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks", "abstract": "We present a learnable physics simulator that provides accurate motion and\nforce-torque prediction of robot end effectors in contact-rich manipulation.\nThe proposed model extends the state-of-the-art GNN-based simulator (FIGNet)\nwith novel node and edge types, enabling action-conditional predictions for\ncontrol and state estimation tasks. In simulation, the MPC agent using our\nmodel matches the performance of the same controller with the ground truth\ndynamics model in a challenging peg-in-hole task, while in the real-world\nexperiment, our model achieves a 50% improvement in motion prediction accuracy\nand 3$\\times$ increase in force-torque prediction precision over the baseline\nphysics simulator. Source code and data are publicly available.", "published": "2025-09-15 17:15:31", "link": "http://arxiv.org/abs/2509.12151v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Do machine learning climate models work in changing climate dynamics?", "abstract": "Climate change is accelerating the frequency and severity of unprecedented\nevents, deviating from established patterns. Predicting these\nout-of-distribution (OOD) events is critical for assessing risks and guiding\nclimate adaptation. While machine learning (ML) models have shown promise in\nproviding precise, high-speed climate predictions, their ability to generalize\nunder distribution shifts remains a significant limitation that has been\nunderexplored in climate contexts. This research systematically evaluates\nstate-of-the-art ML-based climate models in diverse OOD scenarios by adapting\nestablished OOD evaluation methodologies to climate data. Experiments on\nlarge-scale datasets reveal notable performance variability across scenarios,\nshedding light on the strengths and limitations of current models. These\nfindings underscore the importance of robust evaluation frameworks and provide\nactionable insights to guide the reliable application of ML for climate risk\nforecasting.", "published": "2025-09-15 17:12:49", "link": "http://arxiv.org/abs/2509.12147v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data", "abstract": "Graph machine learning models often achieve similar overall performance yet\nbehave differently at the node level, failing on different subsets of nodes\nwith varying reliability. Standard evaluation metrics such as accuracy obscure\nthese fine grained differences, making it difficult to diagnose when and where\nmodels fail. We introduce NodePro, a node profiling framework that enables\nfine-grained diagnosis of model behavior by assigning interpretable profile\nscores to individual nodes. These scores combine data-centric signals, such as\nfeature dissimilarity, label uncertainty, and structural ambiguity, with\nmodel-centric measures of prediction confidence and consistency during\ntraining. By aligning model behavior with these profiles, NodePro reveals\nsystematic differences between models, even when aggregate metrics are\nindistinguishable. We show that node profiles generalize to unseen nodes,\nsupporting prediction reliability without ground-truth labels. Finally, we\ndemonstrate the utility of NodePro in identifying semantically inconsistent or\ncorrupted nodes in a structured knowledge graph, illustrating its effectiveness\nin real-world settings.", "published": "2025-09-15 16:18:54", "link": "http://arxiv.org/abs/2509.12094v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System", "abstract": "Conventional autonomous trading systems struggle to balance computational\nefficiency and market responsiveness due to their fixed operating frequency. We\npropose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework\nthat addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market\nvolatility and dynamically activate specialized Time Frame Agents for\nhigh-frequency or low-frequency trading as needed. During back-testing on AAPL\nstock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of\n25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard\nbenchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return)\nand the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic,\nhierarchical agents can achieve superior risk-adjusted returns while\nmaintaining high computational efficiency.", "published": "2025-09-15 15:31:47", "link": "http://arxiv.org/abs/2509.12048v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework", "abstract": "Traffic flow forecasting is essential for managing congestion, improving\nsafety, and optimizing various transportation systems. However, it remains a\nprevailing challenge due to the stochastic nature of urban traffic and\nenvironmental factors. Better predictions require models capable of\naccommodating the traffic variability influenced by multiple dynamic and\ncomplex interdependent factors. In this work, we propose a Graph Neural Network\n(GNN) framework to address the stochasticity by leveraging adaptive adjacency\nmatrices using log-normal distributions and Coefficient of Variation (CV)\nvalues to reflect real-world travel time variability. Additionally, weather\nfactors such as temperature, wind speed, and precipitation adjust edge weights\nand enable GNN to capture evolving spatio-temporal dependencies across traffic\nstations. This enhancement over the static adjacency matrix allows the model to\nadapt effectively to traffic stochasticity and changing environmental\nconditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)\nframework to provide reliable uncertainty quantification, achieving target\ncoverage while maintaining acceptable prediction intervals. Experimental\nresults demonstrate that the proposed model, in comparison with baseline\nmethods, showed better prediction accuracy and uncertainty bounds. We, then,\nvalidate this method by constructing traffic scenarios in SUMO and applying\nMonte-Carlo simulation to derive a travel time distribution for a Vehicle Under\nTest (VUT) to reflect real-world variability. The simulated mean travel time of\nthe VUT falls within the intervals defined by INRIX historical data, verifying\nthe model's robustness.", "published": "2025-09-15 15:25:43", "link": "http://arxiv.org/abs/2509.12043v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Learning non-Markovian Dynamical Systems with Signature-based Encoders", "abstract": "Neural ordinary differential equations offer an effective framework for\nmodeling dynamical systems by learning a continuous-time vector field. However,\nthey rely on the Markovian assumption - that future states depend only on the\ncurrent state - which is often untrue in real-world scenarios where the\ndynamics may depend on the history of past states. This limitation becomes\nespecially evident in settings involving the continuous control of complex\nsystems with delays and memory effects. To capture historical dependencies,\nexisting approaches often rely on recurrent neural network (RNN)-based\nencoders, which are inherently discrete and struggle with continuous modeling.\nIn addition, they may exhibit poor training behavior. In this work, we\ninvestigate the use of the signature transform as an encoder for learning\nnon-Markovian dynamics in a continuous-time setting. The signature transform\noffers a continuous-time alternative with strong theoretical foundations and\nproven efficiency in summarizing multidimensional information in time. We\nintegrate a signature-based encoding scheme into encoder-decoder dynamics\nmodels and demonstrate that it outperforms RNN-based alternatives in test\nperformance on synthetic benchmarks.", "published": "2025-09-15 15:01:22", "link": "http://arxiv.org/abs/2509.12022v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures", "abstract": "Audio deepfake detection systems based on frozen pre-trained self-supervised\nlearning (SSL) encoders show a high level of performance when combined with\nlayer-weighted pooling methods, such as multi-head factorized attentive pooling\n(MHFA). However, they still struggle to generalize to out-of-domain (OOD)\nconditions. We tackle this problem by studying the behavior of six different\npre-trained SSLs, on four different test corpora. We perform a layer-by-layer\nanalysis to determine which layers contribute most. Next, we study the pooling\nhead, comparing a strategy based on a single layer with automatic selection via\nMHFA. We observed that selecting the best layer gave very good results, while\nreducing system parameters by up to 80%. A wide variation in performance as a\nfunction of test corpus and SSL model is also observed, showing that the\npre-training strategy of the encoder plays a role. Finally, score-level fusion\nof several encoders improved generalization to OOD attacks.", "published": "2025-09-15 14:50:21", "link": "http://arxiv.org/abs/2509.12003v1", "categories": ["cs.SD", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Learning from Uncertain Similarity and Unlabeled Data", "abstract": "Existing similarity-based weakly supervised learning approaches often rely on\nprecise similarity annotations between data pairs, which may inadvertently\nexpose sensitive label information and raise privacy risks. To mitigate this\nissue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel\nframework where each similarity pair is embedded with an uncertainty component\nto reduce label leakage. In this paper, we propose an unbiased risk estimator\nthat learns from uncertain similarity and unlabeled data. Additionally, we\ntheoretically prove that the estimator achieves statistically optimal\nparametric convergence rates. Extensive experiments on both benchmark and\nreal-world datasets show that our method achieves superior classification\nperformance compared to conventional similarity-based approaches.", "published": "2025-09-15 14:29:36", "link": "http://arxiv.org/abs/2509.11984v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training", "abstract": "Neural network (NN) training is inherently a large-scale matrix optimization\nproblem, yet the matrix structure of NN parameters has long been overlooked.\nRecently, the optimizer Muon \\cite{jordanmuon}, which explicitly exploits this\nstructure, has gained significant attention for its strong performance in\nfoundation model training. A key component contributing to Muon's success is\nmatrix orthogonalization. In this paper, we propose {\\it low-rank\northogonalization}, which explicitly leverages the low-rank nature of gradients\nduring NN training. Building on this, we propose low-rank matrix-signed\ngradient descent and a low-rank variant of Muon. Our numerical experiments\ndemonstrate the superior performance of low-rank orthogonalization, with the\nlow-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --\nsurpassing the performance of the carefully tuned vanilla Muon. Theoretically,\nwe establish the iteration complexity of the low-rank matrix-signed gradient\ndescent for finding an approximate stationary solution, as well as that of\nlow-rank Muon for finding an approximate stochastic stationary solution under\nheavy-tailed noise.", "published": "2025-09-15 14:28:53", "link": "http://arxiv.org/abs/2509.11983v1", "categories": ["cs.LG", "math.OC", "49M37, 90C30, 90C90"], "primary_category": "cs.LG"}
{"title": "Examining the Relationship between Scientific Publishing Activity and Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras", "abstract": "Financial bubbles often arrive without much warning, but create long-lasting\neconomic effects. For example, during the dot-com bubble, innovative\ntechnologies created market disruptions through excitement for a promised\nbright future. Such technologies originated from research where scientists had\ndeveloped them for years prior to their entry into the markets. That raises a\nquestion on the possibility of analyzing scientific publishing data (e.g.\ncitation networks) leading up to a bubble for signals that may forecast the\nrise and fall of similar future bubbles. To that end, we utilized temporal SNAs\nto detect possible relationships between the publication citation networks of\nscientists and financial market data during two modern eras of rapidly shifting\ntechnology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024.\nResults showed that the patterns from the dot-com era (which did end in a\nbubble) did not definitively predict the rise and fall of an AI bubble. While\nyearly citation networks reflected possible changes in publishing behavior of\nscientists between the two eras, there was a subset of AI era scientists whose\npublication influence patterns mirrored those during the dot-com era. Upon\nfurther analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH),\nthe data seems to suggest two possibilities for the AI era: unprecedented form\nof financial bubble unseen or that no bubble exists. In conclusion, our\nfindings imply that the patterns present in the dot-com era do not effectively\ntranslate in such a manner to apply them to the AI market.", "published": "2025-09-15 14:28:00", "link": "http://arxiv.org/abs/2509.11982v1", "categories": ["cs.LG", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Deep operator network for surrogate modeling of poroelasticity with random permeability fields", "abstract": "Poroelasticity -- coupled fluid flow and elastic deformation in porous media\n-- often involves spatially variable permeability, especially in subsurface\nsystems. In such cases, simulations with random permeability fields are widely\nused for probabilistic analysis, uncertainty quantification, and inverse\nproblems. These simulations require repeated forward solves that are often\nprohibitively expensive, motivating the development of efficient surrogate\nmodels. However, efficient surrogate modeling techniques for poroelasticity\nwith random permeability fields remain scarce. In this study, we propose a\nsurrogate modeling framework based on the deep operator network (DeepONet), a\nneural architecture designed to learn mappings between infinite-dimensional\nfunction spaces. The proposed surrogate model approximates the solution\noperator that maps random permeability fields to transient poroelastic\nresponses. To enhance predictive accuracy and stability, we integrate three\nstrategies: nondimensionalization of the governing equations, input\ndimensionality reduction via Karhunen--Lo\\'eve expansion, and a two-step\ntraining procedure that decouples the optimization of branch and trunk\nnetworks. The methodology is evaluated on two benchmark problems in\nporoelasticity: soil consolidation and ground subsidence induced by groundwater\nextraction. In both cases, the DeepONet achieves substantial speedup in\ninference while maintaining high predictive accuracy across a wide range of\npermeability statistics. These results highlight the potential of the proposed\napproach as a scalable and efficient surrogate modeling technique for\nporoelastic systems with random permeability fields.", "published": "2025-09-15 14:18:49", "link": "http://arxiv.org/abs/2509.11966v1", "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "cs.LG"}
{"title": "Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation", "abstract": "The modeling and prediction of multivariate spatio-temporal data involve\nnumerous challenges. Dimension reduction methods can significantly simplify\nthis process, provided that they account for the complex dependencies between\nvariables and across time and space. Nonlinear blind source separation has\nemerged as a promising approach, particularly following recent advances in\nidentifiability results. Building on these developments, we introduce the\nidentifiable autoregressive variational autoencoder, which ensures the\nidentifiability of latent components consisting of nonstationary autoregressive\nprocesses. The blind source separation efficacy of the proposed method is\nshowcased through a simulation study, where it is compared against\nstate-of-the-art methods, and the spatio-temporal prediction performance is\nevaluated against several competitors on air pollution and weather datasets.", "published": "2025-09-15 14:17:06", "link": "http://arxiv.org/abs/2509.11962v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "TabStruct: Measuring Structural Fidelity of Tabular Data", "abstract": "Evaluating tabular generators remains a challenging problem, as the unique\ncausal structural prior of heterogeneous tabular data does not lend itself to\nintuitive human inspection. Recent work has introduced structural fidelity as a\ntabular-specific evaluation dimension to assess whether synthetic data complies\nwith the causal structures of real data. However, existing benchmarks often\nneglect the interplay between structural fidelity and conventional evaluation\ndimensions, thus failing to provide a holistic understanding of model\nperformance. Moreover, they are typically limited to toy datasets, as\nquantifying existing structural fidelity metrics requires access to\nground-truth causal structures, which are rarely available for real-world\ndatasets. In this paper, we propose a novel evaluation framework that jointly\nconsiders structural fidelity and conventional evaluation dimensions. We\nintroduce a new evaluation metric, $\\textbf{global utility}$, which enables the\nassessment of structural fidelity even in the absence of ground-truth causal\nstructures. In addition, we present $\\textbf{TabStruct}$, a comprehensive\nevaluation benchmark offering large-scale quantitative analysis on 13 tabular\ngenerators from nine distinct categories, across 29 datasets. Our results\ndemonstrate that global utility provides a task-independent, domain-agnostic\nlens for tabular generator performance. We release the TabStruct benchmark\nsuite, including all datasets, evaluation pipelines, and raw results. Code is\navailable at https://github.com/SilenceX12138/TabStruct.", "published": "2025-09-15 14:08:20", "link": "http://arxiv.org/abs/2509.11950v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Quantum Noise Tomography with Physics-Informed Neural Networks", "abstract": "Characterizing the environmental interactions of quantum systems is a\ncritical bottleneck in the development of robust quantum technologies.\nTraditional tomographic methods are often data-intensive and struggle with\nscalability. In this work, we introduce a novel framework for performing\nLindblad tomography using Physics-Informed Neural Networks (PINNs). By\nembedding the Lindblad master equation directly into the neural network's loss\nfunction, our approach simultaneously learns the quantum state's evolution and\ninfers the underlying dissipation parameters from sparse, time-series\nmeasurement data. Our results show that PINNs can reconstruct both the system\ndynamics and the functional form of unknown noise parameters, presenting a\nsample-efficient and scalable solution for quantum device characterization.\nUltimately, our method produces a fully-differentiable digital twin of a noisy\nquantum system by learning its governing master equation.", "published": "2025-09-15 13:30:50", "link": "http://arxiv.org/abs/2509.11911v1", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "primary_category": "quant-ph"}
{"title": "High Effort, Low Gain: Fundamental Limits of Active Learning for Linear Dynamical Systems", "abstract": "In this work, we consider the problem of identifying an unknown linear\ndynamical system given a finite hypothesis class. In particular, we analyze the\neffect of the excitation input on the sample complexity of identifying the true\nsystem with high probability. To this end, we present sample complexity lower\nbounds that capture the choice of the selected excitation input. The sample\ncomplexity lower bound gives rise to a system theoretic condition to determine\nthe potential benefit of experiment design. Informed by the analysis of the\nsample complexity lower bound, we propose a persistent excitation (PE)\ncondition tailored to the considered setting, which we then use to establish\nsample complexity upper bounds. Notably, the \\acs{PE} condition is weaker than\nin the case of an infinite hypothesis class and allows analyzing different\nexcitation inputs modularly. Crucially, the lower and upper bounds share the\nsame dependency on key problem parameters. Finally, we leverage these insights\nto propose an active learning algorithm that sequentially excites the system\noptimally with respect to the current estimate, and provide sample complexity\nguarantees for the presented algorithm. Concluding simulations showcase the\neffectiveness of the proposed algorithm.", "published": "2025-09-15 13:29:24", "link": "http://arxiv.org/abs/2509.11907v1", "categories": ["eess.SY", "cs.LG", "cs.SY", "stat.ML"], "primary_category": "eess.SY"}
{"title": "Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting", "abstract": "This study develops and evaluates a novel hybridWavelet SARIMA Transformer,\nWST framework to forecast using monthly rainfall across five meteorological\nsubdivisions of Northeast India over the 1971 to 2023 period. The approach\nemploys the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet\nfamilies such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift\ninvariant, multiresolution decomposition of the rainfall series. Linear and\nseasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear\ncomponents are modeled by a Transformer network, and forecasts are\nreconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20\ntrain test split and multiple performance indices such as, RMSE, MAE, SMAPE,\nWillmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes\nE1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across\nall subdivisions, WHST consistently achieved lower forecast errors, stronger\nagreement with observed rainfall, and unbiased predictions compared with stand\nalone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual\nadequacy was confirmed through the Ljung Box test, while Taylor diagrams\nprovided an integrated assessment of correlation, variance fidelity, and RMSE,\nfurther reinforcing the robustness of the proposed approach. The results\nhighlight the effectiveness of integrating multiresolution signal decomposition\nwith complementary linear and deep learning models for hydroclimatic\nforecasting. Beyond rainfall, the proposed WST framework offers a scalable\nmethodology for forecasting complex environmental time series, with direct\nimplications for flood risk management, water resources planning, and climate\nadaptation strategies in data-sparse and climate-sensitive regions.", "published": "2025-09-15 13:27:19", "link": "http://arxiv.org/abs/2509.11903v1", "categories": ["stat.AP", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.AP"}
{"title": "Transparent and Fair Profiling in Employment Services: Evidence from Switzerland", "abstract": "Long-term unemployment (LTU) is a challenge for both jobseekers and public\nemployment services. Statistical profiling tools are increasingly used to\npredict LTU risk. Some profiling tools are opaque, black-box machine learning\nmodels, which raise issues of transparency and fairness. This paper\ninvestigates whether interpretable models could serve as an alternative, using\nadministrative data from Switzerland. Traditional statistical, interpretable,\nand black-box models are compared in terms of predictive performance,\ninterpretability, and fairness. It is shown that explainable boosting machines,\na recent interpretable model, perform nearly as well as the best black-box\nmodels. It is also shown how model sparsity, feature smoothing, and fairness\nmitigation can enhance transparency and fairness with only minor losses in\nperformance. These findings suggest that interpretable profiling provides an\naccountable and trustworthy alternative to black-box models without\ncompromising performance.", "published": "2025-09-15 12:30:10", "link": "http://arxiv.org/abs/2509.11847v1", "categories": ["cs.LG", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Visualization and Analysis of the Loss Landscape in Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) are powerful models for graph-structured data,\nwith broad applications. However, the interplay between GNN parameter\noptimization, expressivity, and generalization remains poorly understood. We\naddress this by introducing an efficient learnable dimensionality reduction\nmethod for visualizing GNN loss landscapes, and by analyzing the effects of\nover-smoothing, jumping knowledge, quantization, sparsification, and\npreconditioner on GNN optimization. Our learnable projection method surpasses\nthe state-of-the-art PCA-based approach, enabling accurate reconstruction of\nhigh-dimensional parameters with lower memory usage. We further show that\narchitecture, sparsification, and optimizer's preconditioning significantly\nimpact the GNN optimization landscape and their training process and final\nprediction performance. These insights contribute to developing more efficient\ndesigns of GNN architectures and training strategies.", "published": "2025-09-15 11:22:55", "link": "http://arxiv.org/abs/2509.11792v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Synthetic vs. Real Training Data for Visual Navigation", "abstract": "This paper investigates how the performance of visual navigation policies\ntrained in simulation compares to policies trained with real-world data.\nPerformance degradation of simulator-trained policies is often significant when\nthey are evaluated in the real world. However, despite this well-known\nsim-to-real gap, we demonstrate that simulator-trained policies can match the\nperformance of their real-world-trained counterparts.\n  Central to our approach is a navigation policy architecture that bridges the\nsim-to-real appearance gap by leveraging pretrained visual representations and\nruns real-time on robot hardware. Evaluations on a wheeled mobile robot show\nthat the proposed policy, when trained in simulation, outperforms its\nreal-world-trained version by 31% and the prior state-of-the-art methods by 50%\nin navigation success rate. Policy generalization is verified by deploying the\nsame model onboard a drone.\n  Our results highlight the importance of diverse image encoder pretraining for\nsim-to-real generalization, and identify on-policy learning as a key advantage\nof simulated training over training with real data.", "published": "2025-09-15 11:22:40", "link": "http://arxiv.org/abs/2509.11791v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios", "abstract": "Real-time fall detection is crucial for enabling timely interventions and\nmitigating the severe health consequences of falls, particularly in older\nadults. However, existing methods often rely on simulated data or assumptions\nsuch as prior knowledge of fall events, limiting their real-world\napplicability. Practical deployment also requires efficient computation and\nrobust evaluation metrics tailored to continuous monitoring. This paper\npresents a real-time fall detection framework for continuous monitoring without\nprior knowledge of fall events. Using over 60 hours of inertial measurement\nunit (IMU) data from the FARSEEING real-world falls dataset, we employ recent\nefficient classifiers to compute fall probabilities in streaming mode. To\nenhance robustness, we introduce a cost-sensitive learning strategy that tunes\nthe decision threshold using a cost function reflecting the higher risk of\nmissed falls compared to false alarms. Unlike many methods that achieve high\nrecall only at the cost of precision, our framework achieved Recall of 1.00,\nPrecision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls\nwhile keeping false alarms low, with average inference time below 5 ms per\nsample. These results demonstrate that cost-sensitive threshold tuning enhances\nthe robustness of accelerometer-based fall detection. They also highlight the\npotential of our computationally efficient framework for deployment in\nreal-time wearable sensor systems for continuous monitoring.", "published": "2025-09-15 11:19:42", "link": "http://arxiv.org/abs/2509.11789v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Multimodal Regression for Enzyme Turnover Rates Prediction", "abstract": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.", "published": "2025-09-15 11:07:26", "link": "http://arxiv.org/abs/2509.11782v1", "categories": ["cs.LG", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Stabilizing PINNs: A regularization scheme for PINN training to avoid unstable fixed points of dynamical systems", "abstract": "It was recently shown that the loss function used for training\nphysics-informed neural networks (PINNs) exhibits local minima at solutions\ncorresponding to fixed points of dynamical systems. In the forward setting,\nwhere the PINN is trained to solve initial value problems, these local minima\ncan interfere with training and potentially leading to physically incorrect\nsolutions. Building on stability theory, this paper proposes a regularization\nscheme that penalizes solutions corresponding to unstable fixed points.\nExperimental results on four dynamical systems, including the Lotka-Volterra\nmodel and the van der Pol oscillator, show that our scheme helps avoiding\nphysically incorrect solutions and substantially improves the training success\nrate of PINNs.", "published": "2025-09-15 10:44:30", "link": "http://arxiv.org/abs/2509.11768v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Data Fusion and Machine Learning for Ship Fuel Consumption Modelling -- A Case of Bulk Carrier Vessel", "abstract": "There is an increasing push for operational measures to reduce ships' bunker\nfuel consumption and carbon emissions, driven by the International Maritime\nOrganization (IMO) mandates. Key performance indicators such as the Energy\nEfficiency Operational Indicator (EEOI) focus on fuel efficiency. Strategies\nlike trim optimization, virtual arrival, and green routing have emerged. The\ntheoretical basis for these approaches lies in accurate prediction of fuel\nconsumption as a function of sailing speed, displacement, trim, climate, and\nsea state. This study utilized 296 voyage reports from a bulk carrier vessel\nover one year (November 16, 2021 to November 21, 2022) and 28 parameters,\nintegrating hydrometeorological big data from the Copernicus Marine Environment\nMonitoring Service (CMEMS) with 19 parameters and the European Centre for\nMedium-Range Weather Forecasts (ECMWF) with 61 parameters. The objective was to\nevaluate whether fusing external public data sources enhances modeling accuracy\nand to highlight the most influential parameters affecting fuel consumption.\nThe results reveal a strong potential for machine learning techniques to\npredict ship fuel consumption accurately by combining voyage reports with\nclimate and sea data. However, validation on similar classes of vessels remains\nnecessary to confirm generalizability.", "published": "2025-09-15 10:01:14", "link": "http://arxiv.org/abs/2509.11750v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "Analysing Python Machine Learning Notebooks with Moose", "abstract": "Machine Learning (ML) code, particularly within notebooks, often exhibits\nlower quality compared to traditional software. Bad practices arise at three\ndistinct levels: general Python coding conventions, the organizational\nstructure of the notebook itself, and ML-specific aspects such as\nreproducibility and correct API usage. However, existing analysis tools\ntypically focus on only one of these levels and struggle to capture ML-specific\nsemantics, limiting their ability to detect issues. This paper introduces\nVespucci Linter, a static analysis tool with multi-level capabilities, built on\nMoose and designed to address this challenge. Leveraging a metamodeling\napproach that unifies the notebook's structural elements with Python code\nentities, our linter enables a more contextualized analysis to identify issues\nacross all three levels. We implemented 22 linting rules derived from the\nliterature and applied our tool to a corpus of 5,000 notebooks from the Kaggle\nplatform. The results reveal violations at all levels, validating the relevance\nof our multi-level approach and demonstrating Vespucci Linter's potential to\nimprove the quality and reliability of ML development in notebook environments.", "published": "2025-09-15 09:59:49", "link": "http://arxiv.org/abs/2509.11748v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters", "abstract": "Understanding how atmospheric molecular clusters form and grow is key to\nresolving one of the biggest uncertainties in climate modelling: the formation\nof new aerosol particles. While quantum chemistry offers accurate insights into\nthese early-stage clusters, its steep computational costs limit large-scale\nexploration. In this work, we present a fast, interpretable, and surprisingly\npowerful alternative: $k$-nearest neighbour ($k$-NN) regression model. By\nleveraging chemically informed distance metrics, including a kernel-induced\nmetric and one learned via metric learning for kernel regression (MLKR), we\nshow that simple $k$-NN models can rival more complex kernel ridge regression\n(KRR) models in accuracy, while reducing computational time by orders of\nmagnitude. We perform this comparison with the well-established\nFaber-Christensen-Huang-Lilienfeld (FCHL19) molecular descriptor, but other\ndescriptors (e.g., FCHL18, MBDF, and CM) can be shown to have similar\nperformance. Applied to both simple organic molecules in the QM9 benchmark set\nand large datasets of atmospheric molecular clusters (sulphuric acid-water and\nsulphuric-multibase -base systems), our $k$-NN models achieve near-chemical\naccuracy, scale seamlessly to datasets with over 250,000 entries, and even\nappears to extrapolate to larger unseen clusters with minimal error (often\nnearing 1 kcal/mol). With built-in interpretability and straightforward\nuncertainty estimation, this work positions $k$-NN as a potent tool for\naccelerating discovery in atmospheric chemistry and beyond.", "published": "2025-09-15 09:29:39", "link": "http://arxiv.org/abs/2509.11728v1", "categories": ["cs.LG", "I.6.3; J.2"], "primary_category": "cs.LG"}
{"title": "Neural Audio Codecs for Prompt-Driven Universal Source Separation", "abstract": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible.", "published": "2025-09-15 09:12:57", "link": "http://arxiv.org/abs/2509.11717v1", "categories": ["cs.SD", "cs.LG"], "primary_category": "cs.SD"}
{"title": "EMeRALDS: Electronic Medical Record Driven Automated Lung Nodule Detection and Classification in Thoracic CT Images", "abstract": "Objective: Lung cancer is a leading cause of cancer-related mortality\nworldwide, primarily due to delayed diagnosis and poor early detection. This\nstudy aims to develop a computer-aided diagnosis (CAD) system that leverages\nlarge vision-language models (VLMs) for the accurate detection and\nclassification of pulmonary nodules in computed tomography (CT) scans.\n  Methods: We propose an end-to-end CAD pipeline consisting of two modules: (i)\na detection module (CADe) based on the Segment Anything Model 2 (SAM2), in\nwhich the standard visual prompt is replaced with a text prompt encoded by CLIP\n(Contrastive Language-Image Pretraining), and (ii) a diagnosis module (CADx)\nthat calculates similarity scores between segmented nodules and radiomic\nfeatures. To add clinical context, synthetic electronic medical records (EMRs)\nwere generated using radiomic assessments by expert radiologists and combined\nwith similarity scores for final classification. The method was tested on the\npublicly available LIDC-IDRI dataset (1,018 CT scans).\n  Results: The proposed approach demonstrated strong performance in zero-shot\nlung nodule analysis. The CADe module achieved a Dice score of 0.92 and an IoU\nof 0.85 for nodule segmentation. The CADx module attained a specificity of 0.97\nfor malignancy classification, surpassing existing fully supervised methods.\n  Conclusions: The integration of VLMs with radiomics and synthetic EMRs allows\nfor accurate and clinically relevant CAD of pulmonary nodules in CT scans. The\nproposed system shows strong potential to enhance early lung cancer detection,\nincrease diagnostic confidence, and improve patient management in routine\nclinical workflows.", "published": "2025-09-15 09:11:17", "link": "http://arxiv.org/abs/2509.11714v1", "categories": ["eess.IV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction", "abstract": "Next location prediction is a key task in human mobility analysis, crucial\nfor applications like smart city resource allocation and personalized\nnavigation services. However, existing methods face two significant challenges:\nfirst, they fail to address the dynamic imbalance between periodic and chaotic\nmobile patterns, leading to inadequate adaptation over sparse trajectories;\nsecond, they underutilize contextual cues, such as temporal regularities in\narrival times, which persist even in chaotic patterns and offer stronger\npredictability than spatial forecasts due to reduced search spaces. To tackle\nthese challenges, we propose \\textbf{\\method}, a\n\\underline{\\textbf{C}}h\\underline{\\textbf{A}}otic \\underline{\\textbf{N}}eural\n\\underline{\\textbf{O}}scillator n\\underline{\\textbf{E}}twork for next location\nprediction, which introduces a biologically inspired Chaotic Neural Oscillatory\nAttention mechanism to inject adaptive variability into traditional attention,\nenabling balanced representation of evolving mobility behaviors, and employs a\nTri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to\nfuse multimodal ``who-when-where'' contexts in a joint framework for enhanced\nprediction performance. Extensive experiments on two real-world datasets\ndemonstrate that CANOE consistently and significantly outperforms a sizeable\ncollection of state-of-the-art baselines, yielding 3.17\\%-13.11\\% improvement\nover the best-performing baselines across different cases. In particular, CANOE\ncan make robust predictions over mobility trajectories of different mobility\nchaotic levels. A series of ablation studies also supports our key design\nchoices. Our code is available at: https://github.com/yuqian2003/CANOE.", "published": "2025-09-15 09:10:48", "link": "http://arxiv.org/abs/2509.11713v1", "categories": ["cs.LG", "cs.NI"], "primary_category": "cs.LG"}
{"title": "An Interventional Approach to Real-Time Disaster Assessment via Causal Attribution", "abstract": "Traditional disaster analysis and modelling tools for assessing the severity\nof a disaster are predictive in nature. Based on the past observational data,\nthese tools prescribe how the current input state (e.g., environmental\nconditions, situation reports) results in a severity assessment. However, these\nsystems are not meant to be interventional in the causal sense, where the user\ncan modify the current input state to simulate counterfactual \"what-if\"\nscenarios. In this work, we provide an alternative interventional tool that\ncomplements traditional disaster modelling tools by leveraging real-time data\nsources like satellite imagery, news, and social media. Our tool also helps\nunderstand the causal attribution of different factors on the estimated\nseverity, over any given region of interest. In addition, we provide actionable\nrecourses that would enable easier mitigation planning. Our source code is\npublicly available.", "published": "2025-09-15 08:17:52", "link": "http://arxiv.org/abs/2509.11676v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks", "abstract": "This paper introduces SpaPool, a novel pooling method that combines the\nstrengths of both dense and sparse techniques for a graph neural network.\nSpaPool groups vertices into an adaptive number of clusters, leveraging the\nbenefits of both dense and sparse approaches. It aims to maintain the\nstructural integrity of the graph while reducing its size efficiently.\nExperimental results on several datasets demonstrate that SpaPool achieves\ncompetitive performance compared to existing pooling techniques and excels\nparticularly on small-scale graphs. This makes SpaPool a promising method for\napplications requiring efficient and effective graph processing.", "published": "2025-09-15 08:16:40", "link": "http://arxiv.org/abs/2509.11675v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Assessing On-the-Ground Disaster Impact Using Online Data Sources", "abstract": "Assessing the impact of a disaster in terms of asset losses and human\ncasualties is essential for preparing effective response plans. Traditional\nmethods include offline assessments conducted on the ground, where volunteers\nand first responders work together to collect the estimate of losses through\nwindshield surveys or on-ground inspection. However, these methods have a time\ndelay and are prone to different biases. Recently, various online data sources,\nincluding social media, news reports, aerial imagery, and satellite data, have\nbeen utilized to evaluate the impact of disasters. Online data sources provide\nreal-time data streams for estimating the offline impact. Limited research\nexists on how different online sources help estimate disaster impact at a given\nadministrative unit. In our work, we curate a comprehensive dataset by\ncollecting data from multiple online sources for a few billion-dollar disasters\nat the county level. We also analyze how online estimates compare with\ntraditional offline-based impact estimates for the disaster. Our findings\nprovide insight into how different sources can provide complementary\ninformation to assess the disaster.", "published": "2025-09-15 07:08:48", "link": "http://arxiv.org/abs/2509.11634v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay", "abstract": "Anomaly detection in dynamic graphs is essential for identifying malicious\nactivities, fraud, and unexpected behaviors in real-world systems such as\ncybersecurity and power grids. However, existing approaches struggle with\nscalability, probabilistic interpretability, and adaptability to evolving\ntraffic patterns. In this paper, we propose ADAPTIVE-GRAPHSKETCH, a lightweight\nand scalable framework for real-time anomaly detection in streaming edge data.\nOur method integrates temporal multi-tensor sketching with Count-Min Sketch\nusing Conservative Update (CMS-CU) to compactly track edge frequency patterns\nwith bounded memory, while mitigating hash collision issues. We incorporate\nBayesian inference for probabilistic anomaly scoring and apply Exponentially\nWeighted Moving Average (EWMA) for adaptive thresholding tuned to burst\nintensity. Extensive experiments on four real-world intrusion detection\ndatasets demonstrate that ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art\nbaselines such as ANOEDGE-G/L, MIDAS-R, and F-FADE, achieving up to 6.5% AUC\ngain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019, while processing 20\nmillion edges in under 3.4 seconds using only 10 hash functions. Our results\nshow that ADAPTIVE-GRAPHSKETCH is practical and effective for fast, accurate\nanomaly detection in large-scale streaming graphs.\n  Keywords: Anomaly Detection, Streaming, Real-time, Dynamic Graphs, Edge\nStreams, Tensor Sketching", "published": "2025-09-15 06:57:35", "link": "http://arxiv.org/abs/2509.11633v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Topology Structure Optimization of Reservoirs Using GLMY Homology", "abstract": "Reservoir is an efficient network for time series processing. It is well\nknown that network structure is one of the determinants of its performance.\nHowever, the topology structure of reservoirs, as well as their performance, is\nhard to analyzed, due to the lack of suitable mathematical tools. In this\npaper, we study the topology structure of reservoirs using persistent GLMY\nhomology theory, and develop a method to improve its performance. Specifically,\nit is found that the reservoir performance is closely related to the\none-dimensional GLMY homology groups. Then, we develop a reservoir structure\noptimization method by modifying the minimal representative cycles of\none-dimensional GLMY homology groups. Finally, by experiments, it is validated\nthat the performance of reservoirs is jointly influenced by the reservoir\nstructure and the periodicity of the dataset.", "published": "2025-09-15 06:11:29", "link": "http://arxiv.org/abs/2509.11612v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals", "abstract": "Cardiovascular diseases (CVDs) are the leading cause of death worldwide,\naccounting for approximately 17.9 million deaths each year. Early detection is\ncritical, creating a demand for accurate and inexpensive pre-screening methods.\nDeep learning has recently been applied to classify abnormal heart sounds\nindicative of CVDs using synchronised phonocardiogram (PCG) and\nelectrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,\nstate-of-the-art architectures remain underutilised due to the limited\navailability of synchronised and multichannel datasets. Augmented datasets and\npre-trained models provide a pathway to overcome these limitations, enabling\ntransformer-based architectures to be trained effectively. This work combines\ntraditional signal processing with denoising diffusion models, WaveGrad and\nDiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based\nclassifier on multimodal and multichannel heart sound datasets. The approach\nachieves state-of-the-art performance. On the Computing in Cardiology (CinC)\n2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),\nsensitivity, specificity and Matthew's correlation coefficient (MCC) reach\n92.48\\%, 93.05\\%, 93.63\\%, 92.48\\%, 94.93\\% and 0.8283, respectively. Using the\nsynchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\\%,\n92.21\\%, 94.35\\%, 90.10\\%, 95.12\\% and 0.8380 are achieved for accuracy, UAR,\nsensitivity, specificity and MCC, respectively. Using a wearable vest dataset\nconsisting of mPCG data, the model achieves 77.13\\% accuracy, 74.25\\% UAR,\n86.47\\% sensitivity, 62.04\\% specificity, and 0.5082 MCC. These results\ndemonstrate the effectiveness of transformer-based models for CVD detection\nwhen supported by augmented datasets, highlighting their potential to advance\nmultimodal and multichannel heart sound classification.", "published": "2025-09-15 05:52:41", "link": "http://arxiv.org/abs/2509.11606v1", "categories": ["cs.SD", "cs.LG", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Learning Singularity-Encoded Green's Functions with Application to Iterative Methods", "abstract": "Green's function provides an inherent connection between theoretical analysis\nand numerical methods for elliptic partial differential equations, and general\nabsence of its closed-form expression necessitates surrogate modeling to guide\nthe design of effective solvers. Unfortunately, numerical computation of\nGreen's function remains challenging due to its doubled dimensionality and\nintrinsic singularity. In this paper, we present a novel singularity-encoded\nlearning approach to resolve these problems in an unsupervised fashion. Our\nmethod embeds the Green's function within a one-order higher-dimensional space\nby encoding its prior estimate as an augmented variable, followed by a neural\nnetwork parametrization to manage the increased dimensionality. By projecting\nthe trained neural network solution back onto the original domain, our deep\nsurrogate model exploits its spectral bias to accelerate conventional iterative\nschemes, serving either as a preconditioner or as part of a hybrid solver. The\neffectiveness of our proposed method is empirically verified through numerical\nexperiments with two and four dimensional Green's functions, achieving\nsatisfactory resolution of singularities and acceleration of iterative solvers.", "published": "2025-09-15 04:53:22", "link": "http://arxiv.org/abs/2509.11580v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Compressed Sensing: Mathematical Foundations, Implementation, and Advanced Optimization Techniques", "abstract": "Compressed sensing is a signal processing technique that allows for the\nreconstruction of a signal from a small set of measurements. The key idea\nbehind compressed sensing is that many real-world signals are inherently\nsparse, meaning that they can be efficiently represented in a different space\nwith only a few components compared to their original space representation. In\nthis paper we will explore the mathematical formulation behind compressed\nsensing, its logic and pathologies, and apply compressed sensing to real world\nsignals.", "published": "2025-09-15 03:29:18", "link": "http://arxiv.org/abs/2509.11550v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "E-ROBOT: a dimension-free method for robust statistics and machine learning via Schr\u00f6dinger bridge", "abstract": "We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)\nframework, a novel method that combines the robustness of ROBOT with the\ncomputational and statistical benefits of entropic regularization. We show\nthat, rooted in the Schr\\\"{o}dinger bridge problem theory, E-ROBOT defines the\nrobust Sinkhorn divergence $\\overline{W}_{\\varepsilon,\\lambda}$, where the\nparameter $\\lambda$ controls robustness and $\\varepsilon$ governs the\nregularization strength. Letting $n\\in \\mathbb{N}$ denote the sample size, a\ncentral theoretical contribution is establishing that the sample complexity of\n$\\overline{W}_{\\varepsilon,\\lambda}$ is $\\mathcal{O}(n^{-1/2})$, thereby\navoiding the curse of dimensionality that plagues standard ROBOT. This\ndimension-free property unlocks the use of $\\overline{W}_{\\varepsilon,\\lambda}$\nas a loss function in large-dimensional statistical and machine learning tasks.\nWith this regard, we demonstrate its utility through four applications:\ngoodness-of-fit testing; computation of barycenters for corrupted 2D and 3D\nshapes; definition of gradient flows; and image colour transfer. From the\ncomputation standpoint, a perk of our novel method is that it can be easily\nimplemented by modifying existing (\\texttt{Python}) routines. From the\ntheoretical standpoint, our work opens the door to many research directions in\nstatistics and machine learning: we discuss some of them.", "published": "2025-09-15 02:49:04", "link": "http://arxiv.org/abs/2509.11532v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks", "abstract": "Deep learning models are vulnerable to adversarial examples, posing critical\nsecurity challenges in real-world applications. While Adversarial Training (AT\n) is a widely adopted defense mechanism to enhance robustness, it often incurs\na trade-off by degrading performance on unperturbed, natural data. Recent\nefforts have highlighted that larger models exhibit enhanced robustness over\ntheir smaller counterparts. In this paper, we empirically demonstrate that such\nrobustness can be systematically distilled from large teacher models into\ncompact student models. To achieve better performance, we introduce Dice\nAdversarial Robustness Distillation (DARD), a novel method designed to transfer\nrobustness through a tailored knowledge distillation paradigm. Additionally, we\npropose Dice Projected Gradient Descent (DPGD), an adversarial example\ngeneralization method optimized for effective attack. Our extensive experiments\ndemonstrate that the DARD approach consistently outperforms adversarially\ntrained networks with the same architecture, achieving superior robustness and\nstandard accuracy.", "published": "2025-09-15 02:31:30", "link": "http://arxiv.org/abs/2509.11525v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification", "abstract": "Class imbalance in supervised classification often degrades model performance\nby biasing predictions toward the majority class, particularly in critical\napplications such as medical diagnosis and fraud detection. Traditional\noversampling techniques, including SMOTE and its variants, generate synthetic\nminority samples via local interpolation but fail to capture global data\ndistributions in high-dimensional spaces. Deep generative models based on GANs\noffer richer distribution modeling yet suffer from training instability and\nmode collapse under severe imbalance. To overcome these limitations, we\nintroduce an oversampling framework that learns a parametric transformation to\nmap majority samples into the minority distribution. Our approach minimizes the\nmaximum mean discrepancy (MMD) between transformed and true minority samples\nfor global alignment, and incorporates a triplet loss regularizer to enforce\nboundary awareness by guiding synthesized samples toward challenging borderline\nregions. We evaluate our method on 29 synthetic and real-world datasets,\ndemonstrating consistent improvements over classical and generative baselines\nin AUROC, G-mean, F1-score, and MCC. These results confirm the robustness,\ncomputational efficiency, and practical utility of the proposed framework for\nimbalanced classification tasks.", "published": "2025-09-15 01:47:29", "link": "http://arxiv.org/abs/2509.11511v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach", "abstract": "As underwater human activities are increasing, the demand for underwater\ncommunication service presents a significant challenge. Existing underwater\ndiver communication methods face hurdles due to inherent disadvantages and\ncomplex underwater environments. To address this issue, we propose a scheme\nthat utilizes maritime unmanned systems to assist divers with reliable and\nhigh-speed communication. Multiple AUVs are equipped with optical and acoustic\nmultimodal communication devices as relay nodes, providing adaptive\ncommunication services based on changes in the diver's activity area. By using\na multi-agent reinforcement learning (MARL) approach to control the cooperative\nmovement of AUVs, high-speed and reliable data transmission between divers can\nbe achieved. At the same time, utilizing the advantages of on-demand deployment\nand wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to\ncoordinate and forward information from AUVs, and controlling AUVs to\nadaptively select relay USV nodes for data transmission, high-quality\ncommunication between divers and surface platform can be achieved. Through\nsimulation verification, the proposed scheme can effectively achieve reliable\nand high-speed communication for divers.", "published": "2025-09-15 01:44:28", "link": "http://arxiv.org/abs/2509.11508v1", "categories": ["cs.MA", "cs.LG", "cs.RO"], "primary_category": "cs.MA"}
{"title": "OASIS: A Deep Learning Framework for Universal Spectroscopic Analysis Driven by Novel Loss Functions", "abstract": "The proliferation of spectroscopic data across various scientific and\nengineering fields necessitates automated processing. We introduce OASIS\n(Omni-purpose Analysis of Spectra via Intelligent Systems), a machine learning\n(ML) framework for technique-independent, automated spectral analysis,\nencompassing denoising, baseline correction, and comprehensive peak parameter\n(location, intensity, FWHM) retrieval without human intervention. OASIS\nachieves its versatility through models trained on a strategically designed\nsynthetic dataset incorporating features from numerous spectroscopy techniques.\nCritically, the development of innovative, task-specific loss functions-such as\nthe vicinity peak response (ViPeR) for peak localization-enabled the creation\nof compact yet highly accurate models from this dataset, validated with\nexperimental data from Raman, UV-vis, and fluorescence spectroscopy. OASIS\ndemonstrates significant potential for applications including in situ\nexperiments, high-throughput optimization, and online monitoring. This study\nunderscores the optimization of the loss function as a key resource-efficient\nstrategy to develop high-performance ML models.", "published": "2025-09-15 01:28:51", "link": "http://arxiv.org/abs/2509.11499v1", "categories": ["cs.LG", "physics.data-an"], "primary_category": "cs.LG"}
{"title": "Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks", "abstract": "Drug repurposing has historically been an economically infeasible process for\nidentifying novel uses for abandoned drugs. Modern machine learning has enabled\nthe identification of complex biochemical intricacies in candidate drugs;\nhowever, many studies rely on simplified datasets with known drug-disease\nsimilarities. We propose a machine learning pipeline that uses unsupervised\ndeep embedded clustering, combined with supervised graph neural network link\nprediction to identify new drug-disease links from multi-omic data.\nUnsupervised autoencoder and cluster training reduced the dimensionality of\nomic data into a compressed latent embedding. A total of 9,022 unique drugs\nwere partitioned into 35 clusters with a mean silhouette score of 0.8550. Graph\nneural networks achieved strong statistical performance, with a prediction\naccuracy of 0.901, receiver operating characteristic area under the curve of\n0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster link\nprobabilities exceeding 99 percent was generated. This study could provide new\ndrug-disease link prospects across unrelated disease domains, while advancing\nthe understanding of machine learning in drug repurposing studies.", "published": "2025-09-15 01:04:37", "link": "http://arxiv.org/abs/2509.11493v1", "categories": ["cs.LG", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Preconditioned subgradient method for composite optimization: overparameterization and fast convergence", "abstract": "Composite optimization problems involve minimizing the composition of a\nsmooth map with a convex function. Such objectives arise in numerous data\nscience and signal processing applications, including phase retrieval, blind\ndeconvolution, and collaborative filtering. The subgradient method achieves\nlocal linear convergence when the composite loss is well-conditioned. However,\nif the smooth map is, in a certain sense, ill-conditioned or overparameterized,\nthe subgradient method exhibits much slower sublinear convergence even when the\nconvex function is well-conditioned. To overcome this limitation, we introduce\na Levenberg-Morrison-Marquardt subgradient method that converges linearly under\nmild regularity conditions at a rate determined solely by the convex function.\nFurther, we demonstrate that these regularity conditions hold for several\nproblems of practical interest, including square-variable formulations, matrix\nsensing, and tensor factorization. Numerical experiments illustrate the\nbenefits of our method.", "published": "2025-09-15 00:26:47", "link": "http://arxiv.org/abs/2509.11486v1", "categories": ["math.OC", "cs.LG", "stat.ML", "65K05, 65K10, 90C30, 90C06, 68U15"], "primary_category": "math.OC"}
{"title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings", "abstract": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.", "published": "2025-09-15 11:16:04", "link": "http://arxiv.org/abs/2509.11787v1", "categories": ["cs.SE", "cs.MA"], "primary_category": "cs.SE"}
{"title": "Nash Equilibrium and Belief Evolution in Differential Games", "abstract": "This study investigates differential games with motion-payoff uncertainty in\ncontinuous-time settings. We propose a framework where players update their\nbeliefs about uncertain parameters using continuous Bayesian updating.\nTheoretical proofs leveraging key probability theorems demonstrate that\nplayers' beliefs converge to the true parameter values, ensuring stability and\naccuracy in long-term estimations. We further derive Nash Equilibrium\nstrategies with continuous Bayesian updating for players, emphasizing the role\nof belief updates in decision-making processes. Additionally, we establish the\nconvergence of Nash Equilibrium strategies with continuous Bayesian updating.\nThe efficacy of both continuous and dynamic Bayesian updating is examined in\nthe context of pollution control games, showing convergence in players'\nestimates under small time intervals in discrete scenarios.", "published": "2025-09-15 09:41:18", "link": "http://arxiv.org/abs/2509.11739v1", "categories": ["cs.MA", "cs.GT"], "primary_category": "cs.MA"}
{"title": "Contractive kinetic Langevin samplers beyond global Lipschitz continuity", "abstract": "In this paper, we examine the problem of sampling from log-concave\ndistributions with (possibly) superlinear gradient growth under kinetic\n(underdamped) Langevin algorithms. Using a carefully tailored taming scheme, we\npropose two novel discretizations of the kinetic Langevin SDE, and we show that\nthey are both contractive and satisfy a log-Sobolev inequality. Building on\nthis, we establish a series of non-asymptotic bounds in $2$-Wasserstein\ndistance between the law reached by each algorithm and the underlying target\nmeasure.", "published": "2025-09-15 15:14:45", "link": "http://arxiv.org/abs/2509.12031v1", "categories": ["math.PR", "cs.NA", "math.NA", "stat.ML", "math.PR, math.NA"], "primary_category": "math.PR"}
{"title": "RJD-BASE: Multi-Modal Spectral Clustering via Randomized Joint Diagonalization", "abstract": "We revisit the problem of spectral clustering in multimodal settings, where\neach data modality is encoded as a graph Laplacian. While classical\napproaches--including joint diagonalization, spectral co-regularization, and\nmultiview clustering--attempt to align embeddings across modalities, they often\nrely on costly iterative refinement and may fail to directly target the\nspectral subspace relevant for clustering. In this work, we introduce two key\ninnovations. First, we bring the power of randomization to this setting by\nsampling random convex combinations of Laplacians as a simple and scalable\nalternative to explicit eigenspace alignment. Second, we propose a principled\nselection rule based on Bottom-$k$ Aggregated Spectral Energy (BASE)--a\n$k$-dimensional extension of the directional smoothness objective from recent\nminimax formulations--which we uniquely apply as a selection mechanism rather\nthan an optimization target. The result is Randomized Joint Diagonalization\nwith BASE Selection (RJD-BASE), a method that is easily implementable,\ncomputationally efficient, aligned with the clustering objective, and grounded\nin decades of progress in standard eigensolvers. Through experiments on\nsynthetic and real-world datasets, we show that RJD-BASE reliably selects\nhigh-quality embeddings, outperforming classical multimodal clustering methods\nat low computational cost.", "published": "2025-09-15 14:27:08", "link": "http://arxiv.org/abs/2509.11981v1", "categories": ["math.NA", "cs.NA", "eess.SP"], "primary_category": "math.NA"}
{"title": "Adaptive least-squares space-time finite element methods for convection-diffusion problems", "abstract": "In this paper we formulate and analyse adaptive (space-time) least-squares\nfinite element methods for the solution of convection-diffusion equations. The\nconvective derivative $\\mathbf{v} \\cdot \\nabla u$ is considered as part of the\ntotal time derivative $\\frac{d}{dt}u = \\partial_t u + \\mathbf{v} \\cdot \\nabla\nu$, and therefore we can use a rather standard stability and error analysis for\nrelated space-time finite element methods. For stationary problems we restrict\nthe ansatz space $H^1_0(\\Omega)$ such that the convective derivative is\nconsidered as an element of the dual $H^{-1}(\\Omega)$ of the test space\n$H^1_0(\\Omega)$, which also allows unbounded velocities $\\mathbf{v}$. While the\ndiscrete finite element schemes are always unique solvable, the numerical\nsolutions may suffer from a bad approximation property of the finite element\nspace when considering convection dominated problems, i.e., small diffusion\ncoefficients. Instead of adding suitable stabilization terms, we aim to resolve\nthe solutions by using adaptive (space-time) finite element methods. For this\nwe introduce a least-squares approach where the discrete adjoint defines local\na posteriori error indicators to drive an adaptive scheme. Numerical examples\nillustrate the theoretical considerations.", "published": "2025-09-15 14:12:17", "link": "http://arxiv.org/abs/2509.11955v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "X-ray imaging from nonlinear waves: numerical reconstruction of a cubic nonlinearity", "abstract": "We study an inverse boundary value problem for the nonlinear wave equation in\n$2 + 1$ dimensions. The objective is to recover an unknown potential $q(x, t)$\nfrom the associated Dirichlet-to-Neumann map using real-valued waves. We\npropose a direct numerical reconstruction method for the Radon transform of\n$q$, which can then be inverted using standard X-ray tomography techniques to\ndetermine $q$. Our implementation introduces a spectral regularization\nprocedure to stabilize the numerical differentiation step required in the\nreconstruction, improving robustness with respect to noise in the boundary\ndata. We also give rigorous justification and stability estimates for the\nregularized spectral differentiation of noisy measurements. A direct pointwise\nreconstruction method for $q$ is also implemented for comparison. Numerical\nexperiments demonstrate the feasibility of recovering potentials from boundary\nmeasurements of nonlinear waves and illustrate the advantages of the\nRadon-based reconstruction.", "published": "2025-09-15 14:08:56", "link": "http://arxiv.org/abs/2509.11951v1", "categories": ["math.NA", "cs.NA", "math.AP", "65M32, 35R30, 35L05"], "primary_category": "math.NA"}
{"title": "A Chebyshev--Ritz Spectral Framework for Nonlinear Vibration of CNT-Reinforced Composite Beams", "abstract": "This study develops a spectral Ritz formulation for the nonlinear free\nvibration analysis of carbon nanotube-reinforced composite (CNTRC) beams.\nBoundary-adapted Chebyshev basis functions are constructed to exactly satisfy\nclamped and simply supported boundary conditions. The governing equations\nincorporate von~K\\'{a}rm\\'{a}n geometric nonlinearity, while the effective\nmaterial properties for both uniform and functionally graded (FG) CNT\ndistributions are evaluated using a modified rule of mixtures. Discretization\nvia the Chebyshev-Ritz approach produces a reduced-order model exhibiting\nexponential convergence; for basis sizes $N \\geq 12$, the fundamental frequency\nerror remains below $0.1\\%$ relative to published benchmarks.\n  Computational results demonstrate substantial efficiency gains, with the\nspectral approach requiring significantly less time than high-fidelity finite\nelement discretizations of comparable accuracy. Parametric studies reveal that\nthe fundamental frequency increases with CNT volume fraction and is sensitive\nto the interfacial load-transfer efficiency parameter $\\eta_E$. Selected FG\npatterns are shown to enhance stiffness relative to uniformly distributed CNTs.\n  Validation against established numerical benchmarks yields relative\ndifferences of only a few percent. The current limitation of the method is its\nreliance on the Euler-Bernoulli beam assumption, which neglects transverse\nshear deformation and damping; addressing these effects is proposed for future\nwork. All numerical data and scripts are provided as supplementary material to\nensure reproducibility.", "published": "2025-09-15 14:05:03", "link": "http://arxiv.org/abs/2509.11946v1", "categories": ["math.NA", "cs.NA", "math.DS", "74H45, 74S30, 74E30, 70K30, 74K10"], "primary_category": "math.NA"}
{"title": "Numerical analysis of fluid estimation for source terms in neutral particles simulation", "abstract": "In plasma edge simulations, kinetic Monte Carlo (MC) is often used to\nsimulate neutral particles and estimate source terms. For large-sized reactors,\nlike ITER and DEMO, high particle collision rates lead to a substantial\ncomputational cost for such schemes. To address this challenge, an\nasymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method\nand a corresponding fluid estimation technique have been proposed in the\nliterature. In this work, we perform numerical analysis on the convergence of\nKDMC with the fluid estimation. To do so, we compare the accuracy of the\nanalyzed algorithm with the accuracy of an approximate fluid method using the\nkinetic MC method as a reference. In a one-dimensional test case, KDMC with the\nfluid estimation achieves at least one order of magnitude lower errors than the\nfluid method for both high- and low-collisional regimes. Moreover, KDMC with\nthe fluid estimation outperforms the kinetic MC method with a clear speed-up.\nOverall, our analysis confirms the effectiveness of the discussed algorithm.", "published": "2025-09-15 13:01:08", "link": "http://arxiv.org/abs/2509.11883v1", "categories": ["cs.CE", "cs.NA", "math.NA"], "primary_category": "cs.CE"}
{"title": "Numerical Approximation of the logarithmic Laplacian via sinc-basis", "abstract": "In recent works, the authors of this chapter have shown with co-authors how a\nbasis consisting of dilated and shifted $\\text{sinc}$-functions can be used to\nsolve fractional partial differential equations. As a model problem, the\nfractional Dirichlet problem with homogeneous exterior value conditions was\nsolved. In this work, we briefly recap the algorithms developed there and that\n-- from a computational point of view -- they can be used to solve nonlocal\nequations given through different operators as well. As an example, we\nnumerically solve the Dirichlet problem for the logarithmic Laplacian\n$\\log(-\\Delta)$ which has the Fourier symbol $\\log(\\left|\\omega\\right|^2)$ and\ncompute its Eigenvalues on disks with different radii in $\\mathbb R^2$.", "published": "2025-09-15 08:46:34", "link": "http://arxiv.org/abs/2509.11693v1", "categories": ["math.NA", "cs.NA", "35R11, 65N35", "G.1"], "primary_category": "math.NA"}
{"title": "Linear and Nonlinear Boundary Conditions: What's the difference?", "abstract": "In previous work, we derived new energy and entropy stable open boundary\nconditions and implementation procedures for linear and nonlinear initial\nboundary value problems. These boundary procedures results in estimates bounded\nby external data only. Interestingly, these new boundary conditions generalize\nthe well known classical characteristic boundary conditions for linear problems\nto the nonlinear setting. We discuss the similarities and differences between\nthese two boundary procedures and point out the advantages with the new\nprocedures. In particular we show that the new boundary conditions bound both\nlinear and nonlinear initial boundary value problems and can be implemented\nboth strongly and weakly.", "published": "2025-09-15 07:40:28", "link": "http://arxiv.org/abs/2509.11651v1", "categories": ["math.NA", "cs.NA", "35M22, 35L04"], "primary_category": "math.NA"}
{"title": "Strong convergence rates of stochastic theta methods for index 1 stochastic differential algebraic equations under non-globally Lipschitz conditions", "abstract": "This work investigates numerical approximations of index 1 stochastic\ndifferential algebraic equations (SDAEs) with non-constant singular matrices\nunder non-global Lipschitz conditions. Analyzing the strong convergence rates\nof numerical solutions in this setting is highly nontrivial, due to both the\nsingularity of the constraint matrix and the superlinear growth of the\ncoefficients. To address these challenges, we develop an approach for\nestablishing mean square convergence rates of numerical methods for SDAEs under\nglobal monotonicity conditions. Specifically, we prove that each stochastic\ntheta method with $\\theta \\in [\\frac{1}{2},1]$ achieves a mean square\nconvergence rate of order $\\frac{1}{2}$. Theoretical findings are further\nvalidated through a series of numerical experiments.", "published": "2025-09-15 06:21:47", "link": "http://arxiv.org/abs/2509.11618v1", "categories": ["math.NA", "cs.NA", "60H10, 65C20, 65L20"], "primary_category": "math.NA"}
{"title": "Neural solver for sixth-order ordinary differential equations", "abstract": "A method for approximating sixth-order ordinary differential equations is\nproposed, which utilizes a deep learning feedforward artificial neural network,\nreferred to as a neural solver. The efficacy of this unsupervised machine\nlearning method is demonstrated through the solution of two distinct boundary\nvalue problems (BVPs), with the method being extended to include the solution\nof a sixth-order ordinary differential equation (ODE). The proposed mean\nsquared loss function is comprised of two terms: the differential equation is\nsatisfied by the first term, while the initial or boundary conditions are\nsatisfied by the second. The total loss function is minimized using a\nquasi-Newton optimization method to obtain the desired network output. The\napproximation capability of the proposed method is verified for sixth-order\nODEs. Point-wise comparisons of the approximations show strong agreement with\navailable exact solutions. The proposed algorithm minimizes the overall\nlearnable network hyperparameters in a given BVP. Simple minimization of the\ntotal loss function yields highly accurate results even with a low number of\nepochs. Therefore, the proposed framework offers an attractive setting for the\ncomputational mathematics community.", "published": "2025-09-15 03:16:20", "link": "http://arxiv.org/abs/2509.11541v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Convergence of a Second-Order Projection Method to Leray-Hopf Solutions of the Incompressible Navier-Stokes Equations", "abstract": "We analyze a second-order projection method for the incompressible\nNavier-Stokes equations on bounded Lipschitz domains. The scheme employs a\nBackward Differentiation Formula of order two (BDF2) for the time\ndiscretization, combined with conforming finite elements in space. Projection\nmethods are widely used to enforce incompressibility, yet rigorous convergence\nresults for possibly non-smooth solutions have so far been restricted to\nfirst-order schemes. We establish, for the first time, convergence (up to\nsubsequence) of a second-order projection method to Leray-Hopf weak solutions\nunder minimal assumptions on the data, namely $u_0 \\in\nL^2_{\\text{div}}(\\Omega)$ and $f \\in L^2(0,T;L^2_{\\text{div}}(\\Omega))$. Our\nanalysis relies on two ingredients: A discrete energy inequality providing\nuniform $L^{\\infty}(0,T;L^2(\\Omega))$ and $L^2(0,T;H^1_0(\\Omega))$ bounds for\nsuitable interpolants of the discrete velocities, and a compactness argument\ncombining Simon's theorem with refined time-continuity estimates. These tools\novercome the difficulty that only the projected velocity satisfies an\napproximate divergence-free condition, while the intermediate velocity is\ncontrolled in space. We conclude that a subsequence of the approximations\nconverges to a Leray-Hopf weak solution. This result provides the first\nrigorous convergence proof for a higher-order projection method under no\nadditional assumptions on the solution beyond those following from the standard\na priori energy estimate.", "published": "2025-09-15 00:11:40", "link": "http://arxiv.org/abs/2509.11483v1", "categories": ["math.NA", "cs.NA", "math.AP", "65M12, 65M60, 76D05, 76M10"], "primary_category": "math.NA"}
{"title": "Meta-Learning Neural Process for Implied Volatility Surfaces with SABR-induced Priors", "abstract": "Constructing the implied volatility surface (IVS) is reframed as a\nmeta-learning problem training across trading days to learn a general process\nthat reconstructs a full IVS from few quotes, eliminating daily recalibration.\nWe introduce the Volatility Neural Process, an attention-based model that uses\na two-stage training: pre-training on SABR-generated surfaces to encode a\nfinancial prior, followed by fine-tuning on market data. On S&P 500 options\n(2006-2023; out-of-sample 2019-2023), our model outperforms SABR, SSVI,\nGaussian Process, and an ablation trained only on real data. Relative to the\nablation, the SABR-induced prior reduces RMSE by about 40% and dominates in\nmid- and long-maturity regions where quotes are sparse. The learned prior\nsuppresses large errors, providing a practical, data-efficient route to stable\nIVS construction with a single deployable model.", "published": "2025-09-15 13:45:31", "link": "http://arxiv.org/abs/2509.11928v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Group Survival Probability under Contagion in Microlending", "abstract": "In this paper we apply a probabilistic approach to analyze the impact of\ncontagious default among investment group members. A general formula is given\nto compute the group survival probability with the presence of contagion\neffect. Special cases of this probability model are examined. In particular, we\nshow that if the investment group is homogeneous, defined in the paper, then\nincluding more members into the group will eventually lead to default with\nprobability 1, contrasting with the non-contagious scenario, where the default\nprobability increases monotonically with respect to the group size. Also, we\nprovide an upper bound of the optimal group size under the homogeneous setup;\nso, one can run a linear search with finite time to locate this optimizer.", "published": "2025-09-15 04:53:22", "link": "http://arxiv.org/abs/2509.11579v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Bootstrapping Liquidity in BTC-Denominated Prediction Markets", "abstract": "Prediction markets have gained adoption as on-chain mechanisms for\naggregating information, with platforms such as Polymarket demonstrating demand\nfor stablecoin-denominated markets. However, denominating in\nnon-interest-bearing stablecoins introduces inefficiencies: participants face\nopportunity costs relative to the fiat risk-free rate, and Bitcoin holders in\nparticular lose exposure to BTC appreciation when converting into stablecoins.\nThis paper explores the case for prediction markets denominated in Bitcoin,\ntreating BTC as a deflationary settlement asset analogous to gold under the\nclassical gold standard. We analyse three methods of supplying liquidity to a\nnewly created BTC-denominated prediction market: cross-market making against\nexisting stablecoin venues, automated market making, and DeFi-based redirection\nof user trades. For each approach we evaluate execution mechanics, risks\n(slippage, exchange-rate risk, and liquidation risk), and capital efficiency.\nOur analysis shows that cross-market making provides the most user-friendly\nrisk profile, though it requires active professional makers or\nplatform-subsidised liquidity. DeFi redirection offers rapid bootstrapping and\nreuse of existing USDC liquidity, but exposes users to liquidation thresholds\nand exchange-rate volatility, reducing capital efficiency. Automated market\nmaking is simple to deploy but capital-inefficient and exposes liquidity\nproviders to permanent loss. The results suggest that BTC-denominated\nprediction markets are feasible, but their success depends critically on the\nchoice of liquidity provisioning mechanism and the trade-off between user\nsafety and deployment convenience.", "published": "2025-09-15 14:42:17", "link": "http://arxiv.org/abs/2509.11990v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Sentiment Feedback in Equity Markets: Asymmetries, Retail Heterogeneity, and Structural Calibration", "abstract": "We study how sentiment shocks propagate through equity returns and investor\nclientele using four independent proxies with sign-aligned kappa-rho\nparameters. A structural calibration links a one standard deviation innovation\nin sentiment to a pricing impact of 1.06 basis points with persistence\nparameter rho = 0.940, yielding a half-life of 11.2 months. The impulse\nresponse peaks around the 12-month horizon, indicating slow-moving\namplification. Cross-sectionally, a simple D10-D1 portfolio earns 4.0 basis\npoints per month with Sharpe ratios of 0.18-0.85, consistent with tradable\nexposure to the sentiment factor. Three regularities emerge: (i) positive\nsentiment innovations transmit more strongly than negative shocks\n(amplification asymmetry); (ii) effects are concentrated in retail-tilted and\nnon-optionable stocks (clientele heterogeneity); and (iii) responses are\nstate-dependent across volatility regimes - larger on impact in high-VIX months\nbut more persistent in low-VIX months. Baseline time-series fits are\nparsimonious (R2 ~ 0.001; 420 monthly observations), yet the calibrated\ndynamics reconcile modest impact estimates with sizable long-short payoffs.\nConsistent with Miller (1977), a one standard deviation sentiment shock has\n1.72-8.69 basis points larger effects in low-breadth stocks across horizons of\n1-12 months, is robust to institutional flows, and exhibits volatility state\ndependence - larger on impact but less persistent in high-VIX months, smaller\non impact but more persistent in low-VIX months.", "published": "2025-09-15 14:22:04", "link": "http://arxiv.org/abs/2509.11970v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "A comparison between geostatistical and machine learning models for spatio-temporal prediction of PM2.5 data", "abstract": "Ambient air pollution poses significant health and environmental challenges.\nExposure to high concentrations of PM$_{2.5}$ have been linked to increased\nrespiratory and cardiovascular hospital admissions, more emergency department\nvisits and deaths. Traditional air quality monitoring systems such as\nEPA-certified stations provide limited spatial and temporal data. The advent of\nlow-cost sensors has dramatically improved the granularity of air quality data,\nenabling real-time, high-resolution monitoring. This study exploits the\nextensive data from PurpleAir sensors to assess and compare the effectiveness\nof various statistical and machine learning models in producing accurate hourly\nPM$_{2.5}$ maps across California. We evaluate traditional geostatistical\nmethods, including kriging and land use regression, against advanced machine\nlearning approaches such as neural networks, random forests, and support vector\nmachines, as well as ensemble model. Our findings enhanced the predictive\naccuracy of PM2.5 concentration by correcting the bias in PurpleAir data with\nan ensemble model, which incorporating both spatiotemporal dependencies and\nmachine learning models.", "published": "2025-09-15 15:32:57", "link": "http://arxiv.org/abs/2509.12051v1", "categories": ["stat.AP", "stat.ML"], "primary_category": "stat.AP"}
{"title": "EEND-SAA: Enrollment-Less Main Speaker Voice Activity Detection Using Self-Attention Attractors", "abstract": "Voice activity detection (VAD) is essential in speech-based systems, but\ntraditional methods detect only speech presence without identifying speakers.\nTarget-speaker VAD (TS-VAD) extends this by detecting the speech of a known\nspeaker using a short enrollment utterance, but this assumption fails in\nopen-domain scenarios such as meetings or customer service calls, where the\nmain speaker is unknown. We propose EEND-SAA, an enrollment-less,\nstreaming-compatible framework for main-speaker VAD, which identifies the\nprimary speaker without prior knowledge. Unlike TS-VAD, our method determines\nthe main speaker as the one who talks more steadily and clearly, based on\nspeech continuity and volume. We build our model on EEND using two\nself-attention attractors in a Transformer and apply causal masking for\nreal-time use. Experiments on multi-speaker LibriSpeech mixtures show that\nEEND-SAA reduces main-speaker DER from 6.63% to 3.61% and improves F1 from\n0.9667 to 0.9818 over the SA-EEND baseline, achieving state-of-the-art\nperformance under conditions involving speaker overlap and noise.", "published": "2025-09-15 14:13:26", "link": "http://arxiv.org/abs/2509.11957v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Meta Fluid Antenna: Architecture Design, Performance Analysis, Experimental Examination", "abstract": "Fluid antenna systems (FAS) have recently emerged as a promising solution for\nsixth-generation (6G) ultra-dense connectivity. These systems utilize dynamic\nradiating and/or shaping techniques to mitigate interference and improve\nspectral efficiency without relying on channel state information (CSI). The\nreported improvements achieved by employing a single dynamically activated\nradiating position in fluid antenna multiple access (FAMA) are significant. To\nfully realize the potential of FAMA in multi-user multiplexing, we propose\nleveraging the unique fast-switching capabilities of a single radio-frequency\n(RF)-chain meta-fluid antenna structure to achieve multi-activation. This\nallows for a significantly larger set of independent radiating states without\nrequiring additional signal processing. Simulations demonstrate that\nmulti-activation FAMA enables robust multi-user multiplexing with a higher\nsignal-to-interference ratio (SIR) under various Rayleigh-fading environments\ncompared to other single RF-chain technologies. We further show that the SIR\ncan be optimized within a 15~$\\mu s$ timeframe under a multi-user\nRayleigh-fading channel, making the proposed scheme highly suitable for\nfast-changing wireless environments. Verified through the theoretical Jakes'\nmodel, full three-dimensional (3D) electromagnetic (EM) simulations and\nexperimental validation, multi-activation FAMA enables effective CSI-free,\nmulti-user communication, offering a scalable solution for high-capacity\nwireless networks.", "published": "2025-09-15 15:15:24", "link": "http://arxiv.org/abs/2509.12032v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optimized Sparse Network Coverage via L1-norm Minimization", "abstract": "The selection of nodes that can serve as cluster heads, local sinks and\ngateways is a critical challenge in distributed sensor and communication\nnetworks. This paper presents a novel framework for identifying a minimal set\nof nexus nodes to ensure full network coverage while minimizing cost. By\nformulating the problem as a convex relaxation of the NP-hard set cover\nproblem, we integrate the graph theoretic centrality measures of node degree\nand betweenness centrality into a cost function optimized via a relaxed L1-norm\nminimization. The proposed approach is applicable to static and dynamic network\nscenarios and does not require location or distance estimation. Through\nsimulations across various graph models and dynamic conditions, it is shown\nthat the method achieves faster execution times (lower complexity) and\ncompetitive sparsity compared to classical greedy and genetic algorithms (GA),\noffering a robust, distributed, and cost-efficient node selection solution.", "published": "2025-09-15 14:43:52", "link": "http://arxiv.org/abs/2509.11994v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Multi-Stage Location Optimization Through Power Delay Profile Alignment Using Site-Specific Wireless Ray Tracing", "abstract": "Ray tracing (RT) simulations require accurate transmitter (TX) and receiver\n(RX) location information from real-world measurements to accurately\ncharacterize wireless propagation behavior in an environment. Such wireless\npropagation measurements typically employ GPS-based logging for TX/RX\nlocations, which can produce meter-level errors that lead to unreliable RT\ncalibration and validation. These location misalignments cause inaccurate\ninteractions between RT-generated multipath components (MPCs) and the modeled\n3D environment, which lead to erroneous channel predictions, and severe\ndiscrepancies between simulated and measured power delay profiles (PDPs) and\nchannel characteristics. Moreover, the same RT-generated PDPs using inaccurate\nlocations result in calibration errors when adjusting material properties such\nas conductivity and permittivity.\n  This paper presents a systematic multi-stage TX/RX location calibration\nframework to correct location errors and consequently align measured and\nsimulated omnidirectional PDPs.\n  Optimization is performed using a computationally efficient multi-stage grid\nsearch and the Powell method. Applying the location calibration framework to\nNYU WIRELESS urban-microcell (UMi) measurements at 6.75 GHz and 16.95 GHz\ncorrected TX/RX location errors of up to 7 m. The framework reduced the\ncomposite loss function by 42.3\\% for line-of-sight (LOS) and 13.5\\% for\nnon-line-of-sight (NLOS) scenarios. Furthermore, peak power prediction accuracy\nimproved by approximately 1 dB on average. Such improved geometric alignment\nenables accurate channel prediction, vital for beam management and\ninfrastructure deployment for next-generation wireless networks.", "published": "2025-09-15 13:38:18", "link": "http://arxiv.org/abs/2509.11923v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "WAFER: A new method to retrieve sun-induced fluorescence based on spectral wavelet decompositions", "abstract": "Sun-induced fluorescence (SIF) as a close remote sensing based proxy for\nphotosynthesis is accepted as a useful measure to remotely monitor vegetation\nhealth and gross primary productivity. In this work we present the new\nretrieval method WAFER (WAvelet decomposition FluorEscence Retrieval) based on\nwavelet decompositions of the measured spectra of reflected radiance as well as\na reference radiance not containing fluorescence. By comparing absolute\nabsorption line depths by means of the corresponding wavelet coefficients, a\nrelative reflectance is retrieved independently of the fluorescence, i.e.\nwithout introducing a coupling between reflectance and fluorescence. The\nfluorescence can then be derived as the remaining offset. This method can be\napplied to arbitrary chosen wavelength windows in the whole spectral range,\nsuch that all the spectral data available is exploited, including the\nseparation into several frequency (i.e. width of absorption lines) levels and\nwithout the need of extensive training datasets. At the same time, the\nassumptions about the reflectance shape are minimal and no spectral shape\nassumptions are imposed on the fluorescence, which not only avoids biases\narising from wrong or differing fluorescence models across different spatial\nscales and retrieval methods but also allows for the exploration of this\nspectral shape for different measurement setups. WAFER is tested on a synthetic\ndataset as well as several diurnal datasets acquired with a field spectrometer\n(FloX) over an agricultural site. We compare the WAFER method to two\nestablished retrieval methods, namely the improved Fraunhofer line\ndiscrimination (iFLD) method and spectral fitting method (SFM) and find a good\nagreement with the added possibility of exploring the true spectral shape of\nthe offset signal and free choice of the retrieval window. (abbreviated)", "published": "2025-09-15 12:14:15", "link": "http://arxiv.org/abs/2509.11829v1", "categories": ["physics.geo-ph", "eess.SP"], "primary_category": "physics.geo-ph"}
{"title": "Attention-Enhanced Learning for Sensing-Assisted Long-Term Beam Tracking in mmWave Communications", "abstract": "Beam training and prediction in millimeter-wave communications are highly\nchallenging due to fast time-varying channels and sensitivity to blockages and\nmobility. In this context, infrastructure-mounted cameras can capture rich\nenvironmental information that can facilitate beam tracking design. In this\nwork, we develop an efficient attention-enhanced machine learning model for\nlong-term beam tracking built upon convolutional neural networks and gated\nrecurrent units to predict both current and future beams from past observed\nimages. The integrated temporal attention mechanism substantially improves its\npredictive performance. Numerical results demonstrate that the proposed design\nachieves Top-5 beam prediction accuracies exceeding 90% across both current and\nsix future time slots, significantly reducing overhead arising from sensing and\nprocessing for beam training. It further attains 97% of state-of-the-art\nperformance with only 3% of the computational complexity.", "published": "2025-09-15 09:28:02", "link": "http://arxiv.org/abs/2509.11725v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Low-Altitude Wireless Networks: A Survey", "abstract": "The rapid development of the low-altitude economy has imposed unprecedented\ndemands on wireless infrastructure to accommodate large-scale drone deployments\nand facilitate intelligent services in dynamic airspace environments. However,\nunlocking its full potential in practical applications presents significant\nchallenges. Traditional aerial systems predominantly focus on air-ground\ncommunication services, often neglecting the integration of sensing,\ncomputation, control, and energy-delivering functions, which hinders the\nability to meet diverse mission-critical demands. Besides, the absence of\nsystematic low-altitude airspace planning and management exacerbates issues\nregarding dynamic interference in three-dimensional space, coverage\ninstability, and scalability. To overcome these challenges, a comprehensive\nframework, termed low-altitude wireless network (LAWN), has emerged to\nseamlessly integrate communication, sensing, computation, control, and air\ntraffic management into a unified design. This article provides a comprehensive\noverview of LAWN systems, introducing LAWN system fundamentals and the\nevolution of functional designs. Subsequently, we delve into performance\nevaluation metrics and review critical concerns surrounding privacy and\nsecurity in the open-air network environment. Finally, we present the\ncutting-edge developments in airspace structuring and air traffic management,\nproviding insights to facilitate the practical deployment of LAWNs.", "published": "2025-09-15 05:57:34", "link": "http://arxiv.org/abs/2509.11607v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RadioLAM: A Large AI Model for Fine-Grained 3D Radio Map Estimation", "abstract": "A radio map captures the spatial distribution of wireless channel parameters,\nsuch as the strength of the signal received, across a geographic area. The\nproblem of fine-grained three-dimensional (3D) radio map estimation involves\ninferring a high-resolution radio map for the two-dimensional (2D) area at an\narbitrary target height within a 3D region of interest, using radio samples\ncollected by sensors sparsely distributed in that 3D region. Solutions to the\nproblem are crucial for efficient spectrum management in 3D spaces,\nparticularly for drones in the rapidly developing low-altitude economy.\nHowever, this problem is challenging due to ultra-sparse sampling, where the\nnumber of collected radio samples is far fewer than the desired resolution of\nthe radio map to be estimated. In this paper, we design a Large Artificial\nIntelligence Model (LAM) called RadioLAM for the problem. RadioLAM employs the\ncreative power and the strong generalization capability of LAM to address the\nultra-sparse sampling challenge. It consists of three key blocks: 1) an\naugmentation block, using the radio propagation model to project the radio\nsamples collected at different heights to the 2D area at the target height; 2)\na generation block, leveraging an LAM under an Mixture of Experts (MoE)\narchitecture to generate a candidate set of fine-grained radio maps for the\ntarget 2D area; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best map from the candidate set. Extensive simulations\nshow that RadioLAM is able to solve the fine-grained 3D radio map estimation\nproblem efficiently from an ultra-low sampling rate of 0.1%, and significantly\noutperforms the state-of-the-art.", "published": "2025-09-15 04:34:23", "link": "http://arxiv.org/abs/2509.11571v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Stacked Intelligent Metasurface for End-to-End OFDM System", "abstract": "Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabled\nwave-domain signal processing have emerged as promising research directions for\noffloading baseband digital processing tasks and efficiently simplifying\ntransceiver design. However, existing architectures are limited to employing\nSIM (DPSIM) for a single communication function, such as precoding or\ncombining. To further enhance the overall performance of SIM (DPSIM)-assisted\nsystems and achieve end-to-end (E2E) joint optimization from the transmitted\nbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2E\northogonal frequency-division multiplexing (OFDM) system, where traditional\ncommunication tasks such as modulation, precoding, combining, and demodulation\nare performed simultaneously during electromagnetic (EM) forward propagation.\nFurthermore, inspired by the idea of abstracting real metasurfaces as hidden\nlayers of a neural network, we propose the electromagnetic neural network\n(EMNN) to enable the control of the E2E OFDM communication system. In addition,\ntransfer learning is introduced into the model training, and a training and\ndeployment framework for the EMNN is designed. Simulation results demonstrate\nthat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems can\nachieve robust bitstream transmission under complex channel conditions. Our\nstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2E\nOFDM systems in the design of next-generation transceivers.", "published": "2025-09-15 03:31:41", "link": "http://arxiv.org/abs/2509.11551v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Simplified Design Approach for Via Transitions up to 67 GHz", "abstract": "A systematic approach for high-speed via transition design is proposed. The\neffects of via barrel radius, anti-pad size, and the distance from adjacent\nstitching (GND) vias on bandwidth are analyzed and characterized. Guidelines\nfor selecting parameter values are provided and validated by correlating 3D\nfull-wave FEM simulation results with actual measurements of the coupon board.\nWhen a sufficient number of stitching vias are used, the via structure can be\napproximated as a coaxial transmission line. The proposed methodology builds on\nthis approximation and also considers high-order modes. With this framework,\nengineers can easily optimize design parameters while intuitively understanding\nhow geometry affects bandwidth. This approach also allows engineers with\nlimited access to expensive and computationally intensive 3D FEM tools to\ndesign high bandwidth vias up to 67 GHz.", "published": "2025-09-15 03:21:39", "link": "http://arxiv.org/abs/2509.11542v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Cooperative UAV-mounted RISs-assisted Energy-efficient Communications", "abstract": "Cooperative reconfigurable intelligent surfaces (RISs) are promising\ntechnologies for 6G networks to support a great number of users. Compared with\nthe fixed RISs, the properly deployed RISs may improve the communication\nperformance with less communication energy consumption, thereby improving the\nenergy efficiency. In this paper, we consider a cooperative unmanned aerial\nvehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs\nare carried and enhanced by UAVs to serve multiple ground users (GUs)\nsimultaneously such that achieving the three-dimensional (3D) mobility and\nopportunistic deployment. Specifically, we formulate an energy-efficient\ncommunication problem based on multi-objective optimization framework\n(EEComm-MOF) to jointly consider the beamforming vector of base station (BS),\nthe location deployment and the discrete phase shifts of UAV-RIS system so as\nto simultaneously maximize the minimum available rate over all GUs, maximize\nthe total available rate of all GUs, and minimize the total energy consumption\nof the system, while the transmit power constraint of BS is considered. To\ncomprehensively solve EEComm-MOF which is an NP-hard and non-convex problem\nwith constraints, a non-dominated sorting genetic algorithm-II with a\ncontinuous solution processing mechanism, a discrete solution processing\nmechanism, and a complex solution processing mechanism (INSGA-II-CDC) is\nproposed. Simulations results demonstrate that the proposed INSGA-II-CDC can\nsolve EEComm-MOF effectively and outperforms other benchmarks under different\nparameter settings. Moreover, the stability of INSGA-II-CDC and the\neffectiveness of the improved mechanisms are verified. Finally, the\nimplementability analysis of the algorithm is given.", "published": "2025-09-15 02:56:18", "link": "http://arxiv.org/abs/2509.11533v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Radio Frequency Amplitude-Modulation to Frequency-Modulation Signal Converter", "abstract": "In this project, we wanted to discover an analog topology that could\neffectively convert amplitude-modulated (AM) signals to frequency-modulated\n(FM) signals, while also ensuring that both sets of signals were within their\nrespective radio frequency (RF) bands. To that end, an effective topology for\ndoing so was developed, characterized, and demonstrated, requiring the ability\nto de-modulate incoming signals from the AM radio band--spanning from 530 kHz\nto 1700 kHz--and re-modulate these signals into the FM radio band--spanning\nfrom 88 MHz to 108 MHz. These bands are separated by roughly 86 MHz, presenting\nthe need for the topology to radically alter the incoming frequency before\nre-broadcasting. At its simplest implementation, this required an AM\ndemodulation circuit coupled to a voltage controlled oscillator (VCO).\nTogether, these two circuits translated variations in the incoming envelope\nsignal to variations in the output frequency while still maintaining\nhigh-fidelity audio, similar to how existing radio receiving and broadcasting\nare done. Altogether, the project not only developed a working system but also\nprovided valuable instruction in the design, analysis, and construction of\neffective RF circuits--invaluable to future endeavors within analog\nelectronics.", "published": "2025-09-15 01:45:50", "link": "http://arxiv.org/abs/2509.11510v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "abstract": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.", "published": "2025-09-15 16:33:56", "link": "http://arxiv.org/abs/2509.12108v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MillStone: How Open-Minded Are LLMs?", "abstract": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.", "published": "2025-09-15 14:18:51", "link": "http://arxiv.org/abs/2509.11967v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "abstract": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.", "published": "2025-09-15 03:32:50", "link": "http://arxiv.org/abs/2509.11552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation", "abstract": "Current AI alignment through RLHF follows a single directional paradigm that\nAI conforms to human preferences while treating human cognition as fixed. We\npropose a shift to co-alignment through Bidirectional Cognitive Alignment\n(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,\nrepresentation mapping, and KL-budget constraints for controlled co-evolution.\nIn collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,\nwith 230% better mutual adaptation and 332% better protocol convergence.\nEmergent protocols outperformed handcrafted ones by 84%, while bidirectional\nadaptation unexpectedly improved safety (+23% out-of-distribution robustness).\nThe 46% synergy improvement demonstrates optimal collaboration exists at the\nintersection, not union, of human and AI capabilities, validating the shift\nfrom single-directional to co-alignment paradigms.", "published": "2025-09-15 17:41:16", "link": "http://arxiv.org/abs/2509.12179v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) are susceptible to the implicit\nreasoning risk, wherein innocuous unimodal inputs synergistically assemble into\nrisky multimodal data that produce harmful outputs. We attribute this\nvulnerability to the difficulty of MLLMs maintaining safety alignment through\nlong-chain reasoning. To address this issue, we introduce\nSafe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring\ninterpretable reasoning paths tailored for such a cross-modal challenge. A\nnovel training framework, Safety-aware Reasoning Path Optimization (SRPO), is\nalso designed based on the SSUI dataset to align the MLLM's internal reasoning\nprocess with human safety values. Experimental results show that our\nSRPO-trained models achieve state-of-the-art results on key safety benchmarks,\nincluding the proposed Reasoning Path Benchmark (RSBench), significantly\noutperforming both open-source and top-tier commercial MLLMs.", "published": "2025-09-15 15:40:58", "link": "http://arxiv.org/abs/2509.12060v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning", "abstract": "GBPP is a fast learning based scorer that selects a robot base pose for\ngrasping from a single RGB-D snapshot. The method uses a two stage curriculum:\n(1) a simple distance-visibility rule auto-labels a large dataset at low cost;\nand (2) a smaller set of high fidelity simulation trials refines the model to\nmatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLP\nscores dense grids of candidate poses, enabling rapid online selection without\nfull task-and-motion optimization. In simulation and on a real mobile\nmanipulator, GBPP outperforms proximity and geometry only baselines, choosing\nsafer and more reachable stances and degrading gracefully when wrong. The\nresults offer a practical recipe for data efficient, geometry aware base\nplacement: use inexpensive heuristics for coverage, then calibrate with\ntargeted simulation.", "published": "2025-09-15 05:25:40", "link": "http://arxiv.org/abs/2509.11594v2", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "RailSafeNet: Visual Scene Understanding for Tram Safety", "abstract": "Tram-human interaction safety is an important challenge, given that trams\nfrequently operate in densely populated areas, where collisions can range from\nminor injuries to fatal outcomes. This paper addresses the issue from the\nperspective of designing a solution leveraging digital image processing, deep\nlearning, and artificial intelligence to improve the safety of pedestrians,\ndrivers, cyclists, pets, and tram passengers. We present RailSafeNet, a\nreal-time framework that fuses semantic segmentation, object detection and a\nrule-based Distance Assessor to highlight track intrusions. Using only\nmonocular video, the system identifies rails, localises nearby objects and\nclassifies their risk by comparing projected distances with the standard 1435mm\nrail gauge. Experiments on the diverse RailSem19 dataset show that a\nclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),\nwhile a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated\nat an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore\ndelivers accurate, annotation-light scene understanding that can warn drivers\nbefore dangerous situations escalate. Code available at\nhttps://github.com/oValach/RailSafeNet.", "published": "2025-09-15 16:51:21", "link": "http://arxiv.org/abs/2509.12125v2", "categories": ["cs.CV", "68T45 (Primary), 68T07", "I.4.8"], "primary_category": "cs.CV"}
{"title": "Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI", "abstract": "Multimodal learning has attracted much attention in recent years due to its\nability to effectively utilize data features from a variety of different\nmodalities. Diagnosing the vulnerability of atherosclerotic plaques directly\nfrom carotid 3D MRI images is relatively challenging for both radiologists and\nconventional 3D vision networks. In clinical practice, radiologists assess\npatient conditions using a multimodal approach that incorporates various\nimaging modalities and domain-specific expertise, paving the way for the\ncreation of multimodal diagnostic networks. In this paper, we have developed an\neffective strategy to leverage radiologists' domain knowledge to automate the\ndiagnosis of carotid plaque vulnerability through Variation inference and\nMultimodal knowledge Distillation (VMD). This method excels in harnessing\ncross-modality prior knowledge from limited image annotations and radiology\nreports within training data, thereby enhancing the diagnostic network's\naccuracy for unannotated 3D MRI images. We conducted in-depth experiments on\nthe dataset collected in-house and verified the effectiveness of the VMD\nstrategy we proposed.", "published": "2025-09-15 13:38:35", "link": "http://arxiv.org/abs/2509.11924v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images", "abstract": "Reconstructing 3D face from a single unconstrained image remains a\nchallenging problem due to diverse conditions in unconstrained environments.\nRecently, learning-based methods have achieved notable results by effectively\ncapturing complex facial structures and details across varying conditions.\nConsequently, many existing approaches employ projection-based losses between\ngenerated and input images to constrain model training. However, learning-based\nmethods for 3D face reconstruction typically require substantial amounts of 3D\nfacial data, which is difficult and costly to obtain. Consequently, to reduce\nreliance on labeled 3D face datasets, many existing approaches employ\nprojection-based losses between generated and input images to constrain model\ntraining. Nonetheless, despite these advancements, existing approaches\nfrequently struggle to capture detailed and multi-scale features under diverse\nfacial attributes and conditions, leading to incomplete or less accurate\nreconstructions. In this paper, we propose a Multi-Scale Feature Fusion with\nMulti-Attribute (MSMA) framework for 3D face reconstruction from unconstrained\nimages. Our method integrates multi-scale feature fusion with a focus on\nmulti-attribute learning and leverages a large-kernel attention module to\nenhance the precision of feature extraction across scales, enabling accurate 3D\nfacial parameter estimation from a single 2D image. Comprehensive experiments\non the MICC Florence, Facewarehouse and custom-collect datasets demonstrate\nthat our approach achieves results on par with current state-of-the-art\nmethods, and in some instances, surpasses SOTA performance across challenging\nconditions.", "published": "2025-09-15 10:30:08", "link": "http://arxiv.org/abs/2509.11763v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection", "abstract": "Video Anomaly Detection (VAD) is critical for surveillance and public safety.\nHowever, existing benchmarks are limited to either frame-level or video-level\ntasks, restricting a holistic view of model generalization. This work first\nintroduces a softmax-based frame allocation strategy that prioritizes\nanomaly-dense segments while maintaining full-video coverage, enabling balanced\nsampling across temporal scales. Building on this process, we construct two\ncomplementary benchmarks. The image-based benchmark evaluates frame-level\nreasoning with representative frames, while the video-based benchmark extends\nto temporally localized segments and incorporates an abnormality scoring task.\nExperiments on UCF-Crime demonstrate improvements at both the frame and video\nlevels, and ablation studies confirm clear advantages of anomaly-focused\nsampling over uniform and random baselines.", "published": "2025-09-15 05:48:22", "link": "http://arxiv.org/abs/2509.11605v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework", "abstract": "Despite the remarkable success of Self-Supervised Learning (SSL), its\ngeneralization is fundamentally hindered by Shortcut Learning, where models\nexploit superficial features like texture instead of intrinsic structure. We\nexperimentally verify this flaw within the generative paradigm (e.g., MAE) and\nargue it is a systemic issue also affecting discriminative methods, identifying\nit as the root cause of their failure on unseen domains. While existing methods\noften tackle this at a surface level by aligning or separating domain-specific\nfeatures, they fail to alter the underlying learning mechanism that fosters\nshortcut dependency.To address this at its core, we propose HyGDL (Hybrid\nGenerative-Discriminative Learning Framework), a hybrid framework that achieves\nexplicit content-style disentanglement. Our approach is guided by the\nInvariance Pre-training Principle: forcing a model to learn an invariant\nessence by systematically varying a bias (e.g., style) at the input while\nkeeping the supervision signal constant. HyGDL operates on a single encoder and\nanalytically defines style as the component of a representation that is\northogonal to its style-invariant content, derived via vector projection. This\nis operationalized through a synergistic design: (1) a self-distillation\nobjective learns a stable, style-invariant content direction; (2) an analytical\nprojection then decomposes the representation into orthogonal content and style\nvectors; and (3) a style-conditioned reconstruction objective uses these\nvectors to restore the image, providing end-to-end supervision. Unlike prior\nmethods that rely on implicit heuristics, this principled disentanglement\nallows HyGDL to learn truly robust representations, demonstrating superior\nperformance on benchmarks designed to diagnose shortcut learning.", "published": "2025-09-15 05:28:32", "link": "http://arxiv.org/abs/2509.11598v2", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection", "abstract": "Camouflaged object detection (COD) aims to segment objects that blend into\ntheir surroundings. However, most existing studies overlook the semantic\ndifferences among textual prompts of different targets as well as fine-grained\nfrequency features. In this work, we propose a novel Semantic and Frequency\nGuided Network (SFGNet), which incorporates semantic prompts and\nfrequency-domain features to capture camouflaged objects and improve boundary\nperception. We further design Multi-Band Fourier Module(MBFM) to enhance the\nability of the network in handling complex backgrounds and blurred boundaries.\nIn addition, we design an Interactive Structure Enhancement Block (ISEB) to\nensure structural integrity and boundary details in the predictions. Extensive\nexperiments conducted on three COD benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches. The core code of\nthe model is available at the following link:\nhttps://github.com/winter794444/SFGNetICASSP2026.", "published": "2025-09-15 03:15:31", "link": "http://arxiv.org/abs/2509.11539v2", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News", "abstract": "Financial news plays a critical role in the information diffusion process in\nfinancial markets and is a known driver of stock prices. However, the\ninformation in each news article is not necessarily self-contained, often\nrequiring a broader understanding of the historical news coverage for accurate\ninterpretation. Further, identifying and incorporating the most relevant\ncontextual information presents significant challenges. In this work, we\nexplore the value of historical context in the ability of large language models\nto understand the market impact of financial news. We find that historical\ncontext provides a consistent and significant improvement in performance across\nmethods and time horizons. To this end, we propose an efficient and effective\ncontextualization method that uses a large LM to process the main article,\nwhile a small LM encodes the historical context into concise summary embeddings\nthat are then aligned with the large model's representation space. We explore\nthe behavior of the model through multiple qualitative and quantitative\ninterpretability tests and reveal insights into the value of contextualization.\nFinally, we demonstrate that the value of historical context in model\npredictions has real-world applications, translating to substantial\nimprovements in simulated investment performance.", "published": "2025-09-15 23:51:13", "link": "http://arxiv.org/abs/2509.12519v1", "categories": ["cs.CE", "cs.CL", "q-fin.CP", "I.2.7; J.4"], "primary_category": "cs.CE"}
{"title": "A comparison of pipelines for the translation of a low resource language based on transformers", "abstract": "This work compares three pipelines for training transformer-based neural\nnetworks to produce machine translators for Bambara, a Mand\\`e language spoken\nin Africa by about 14,188,850 people. The first pipeline trains a simple\ntransformer to translate sentences from French into Bambara. The second\nfine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures\nfor French-to-Bambara translation. Models from the first two pipelines were\ntrained with different hyperparameter combinations to improve BLEU and chrF\nscores, evaluated on both test sentences and official Bambara benchmarks. The\nthird pipeline uses language distillation with a student-teacher dual neural\nnetwork to integrate Bambara into a pre-trained LaBSE model, which provides\nlanguage-agnostic embeddings. A BERT extension is then applied to LaBSE to\ngenerate translations. All pipelines were tested on Dokotoro (medical) and\nBayelemagaba (mixed domains). Results show that the first pipeline, although\nsimpler, achieves the best translation accuracy (10% BLEU, 21% chrF on\nBayelemagaba), consistent with low-resource translation results. On the Yiri\ndataset, created for this work, it achieves 33.81% BLEU and 41% chrF.\nInstructor-based models perform better on single datasets than on aggregated\ncollections, suggesting they capture dataset-specific patterns more\neffectively.", "published": "2025-09-15 23:36:49", "link": "http://arxiv.org/abs/2509.12514v1", "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FunAudio-ASR Technical Report", "abstract": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings.", "published": "2025-09-15 23:19:36", "link": "http://arxiv.org/abs/2509.12508v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "abstract": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond.", "published": "2025-09-15 21:47:52", "link": "http://arxiv.org/abs/2509.12476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Language Model Understand Language?", "abstract": "Despite advances in natural language generation and understanding, LM still\nstruggle with fine grained linguistic phenomena such as tense, negation, voice,\nand modality which are the elements central to effective human communication.\nIn the context of the United Nations SDG 4, where linguistic clarity is\ncritical, the deployment of LMs in educational technologies demands careful\nscrutiny. As LMs are increasingly powering applications like tutoring systems,\nautomated grading, and translation, their alignment with human linguistic\ninterpretation becomes essential for effective learning. In this study, we\nconduct a evaluation of SOTA language models across these challenging contexts\nin both English and Bengali. To ensure a structured assessment, we introduce a\nnew Route for Evaluation of Cognitive Inference in Systematic Environments\nguidelines. Our proposed LUCID dataset, composed of carefully crafted sentence\npairs in English and Bengali, specifically challenges these models on critical\naspects of language comprehension, including negation, tense, voice variations.\nWe assess the performance of SOTA models including MISTRAL-SABA-24B,\nLLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard\nmetrics like Pearson correlation, Spearman correlation, and Mean Absolute\nError, as well as novel, linguistically inspired metric the HCE accuracy. The\nHCE accuracy measures how often model predictions fall within one standard\ndeviation of the mean human rating, thus capturing human like tolerance for\nvariability in language interpretation. Our findings highlight Compound-Beta as\nthe most balanced model, consistently achieving high correlations and low MAEs\nacross diverse language conditions. It records the highest Pearson correlation\nin English and demonstrates robust performance on mixed-language data,\nindicating a strong alignment with human judgments in cross lingual scenarios.", "published": "2025-09-15 21:09:09", "link": "http://arxiv.org/abs/2509.12459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning", "abstract": "The effectiveness of in-context learning relies heavily on selecting\ndemonstrations that provide all the necessary information for a given test\ninput. To achieve this, it is crucial to identify and cover fine-grained\nknowledge requirements. However, prior methods often retrieve demonstrations\nbased solely on embedding similarity or generation probability, resulting in\nirrelevant or redundant examples. In this paper, we propose TopicK, a topic\ncoverage-based retrieval framework that selects demonstrations to\ncomprehensively cover topic-level knowledge relevant to both the test input and\nthe model. Specifically, TopicK estimates the topics required by the input and\nassesses the model's knowledge on those topics. TopicK then iteratively selects\ndemonstrations that introduce previously uncovered required topics, in which\nthe model exhibits low topical knowledge. We validate the effectiveness of\nTopicK through extensive experiments across various datasets and both open- and\nclosed-source LLMs. Our source code is available at\nhttps://github.com/WonbinKweon/TopicK_EMNLP2025.", "published": "2025-09-15 21:00:28", "link": "http://arxiv.org/abs/2509.12451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts", "abstract": "The increasing deployment of Large Language Models (LLMs) in healthcare\nnecessitates a rigorous evaluation of their factual reliability. However,\nexisting benchmarks are often limited by narrow domains of data, failing to\ncapture the complexity of real-world medical information. To address this\ncritical gap, we introduce MedFact, a new and challenging benchmark for Chinese\nmedical fact-checking. MedFact comprises 2,116 expert-annotated instances\ncurated from diverse real-world texts, spanning 13 medical specialties, 8\nfine-grained error types, 4 writing styles, and multiple difficulty levels. Its\nconstruction employs a hybrid AI-human framework where iterative expert\nfeedback refines an AI-driven, multi-criteria filtering process, ensuring both\nhigh data quality and difficulty. We conduct a comprehensive evaluation of 20\nleading LLMs, benchmarking their performance on veracity classification and\nerror localization against a human expert baseline. Our results reveal that\nwhile models can often determine if a text contains an error, precisely\nlocalizing it remains a substantial challenge, with even top-performing models\nfalling short of human performance. Furthermore, our analysis uncovers a\nfrequent ``over-criticism'' phenomenon, a tendency for models to misidentify\ncorrect information as erroneous, which is exacerbated by advanced reasoning\ntechniques such as multi-agent collaboration and inference-time scaling. By\nhighlighting these critical challenges for deploying LLMs in medical\napplications, MedFact provides a robust resource to drive the development of\nmore factually reliable and medically aware models.", "published": "2025-09-15 20:46:21", "link": "http://arxiv.org/abs/2509.12440v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition", "abstract": "Understanding user intents from UI interaction trajectories remains a\nchallenging, yet crucial, frontier in intelligent agent development. While\nmassive, datacenter-based, multi-modal large language models (MLLMs) possess\ngreater capacity to handle the complexities of such sequences, smaller models\nwhich can run on-device to provide a privacy-preserving, low-cost, and\nlow-latency user experience, struggle with accurate intent inference. We\naddress these limitations by introducing a novel decomposed approach: first, we\nperform structured interaction summarization, capturing key information from\neach user action. Second, we perform intent extraction using a fine-tuned model\noperating on the aggregated summaries. This method improves intent\nunderstanding in resource-constrained models, even surpassing the base\nperformance of large MLLMs.", "published": "2025-09-15 20:20:30", "link": "http://arxiv.org/abs/2509.12423v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering", "abstract": "Evaluating natural language generation (NLG) systems in the medical domain\npresents unique challenges due to the critical demands for accuracy, relevance,\nand domain-specific expertise. Traditional automatic evaluation metrics, such\nas BLEU, ROUGE, and BERTScore, often fall short in distinguishing between\nhigh-quality outputs, especially given the open-ended nature of medical\nquestion answering (QA) tasks where multiple valid responses may exist. In this\nwork, we introduce MORQA (Medical Open-Response QA), a new multilingual\nbenchmark designed to assess the effectiveness of NLG evaluation metrics across\nthree medical visual and text-based QA datasets in English and Chinese. Unlike\nprior resources, our datasets feature 2-4+ gold-standard answers authored by\nmedical professionals, along with expert human ratings for three English and\nChinese subsets. We benchmark both traditional metrics and large language model\n(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based\napproaches significantly outperform traditional metrics in correlating with\nexpert judgments. We further analyze factors driving this improvement,\nincluding LLMs' sensitivity to semantic nuances and robustness to variability\namong reference answers. Our results provide the first comprehensive,\nmultilingual qualitative study of NLG evaluation in the medical domain,\nhighlighting the need for human-aligned evaluation methods. All datasets and\nannotations will be publicly released to support future research.", "published": "2025-09-15 19:51:57", "link": "http://arxiv.org/abs/2509.12405v1", "categories": ["cs.CL", "68T50 (Primary) 68T45 (Secondary)", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection", "abstract": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.", "published": "2025-09-15 19:26:17", "link": "http://arxiv.org/abs/2509.12385v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "abstract": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework.", "published": "2025-09-15 19:20:21", "link": "http://arxiv.org/abs/2509.12382v1", "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables", "abstract": "As LLMs excel on standard reading comprehension benchmarks, attention is\nshifting toward evaluating their capacity for complex abstract reasoning and\ninference. Literature-based benchmarks, with their rich narrative and moral\ndepth, provide a compelling framework for evaluating such deeper comprehension\nskills. Here, we present MORABLES, a human-verified benchmark built from fables\nand short stories drawn from historical literature. The main task is structured\nas multiple-choice questions targeting moral inference, with carefully crafted\ndistractors that challenge models to go beyond shallow, extractive question\nanswering. To further stress-test model robustness, we introduce adversarial\nvariants designed to surface LLM vulnerabilities and shortcuts due to issues\nsuch as data contamination. Our findings show that, while larger models\noutperform smaller ones, they remain susceptible to adversarial manipulation\nand often rely on superficial patterns rather than true moral reasoning. This\nbrittleness results in significant self-contradiction, with the best models\nrefuting their own answers in roughly 20% of cases depending on the framing of\nthe moral choice. Interestingly, reasoning-enhanced models fail to bridge this\ngap, suggesting that scale - not reasoning ability - is the primary driver of\nperformance.", "published": "2025-09-15 19:06:10", "link": "http://arxiv.org/abs/2509.12371v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exact Coset Sampling for Quantum Lattice Algorithms", "abstract": "We give a simple, fully correct, and assumption-light replacement for the\ncontested \"domain-extension\" in Step 9 of a recent windowed-QFT lattice\nalgorithm with complex-Gaussian windows~\\citep{chen2024quantum}. The published\nStep~9 suffers from a periodicity/support mismatch. We present a pair-shift\ndifference construction that coherently cancels all unknown offsets, produces\nan exact uniform CRT-coset state over $\\mathbb{Z}_{P}$, and then uses the QFT\nto enforce the intended modular linear relation. The unitary is reversible,\nuses $\\mathrm{poly}(\\log M_2)$ gates, and preserves the algorithm's\nasymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice.", "published": "2025-09-15 18:10:28", "link": "http://arxiv.org/abs/2509.12341v1", "categories": ["quant-ph", "cs.CL", "cs.CR"], "primary_category": "quant-ph"}
{"title": "MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch", "abstract": "Recently, embedding resources, including models, benchmarks, and datasets,\nhave been widely released to support a variety of languages. However, the Dutch\nlanguage remains underrepresented, typically comprising only a small fraction\nof the published multilingual resources. To address this gap and encourage the\nfurther development of Dutch embeddings, we introduce new resources for their\nevaluation and generation. First, we introduce the Massive Text Embedding\nBenchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and\nnewly created ones, covering a wide range of tasks. Second, we provide a\ntraining dataset compiled from available Dutch retrieval datasets, complemented\nwith synthetic data generated by large language models to expand task coverage\nbeyond retrieval. Finally, we release a series of E5-NL models compact yet\nefficient embedding models that demonstrate strong performance across multiple\ntasks. We make our resources publicly available through the Hugging Face Hub\nand the MTEB package.", "published": "2025-09-15 18:08:08", "link": "http://arxiv.org/abs/2509.12340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification", "abstract": "Anomaly detection and classification in medical imaging are critical for\nearly diagnosis but remain challenging due to limited annotated data, class\nimbalance, and the high cost of expert labeling. Emerging vision foundation\nmodels such as DINOv2, pretrained on extensive, unlabeled datasets, offer\ngeneralized representations that can potentially alleviate these limitations.\nIn this study, we propose an attention-based global aggregation framework\ntailored specifically for 3D medical image anomaly classification. Leveraging\nthe self-supervised DINOv2 model as a pretrained feature extractor, our method\nprocesses individual 2D axial slices of brain MRIs, assigning adaptive\nslice-level importance weights through a soft attention mechanism. To further\naddress data scarcity, we employ a composite loss function combining supervised\ncontrastive learning with class-variance regularization, enhancing inter-class\nseparability and intra-class consistency. We validate our framework on the ADNI\ndataset and an institutional multi-class headache cohort, demonstrating strong\nanomaly classification performance despite limited data availability and\nsignificant class imbalance. Our results highlight the efficacy of utilizing\npretrained 2D foundation models combined with attention-based slice aggregation\nfor robust volumetric anomaly detection in medical imaging. Our implementation\nis publicly available at https://github.com/Rafsani/DinoAtten3D.git.", "published": "2025-09-15 23:31:40", "link": "http://arxiv.org/abs/2509.12512v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Graph Coloring Below Guarantees via Co-Triangle Packing", "abstract": "In the $\\ell$-Coloring Problem, we are given a graph on $n$ nodes, and tasked\nwith determining if its vertices can be properly colored using $\\ell$ colors.\nIn this paper we study below-guarantee graph coloring, which tests whether an\n$n$-vertex graph can be properly colored using $g-k$ colors, where $g$ is a\ntrivial upper bound such as $n$. We introduce an algorithmic framework that\nbuilds on a packing of co-triangles $\\overline{K_3}$ (independent sets of three\nvertices): the algorithm greedily finds co-triangles and employs a win-win\nanalysis. If many are found, we immediately return YES; otherwise these\nco-triangles form a small co-triangle modulator, whose deletion makes the graph\nco-triangle-free.\n  Extending the work of [Gutin et al., SIDMA 2021], who solved $\\ell$-Coloring\n(for any $\\ell$) in randomized $O^*(2^{k})$ time when given a\n$\\overline{K_2}$-free modulator of size $k$, we show that this problem can\nlikewise be solved in randomized $O^*(2^{k})$ time when given a\n$\\overline{K_3}$-free modulator of size~$k$.\n  This result in turn yields a randomized $O^{*}(2^{3k/2})$ algorithm for\n$(n-k)$-Coloring (also known as Dual Coloring), improving the previous\n$O^{*}(4^{k})$ bound. We then introduce a smaller parameterization,\n$(\\omega+\\overline{\\mu}-k)$-Coloring, where $\\omega$ is the clique number and\n$\\overline{\\mu}$ is the size of a maximum matching in the complement graph;\nsince $\\omega+\\overline{\\mu}\\le n$ for any graph, this problem is strictly\nharder. Using the same co-triangle-packing argument, we obtain a randomized\n$O^{*}(2^{6k})$ algorithm, establishing its fixed-parameter tractability for a\nsmaller parameter. Complementing this finding, we show that no fixed-parameter\ntractable algorithm exists for $(\\omega-k)$-Coloring or\n$(\\overline{\\mu}-k)$-Coloring under standard complexity assumptions.", "published": "2025-09-15 18:19:48", "link": "http://arxiv.org/abs/2509.12347v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "What News Recommendation Research Did (But Mostly Didn't) Teach Us About Building A News Recommender", "abstract": "One of the goals of recommender systems research is to provide insights and\nmethods that can be used by practitioners to build real-world systems that\ndeliver high-quality recommendations to actual people grounded in their genuine\ninterests and needs. We report on our experience trying to apply the news\nrecommendation literature to build POPROX, a live platform for news\nrecommendation research, and reflect on the extent to which the current state\nof research supports system-building efforts. Our experience highlights several\nunexpected challenges encountered in building personalization features that are\ncommonly found in products from news aggregators and publishers, and shows how\nthose difficulties are connected to surprising gaps in the literature. Finally,\nwe offer a set of lessons learned from building a live system with a persistent\nuser base and highlight opportunities to make future news recommendation\nresearch more applicable and impactful in practice.", "published": "2025-09-15 18:50:52", "link": "http://arxiv.org/abs/2509.12361v1", "categories": ["cs.IR", "cs.CY", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation", "abstract": "Generative paradigm, especially powered by Large Language Models (LLMs), has\nemerged as a new solution to the next point-of-interest (POI) recommendation.\nPioneering studies usually adopt a two-stage pipeline, starting with a\ntokenizer converting POIs into discrete identifiers that can be processed by\nLLMs, followed by POI behavior prediction tasks to instruction-tune LLM for\nnext POI recommendation. Despite of remarkable progress, they still face two\nlimitations: (1) existing tokenizers struggle to encode heterogeneous signals\nin the recommendation data, suffering from information loss issue, and (2)\nprevious instruction-tuning tasks only focus on users' POI visit behavior while\nignore other behavior types, resulting in insufficient understanding of\nmobility. To address these limitations, we propose KGTB (Knowledge Graph\nTokenization for Behavior-aware generative next POI recommendation).\nSpecifically, KGTB organizes the recommendation data in a knowledge graph (KG)\nformat, of which the structure can seamlessly preserve the heterogeneous\ninformation. Then, a KG-based tokenizer is developed to quantize each node into\nan individual structural ID. This process is supervised by the KG's structure,\nthus reducing the loss of heterogeneous information. Using generated IDs, KGTB\nproposes multi-behavior learning that introduces multiple behavior-specific\nprediction tasks for LLM fine-tuning, e.g., POI, category, and region visit\nbehaviors. Learning on these behavior tasks provides LLMs with comprehensive\ninsights on the target POI visit behavior. Experiments on four real-world city\ndatasets demonstrate the superior performance of KGTB.", "published": "2025-09-15 18:25:00", "link": "http://arxiv.org/abs/2509.12350v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Digital Voices of Survival: From Social Media Disclosures to Support Provisions for Domestic Violence Victims", "abstract": "Domestic Violence (DV) is a pervasive public health problem characterized by\npatterns of coercive and abusive behavior within intimate relationships. With\nthe rise of social media as a key outlet for DV victims to disclose their\nexperiences, online self-disclosure has emerged as a critical yet underexplored\navenue for support-seeking. In addition, existing research lacks a\ncomprehensive and nuanced understanding of DV self-disclosure, support\nprovisions, and their connections. To address these gaps, this study proposes a\nnovel computational framework for modeling DV support-seeking behavior\nalongside community support mechanisms. The framework consists of four key\ncomponents: self-disclosure detection, post clustering, topic summarization,\nand support extraction and mapping. We implement and evaluate the framework\nwith data collected from relevant social media communities. Our findings not\nonly advance existing knowledge on DV self-disclosure and online support\nprovisions but also enable victim-centered digital interventions.", "published": "2025-09-15 05:32:42", "link": "http://arxiv.org/abs/2509.12288v1", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Exploring the entropic region", "abstract": "The paper explores three known methods, their variants and limitations, that\ncan be used to obtain new entropy inequalities. The Copy Lemma was distilled\nfrom the original Zhang-Yeung construction which produced the first non-Shannon\ninequality. Its iterated version, effects of symmetrizations, and connections\nwith polyhedral vertex enumeration are discussed. Another method, derived from\nthe principle of maximum entropy, has the Copy Lemma as a special case.\nNevertheless, none of the two presented variants is known to generate more\ninequalities than the iterated Copy Lemma. Finally, the Ahlswede-K\\\"orner\nmethod is shown to employ a hidden application of the Copy Lemma - the\nunderlying lemma alone cannot generate new inequalities -, which makes this\nmethod strictly weaker than the Copy Lemma. The paper is written in a tutorial\nstyle and concludes with a list of open questions and research problems.", "published": "2025-09-15 20:45:25", "link": "http://arxiv.org/abs/2509.12439v1", "categories": ["cs.IT", "math.IT", "06B35, 26A12, 52B12, 90C29"], "primary_category": "cs.IT"}
{"title": "Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention across various domains. However, their widespread adoption has also\nraised serious safety concerns. In this paper, we uncover a new safety risk of\nMLLMs: the output preference of MLLMs can be arbitrarily manipulated by\ncarefully optimized images. Such attacks often generate contextually relevant\nyet biased responses that are neither overtly harmful nor unethical, making\nthem difficult to detect. Specifically, we introduce a novel method, Preference\nHijacking (Phi), for manipulating the MLLM response preferences using a\npreference hijacked image. Our method works at inference time and requires no\nmodel modifications. Additionally, we introduce a universal hijacking\nperturbation -- a transferable component that can be embedded into different\nimages to hijack MLLM responses toward any attacker-specified preferences.\nExperimental results across various tasks demonstrate the effectiveness of our\napproach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.", "published": "2025-09-15 23:55:57", "link": "http://arxiv.org/abs/2509.12521v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents", "abstract": "One of the main goals of robotics and intelligent agent research is to enable\nnatural communication with humans in physically situated settings. While recent\nwork has focused on verbal modes such as language and speech, non-verbal\ncommunication is crucial for flexible interaction. We present a framework for\ngenerating pointing gestures in embodied agents by combining imitation and\nreinforcement learning. Using a small motion capture dataset, our method learns\na motor control policy that produces physically valid, naturalistic gestures\nwith high referential accuracy. We evaluate the approach against supervised\nlearning and retrieval baselines in both objective metrics and a virtual\nreality referential game with human users. Results show that our system\nachieves higher naturalness and accuracy than state-of-the-art supervised\nmodels, highlighting the promise of imitation-RL for communicative gesture\ngeneration and its potential application to robots.", "published": "2025-09-15 23:15:15", "link": "http://arxiv.org/abs/2509.12507v1", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T07, 68T40", "I.2.9; I.2.6"], "primary_category": "cs.RO"}
{"title": "Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model", "abstract": "Time-series forecasting and causal discovery are central in neuroscience, as\npredicting brain activity and identifying causal relationships between neural\npopulations and circuits can shed light on the mechanisms underlying cognition\nand disease. With the rise of foundation models, an open question is how they\ncompare to traditional methods for brain signal forecasting and causality\nanalysis, and whether they can be applied in a zero-shot setting. In this work,\nwe evaluate a foundation model against classical methods for inferring\ndirectional interactions from spontaneous brain activity measured with\nfunctional magnetic resonance imaging (fMRI) in humans. Traditional approaches\noften rely on Wiener-Granger causality. We tested the forecasting ability of\nthe foundation model in both zero-shot and fine-tuned settings, and assessed\ncausality by comparing Granger-like estimates from the model with standard\nGranger causality. We validated the approach using synthetic time series\ngenerated from ground-truth causal models, including logistic map coupling and\nOrnstein-Uhlenbeck processes. The foundation model achieved competitive\nzero-shot forecasting fMRI time series (mean absolute percentage error of 0.55\nin controls and 0.27 in patients). Although standard Granger causality did not\nshow clear quantitative differences between models, the foundation model\nprovided a more precise detection of causal interactions.\n  Overall, these findings suggest that foundation models offer versatility,\nstrong zero-shot performance, and potential utility for forecasting and causal\ndiscovery in time-series data.", "published": "2025-09-15 22:43:23", "link": "http://arxiv.org/abs/2509.12497v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "SamudrACE: Fast and Accurate Coupled Climate Modeling with 3D Ocean and Atmosphere Emulators", "abstract": "Traditional numerical global climate models simulate the full Earth system by\nexchanging boundary conditions between separate simulators of the atmosphere,\nocean, sea ice, land surface, and other geophysical processes. This paradigm\nallows for distributed development of individual components within a common\nframework, unified by a coupler that handles translation between realms via\nspatial or temporal alignment and flux exchange. Following a similar approach\nadapted for machine learning-based emulators, we present SamudrACE: a coupled\nglobal climate model emulator which produces centuries-long simulations at\n1-degree horizontal, 6-hourly atmospheric, and 5-daily oceanic resolution, with\n145 2D fields spanning 8 atmospheric and 19 oceanic vertical levels, plus sea\nice, surface, and top-of-atmosphere variables. SamudrACE is highly stable and\nhas low climate biases comparable to those of its components with prescribed\nboundary forcing, with realistic variability in coupled climate phenomena such\nas ENSO that is not possible to simulate in uncoupled mode.", "published": "2025-09-15 22:27:26", "link": "http://arxiv.org/abs/2509.12490v1", "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "physics.ao-ph"}
{"title": "Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures", "abstract": "We propose a novel neural network architecture, called Non-Trainable\nModification (NTM), for computing Nash equilibria in stochastic differential\ngames (SDGs) on graphs. These games model a broad class of graph-structured\nmulti-agent systems arising in finance, robotics, energy, and social dynamics,\nwhere agents interact locally under uncertainty. The NTM architecture imposes a\ngraph-guided sparsification on feedforward neural networks, embedding fixed,\nnon-trainable components aligned with the underlying graph topology. This\ndesign enhances interpretability and stability, while significantly reducing\nthe number of trainable parameters in large-scale, sparse settings. We\ntheoretically establish a universal approximation property for NTM in static\ngames on graphs and numerically validate its expressivity and robustness\nthrough supervised learning tasks. Building on this foundation, we incorporate\nNTM into two state-of-the-art game solvers, Direct Parameterization and Deep\nBSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical\nexperiments on three SDGs across various graph structures demonstrate that\nNTM-based methods achieve performance comparable to their fully trainable\ncounterparts, while offering improved computational efficiency.", "published": "2025-09-15 22:11:56", "link": "http://arxiv.org/abs/2509.12484v1", "categories": ["cs.LG", "cs.GT", "math.OC"], "primary_category": "cs.LG"}
{"title": "PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization", "abstract": "The rapid advancement of generative AI has democratized access to powerful\ntools such as Text-to-Image models. However, to generate high-quality images,\nusers must still craft detailed prompts specifying scene, style, and\ncontext-often through multiple rounds of refinement. We propose PromptSculptor,\na novel multi-agent framework that automates this iterative prompt optimization\nprocess. Our system decomposes the task into four specialized agents that work\ncollaboratively to transform a short, vague user prompt into a comprehensive,\nrefined prompt. By leveraging Chain-of-Thought reasoning, our framework\neffectively infers hidden context and enriches scene and background details. To\niteratively refine the prompt, a self-evaluation agent aligns the modified\nprompt with the original input, while a feedback-tuning agent incorporates user\nfeedback for further refinement. Experimental results demonstrate that\nPromptSculptor significantly enhances output quality and reduces the number of\niterations needed for user satisfaction. Moreover, its model-agnostic design\nallows seamless integration with various T2I models, paving the way for\nindustrial applications.", "published": "2025-09-15 20:52:11", "link": "http://arxiv.org/abs/2509.12446v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Scattering theory for Stokes flow in complex branched structures", "abstract": "Slow, viscous flow in branched structures arises in many biological and\nengineering settings. Direct numerical simulation of flow in such complicated\nmulti-scale geometry, however, is a computationally intensive task. We propose\na scattering theory framework that dramatically reduces this cost by\ndecomposing networks into components connected by short straight channels.\nExploiting the phenomenon of rapid return to Poiseuille flow (Saint-Venant's\nprinciple in the context of elasticity), we compute a high-order accurate\nscattering matrix for each component via boundary integral equations. These\nprecomputed components can then be assembled into arbitrary branched\nstructures, and the precomputed local solutions on each component can be\nassembled into an accurate global solution. The method is modular, has\nnegligible cost, and appears to be the first full-fidelity solver that makes\nuse of the return to Poiseuille flow phenomenon. In our two-dimensional\nexamples, it matches the accuracy of full-domain solvers while requiring only a\nfraction of the computational effort.", "published": "2025-09-15 22:58:37", "link": "http://arxiv.org/abs/2509.12500v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Nonlocal Neural Tangent Kernels via Parameter-Space Interactions", "abstract": "The Neural Tangent Kernel (NTK) framework has provided deep insights into the\ntraining dynamics of neural networks under gradient flow. However, it relies on\nthe assumption that the network is differentiable with respect to its\nparameters, an assumption that breaks down when considering non-smooth target\nfunctions or parameterized models exhibiting non-differentiable behavior. In\nthis work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the\nlocal gradient with a nonlocal interaction-based approximation in parameter\nspace. Nonlocal gradients are known to exist for a wider class of functions\nthan the standard gradient. This allows NTK theory to be extended to nonsmooth\nfunctions, stochastic estimators, and broader families of models. We explore\nboth fixed-kernel and attention-based formulations of this nonlocal operator.\nWe illustrate the new formulation with numerical studies.", "published": "2025-09-15 21:23:47", "link": "http://arxiv.org/abs/2509.12467v1", "categories": ["cs.LG", "cs.NA", "math.NA", "90C56"], "primary_category": "cs.LG"}
{"title": "Data-driven balanced truncation for linear systems with quadratic outputs", "abstract": "We develop the framework for a non-intrusive, quadrature-based method for\napproximate balanced truncation (QuadBT) of linear systems with quadratic\noutputs, thus extending the applicability of QuadBT, which was originally\ndesigned for data-driven balanced truncation of standard linear systems with\nlinear outputs only. The new approach makes use of the time-domain and\nfrequency-domain quadrature-based representation of the system's infinite\nGramians, only implicitly. We show that by sampling solely the extended impulse\nresponses of the original system and their derivatives (or the corresponding\ntransfer functions), we construct a reduced-order model that mimics the\napproximation quality of the intrusive (projection-based) balanced truncation.\nWe validate the proposed framework on a numerical example.", "published": "2025-09-15 19:37:15", "link": "http://arxiv.org/abs/2509.12393v1", "categories": ["math.NA", "cs.NA", "math.DS"], "primary_category": "math.NA"}
{"title": "The Green's Function on Rhombic Flat Tori", "abstract": "We obtain the Green's function $G$ for any flat rhombic torus $T$, always\nwith numerical values of significant digits up to the fourth decimal place\n(noting that $G$ is unique for $|T|=1$ and $\\int_TGdA=0$). This precision is\nguaranteed by the strategies we adopt, which include theorems such as the\nLegendre Relation, properties of the Weierstra\\ss\\,P-Function, and also the\nalgorithmic control of numerical errors. Our code uses complex integration\nroutines developed by H. Karcher, who also introduced the symmetric\nP-Weierstra\\ss\\,function, and these resources simplify the computation of\nelliptic functions considerably.", "published": "2025-09-15 17:34:50", "link": "http://arxiv.org/abs/2509.12299v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Limit Order Book Dynamics", "abstract": "Reinforcement Learning has emerged as a promising framework for developing\nadaptive and data-driven strategies, enabling market makers to optimize\ndecision-making policies based on interactions with the limit order book\nenvironment. This paper explores the integration of a reinforcement learning\nagent in a market-making context, where the underlying market dynamics have\nbeen explicitly modeled to capture observed stylized facts of real markets,\nincluding clustered order arrival times, non-stationary spreads and return\ndrifts, stochastic order quantities and price volatility. These mechanisms aim\nto enhance stability of the resulting control agent, and serve to incorporate\ndomain-specific knowledge into the agent policy learning process. Our\ncontributions include a practical implementation of a market making agent based\non the Proximal-Policy Optimization (PPO) algorithm, alongside a comparative\nevaluation of the agent's performance under varying market conditions via a\nsimulator-based environment. As evidenced by our analysis of the financial\nreturn and risk metrics when compared to a closed-form optimal solution, our\nresults suggest that the reinforcement learning agent can effectively be used\nunder non-stationary market conditions, and that the proposed simulator-based\nenvironment can serve as a valuable tool for training and pre-training\nreinforcement learning agents in market-making scenarios.", "published": "2025-09-15 21:08:13", "link": "http://arxiv.org/abs/2509.12456v1", "categories": ["q-fin.TR", "cs.AI"], "primary_category": "q-fin.TR"}
{"title": "Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning", "abstract": "Scientific machine learning increasingly uses spectral methods to understand\nphysical systems. Current spectral learning approaches provide only point\nestimates without uncertainty quantification, limiting their use in\nsafety-critical applications where prediction confidence is essential.\nParametric matrix models have emerged as powerful tools for scientific machine\nlearning, achieving exceptional performance by learning governing equations.\nHowever, their deterministic nature limits deployment in uncertainty\nquantification applications. We introduce Bayesian parametric matrix models\n(B-PMMs), a principled framework that extends PMMs to provide uncertainty\nestimates while preserving their spectral structure and computational\nefficiency. B-PMM addresses the fundamental challenge of quantifying\nuncertainty in matrix eigenvalue problems where standard Bayesian methods fail\ndue to the geometric constraints of spectral decomposition. The theoretical\ncontributions include: (i) adaptive spectral decomposition with regularized\nmatrix perturbation bounds that characterize eigenvalue uncertainty\npropagation, (ii) structured variational inference algorithms using\nmanifold-aware matrix-variate Gaussian posteriors that respect Hermitian\nconstraints, and (iii) finite-sample calibration guarantees with explicit\ndependence on spectral gaps and problem conditioning. Experimental validation\nacross matrix dimensions from 5x5 to 500x500 with perfect convergence rates\ndemonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <\n0.05) while maintaining favorable scaling. The framework exhibits graceful\ndegradation under spectral ill-conditioning and provides reliable uncertainty\nestimates even in near-degenerate regimes. The proposed framework supports\nrobust spectral learning in uncertainty-critical domains and lays the\ngroundwork for broader Bayesian spectral machine learning.", "published": "2025-09-15 19:52:35", "link": "http://arxiv.org/abs/2509.12406v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Reduced Order Modeling of Energetic Materials Using Physics-Aware Recurrent Convolutional Neural Networks in a Latent Space (LatentPARC)", "abstract": "Physics-aware deep learning (PADL) has gained popularity for use in complex\nspatiotemporal dynamics (field evolution) simulations, such as those that arise\nfrequently in computational modeling of energetic materials (EM). Here, we show\nthat the challenge PADL methods face while learning complex field evolution\nproblems can be simplified and accelerated by decoupling it into two tasks:\nlearning complex geometric features in evolving fields and modeling dynamics\nover these features in a lower dimensional feature space. To accomplish this,\nwe build upon our previous work on physics-aware recurrent convolutions (PARC).\nPARC embeds knowledge of underlying physics into its neural network\narchitecture for more robust and accurate prediction of evolving physical\nfields. PARC was shown to effectively learn complex nonlinear features such as\nthe formation of hotspots and coupled shock fronts in various initiation\nscenarios of EMs, as a function of microstructures, serving effectively as a\nmicrostructure-aware burn model. In this work, we further accelerate PARC and\nreduce its computational cost by projecting the original dynamics onto a\nlower-dimensional invariant manifold, or 'latent space.' The projected latent\nrepresentation encodes the complex geometry of evolving fields (e.g.\ntemperature and pressure) in a set of data-driven features. The reduced\ndimension of this latent space allows us to learn the dynamics during the\ninitiation of EM with a lighter and more efficient model. We observe a\nsignificant decrease in training and inference time while maintaining results\ncomparable to PARC at inference. This work takes steps towards enabling rapid\nprediction of EM thermomechanics at larger scales and characterization of EM\nstructure-property-performance linkages at a full application scale.", "published": "2025-09-15 19:48:04", "link": "http://arxiv.org/abs/2509.12401v1", "categories": ["cond-mat.mtrl-sci", "stat.ML"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization", "abstract": "Modern deep learning models excel at pattern recognition but remain\nfundamentally limited by their reliance on spurious correlations, leading to\npoor generalization and a demand for massive datasets. We argue that a key\ningredient for human-like intelligence-robust, sample-efficient learning-stems\nfrom an understanding of causal mechanisms. In this work, we introduce\nCausal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer\nthe latent causal structure of a task distribution. CSML comprises three key\nmodules: a perception module that maps raw inputs to disentangled symbolic\nrepresentations; a differentiable causal induction module that discovers the\nunderlying causal graph governing these symbols and a graph-based reasoning\nmodule that leverages this graph to make predictions. By meta-learning a shared\ncausal world model across a distribution of tasks, CSML can rapidly adapt to\nnovel tasks, including those requiring reasoning about interventions and\ncounterfactuals, from only a handful of examples. We introduce CausalWorld, a\nnew physics-based benchmark designed to test these capabilities. Our\nexperiments show that CSML dramatically outperforms state-of-the-art\nmeta-learning and neuro-symbolic baselines, particularly on tasks demanding\ntrue causal inference.", "published": "2025-09-15 19:28:09", "link": "http://arxiv.org/abs/2509.12387v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Jackknife Variance Estimation for H\u00e1jek-Dominated Generalized U-Statistics", "abstract": "We prove ratio-consistency of the jackknife variance estimator, and certain\nvariants, for a broad class of generalized U-statistics whose variance is\nasymptotically dominated by their H\\'ajek projection, with the classical\nfixed-order case recovered as a special instance. This H\\'ajek projection\ndominance condition unifies and generalizes several criteria in the existing\nliterature, placing the simple nonparametric jackknife on the same footing as\nthe infinitesimal jackknife in the generalized setting. As an illustration, we\napply our result to the two-scale distributional nearest-neighbor regression\nestimator, obtaining consistent variance estimates under substantially weaker\nconditions than previously required.", "published": "2025-09-15 18:35:23", "link": "http://arxiv.org/abs/2509.12356v1", "categories": ["math.ST", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition", "abstract": "Speech emotion recognition systems often predict a consensus value generated\nfrom the ratings of multiple annotators. However, these models have limited\nability to predict the annotation of any one person. Alternatively, models can\nlearn to predict the annotations of all annotators. Adapting such models to new\nannotators is difficult as new annotators must individually provide sufficient\nlabeled training data. We propose to leverage inter-annotator similarity by\nusing a model pre-trained on a large annotator population to identify a\nsimilar, previously seen annotator. Given a new, previously unseen, annotator\nand limited enrollment data, we can make predictions for a similar annotator,\nenabling off-the-shelf annotation of unseen data in target datasets, providing\na mechanism for extremely low-cost personalization. We demonstrate our approach\nsignificantly outperforms other off-the-shelf approaches, paving the way for\nlightweight emotion adaptation, practical for real-world deployment.", "published": "2025-09-15 15:52:09", "link": "http://arxiv.org/abs/2509.12295v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning", "abstract": "Accurate and generalizable blood pressure (BP) estimation is vital for the\nearly detection and management of cardiovascular diseases. In this study, we\nenforce subject-level data splitting on a public multi-wavelength\nphotoplethysmography (PPG) dataset and propose a generalizable BP estimation\nframework based on curriculum-adversarial learning. Our approach combines\ncurriculum learning, which transitions from hypertension classification to BP\nregression, with domain-adversarial training that confuses subject identity to\nencourage the learning of subject-invariant features. Experiments show that\nmulti-channel fusion consistently outperforms single-channel models. On the\nfour-wavelength PPG dataset, our method achieves strong performance under\nstrict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for\nsystolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP).\nAdditionally, ablation studies validate the effectiveness of both the\ncurriculum and adversarial components. These results highlight the potential of\nleveraging complementary information in multi-wavelength PPG and\ncurriculum-adversarial strategies for accurate and robust BP estimation.", "published": "2025-09-15 23:43:26", "link": "http://arxiv.org/abs/2509.12518v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG", "abstract": "Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.\nTraditional SpO2 estimation methods often rely on complex clinical calibration,\nmaking them unsuitable for low-power, wearable applications. In this paper, we\npropose a transfer learning-based framework for the rapid adaptation of SpO2\nestimation to energy-efficient wearable devices using low-sampling-rate (25Hz)\ndual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long\nShort-Term Memory (BiLSTM) model with self-attention on a public clinical\ndataset, then fine-tune it using data collected from our wearable We-Be band\nand an FDA-approved reference pulse oximeter. Experimental results show that\nour approach achieves a mean absolute error (MAE) of 2.967% on the public\ndataset and 2.624% on the private dataset, significantly outperforming\ntraditional calibration and non-transferred machine learning baselines.\nMoreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,\nexcluding baseline draw. Our method also attains an MAE of 3.284% in\ninstantaneous SpO2 prediction, effectively capturing rapid fluctuations. These\nresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring\non wearable devices without the need for clinical calibration.", "published": "2025-09-15 23:37:13", "link": "http://arxiv.org/abs/2509.12515v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device", "abstract": "Wearable photoplethysmography (PPG) is embedded in billions of devices, yet\nits optical waveform is easily corrupted by motion, perfusion loss, and ambient\nlight, jeopardizing downstream cardiometric analytics. Existing signal-quality\nassessment (SQA) methods rely either on brittle heuristics or on data-hungry\nsupervised models. We introduce the first fully unsupervised SQA pipeline for\nwrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,\nunlabeled data from heterogeneous sources (varying in device and sampling\nfrequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,\nthe learned representation is stable across differences in LED wavelength,\ndrive intensity, and device optics, as well as wrist motion). Stage 2 converts\neach 512-D encoder embedding into a 4-D topological signature via persistent\nhomology (PH) and clusters these signatures with HDBSCAN. To produce a binary\nsignal-quality index (SQI), the acceptable PPG signals are represented by the\ndensest cluster while the remaining clusters are assumed to mainly contain\npoor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,\nDavies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,\nrespectively, on a stratified sample of 10,000 windows. In this study, we\npropose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)\nframework that offers a drop-in, scalable, cross-device quality gate for PPG\nsignals.", "published": "2025-09-15 23:22:02", "link": "http://arxiv.org/abs/2509.12510v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Partial Secrecy Analysis in Wireless Systems: Diversity-Enhanced PLS over Generalized Fading Channels", "abstract": "Securing information in future mobile networks is challenging, especially for\ndevices with limited computational resources. Physical layer security (PLS)\noffers a viable solution by leveraging wireless channel randomness. When full\nsecrecy is unattainable, the partial secrecy regime provides a realistic\nalternative. This work analyzes partial secrecy performance under the\ngeneralized multicluster fluctuating two-ray (MFTR) fading model, which\nsubsumes many classical fading cases. We study a system with a transmitter (A),\nlegitimate receiver (B), and eavesdropper (E), both B and E using antenna\narrays with maximal ratio combining (MRC), under i.n.i.d. fading. Exact and\nclosed-form approximations are derived for key secrecy metrics: generalized\nsecrecy outage probability (GSOP), average fractional equivocation (AFE), and\naverage information leakage rate (AILR). The results, validated by Monte Carlo\nsimulations, retain constant complexity regardless of diversity order. The MFTR\nmodel's flexibility enables comprehensive assessment across fading conditions,\nshowing that more MRC branches at B enhance secrecy performance depending on\nthe A-E link characteristics.", "published": "2025-09-15 18:45:03", "link": "http://arxiv.org/abs/2509.12359v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "FAS-ARIS: Turning Multipath Challenges Into Localization Opportunities", "abstract": "Traditional single-input single-output (SISO) systems face fundamental\nlimitations in achieving accurate three-dimensional (3D) localization due to\nlimited spatial degrees of freedom (DoF) and the adverse impact of multipath\npropagation. This paper proposes a novel fluid antenna system (FAS)-active\nreconfigurable intelligent surface (ARIS) framework that transforms multipath\neffects from a hindrance into a resource for enhanced localization. By\nsynergistically combining the signal amplification capabilities of ARIS with\nthe spatial diversity enabled by FAS, the proposed system achieves robust 3D\nuser equipment (UE) positioning -- without relying on auxiliary information\nsuch as time-of-arrival (ToA) or frequency diversity. The system exploits both\nline-of-sight (LoS) and non-line-of-sight (NLoS) components through a tailored\nsignal decoupling strategy. We design novel UE pilot sequences and ARIS phase\nconfigurations to effectively separate LoS and NLoS channels, enabling\nindependent parameter estimation. A multi-stage estimation algorithm is then\napplied: the multiple signal classification (MUSIC) algorithm estimates\nangle-of-arrival (AoA) from the direct path, while maximum likelihood\nestimation with interior-point refinement recovers cascaded channel parameters\nfrom the reflected path. Finally, geometric triangulation using least-squares\nestimation determines the UE's 3D position based on the extracted AoA\ninformation. Comprehensive performance analysis, including the derivation of\nCram\\'{e}r-Rao bounds for both channel and position estimation, establishes\ntheoretical benchmarks. Simulation results confirm that the proposed FAS-ARIS\nframework achieves near-optimal localization accuracy while maintaining\nrobustness in rich multipath environments -- effectively turning conventional\nlocalization challenges into advantages.", "published": "2025-09-15 18:23:54", "link": "http://arxiv.org/abs/2509.12348v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "abstract": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.", "published": "2025-09-15 12:35:14", "link": "http://arxiv.org/abs/2509.11860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Sequential Hypothesis Testing with Non-Homogeneous Costs", "abstract": "We study the Non-Homogeneous Sequential Hypothesis Testing (NHSHT), where a\nsingle active Decision-Maker (DM) selects actions with heterogeneous positive\ncosts to identify the true hypothesis under an average error constraint\n\\(\\delta\\), while minimizing expected total cost paid. Under standard\narguments, we show that the objective decomposes into the product of the mean\nnumber of samples and the mean per-action cost induced by the policy. This\nleads to a key design principle: one should optimize the ratio of expectations\n(expected information gain per expected cost) rather than the expectation of\nper-step information-per-cost (\"bit-per-buck\"), which can be suboptimal. We\nadapt the Chernoff scheme to NHSHT, preserving its classical \\(\\log 1/\\delta\\)\nscaling. In simulations, the adapted scheme reduces mean cost by up to 50\\%\nrelative to the classic Chernoff policy and by up to 90\\% relative to the naive\nbit-per-buck heuristic.", "published": "2025-09-15 06:55:04", "link": "http://arxiv.org/abs/2509.11632v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model", "abstract": "Time-series forecasting and causal discovery are central in neuroscience, as\npredicting brain activity and identifying causal relationships between neural\npopulations and circuits can shed light on the mechanisms underlying cognition\nand disease. With the rise of foundation models, an open question is how they\ncompare to traditional methods for brain signal forecasting and causality\nanalysis, and whether they can be applied in a zero-shot setting. In this work,\nwe evaluate a foundation model against classical methods for inferring\ndirectional interactions from spontaneous brain activity measured with\nfunctional magnetic resonance imaging (fMRI) in humans. Traditional approaches\noften rely on Wiener-Granger causality. We tested the forecasting ability of\nthe foundation model in both zero-shot and fine-tuned settings, and assessed\ncausality by comparing Granger-like estimates from the model with standard\nGranger causality. We validated the approach using synthetic time series\ngenerated from ground-truth causal models, including logistic map coupling and\nOrnstein-Uhlenbeck processes. The foundation model achieved competitive\nzero-shot forecasting fMRI time series (mean absolute percentage error of 0.55\nin controls and 0.27 in patients). Although standard Granger causality did not\nshow clear quantitative differences between models, the foundation model\nprovided a more precise detection of causal interactions.\n  Overall, these findings suggest that foundation models offer versatility,\nstrong zero-shot performance, and potential utility for forecasting and causal\ndiscovery in time-series data.", "published": "2025-09-15 22:43:23", "link": "http://arxiv.org/abs/2509.12497v2", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Numerical Optimization Methods in the environment with Quantum Noise", "abstract": "The accurate calculation of electronic potential energy surfaces for ground\nand excited states is crucial for understanding photochemical processes,\nparticularly near conical intersections. While classical methods are limited by\nscaling and quantum algorithms by hardware, this thesis focuses on the\nState-Averaged Orbital-Optimized Variational Quantum Eigensolver (SA-OO-VQE).\nThis hybrid quantum-classical algorithm provides a balanced description of\nmultiple electronic states by combining quantum state preparation with\nclassical state-averaged orbital optimization.\n  A key contribution is the implementation and evaluation of the Differential\nEvolution algorithm within the SA-OO-VQE framework, with a comparative study\nagainst classical optimizers like the Broyden-Fletcher-Goldfarb-Shanno (BFGS)\nand Sequential Least Squares Programming (SLSQP) algorithms. The performance of\nthese optimizers is assessed by calculating ground and first excited state\nenergies for H$_2$, H$_4$, and LiH.\n  The thesis also demonstrates SA-OO-VQE's capability to accurately model\npotential energy surfaces near conical intersections, using formaldimine as a\ncase study. The results show that orbital optimization is essential for\ncorrectly capturing the potential energy surface topology, a task where\nstandard methods with fixed orbitals fail. Our findings indicate that while\nDifferential Evolution presents efficiency challenges, gradient-based methods\nlike BFGS and SLSQP offer superior performance, confirming that the SA-OO-VQE\napproach is crucial for treating complex electronic structures.", "published": "2025-09-15 19:00:27", "link": "http://arxiv.org/abs/2509.13367v1", "categories": ["quant-ph", "cs.NA", "math.NA"], "primary_category": "quant-ph"}
{"title": "FunAudio-ASR Technical Report", "abstract": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings.", "published": "2025-09-15 23:19:36", "link": "http://arxiv.org/abs/2509.12508v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support", "abstract": "E-Commerce customer support requires quick and accurate answers grounded in\nproduct data and past support cases. This paper develops a novel\nretrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs)\nto improve the relevance of the answer and the factual grounding. We examine\nrecent advances in knowledge-augmented RAG and chatbots based on large language\nmodels (LLM) in customer support, including Microsoft's GraphRAG and hybrid\nretrieval architectures. We then propose a new answer synthesis algorithm that\ncombines structured subgraphs from a domain-specific KG with text documents\nretrieved from support archives, producing more coherent and grounded\nresponses. We detail the architecture and knowledge flow of our system, provide\ncomprehensive experimental evaluation, and justify its design in real-time\nsupport settings. Our implementation demonstrates 23\\% improvement in factual\naccuracy and 89\\% user satisfaction in e-Commerce QA scenarios.", "published": "2025-09-15 04:17:42", "link": "http://arxiv.org/abs/2509.14267v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Variational Gaussian Approximation in Replica Analysis of Parametric Models", "abstract": "We revisit the replica method for analyzing inference and learning in\nparametric models, considering situations where the data-generating\ndistribution is unknown or analytically intractable. Instead of assuming\nidealized distributions to carry out quenched averages analytically, we use a\nvariational Gaussian approximation for the replicated system in grand canonical\nformalism in which the data average can be deferred and replaced by empirical\naverages, leading to stationarity conditions that adaptively determine the\nparameters of the trial Hamiltonian for each dataset. This approach clarifies\nhow fluctuations affect information extraction and connects directly with the\nresults of mathematical statistics or learning theory such as information\ncriteria. As a concrete application, we analyze linear regression and derive\nlearning curves. This includes cases with real-world datasets, where exact\nreplica calculations are not feasible.", "published": "2025-09-15 11:01:42", "link": "http://arxiv.org/abs/2509.11780v1", "categories": ["cond-mat.dis-nn", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cond-mat.dis-nn"}
{"title": "SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models", "abstract": "High-quality Text-to-Speech (TTS) model training requires extensive and\ndiverse text and speech data. It is challenging to procure such data from real\nsources due to issues of domain specificity, licensing, and scalability. Large\nlanguage models (LLMs) can certainly generate textual data, but they create\nrepetitive text with insufficient variation in the prompt during the generation\nprocess. Another important aspect in TTS training data is text normalization.\nTools for normalization might occasionally introduce anomalies or overlook\nvaluable patterns, and thus impact data quality. Furthermore, it is also\nimpractical to rely on voice artists for large scale speech recording in\ncommercial TTS systems with standardized voices. To address these challenges,\nwe propose SpeechWeave, a synthetic speech data generation pipeline that is\ncapable of automating the generation of multilingual, domain-specific datasets\nfor training TTS models. Our experiments reveal that our pipeline generates\ndata that is 10-48% more diverse than the baseline across various linguistic\nand phonetic metrics, along with speaker-standardized speech audio while\ngenerating approximately 97% correctly normalized text. Our approach enables\nscalable, high-quality data generation for TTS training, improving diversity,\nnormalization, and voice consistency in the generated datasets.", "published": "2025-09-15 15:11:43", "link": "http://arxiv.org/abs/2509.14270v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MALLM: Multi-Agent Large Language Models Framework", "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective\nintelligence by scaling test-time compute and leveraging expertise. Current\nframeworks for multi-agent debate are often designed towards tool use, lack\nintegrated evaluation, or provide limited configurability of agent personas,\nresponse generators, discussion paradigms, and decision protocols. We introduce\nMALLM (Multi-Agent Large Language Models), an open-source framework that\nenables systematic analysis of MAD components. MALLM offers more than 144\nunique configurations of MAD, including (1) agent personas (e.g., Expert,\nPersonality), (2) response generators (e.g., Critical, Reasoning), (3)\ndiscussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,\nVoting, Consensus). MALLM uses simple configuration files to define a debate.\nFurthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro,\nWinoGrande) and provides an evaluation pipeline for easy comparison of MAD\nconfigurations. MALLM enables researchers to systematically configure, run, and\nevaluate debates for their problems, facilitating the understanding of the\ncomponents and their interplay.", "published": "2025-09-15 07:48:02", "link": "http://arxiv.org/abs/2509.11656v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "A.1; I.2.7"], "primary_category": "cs.MA"}
{"title": "Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing", "abstract": "We present a probabilistic framework to accurately estimate dimensions of\nadditively manufactured components. Using a dataset of 405 parts from nine\nproduction runs involving two machines, three polymer materials, and two-part\nconfigurations, we examine five key design features. To capture both design\ninformation and manufacturing variability, we employ models integrating\ncontinuous and categorical factors. For predicting Difference from Target (DFT)\nvalues, we test deterministic and probabilistic machine learning methods.\nDeterministic models, trained on 80% of the dataset, provide precise point\nestimates, with Support Vector Regression (SVR) achieving accuracy close to\nprocess repeatability. To address systematic deviations, we adopt Gaussian\nProcess Regression (GPR) and Bayesian Neural Networks (BNNs). GPR delivers\nstrong predictive performance and interpretability, while BNNs capture both\naleatoric and epistemic uncertainties. We investigate two BNN approaches: one\nbalancing accuracy and uncertainty capture, and another offering richer\nuncertainty decomposition but with lower dimensional accuracy. Our results\nunderscore the importance of quantifying epistemic uncertainty for robust\ndecision-making, risk assessment, and model improvement. We discuss trade-offs\nbetween GPR and BNNs in terms of predictive power, interpretability, and\ncomputational efficiency, noting that model choice depends on analytical needs.\nBy combining deterministic precision with probabilistic uncertainty\nquantification, our study provides a rigorous foundation for uncertainty-aware\npredictive modeling in AM. This approach not only enhances dimensional accuracy\nbut also supports reliable, risk-informed design strategies, thereby advancing\ndata-driven manufacturing methodologies.", "published": "2025-09-15 18:31:36", "link": "http://arxiv.org/abs/2509.16233v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "PromptSculptor: Multi-Agent Based Text-to-Image Prompt Optimization", "abstract": "The rapid advancement of generative AI has democratized access to powerful\ntools such as Text-to-Image models. However, to generate high-quality images,\nusers must still craft detailed prompts specifying scene, style, and\ncontext-often through multiple rounds of refinement. We propose PromptSculptor,\na novel multi-agent framework that automates this iterative prompt optimization\nprocess. Our system decomposes the task into four specialized agents that work\ncollaboratively to transform a short, vague user prompt into a comprehensive,\nrefined prompt. By leveraging Chain-of-Thought reasoning, our framework\neffectively infers hidden context and enriches scene and background details. To\niteratively refine the prompt, a self-evaluation agent aligns the modified\nprompt with the original input, while a feedback-tuning agent incorporates user\nfeedback for further refinement. Experimental results demonstrate that\nPromptSculptor significantly enhances output quality and reduces the number of\niterations needed for user satisfaction. Moreover, its model-agnostic design\nallows seamless integration with various T2I models, paving the way for\nindustrial applications.", "published": "2025-09-15 20:52:11", "link": "http://arxiv.org/abs/2509.12446v2", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Group Survival Probability under Contagion in Microlending", "abstract": "In the context of micro-finance, a group of individuals undertake business\nprojects that may interfere with one another. A contagious default happens if\none person's project failure leads to the default of another group member. In\nthis paper, we apply a probabilistic approach to analyze the impact of such\ncontagion among investment group members. Firstly, a general formula is\nprovided to compute the group survival probability with the presence of\ncontagion effect. Then, special cases of this probability model are examined in\ndetail. In particular, we show that if the investment group is homogeneous,\ndefined in the paper, then including more members into the group will\neventually lead to default with probability 1. This differs from the\nnon-contagious scenario, where the default probability decreases monotonically\nwith respect to the group size. Afterwards, we provide an upper bound of the\noptimal group size under the homogeneous setup; so, one can run a linear search\nwithin finite time to locate this optimizer.", "published": "2025-09-15 04:53:22", "link": "http://arxiv.org/abs/2509.11579v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
