{"title": "Modeling language contact with the Iterated Learning Model", "abstract": "Contact between languages has the potential to transmit vocabulary and other\nlanguage features; however, this does not always happen. Here, an iterated\nlearning model is used to examine, in a simple way, the resistance of languages\nto change during language contact. Iterated learning models are agent-based\nmodels of language change, they demonstrate that languages that are expressive\nand compositional arise spontaneously as a consequence of a language\ntransmission bottleneck. A recently introduced type of iterated learning model,\nthe Semi-Supervised ILM is used to simulate language contact. These simulations\ndo not include many of the complex factors involved in language contact and do\nnot model a population of speakers; nonetheless the model demonstrates that the\ndynamics which lead languages in the model to spontaneously become expressive\nand compositional, also cause a language to maintain its core traits even after\nmixing with another language.", "published": "2024-06-11 01:43:23", "link": "http://arxiv.org/abs/2406.06878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large\n  Language Models", "abstract": "Simultaneous Machine Translation (SiMT) generates target translations while\nreading the source sentence. It relies on a policy to determine the optimal\ntiming for reading sentences and generating translations. Existing SiMT methods\ngenerally adopt the traditional Transformer architecture, which concurrently\ndetermines the policy and generates translations. While they excel at\ndetermining policies, their translation performance is suboptimal. Conversely,\nLarge Language Models (LLMs), trained on extensive corpora, possess superior\ngeneration capabilities, but it is difficult for them to acquire translation\npolicy through the training methods of SiMT. Therefore, we introduce\nAgent-SiMT, a framework combining the strengths of LLMs and traditional SiMT\nmethods. Agent-SiMT contains the policy-decision agent and the translation\nagent. The policy-decision agent is managed by a SiMT model, which determines\nthe translation policy using partial source sentence and translation. The\ntranslation agent, leveraging an LLM, generates translation based on the\npartial source sentence. The two agents collaborate to accomplish SiMT.\nExperiments demonstrate that Agent-SiMT attains state-of-the-art performance.", "published": "2024-06-11 03:09:20", "link": "http://arxiv.org/abs/2406.06910v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-Hoc Answer Attribution for Grounded and Trustworthy Long Document\n  Comprehension: Task, Insights, and Challenges", "abstract": "Attributing answer text to its source document for information-seeking\nquestions is crucial for building trustworthy, reliable, and accountable\nsystems. We formulate a new task of post-hoc answer attribution for long\ndocument comprehension (LDC). Owing to the lack of long-form abstractive and\ninformation-seeking LDC datasets, we refactor existing datasets to assess the\nstrengths and weaknesses of existing retrieval-based and proposed answer\ndecomposition and textual entailment-based optimal selection attribution\nsystems for this task. We throw light on the limitations of existing datasets\nand the need for datasets to assess the actual performance of systems on this\ntask.", "published": "2024-06-11 04:31:54", "link": "http://arxiv.org/abs/2406.06938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Probabilistic Framework for LLM Hallucination Detection via Belief\n  Tree Propagation", "abstract": "This paper focuses on the task of hallucination detection, which aims to\ndetermine the truthfulness of LLM-generated statements. To address this\nproblem, a popular class of methods utilize the LLM's self-consistencies in its\nbeliefs in a set of logically related augmented statements generated by the\nLLM, which does not require external knowledge databases and can work with both\nwhite-box and black-box LLMs. However, in many existing approaches, the\naugmented statements tend to be very monotone and unstructured, which makes it\ndifficult to integrate meaningful information from the LLM beliefs in these\nstatements. Also, many methods work with the binarized version of the LLM's\nbelief, instead of the continuous version, which significantly loses\ninformation. To overcome these limitations, in this paper, we propose Belief\nTree Propagation (BTProp), a probabilistic framework for LLM hallucination\ndetection. BTProp introduces a belief tree of logically related statements by\nrecursively decomposing a parent statement into child statements with three\ndecomposition strategies, and builds a hidden Markov tree model to integrate\nthe LLM's belief scores in these statements in a principled way. Experiment\nresults show that our method improves baselines by 3%-9% (evaluated by AUROC\nand AUC-PR) on multiple hallucination detection benchmarks. Code is available\nat https://github.com/UCSB-NLP-Chang/BTProp.", "published": "2024-06-11 05:21:37", "link": "http://arxiv.org/abs/2406.06950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crayon: Customized On-Device LLM via Instant Adapter Blending and\n  Edge-Server Hybrid Inference", "abstract": "The customization of large language models (LLMs) for user-specified tasks\ngets important. However, maintaining all the customized LLMs on cloud servers\nincurs substantial memory and computational overheads, and uploading user data\ncan also lead to privacy concerns. On-device LLMs can offer a promising\nsolution by mitigating these issues. Yet, the performance of on-device LLMs is\ninherently constrained by the limitations of small-scaled models. To overcome\nthese restrictions, we first propose Crayon, a novel approach for on-device LLM\ncustomization. Crayon begins by constructing a pool of diverse base adapters,\nand then we instantly blend them into a customized adapter without extra\ntraining. In addition, we develop a device-server hybrid inference strategy,\nwhich deftly allocates more demanding queries or non-customized tasks to a\nlarger, more capable LLM on a server. This ensures optimal performance without\nsacrificing the benefits of on-device customization. We carefully craft a novel\nbenchmark from multiple question-answer datasets, and show the efficacy of our\nmethod in the LLM customization.", "published": "2024-06-11 07:00:08", "link": "http://arxiv.org/abs/2406.07007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effectively Compress KV Heads for LLM", "abstract": "The advent of pre-trained large language models (LLMs) has revolutionized\nvarious natural language processing tasks. These models predominantly employ an\nauto-regressive decoding mechanism that utilizes Key-Value (KV) caches to\neliminate redundant calculations for previous tokens. Nevertheless, as context\nlengths and batch sizes increase, the linear expansion in memory footprint of\nKV caches becomes a key bottleneck of LLM deployment, which decreases\ngeneration speeds significantly. To mitigate this issue, previous techniques\nlike multi-query attention (MQA) and grouped-query attention (GQA) have been\ndeveloped, in order to reduce KV heads to accelerate inference with comparable\naccuracy to multi-head attention (MHA). Despite their effectiveness, existing\nstrategies for compressing MHA often overlook the intrinsic properties of the\nKV caches. In this work, we explore the low-rank characteristics of the KV\ncaches and propose a novel approach for compressing KV heads. In particular, we\ncarefully optimize the MHA-to-GQA transformation to minimize compression error,\nand to remain compatible with rotary position embeddings (RoPE), we also\nintroduce specialized strategies for key caches with RoPE. We demonstrate that\nour method can compress half or even three-quarters of KV heads while\nmaintaining performance comparable to the original LLMs, which presents a\npromising direction for more efficient LLM deployment in resource-constrained\nenvironments.", "published": "2024-06-11 08:37:33", "link": "http://arxiv.org/abs/2406.07056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level\n  Hallucination Evaluation", "abstract": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), achieving remarkable performance across diverse\ntasks and enabling widespread real-world applications. However, LLMs are prone\nto hallucination, generating content that either conflicts with established\nknowledge or is unfaithful to the original sources. Existing hallucination\nbenchmarks primarily focus on sentence- or passage-level hallucination\ndetection, neglecting dialogue-level evaluation, hallucination localization,\nand rationale provision. They also predominantly target factuality\nhallucinations while underestimating faithfulness hallucinations, often relying\non labor-intensive or non-specialized evaluators. To address these limitations,\nwe propose HalluDial, the first comprehensive large-scale benchmark for\nautomatic dialogue-level hallucination evaluation. HalluDial encompasses both\nspontaneous and induced hallucination scenarios, covering factuality and\nfaithfulness hallucinations. The benchmark includes 4,094 dialogues with a\ntotal of 146,856 samples. Leveraging HalluDial, we conduct a comprehensive\nmeta-evaluation of LLMs' hallucination evaluation capabilities in\ninformation-seeking dialogues and introduce a specialized judge language model,\nHalluJudge. The high data quality of HalluDial enables HalluJudge to achieve\nsuperior or competitive performance in hallucination evaluation, facilitating\nthe automatic assessment of dialogue-level hallucinations in LLMs and providing\nvaluable insights into this phenomenon. The dataset and the code are available\nat https://github.com/FlagOpen/HalluDial.", "published": "2024-06-11 08:56:18", "link": "http://arxiv.org/abs/2406.07070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for\n  Question Answering over Knowledge Graphs", "abstract": "Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning\nautonomous language agents in various real-life applications. To improve the\nneural-symbolic reasoning capabilities of language agents powered by Large\nLanguage Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning\nAgent (DARA) framework. DARA effectively parses questions into formal queries\nthrough a dual mechanism: high-level iterative task decomposition and low-level\ntask grounding. Importantly, DARA can be efficiently trained with a small\nnumber of high-quality reasoning trajectories. Our experimental results\ndemonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms\nboth in-context learning-based agents with GPT-4 and alternative fine-tuned\nagents, across different benchmarks in zero-shot evaluation, making such models\nmore accessible for real-life applications. We also show that DARA attains\nperformance comparable to state-of-the-art enumerating-and-ranking-based\nmethods for KGQA.", "published": "2024-06-11 09:09:37", "link": "http://arxiv.org/abs/2406.07080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Exploring Large Language Models for Document-Level Machine\n  Translation with In-context Learning", "abstract": "Large language models (LLMs) exhibit outstanding performance in machine\ntranslation via in-context learning. In contrast to sentence-level translation,\ndocument-level translation (DOCMT) by LLMs based on in-context learning faces\ntwo major challenges: firstly, document translations generated by LLMs are\noften incoherent; secondly, the length of demonstration for in-context learning\nis usually limited. To address these issues, we propose a Context-Aware\nPrompting method (CAP), which enables LLMs to generate more accurate, cohesive,\nand coherent translations via in-context learning. CAP takes into account\nmulti-level attention, selects the most relevant sentences to the current one\nas context, and then generates a summary from these collected sentences.\nSubsequently, sentences most similar to the summary are retrieved from the\ndatastore as demonstrations, which effectively guide LLMs in generating\ncohesive and coherent translations. We conduct extensive experiments across\nvarious DOCMT tasks, and the results demonstrate the effectiveness of our\napproach, particularly in zero pronoun translation (ZPT) and literary\ntranslation tasks.", "published": "2024-06-11 09:11:17", "link": "http://arxiv.org/abs/2406.07081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Recipe for Long Context Extension via Middle-Focused\n  Positional Encoding", "abstract": "Recently, many methods have been developed to extend the context length of\npre-trained large language models (LLMs), but they often require fine-tuning at\nthe target length ($\\gg4K$) and struggle to effectively utilize information\nfrom the middle part of the context. To address these issues, we propose\n$\\textbf{C}$ontinuity-$\\textbf{R}$elativity ind$\\textbf{E}$xing with\ng$\\textbf{A}$ussian $\\textbf{M}$iddle ($\\texttt{CREAM}$), which interpolates\npositional encodings by manipulating position indices. Apart from being simple,\n$\\texttt{CREAM}$ is training-efficient: it only requires fine-tuning at the\npre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much\nlonger target context length (e.g., 256K). To ensure that the model focuses\nmore on the information in the middle, we introduce a truncated Gaussian to\nencourage sampling from the middle part of the context during fine-tuning, thus\nalleviating the \"Lost-in-the-Middle\" problem faced by long-context LLMs.\nExperimental results show that $\\texttt{CREAM}$ successfully extends LLMs to\nthe target length for both Base and Chat versions of $\\texttt{Llama2-7B}$ with\n\"Never Miss A Beat\". Our code is publicly available at\nhttps://github.com/bigai-nlco/cream.", "published": "2024-06-11 10:35:49", "link": "http://arxiv.org/abs/2406.07138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Language Models to Self-Improve by Learning from Language\n  Feedback", "abstract": "Aligning Large Language Models (LLMs) with human intentions and values is\ncrucial yet challenging. Current methods primarily rely on human preferences,\nwhich are costly and insufficient in capturing nuanced feedback expressed in\nnatural language. In this paper, we present Self-Refinement Tuning (SRT), a\nmethod that leverages model feedback for alignment, thereby reducing reliance\non human annotations. SRT uses a base language model (e.g., Tulu2) to generate\ninitial responses, which are critiqued and refined by a more advanced model\n(e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and\nimprove its outputs, facilitating continuous learning. SRT further optimizes\nthe model by learning from its self-generated feedback and refinements,\ncreating a feedback loop that promotes model improvement. Our empirical\nevaluations demonstrate that SRT significantly outperforms strong baselines\nacross diverse tasks and model sizes. When applied to a 70B parameter model,\nSRT increases the win rate from 9.6\\% to 25.8\\% on the AlpacaEval 2.0\nbenchmark, surpassing well-established systems such as GPT-4-0314, Claude 2,\nand Gemini. Our analysis highlights the crucial role of language feedback in\nthe success of SRT, suggesting potential for further exploration in this\ndirection.", "published": "2024-06-11 11:20:05", "link": "http://arxiv.org/abs/2406.07168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decipherment-Aware Multilingual Learning in Jointly Trained Language\n  Models", "abstract": "The principle that governs unsupervised multilingual learning (UCL) in\njointly trained language models (mBERT as a popular example) is still being\ndebated. Many find it surprising that one can achieve UCL with multiple\nmonolingual corpora. In this work, we anchor UCL in the context of language\ndecipherment and show that the joint training methodology is a decipherment\nprocess pivotal for UCL. In a controlled setting, we investigate the effect of\ndifferent decipherment settings on the multilingual learning performance and\nconsolidate the existing opinions on the contributing factors to\nmultilinguality. From an information-theoretic perspective we draw a limit to\nthe UCL performance and demonstrate the importance of token alignment in\nchallenging decipherment settings caused by differences in the data domain,\nlanguage order and tokenization granularity. Lastly, we apply lexical alignment\nto mBERT and investigate the contribution of aligning different lexicon groups\nto downstream performance.", "published": "2024-06-11 13:10:30", "link": "http://arxiv.org/abs/2406.07231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Hallucination in Simultaneous Machine Translation", "abstract": "It is widely known that hallucination is a critical issue in Simultaneous\nMachine Translation (SiMT) due to the absence of source-side information. While\nmany efforts have been made to enhance performance for SiMT, few of them\nattempt to understand and analyze hallucination in SiMT. Therefore, we conduct\na comprehensive analysis of hallucination in SiMT from two perspectives:\nunderstanding the distribution of hallucination words and the target-side\ncontext usage of them. Intensive experiments demonstrate some valuable findings\nand particularly show that it is possible to alleviate hallucination by\ndecreasing the over usage of target-side information for SiMT.", "published": "2024-06-11 13:20:07", "link": "http://arxiv.org/abs/2406.07239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in\n  Generative LLMs", "abstract": "Generative large language models (LLMs) have been shown to exhibit harmful\nbiases and stereotypes. While safety fine-tuning typically takes place in\nEnglish, if at all, these models are being used by speakers of many different\nlanguages. There is existing evidence that the performance of these models is\ninconsistent across languages and that they discriminate based on demographic\nfactors of the user. Motivated by this, we investigate whether the social\nstereotypes exhibited by LLMs differ as a function of the language used to\nprompt them, while controlling for cultural differences and task accuracy. To\nthis end, we present MBBQ (Multilingual Bias Benchmark for Question-answering),\na carefully curated version of the English BBQ dataset extended to Dutch,\nSpanish, and Turkish, which measures stereotypes commonly held across these\nlanguages. We further complement MBBQ with a parallel control dataset to\nmeasure task performance on the question-answering task independently of bias.\nOur results based on several open-source and proprietary LLMs confirm that some\nnon-English languages suffer from bias more than English, even when controlling\nfor cultural shifts. Moreover, we observe significant cross-lingual differences\nin bias behaviour for all except the most accurate models. With the release of\nMBBQ, we hope to encourage further research on bias in multilingual settings.\nThe dataset and code are available at https://github.com/Veranep/MBBQ.", "published": "2024-06-11 13:23:14", "link": "http://arxiv.org/abs/2406.07243v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Sexism Classification: Fine-Tuned XLM-RoBERTa and GPT-3.5\n  Few-Shot Learning", "abstract": "Sexism in online content is a pervasive issue that necessitates effective\nclassification techniques to mitigate its harmful impact. Online platforms\noften have sexist comments and posts that create a hostile environment,\nespecially for women and minority groups. This content not only spreads harmful\nstereotypes but also causes emotional harm. Reliable methods are essential to\nfind and remove sexist content, making online spaces safer and more welcoming.\nTherefore, the sEXism Identification in Social neTworks (EXIST) challenge\naddresses this issue at CLEF 2024. This study aims to improve sexism\nidentification in bilingual contexts (English and Spanish) by leveraging\nnatural language processing models. The tasks are to determine whether a text\nis sexist and what the source intention behind it is. We fine-tuned the\nXLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to\nclassify sexist content. The XLM-RoBERTa model exhibited robust performance in\nhandling complex linguistic structures, while GPT-3.5's few-shot learning\ncapability allowed for rapid adaptation to new data with minimal labeled\nexamples. Our approach using XLM-RoBERTa achieved 4th place in the soft-soft\nevaluation of Task 1 (sexism identification). For Task 2 (source intention), we\nachieved 2nd place in the soft-soft evaluation.", "published": "2024-06-11 14:15:33", "link": "http://arxiv.org/abs/2406.07287v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning with HED-IT: The impact of human post-editing for dialogical\n  language models", "abstract": "Automatic methods for generating and gathering linguistic data have proven\neffective for fine-tuning Language Models (LMs) in languages less resourced\nthan English. Still, while there has been emphasis on data quantity, less\nattention has been given to its quality. In this work, we investigate the\nimpact of human intervention on machine-generated data when fine-tuning\ndialogical models. In particular, we study (1) whether post-edited dialogues\nexhibit higher perceived quality compared to the originals that were\nautomatically generated; (2) whether fine-tuning with post-edited dialogues\nresults in noticeable differences in the generated outputs; and (3) whether\npost-edited dialogues influence the outcomes when considering the parameter\nsize of the LMs. To this end we created HED-IT, a large-scale dataset where\nmachine-generated dialogues are paired with the version post-edited by humans.\nUsing both the edited and unedited portions of HED-IT, we fine-tuned three\ndifferent sizes of an LM. Results from both human and automatic evaluation show\nthat the different quality of training data is clearly perceived and it has an\nimpact also on the models trained on such data. Additionally, our findings\nindicate that larger models are less sensitive to data quality, whereas this\nhas a crucial impact on smaller models. These results enhance our comprehension\nof the impact of human intervention on training data in the development of\nhigh-quality LMs.", "published": "2024-06-11 14:16:14", "link": "http://arxiv.org/abs/2406.07288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLIMPSE: Pragmatically Informative Multi-Document Summarization for\n  Scholarly Reviews", "abstract": "Scientific peer review is essential for the quality of academic publications.\nHowever, the increasing number of paper submissions to conferences has strained\nthe reviewing process. This surge poses a burden on area chairs who have to\ncarefully read an ever-growing volume of reviews and discern each reviewer's\nmain arguments as part of their decision process. In this paper, we introduce\n\\sys, a summarization method designed to offer a concise yet comprehensive\noverview of scholarly reviews. Unlike traditional consensus-based methods, \\sys\nextracts both common and unique opinions from the reviews. We introduce novel\nuniqueness scores based on the Rational Speech Act framework to identify\nrelevant sentences in the reviews. Our method aims to provide a pragmatic\nglimpse into all reviews, offering a balanced perspective on their opinions.\nOur experimental results with both automatic metrics and human evaluation show\nthat \\sys generates more discriminative summaries than baseline methods in\nterms of human evaluation while achieving comparable performance with these\nmethods in terms of automatic metrics.", "published": "2024-06-11 15:27:01", "link": "http://arxiv.org/abs/2406.07359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "abstract": "Large Language Models (LLMs) possess extensive knowledge and strong\ncapabilities in performing in-context reasoning. However, previous work\nchallenges their out-of-context reasoning ability, i.e., the ability to infer\ninformation from their training data, instead of from the context or prompt.\nThis paper focuses on a significant aspect of out-of-context reasoning:\nOut-of-Context Knowledge Reasoning (OCKR), which is to combine multiple\nknowledge to infer new knowledge. We designed a synthetic dataset with seven\nrepresentative OCKR tasks to systematically assess the OCKR capabilities of\nLLMs. Using this dataset, we evaluated several LLMs and discovered that their\nproficiency in this aspect is limited, regardless of whether the knowledge is\ntrained in a separate or adjacent training settings. Moreover, training the\nmodel to reason with reasoning examples does not result in significant\nimprovement, while training the model to perform explicit knowledge retrieval\nhelps for retrieving attribute knowledge but not the relation knowledge,\nindicating that the model's limited OCKR capabilities are due to difficulties\nin knowledge retrieval. Furthermore, we treat cross-lingual knowledge transfer\nas a distinct form of OCKR, and evaluate this ability. Our results show that\nthe evaluated model also exhibits limited ability in transferring knowledge\nacross languages.", "published": "2024-06-11 15:58:59", "link": "http://arxiv.org/abs/2406.07393v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MINERS: Multilingual Language Models as Semantic Retrievers", "abstract": "Words have been represented in a high-dimensional vector space that encodes\ntheir semantic similarities, enabling downstream applications such as\nretrieving synonyms, antonyms, and relevant contexts. However, despite recent\nadvances in multilingual language models (LMs), the effectiveness of these\nmodels' representations in semantic retrieval contexts has not been\ncomprehensively explored. To fill this gap, this paper introduces the MINERS, a\nbenchmark designed to evaluate the ability of multilingual LMs in semantic\nretrieval tasks, including bitext mining and classification via\nretrieval-augmented contexts. We create a comprehensive framework to assess the\nrobustness of LMs in retrieving samples across over 200 diverse languages,\nincluding extremely low-resource languages in challenging cross-lingual and\ncode-switching settings. Our results demonstrate that by solely retrieving\nsemantically similar embeddings yields performance competitive with\nstate-of-the-art approaches, without requiring any fine-tuning.", "published": "2024-06-11 16:26:18", "link": "http://arxiv.org/abs/2406.07424v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Document-Level Relation Extraction Models to Entity\n  Name Variations", "abstract": "Driven by the demand for cross-sentence and large-scale relation extraction,\ndocument-level relation extraction (DocRE) has attracted increasing research\ninterest. Despite the continuous improvement in performance, we find that\nexisting DocRE models which initially perform well may make more mistakes when\nmerely changing the entity names in the document, hindering the generalization\nto novel entity names. To this end, we systematically investigate the\nrobustness of DocRE models to entity name variations in this work. We first\npropose a principled pipeline to generate entity-renamed documents by replacing\nthe original entity names with names from Wikidata. By applying the pipeline to\nDocRED and Re-DocRED datasets, we construct two novel benchmarks named\nEnv-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results\nshow that both three representative DocRE models and two in-context learned\nlarge language models consistently lack sufficient robustness to entity name\nvariations, particularly on cross-sentence relation instances and documents\nwith more entities. Finally, we propose an entity variation robust training\nmethod which not only improves the robustness of DocRE models but also enhances\ntheir understanding and reasoning capabilities. We further verify that the\nbasic idea of this method can be effectively transferred to in-context learning\nfor DocRE as well.", "published": "2024-06-11 16:51:14", "link": "http://arxiv.org/abs/2406.07444v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Advancing Annotation of Stance in Social Media Posts: A Comparative\n  Analysis of Large Language Models and Crowd Sourcing", "abstract": "In the rapidly evolving landscape of Natural Language Processing (NLP), the\nuse of Large Language Models (LLMs) for automated text annotation in social\nmedia posts has garnered significant interest. Despite the impressive\ninnovations in developing LLMs like ChatGPT, their efficacy, and accuracy as\nannotation tools are not well understood. In this paper, we analyze the\nperformance of eight open-source and proprietary LLMs for annotating the stance\nexpressed in social media posts, benchmarking their performance against human\nannotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the\nconditions under which LLMs are likely to disagree with human judgment. A\nsignificant finding of our study is that the explicitness of text expressing a\nstance plays a critical role in how faithfully LLMs' stance judgments match\nhumans'. We argue that LLMs perform well when human annotators do, and when\nLLMs fail, it often corresponds to situations in which human annotators\nstruggle to reach an agreement. We conclude with recommendations for a\ncomprehensive approach that combines the precision of human expertise with the\nscalability of LLM predictions. This study highlights the importance of\nimproving the accuracy and comprehensiveness of automated stance detection,\naiming to advance these technologies for more efficient and unbiased analysis\nof social media.", "published": "2024-06-11 17:26:07", "link": "http://arxiv.org/abs/2406.07483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrasing in Affirmative Terms Improves Negation Understanding", "abstract": "Negation is a common linguistic phenomenon. Yet language models face\nchallenges with negation in many natural language understanding tasks such as\nquestion answering and natural language inference. In this paper, we experiment\nwith seamless strategies that incorporate affirmative interpretations (i.e.,\nparaphrases without negation) to make models more robust against negation.\nCrucially, our affirmative interpretations are obtained automatically. We show\nimprovements with CondaQA, a large corpus requiring reasoning with negation,\nand five natural language understanding tasks.", "published": "2024-06-11 17:30:03", "link": "http://arxiv.org/abs/2406.07492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling\n  Queer Voices", "abstract": "Modern voice cloning models claim to be able to capture a diverse range of\nvoices. We test the ability of a typical pipeline to capture the style known\ncolloquially as \"gay voice\" and notice a homogenisation effect: synthesised\nspeech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants)\nthan its corresponding ground-truth for speakers with \"gay voice\", but ratings\nactually increase for control speakers. Loss of \"gay voice\" has implications\nfor accessibility. We also find that for speakers with \"gay voice\", loss of\n\"gay voice\" corresponds to lower similarity ratings.\n  However, we caution that improving the ability of such models to synthesise\n``gay voice'' comes with a great number of risks. We use this pipeline as a\nstarting point for a discussion on the ethics of modelling queer voices more\nbroadly. Collecting \"clean\" queer data has safety and fairness ramifications,\nand the resulting technology may cause harms from mockery to death.", "published": "2024-06-11 17:39:46", "link": "http://arxiv.org/abs/2406.07504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "THaLLE: Text Hyperlocally Augmented Large Language Extension --\n  Technical Report", "abstract": "Recent advancements in Large Language Models (LLMs) have revealed new\ncapabilities and opportunities across the technological landscape. However, the\npracticality of very large LLMs is challenged by their high compute cost, which\ndoes not justify the benefits given their limited capability compared to\nhumans. While smaller, more practical LLMs have shown potential in financial\nanalysis, though they are not yet fully proficient, as evidenced by their\nnear-passing performance on the Chartered Financial Analyst (CFA) exam. In this\nwork, we present Financial Analyst Extension to our Text Hyperlocally Augmented\nLarge Language Extension (THaLLE), a series of 8B LLMs consistently achieving\nhighest performance on mock CFA exams against models of comparable size. We\nthoroughly document the fine-tuning techniques used to facilitate future\nresearch. Additionally, we introduce the use of Flare CFA, a publicly available\ndataset for evaluating LLMs as a financial advisor.", "published": "2024-06-11 17:40:00", "link": "http://arxiv.org/abs/2406.07505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sustainable self-supervised learning for speech representations", "abstract": "Sustainable artificial intelligence focuses on data, hardware, and algorithms\nto make machine learning models more environmentally responsible. In\nparticular, machine learning models for speech representations are\ncomputationally expensive, generating environmental concerns because of their\nhigh energy consumption. Thus, we propose a sustainable self-supervised model\nto learn speech representation, combining optimizations in neural layers and\ntraining to reduce computing costs. The proposed model improves over a\nresource-efficient baseline, reducing both memory usage and computing cost\nestimations. It pretrains using a single GPU in less than a day. On top of\nthat, it improves the error rate performance of the baseline in downstream task\nevaluations. When comparing it to large speech representation approaches, there\nis an order of magnitude reduction in memory usage, while computing cost\nreductions represent almost three orders of magnitude improvement.", "published": "2024-06-11 20:21:36", "link": "http://arxiv.org/abs/2406.07696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiPragEval: Multilingual Pragmatic Evaluation of Large Language\n  Models", "abstract": "As the capabilities of Large Language Models (LLMs) expand, it becomes\nincreasingly important to evaluate them beyond basic knowledge assessment,\nfocusing on higher-level language understanding. This study introduces\nMultiPragEval, the first multilingual pragmatic evaluation of LLMs, designed\nfor English, German, Korean, and Chinese. Comprising 1200 question units\ncategorized according to Grice's Cooperative Principle and its four\nconversational maxims, MultiPragEval enables an in-depth assessment of LLMs'\ncontextual awareness and their ability to infer implied meanings. Our findings\ndemonstrate that Claude3-Opus significantly outperforms other models in all\ntested languages, establishing a state-of-the-art in the field. Among\nopen-source models, Solar-10.7B and Qwen1.5-14B emerge as strong competitors.\nBy analyzing pragmatic inference, we provide valuable insights into the\ncapabilities essential for advanced language comprehension in AI systems.", "published": "2024-06-11 21:46:03", "link": "http://arxiv.org/abs/2406.07736v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LT4SG@SMM4H24: Tweets Classification for Digital Epidemiology of\n  Childhood Health Outcomes Using Pre-Trained Language Models", "abstract": "This paper presents our approaches for the SMM4H24 Shared Task 5 on the\nbinary classification of English tweets reporting children's medical disorders.\nOur first approach involves fine-tuning a single RoBERTa-large model, while the\nsecond approach entails ensembling the results of three fine-tuned\nBERTweet-large models. We demonstrate that although both approaches exhibit\nidentical performance on validation data, the BERTweet-large ensemble excels on\ntest data. Our best-performing system achieves an F1-score of 0.938 on test\ndata, outperforming the benchmark classifier by 1.18%.", "published": "2024-06-11 22:48:18", "link": "http://arxiv.org/abs/2406.07759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37\n  Emotion Datasets", "abstract": "Foundation models have shown great promise in speech emotion recognition\n(SER) by leveraging their pre-trained representations to capture emotion\npatterns in speech signals. To further enhance SER performance across various\nlanguages and domains, we propose a novel twofold approach. First, we gather\nEmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus\nwith 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second,\nwe introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone\nextension and fine-tuning on EmoSet++. We duplicate each encoder layer and its\nweights, then freeze the first duplicate, integrating an extra zero-initialized\nlinear layer and skip connections to preserve functionality and ensure its\nadaptability for subsequent fine-tuning. Our evaluation on unseen datasets\nshows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks.\nModel and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.", "published": "2024-06-11 21:30:15", "link": "http://arxiv.org/abs/2406.10275v1", "categories": ["cs.CL", "68T10", "I.2"], "primary_category": "cs.CL"}
{"title": "What's in an embedding? Would a rose by any embedding smell as sweet?", "abstract": "Large Language Models (LLMs) are often criticized for lacking true\n\"understanding\" and the ability to \"reason\" with their knowledge, being seen\nmerely as autocomplete systems. We believe that this assessment might be\nmissing a nuanced insight. We suggest that LLMs do develop a kind of empirical\n\"understanding\" that is \"geometry\"-like, which seems adequate for a range of\napplications in NLP, computer vision, coding assistance, etc. However, this\n\"geometric\" understanding, built from incomplete and noisy data, makes them\nunreliable, difficult to generalize, and lacking in inference capabilities and\nexplanations, similar to the challenges faced by heuristics-based expert\nsystems decades ago.\n  To overcome these limitations, we suggest that LLMs should be integrated with\nan \"algebraic\" representation of knowledge that includes symbolic AI elements\nused in expert systems. This integration aims to create large knowledge models\n(LKMs) that not only possess \"deep\" knowledge grounded in first principles, but\nalso have the ability to reason and explain, mimicking human expert\ncapabilities. To harness the full potential of generative AI safely and\neffectively, a paradigm shift is needed from LLM to more comprehensive LKM.", "published": "2024-06-11 01:10:40", "link": "http://arxiv.org/abs/2406.06870v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evolving Subnetwork Training for Large Language Models", "abstract": "Large language models have ushered in a new era of artificial intelligence\nresearch. However, their substantial training costs hinder further development\nand widespread adoption. In this paper, inspired by the redundancy in the\nparameters of large language models, we propose a novel training paradigm:\nEvolving Subnetwork Training (EST). EST samples subnetworks from the layers of\nthe large language model and from commonly used modules within each layer,\nMulti-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually\nincreasing the size of the subnetworks during the training process, EST can\nsave the cost of training. We apply EST to train GPT2 model and TinyLlama\nmodel, resulting in 26.7\\% FLOPs saving for GPT2 and 25.0\\% for TinyLlama\nwithout an increase in loss on the pre-training dataset. Moreover, EST leads to\nperformance improvements in downstream tasks, indicating that it benefits\ngeneralization. Additionally, we provide intuitive theoretical studies based on\ntraining dynamics and Dropout theory to ensure the feasibility of EST. Our code\nis available at https://github.com/OpenDFM/EST.", "published": "2024-06-11 05:44:56", "link": "http://arxiv.org/abs/2406.06962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Boundary Ambiguity and Inherent Bias for Text Classification\n  in the Era of Large Language Models", "abstract": "Text classification is a crucial task encountered frequently in practical\nscenarios, yet it is still under-explored in the era of large language models\n(LLMs). This study shows that LLMs are vulnerable to changes in the number and\narrangement of options in text classification. Our extensive empirical analyses\nreveal that the key bottleneck arises from ambiguous decision boundaries and\ninherent biases towards specific tokens and positions. To mitigate these\nissues, we make the first attempt and propose a novel two-stage classification\nframework for LLMs. Our approach is grounded in the empirical observation that\npairwise comparisons can effectively alleviate boundary ambiguity and inherent\nbias. Specifically, we begin with a self-reduction technique to efficiently\nnarrow down numerous options, which contributes to reduced decision space and a\nfaster comparison process. Subsequently, pairwise contrastive comparisons are\nemployed in a chain-of-thought manner to draw out nuances and distinguish\nconfusable options, thus refining the ambiguous decision boundary. Extensive\nexperiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify\nthe effectiveness of our framework. Furthermore, benefitting from our\nframework, various LLMs can achieve consistent improvements. Our code and data\nare available in \\url{https://github.com/Chuge0335/PC-CoT}.", "published": "2024-06-11 06:53:19", "link": "http://arxiv.org/abs/2406.07001v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoreauPruner: Robust Pruning of Large Language Models against Weight\n  Perturbations", "abstract": "Few-shot gradient methods have been extensively utilized in existing model\npruning methods, where the model weights are regarded as static values and the\neffects of potential weight perturbations are not considered. However, the\nwidely used large language models (LLMs) have several billion model parameters,\nwhich could increase the fragility of few-shot gradient pruning. In this work,\nwe experimentally show that one-shot gradient pruning algorithms could lead to\nunstable results under perturbations to model weights. And the minor error of\nswitching between data formats bfloat16 and float16 could result in drastically\ndifferent outcomes. To address such instabilities, we leverage optimization\nanalysis and propose an LLM structural pruning method, called MoreauPruner,\nwith provable robustness against weight perturbations. In MoreauPruner, the\nmodel weight importance is estimated based on the neural network's Moreau\nenvelope, which can be flexibly combined with $\\ell_1$-norm regularization\ntechniques to induce the sparsity required in the pruning task. We extensively\nevaluate the MoreauPruner algorithm on several well-known LLMs, including\nLLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest\nthe robustness of MoreauPruner against weight perturbations, and indicate the\nMoreauPruner's successful accuracy-based scores in comparison to several\nexisting pruning methods. We have released the code in\n\\url{https://github.com/ShiningSord/MoreauPruner}.", "published": "2024-06-11 07:19:04", "link": "http://arxiv.org/abs/2406.07017v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Multi-hop Logical Reasoning in Knowledge Graphs with\n  Context-Aware Query Representation Learning", "abstract": "Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural\nlanguage processing, with numerous approaches aiming to answer First-Order\nLogic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g.,\nbeta distribution)-based methodologies have effectively addressed complex FOL\nqueries. However, a common challenge across these methods lies in determining\naccurate geometric bounds or probability parameters for these queries. The\nchallenge arises because existing methods rely on linear sequential operations\nwithin their computation graphs, overlooking the logical structure of the query\nand the relation-induced information that can be gleaned from the relations of\nthe query, which we call the context of the query. To address the problem, we\npropose a model-agnostic methodology that enhances the effectiveness of\nexisting multi-hop logical reasoning approaches by fully integrating the\ncontext of the FOL query graph. Our approach distinctively discerns (1) the\nstructural context inherent to the query structure and (2) the relation-induced\ncontext unique to each node in the query graph as delineated in the\ncorresponding knowledge graph. This dual-context paradigm helps nodes within a\nquery graph attain refined internal representations throughout the multi-hop\nreasoning steps. Through experiments on two datasets, our method consistently\nenhances the three multi-hop reasoning foundation models, achieving performance\nimprovements of up to 19.5%. Our code is available at\nhttps://github.com/kjh9503/caqr.", "published": "2024-06-11 07:48:20", "link": "http://arxiv.org/abs/2406.07034v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Paying More Attention to Source Context: Mitigating Unfaithful\n  Translations from Large Language Model", "abstract": "Large language models (LLMs) have showcased impressive multilingual machine\ntranslation ability. However, unlike encoder-decoder style models, decoder-only\nLLMs lack an explicit alignment between source and target contexts. Analyzing\ncontribution scores during generation processes revealed that LLMs can be\nbiased towards previously generated tokens over corresponding source tokens,\nleading to unfaithful translations. To address this issue, we propose to\nencourage LLMs to pay more attention to the source context from both source and\ntarget perspectives in zeroshot prompting: 1) adjust source context attention\nweights; 2) suppress irrelevant target prefix influence; Additionally, we\npropose 3) avoiding over-reliance on the target prefix in instruction tuning.\nExperimental results from both human-collected unfaithfulness test sets\nfocusing on LLM-generated unfaithful translations and general test sets, verify\nour methods' effectiveness across multiple language pairs. Further human\nevaluation shows our method's efficacy in reducing hallucinatory translations\nand facilitating faithful translation generation.", "published": "2024-06-11 07:49:04", "link": "http://arxiv.org/abs/2406.07036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoEvol: Constructing Better Responses for Instruction Finetuning through\n  Multi-Agent Cooperation", "abstract": "In recent years, instruction fine-tuning (IFT) on large language models\n(LLMs) has garnered considerable attention to enhance model performance on\nunseen tasks. Attempts have been made on automatic construction and effective\nselection for IFT data. However, we posit that previous methods have not fully\nharnessed the potential of LLMs for enhancing data quality. The responses\nwithin IFT data could be further enhanced by leveraging the capabilities of\nLLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent\ncooperation framework for the improvement of responses to instructions. To\neffectively refine the responses, we develop an iterative framework following a\ndebate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is\nfurther devised to ensure the diversity and reliability of editing suggestions\nwithin the framework. Empirically, models equipped with CoEvol outperform\ncompetitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its\neffectiveness in enhancing instruction-following capabilities for LLMs.", "published": "2024-06-11 08:35:37", "link": "http://arxiv.org/abs/2406.07054v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merging Improves Self-Critique Against Jailbreak Attacks", "abstract": "The robustness of large language models (LLMs) against adversarial\nmanipulations, such as jailbreak attacks, remains a significant challenge. In\nthis work, we propose an approach that enhances the self-critique capability of\nthe LLM and further fine-tunes it over sanitized synthetic data. This is done\nwith the addition of an external critic model that can be merged with the\noriginal, thus bolstering self-critique capabilities and improving the\nrobustness of the LLMs response to adversarial prompts. Our results demonstrate\nthat the combination of merging and self-critique can reduce the attack success\nrate of adversaries significantly, thus offering a promising defense mechanism\nagainst jailbreak attacks. Code, data and models released at\nhttps://github.com/vicgalle/merging-self-critique-jailbreaks .", "published": "2024-06-11 12:01:09", "link": "http://arxiv.org/abs/2406.07188v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Commonsense Bias Classification by Mitigating the Influence of\n  Demographic Terms", "abstract": "Understanding commonsense knowledge is crucial in the field of Natural\nLanguage Processing (NLP). However, the presence of demographic terms in\ncommonsense knowledge poses a potential risk of compromising the performance of\nNLP models. This study aims to investigate and propose methods for enhancing\nthe performance and effectiveness of a commonsense polarization classifier by\nmitigating the influence of demographic terms. Three methods are introduced in\nthis paper: (1) hierarchical generalization of demographic terms (2)\nthreshold-based augmentation and (3) integration of hierarchical generalization\nand threshold-based augmentation methods (IHTA). The first method involves\nreplacing demographic terms with more general ones based on a term hierarchy\nontology, aiming to mitigate the influence of specific terms. To address the\nlimited bias-related information, the second method measures the polarization\nof demographic terms by comparing the changes in the model's predictions when\nthese terms are masked versus unmasked. This method augments commonsense\nsentences containing terms with high polarization values by replacing their\npredicates with synonyms generated by ChatGPT. The third method combines the\ntwo approaches, starting with threshold-based augmentation followed by\nhierarchical generalization. The experiments show that the first method\nincreases the accuracy over the baseline by 2.33%, and the second one by 0.96%\nover standard augmentation methods. The IHTA techniques yielded an 8.82% and\n9.96% higher accuracy than threshold-based and standard augmentation methods,\nrespectively.", "published": "2024-06-11 13:09:16", "link": "http://arxiv.org/abs/2406.07229v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation\n  through Dual Learning Feedback Mechanisms", "abstract": "Recently, large language models (LLMs) enhanced by self-reflection have\nachieved promising performance on machine translation. The key idea is guiding\nLLMs to generate translation with human-like feedback. However, existing\nself-reflection methods lack effective feedback information, limiting the\ntranslation performance. To address this, we introduce a DUAL-REFLECT\nframework, leveraging the dual learning of translation tasks to provide\neffective feedback, thereby enhancing the models' self-reflective abilities and\nimproving translation performance. The application of this method across\nvarious translation tasks has proven its effectiveness in improving translation\naccuracy and eliminating ambiguities, especially in translation tasks with\nlow-resource language pairs.", "published": "2024-06-11 13:10:39", "link": "http://arxiv.org/abs/2406.07232v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scholarly Question Answering using Large Language Models in the\n  NFDI4DataScience Gateway", "abstract": "This paper introduces a scholarly Question Answering (QA) system on top of\nthe NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based\n(RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a\nunified and intuitive interface for querying various scientific databases using\nfederated search. The RAG-based scholarly QA, powered by a Large Language Model\n(LLM), facilitates dynamic interaction with search results, enhancing filtering\ncapabilities and fostering a conversational engagement with the Gateway search.\nThe effectiveness of both the Gateway and the scholarly QA system is\ndemonstrated through experimental analysis.", "published": "2024-06-11 13:36:19", "link": "http://arxiv.org/abs/2406.07257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instruct Large Language Models to Drive like Humans", "abstract": "Motion planning in complex scenarios is the core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto plan the future trajectory. Recent methods seek the knowledge preserved in\nlarge language models (LLMs) and apply them in the driving scenarios. Despite\nthe promising results, it is still unclear whether the LLM learns the\nunderlying human logic to drive. In this paper, we propose an InstructDriver\nmethod to transform LLM into a motion planner with explicit instruction tuning\nto align its behavior with humans. We derive driving instruction data based on\nhuman logic (e.g., do not cause collisions) and traffic rules (e.g., proceed\nonly when green lights). We then employ an interpretable InstructChain module\nto further reason the final planning reflecting the instructions. Our\nInstructDriver allows the injection of human rules and learning from driving\ndata, enabling both interpretability and data scalability. Different from\nexisting methods that experimented on closed-loop or simulated settings, we\nadopt the real-world closed-loop motion planning nuPlan benchmark for better\nevaluation. InstructDriver demonstrates the effectiveness of the LLM planner in\na real-world closed-loop setting. Our code is publicly available at\nhttps://github.com/bonbon-rj/InstructDriver.", "published": "2024-06-11 14:24:45", "link": "http://arxiv.org/abs/2406.07296v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\n  Generation for Question-Answering", "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.", "published": "2024-06-11 15:15:33", "link": "http://arxiv.org/abs/2406.07348v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad\n  Prediction", "abstract": "Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based\nelements, including aspect term, opinion term, aspect category, and sentiment\npolarity. In practice, unseen aspects, due to distinct data distribution,\nimpose many challenges for a trained neural model. Motivated by this, this work\nformulates ASQP into the few-shot scenario, which aims for fast adaptation in\nreal applications. Therefore, we first construct a few-shot ASQP dataset (FSQP)\nthat contains richer categories and is more balanced for the few-shot study.\nMoreover, recent methods extract quads through a generation paradigm, which\ninvolves converting the input sentence into a templated target sequence.\nHowever, they primarily focus on the utilization of a single template or the\nconsideration of different template orders, thereby overlooking the\ncorrelations among various templates. To tackle this issue, we further propose\na Broadview Soft Prompting (BvSP) method that aggregates multiple templates\nwith a broader view by taking into account the correlation between the\ndifferent templates. Specifically, BvSP uses the pre-trained language model to\nselect the most relevant k templates with Jensen-Shannon divergence. BvSP\nfurther introduces soft prompts to guide the pre-trained language model using\nthe selected templates. Then, we aggregate the results of multi-templates by\nvoting mechanism. Empirical results demonstrate that BvSP significantly\noutperforms the stateof-the-art methods under four few-shot settings and other\npublic datasets. Our code and dataset are available at\nhttps://github.com/byinhao/BvSP.", "published": "2024-06-11 15:32:32", "link": "http://arxiv.org/abs/2406.07365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Constrained-Based Causal Discovery", "abstract": "Causality is essential for understanding complex systems, such as the\neconomy, the brain, and the climate. Constructing causal graphs often relies on\neither data-driven or expert-driven approaches, both fraught with challenges.\nThe former methods, like the celebrated PC algorithm, face issues with data\nrequirements and assumptions of causal sufficiency, while the latter demand\nsubstantial time and domain knowledge. This work explores the capabilities of\nLarge Language Models (LLMs) as an alternative to domain experts for causal\ngraph generation. We frame conditional independence queries as prompts to LLMs\nand employ the PC algorithm with the answers. The performance of the LLM-based\nconditional independence oracle on systems with known causal graphs shows a\nhigh degree of variability. We improve the performance through a proposed\nstatistical-inspired voting schema that allows some control over false-positive\nand false-negative rates. Inspecting the chain-of-thought argumentation, we\nfind causal reasoning to justify its answer to a probabilistic query. We show\nevidence that knowledge-based CIT could eventually become a complementary tool\nfor data-driven causal discovery.", "published": "2024-06-11 15:45:24", "link": "http://arxiv.org/abs/2406.07378v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "VersiCode: Towards Version-controllable Code Generation", "abstract": "Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.", "published": "2024-06-11 16:15:06", "link": "http://arxiv.org/abs/2406.07411v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Learning Domain-Invariant Features for Out-of-Context News Detection", "abstract": "Out-of-context news is a common type of misinformation on online media\nplatforms. This involves posting a caption, alongside a mismatched news image.\nExisting out-of-context news detection models only consider the scenario where\npre-labeled data is available for each domain, failing to address the\nout-of-context news detection on unlabeled domains (e.g. news topics or\nagencies). In this work, we therefore focus on domain adaptive out-of-context\nnews detection. In order to effectively adapt the detection model to unlabeled\nnews topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation\nwith Test-Time Adaptation) which applies contrastive learning and maximum mean\ndiscrepancy (MMD) to learn domain-invariant features. In addition, we leverage\ntest-time target domain statistics to further assist domain adaptation.\nExperimental results show that our approach outperforms baselines in most\ndomain adaptation settings on two public datasets, by as much as 2.93% in F1\nand 2.08% in accuracy.", "published": "2024-06-11 16:34:02", "link": "http://arxiv.org/abs/2406.07430v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Textual Similarity as a Key Metric in Machine Translation Quality\n  Estimation", "abstract": "Machine Translation (MT) Quality Estimation (QE) assesses translation\nreliability without reference texts. This study introduces \"textual similarity\"\nas a new metric for QE, using sentence transformers and cosine similarity to\nmeasure semantic closeness. Analyzing data from the MLQE-PE dataset, we found\nthat textual similarity exhibits stronger correlations with human scores than\ntraditional metrics (hter, model evaluation, sentence probability etc.).\nEmploying GAMMs as a statistical tool, we demonstrated that textual similarity\nconsistently outperforms other metrics across multiple language pairs in\npredicting human scores. We also found that \"hter\" actually failed to predict\nhuman scores in QE. Our findings highlight the effectiveness of textual\nsimilarity as a robust QE metric, recommending its integration with other\nmetrics into QE frameworks and MT system training for improved accuracy and\nusability.", "published": "2024-06-11 16:48:17", "link": "http://arxiv.org/abs/2406.07440v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio\n  Understanding in Video-LLMs", "abstract": "In this paper, we present the VideoLLaMA 2, a set of Video Large Language\nModels (Video-LLMs) designed to enhance spatial-temporal modeling and audio\nunderstanding in video and audio-oriented tasks. Building upon its predecessor,\nVideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)\nconnector, which effectively captures the intricate spatial and temporal\ndynamics of video data. Additionally, we integrate an Audio Branch into the\nmodel through joint training, thereby enriching the multimodal understanding\ncapabilities of the model by seamlessly incorporating audio cues. Comprehensive\nevaluations on multiple-choice video question answering (MC-VQA), open-ended\nvideo question answering (OE-VQA), and video captioning (VC) tasks demonstrate\nthat VideoLLaMA 2 consistently achieves competitive results among open-source\nmodels and even gets close to some proprietary models on several benchmarks.\nFurthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and\naudio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.\nThese advancements underline VideoLLaMA 2's superior performance in multimodal\ncomprehension, setting a new standard for intelligent video analysis systems.\nAll models are public to facilitate further research.", "published": "2024-06-11 17:22:23", "link": "http://arxiv.org/abs/2406.07476v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CADS: A Systematic Literature Review on the Challenges of Abstractive\n  Dialogue Summarization", "abstract": "Abstractive dialogue summarization is the task of distilling conversations\ninto informative and concise summaries. Although reviews have been conducted on\nthis topic, there is a lack of comprehensive work detailing the challenges of\ndialogue summarization, unifying the differing understanding of the task, and\naligning proposed techniques, datasets, and evaluation metrics with the\nchallenges. This article summarizes the research on Transformer-based\nabstractive summarization for English dialogues by systematically reviewing\n1262 unique research papers published between 2019 and 2024, relying on the\nSemantic Scholar and DBLP databases. We cover the main challenges present in\ndialog summarization (i.e., language, structure, comprehension, speaker,\nsalience, and factuality) and link them to corresponding techniques such as\ngraph-based approaches, additional training tasks, and planning strategies,\nwhich typically overly rely on BART-based encoder-decoder models. We find that\nwhile some challenges, like language, have seen considerable progress, mainly\ndue to training methods, others, such as comprehension, factuality, and\nsalience, remain difficult and hold significant research opportunities. We\ninvestigate how these approaches are typically assessed, covering the datasets\nfor the subdomains of dialogue (e.g., meeting, medical), the established\nautomatic metrics and human evaluation approaches for assessing scores and\nannotator agreement. We observe that only a few datasets span across all\nsubdomains. The ROUGE metric is the most used, while human evaluation is\nfrequently reported without sufficient detail on inner-annotator agreement and\nannotation guidelines. Additionally, we discuss the possible implications of\nthe recently explored large language models and conclude that despite a\npotential shift in relevance and difficulty, our described challenge taxonomy\nremains relevant.", "published": "2024-06-11 17:30:22", "link": "http://arxiv.org/abs/2406.07494v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Image Textualization: An Automatic Framework for Creating Accurate and\n  Detailed Image Descriptions", "abstract": "Image description datasets play a crucial role in the advancement of various\napplications such as image understanding, text-to-image generation, and\ntext-image retrieval. Currently, image description datasets primarily originate\nfrom two sources. One source is the scraping of image-text pairs from the web.\nDespite their abundance, these descriptions are often of low quality and noisy.\nAnother is through human labeling. Datasets such as COCO are generally very\nshort and lack details. Although detailed image descriptions can be annotated\nby humans, the high annotation cost limits the feasibility. These limitations\nunderscore the need for more efficient and scalable methods to generate\naccurate and detailed image descriptions. In this paper, we propose an\ninnovative framework termed Image Textualization (IT), which automatically\nproduces high-quality image descriptions by leveraging existing multi-modal\nlarge language models (MLLMs) and multiple vision expert models in a\ncollaborative manner, which maximally convert the visual information into text.\nTo address the current lack of benchmarks for detailed descriptions, we propose\nseveral benchmarks for comprehensive evaluation, which verifies the quality of\nimage descriptions created by our framework. Furthermore, we show that\nLLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved\ncapability to generate richer image descriptions, substantially increasing the\nlength and detail of their output with less hallucination.", "published": "2024-06-11 17:37:45", "link": "http://arxiv.org/abs/2406.07502v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling", "abstract": "Efficiently modeling sequences with infinite context length has long been a\nchallenging problem. Previous approaches have either suffered from quadratic\ncomputational complexity or limited extrapolation ability in length\ngeneralization. In this work, we present Samba, a simple hybrid architecture\nthat layer-wise combines Mamba, a selective State Space Model (SSM), with\nSliding Window Attention (SWA). Samba selectively compresses a given sequence\ninto recurrent hidden states while still maintaining the ability to precisely\nrecall recent memories with the attention mechanism. We scale Samba up to 3.8B\nparameters with 3.2T training tokens and demonstrate that it significantly\noutperforms state-of-the-art models across a variety of benchmarks. Pretrained\non sequences of 4K length, Samba shows improved perplexity in context lengths\nof up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba\nefficiently extrapolates to a 256K context length with perfect memory recall on\nthe Passkey Retrieval task, and exhibits superior retrieval extrapolation on\nthe challenging Phonebook task compared to full-attention models. As a\nlinear-time sequence model, Samba achieves a 3.73x higher throughput compared\nto Transformers with grouped-query attention for user prompts of 128K length,\nand a 3.64x speedup when generating 64K tokens with unlimited streaming. Our\ncode for training on open source data is publicly available at\nhttps://github.com/microsoft/Samba.", "published": "2024-06-11 17:50:51", "link": "http://arxiv.org/abs/2406.07522v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs\n  Evaluation, Benchmark, and Arena", "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.", "published": "2024-06-11 17:59:47", "link": "http://arxiv.org/abs/2406.07545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AIM: Let Any Multi-modal Large Language Models Embrace Efficient\n  In-Context Learning", "abstract": "In-context learning (ICL) facilitates Large Language Models (LLMs) exhibiting\nemergent ability on downstream tasks without updating billions of parameters.\nHowever, in the area of multi-modal Large Language Models (MLLMs), two problems\nhinder the application of multi-modal ICL: (1) Most primary MLLMs are only\ntrained on single-image datasets, making them unable to read multi-modal\ndemonstrations. (2) With the demonstrations increasing, thousands of visual\ntokens highly challenge hardware and degrade ICL performance. During\npreliminary explorations, we discovered that the inner LLM tends to focus more\non the linguistic modality within multi-modal demonstrations to generate\nresponses. Therefore, we propose a general and light-weighted framework\n\\textbf{AIM} to tackle the mentioned problems through \\textbf{A}ggregating\n\\textbf{I}mage information of \\textbf{M}ultimodal demonstrations to the dense\nlatent space of the corresponding linguistic part. Specifically, AIM first uses\nthe frozen backbone MLLM to read each image-text demonstration and extracts the\nvector representations on top of the text. These vectors naturally fuse the\ninformation of the image-text pair, and AIM transforms them into fused virtual\ntokens acceptable for the inner LLM via a trainable projection layer.\nUltimately, these fused tokens function as variants of multi-modal\ndemonstrations, fed into the MLLM to direct its response to the current query\nas usual. Because these fused tokens stem from the textual component of the\nimage-text pair, a multi-modal demonstration is nearly reduced to a pure\ntextual demonstration, thus seamlessly applying to any MLLMs. With its de facto\nMLLM frozen, AIM is parameter-efficient and we train it on public multi-modal\nweb corpora which have nothing to do with downstream test tasks.", "published": "2024-06-11 08:12:43", "link": "http://arxiv.org/abs/2406.07588v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Tag and correct: high precision post-editing approach to correction of\n  speech recognition errors", "abstract": "This paper presents a new approach to the problem of correcting speech\nrecognition errors by means of post-editing. It consists of using a neural\nsequence tagger that learns how to correct an ASR (Automatic Speech\nRecognition) hypothesis word by word and a corrector module that applies\ncorrections returned by the tagger. The proposed solution is applicable to any\nASR system, regardless of its architecture, and provides high-precision control\nover errors being corrected. This is especially crucial in production\nenvironments, where avoiding the introduction of new mistakes by the error\ncorrection model may be more important than the net gain in overall results.\nThe results show that the performance of the proposed error correction models\nis comparable with previous approaches while requiring much smaller resources\nto train, which makes it suitable for industrial applications, where both\ninference latency and training times are critical factors that limit the use of\nother techniques.", "published": "2024-06-11 09:52:33", "link": "http://arxiv.org/abs/2406.07589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OPTune: Efficient Online Preference Tuning", "abstract": "Reinforcement learning with human feedback~(RLHF) is critical for aligning\nLarge Language Models (LLMs) with human preference. Compared to the widely\nstudied offline version of RLHF, \\emph{e.g.} direct preference optimization\n(DPO), recent works have shown that the online variants achieve even better\nalignment. However, online alignment requires on-the-fly generation of new\ntraining data, which is costly, hard to parallelize, and suffers from varying\nquality and utility. In this paper, we propose a more efficient data\nexploration strategy for online preference tuning (OPTune), which does not rely\non human-curated or pre-collected teacher responses but dynamically samples\ninformative responses for on-policy preference alignment. During data\ngeneration, OPTune only selects prompts whose (re)generated responses can\npotentially provide more informative and higher-quality training signals than\nthe existing responses. In the training objective, OPTune reweights each\ngenerated response (pair) by its utility in improving the alignment so that\nlearning can be focused on the most helpful samples. Throughout our\nevaluations, OPTune'd LLMs maintain the instruction-following benefits provided\nby standard preference tuning whilst enjoying 1.27-1.56x faster training speed\ndue to the efficient data exploration strategy.", "published": "2024-06-11 18:55:04", "link": "http://arxiv.org/abs/2406.07657v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Test-Time Fairness and Robustness in Large Language Models", "abstract": "Frontier Large Language Models (LLMs) can be socially discriminatory or\nsensitive to spurious features of their inputs. Because only well-resourced\ncorporations can train frontier LLMs, we need robust test-time strategies to\ncontrol such biases. Existing solutions, which instruct the LLM to be fair or\nrobust, rely on the model's implicit understanding of bias. Causality provides\na rich formalism through which we can be explicit about our debiasing\nrequirements. Yet, as we show, a naive application of the standard causal\ndebiasing strategy, counterfactual data augmentation, fails under standard\nassumptions to debias predictions at an individual level at test time. To\naddress this, we develop a stratified notion of debiasing called stratified\ninvariance, which can capture a range of debiasing requirements from population\nlevel to individual level through an additional measurement that stratifies the\npredictions. We present a complete observational test for stratified\ninvariance. Finally, we introduce a data augmentation strategy that guarantees\nstratified invariance at test time under suitable assumptions, together with a\nprompting strategy that encourages stratified invariance in LLMs. We show that\nour prompting strategy, unlike implicit instructions, consistently reduces the\nbias of frontier LLMs across a suite of synthetic and real-world benchmarks\nwithout requiring additional data, finetuning or pre-training.", "published": "2024-06-11 20:05:15", "link": "http://arxiv.org/abs/2406.07685v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer Models in Education: Summarizing Science Textbooks with\n  AraBART, MT5, AraT5, and mBART", "abstract": "Recently, with the rapid development in the fields of technology and the\nincreasing amount of text t available on the internet, it has become urgent to\ndevelop effective tools for processing and understanding texts in a way that\nsummaries the content without losing the fundamental essence of the\ninformation. Given this challenge, we have developed an advanced text\nsummarization system targeting Arabic textbooks. Relying on modern natu-ral\nlanguage processing models such as MT5, AraBART, AraT5, and mBART50, this\nsystem evaluates and extracts the most important sentences found in biology\ntextbooks for the 11th and 12th grades in the Palestinian curriculum, which\nenables students and teachers to obtain accurate and useful summaries that help\nthem easily understand the content. We utilized the Rouge metric to evaluate\nthe performance of the trained models. Moreover, experts in education Edu\ntextbook authoring assess the output of the trained models. This approach aims\nto identify the best solutions and clarify areas needing improvement. This\nresearch provides a solution for summarizing Arabic text. It enriches the field\nby offering results that can open new horizons for research and development in\nthe technologies for understanding and generating the Arabic language.\nAdditionally, it contributes to the field with Arabic texts through creating\nand compiling schoolbook texts and building a dataset.", "published": "2024-06-11 20:14:09", "link": "http://arxiv.org/abs/2406.07692v1", "categories": ["cs.CL", "cs.ET"], "primary_category": "cs.CL"}
{"title": "REAL Sampling: Boosting Factuality and Diversity of Open-Ended\n  Generation via Asymptotic Entropy", "abstract": "Decoding methods for large language models (LLMs) usually struggle with the\ntradeoff between ensuring factuality and maintaining diversity. For example, a\nhigher p threshold in the nucleus (top-p) sampling increases the diversity but\ndecreases the factuality, and vice versa. In this paper, we propose REAL\n(Residual Entropy from Asymptotic Line) sampling, a decoding method that\nachieves improved factuality and diversity over nucleus sampling by predicting\nan adaptive threshold of $p$. Specifically, REAL sampling predicts the\nstep-wise likelihood of an LLM to hallucinate, and lowers the p threshold when\nan LLM is likely to hallucinate. Otherwise, REAL sampling increases the p\nthreshold to boost the diversity. To predict the step-wise hallucination\nlikelihood without supervision, we construct a Token-level Hallucination\nForecasting (THF) model to predict the asymptotic entropy (i.e., inherent\nuncertainty) of the next token by extrapolating the next-token entropies from a\nseries of LLMs with different sizes. If a LLM's entropy is higher than the\nasymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF\nmodel predicts a high hallucination hazard, which leads to a lower p threshold\nin REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL\nsampling based on a 70M THF model can substantially improve the factuality and\ndiversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and\nhuman evaluation. After combined with contrastive decoding, REAL sampling\noutperforms 9 sampling methods, and generates texts that are more factual than\nthe greedy sampling and more diverse than the nucleus sampling with $p=0.5$.\nFurthermore, the predicted asymptotic entropy is also a useful unsupervised\nsignal for hallucination detection tasks.", "published": "2024-06-11 21:44:49", "link": "http://arxiv.org/abs/2406.07735v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception\n  and Humor Recognition", "abstract": "The Multimodal Sentiment Analysis Challenge (MuSe) 2024 addresses two\ncontemporary multimodal affect and sentiment analysis problems: In the Social\nPerception Sub-Challenge (MuSe-Perception), participants will predict 16\ndifferent social attributes of individuals such as assertiveness, dominance,\nlikability, and sincerity based on the provided audio-visual data. The\nCross-Cultural Humor Detection Sub-Challenge (MuSe-Humor) dataset expands upon\nthe Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset, focusing on\nthe detection of spontaneous humor in a cross-lingual and cross-cultural\nsetting. The main objective of MuSe 2024 is to unite a broad audience from\nvarious research domains, including multimodal sentiment analysis, audio-visual\naffective computing, continuous signal processing, and natural language\nprocessing. By fostering collaboration and exchange among experts in these\nfields, the MuSe 2024 endeavors to advance the understanding and application of\nsentiment analysis and affective computing across multiple modalities. This\nbaseline paper provides details on each sub-challenge and its corresponding\ndataset, extracted features from each data modality, and discusses challenge\nbaselines. For our baseline system, we make use of a range of Transformers and\nexpert-designed features and train Gated Recurrent Unit (GRU)-Recurrent Neural\nNetwork (RNN) models on them, resulting in a competitive baseline system. On\nthe unseen test datasets of the respective sub-challenges, it achieves a mean\nPearson's Correlation Coefficient ($\\rho$) of 0.3573 for MuSe-Perception and an\nArea Under the Curve (AUC) value of 0.8682 for MuSe-Humor.", "published": "2024-06-11 22:26:20", "link": "http://arxiv.org/abs/2406.07753v1", "categories": ["cs.AI", "cs.CL", "68T10", "I.2"], "primary_category": "cs.AI"}
{"title": "Question-Answering (QA) Model for a Personalized Learning Assistant for\n  Arabic Language", "abstract": "This paper describes the creation, optimization, and assessment of a\nquestion-answering (QA) model for a personalized learning assistant that uses\nBERT transformers customized for the Arabic language. The model was\nparticularly finetuned on science textbooks in Palestinian curriculum. Our\napproach uses BERT's brilliant capabilities to automatically produce correct\nanswers to questions in the field of science education. The model's ability to\nunderstand and extract pertinent information is improved by finetuning it using\n11th and 12th grade biology book in Palestinian curriculum. This increases the\nmodel's efficacy in producing enlightening responses. Exact match (EM) and F1\nscore metrics are used to assess the model's performance; the results show an\nEM score of 20% and an F1 score of 51%. These findings show that the model can\ncomprehend and react to questions in the context of Palestinian science book.\nThe results demonstrate the potential of BERT-based QA models to support\nlearning and understanding Arabic students questions.", "published": "2024-06-11 20:23:31", "link": "http://arxiv.org/abs/2406.08519v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Automated Question Generation for Science Tests in Arabic Language Using\n  NLP Techniques", "abstract": "Question generation for education assessments is a growing field within\nartificial intelligence applied to education. These question-generation tools\nhave significant importance in the educational technology domain, such as\nintelligent tutoring systems and dialogue-based platforms. The automatic\ngeneration of assessment questions, which entail clear-cut answers, usually\nrelies on syntactical and semantic indications within declarative sentences,\nwhich are then transformed into questions. Recent research has explored the\ngeneration of assessment educational questions in Arabic. The reported\nperformance has been adversely affected by inherent errors, including sentence\nparsing inaccuracies, name entity recognition issues, and errors stemming from\nrule-based question transformation. Furthermore, the complexity of lengthy\nArabic sentences has contributed to these challenges. This research presents an\ninnovative Arabic question-generation system built upon a three-stage process:\nkeywords and key phrases extraction, question generation, and subsequent\nranking. The aim is to tackle the difficulties associated with automatically\ngenerating assessment questions in the Arabic language. The proposed approach\nand results show a precision of 83.50%, a recall of 78.68%, and an Fl score of\n80.95%, indicating the framework high efficiency. Human evaluation further\nconfirmed the model efficiency, receiving an average rating of 84%.", "published": "2024-06-11 20:27:45", "link": "http://arxiv.org/abs/2406.08520v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exploring Traffic Crash Narratives in Jordan Using Text Mining Analytics", "abstract": "This study explores traffic crash narratives in an attempt to inform and\nenhance effective traffic safety policies using text-mining analytics. Text\nmining techniques are employed to unravel key themes and trends within the\nnarratives, aiming to provide a deeper understanding of the factors\ncontributing to traffic crashes. This study collected crash data from five\nmajor freeways in Jordan that cover narratives of 7,587 records from 2018-2022.\nAn unsupervised learning method was adopted to learn the pattern from crash\ndata. Various text mining techniques, such as topic modeling, keyword\nextraction, and Word Co-Occurrence Network, were also used to reveal the\nco-occurrence of crash patterns. Results show that text mining analytics is a\npromising method and underscore the multifactorial nature of traffic crashes,\nincluding intertwining human decisions and vehicular conditions. The recurrent\nthemes across all analyses highlight the need for a balanced approach to road\nsafety, merging both proactive and reactive measures. Emphasis on driver\neducation and awareness around animal-related incidents is paramount.", "published": "2024-06-11 20:07:39", "link": "http://arxiv.org/abs/2406.09438v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Flextron: Many-in-One Flexible Large Language Model", "abstract": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.", "published": "2024-06-11 01:16:10", "link": "http://arxiv.org/abs/2406.10260v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FoodSky: A Food-oriented Large Language Model that Passes the Chef and\n  Dietetic Examination", "abstract": "Food is foundational to human life, serving not only as a source of\nnourishment but also as a cornerstone of cultural identity and social\ninteraction. As the complexity of global dietary needs and preferences grows,\nfood intelligence is needed to enable food perception and reasoning for various\ntasks, ranging from recipe generation and dietary recommendation to\ndiet-disease correlation discovery and understanding. Towards this goal, for\npowerful capabilities across various domains and tasks in Large Language Models\n(LLMs), we introduce Food-oriented LLM FoodSky to comprehend food data through\nperception and reasoning. Considering the complexity and typicality of Chinese\ncuisine, we first construct one comprehensive Chinese food corpus FoodEarth\nfrom various authoritative sources, which can be leveraged by FoodSky to\nachieve deep understanding of food-related data. We then propose Topic-based\nSelective State Space Model (TS3M) and the Hierarchical Topic Retrieval\nAugmented Generation (HTRAG) mechanism to enhance FoodSky in capturing\nfine-grained food semantics and generating context-aware food-relevant text,\nrespectively. Our extensive evaluations demonstrate that FoodSky significantly\noutperforms general-purpose LLMs in both chef and dietetic examinations, with\nan accuracy of 67.2% and 66.4% on the Chinese National Chef Exam and the\nNational Dietetic Exam, respectively. FoodSky not only promises to enhance\nculinary creativity and promote healthier eating patterns, but also sets a new\nstandard for domain-specific LLMs that address complex real-world issues in the\nfood domain. An online demonstration of FoodSky is available at\nhttp://222.92.101.211:8200.", "published": "2024-06-11 01:27:00", "link": "http://arxiv.org/abs/2406.10261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model-empowered multimodal strain sensory system for\n  shape recognition, monitoring, and human interaction of tensegrity", "abstract": "A tensegrity-based system is a promising approach for dynamic exploration of\nuneven and unpredictable environments, particularly, space exploration.\nHowever, implementing such systems presents challenges in terms of intelligent\naspects: state recognition, wireless monitoring, human interaction, and smart\nanalyzing and advising function. Here, we introduce a 6-strut tensegrity\nintegrate with 24 multimodal strain sensors by leveraging both deep learning\nmodel and large language models to realize smart tensegrity. Using conductive\nflexible tendons assisted by long short-term memory model, the tensegrity\nachieves the self-shape reconstruction without extern sensors. Through\nintegrating the flask server and gpt-3.5-turbo model, the tensegrity\nautonomously enables to send data to iPhone for wireless monitoring and\nprovides data analysis, explanation, prediction, and suggestions to human for\ndecision making. Finally, human interaction system of the tensegrity helps\nhuman obtain necessary information of tensegrity from the aspect of human\nlanguage. Overall, this intelligent tensegrity-based system with self-sensing\ntendons showcases potential for future exploration, making it a versatile tool\nfor real-world applications.", "published": "2024-06-11 06:26:04", "link": "http://arxiv.org/abs/2406.10264v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Improving Language Models for Emotion Analysis: Insights from Cognitive\n  Science", "abstract": "We propose leveraging cognitive science research on emotions and\ncommunication to improve language models for emotion analysis. First, we\npresent the main emotion theories in psychology and cognitive science. Then, we\nintroduce the main methods of emotion annotation in natural language processing\nand their connections to psychological theories. We also present the two main\ntypes of analyses of emotional communication in cognitive pragmatics. Finally,\nbased on the cognitive science research presented, we propose directions for\nimproving language models for emotion analysis. We suggest that these research\nefforts pave the way for constructing new annotation schemes, methods, and a\npossible benchmark for emotional understanding, considering different facets of\nhuman emotion and communication.", "published": "2024-06-11 07:42:13", "link": "http://arxiv.org/abs/2406.10265v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COVID-19 Twitter Sentiment Classification Using Hybrid Deep Learning\n  Model Based on Grid Search Methodology", "abstract": "In the contemporary era, social media platforms amass an extensive volume of\nsocial data contributed by their users. In order to promptly grasp the opinions\nand emotional inclinations of individuals regarding a product or event, it\nbecomes imperative to perform sentiment analysis on the user-generated content.\nMicroblog comments often encompass both lengthy and concise text entries,\npresenting a complex scenario. This complexity is particularly pronounced in\nextensive textual content due to its rich content and intricate word\ninterrelations compared to shorter text entries. Sentiment analysis of public\nopinion shared on social networking websites such as Facebook or Twitter has\nevolved and found diverse applications. However, several challenges remain to\nbe tackled in this field. The hybrid methodologies have emerged as promising\nmodels for mitigating sentiment analysis errors, particularly when dealing with\nprogressively intricate training data. In this article, to investigate the\nhesitancy of COVID-19 vaccination, we propose eight different hybrid deep\nlearning models for sentiment classification with an aim of improving overall\naccuracy of the model. The sentiment prediction is achieved using embedding,\ndeep learning model and grid search algorithm on Twitter COVID-19 dataset.\nAccording to the study, public sentiment towards COVID-19 immunization appears\nto be improving with time, as evidenced by the gradual decline in vaccine\nreluctance. Through extensive evaluation, proposed model reported an increased\naccuracy of 98.86%, outperforming other models. Specifically, the combination\nof BERT, CNN and GS yield the highest accuracy, while the combination of GloVe,\nBiLSTM, CNN and GS follows closely behind with an accuracy of 98.17%. In\naddition, increase in accuracy in the range of 2.11% to 14.46% is reported by\nthe proposed model in comparisons with existing works.", "published": "2024-06-11 07:48:06", "link": "http://arxiv.org/abs/2406.10266v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unused information in token probability distribution of generative LLM:\n  improving LLM reading comprehension through calculation of expected values", "abstract": "LLM text decoding is key component for perceived LLM quality. We demonstrate\ntwo experiments showing that decoding methods could be improved by manipulation\nof token probabilities. First, we test few LLM on SummEval summary scoring\ndataset, to measure reading comprehension. We compare scores from greedy\ndecoding to expected values over the next token distribution. We scale logits\nby large temperature to increase the entropy of scores. This allows strong\nimprovement of performance on SummEval (in terms of correlations to human\njudgement). We see improvement from 6-8% to 13-28% for 7B Mistral and from\n20%-46% to 37%-56% for Mixtral, beating GPT 4 0314 result on two metrics. Part\nof the gain seems related to positional bias. Secondly, we use\nprobability-based tree sampling algorithm, to examine all most probable\ngenerations for given prompt.", "published": "2024-06-11 09:24:18", "link": "http://arxiv.org/abs/2406.10267v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Conceptual Framework For Trie-Augmented Neural Networks (TANNS)", "abstract": "Trie-Augmented Neural Networks (TANNs) combine trie structures with neural\nnetworks, forming a hierarchical design that enhances decision-making\ntransparency and efficiency in machine learning. This paper investigates the\nuse of TANNs for text and document classification, applying Recurrent Neural\nNetworks (RNNs) and Feed forward Neural Networks (FNNs). We evaluated TANNs on\nthe 20 NewsGroup and SMS Spam Collection datasets, comparing their performance\nwith traditional RNN and FFN Networks with and without dropout regularization.\nThe results show that TANNs achieve similar or slightly better performance in\ntext classification. The primary advantage of TANNs is their structured\ndecision-making process, which improves interpretability. We discuss\nimplementation challenges and practical limitations. Future work will aim to\nrefine the TANNs architecture for more complex classification tasks.", "published": "2024-06-11 17:08:16", "link": "http://arxiv.org/abs/2406.10270v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language\n  Translation at Scale", "abstract": "A persistent challenge in sign language video processing, including the task\nof sign language to written language translation, is how we learn\nrepresentations of sign language in an effective and efficient way that can\npreserve the important attributes of these languages, while remaining invariant\nto irrelevant visual differences. Informed by the nature and linguistics of\nsigned languages, our proposed method focuses on just the most relevant parts\nin a signing video: the face, hands and body posture of the signer. However,\ninstead of using pose estimation coordinates from off-the-shelf pose tracking\nmodels, which have inconsistent performance for hands and faces, we propose to\nlearn the complex handshapes and rich facial expressions of sign languages in a\nself-supervised fashion. Our approach is based on learning from individual\nframes (rather than video sequences) and is therefore much more efficient than\nprior work on sign language pre-training. Compared to a recent model that\nestablished a new state of the art in sign language translation on the How2Sign\ndataset, our approach yields similar translation performance, using less than\n3\\% of the compute.", "published": "2024-06-11 03:00:41", "link": "http://arxiv.org/abs/2406.06907v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Non-autoregressive Generation Framework for End-to-End Simultaneous\n  Speech-to-Speech Translation", "abstract": "Simultaneous translation models play a crucial role in facilitating\ncommunication. However, existing research primarily focuses on text-to-text or\nspeech-to-text models, necessitating additional cascade components to achieve\nspeech-to-speech translation. These pipeline methods suffer from error\npropagation and accumulate delays in each cascade component, resulting in\nreduced synchronization between the speaker and listener. To overcome these\nchallenges, we propose a novel non-autoregressive generation framework for\nsimultaneous speech translation (NAST-S2X), which integrates speech-to-text and\nspeech-to-speech tasks into a unified end-to-end framework. We develop a\nnon-autoregressive decoder capable of concurrently generating multiple text or\nacoustic unit tokens upon receiving fixed-length speech chunks. The decoder can\ngenerate blank or repeated tokens and employ CTC decoding to dynamically adjust\nits latency. Experimental results show that NAST-S2X outperforms\nstate-of-the-art models in both speech-to-text and speech-to-speech tasks. It\nachieves high-quality simultaneous interpretation within a delay of less than 3\nseconds and provides a 28 times decoding speedup in offline generation.", "published": "2024-06-11 04:25:48", "link": "http://arxiv.org/abs/2406.06937v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Missingness-resilient Video-enhanced Multimodal Disfluency Detection", "abstract": "Most existing speech disfluency detection techniques only rely upon acoustic\ndata. In this work, we present a practical multimodal disfluency detection\napproach that leverages available video data together with audio. We curate an\naudiovisual dataset and propose a novel fusion technique with unified\nweight-sharing modality-agnostic encoders to learn the temporal and semantic\ncontext. Our resilient design accommodates real-world scenarios where the video\nmodality may sometimes be missing during inference. We also present alternative\nfusion strategies when both modalities are assured to be complete. In\nexperiments across five disfluency-detection tasks, our unified multimodal\napproach significantly outperforms Audio-only unimodal methods, yielding an\naverage absolute improvement of 10% (i.e., 10 percentage point increase) when\nboth video and audio modalities are always available, and 7% even when video\nmodality is missing in half of the samples.", "published": "2024-06-11 05:47:16", "link": "http://arxiv.org/abs/2406.06964v1", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bridging Language Gaps in Audio-Text Retrieval", "abstract": "Audio-text retrieval is a challenging task, requiring the search for an audio\nclip or a text caption within a database. The predominant focus of existing\nresearch on English descriptions poses a limitation on the applicability of\nsuch models, given the abundance of non-English content in real-world data. To\naddress these linguistic disparities, we propose a language enhancement (LE),\nusing a multilingual text encoder (SONAR) to encode the text data with\nlanguage-specific information. Additionally, we optimize the audio encoder\nthrough the application of consistent ensemble distillation (CED), enhancing\nsupport for variable-length audio-text retrieval. Our methodology excels in\nEnglish audio-text retrieval, demonstrating state-of-the-art (SOTA) performance\non commonly used datasets such as AudioCaps and Clotho. Simultaneously, the\napproach exhibits proficiency in retrieving content in seven other languages\nwith only 10% of additional language-enhanced training data, yielding promising\nresults. The source code is publicly available\nhttps://github.com/zyyan4/ml-clap.", "published": "2024-06-11 07:12:12", "link": "http://arxiv.org/abs/2406.07012v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal\n  Large Language Models", "abstract": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.", "published": "2024-06-11 08:38:13", "link": "http://arxiv.org/abs/2406.07057v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Tool-Augmented Large Language Models: Integrating Insights\n  from Errors in Inference Trees", "abstract": "Tool-augmented large language models (LLMs) leverage tools, often in the form\nof APIs, to improve their reasoning capabilities on complex tasks. This enables\nthem to act as intelligent agents interacting with the real world. The recently\nintroduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first\nsearch-based decision tree (DFSDT) mechanism for multi-step reasoning with\n$16000+$ real-world APIs, effectively enhancing the performance of\ntool-augmented LLMs compared to traditional chain reasoning mechanisms.\nHowever, their approach only employs successful paths from decision trees (also\ncalled inference trees) for supervised fine-tuning (SFT), missing out on the\npotential learning opportunities from failed paths. Inspired by this, we\npropose an inference trajectory optimization framework based on preference\nlearning to address this limitation. We first introduce a novel method for\nconstructing step-wise preference data from tree-like expert trajectories,\nwhich leverages the previously ignored failed explorations in the decision\ntrees. In the subsequent training phase, we first fine-tune the LLM with\nsuccessful tool-usage expert trajectories and then apply direct preference\noptimization (DPO) with the preference data to update the LLM's policy,\nresulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only\nenhances the utilization of original expert data but also broadens the learning\nspace of the model. Our experiments demonstrate that by obtaining insights from\nerrors in inference trees, TP-LLaMA significantly outperforms the baselines\nacross almost all test scenarios by a large margin and exhibits better\ngeneralization capabilities with unseen APIs. At the same time, TP-LLaMA has\nalso demonstrated superior reasoning efficiency compared to the baselines,\nmaking it more suitable for complex tool-usage reasoning tasks.", "published": "2024-06-11 10:00:18", "link": "http://arxiv.org/abs/2406.07115v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating speech with just images", "abstract": "Visually grounded speech models link speech to images. We extend this\nconnection by linking images to text via an existing image captioning system,\nand as a result gain the ability to map speech audio directly to text. This\napproach can be used for speech translation with just images by having the\naudio in a different language from the generated captions. We investigate such\na system on a real low-resource language, Yor\\`ub\\'a, and propose a\nYor\\`ub\\'a-to-English speech translation model that leverages pretrained\ncomponents in order to be able to learn in the low-resource regime. To limit\noverfitting, we find that it is essential to use a decoding scheme that\nproduces diverse image captions for training. Results show that the predicted\ntranslations capture the main semantics of the spoken audio, albeit in a\nsimpler and shorter form.", "published": "2024-06-11 10:29:24", "link": "http://arxiv.org/abs/2406.07133v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Trustworthy and Practical AI for Healthcare: A Guided Deferral System\n  with Large Language Models", "abstract": "Large language models (LLMs) offer a valuable technology for various\napplications in healthcare. However, their tendency to hallucinate and the\nexisting reliance on proprietary systems pose challenges in environments\nconcerning critical decision-making and strict data privacy regulations, such\nas healthcare, where the trust in such systems is paramount. Through combining\nthe strengths and discounting the weaknesses of humans and AI, the field of\nHuman-AI Collaboration (HAIC) presents one front for tackling these challenges\nand hence improving trust. This paper presents a novel HAIC guided deferral\nsystem that can simultaneously parse medical reports for disorder\nclassification, and defer uncertain predictions with intelligent guidance to\nhumans. We develop methodology which builds efficient, effective and\nopen-source LLMs for this purpose, for the real-world deployment in healthcare.\nWe conduct a pilot study which showcases the effectiveness of our proposed\nsystem in practice. Additionally, we highlight drawbacks of standard\ncalibration metrics in imbalanced data scenarios commonly found in healthcare,\nand suggest a simple yet effective solution: the Imbalanced Expected\nCalibration Error.", "published": "2024-06-11 12:41:54", "link": "http://arxiv.org/abs/2406.07212v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Synthetic Dataset for Personal Attribute Inference", "abstract": "Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.", "published": "2024-06-11 12:50:53", "link": "http://arxiv.org/abs/2406.07217v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Autoformalization using Type Checking", "abstract": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.", "published": "2024-06-11 13:01:50", "link": "http://arxiv.org/abs/2406.07222v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scientific Computing with Large Language Models", "abstract": "We provide an overview of the emergence of large language models for\nscientific computing applications. We highlight use cases that involve natural\nlanguage processing of scientific documents and specialized languages designed\nto describe physical systems. For the former, chatbot style applications appear\nin medicine, mathematics and physics and can be used iteratively with domain\nexperts for problem solving. We also review specialized languages within\nmolecular biology, the languages of molecules, proteins, and DNA where language\nmodels are being used to predict properties and even create novel physical\nsystems at much faster rates than traditional computing methods.", "published": "2024-06-11 13:39:07", "link": "http://arxiv.org/abs/2406.07259v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation", "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.", "published": "2024-06-11 13:52:29", "link": "http://arxiv.org/abs/2406.07268v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Speaking Your Language: Spatial Relationships in Interpretable Emergent\n  Communication", "abstract": "Effective communication requires the ability to refer to specific parts of an\nobservation in relation to others. While emergent communication literature\nshows success in developing various language properties, no research has shown\nthe emergence of such positional references. This paper demonstrates how agents\ncan communicate about spatial relationships within their observations. The\nresults indicate that agents can develop a language capable of expressing the\nrelationships between parts of their observation, achieving over 90% accuracy\nwhen trained in a referential game which requires such communication. Using a\ncollocation measure, we demonstrate how the agents create such references. This\nanalysis suggests that agents use a mixture of non-compositional and\ncompositional messages to convey spatial relationships. We also show that the\nemergent language is interpretable by humans. The translation accuracy is\ntested by communicating with the receiver agent, where the receiver achieves\nover 78% accuracy using parts of this lexicon, confirming that the\ninterpretation of the emergent language was successful.", "published": "2024-06-11 14:04:25", "link": "http://arxiv.org/abs/2406.07277v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Can We Achieve High-quality Direct Speech-to-Speech Translation without\n  Parallel Speech Data?", "abstract": "Recently proposed two-pass direct speech-to-speech translation (S2ST) models\ndecompose the task into speech-to-text translation (S2TT) and text-to-speech\n(TTS) within an end-to-end model, yielding promising results. However, the\ntraining of these models still relies on parallel speech data, which is\nextremely challenging to collect. In contrast, S2TT and TTS have accumulated a\nlarge amount of data and pretrained models, which have not been fully utilized\nin the development of S2ST models. Inspired by this, in this paper, we first\nintroduce a composite S2ST model named ComSpeech, which can seamlessly\nintegrate any pretrained S2TT and TTS models into a direct S2ST model.\nFurthermore, to eliminate the reliance on parallel speech data, we propose a\nnovel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It\naligns representations in the latent space through contrastive learning,\nenabling the speech synthesis capability learned from the TTS data to\ngeneralize to S2ST in a zero-shot manner. Experimental results on the CVSS\ndataset show that when the parallel speech data is available, ComSpeech\nsurpasses previous two-pass models like UnitY and Translatotron 2 in both\ntranslation quality and decoding speed. When there is no parallel speech data,\nComSpeech-ZS lags behind \\name by only 0.7 ASR-BLEU and outperforms the\ncascaded models.", "published": "2024-06-11 14:17:12", "link": "http://arxiv.org/abs/2406.07289v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Context and Feedback Embeddings in Spoken Dialogue", "abstract": "Short feedback responses, such as backchannels, play an important role in\nspoken dialogue. So far, most of the modeling of feedback responses has focused\non their timing, often neglecting how their lexical and prosodic form influence\ntheir contextual appropriateness and conversational function. In this paper, we\ninvestigate the possibility of embedding short dialogue contexts and feedback\nresponses in the same representation space using a contrastive learning\nobjective. In our evaluation, we primarily focus on how such embeddings can be\nused as a context-feedback appropriateness metric and thus for feedback\nresponse ranking in U.S. English dialogues. Our results show that the model\noutperforms humans given the same ranking task and that the learned embeddings\ncarry information about the conversational function of feedback responses.", "published": "2024-06-11 14:22:37", "link": "http://arxiv.org/abs/2406.07291v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BertaQA: How Much Do Language Models Know About Local Culture?", "abstract": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but\nmost evaluations have been limited to global or anglocentric subjects. This\nraises the question of how well these models perform on topics relevant to\nother cultures, whose presence on the web is not that prominent. To address\nthis gap, we introduce BertaQA, a multiple-choice trivia dataset that is\nparallel in English and Basque. The dataset consists of a local subset with\nquestions pertinent to the Basque culture, and a global subset with questions\nof broader interest. We find that state-of-the-art LLMs struggle with local\ncultural knowledge, even as they excel on global topics. However, we show that\ncontinued pre-training in Basque significantly improves the models' performance\non Basque culture, even when queried in English. To our knowledge, this is the\nfirst solid evidence of knowledge transfer from a low-resource to a\nhigh-resource language. Our analysis sheds light on the complex interplay\nbetween language and knowledge, and reveals that some prior findings do not\nfully hold when reassessed on local topics. Our dataset and evaluation code are\navailable under open licenses at https://github.com/juletx/BertaQA.", "published": "2024-06-11 14:30:34", "link": "http://arxiv.org/abs/2406.07302v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword\n  Spotting", "abstract": "In this paper, we propose MM-KWS, a novel approach to user-defined keyword\nspotting leveraging multi-modal enrollments of text and speech templates.\nUnlike previous methods that focus solely on either text or speech features,\nMM-KWS extracts phoneme, text, and speech embeddings from both modalities.\nThese embeddings are then compared with the query speech embedding to detect\nthe target keywords. To ensure the applicability of MM-KWS across diverse\nlanguages, we utilize a feature extractor incorporating several multilingual\npre-trained models. Subsequently, we validate its effectiveness on Mandarin and\nEnglish tasks. In addition, we have integrated advanced data augmentation tools\nfor hard case mining to enhance MM-KWS in distinguishing confusable words.\nExperimental results on the LibriPhrase and WenetPhrase datasets demonstrate\nthat MM-KWS outperforms prior methods significantly.", "published": "2024-06-11 14:38:29", "link": "http://arxiv.org/abs/2406.07310v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward", "abstract": "Aligning large language models (LLMs) with human preferences has gained\nsignificant attention, with Proximal Policy Optimization (PPO) as a standard\nyet computationally expensive method and Direct Preference Optimization (DPO)\nas a more efficient alternative. While DPO offers simplicity, it remains\nunderutilized in state-of-the-art LLMs, suggesting potential limitations. In\nthis work, we revisit DPO, analyzing its theoretical foundations and empirical\nperformance to bridge this gap. We identify three key properties, termed 3D\nproperties, that emerge from DPO's learning process: Drastic drop in rejected\nresponse likelihood, Degradation into response suppression, and Dispersion\neffect on unseen responses. We show that these issues arise from DPO's\noptimization dynamics, where the interaction between chosen and rejected\nresponse gradients leads to instability. Our findings are supported by\nexperiments on both a controlled toy model and real-world LLM tasks, including\nmathematical problem-solving and instruction following. To address these\nchallenges, we propose simple regularization techniques that improve training\nstability and performance. Additionally, we examine how preference data\ndistribution impacts DPO's effectiveness, offering insights into how alignment\nmodels handle out-of-domain (OOD) data. Our work connects these observations to\nbroader research and provides a theoretical explanation for DPO's limitations.\nWe hope these insights will guide future advancements in reward-model-free\npreference learning, bringing it closer to reward-model-based approaches.", "published": "2024-06-11 14:59:24", "link": "http://arxiv.org/abs/2406.07327v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "CTC-based Non-autoregressive Textless Speech-to-Speech Translation", "abstract": "Direct speech-to-speech translation (S2ST) has achieved impressive\ntranslation quality, but it often faces the challenge of slow decoding due to\nthe considerable length of speech sequences. Recently, some research has turned\nto non-autoregressive (NAR) models to expedite decoding, yet the translation\nquality typically lags behind autoregressive (AR) models significantly. In this\npaper, we investigate the performance of CTC-based NAR models in S2ST, as these\nmodels have shown impressive results in machine translation. Experimental\nresults demonstrate that by combining pretraining, knowledge distillation, and\nadvanced NAR training techniques such as glancing training and non-monotonic\nlatent alignments, CTC-based NAR models achieve translation quality comparable\nto the AR model, while preserving up to 26.81$\\times$ decoding speedup.", "published": "2024-06-11 15:00:33", "link": "http://arxiv.org/abs/2406.07330v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AI Sandbagging: Language Models can Strategically Underperform on\n  Evaluations", "abstract": "Trustworthy capability evaluations are crucial for ensuring the safety of AI\nsystems, and are becoming a key component of AI regulation. However, the\ndevelopers of an AI system, or the AI system itself, may have incentives for\nevaluations to understate the AI's actual capability. These conflicting\ninterests lead to the problem of sandbagging, which we define as strategic\nunderperformance on an evaluation. In this paper we assess sandbagging\ncapabilities in contemporary language models (LMs). We prompt frontier LMs,\nlike GPT-4 and Claude 3 Opus, to selectively underperform on dangerous\ncapability evaluations, while maintaining performance on general (harmless)\ncapability evaluations. Moreover, we find that models can be fine-tuned, on a\nsynthetic dataset, to hide specific capabilities unless given a password. This\nbehaviour generalizes to high-quality, held-out benchmarks such as WMDP. In\naddition, we show that both frontier and smaller models can be prompted or\npassword-locked to target specific scores on a capability evaluation. We have\nmediocre success in password-locking a model to mimic the answers a weaker\nmodel would give. Overall, our results suggest that capability evaluations are\nvulnerable to sandbagging. This vulnerability decreases the trustworthiness of\nevaluations, and thereby undermines important safety decisions regarding the\ndevelopment and deployment of advanced AI systems.", "published": "2024-06-11 15:26:57", "link": "http://arxiv.org/abs/2406.07358v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models", "abstract": "Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.", "published": "2024-06-11 15:34:43", "link": "http://arxiv.org/abs/2406.07368v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Belief Prediction", "abstract": "Recognizing a speaker's level of commitment to a belief is a difficult task;\nhumans do not only interpret the meaning of the words in context, but also\nunderstand cues from intonation and other aspects of the audio signal. Many\npapers and corpora in the NLP community have approached the belief prediction\ntask using text-only approaches. We are the first to frame and present results\non the multimodal belief prediction task. We use the CB-Prosody corpus (CBP),\ncontaining aligned text and audio with speaker belief annotations. We first\nreport baselines and significant features using acoustic-prosodic features and\ntraditional machine learning methods. We then present text and audio baselines\nfor the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we\npresent our multimodal architecture which fine-tunes on BERT and Whisper and\nuses multiple fusion methods, improving on both modalities alone.", "published": "2024-06-11 17:12:41", "link": "http://arxiv.org/abs/2406.07466v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TextGrad: Automatic \"Differentiation\" via Text", "abstract": "AI is undergoing a paradigm shift, with breakthroughs achieved by systems\norchestrating multiple large language models (LLMs) and other complex\ncomponents. As a result, developing principled and automated optimization\nmethods for compound AI systems is one of the most important new challenges.\nNeural networks faced a similar challenge in its early days until\nbackpropagation and automatic differentiation transformed the field by making\noptimization turn-key. Inspired by this, we introduce TextGrad, a powerful\nframework performing automatic ``differentiation'' via text. TextGrad\nbackpropagates textual feedback provided by LLMs to improve individual\ncomponents of a compound AI system. In our framework, LLMs provide rich,\ngeneral, natural language suggestions to optimize variables in computation\ngraphs, ranging from code snippets to molecular structures. TextGrad follows\nPyTorch's syntax and abstraction and is flexible and easy-to-use. It works\nout-of-the-box for a variety of tasks, where the users only provide the\nobjective function without tuning components or prompts of the framework. We\nshowcase TextGrad's effectiveness and generality across a diverse range of\napplications, from question answering and molecule optimization to radiotherapy\ntreatment planning. Without modifying the framework, TextGrad improves the\nzero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to\n$55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard\ncoding problem solutions, improves prompts for reasoning, designs new druglike\nsmall molecules with desirable in silico binding, and designs radiation\noncology treatment plans with high specificity. TextGrad lays a foundation to\naccelerate the development of the next-generation of AI systems.", "published": "2024-06-11 17:32:21", "link": "http://arxiv.org/abs/2406.07496v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Masked Diffusion Language Models", "abstract": "While diffusion models excel at generating high-quality images, prior work\nreports a significant performance gap between diffusion and autoregressive (AR)\nmethods in language modeling. In this work, we show that simple masked discrete\ndiffusion is more performant than previously thought. We apply an effective\ntraining recipe that improves the performance of masked diffusion models and\nderive a simplified, Rao-Blackwellized objective that results in additional\nimprovements. Our objective has a simple form -- it is a mixture of classical\nmasked language modeling losses -- and can be used to train encoder-only\nlanguage models that admit efficient samplers, including ones that can generate\narbitrary lengths of text semi-autoregressively like a traditional language\nmodel. On language modeling benchmarks, a range of masked diffusion models\ntrained with modern engineering practices achieves a new state-of-the-art among\ndiffusion models, and approaches AR perplexity. We provide the code, along with\na blog post and video tutorial on the project page: https://s-sahoo.com/mdlm", "published": "2024-06-11 17:51:40", "link": "http://arxiv.org/abs/2406.07524v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Situational Awareness Matters in 3D Vision Language Reasoning", "abstract": "Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.", "published": "2024-06-11 17:59:45", "link": "http://arxiv.org/abs/2406.07544v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models\n  Understand Commonsense?", "abstract": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that align with\ncommonsense in real life, which we call Commonsense-T2I. Given two adversarial\ntext prompts containing an identical set of action words with minor\ndifferences, such as \"a lightbulb without electricity\" v.s. \"a lightbulb with\nelectricity\", we evaluate whether T2I models can conduct visual-commonsense\nreasoning, e.g. produce images that fit \"the lightbulb is unlit\" vs. \"the\nlightbulb is lit\" correspondingly. Commonsense-T2I presents an adversarial\nchallenge, providing pairwise text prompts along with expected outputs. The\ndataset is carefully hand-curated by experts and annotated with fine-grained\nlabels, such as commonsense type and likelihood of the expected outputs, to\nassist analyzing model behavior. We benchmark a variety of state-of-the-art\n(sota) T2I models and surprisingly find that, there is still a large gap\nbetween image synthesis and real life photos--even the DALL-E 3 model could\nonly achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only\nachieves 24.92% accuracy. Our experiments show that GPT-enriched prompts cannot\nsolve this challenge, and we include a detailed analysis about possible reasons\nfor such deficiency. We aim for Commonsense-T2I to serve as a high-quality\nevaluation benchmark for T2I commonsense checking, fostering advancements in\nreal life image generation.", "published": "2024-06-11 17:59:48", "link": "http://arxiv.org/abs/2406.07546v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal\n  Large Language Models", "abstract": "Powered by remarkable advancements in Large Language Models (LLMs),\nMultimodal Large Language Models (MLLMs) demonstrate impressive capabilities in\nmanifold tasks. However, the practical application scenarios of MLLMs are\nintricate, exposing them to potential malicious instructions and thereby posing\nsafety risks. While current benchmarks do incorporate certain safety\nconsiderations, they often lack comprehensive coverage and fail to exhibit the\nnecessary rigor and robustness. For instance, the common practice of employing\nGPT-4V as both the evaluator and a model to be evaluated lacks credibility, as\nit tends to exhibit a bias toward its own responses. In this paper, we present\nMLLMGuard, a multidimensional safety evaluation suite for MLLMs, including a\nbilingual image-text evaluation dataset, inference utilities, and a lightweight\nevaluator. MLLMGuard's assessment comprehensively covers two languages (English\nand Chinese) and five important safety dimensions (Privacy, Bias, Toxicity,\nTruthfulness, and Legality), each with corresponding rich subtasks. Focusing on\nthese dimensions, our evaluation dataset is primarily sourced from platforms\nsuch as social media, and it integrates text-based and image-based red teaming\ntechniques with meticulous annotation by human experts. This can prevent\ninaccurate evaluation caused by data leakage when using open-source datasets\nand ensures the quality and challenging nature of our benchmark. Additionally,\na fully automated lightweight evaluator termed GuardRank is developed, which\nachieves significantly higher evaluation accuracy than GPT-4. Our evaluation\nresults across 13 advanced models indicate that MLLMs still have a substantial\njourney ahead before they can be considered safe and responsible.", "published": "2024-06-11 13:41:33", "link": "http://arxiv.org/abs/2406.07594v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "UICoder: Finetuning Large Language Models to Generate User Interface\n  Code through Automated Feedback", "abstract": "Large language models (LLMs) struggle to consistently generate UI code that\ncompiles and produces visually relevant designs. Existing approaches to improve\ngeneration rely on expensive human feedback or distilling a proprietary model.\nIn this paper, we explore the use of automated feedback (compilers and\nmulti-modal models) to guide LLMs to generate high-quality UI code. Our method\nstarts with an existing LLM and iteratively produces improved models by\nself-generating a large synthetic dataset using an original model, applying\nautomated tools to aggressively filter, score, and de-duplicate the data into a\nrefined higher quality dataset. The original LLM is improved by finetuning on\nthis refined dataset. We applied our approach to several open-source LLMs and\ncompared the resulting performance to baseline models with both automated\nmetrics and human preferences. Our evaluation shows the resulting models\noutperform all other downloadable baselines and approach the performance of\nlarger proprietary models.", "published": "2024-06-11 21:53:46", "link": "http://arxiv.org/abs/2406.07739v1", "categories": ["cs.CL", "cs.HC", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Autograding Mathematical Induction Proofs with Natural Language\n  Processing", "abstract": "In mathematical proof education, there remains a need for interventions that\nhelp students learn to write mathematical proofs. Research has shown that\ntimely feedback can be very helpful to students learning new skills. While for\nmany years natural language processing models have struggled to perform well on\ntasks related to mathematical texts, recent developments in natural language\nprocessing have created the opportunity to complete the task of giving students\ninstant feedback on their mathematical proofs. In this paper, we present a set\nof training methods and models capable of autograding freeform mathematical\nproofs by leveraging existing large language models and other machine learning\ntechniques. The models are trained using proof data collected from four\ndifferent proof by induction problems. We use four different robust large\nlanguage models to compare their performances, and all achieve satisfactory\nperformances to various degrees. Additionally, we recruit human graders to\ngrade the same proofs as the training data, and find that the best grading\nmodel is also more accurate than most human graders. With the development of\nthese grading models, we create and deploy an autograder for proof by induction\nproblems and perform a user study with students. Results from the study shows\nthat students are able to make significant improvements to their proofs using\nthe feedback from the autograder, but students still do not trust the AI\nautograders as much as they trust human graders. Future work can improve on the\nautograder feedback and figure out ways to help students trust AI autograders.", "published": "2024-06-11 15:30:26", "link": "http://arxiv.org/abs/2406.10268v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Markov Constraint as Large Language Model Surrogate", "abstract": "This paper presents NgramMarkov, a variant of the Markov constraints. It is\ndedicated to text generation in constraint programming (CP). It involves a set\nof n-grams (i.e., sequence of n words) associated with probabilities given by a\nlarge language model (LLM). It limits the product of the probabilities of the\nn-gram of a sentence. The propagator of this constraint can be seen as an\nextension of the ElementaryMarkov constraint propagator, incorporating the LLM\ndistribution instead of the maximum likelihood estimation of n-grams. It uses a\ngliding threshold, i.e., it rejects n-grams whose local probabilities are too\nlow, to guarantee balanced solutions. It can also be combined with a\n\"look-ahead\" approach to remove n-grams that are very unlikely to lead to\nacceptable sentences for a fixed-length horizon. This idea is based on the\nMDDMarkovProcess constraint propagator, but without explicitly using an MDD\n(Multi-Valued Decision Diagram). The experimental results show that the\ngenerated text is valued in a similar way to the LLM perplexity function. Using\nthis new constraint dramatically reduces the number of candidate sentences\nproduced, improves computation times, and allows larger corpora or smaller\nn-grams to be used. A real-world problem has been solved for the first time\nusing 4-grams instead of 5-grams.", "published": "2024-06-11 16:09:53", "link": "http://arxiv.org/abs/2406.10269v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Connected Speech-Based Cognitive Assessment in Chinese and English", "abstract": "We present a novel benchmark dataset and prediction tasks for investigating\napproaches to assess cognitive function through analysis of connected speech.\nThe dataset consists of speech samples and clinical information for speakers of\nMandarin Chinese and English with different levels of cognitive impairment as\nwell as individuals with normal cognition. These data have been carefully\nmatched by age and sex by propensity score analysis to ensure balance and\nrepresentativity in model training. The prediction tasks encompass mild\ncognitive impairment diagnosis and cognitive test score prediction. This\nframework was designed to encourage the development of approaches to\nspeech-based cognitive assessment which generalise across languages. We\nillustrate it by presenting baseline prediction models that employ\nlanguage-agnostic and comparable features for diagnosis and cognitive test\nscore prediction. The models achieved unweighted average recall was 59.2% in\ndiagnosis, and root mean squared error of 2.89 in score prediction.", "published": "2024-06-11 19:04:29", "link": "http://arxiv.org/abs/2406.10272v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "J.3; I.5.4"], "primary_category": "cs.CL"}
{"title": "Beyond Words: On Large Language Models Actionability in Mission-Critical\n  Risk Analysis", "abstract": "Context. Risk analysis assesses potential risks in specific scenarios. Risk\nanalysis principles are context-less; the same methodology can be applied to a\nrisk connected to health and information technology security. Risk analysis\nrequires a vast knowledge of national and international regulations and\nstandards and is time and effort-intensive. A large language model can quickly\nsummarize information in less time than a human and can be fine-tuned to\nspecific tasks.\n  Aim. Our empirical study aims to investigate the effectiveness of\nRetrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our\nknowledge, no prior study has explored its capabilities in risk analysis.\n  Method. We manually curated 193 unique scenarios leading to 1283\nrepresentative samples from over 50 mission-critical analyses archived by the\nindustrial context team in the last five years. We compared the base GPT-3.5\nand GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned\ncounterparts. We employ two human experts as competitors of the models and\nthree other human experts to review the models and the former human experts'\nanalysis. The reviewers analyzed 5,000 scenario analyses.\n  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs\nare quicker and more actionable. Moreover, our findings show that RAG-assisted\nLLMs have the lowest hallucination rates, effectively uncovering hidden risks\nand complementing human expertise. Thus, the choice of model depends on\nspecific needs, with FTMs for accuracy, RAG for hidden risks discovery, and\nbase models for comprehensiveness and actionability. Therefore, experts can\nleverage LLMs as an effective complementing companion in risk analysis within a\ncondensed timeframe. They can also save costs by averting unnecessary expenses\nassociated with implementing unwarranted countermeasures.", "published": "2024-06-11 19:20:27", "link": "http://arxiv.org/abs/2406.10273v5", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Using General Large Language Models to Classify Mathematical Documents", "abstract": "In this article we report on an initial exploration to assess the viability\nof using the general large language models (LLMs), recently made public, to\nclassify mathematical documents. Automated classification would be useful from\nthe applied perspective of improving the navigation of the literature and the\nmore open-ended goal of identifying relations among mathematical results. The\nMathematical Subject Classification MSC 2020, from MathSciNet and zbMATH, is\nwidely used and there is a significant corpus of ground truth material in the\nopen literature. We have evaluated the classification of preprint articles from\narXiv.org according to MSC 2020. The experiment used only the title and\nabstract alone -- not the entire paper. Since this was early in the use of\nchatbots and the development of their APIs, we report here on what was carried\nout by hand. Of course, the automation of the process will have to follow if it\nis to be generally useful. We found that in about 60% of our sample the LLM\nproduced a primary classification matching that already reported on arXiv. In\nabout half of those instances, there were additional primary classifications\nthat were not detected. In about 40% of our sample, the LLM suggested a\ndifferent classification than what was provided. A detailed examination of\nthese cases, however, showed that the LLM-suggested classifications were in\nmost cases better than those provided.", "published": "2024-06-11 20:15:57", "link": "http://arxiv.org/abs/2406.10274v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "RogueGPT: dis-ethical tuning transforms ChatGPT4 into a Rogue AI in 158\n  Words", "abstract": "The ethical implications and potentials for misuse of Generative Artificial\nIntelligence are increasingly worrying topics. This paper explores how easily\nthe default ethical guardrails of ChatGPT, using its latest customization\nfeatures, can be bypassed by simple prompts and fine-tuning, that can be\neffortlessly accessed by the broad public. This malevolently altered version of\nChatGPT, nicknamed \"RogueGPT\", responded with worrying behaviours, beyond those\ntriggered by jailbreak prompts. We conduct an empirical study of RogueGPT\nresponses, assessing its flexibility in answering questions pertaining to what\nshould be disallowed usage. Our findings raise significant concerns about the\nmodel's knowledge about topics like illegal drug production, torture methods\nand terrorism. The ease of driving ChatGPT astray, coupled with its global\naccessibility, highlights severe issues regarding the data quality used for\ntraining the foundational model and the implementation of ethical safeguards.\nWe thus underline the responsibilities and dangers of user-driven\nmodifications, and the broader effects that these may have on the design of\nsafeguarding and ethical modules implemented by AI programmers.", "published": "2024-06-11 18:59:43", "link": "http://arxiv.org/abs/2407.15009v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy\n  Preference Learning Driven By Synthetic Test Cases", "abstract": "Preference learning provides a promising solution to address the limitations\nof supervised fine-tuning (SFT) for code language models, where the model is\nnot explicitly trained to differentiate between correct and incorrect code.\nRecent findings demonstrate that on-policy data is the key to successful\npreference learning, where the preference data is collected using the same\npolicy LM being trained. Inspired by this, we propose PLUM, an on-policy\n$\\textbf{P}$reference $\\textbf{L}$earning framework A$\\textbf{u}$gmented with\ntest cases for code L$\\textbf{M}$ s. The framework operates in three key\nstages: (1) automatic generation of test cases from natural language\ninstructions, (2) creation of a preference data by evaluating candidate code\nsolutions sampled from the policy, which can then be used to (3) train the\npolicy LM. PLUM levitates the need to train reward models, allowing for large\nscale on-policy and online preference data collation. PLUM is evaluated on both\nstandard benchmarks (HumanEval, MBPP) and more challenging ones\n(LiveCodeBench), delivering substantial improvements over original SFT'ed\nmodels and other execution-feedback-driven approaches. We show PLUM's benefits\nare consistent across various widely-used code LMs even they have been\nwell-trained with SFT. For example, PLUM increases pass rates by up to 4.8% on\naverage on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its\neffectiveness and generalizability. We also demonstrate the benefits of\non-policy and online preference learning by comprehensive experimentation.", "published": "2024-06-11 02:07:18", "link": "http://arxiv.org/abs/2406.06887v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Delving into ChatGPT usage in academic writing through excess vocabulary", "abstract": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion, we present an unbiased, large-scale approach: we study vocabulary\nchanges in 14 million PubMed abstracts from 2010--2024, and show how the\nappearance of LLMs led to an abrupt increase in the frequency of certain style\nwords. This excess word analysis suggests that at least 10% of 2024 abstracts\nwere processed with LLMs. This lower bound differed across disciplines,\ncountries, and journals, reaching 30% for some sub-corpora. We show that LLMs\nhave had an unprecedented impact on the scientific literature, surpassing the\neffect of major world events such as the Covid pandemic.", "published": "2024-06-11 07:16:34", "link": "http://arxiv.org/abs/2406.07016v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Reading Miscue Detection in Primary School through Automatic Speech\n  Recognition", "abstract": "Automatic reading diagnosis systems can benefit both teachers for more\nefficient scoring of reading exercises and students for accessing reading\nexercises with feedback more easily. However, there are limited studies on\nAutomatic Speech Recognition (ASR) for child speech in languages other than\nEnglish, and limited research on ASR-based reading diagnosis systems. This\nstudy investigates how efficiently state-of-the-art (SOTA) pretrained ASR\nmodels recognize Dutch native children speech and manage to detect reading\nmiscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA\nphoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster\nWhisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our\nfindings suggest that Wav2Vec2 Large and Whisper are the two best ASR models\nfor reading miscue detection. Specifically, Wav2Vec2 Large shows the highest\nrecall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an\nF1 score of 0.52.", "published": "2024-06-11 08:41:21", "link": "http://arxiv.org/abs/2406.07060v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Fast Context-Biasing for CTC and Transducer ASR models with CTC-based\n  Word Spotter", "abstract": "Accurate recognition of rare and new words remains a pressing problem for\ncontextualized Automatic Speech Recognition (ASR) systems. Most context-biasing\nmethods involve modification of the ASR model or the beam-search decoding\nalgorithm, complicating model reuse and slowing down inference. This work\npresents a new approach to fast context-biasing with CTC-based Word Spotter\n(CTC-WS) for CTC and Transducer (RNN-T) ASR models. The proposed method matches\nCTC log-probabilities against a compact context graph to detect potential\ncontext-biasing candidates. The valid candidates then replace their greedy\nrecognition counterparts in corresponding frame intervals. A Hybrid\nTransducer-CTC model enables the CTC-WS application for the Transducer model.\nThe results demonstrate a significant acceleration of the context-biasing\nrecognition with a simultaneous improvement in F-score and WER compared to\nbaseline methods. The proposed method is publicly available in the NVIDIA NeMo\ntoolkit.", "published": "2024-06-11 09:37:52", "link": "http://arxiv.org/abs/2406.07096v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling Large Language Model-based Multi-Agent Collaboration", "abstract": "Recent breakthroughs in large language model-driven autonomous agents have\nrevealed that multi-agent collaboration often surpasses each individual through\ncollective reasoning. Inspired by the neural scaling law--increasing neurons\nenhances performance, this study explores whether the continuous addition of\ncollaborative agents can yield similar benefits. Technically, we utilize\ndirected acyclic graphs to organize agents into a multi-agent collaboration\nnetwork (MacNet), upon which their interactive reasoning is topologically\norchestrated for autonomous task solving. Extensive evaluations reveal that it\neffectively supports collaboration among over a thousand agents, with irregular\ntopologies outperforming regular ones. We also identify a collaborative scaling\nlaw--the overall performance follows a logistic growth pattern as agents scale,\nwith collaborative emergence occurring earlier than traditional neural\nemergence. We speculate this may be because scaling agents catalyzes their\nmultidimensional considerations during interactive reflection and refinement,\nthereby producing more comprehensive artifacts. The code is available at\nhttps://github.com/OpenBMB/ChatDev/tree/macnet.", "published": "2024-06-11 11:02:04", "link": "http://arxiv.org/abs/2406.07155v3", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.NI", "cs.SI"], "primary_category": "cs.AI"}
{"title": "EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and\n  Benchmark", "abstract": "Speech emotion recognition (SER) is an important part of human-computer\ninteraction, receiving extensive attention from both industry and academia.\nHowever, the current research field of SER has long suffered from the following\nproblems: 1) There are few reasonable and universal splits of the datasets,\nmaking comparing different models and methods difficult. 2) No commonly used\nbenchmark covers numerous corpus and languages for researchers to refer to,\nmaking reproduction a burden. In this paper, we propose EmoBox, an\nout-of-the-box multilingual multi-corpus speech emotion recognition toolkit,\nalong with a benchmark for both intra-corpus and cross-corpus settings. For\nintra-corpus settings, we carefully designed the data partitioning for\ndifferent datasets. For cross-corpus settings, we employ a foundation SER\nmodel, emotion2vec, to mitigate annotation errors and obtain a test set that is\nfully balanced in speakers and emotions distributions. Based on EmoBox, we\npresent the intra-corpus SER results of 10 pre-trained speech models on 32\nemotion datasets with 14 languages, and the cross-corpus SER results on 4\ndatasets with the fully balanced test sets. To the best of our knowledge, this\nis the largest SER benchmark, across language scopes and quantity scales. We\nhope that our toolkit and benchmark can facilitate the research of SER in the\ncommunity.", "published": "2024-06-11 11:12:51", "link": "http://arxiv.org/abs/2406.07162v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toxic Memes: A Survey of Computational Perspectives on the Detection and\n  Explanation of Meme Toxicities", "abstract": "Internet memes, channels for humor, social commentary, and cultural\nexpression, are increasingly used to spread toxic messages. Studies on the\ncomputational analyses of toxic memes have significantly grown over the past\nfive years, and the only three surveys on computational toxic meme analysis\ncover only work published until 2022, leading to inconsistent terminology and\nunexplored trends. Our work fills this gap by surveying content-based\ncomputational perspectives on toxic memes, and reviewing key developments until\nearly 2024. Employing the PRISMA methodology, we systematically extend the\npreviously considered papers, achieving a threefold result. First, we survey\n119 new papers, analyzing 158 computational works focused on content-based\ntoxic meme analysis. We identify over 30 datasets used in toxic meme analysis\nand examine their labeling systems. Second, after observing the existence of\nunclear definitions of meme toxicity in computational works, we introduce a new\ntaxonomy for categorizing meme toxicity types. We also note an expansion in\ncomputational tasks beyond the simple binary classification of memes as toxic\nor non-toxic, indicating a shift towards achieving a nuanced comprehension of\ntoxicity. Third, we identify three content-based dimensions of meme toxicity\nunder automatic study: target, intent, and conveyance tactics. We develop a\nframework illustrating the relationships between these dimensions and meme\ntoxicities. The survey analyzes key challenges and recent trends, such as\nenhanced cross-modal reasoning, integrating expert and cultural knowledge, the\ndemand for automatic toxicity explanations, and handling meme toxicity in\nlow-resource languages. Also, it notes the rising use of Large Language Models\n(LLMs) and generative AI for detecting and generating toxic memes. Finally, it\nproposes pathways for advancing toxic meme detection and interpretation.", "published": "2024-06-11 15:22:48", "link": "http://arxiv.org/abs/2406.07353v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok,\n  and Other Sources about the 2024 Outbreak of Measles", "abstract": "The work of this paper presents a dataset that contains the data of 4011\nvideos about the ongoing outbreak of measles published on 264 websites on the\ninternet between January 1, 2024, and May 31, 2024. The dataset is available at\nhttps://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube\nand TikTok, which account for 48.6% and 15.2% of the videos, respectively. The\nremainder of the websites include Instagram and Facebook as well as the\nwebsites of various global and local news organizations. For each of these\nvideos, the URL of the video, title of the post, description of the post, and\nthe date of publication of the video are presented as separate attributes in\nthe dataset. After developing this dataset, sentiment analysis (using VADER),\nsubjectivity analysis (using TextBlob), and fine-grain sentiment analysis\n(using DistilRoBERTa-base) of the video titles and video descriptions were\nperformed. This included classifying each video title and video description\ninto (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii)\none of the subjectivity classes i.e. highly opinionated, neutral opinionated,\nor least opinionated, and (iii) one of the fine-grain sentiment classes i.e.\nfear, surprise, joy, sadness, anger, disgust, or neutral. These results are\npresented as separate attributes in the dataset for the training and testing of\nmachine learning algorithms for performing sentiment analysis or subjectivity\nanalysis in this field as well as for other applications. Finally, this paper\nalso presents a list of open research questions that may be investigated using\nthis dataset.", "published": "2024-06-11 20:14:22", "link": "http://arxiv.org/abs/2406.07693v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.SI", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "primary_category": "cs.CY"}
{"title": "Spoken Language Corpora Augmentation with Domain-Specific Voice-Cloned\n  Speech", "abstract": "In this paper we study the impact of augmenting spoken language corpora with\ndomain-specific synthetic samples for the purpose of training a speech\nrecognition system. Using both a conventional neural TTS system and a zero-shot\none with voice cloning ability we generate speech corpora that vary in the\nnumber of voices. We compare speech recognition models trained with addition of\ndifferent amounts of synthetic data generated using these two methods with a\nbaseline model trained solely on voice recordings. We show that while the\nquality of voice-cloned dataset is lower, its increased multivoiceity makes it\nmuch more effective than the one with only a few voices synthesized with the\nuse of a conventional neural TTS system. Furthermore, our experiments indicate\nthat using low variability synthetic speech quickly leads to saturation in the\nquality of the ASR whereas high variability speech provides improvement even\nwhen increasing total amount of data used for training by 30%.", "published": "2024-06-11 09:31:22", "link": "http://arxiv.org/abs/2406.07090v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Clever Hans Effect Found in Automatic Detection of Alzheimer's Disease\n  through Speech", "abstract": "We uncover an underlying bias present in the audio recordings produced from\nthe picture description task of the Pitt corpus, the largest publicly\naccessible database for Alzheimer's Disease (AD) detection research. Even by\nsolely utilizing the silent segments of these audio recordings, we achieve\nnearly 100% accuracy in AD detection. However, employing the same methods to\nother datasets and preprocessed Pitt recordings results in typical levels\n(approximately 80%) of AD detection accuracy. These results demonstrate a\nClever Hans effect in AD detection on the Pitt corpus. Our findings emphasize\nthe crucial importance of maintaining vigilance regarding inherent biases in\ndatasets utilized for training deep learning models, and highlight the\nnecessity for a better understanding of the models' performance.", "published": "2024-06-11 16:14:46", "link": "http://arxiv.org/abs/2406.07410v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance\n  Speech Generation", "abstract": "The multi-codebook speech codec enables the application of large language\nmodels (LLM) in TTS but bottlenecks efficiency and robustness due to\nmulti-sequence prediction. To avoid this obstacle, we propose Single-Codec, a\nsingle-codebook single-sequence codec, which employs a disentangled VQ-VAE to\ndecouple speech into a time-invariant embedding and a phonetically-rich\ndiscrete sequence. Furthermore, the encoder is enhanced with 1) contextual\nmodeling with a BLSTM module to exploit the temporal information, 2) a hybrid\nsampling module to alleviate distortion from upsampling and downsampling, and\n3) a resampling module to encourage discrete units to carry more phonetic\ninformation. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec,\nSingle-Codec demonstrates higher reconstruction quality with a lower bandwidth\nof only 304bps. The effectiveness of Single-Code is further validated by\nLLM-TTS experiments, showing improved naturalness and intelligibility.", "published": "2024-06-11 16:22:57", "link": "http://arxiv.org/abs/2406.07422v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Noise-robust Speech Separation with Fast Generative Correction", "abstract": "Speech separation, the task of isolating multiple speech sources from a mixed\naudio signal, remains challenging in noisy environments. In this paper, we\npropose a generative correction method to enhance the output of a\ndiscriminative separator. By leveraging a generative corrector based on a\ndiffusion model, we refine the separation process for single-channel mixture\nspeech by removing noises and perceptually unnatural distortions. Furthermore,\nwe optimize the generative model using a predictive loss to streamline the\ndiffusion model's reverse process into a single step and rectify any associated\nerrors by the reverse process. Our method achieves state-of-the-art performance\non the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety\nof noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating\nrobustness and strong generalization capabilities.", "published": "2024-06-11 17:08:21", "link": "http://arxiv.org/abs/2406.07461v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Scaling up masked audio encoder learning for general audio\n  classification", "abstract": "Despite progress in audio classification, a generalization gap remains\nbetween speech and other sound domains, such as environmental sounds and music.\nModels trained for speech tasks often fail to perform well on environmental or\nmusical audio tasks, and vice versa. While self-supervised (SSL) audio\nrepresentations offer an alternative, there has been limited exploration of\nscaling both model and dataset sizes for SSL-based general audio\nclassification. We introduce Dasheng, a simple SSL audio encoder, based on the\nefficient masked autoencoder framework. Trained with 1.2 billion parameters on\n272,356 hours of diverse audio, Dasheng obtains significant performance gains\non the HEAR benchmark. It outperforms previous works on CREMA-D, LibriCount,\nSpeech Commands, VoxLingua, and competes well in music and environment\nclassification. Dasheng features inherently contain rich speech, music, and\nenvironmental information, as shown in nearest-neighbor classification\nexperiments. Code is available https://github.com/richermans/dasheng/.", "published": "2024-06-11 06:44:54", "link": "http://arxiv.org/abs/2406.06992v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MR-RawNet: Speaker verification system with multiple temporal\n  resolutions for variable duration utterances using raw waveforms", "abstract": "In speaker verification systems, the utilization of short utterances presents\na persistent challenge, leading to performance degradation primarily due to\ninsufficient phonetic information to characterize the speakers. To overcome\nthis obstacle, we propose a novel structure, MR-RawNet, designed to enhance the\nrobustness of speaker verification systems against variable duration utterances\nusing raw waveforms. The MR-RawNet extracts time-frequency representations from\nraw waveforms via a multi-resolution feature extractor that optimally adjusts\nboth temporal and spectral resolutions simultaneously. Furthermore, we apply a\nmulti-resolution attention block that focuses on diverse and extensive temporal\ncontexts, ensuring robustness against changes in utterance length. The\nexperimental results, conducted on VoxCeleb1 dataset, demonstrate that the\nMR-RawNet exhibits superior performance in handling utterances of variable\nduration compared to other raw waveform-based systems.", "published": "2024-06-11 09:42:47", "link": "http://arxiv.org/abs/2406.07103v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "ICGAN: An implicit conditioning method for interpretable feature control\n  of neural audio synthesis", "abstract": "Neural audio synthesis methods can achieve high-fidelity and realistic sound\ngeneration by utilizing deep generative models. Such models typically rely on\nexternal labels which are often discrete as conditioning information to achieve\nguided sound generation. However, it remains difficult to control the subtle\nchanges in sounds without appropriate and descriptive labels, especially given\na limited dataset. This paper proposes an implicit conditioning method for\nneural audio synthesis using generative adversarial networks that allows for\ninterpretable control of the acoustic features of synthesized sounds. Our\ntechnique creates a continuous conditioning space that enables timbre\nmanipulation without relying on explicit labels. We further introduce an\nevaluation metric to explore controllability and demonstrate that our approach\nis effective in enabling a degree of controlled variation of different\nsynthesized sound effects for in-domain and cross-domain sounds.", "published": "2024-06-11 10:28:02", "link": "http://arxiv.org/abs/2406.07131v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Speech Diarization with Multimodal Prompts", "abstract": "Traditional speaker diarization seeks to detect ``who spoke when'' according\nto speaker characteristics. Extending to target speech diarization, we detect\n``when target event occurs'' according to the semantic characteristics of\nspeech. We propose a novel Multimodal Target Speech Diarization (MM-TSD)\nframework, which accommodates diverse and multi-modal prompts to specify target\nevents in a flexible and user-friendly manner, including semantic language\ndescription, pre-enrolled speech, pre-registered face image, and audio-language\nlogical prompts. We further propose a voice-face aligner module to project\nhuman voice and face representation into a shared space. We develop a\nmulti-modal dataset based on VoxCeleb2 for MM-TSD training and evaluation.\nAdditionally, we conduct comparative analysis and ablation studies for each\ncategory of prompts to validate the efficacy of each component in the proposed\nframework. Furthermore, our framework demonstrates versatility in performing\nvarious signal processing tasks, including speaker diarization and overlap\nspeech detection, using task-specific prompts. MM-TSD achieves robust and\ncomparable performance as a unified system compared to specialized models.\nMoreover, MM-TSD shows capability to handle complex conversations for\nreal-world dataset.", "published": "2024-06-11 12:18:18", "link": "http://arxiv.org/abs/2406.07198v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "ParaCLAP -- Towards a general language-audio model for computational\n  paralinguistic tasks", "abstract": "Contrastive language-audio pretraining (CLAP) has recently emerged as a\nmethod for making audio analysis more generalisable. Specifically, CLAP-style\nmodels are able to `answer' a diverse set of language queries, extending the\ncapabilities of audio models beyond a closed set of labels. However, CLAP\nrelies on a large set of (audio, query) pairs for pretraining. While such sets\nare available for general audio tasks, like captioning or sound event\ndetection, there are no datasets with matched audio and text queries for\ncomputational paralinguistic (CP) tasks. As a result, the community relies on\ngeneric CLAP models trained for general audio with limited success. In the\npresent study, we explore training considerations for ParaCLAP, a CLAP-style\nmodel suited to CP, including a novel process for creating audio-language\nqueries. We demonstrate its effectiveness on a set of computational\nparalinguistic tasks, where it is shown to surpass the performance of\nopen-source state-of-the-art models.", "published": "2024-06-11 12:23:01", "link": "http://arxiv.org/abs/2406.07203v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from\n  Codec-Based Speech Synthesis Systems", "abstract": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic\nanyone's voice with just a 3-second sample from that specific unseen speaker.\nUnfortunately, malicious attackers may exploit these technologies, causing\nmisuse and security issues. Anti-spoofing models have been developed to detect\nfake speech. However, the open question of whether current SOTA anti-spoofing\nmodels can effectively counter deepfake audios from codec-based speech\nsynthesis systems remains unanswered. In this paper, we curate an extensive\ncollection of contemporary SOTA codec models, employing them to re-create\nsynthesized speech. This endeavor leads to the creation of CodecFake, the first\ncodec-based deepfake audio dataset. Additionally, we verify that anti-spoofing\nmodels trained on commonly used datasets cannot detect synthesized speech from\ncurrent codec-based speech generation systems. The proposed CodecFake dataset\nempowers these models to counter this challenge effectively.", "published": "2024-06-11 13:16:09", "link": "http://arxiv.org/abs/2406.07237v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SRC4VC: Smartphone-Recorded Corpus for Voice Conversion Benchmark", "abstract": "We present SRC4VC, a new corpus containing 11 hours of speech recorded on\nsmartphones by 100 Japanese speakers. Although high-quality multi-speaker\ncorpora can advance voice conversion (VC) technologies, they are not always\nsuitable for testing VC when low-quality speech recording is given as the\ninput. To this end, we first asked 100 crowdworkers to record their voice\nsamples using smartphones. Then, we annotated the recorded samples with\nspeaker-wise recording-quality scores and utterance-wise perceived emotion\nlabels. We also benchmark SRC4VC on any-to-any VC, in which we trained a\nmulti-speaker VC model on high-quality speech and used the SRC4VC speakers'\nvoice samples as the source in VC. The results show that the recording quality\nmismatch between the training and evaluation data significantly degrades the VC\nperformance, which can be improved by applying speech enhancement to the\nlow-quality source speech samples.", "published": "2024-06-11 13:34:40", "link": "http://arxiv.org/abs/2406.07254v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise-Robust Voice Conversion by Conditional Denoising Training Using\n  Latent Variables of Recording Quality and Environment", "abstract": "We propose noise-robust voice conversion (VC) which takes into account the\nrecording quality and environment of noisy source speech. Conventional\ndenoising training improves the noise robustness of a VC model by learning\nnoisy-to-clean VC process. However, the naturalness of the converted speech is\nlimited when the noise of the source speech is unseen during the training. To\nthis end, our proposed training conditions a VC model on two latent variables\nrepresenting the recording quality and environment of the source speech. These\nlatent variables are derived from deep neural networks pre-trained on recording\nquality assessment and acoustic scene classification and calculated in an\nutterance-wise or frame-wise manner. As a result, the trained VC model can\nexplicitly learn information about speech degradation during the training.\nObjective and subjective evaluations show that our training improves the\nquality of the converted speech compared to the conventional training.", "published": "2024-06-11 14:07:05", "link": "http://arxiv.org/abs/2406.07280v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comprehensive Investigation on Speaker Augmentation for Speaker\n  Recognition", "abstract": "Data augmentation (DA) has played a pivotal role in the success of deep\nspeaker recognition. Current DA techniques primarily focus on\nspeaker-preserving augmentation, which does not change the speaker trait of the\nspeech and does not create new speakers. Recent research has shed light on the\npotential of speaker augmentation, which generates new speakers to enrich the\ntraining dataset. In this study, we delve into two speaker augmentation\napproaches: speed perturbation (SP) and vocal tract length perturbation (VTLP).\nDespite the empirical utilization of both methods, a comprehensive\ninvestigation into their efficacy is lacking. Our study, conducted using two\npublic datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are\nproficient at generating new speakers, leading to significant performance\nimprovements in speaker recognition. Furthermore, they exhibit distinct\nproperties in sensitivity to perturbation factors and data complexity, hinting\nat the potential benefits of their fusion. Our research underscores the\nsubstantial potential of speaker augmentation, highlighting the importance of\nin-depth exploration and analysis.", "published": "2024-06-11 16:22:34", "link": "http://arxiv.org/abs/2406.07421v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph-based multi-Feature fusion method for speech emotion recognition", "abstract": "Exploring proper way to conduct multi-speech feature fusion for cross-corpus\nspeech emotion recognition is crucial as different speech features could\nprovide complementary cues reflecting human emotion status. While most previous\napproaches only extract a single speech feature for emotion recognition,\nexisting fusion methods such as concatenation, parallel connection, and\nsplicing ignore heterogeneous patterns in the interaction between features and\nfeatures, resulting in performance of existing systems. In this paper, we\npropose a novel graph-based fusion method to explicitly model the relationships\nbetween every pair of speech features. Specifically, we propose a\nmulti-dimensional edge features learning strategy called Graph-based\nmulti-Feature fusion method for speech emotion recognition. It represents each\nspeech feature as a node and learns multi-dimensional edge features to\nexplicitly describe the relationship between each feature-feature pair in the\ncontext of emotion recognition. This way, the learned multi-dimensional edge\nfeatures encode speech feature-level information from both the vertex and edge\ndimensions. Our Approach consists of three modules: an Audio Feature\nGeneration(AFG)module, an Audio-Feature Multi-dimensional Edge Feature(AMEF)\nmodule and a Speech Emotion Recognition (SER) module. The proposed methodology\nyielded satisfactory outcomes on the SEWA dataset. Furthermore, the method\ndemonstrated enhanced performance compared to the baseline in the AVEC 2019\nWorkshop and Challenge. We used data from two cultures as our training and\nvalidation sets: two cultures containing German and Hungarian on the SEWA\ndataset, the CCC scores for German are improved by 17.28% for arousal and 7.93%\nfor liking. The outcomes of our methodology demonstrate a 13% improvement over\nalternative fusion techniques, including those employing one dimensional\nedge-based feature fusion approach.", "published": "2024-06-11 16:45:34", "link": "http://arxiv.org/abs/2406.07437v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A methodological framework and exemplar protocol for the collection and\n  analysis of repeated speech samples", "abstract": "Speech and language biomarkers have the potential to be regular, objective\nassessments of symptom severity in several health conditions, both in-clinic\nand remotely using mobile devices. However, the complex nature of speech and\noften subtle changes associated with health mean that findings are highly\ndependent on methodological and cohort choices. These are often not reported\nadequately in studies investigating speech-based health assessment, hindering\nthe progress of methodological speech research. Our objectives were to)\nfacilitate replicable speech research by presenting an adaptable speech\ncollection and analytical method and design checklist for other researchers to\nadapt for their own experiments and develop an exemplar protocol that reduces\nand controls for confounding factors in repeated recordings of speech,\nincluding device choice, speech elicitation task and non-pathological\nvariability. The presented protocol comprises the elicitation of read speech,\nheld vowels and a picture description collected with a freestanding condenser\nmicrophone, 3 smartphones and a headset. We extracted a set of 14 exemplar\nspeech features. We collected healthy speech from 28 individuals 3 times in 1\nday, repeated at the same times 8-11 weeks later, and from 25 individuals on 3\ndays in 1 week at fixed times. Participant characteristics collected included\nsex, age, native language status and voice use habits. Before each recording,\nwe collected information on recent voice use, food and drink intake, and\nemotional state. The extracted features are presented providing a resource of\nnormative values. Speech data collection, processing, analysis and reporting\ntowards clinical research and practice varies widely. Greater harmonisation of\nstudy protocols and consistent reporting are urgently required to translate\nspeech processing into clinical research and practice.", "published": "2024-06-11 17:32:28", "link": "http://arxiv.org/abs/2406.07497v3", "categories": ["cs.SD", "eess.AS", "J.3"], "primary_category": "cs.SD"}
{"title": "RaD-Net 2: A causal two-stage repairing and denoising speech enhancement\n  network with knowledge distillation and complex axial self-attention", "abstract": "In real-time speech communication systems, speech signals are often degraded\nby multiple distortions. Recently, a two-stage Repair-and-Denoising network\n(RaD-Net) was proposed with superior speech quality improvement in the ICASSP\n2024 Speech Signal Improvement (SSI) Challenge. However, failure to use future\ninformation and constraint receptive field of convolution layers limit the\nsystem's performance. To mitigate these problems, we extend RaD-Net to its\nupgraded version, RaD-Net 2. Specifically, a causality-based knowledge\ndistillation is introduced in the first stage to use future information in a\ncausal way. We use the non-causal repairing network as the teacher to improve\nthe performance of the causal repairing network. In addition, in the second\nstage, complex axial self-attention is applied in the denoising network's\ncomplex feature encoder/decoder. Experimental results on the ICASSP 2024 SSI\nChallenge blind test set show that RaD-Net 2 brings 0.10 OVRL DNSMOS\nimprovement compared to RaD-Net.", "published": "2024-06-11 17:33:10", "link": "http://arxiv.org/abs/2406.07498v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Interspeech 2024 Challenge on Speech Processing Using Discrete Units", "abstract": "Representing speech and audio signals in discrete units has become a\ncompelling alternative to traditional high-dimensional feature vectors.\nNumerous studies have highlighted the efficacy of discrete units in various\napplications such as speech compression and restoration, speech recognition,\nand speech generation. To foster exploration in this domain, we introduce the\nInterspeech 2024 Challenge, which focuses on new speech processing benchmarks\nusing discrete units. It encompasses three pivotal tasks, namely multilingual\nautomatic speech recognition, text-to-speech, and singing voice synthesis, and\naims to assess the potential applicability of discrete units in these tasks.\nThis paper outlines the challenge designs and baseline descriptions. We also\ncollate baseline and selected submission systems, along with preliminary\nfindings, offering valuable contributions to future research in this evolving\nfield.", "published": "2024-06-11 21:08:47", "link": "http://arxiv.org/abs/2406.07725v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DB3V: A Dialect Dominated Dataset of Bird Vocalisation for Cross-corpus\n  Bird Species Recognition", "abstract": "In ornithology, bird species are known to have variedit's widely acknowledged\nthat bird species display diverse dialects in their calls across different\nregions. Consequently, computational methods to identify bird species onsolely\nthrough their calls face critsignificalnt challenges. There is growing interest\nin understanding the impact of species-specific dialects on the effectiveness\nof bird species recognition methods. Despite potential mitigation through the\nexpansion of dialect datasets, the absence of publicly available testing data\ncurrently impedes robust benchmarking efforts. This paper presents the Dialect\nDominated Dataset of Bird Vocalisation, the first cross-corpus dataset that\nfocuses on dialects in bird vocalisations. The DB3V comprises more than 25\nhours of audio recordings from 10 bird species distributed across three\ndistinct regions in the contiguous United States (CONUS). In addition to\npresenting the dataset, we conduct analyses and establish baseline models for\ncross-corpus bird recognition. The data and code are publicly available online:\nhttps://zenodo.org/records/11544734", "published": "2024-06-11 12:44:38", "link": "http://arxiv.org/abs/2406.08517v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioMarkBench: Benchmarking Robustness of Audio Watermarking", "abstract": "The increasing realism of synthetic speech, driven by advancements in\ntext-to-speech models, raises ethical concerns regarding impersonation and\ndisinformation. Audio watermarking offers a promising solution via embedding\nhuman-imperceptible watermarks into AI-generated audios. However, the\nrobustness of audio watermarking against common/adversarial perturbations\nremains understudied. We present AudioMarkBench, the first systematic benchmark\nfor evaluating the robustness of audio watermarking against watermark removal\nand watermark forgery. AudioMarkBench includes a new dataset created from\nCommon-Voice across languages, biological sexes, and ages, 3 state-of-the-art\nwatermarking methods, and 15 types of perturbations. We benchmark the\nrobustness of these methods against the perturbations in no-box, black-box, and\nwhite-box settings. Our findings highlight the vulnerabilities of current\nwatermarking techniques and emphasize the need for more robust and fair audio\nwatermarking solutions. Our dataset and code are publicly available at\nhttps://github.com/moyangkuo/AudioMarkBench.", "published": "2024-06-11 06:18:29", "link": "http://arxiv.org/abs/2406.06979v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot\n  Unsupervised Anomalous Sound Detection for Machine Condition Monitoring", "abstract": "We present the task description of the Detection and Classification of\nAcoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot\nunsupervised anomalous sound detection (ASD) for machine condition monitoring.\nContinuing from last year's DCASE 2023 Challenge Task 2, we organize the task\nas a first-shot problem under domain generalization required settings. The main\ngoal of the first-shot problem is to enable rapid deployment of ASD systems for\nnew kinds of machines without the need for machine-specific hyperparameter\ntunings. This problem setting was realized by (1) giving only one section for\neach machine type and (2) having completely different machine types for the\ndevelopment and evaluation datasets. For the DCASE 2024 Challenge Task 2, data\nof completely new machine types were newly collected and provided as the\nevaluation dataset. In addition, attribute information such as the machine\noperation conditions were concealed for several machine types to mimic\nsituations where such information are unavailable. We will add challenge\nresults and analysis of the submissions after the challenge submission\ndeadline.", "published": "2024-06-11 13:32:40", "link": "http://arxiv.org/abs/2406.07250v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AS-70: A Mandarin stuttered speech dataset for automatic speech\n  recognition and stuttering event detection", "abstract": "The rapid advancements in speech technologies over the past two decades have\nled to human-level performance in tasks like automatic speech recognition (ASR)\nfor fluent speech. However, the efficacy of these models diminishes when\napplied to atypical speech, such as stuttering. This paper introduces AS-70,\nthe first publicly available Mandarin stuttered speech dataset, which stands\nout as the largest dataset in its category. Encompassing conversational and\nvoice command reading speech, AS-70 includes verbatim manual transcription,\nrendering it suitable for various speech-related tasks. Furthermore, baseline\nsystems are established, and experimental results are presented for ASR and\nstuttering event detection (SED) tasks. By incorporating this dataset into the\nmodel fine-tuning, significant improvements in the state-of-the-art ASR models,\ne.g., Whisper and Hubert, are observed, enhancing their inclusivity in\naddressing stuttered speech.", "published": "2024-06-11 13:35:50", "link": "http://arxiv.org/abs/2406.07256v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hearing Anything Anywhere", "abstract": "Recent years have seen immense progress in 3D computer vision and computer\ngraphics, with emerging tools that can virtualize real-world 3D environments\nfor numerous Mixed Reality (XR) applications. However, alongside immersive\nvisual experiences, immersive auditory experiences are equally vital to our\nholistic perception of an environment. In this paper, we aim to reconstruct the\nspatial acoustic characteristics of an arbitrary environment given only a\nsparse set of (roughly 12) room impulse response (RIR) recordings and a planar\nreconstruction of the scene, a setup that is easily achievable by ordinary\nusers. To this end, we introduce DiffRIR, a differentiable RIR rendering\nframework with interpretable parametric models of salient acoustic features of\nthe scene, including sound source directivity and surface reflectivity. This\nallows us to synthesize novel auditory experiences through the space with any\nsource audio. To evaluate our method, we collect a dataset of RIR recordings\nand music in four diverse, real environments. We show that our model\noutperforms state-ofthe-art baselines on rendering monaural and binaural RIRs\nand music at unseen locations, and learns physically interpretable parameters\ncharacterizing acoustic properties of the sound source and surfaces in the\nscene.", "published": "2024-06-11 17:56:14", "link": "http://arxiv.org/abs/2406.07532v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "I.2.10; I.4.8"], "primary_category": "cs.SD"}
{"title": "Pre-training Feature Guided Diffusion Model for Speech Enhancement", "abstract": "Speech enhancement significantly improves the clarity and intelligibility of\nspeech in noisy environments, improving communication and listening\nexperiences. In this paper, we introduce a novel pretraining feature-guided\ndiffusion model tailored for efficient speech enhancement, addressing the\nlimitations of existing discriminative and generative models. By integrating\nspectral features into a variational autoencoder (VAE) and leveraging\npre-trained features for guidance during the reverse process, coupled with the\nutilization of the deterministic discrete integration method (DDIM) to\nstreamline sampling steps, our model improves efficiency and speech enhancement\nquality. Demonstrating state-of-the-art results on two public datasets with\ndifferent SNRs, our model outshines other baselines in efficiency and\nrobustness. The proposed method not only optimizes performance but also\nenhances practical deployment capabilities, without increasing computational\ndemands.", "published": "2024-06-11 18:22:59", "link": "http://arxiv.org/abs/2406.07646v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards better visualizations of urban sound environments: insights from\n  interviews", "abstract": "Urban noise maps and noise visualizations traditionally provide macroscopic\nrepresentations of noise levels across cities. However, those representations\nfail at accurately gauging the sound perception associated with these sound\nenvironments, as perception highly depends on the sound sources involved. This\npaper aims at analyzing the need for the representations of sound sources, by\nidentifying the urban stakeholders for whom such representations are assumed to\nbe of importance. Through spoken interviews with various urban stakeholders, we\nhave gained insight into current practices, the strengths and weaknesses of\nexisting tools and the relevance of incorporating sound sources into existing\nurban sound environment representations. Three distinct use of sound source\nrepresentations emerged in this study: 1) noise-related complaints for\nindustrials and specialized citizens, 2) soundscape quality assessment for\ncitizens, and 3) guidance for urban planners. Findings also reveal diverse\nperspectives for the use of visualizations, which should use indicators adapted\nto the target audience, and enable data accessibility.", "published": "2024-06-11 07:39:48", "link": "http://arxiv.org/abs/2407.16889v1", "categories": ["cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.CY"}
{"title": "Broadband MEMS Microphone Arrays with Reduced Aperture Through\n  3D-Printed Waveguides", "abstract": "In this paper we present a passive and cost-effective method for increasing\nthe frequency range of ultrasound MEMS microphone arrays when using beamforming\ntechniques. By applying a 3D-printed construction that reduces the acoustic\naperture of the MEMS microphones we can create a regularly spaced microphone\narray layout with much smaller inter-element spacing than could be accomplished\non a printed circuit board due to the physical size of the MEMS elements. This\nmethod allows the use of ultrasound sensors incorporating microphone arrays in\ncombination with beamforming techniques without aliases due to grating lobes in\napplications such as sound source localization or the emulation of bat HRTFs.", "published": "2024-06-11 19:12:16", "link": "http://arxiv.org/abs/2406.07663v1", "categories": ["eess.SY", "cs.RO", "cs.SD", "cs.SY", "eess.AS"], "primary_category": "eess.SY"}
{"title": "FastAST: Accelerating Audio Spectrogram Transformer via Token Merging\n  and Cross-Model Knowledge Distillation", "abstract": "Audio classification models, particularly the Audio Spectrogram Transformer\n(AST), play a crucial role in efficient audio analysis. However, optimizing\ntheir efficiency without compromising accuracy remains a challenge. In this\npaper, we introduce FastAST, a framework that integrates Token Merging (ToMe)\ninto the AST framework. FastAST enhances inference speed without requiring\nextensive retraining by merging similar tokens in audio spectrograms.\nFurthermore, during training, FastAST brings about significant speed\nimprovements. The experiments indicate that FastAST can increase audio\nclassification throughput with minimal impact on accuracy. To mitigate the\naccuracy impact, we integrate Cross-Model Knowledge Distillation (CMKD) into\nthe FastAST framework. Integrating ToMe and CMKD into AST results in improved\naccuracy compared to AST while maintaining faster inference speeds. FastAST\nrepresents a step towards real-time, resource-efficient audio analysis.", "published": "2024-06-11 19:50:50", "link": "http://arxiv.org/abs/2406.07676v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS", "68T10"], "primary_category": "cs.SD"}
