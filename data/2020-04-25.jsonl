{"title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About\n  People?", "abstract": "Social biases are encoded in word embeddings. This presents a unique\nopportunity to study society historically and at scale, and a unique danger\nwhen embeddings are used in downstream applications. Here, we investigate the\nextent to which publicly-available word embeddings accurately reflect beliefs\nabout certain kinds of people as measured via traditional survey methods. We\nfind that biases found in word embeddings do, on average, closely mirror survey\ndata across seventeen dimensions of social meaning. However, we also find that\nbiases in embeddings are much more reflective of survey data for some\ndimensions of meaning (e.g. gender) than others (e.g. race), and that we can be\nhighly confident that embedding-based measures reflect survey data only for the\nmost salient biases.", "published": "2020-04-25 02:42:12", "link": "http://arxiv.org/abs/2004.12043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Heterogeneous Graph with Factual, Temporal and Logical Knowledge for\n  Question Answering Over Dynamic Contexts", "abstract": "We study question answering over a dynamic textual environment. Although\nneural network models achieve impressive accuracy via learning from\ninput-output examples, they rarely leverage various types of knowledge and are\ngenerally not interpretable. In this work, we propose a graph-based approach,\nwhere a heterogeneous graph is automatically built with factual knowledge of\nthe context, temporal knowledge of the past states, and logical knowledge that\ncombines human-curated knowledge bases and rule bases. We develop a graph\nneural network over the constructed graph, and train the model in an end-to-end\nmanner. Experimental results on a benchmark dataset show that the injection of\nvarious types of knowledge improves a strong neural network baseline. An\nadditional benefit of our approach is that the graph itself naturally serves as\na rational behind the decision making.", "published": "2020-04-25 04:53:54", "link": "http://arxiv.org/abs/2004.12057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained\n  Model Lead to the Promised Land?", "abstract": "Fine-tuning pretrained model has achieved promising performance on standard\nNER benchmarks. Generally, these benchmarks are blessed with strong name\nregularity, high mention coverage and sufficient context diversity.\nUnfortunately, when scaling NER to open situations, these advantages may no\nlonger exist. And therefore it raises a critical question of whether previous\ncreditable approaches can still work well when facing these challenges. As\nthere is no currently available dataset to investigate this problem, this paper\nproposes to conduct randomization test on standard benchmarks. Specifically, we\nerase name regularity, mention coverage and context diversity respectively from\nthe benchmarks, in order to explore their impact on the generalization ability\nof models. To further verify our conclusions, we also construct a new open NER\ndataset that focuses on entity types with weaker name regularity and lower\nmention coverage to verify our conclusion. From both randomization test and\nempirical experiments, we draw the conclusions that 1) name regularity is\ncritical for the models to generalize to unseen mentions; 2) high mention\ncoverage may undermine the model generalization ability and 3) context patterns\nmay not require enormous data to capture when using pretrained encoders.", "published": "2020-04-25 12:30:16", "link": "http://arxiv.org/abs/2004.12126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does NLP Benefit Legal System: A Summary of Legal Artificial\n  Intelligence", "abstract": "Legal Artificial Intelligence (LegalAI) focuses on applying the technology of\nartificial intelligence, especially natural language processing, to benefit\ntasks in the legal domain. In recent years, LegalAI has drawn increasing\nattention rapidly from both AI researchers and legal professionals, as LegalAI\nis beneficial to the legal system for liberating legal professionals from a\nmaze of paperwork. Legal professionals often think about how to solve tasks\nfrom rule-based and symbol-based methods, while NLP researchers concentrate\nmore on data-driven and embedding methods. In this paper, we introduce the\nhistory, the current state, and the future directions of research in LegalAI.\nWe illustrate the tasks from the perspectives of legal professionals and NLP\nresearchers and show several representative applications in LegalAI. We conduct\nexperiments and provide an in-depth analysis of the advantages and\ndisadvantages of existing works to explore possible future directions. You can\nfind the implementation of our work from https://github.com/thunlp/CLAIM.", "published": "2020-04-25 14:45:15", "link": "http://arxiv.org/abs/2004.12158v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Discourse Parsing-inspired Semantic Storytelling", "abstract": "Previous work of ours on Semantic Storytelling uses text analytics procedures\nincluding Named Entity Recognition and Event Detection. In this paper, we\noutline our longer-term vision on Semantic Storytelling and describe the\ncurrent conceptual and technical approach. In the project that drives our\nresearch we develop AI-based technologies that are verified by partners from\nindustry. One long-term goal is the development of an approach for Semantic\nStorytelling that has broad coverage and that is, furthermore, robust. We\nprovide first results on experiments that involve discourse parsing, applied to\na concrete use case, \"Explore the Neighbourhood!\", which is based on a\nsemi-automatically collected data set with documents about noteworthy people in\none of Berlin's districts. Though automatically obtaining annotations for\ncoherence relations from plain text is a non-trivial challenge, our preliminary\nresults are promising. We envision our approach to be combined with additional\nfeatures (NER, coreference resolution, knowledge graphs", "published": "2020-04-25 17:09:56", "link": "http://arxiv.org/abs/2004.12190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Contextualization of Word Representations with Semantic\n  Class Probing", "abstract": "Pretrained language models have achieved a new state of the art on many NLP\ntasks, but there are still many open questions about how and why they work so\nwell. We investigate the contextualization of words in BERT. We quantify the\namount of contextualization, i.e., how well words are interpreted in context,\nby studying the extent to which semantic classes of a word can be inferred from\nits contextualized embeddings. Quantifying contextualization helps in\nunderstanding and utilizing pretrained language models. We show that top layer\nrepresentations achieve high accuracy inferring semantic classes; that the\nstrongest contextualization effects occur in the lower layers; that local\ncontext is mostly sufficient for semantic class inference; and that top layer\nrepresentations are more task-specific after finetuning while lower layer\nrepresentations are more transferable. Finetuning uncovers task related\nfeatures, but pretrained knowledge is still largely preserved.", "published": "2020-04-25 17:49:37", "link": "http://arxiv.org/abs/2004.12198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCQA: Multimodal Co-attention Based Network for Question Answering", "abstract": "We present MCQA, a learning-based algorithm for multimodal question\nanswering. MCQA explicitly fuses and aligns the multimodal input (i.e. text,\naudio, and video), which forms the context for the query (question and answer).\nOur approach fuses and aligns the question and the answer within this context.\nMoreover, we use the notion of co-attention to perform cross-modal alignment\nand multimodal context-query alignment. Our context-query alignment module\nmatches the relevant parts of the multimodal context and the query with each\nother and aligns them to improve the overall performance. We evaluate the\nperformance of MCQA on Social-IQ, a benchmark dataset for multimodal question\nanswering. We compare the performance of our algorithm with prior methods and\nobserve an accuracy improvement of 4-7%.", "published": "2020-04-25 21:37:12", "link": "http://arxiv.org/abs/2004.12238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-scale Industrial and Professional Occupation Dataset", "abstract": "There has been growing interest in utilizing occupational data mining and\nanalysis. In today's job market, occupational data mining and analysis is\ngrowing in importance as it enables companies to predict employee turnover,\nmodel career trajectories, screen through resumes and perform other human\nresource tasks. A key requirement to facilitate these tasks is the need for an\noccupation-related dataset. However, most research use proprietary datasets or\ndo not make their dataset publicly available, thus impeding development in this\narea. To solve this issue, we present the Industrial and Professional\nOccupation Dataset (IPOD), which comprises 192k job titles belonging to 56k\nLinkedIn users. In addition to making IPOD publicly available, we also: (i)\nmanually annotate each job title with its associated level of seniority, domain\nof work and location; and (ii) provide embedding for job titles and discuss\nvarious use cases. This dataset is publicly available at\nhttps://github.com/junhua/ipod.", "published": "2020-04-25 10:45:48", "link": "http://arxiv.org/abs/2005.02780v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "All Word Embeddings from One Embedding", "abstract": "In neural network-based models for natural language processing (NLP), the\nlargest part of the parameters often consists of word embeddings. Conventional\nmodels prepare a large embedding matrix whose size depends on the vocabulary\nsize. Therefore, storing these models in memory and disk storage is costly. In\nthis study, to reduce the total number of parameters, the embeddings for all\nwords are represented by transforming a shared embedding. The proposed method,\nALONE (all word embeddings from one), constructs the embedding of a word by\nmodifying the shared embedding with a filter vector, which is word-specific but\nnon-trainable. Then, we input the constructed embedding into a feed-forward\nneural network to increase its expressiveness. Naively, the filter vectors\noccupy the same memory size as the conventional embedding matrix, which depends\non the vocabulary size. To solve this issue, we also introduce a\nmemory-efficient filter construction approach. We indicate our ALONE can be\nused as word representation sufficiently through an experiment on the\nreconstruction of pre-trained word embeddings. In addition, we also conduct\nexperiments on NLP application tasks: machine translation and summarization. We\ncombined ALONE with the current state-of-the-art encoder-decoder model, the\nTransformer, and achieved comparable scores on WMT 2014 English-to-German\ntranslation and DUC 2004 very short summarization with less parameters.", "published": "2020-04-25 07:38:08", "link": "http://arxiv.org/abs/2004.12073v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Named Entity Based Approach to Model Recipes", "abstract": "Traditional cooking recipes follow a structure which can be modelled very\nwell if the rules and semantics of the different sections of the recipe text\nare analyzed and represented accurately. We propose a structure that can\naccurately represent the recipe as well as a pipeline to infer the best\nrepresentation of the recipe in this uniform structure. The Ingredients section\nin a recipe typically lists down the ingredients required and corresponding\nattributes such as quantity, temperature, and processing state. This can be\nmodelled by defining these attributes and their values. The physical entities\nwhich make up a recipe can be broadly classified into utensils, ingredients and\ntheir combinations that are related by cooking techniques. The instruction\nsection lists down a series of events in which a cooking technique or process\nis applied upon these utensils and ingredients. We model these relationships in\nthe form of tuples. Thus, using a combination of these methods we model cooking\nrecipe in the dataset RecipeDB to show the efficacy of our method. This mined\ninformation model can have several applications which include translating\nrecipes between languages, determining similarity between recipes, generation\nof novel recipes and estimation of the nutritional profile of recipes. For the\npurpose of recognition of ingredient attributes, we train the Named Entity\nRelationship (NER) models and analyze the inferences with the help of K-Means\nclustering. Our model presented with an F1 score of 0.95 across all datasets.\nWe use a similar NER tagging model for labelling cooking techniques (F1 score =\n0.88) and utensils (F1 score = 0.90) within the instructions section. Finally,\nwe determine the temporal sequence of relationships between ingredients,\nutensils and cooking techniques for modeling the instruction steps.", "published": "2020-04-25 16:37:26", "link": "http://arxiv.org/abs/2004.12184v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MixText: Linguistically-Informed Interpolation of Hidden Space for\n  Semi-Supervised Text Classification", "abstract": "This paper presents MixText, a semi-supervised learning method for text\nclassification, which uses our newly designed data augmentation method called\nTMix. TMix creates a large amount of augmented training samples by\ninterpolating text in hidden space. Moreover, we leverage recent advances in\ndata augmentation to guess low-entropy labels for unlabeled data, hence making\nthem as easy to use as labeled data.By mixing labeled, unlabeled and augmented\ndata, MixText significantly outperformed current pre-trained and fined-tuned\nmodels and other state-of-the-art semi-supervised learning methods on several\ntext classification benchmarks. The improvement is especially prominent when\nsupervision is extremely limited. We have publicly released our code at\nhttps://github.com/GT-SALT/MixText.", "published": "2020-04-25 21:37:36", "link": "http://arxiv.org/abs/2004.12239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Linguistically Driven Framework for Query Expansion via Grammatical\n  Constituent Highlighting and Role-Based Concept Weighting", "abstract": "In this paper, we propose a linguistically-motivated query expansion\nframework that recognizes and en-codes significant query constituents that\ncharacterize query intent in order to improve retrieval performance.\nConcepts-of-Interest are recognized as the core concepts that represent the\ngist of the search goal whilst the remaining query constituents which serve to\nspecify the search goal and complete the query structure are classified as\ndescriptive, relational or structural. Acknowledging the need to form\nsemantically-associated base pairs for the purpose of extracting related\npotential expansion concepts, an algorithm which capitalizes on syntactical\ndependencies to capture relationships between adjacent and non-adjacent query\nconcepts is proposed. Lastly, a robust weighting scheme that duly emphasizes\nthe importance of query constituents based on their linguistic role within the\nexpanded query is presented. We demonstrate improvements in retrieval\neffectiveness in terms of increased mean average precision (MAP) garnered by\nthe proposed linguistic-based query expansion framework through experimentation\non the TREC ad hoc test collections.", "published": "2020-04-25 01:43:00", "link": "http://arxiv.org/abs/2004.13481v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Deep Multimodal Neural Architecture Search", "abstract": "Designing effective neural networks is fundamentally important in deep\nmultimodal learning. Most existing works focus on a single task and design\nneural architectures manually, which are highly task-specific and hard to\ngeneralize to different tasks. In this paper, we devise a generalized deep\nmultimodal neural architecture search (MMnas) framework for various multimodal\nlearning tasks. Given multimodal input, we first define a set of primitive\noperations, and then construct a deep encoder-decoder based unified backbone,\nwhere each encoder or decoder block corresponds to an operation searched from a\npredefined operation pool. On top of the unified backbone, we attach\ntask-specific heads to tackle different multimodal learning tasks. By using a\ngradient-based NAS algorithm, the optimal architectures for different tasks are\nlearned efficiently. Extensive ablation studies, comprehensive analysis, and\ncomparative experimental results show that the obtained MMnasNet significantly\noutperforms existing state-of-the-art approaches across three multimodal\nlearning tasks (over five datasets), including visual question answering,\nimage-text matching, and visual grounding.", "published": "2020-04-25 07:00:32", "link": "http://arxiv.org/abs/2004.12070v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Jointly Trained Transformers models for Spoken Language Translation", "abstract": "Conventional spoken language translation (SLT) systems are pipeline based\nsystems, where we have an Automatic Speech Recognition (ASR) system to convert\nthe modality of source from speech to text and a Machine Translation (MT)\nsystems to translate source text to text in target language. Recent progress in\nthe sequence-sequence architectures have reduced the performance gap between\nthe pipeline based SLT systems (cascaded ASR-MT) and End-to-End approaches.\nThough End-to-End and cascaded ASR-MT systems are reaching to the comparable\nlevels of performances, we can see a large performance gap using the ASR\nhypothesis and oracle text w.r.t MT models. This performance gap indicates that\nthe MT systems are prone to large performance degradation due to noisy ASR\nhypothesis as opposed to oracle text transcript. In this work this degradation\nin the performance is reduced by creating an end to-end differentiable pipeline\nbetween the ASR and MT systems. In this work, we train SLT systems with ASR\nobjective as an auxiliary loss and both the networks are connected through the\nneural hidden representations. This train ing would have an End-to-End\ndifferentiable path w.r.t to the final objective function as well as utilize\nthe ASR objective for better performance of the SLT systems. This architecture\nhas improved from BLEU from 36.8 to 44.5. Due to the Multi-task training the\nmodel also generates the ASR hypothesis which are used by a pre-trained MT\nmodel. Combining the proposed systems with the MT model has increased the BLEU\nscore by 1. All the experiments are reported on English-Portuguese speech\ntranslation task using How2 corpus. The final BLEU score is on-par with the\nbest speech translation system on How2 dataset with no additional training data\nand language model and much less parameters.", "published": "2020-04-25 11:28:39", "link": "http://arxiv.org/abs/2004.12111v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Learning to Update Natural Language Comments Based on Code Changes", "abstract": "We formulate the novel task of automatically updating an existing natural\nlanguage comment based on changes in the body of code it accompanies. We\npropose an approach that learns to correlate changes across two distinct\nlanguage representations, to generate a sequence of edits that are applied to\nthe existing comment to reflect the source code modifications. We train and\nevaluate our model using a dataset that we collected from commit histories of\nopen-source software projects, with each example consisting of a concurrent\nupdate to a method and its corresponding comment. We compare our approach\nagainst multiple baselines using both automatic metrics and human evaluation.\nResults reflect the challenge of this task and that our model outperforms\nbaselines with respect to making edits.", "published": "2020-04-25 15:37:46", "link": "http://arxiv.org/abs/2004.12169v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "QURATOR: Innovative Technologies for Content and Data Curation", "abstract": "In all domains and sectors, the demand for intelligent systems to support the\nprocessing and generation of digital content is rapidly increasing. The\navailability of vast amounts of content and the pressure to publish new content\nquickly and in rapid succession requires faster, more efficient and smarter\nprocessing and generation methods. With a consortium of ten partners from\nresearch and industry and a broad range of expertise in AI, Machine Learning\nand Language Technologies, the QURATOR project, funded by the German Federal\nMinistry of Education and Research, develops a sustainable and innovative\ntechnology platform that provides services to support knowledge workers in\nvarious industries to address the challenges they face when curating digital\ncontent. The project's vision and ambition is to establish an ecosystem for\ncontent curation technologies that significantly pushes the current state of\nthe art and transforms its region, the metropolitan area Berlin-Brandenburg,\ninto a global centre of excellence for curation technologies.", "published": "2020-04-25 17:21:15", "link": "http://arxiv.org/abs/2004.12195v1", "categories": ["cs.DL", "cs.CL", "cs.HC"], "primary_category": "cs.DL"}
{"title": "Hierarchical Multi Task Learning with Subword Contextual Embeddings for\n  Languages with Rich Morphology", "abstract": "Morphological information is important for many sequence labeling tasks in\nNatural Language Processing (NLP). Yet, existing approaches rely heavily on\nmanual annotations or external software to capture this information. In this\nstudy, we propose using subword contextual embeddings to capture the\nmorphological information for languages with rich morphology. In addition, we\nincorporate these embeddings in a hierarchical multi-task setting which is not\nemployed before, to the best of our knowledge. Evaluated on Dependency Parsing\n(DEP) and Named Entity Recognition (NER) tasks, which are shown to benefit\ngreatly from morphological information, our final model outperforms previous\nstate-of-the-art models on both tasks for the Turkish language. Besides, we\nshow a net improvement of 18.86% and 4.61% F-1 over the previously proposed\nmulti-task learner in the same setting for the DEP and the NER tasks,\nrespectively. Empirical results for five different MTL settings show that\nincorporating subword contextual embeddings brings significant improvements for\nboth tasks. In addition, we observed that multi-task learning consistently\nimproves the performance of the DEP component.", "published": "2020-04-25 22:55:56", "link": "http://arxiv.org/abs/2004.12247v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Word Embeddings and N-grams for Unsupervised Document\n  Summarization", "abstract": "Graph-based extractive document summarization relies on the quality of the\nsentence similarity graph. Bag-of-words or tf-idf based sentence similarity\nuses exact word matching, but fails to measure the semantic similarity between\nindividual words or to consider the semantic structure of sentences. In order\nto improve the similarity measure between sentences, we employ off-the-shelf\ndeep embedding features and tf-idf features, and introduce a new text\nsimilarity metric. An improved sentence similarity graph is built and used in a\nsubmodular objective function for extractive summarization, which consists of a\nweighted coverage term and a diversity term. A Transformer based compression\nmodel is developed for sentence compression to aid in document summarization.\nOur summarization approach is extractive and unsupervised. Experiments\ndemonstrate that our approach can outperform the tf-idf based approach and\nachieve state-of-the-art performance on the DUC04 dataset, and comparable\nperformance to the fully supervised learning methods on the CNN/DM and NYT\ndatasets.", "published": "2020-04-25 00:22:46", "link": "http://arxiv.org/abs/2004.14119v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Reevaluating Adversarial Examples in Natural Language", "abstract": "State-of-the-art attacks on NLP models lack a shared definition of a what\nconstitutes a successful attack. We distill ideas from past work into a unified\nframework: a successful natural language adversarial example is a perturbation\nthat fools the model and follows some linguistic constraints. We then analyze\nthe outputs of two state-of-the-art synonym substitution attacks. We find that\ntheir perturbations often do not preserve semantics, and 38% introduce\ngrammatical errors. Human surveys reveal that to successfully preserve\nsemantics, we need to significantly increase the minimum cosine similarities\nbetween the embeddings of swapped words and between the sentence encodings of\noriginal and perturbed sentences.With constraints adjusted to better preserve\nsemantics and grammaticality, the attack success rate drops by over 70\npercentage points.", "published": "2020-04-25 03:09:48", "link": "http://arxiv.org/abs/2004.14174v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Visual Cues in Audiovisual Speech Enhancement", "abstract": "We present an introspection of an audiovisual speech enhancement model. In\nparticular, we focus on interpreting how a neural audiovisual speech\nenhancement model uses visual cues to improve the quality of the target speech\nsignal. We show that visual cues provide not only high-level information about\nspeech activity, i.e., speech/silence, but also fine-grained visual information\nabout the place of articulation. One byproduct of this finding is that the\nlearned visual embeddings can be used as features for other visual speech\napplications. We demonstrate the effectiveness of the learned visual embeddings\nfor classifying visemes (the visual analogy to phonemes). Our results provide\ninsight into important aspects of audiovisual speech enhancement and\ndemonstrate how such models can be used for self-supervision tasks for visual\nspeech applications.", "published": "2020-04-25 01:00:03", "link": "http://arxiv.org/abs/2004.12031v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "L-Vector: Neural Label Embedding for Domain Adaptation", "abstract": "We propose a novel neural label embedding (NLE) scheme for the domain\nadaptation of a deep neural network (DNN) acoustic model with unpaired data\nsamples from source and target domains. With NLE method, we distill the\nknowledge from a powerful source-domain DNN into a dictionary of label\nembeddings, or l-vectors, one for each senone class. Each l-vector is a\nrepresentation of the senone-specific output distributions of the source-domain\nDNN and is learned to minimize the average L2, Kullback-Leibler (KL) or\nsymmetric KL distance to the output vectors with the same label through simple\naveraging or standard back-propagation. During adaptation, the l-vectors serve\nas the soft targets to train the target-domain model with cross-entropy loss.\nWithout parallel data constraint as in the teacher-student learning, NLE is\nspecially suited for the situation where the paired target-domain data cannot\nbe simulated from the source-domain data. We adapt a 6400 hours\nmulti-conditional US English acoustic model to each of the 9 accented English\n(80 to 830 hours) and kids' speech (80 hours). NLE achieves up to 14.1%\nrelative word error rate reduction over direct re-training with one-hot labels.", "published": "2020-04-25 06:40:31", "link": "http://arxiv.org/abs/2004.13480v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic\n  Question Answering and Spatial Semantic Lifting", "abstract": "Learning knowledge graph (KG) embeddings is an emerging technique for a\nvariety of downstream tasks such as summarization, link prediction, information\nretrieval, and question answering. However, most existing KG embedding models\nneglect space and, therefore, do not perform well when applied to (geo)spatial\ndata and tasks. For those models that consider space, most of them primarily\nrely on some notions of distance. These models suffer from higher computational\ncomplexity during training while still losing information beyond the relative\ndistance between entities. In this work, we propose a location-aware KG\nembedding model called SE-KGE. It directly encodes spatial information such as\npoint coordinates or bounding boxes of geographic entities into the KG\nembedding space. The resulting model is capable of handling different types of\nspatial reasoning. We also construct a geographic knowledge graph as well as a\nset of geographic query-answer pairs called DBGeo to evaluate the performance\nof SE-KGE in comparison to multiple baselines. Evaluation results show that\nSE-KGE outperforms these baselines on the DBGeo dataset for geographic logic\nquery answering task. This demonstrates the effectiveness of our\nspatially-explicit model and the importance of considering the scale of\ndifferent geographic entities. Finally, we introduce a novel downstream task\ncalled spatial semantic lifting which links an arbitrary location in the study\narea to entities in the KG via some relations. Evaluation on DBGeo shows that\nour model outperforms the baseline by a substantial margin.", "published": "2020-04-25 17:46:31", "link": "http://arxiv.org/abs/2004.14171v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG", "stat.ML", "I.2.4; I.1.3; I.2.2"], "primary_category": "cs.DB"}
{"title": "Sound Event Detection Utilizing Graph Laplacian Regularization with\n  Event Co-occurrence", "abstract": "A limited number of types of sound event occur in an acoustic scene and some\nsound events tend to co-occur in the scene; for example, the sound events\n\"dishes\" and \"glass jingling\" are likely to co-occur in the acoustic scene\n\"cooking\". In this paper, we propose a method of sound event detection using\ngraph Laplacian regularization with sound event co-occurrence taken into\naccount. In the proposed method, the occurrences of sound events are expressed\nas a graph whose nodes indicate the frequencies of event occurrence and whose\nedges indicate the sound event co-occurrences. This graph representation is\nthen utilized for the model training of sound event detection, which is\noptimized under an objective function with a regularization term considering\nthe graph structure of sound event occurrence and co-occurrence. Evaluation\nexperiments using the TUT Sound Events 2016 and 2017 detasets, and the TUT\nAcoustic Scenes 2016 dataset show that the proposed method improves the\nperformance of sound event detection by 7.9 percentage points compared with the\nconventional CNN-BiGRU-based detection method in terms of the segment-based F1\nscore. In particular, the experimental results indicate that the proposed\nmethod enables the detection of co-occurring sound events more accurately than\nthe conventional method.", "published": "2020-04-25 03:14:53", "link": "http://arxiv.org/abs/2004.12046v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Depthwise Separable Convolutional ResNet with Squeeze-and-Excitation\n  Blocks for Small-footprint Keyword Spotting", "abstract": "One difficult problem of keyword spotting is how to miniaturize its memory\nfootprint while maintain a high precision. Although convolutional neural\nnetworks have shown to be effective to the small-footprint keyword spotting\nproblem, they still need hundreds of thousands of parameters to achieve good\nperformance. In this paper, we propose an efficient model based on depthwise\nseparable convolution layers and squeeze-and-excitation blocks. Specifically,\nwe replace the standard convolution by the depthwise separable convolution,\nwhich reduces the number of the parameters of the standard convolution without\nsignificant performance degradation. We further improve the performance of the\ndepthwise separable convolution by reweighting the output feature maps of the\nfirst convolution layer with a so-called squeeze-and-excitation block. We\ncompared the proposed method with five representative models on two\nexperimental settings of the Google Speech Commands dataset. Experimental\nresults show that the proposed method achieves the state-of-the-art\nperformance. For example, it achieves a classification error rate of 3.29% with\na number of parameters of 72K in the first experiment, which significantly\noutperforms the comparison methods given a similar model size. It achieves an\nerror rate of 3.97% with a number of parameters of 10K, which is also slightly\nbetter than the state-of-the-art comparison method given a similar model size.", "published": "2020-04-25 17:53:33", "link": "http://arxiv.org/abs/2004.12200v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Active Voice Authentication", "abstract": "Active authentication refers to a new mode of identity verification in which\nbiometric indicators are continuously tested to provide real-time or near\nreal-time monitoring of an authorized access to a service or use of a device.\nThis is in contrast to the conventional authentication systems where a single\ntest in form of a verification token such as a password is performed. In active\nvoice authentication (AVA), voice is the biometric modality. This paper\ndescribes an ensemble of techniques that make reliable speaker verification\npossible using unconventionally short voice test signals. These techniques\ninclude model adaptation and minimum verification error (MVE) training that are\ntailored for the extremely short training and testing requirements. A database\nof 25 speakers is recorded for developing this system. In our off-line\nevaluation on this dataset, the system achieves an average windowed-based equal\nerror rates of 3-4% depending on the model configuration, which is remarkable\nconsidering that only 1 second of voice data is used to make every single\nauthentication decision. On the NIST SRE 2001 Dataset, the system provides a\n3.88% absolute gain over i-vector when the duration of test segment is 1\nsecond. A real-time demonstration system has been implemented on Microsoft\nSurface Pro.", "published": "2020-04-25 07:08:41", "link": "http://arxiv.org/abs/2004.12071v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A session-based song recommendation approach involving user\n  characterization along the play power-law distribution", "abstract": "In recent years, streaming music platforms have become very popular mainly\ndue to the huge number of songs these systems make available to users. This\nenormous availability means that recommendation mechanisms that help users to\nselect the music they like need to be incorporated. However, developing\nreliable recommender systems in the music field involves dealing with many\nproblems, some of which are generic and widely studied in the literature, while\nothers are specific to this application domain and are therefore less\nwell-known. This work is focused on two important issues that have not received\nmuch attention: managing gray-sheep users and obtaining implicit ratings. The\nfirst one is usually addressed by resorting to content information that is\noften difficult to obtain. The other drawback is related to the sparsity\nproblem that arises when there are obstacles to gather explicit ratings. In\nthis work, the referred shortcomings are addressed by means of a recommendation\napproach based on the users' streaming sessions. The method is aimed at\nmanaging the well-known power-law probability distribution representing the\nlistening behavior of users. This proposal improves the recommendation\nreliability of collaborative filtering methods while reducing the complexity of\nthe procedures used so far to deal with the gray-sheep problem.", "published": "2020-04-25 07:17:03", "link": "http://arxiv.org/abs/2004.13007v1", "categories": ["cs.IR", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
