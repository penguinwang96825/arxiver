{"title": "Towards Learning Transferable Conversational Skills using\n  Multi-dimensional Dialogue Modelling", "abstract": "Recent statistical approaches have improved the robustness and scalability of\nspoken dialogue systems. However, despite recent progress in domain adaptation,\ntheir reliance on in-domain data still limits their cross-domain scalability.\nIn this paper, we argue that this problem can be addressed by extending current\nmodels to reflect and exploit the multi-dimensional nature of human dialogue.\nWe present our multi-dimensional, statistical dialogue management framework, in\nwhich transferable conversational skills can be learnt by separating out\ndomain-independent dimensions of communication and using multi-agent\nreinforcement learning. Our initial experiments with a simulated user show that\nwe can speed up the learning process by transferring learnt policies.", "published": "2018-03-31 10:15:44", "link": "http://arxiv.org/abs/1804.00146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-depth Question classification using Convolutional Neural Networks", "abstract": "Convolutional neural networks for computer vision are fairly intuitive. In a\ntypical CNN used in image classification, the first layers learn edges, and the\nfollowing layers learn some filters that can identify an object. But CNNs for\nNatural Language Processing are not used often and are not completely\nintuitive. We have a good idea about what the convolution filters learn for the\ntask of text classification, and to that, we propose a neural network structure\nthat will be able to give good results in less time. We will be using\nconvolutional neural networks to predict the primary or broader topic of a\nquestion, and then use separate networks for each of these predicted topics to\naccurately classify their sub-topics.", "published": "2018-03-31 19:52:26", "link": "http://arxiv.org/abs/1804.00968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Language for Function Signature Representations", "abstract": "Recent work by (Richardson and Kuhn, 2017a,b; Richardson et al., 2018) looks\nat semantic parser induction and question answering in the domain of source\ncode libraries and APIs. In this brief note, we formalize the representations\nbeing learned in these studies and introduce a simple domain specific language\nand a systematic translation from this language to first-order logic. By\nrecasting the target representations in terms of classical logic, we aim to\nbroaden the applicability of existing code datasets for investigating more\ncomplex natural language understanding and reasoning problems in the software\ndomain.", "published": "2018-03-31 13:01:29", "link": "http://arxiv.org/abs/1804.00987v2", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Speaker Verification in Emotional Talking Environments based on\n  Three-Stage Framework", "abstract": "This work is dedicated to introducing, executing, and assessing a three-stage\nspeaker verification framework to enhance the degraded speaker verification\nperformance in emotional talking environments. Our framework is comprised of\nthree cascaded stages: gender identification stage followed by an emotion\nidentification stage followed by a speaker verification stage. The proposed\nframework has been assessed on two distinct and independent emotional speech\ndatasets: our collected dataset and Emotional Prosody Speech and Transcripts\ndataset. Our results demonstrate that speaker verification based on both gender\ncues and emotion cues is superior to each of speaker verification based on\ngender cues only, emotion cues only, and neither gender cues nor emotion cues.\nThe achieved average speaker verification performance based on the suggested\nmethodology is very similar to that attained in subjective assessment by human\nlisteners.", "published": "2018-03-31 10:49:43", "link": "http://arxiv.org/abs/1804.00155v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emirati-Accented Speaker Identification in each of Neutral and Shouted\n  Talking Environments", "abstract": "This work is devoted to capturing Emirati-accented speech database (Arabic\nUnited Arab Emirates database) in each of neutral and shouted talking\nenvironments in order to study and enhance text-independent Emirati-accented\nspeaker identification performance in shouted environment based on each of\nFirst-Order Circular Suprasegmental Hidden Markov Models (CSPHMM1s),\nSecond-Order Circular Suprasegmental Hidden Markov Models (CSPHMM2s), and\nThird-Order Circular Suprasegmental Hidden Markov Models (CSPHMM3s) as\nclassifiers. In this research, our database was collected from fifty Emirati\nnative speakers (twenty five per gender) uttering eight common Emirati\nsentences in each of neutral and shouted talking environments. The extracted\nfeatures of our collected database are called Mel-Frequency Cepstral\nCoefficients (MFCCs). Our results show that average Emirati-accented speaker\nidentification performance in neutral environment is 94.0%, 95.2%, and 95.9%\nbased on CSPHMM1s, CSPHMM2s, and CSPHMM3s, respectively. On the other hand, the\naverage performance in shouted environment is 51.3%, 55.5%, and 59.3% based,\nrespectively, on CSPHMM1s, CSPHMM2s, and CSPHMM3s. The achieved average speaker\nidentification performance in shouted environment based on CSPHMM3s is very\nsimilar to that obtained in subjective assessment by human listeners.", "published": "2018-03-31 10:46:38", "link": "http://arxiv.org/abs/1804.00981v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
