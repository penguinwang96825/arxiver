{"title": "Boosting Entity Mention Detection for Targetted Twitter Streams with\n  Global Contextual Embeddings", "abstract": "Microblogging sites, like Twitter, have emerged as ubiquitous sources of\ninformation. Two important tasks related to the automatic extraction and\nanalysis of information in Microblogs are Entity Mention Detection (EMD) and\nEntity Detection (ED). The state-of-the-art EMD systems aim to model the\nnon-literary nature of microblog text by training upon offline static datasets.\nThey extract a combination of surface-level features -- orthographic, lexical,\nand semantic -- from individual messages for noisy text modeling and entity\nextraction. But given the constantly evolving nature of microblog streams,\ndetecting all entity mentions from such varying yet limited context of short\nmessages remains a difficult problem. To this end, we propose a framework named\nEMD Globalizer, better suited for the execution of EMD learners on microblog\nstreams. It deviates from the processing of isolated microblog messages by\nexisting EMD systems, where learned knowledge from the immediate context of a\nmessage is used to suggest entities. After an initial extraction of entity\ncandidates by an EMD system, the proposed framework leverages occurrence mining\nto find additional candidate mentions that are missed during this first\ndetection. Aggregating the local contextual representations of these mentions,\na global embedding is drawn from the collective context of an entity candidate\nwithin a stream. The global embeddings are then utilized to separate entities\nwithin the candidates from false positives. All mentions of said entities from\nthe stream are produced in the framework's final outputs. Our experiments show\nthat EMD Globalizer can enhance the effectiveness of all existing EMD systems\nthat we tested (on average by 25.61%) with a small additional computational\noverhead.", "published": "2022-01-28 01:44:05", "link": "http://arxiv.org/abs/2201.11885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "abstract": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.", "published": "2022-01-28 08:59:57", "link": "http://arxiv.org/abs/2201.11990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PCL: Peer-Contrastive Learning with Diverse Augmentations for\n  Unsupervised Sentence Embeddings", "abstract": "Learning sentence embeddings in an unsupervised manner is fundamental in\nnatural language processing. Recent common practice is to couple pre-trained\nlanguage models with unsupervised contrastive learning, whose success relies on\naugmenting a sentence with a semantically-close positive instance to construct\ncontrastive pairs. Nonetheless, existing approaches usually depend on a\nmono-augmenting strategy, which causes learning shortcuts towards the\naugmenting biases and thus corrupts the quality of sentence embeddings. A\nstraightforward solution is resorting to more diverse positives from a\nmulti-augmenting strategy, while an open question remains about how to\nunsupervisedly learn from the diverse positives but with uneven augmenting\nqualities in the text field. As one answer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations. PCL constructs diverse contrastive\npositives and negatives at the group level for unsupervised sentence\nembeddings. PCL performs peer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias ability and an effective way to\nlearn from diverse augmentations. Experiments on STS benchmarks verify the\neffectiveness of PCL against its competitors in unsupervised sentence\nembeddings.", "published": "2022-01-28 13:02:41", "link": "http://arxiv.org/abs/2201.12093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Protum: A New Method For Prompt Tuning Based on \"[MASK]\"", "abstract": "Recently, prompt tuning \\cite{lester2021power} has gradually become a new\nparadigm for NLP, which only depends on the representation of the words by\nfreezing the parameters of pre-trained language models (PLMs) to obtain\nremarkable performance on downstream tasks. It maintains the consistency of\nMasked Language Model (MLM) \\cite{devlin2018bert} task in the process of\npre-training, and avoids some issues that may happened during fine-tuning.\nNaturally, we consider that the \"[MASK]\" tokens carry more useful information\nthan other tokens because the model combines with context to predict the masked\ntokens. Among the current prompt tuning methods, there will be a serious\nproblem of random composition of the answer tokens in prediction when they\npredict multiple words so that they have to map tokens to labels with the help\nverbalizer. In response to the above issue, we propose a new \\textbf{Pro}mpt\n\\textbf{Tu}ning based on \"[\\textbf{M}ASK]\" (\\textbf{Protum}) method in this\npaper, which constructs a classification task through the information carried\nby the hidden layer of \"[MASK]\" tokens and then predicts the labels directly\nrather than the answer tokens. At the same time, we explore how different\nhidden layers under \"[MASK]\" impact on our classification model on many\ndifferent data sets. Finally, we find that our \\textbf{Protum} can achieve much\nbetter performance than fine-tuning after continuous pre-training with less\ntime consumption. Our model facilitates the practical application of large\nmodels in NLP.", "published": "2022-01-28 13:34:30", "link": "http://arxiv.org/abs/2201.12109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Broad Coverage Named Entity Resource: A Data-Efficient\n  Approach for Many Diverse Languages", "abstract": "Parallel corpora are ideal for extracting a multilingual named entity (MNE)\nresource, i.e., a dataset of names translated into multiple languages. Prior\nwork on extracting MNE datasets from parallel corpora required resources such\nas large monolingual corpora or word aligners that are unavailable or perform\npoorly for underresourced languages. We present CLC-BN, a new method for\ncreating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus\nof more than 1000 languages. CLC-BN learns a neural transliteration model from\nparallel-corpus statistics, without requiring any other bilingual resources,\nword aligners, or seed data. Experimental results show that CLC-BN clearly\noutperforms prior work. We release an MNE resource for 1340 languages and\ndemonstrate its effectiveness in two downstream tasks: knowledge graph\naugmentation and bilingual lexicon induction.", "published": "2022-01-28 16:19:50", "link": "http://arxiv.org/abs/2201.12219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schema-Free Dependency Parsing via Sequence Generation", "abstract": "Dependency parsing aims to extract syntactic dependency structure or semantic\ndependency structure for sentences. Existing methods suffer the drawbacks of\nlacking universality or highly relying on the auxiliary decoder. To remedy\nthese drawbacks, we propose to achieve universal and schema-free Dependency\nParsing (DP) via Sequence Generation (SG) DPSG by utilizing only the\npre-trained language model (PLM) without any auxiliary structures or parsing\nalgorithms. We first explore different serialization designing strategies for\nconverting parsing structures into sequences. Then we design dependency units\nand concatenate these units into the sequence for DPSG. Thanks to the high\nflexibility of the sequence generation, our DPSG can achieve both syntactic DP\nand semantic DP using a single model. By concatenating the prefix to indicate\nthe specific schema with the sequence, our DPSG can even accomplish\nmulti-schemata parsing. The effectiveness of our DPSG is demonstrated by the\nexperiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and\nSemEval16. DPSG achieves comparable results with the first-tier methods on all\nthe benchmarks and even the state-of-the-art (SOTA) performance in CODT and\nSemEval16. This paper demonstrates our DPSG has the potential to be a new\nparsing paradigm. We will release our codes upon acceptance.", "published": "2022-01-28 20:32:04", "link": "http://arxiv.org/abs/2201.12407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language\n  Models: A Survey", "abstract": "While commonsense knowledge acquisition and reasoning has traditionally been\na core research topic in the knowledge representation and reasoning community,\nrecent years have seen a surge of interest in the natural language processing\ncommunity in developing pre-trained models and testing their ability to address\na variety of newly designed commonsense knowledge reasoning and generation\ntasks. This paper presents a survey of these tasks, discusses the strengths and\nweaknesses of state-of-the-art pre-trained models for commonsense reasoning and\ngeneration as revealed by these tasks, and reflects on future research\ndirections.", "published": "2022-01-28 21:55:09", "link": "http://arxiv.org/abs/2201.12438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dominant Set-based Active Learning for Text Classification and its\n  Application to Online Social Media", "abstract": "Recent advances in natural language processing (NLP) in online social media\nare evidently owed to large-scale datasets. However, labeling, storing, and\nprocessing a large number of textual data points, e.g., tweets, has remained\nchallenging. On top of that, in applications such as hate speech detection,\nlabeling a sufficiently large dataset containing offensive content can be\nmentally and emotionally taxing for human annotators. Thus, NLP methods that\ncan make the best use of significantly less labeled data points are of great\ninterest. In this paper, we present a novel pool-based active learning method\nthat can be used for the training of large unlabeled corpus with minimum\nannotation cost. For that, we propose to find the dominant sets of local\nclusters in the feature space. These sets represent maximally cohesive\nstructures in the data. Then, the samples that do not belong to any of the\ndominant sets are selected to be used to train the model, as they represent the\nboundaries of the local clusters and are more challenging to classify. Our\nproposed method does not have any parameters to be tuned, making it\ndataset-independent, and it can approximately achieve the same classification\naccuracy as full training data, with significantly fewer data points.\nAdditionally, our method achieves a higher performance in comparison to the\nstate-of-the-art active learning strategies. Furthermore, our proposed\nalgorithm is able to incorporate conventional active learning scores, such as\nuncertainty-based scores, into its selection criteria. We show the\neffectiveness of our method on different datasets and using different neural\nnetwork architectures.", "published": "2022-01-28 19:19:03", "link": "http://arxiv.org/abs/2202.00540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and\n  Paired Classifiers", "abstract": "We present a novel multiple-source unsupervised model for text classification\nunder domain shift. Our model exploits the update rates in document\nrepresentations to dynamically integrate domain encoders. It also employs a\nprobabilistic heuristic to infer the error rate in the target domain in order\nto pair source classifiers. Our heuristic exploits data transformation cost and\nthe classifier accuracy in the target feature space. We have used real world\nscenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We\nalso used pretrained multi-layer transformers as the document encoder in the\nexperiments to demonstrate whether the improvement achieved by domain\nadaptation models can be delivered by out-of-the-box language model\npretraining. The experiments testify that our model is the top performing\napproach in this setting.", "published": "2022-01-28 00:50:01", "link": "http://arxiv.org/abs/2201.11870v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.", "published": "2022-01-28 02:33:07", "link": "http://arxiv.org/abs/2201.11903v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linear Adversarial Concept Erasure", "abstract": "Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.", "published": "2022-01-28 13:00:17", "link": "http://arxiv.org/abs/2201.12091v8", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Kernelized Concept Erasure", "abstract": "The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.", "published": "2022-01-28 15:45:13", "link": "http://arxiv.org/abs/2201.12191v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generative Cooperative Networks for Natural Language Generation", "abstract": "Generative Adversarial Networks (GANs) have known a tremendous success for\nmany continuous generation tasks, especially in the field of image generation.\nHowever, for discrete outputs such as language, optimizing GANs remains an open\nproblem with many instabilities, as no gradient can be properly back-propagated\nfrom the discriminator output to the generator parameters. An alternative is to\nlearn the generator network via reinforcement learning, using the discriminator\nsignal as a reward, but such a technique suffers from moving rewards and\nvanishing gradient problems. Finally, it often falls short compared to direct\nmaximum-likelihood approaches. In this paper, we introduce Generative\nCooperative Networks, in which the discriminator architecture is cooperatively\nused along with the generation policy to output samples of realistic texts for\nthe task at hand. We give theoretical guarantees of convergence for our\napproach, and study various efficient decoding schemes to empirically achieve\nstate-of-the-art results in two main NLG tasks.", "published": "2022-01-28 18:36:57", "link": "http://arxiv.org/abs/2201.12320v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Unified Approach to Entity-Centric Context Tracking in Social\n  Conversations", "abstract": "In human-human conversations, Context Tracking deals with identifying\nimportant entities and keeping track of their properties and relationships.\nThis is a challenging problem that encompasses several subtasks such as slot\ntagging, coreference resolution, resolving plural mentions and entity linking.\nWe approach this problem as an end-to-end modeling task where the\nconversational context is represented by an entity repository containing the\nentity references mentioned so far, their properties and the relationships\nbetween them. The repository is updated turn-by-turn, thus making training and\ninference computationally efficient even for long conversations. This paper\nlays the groundwork for an investigation of this framework in two ways. First,\nwe release Contrack, a large scale human-human conversation corpus for context\ntracking with people and location annotations. It contains over 7000\nconversations with an average of 11.8 turns, 5.8 entities and 15.2 references\nper conversation. Second, we open-source a neural network architecture for\ncontext tracking. Finally we compare this network to state-of-the-art\napproaches for the subtasks it subsumes and report results on the involved\ntradeoffs.", "published": "2022-01-28 20:38:13", "link": "http://arxiv.org/abs/2201.12409v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval", "abstract": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .", "published": "2022-01-28 21:38:56", "link": "http://arxiv.org/abs/2201.12431v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural-FST Class Language Model for End-to-End Speech Recognition", "abstract": "We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech\nrecognition, a novel method that combines neural network language models\n(NNLMs) and finite state transducers (FSTs) in a mathematically consistent\nframework. Our method utilizes a background NNLM which models generic\nbackground text together with a collection of domain-specific entities modeled\nas individual FSTs. Each output token is generated by a mixture of these\ncomponents; the mixture weights are estimated with a separately trained neural\ndecider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in\nterms of Word Error Rate. NFCLM achieves similar performance as traditional\nNNLM and FST shallow fusion while being less prone to overbiasing and 12 times\nmore compact, making it more suitable for on-device usage.", "published": "2022-01-28 00:20:57", "link": "http://arxiv.org/abs/2201.11867v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "\"That's so cute!\": The CARE Dataset for Affective Response Detection", "abstract": "Social media plays an increasing role in our communication with friends and\nfamily, and our consumption of information and entertainment. Hence, to design\neffective ranking functions for posts on social media, it would be useful to\npredict the affective response to a post (e.g., whether the user is likely to\nbe humored, inspired, angered, informed). Similar to work on emotion\nrecognition (which focuses on the affect of the publisher of the post), the\ntraditional approach to recognizing affective response would involve an\nexpensive investment in human annotation of training data.\n  We introduce CARE$_{db}$, a dataset of 230k social media posts annotated\naccording to 7 affective responses using the Common Affective Response\nExpression (CARE) method. The CARE method is a means of leveraging the signal\nthat is present in comments that are posted in response to a post, providing\nhigh-precision evidence about the affective response of the readers to the post\nwithout human annotation. Unlike human annotation, the annotation process we\ndescribe here can be iterated upon to expand the coverage of the method,\nparticularly for new affective responses. We present experiments that\ndemonstrate that the CARE annotations compare favorably with crowd-sourced\nannotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models\nfor predicting affective response as well as emotion detection, demonstrating\nthe utility of the dataset for related tasks.", "published": "2022-01-28 02:17:50", "link": "http://arxiv.org/abs/2201.11895v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "A Secure and Efficient Federated Learning Framework for NLP", "abstract": "In this work, we consider the problem of designing secure and efficient\nfederated learning (FL) frameworks. Existing solutions either involve a trusted\naggregator or require heavyweight cryptographic primitives, which degrades\nperformance significantly. Moreover, many existing secure FL designs work only\nunder the restrictive assumption that none of the clients can be dropped out\nfrom the training protocol. To tackle these problems, we propose SEFL, a secure\nand efficient FL framework that (1) eliminates the need for the trusted\nentities; (2) achieves similar and even better model accuracy compared with\nexisting FL designs; (3) is resilient to client dropouts. Through extensive\nexperimental studies on natural language processing (NLP) tasks, we demonstrate\nthat the SEFL achieves comparable accuracy compared to existing FL solutions,\nand the proposed pruning technique can improve runtime performance up to 13.7x.", "published": "2022-01-28 05:01:25", "link": "http://arxiv.org/abs/2201.11934v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising\n  Diffusion GANs", "abstract": "Denoising diffusion probabilistic models (DDPMs) are expressive generative\nmodels that have been used to solve a variety of speech synthesis problems.\nHowever, because of their high sampling costs, DDPMs are difficult to use in\nreal-time speech processing applications. In this paper, we introduce\nDiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving\nhigh-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising\ndiffusion generative adversarial networks (GANs), which adopt an\nadversarially-trained expressive model to approximate the denoising\ndistribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can\ngenerate high-fidelity speech samples within only 4 denoising steps. We present\nan active shallow diffusion mechanism to further speed up inference. A\ntwo-stage training scheme is proposed, with a basic TTS acoustic model trained\nat stage one providing valuable prior information for a DDPM trained at stage\ntwo. Our experiments show that DiffGAN-TTS can achieve high synthesis\nperformance with only 1 denoising step.", "published": "2022-01-28 07:41:10", "link": "http://arxiv.org/abs/2201.11972v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving End-to-End Models for Set Prediction in Spoken Language\n  Understanding", "abstract": "The goal of spoken language understanding (SLU) systems is to determine the\nmeaning of the input speech signal, unlike speech recognition which aims to\nproduce verbatim transcripts. Advances in end-to-end (E2E) speech modeling have\nmade it possible to train solely on semantic entities, which are far cheaper to\ncollect than verbatim transcripts. We focus on this set prediction problem,\nwhere entity order is unspecified. Using two classes of E2E models, RNN\ntransducers and attention based encoder-decoders, we show that these models\nwork best when the training entity sequence is arranged in spoken order. To\nimprove E2E SLU models when entity spoken order is unknown, we propose a novel\ndata augmentation technique along with an implicit attention based alignment\nmethod to infer the spoken order. F1 scores significantly increased by more\nthan 11% for RNN-T and about 2% for attention based encoder-decoder SLU models,\noutperforming previously reported results.", "published": "2022-01-28 13:23:17", "link": "http://arxiv.org/abs/2201.12105v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rethinking Attention-Model Explainability through Faithfulness Violation\n  Test", "abstract": "Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.", "published": "2022-01-28 13:42:31", "link": "http://arxiv.org/abs/2201.12114v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Can Wikipedia Help Offline Reinforcement Learning?", "abstract": "Fine-tuning reinforcement learning (RL) models has been challenging because\nof a lack of large scale off-the-shelf datasets as well as high variance in\ntransferability among different environments. Recent work has looked at\ntackling offline RL from the perspective of sequence modeling with improved\nresults as result of the introduction of the Transformer architecture. However,\nwhen the model is trained from scratch, it suffers from slow convergence\nspeeds. In this paper, we look to take advantage of this formulation of\nreinforcement learning as sequence modeling and investigate the transferability\nof pre-trained sequence models on other domains (vision, language) when\nfinetuned on offline RL tasks (control, games). To this end, we also propose\ntechniques to improve transfer between these domains. Results show consistent\nperformance gains in terms of both convergence speed and reward on a variety of\nenvironments, accelerating training by 3-6x and achieving state-of-the-art\nperformance in a variety of tasks using Wikipedia-pretrained and GPT2 language\nmodels. We hope that this work not only brings light to the potentials of\nleveraging generic sequence modeling techniques and pre-trained models for RL,\nbut also inspires future work on sharing knowledge between generative modeling\ntasks of completely different domains.", "published": "2022-01-28 13:55:35", "link": "http://arxiv.org/abs/2201.12122v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reducing language context confusion for end-to-end code-switching\n  automatic speech recognition", "abstract": "Code-switching deals with alternative languages in communication process.\nTraining end-to-end (E2E) automatic speech recognition (ASR) systems for\ncode-switching is especially challenging as code-switching training data are\nalways insufficient to combat the increased multilingual context confusion due\nto the presence of more than one language. We propose a language-related\nattention mechanism to reduce multilingual context confusion for the E2E\ncode-switching ASR model based on the Equivalence Constraint (EC) Theory. The\nlinguistic theory requires that any monolingual fragment that occurs in the\ncode-switching sentence must occur in one of the monolingual sentences. The\ntheory establishes a bridge between monolingual data and code-switching data.\nWe leverage this linguistics theory to design the code-switching E2E ASR model.\nThe proposed model efficiently transfers language knowledge from rich\nmonolingual data to improve the performance of the code-switching ASR model. We\nevaluate our model on ASRU 2019 Mandarin-English code-switching challenge\ndataset. Compared to the baseline model, our proposed model achieves a 17.12%\nrelative error reduction.", "published": "2022-01-28 14:39:29", "link": "http://arxiv.org/abs/2201.12155v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Describing Differences between Text Distributions with Natural Language", "abstract": "How do two distributions of texts differ? Humans are slow at answering this,\nsince discovering patterns might require tediously reading through hundreds of\nsamples. We propose to automatically summarize the differences by \"learning a\nnatural language hypothesis\": given two distributions $D_{0}$ and $D_{1}$, we\nsearch for a description that is more often true for $D_{1}$, e.g., \"is\nmilitary-related.\" To tackle this problem, we fine-tune GPT-3 to propose\ndescriptions with the prompt: \"[samples of $D_{0}$] + [samples of $D_{1}$] +\nthe difference between them is_____.\" We then re-rank the descriptions by\nchecking how often they hold on a larger set of samples with a learned\nverifier. On a benchmark of 54 real-world binary classification tasks, while\nGPT-3 Curie (13B) only generates a description similar to human annotation 7%\nof the time, the performance reaches 61% with fine-tuning and re-ranking, and\nour best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to\ndescribe distribution shifts, debug dataset shortcuts, summarize unknown tasks,\nand label text clusters, and present analyses based on automatically generated\ndescriptions.", "published": "2022-01-28 18:38:13", "link": "http://arxiv.org/abs/2201.12323v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Creation and Human-assisted Curation of Computable Scientific\n  Models from Code and Text", "abstract": "Scientific models hold the key to better understanding and predicting the\nbehavior of complex systems. The most comprehensive manifestation of a\nscientific model, including crucial assumptions and parameters that underpin\nits usability, is usually embedded in associated source code and documentation,\nwhich may employ a variety of (potentially outdated) programming practices and\nlanguages. Domain experts cannot gain a complete understanding of the\nimplementation of a scientific model if they are not familiar with the code.\nFurthermore, rapid research and development iterations make it challenging to\nkeep up with constantly evolving scientific model codebases. To address these\nchallenges, we develop a system for the automated creation and human-assisted\ncuration of a knowledge graph of computable scientific models that analyzes a\nmodel's code in the context of any associated inline comments and external\ndocumentation. Our system uses knowledge-driven as well as data-driven\napproaches to identify and extract relevant concepts from code and equations\nfrom textual documents to semantically annotate models using domain\nterminology. These models are converted into executable Python functions and\nthen can further be composed into complex workflows to answer different forms\nof domain-driven questions. We present experimental results obtained using a\ndataset of code and associated text derived from NASA's Hypersonic Aerodynamics\nwebsite.", "published": "2022-01-28 17:31:38", "link": "http://arxiv.org/abs/2202.13739v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based\n  Multi-Modal Fact-Checking", "abstract": "Identifying fake news is a very difficult task, especially when considering\nthe multiple modes of conveying information through text, image, video and/or\naudio. We attempted to tackle the problem of automated\nmisinformation/disinformation detection in multi-modal news sources (including\ntext and images) through our simple, yet effective, approach in the FACTIFY\nshared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of\n74.807%, which was the fourth best out of all the submissions. In this paper we\nwill explain our approach to undertake the shared task.", "published": "2022-01-28 18:13:03", "link": "http://arxiv.org/abs/2203.07990v1", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Impact of Naturalistic Field Acoustic Environments on Forensic\n  Text-independent Speaker Verification System", "abstract": "Audio analysis for forensic speaker verification offers unique challenges in\nsystem performance due in part to data collected in naturalistic field acoustic\nenvironments where location/scenario uncertainty is common in the forensic data\ncollection process. Forensic speech data as potential evidence can be obtained\nin random naturalistic environments resulting in variable data quality. Speech\nsamples may include variability due to vocal efforts such as yelling over 911\nemergency calls, whereas others might be whisper or situational stressed voice\nin a field location or interview room. Such speech variability consists of\nintrinsic and extrinsic characteristics and makes forensic speaker verification\na complicated and daunting task. Extrinsic properties include recording\nequipment such as microphone type and placement, ambient noise, room\nconfiguration including reverberation, and other environmental scenario-based\nissues. Some factors, such as noise and non-target speech, will impact the\nverification system performance by their mere presence. To investigate the\nimpact of field acoustic environments, we performed a speaker verification\nstudy based on the CRSS-Forensic corpus with audio collected from 8 field\nlocations including police interviews. This investigation includes an analysis\nof the impact of seven unseen acoustic environments on speaker verification\nsystem performance using an x-Vector system.", "published": "2022-01-28 03:03:56", "link": "http://arxiv.org/abs/2201.13246v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT\n  Domain", "abstract": "Frequency domain processing, and in particular the use of Modified Discrete\nCosine Transform (MDCT), is the most widespread approach to audio coding.\nHowever, at low bitrates, audio quality, especially for speech, degrades\ndrastically due to the lack of available bits to directly code the transform\ncoefficients. Traditionally, post-filtering has been used to mitigate artefacts\nin the coded speech by exploiting a-priori information of the source and extra\ntransmitted parameters. Recently, data-driven post-filters have shown better\nresults, but at the cost of significant additional complexity and delay. In\nthis work, we propose a mask-based post-filter operating directly in MDCT\ndomain of the codec, inducing no extra delay. The real-valued mask is applied\nto the quantized MDCT coefficients and is estimated from a relatively\nlightweight convolutional encoder-decoder network. Our solution is tested on\nthe recently standardized low-delay, low-complexity codec (LC3) at lowest\npossible bitrate of 16 kbps. Objective and subjective assessments clearly show\nthe advantage of this approach over the conventional post-filter, with an\naverage improvement of 10 MUSHRA points over the LC3 coded speech.", "published": "2022-01-28 11:08:02", "link": "http://arxiv.org/abs/2201.12039v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Star Temporal Classification: Sequence Classification with Partially\n  Labeled Data", "abstract": "We develop an algorithm which can learn from partially labeled and\nunsegmented sequential data. Most sequential loss functions, such as\nConnectionist Temporal Classification (CTC), break down when many labels are\nmissing. We address this problem with Star Temporal Classification (STC) which\nuses a special star token to allow alignments which include all possible tokens\nwhenever a token could be missing. We express STC as the composition of\nweighted finite-state transducers (WFSTs) and use GTN (a framework for\nautomatic differentiation with WFSTs) to compute gradients. We perform\nextensive experiments on automatic speech recognition. These experiments show\nthat STC can recover most of the performance of supervised baseline when up to\n70% of the labels are missing. We also perform experiments in handwriting\nrecognition to show that our method easily applies to other sequence\nclassification tasks.", "published": "2022-01-28 16:03:17", "link": "http://arxiv.org/abs/2201.12208v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatic Audio Captioning using Attention weighted Event based\n  Embeddings", "abstract": "Automatic Audio Captioning (AAC) refers to the task of translating audio into\na natural language that describes the audio events, source of the events and\ntheir relationships. The limited samples in AAC datasets at present, has set up\na trend to incorporate transfer learning with Audio Event Detection (AED) as a\nparent task. Towards this direction, in this paper, we propose an\nencoder-decoder architecture with light-weight (i.e. with lesser learnable\nparameters) Bi-LSTM recurrent layers for AAC and compare the performance of two\nstate-of-the-art pre-trained AED models as embedding extractors. Our results\nshow that an efficient AED based embedding extractor combined with temporal\nattention and augmentation techniques is able to surpass existing literature\nwith computationally intensive architectures. Further, we provide evidence of\nthe ability of the non-uniform attention weighted encoding generated as a part\nof our model to facilitate the decoder glance over specific sections of the\naudio while generating each token.", "published": "2022-01-28 05:54:19", "link": "http://arxiv.org/abs/2201.12352v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dual Learning Music Composition and Dance Choreography", "abstract": "Music and dance have always co-existed as pillars of human activities,\ncontributing immensely to the cultural, social, and entertainment functions in\nvirtually all societies. Notwithstanding the gradual systematization of music\nand dance into two independent disciplines, their intimate connection is\nundeniable and one art-form often appears incomplete without the other. Recent\nresearch works have studied generative models for dance sequences conditioned\non music. The dual task of composing music for given dances, however, has been\nlargely overlooked. In this paper, we propose a novel extension, where we\njointly model both tasks in a dual learning approach. To leverage the duality\nof the two modalities, we introduce an optimal transport objective to align\nfeature embeddings, as well as a cycle consistency loss to foster overall\nconsistency. Experimental results demonstrate that our dual learning framework\nimproves individual task performance, delivering generated music compositions\nand dance choreographs that are realistic and faithful to the conditioned\ninputs.", "published": "2022-01-28 09:20:28", "link": "http://arxiv.org/abs/2201.11999v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
