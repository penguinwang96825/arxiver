{"title": "Representations of Syntax [MASK] Useful: Effects of Constituency and\n  Dependency Structure in Recursive LSTMs", "abstract": "Sequence-based neural networks show significant sensitivity to syntactic\nstructure, but they still perform less well on syntactic tasks than tree-based\nnetworks. Such tree-based networks can be provided with a constituency parse, a\ndependency parse, or both. We evaluate which of these two representational\nschemes more effectively introduces biases for syntactic structure that\nincrease performance on the subject-verb agreement prediction task. We find\nthat a constituency-based network generalizes more robustly than a\ndependency-based one, and that combining the two types of structure does not\nyield further improvement. Finally, we show that the syntactic robustness of\nsequential models can be substantially improved by fine-tuning on a small\namount of constructed data, suggesting that data augmentation is a viable\nalternative to explicit constituency structure for imparting the syntactic\nbiases that sequential models are lacking.", "published": "2020-04-30 18:00:06", "link": "http://arxiv.org/abs/2005.00019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Factual Consistency Between a Response and Persona Facts", "abstract": "Neural models for response generation produce responses that are semantically\nplausible but not necessarily factually consistent with facts describing the\nspeaker's persona. These models are trained with fully supervised learning\nwhere the objective function barely captures factual consistency. We propose to\nfine-tune these models by reinforcement learning and an efficient reward\nfunction that explicitly captures the consistency between a response and\npersona facts as well as semantic plausibility. Our automatic and human\nevaluations on the PersonaChat corpus confirm that our approach increases the\nrate of responses that are factually consistent with persona facts over its\nsupervised counterpart while retaining the language quality of responses.", "published": "2020-04-30 18:08:22", "link": "http://arxiv.org/abs/2005.00036v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressively Pretrained Dense Corpus Index for Open-Domain Question\n  Answering", "abstract": "To extract answers from a large corpus, open-domain question answering (QA)\nsystems usually rely on information retrieval (IR) techniques to narrow the\nsearch space. Standard inverted index methods such as TF-IDF are commonly used\nas thanks to their efficiency. However, their retrieval performance is limited\nas they simply use shallow and sparse lexical features. To break the IR\nbottleneck, recent studies show that stronger retrieval performance can be\nachieved by pretraining a effective paragraph encoder that index paragraphs\ninto dense vectors. Once trained, the corpus can be pre-encoded into\nlow-dimensional vectors and stored within an index structure where the\nretrieval can be efficiently implemented as maximum inner product search.\n  Despite the promising results, pretraining such a dense index is expensive\nand often requires a very large batch size. In this work, we propose a simple\nand resource-efficient method to pretrain the paragraph encoder. First, instead\nof using heuristically created pseudo question-paragraph pairs for pretraining,\nwe utilize an existing pretrained sequence-to-sequence model to build a strong\nquestion generator that creates high-quality pretraining data. Second, we\npropose a progressive pretraining algorithm to ensure the existence of\neffective negative samples in each batch. Across three datasets, our method\noutperforms an existing dense retrieval method that uses 7 times more\ncomputational resources for pretraining.", "published": "2020-04-30 18:09:50", "link": "http://arxiv.org/abs/2005.00038v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UiO-UvA at SemEval-2020 Task 1: Contextualised Embeddings for Lexical\n  Semantic Change Detection", "abstract": "We apply contextualised word embeddings to lexical semantic change detection\nin the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking\nwords by the degree of their semantic drift over time. We analyse the\nperformance of two contextualising architectures (BERT and ELMo) and three\nchange detection algorithms. We find that the most effective algorithms rely on\nthe cosine similarity between averaged token embeddings and the pairwise\ndistances between token embeddings. They outperform strong baselines by a large\nmargin (in the post-evaluation phase, we have the best Subtask 2 submission for\nSemEval-2020 Task 1), but interestingly, the choice of a particular algorithm\ndepends on the distribution of gold scores in the test set.", "published": "2020-04-30 18:43:57", "link": "http://arxiv.org/abs/2005.00050v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "abstract": "The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml", "published": "2020-04-30 18:54:43", "link": "http://arxiv.org/abs/2005.00052v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Controlled Neural Argument Generation", "abstract": "We rely on arguments in our daily lives to deliver our opinions and base them\non evidence, making them more convincing in turn. However, finding and\nformulating arguments can be challenging. In this work, we train a language\nmodel for argument generation that can be controlled on a fine-grained level to\ngenerate sentence-level arguments for a given topic, stance, and aspect. We\ndefine argument aspect detection as a necessary method to allow this\nfine-granular control and crowdsource a dataset with 5,032 arguments annotated\nwith aspects. Our evaluation shows that our generation model is able to\ngenerate high-quality, aspect-specific arguments. Moreover, these arguments can\nbe used to improve the performance of stance detection models via data\naugmentation and to generate counter-arguments. We publish all datasets and\ncode to fine-tune the language model.", "published": "2020-04-30 20:17:22", "link": "http://arxiv.org/abs/2005.00084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for\n  Indic Languages", "abstract": "We present the IndicNLP corpus, a large-scale, general-domain corpus\ncontaining 2.7 billion words for 10 Indian languages from two language\nfamilies. We share pre-trained word embeddings trained on these corpora. We\ncreate news article category classification datasets for 9 languages to\nevaluate the embeddings. We show that the IndicNLP embeddings significantly\noutperform publicly available pre-trained embedding on multiple evaluation\ntasks. We hope that the availability of the corpus will accelerate Indic NLP\nresearch. The resources are available at\nhttps://github.com/ai4bharat-indicnlp/indicnlp_corpus.", "published": "2020-04-30 20:21:02", "link": "http://arxiv.org/abs/2005.00085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Unsupervised Relation Extraction", "abstract": "Unsupervised relation extraction (URE) extracts relations between named\nentities from raw text without manually-labelled data and existing knowledge\nbases (KBs). URE methods can be categorised into generative and discriminative\napproaches, which rely either on hand-crafted features or surface form.\nHowever, we demonstrate that by using only named entities to induce relation\ntypes, we can outperform existing methods on two popular datasets. We conduct a\ncomparison and evaluation of our findings with other URE techniques, to\nascertain the important features in URE. We conclude that entity types provide\na strong inductive bias for URE.", "published": "2020-04-30 20:22:45", "link": "http://arxiv.org/abs/2005.00087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Neural Coreference Resolution in Constant Memory", "abstract": "We investigate modeling coreference resolution under a fixed memory\nconstraint by extending an incremental clustering algorithm to utilize\ncontextualized encoders and neural components. Given a new sentence, our\nend-to-end algorithm proposes and scores each mention span against explicit\nentity representations created from the earlier document context (if any).\nThese spans are then used to update the entity's representations before being\nforgotten; we only retain a fixed set of salient entities throughout the\ndocument. In this work, we successfully convert a high-performing model (Joshi\net al., 2020), asymptotically reducing its memory usage to constant space with\nonly a 0.3% relative loss in F1 on OntoNotes 5.0.", "published": "2020-04-30 22:31:20", "link": "http://arxiv.org/abs/2005.00128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Pre-trained Transformers for Arabic Information\n  Extraction", "abstract": "Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019)\nand XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable the\neffective cross-lingual zero-shot transfer. However, their performance on\nArabic information extraction (IE) tasks is not very well studied. In this\npaper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is\ndesigned specifically for Arabic NLP and English-to-Arabic zero-shot transfer\nlearning. We study GigaBERT's effectiveness on zero-short transfer across four\nIE tasks: named entity recognition, part-of-speech tagging, argument role\nlabeling, and relation extraction. Our best model significantly outperforms\nmBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised\nand zero-shot transfer settings. We have made our pre-trained models publicly\navailable at https://github.com/lanwuwei/GigaBERT.", "published": "2020-04-30 00:01:08", "link": "http://arxiv.org/abs/2004.14519v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Sentence Order in Document Alignment", "abstract": "We present a simple document alignment method that incorporates sentence\norder information in both candidate generation and candidate re-scoring. Our\nmethod results in 61% relative reduction in error compared to the best\npreviously published result on the WMT16 document alignment shared task. Our\nmethod improves downstream MT performance on web-scraped Sinhala--English\ndocuments from ParaCrawl, outperforming the document alignment method used in\nthe most recent ParaCrawl release. It also outperforms a comparable corpora\nmethod which uses the same multilingual embeddings, demonstrating that\nexploiting sentence order is beneficial even if the end goal is sentence-level\nbitext.", "published": "2020-04-30 00:11:34", "link": "http://arxiv.org/abs/2004.14523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulated Multiple Reference Training Improves Low-Resource Machine\n  Translation", "abstract": "Many valid translations exist for a given sentence, yet machine translation\n(MT) is trained with a single reference translation, exacerbating data sparsity\nin low-resource settings. We introduce Simulated Multiple Reference Training\n(SMRT), a novel MT training method that approximates the full space of possible\ntranslations by sampling a paraphrase of the reference sentence from a\nparaphraser and training the MT model to predict the paraphraser's distribution\nover possible tokens. We demonstrate the effectiveness of SMRT in low-resource\nsettings when translating to English, with improvements of 1.2 to 7.0 BLEU. We\nalso find SMRT is complementary to back-translation.", "published": "2020-04-30 00:11:53", "link": "http://arxiv.org/abs/2004.14524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stay Hungry, Stay Focused: Generating Informative and Specific Questions\n  in Information-Seeking Conversations", "abstract": "We investigate the problem of generating informative questions in\ninformation-asymmetric conversations. Unlike previous work on question\ngeneration which largely assumes knowledge of what the answer might be, we are\ninterested in the scenario where the questioner is not given the context from\nwhich answers are drawn, but must reason pragmatically about how to acquire new\ninformation, given the shared conversation history. We identify two core\nchallenges: (1) formally defining the informativeness of potential questions,\nand (2) exploring the prohibitively large space of potential questions to find\nthe good candidates. To generate pragmatic questions, we use reinforcement\nlearning to optimize an informativeness metric we propose, combined with a\nreward function designed to promote more specific questions. We demonstrate\nthat the resulting pragmatic questioner substantially improves the\ninformativeness and specificity of questions generated over a baseline model,\nas evaluated by our metrics as well as humans.", "published": "2020-04-30 00:49:14", "link": "http://arxiv.org/abs/2004.14530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Encoders for Modeling and Interpreting Screenplays", "abstract": "While natural language understanding of long-form documents is still an open\nchallenge, such documents often contain structural information that can inform\nthe design of models for encoding them. Movie scripts are an example of such\nrichly structured text - scripts are segmented into scenes, which are further\ndecomposed into dialogue and descriptive components. In this work, we propose a\nneural architecture for encoding this structure, which performs robustly on a\npair of multi-label tag classification datasets, without the need for\nhandcrafted features. We add a layer of insight by augmenting an unsupervised\n\"interpretability\" module to the encoder, allowing for the extraction and\nvisualization of narrative trajectories. Though this work specifically tackles\nscreenplays, we discuss how the underlying approach can be generalized to a\nrange of structured documents.", "published": "2020-04-30 01:15:40", "link": "http://arxiv.org/abs/2004.14532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Segmentation by Cross Segment Attention", "abstract": "Document and discourse segmentation are two fundamental NLP tasks pertaining\nto breaking up text into constituents, which are commonly used to help\ndownstream tasks such as information retrieval or text summarization. In this\nwork, we propose three transformer-based architectures and provide\ncomprehensive comparisons with previously proposed approaches on three standard\ndatasets. We establish a new state-of-the-art, reducing in particular the error\nrates by a large margin in all cases. We further analyze model sizes and find\nthat we can build models with many fewer parameters while keeping good\nperformance, thus facilitating real-world applications.", "published": "2020-04-30 01:36:52", "link": "http://arxiv.org/abs/2004.14535v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAVAT: Token-Aware Virtual Adversarial Training for Language\n  Understanding", "abstract": "Gradient-based adversarial training is widely used in improving the\nrobustness of neural networks, while it cannot be easily adapted to natural\nlanguage processing tasks since the embedding space is discrete. In natural\nlanguage processing fields, virtual adversarial training is introduced since\ntexts are discrete and cannot be perturbed by gradients directly.\nAlternatively, virtual adversarial training, which generates perturbations on\nthe embedding space, is introduced in NLP tasks. Despite its success, existing\nvirtual adversarial training methods generate perturbations roughly constrained\nby Frobenius normalization balls. To craft fine-grained perturbations, we\npropose a Token-Aware Virtual Adversarial Training method. We introduce a\ntoken-level accumulated perturbation vocabulary to initialize the perturbations\nbetter and use a token-level normalization ball to constrain these\nperturbations pertinently. Experiments show that our method improves the\nperformance of pre-trained models such as BERT and ALBERT in various tasks by a\nconsiderable margin. The proposed method improves the score of the GLUE\nbenchmark from 78.3 to 80.9 using BERT model and it also enhances the\nperformance of sequence labeling and text classification tasks.", "published": "2020-04-30 02:03:24", "link": "http://arxiv.org/abs/2004.14543v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Filtering before Iteratively Referring for Knowledge-Grounded Response\n  Selection in Retrieval-Based Chatbots", "abstract": "The challenges of building knowledge-grounded retrieval-based chatbots lie in\nhow to ground a conversation on its background knowledge and how to match\nresponse candidates with both context and knowledge simultaneously. This paper\nproposes a method named Filtering before Iteratively REferring (FIRE) for this\ntask. In this method, a context filter and a knowledge filter are first built,\nwhich derive knowledge-aware context representations and context-aware\nknowledge representations respectively by global and bidirectional attention.\nBesides, the entries irrelevant to the conversation are discarded by the\nknowledge filter. After that, iteratively referring is performed between\ncontext and response representations as well as between knowledge and response\nrepresentations, in order to collect deep matching features for scoring\nresponse candidates. Experimental results show that FIRE outperforms previous\nmethods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with\noriginal and revised personas respectively, and margins larger than 3.1% on the\nCMU_DoG dataset in terms of top-1 accuracy. We also show that FIRE is more\ninterpretable by visualizing the knowledge grounding process.", "published": "2020-04-30 02:27:12", "link": "http://arxiv.org/abs/2004.14550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User-Guided Aspect Classification for Domain-Specific Texts", "abstract": "Aspect classification, identifying aspects of text segments, facilitates\nnumerous applications, such as sentiment analysis and review summarization. To\nalleviate the human effort on annotating massive texts, in this paper, we study\nthe problem of classifying aspects based on only a few user-provided seed words\nfor pre-defined aspects. The major challenge lies in how to handle the noisy\nmisc aspect, which is designed for texts without any pre-defined aspects. Even\ndomain experts have difficulties to nominate seed words for the misc aspect,\nmaking existing seed-driven text classification methods not applicable. We\npropose a novel framework, ARYA, which enables mutual enhancements between\npre-defined aspects and the misc aspect via iterative classifier training and\nseed updating. Specifically, it trains a classifier for pre-defined aspects and\nthen leverages it to induce the supervision for the misc aspect. The prediction\nresults of the misc aspect are later utilized to filter out noisy seed words\nfor pre-defined aspects. Experiments in two domains demonstrate the superior\nperformance of our proposed framework, as well as the necessity and importance\nof properly modeling the misc aspect.", "published": "2020-04-30 03:14:16", "link": "http://arxiv.org/abs/2004.14555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RikiNet: Reading Wikipedia Pages for Natural Question Answering", "abstract": "Reading long documents to answer open-domain questions remains challenging in\nnatural language understanding. In this paper, we introduce a new model, called\nRikiNet, which reads Wikipedia pages for natural question answering. RikiNet\ncontains a dynamic paragraph dual-attention reader and a multi-level cascaded\nanswer predictor. The reader dynamically represents the document and question\nby utilizing a set of complementary attention mechanisms. The representations\nare then fed into the predictor to obtain the span of the short answer, the\nparagraph of the long answer, and the answer type in a cascaded manner. On the\nNatural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1\non long-answer and short-answer tasks. To our best knowledge, it is the first\nsingle model that outperforms the single human performance. Furthermore, an\nensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer\ntasks, achieving the best performance on the official NQ leaderboard", "published": "2020-04-30 03:29:21", "link": "http://arxiv.org/abs/2004.14560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Machine Translation Evaluation in Many Languages via Zero-Shot\n  Paraphrasing", "abstract": "We frame the task of machine translation evaluation as one of scoring machine\ntranslation output with a sequence-to-sequence paraphraser, conditioned on a\nhuman reference. We propose training the paraphraser as a multilingual NMT\nsystem, treating paraphrasing as a zero-shot translation task (e.g., Czech to\nCzech). This results in the paraphraser's output mode being centered around a\ncopy of the input sequence, which represents the best case scenario where the\nMT system output matches a human reference. Our method is simple and intuitive,\nand does not require human judgements for training. Our single model (trained\nin 39 languages) outperforms or statistically ties with all prior metrics on\nthe WMT 2019 segment-level shared metrics task in all languages (excluding\nGujarati where the model had no training data). We also explore using our model\nfor the task of quality estimation as a metric--conditioning on the source\ninstead of the reference--and find that it significantly outperforms every\nsubmission to the WMT 2019 shared task on quality estimation in every language\npair.", "published": "2020-04-30 03:32:34", "link": "http://arxiv.org/abs/2004.14564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Naturalness of Language in Task-oriented Dialogues via\n  Adversarial Training", "abstract": "The natural language generation (NLG) module in a task-oriented dialogue\nsystem produces user-facing utterances conveying required information. Thus, it\nis critical for the generated response to be natural and fluent. We propose to\nintegrate adversarial training to produce more human-like responses. The model\nuses Straight-Through Gumbel-Softmax estimator for gradient computation. We\nalso propose a two-stage training scheme to boost performance. Empirical\nresults show that the adversarial training can effectively improve the quality\nof language generation in both automatic and human evaluations. For example, in\nthe RNN-LG Restaurant dataset, our model AdvNLG outperforms the previous\nstate-of-the-art result by 3.6% in BLEU.", "published": "2020-04-30 03:35:20", "link": "http://arxiv.org/abs/2004.14565v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "memeBot: Towards Automatic Image Meme Generation", "abstract": "Image memes have become a widespread tool used by people for interacting and\nexchanging ideas over social media, blogs, and open messengers. This work\nproposes to treat automatic image meme generation as a translation process, and\nfurther present an end to end neural and probabilistic approach to generate an\nimage-based meme for any given sentence using an encoder-decoder architecture.\nFor a given input sentence, an image meme is generated by combining a meme\ntemplate image and a text caption where the meme template image is selected\nfrom a set of popular candidates using a selection module, and the meme caption\nis generated by an encoder-decoder model. An encoder is used to map the\nselected meme template and the input sentence into a meme embedding and a\ndecoder is used to decode the meme caption from the meme embedding. The\ngenerated natural language meme caption is conditioned on the input sentence\nand the selected meme template. The model learns the dependencies between the\nmeme captions and the meme template images and generates new memes using the\nlearned dependencies. The quality of the generated captions and the generated\nmemes is evaluated through both automated and human evaluation. An experiment\nis designed to score how well the generated memes can represent the tweets from\nTwitter conversations. Experiments on Twitter data show the efficacy of the\nmodel in generating memes for sentences in online social interaction.", "published": "2020-04-30 03:48:14", "link": "http://arxiv.org/abs/2004.14571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Contextualized Neural Language Models for Temporal Dependency\n  Parsing", "abstract": "Extracting temporal relations between events and time expressions has many\napplications such as constructing event timelines and time-related question\nanswering. It is a challenging problem which requires syntactic and semantic\ninformation at sentence or discourse levels, which may be captured by deep\ncontextualized language models (LMs) such as BERT (Devlin et al., 2019). In\nthis paper, we develop several variants of BERT-based temporal dependency\nparser, and show that BERT significantly improves temporal dependency parsing\n(Zhang and Xue, 2018a). We also present a detailed analysis on why deep\ncontextualized neural LMs help and where they may fall short. Source code and\nresources are made available at https://github.com/bnmin/tdp_ranking.", "published": "2020-04-30 03:59:13", "link": "http://arxiv.org/abs/2004.14577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic2Text: High-Fidelity Natural Language Generation from Logical Forms", "abstract": "Previous works on Natural Language Generation (NLG) from structured data have\nprimarily focused on surface-level descriptions of record sequences. However,\nfor complex structured data, e.g., multi-row tables, it is often desirable for\nan NLG system to describe interesting facts from logical inferences across\nrecords. If only provided with the table, it is hard for existing models to\nproduce controllable and high-fidelity logical generations. In this work, we\nformulate logical level NLG as generation from logical forms in order to obtain\ncontrollable, high-fidelity, and faithful generations. We present a new\nlarge-scale dataset, \\textsc{Logic2Text}, with 10,753 descriptions involving\ncommon logic types paired with the underlying logical forms. The logical forms\nshow diversified graph structure of free schema, which poses great challenges\non the model's ability to understand the semantics. We experiment on (1)\nFully-supervised training with the full datasets, and (2) Few-shot setting,\nprovided with hundreds of paired examples; We compare several popular\ngeneration models and analyze their performances. We hope our dataset can\nencourage research towards building an advanced NLG system capable of natural,\nfaithful, and human-like generation. The dataset and code are available at\nhttps://github.com/czyssrs/Logic2Text.", "published": "2020-04-30 04:06:06", "link": "http://arxiv.org/abs/2004.14579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Music Helps You Read: Using Transfer to Study Linguistic\n  Structure in Language Models", "abstract": "We propose transfer learning as a method for analyzing the encoding of\ngrammatical structure in neural language models. We train LSTMs on\nnon-linguistic data and evaluate their performance on natural language to\nassess which kinds of data induce generalizable structural features that LSTMs\ncan use for natural language. We find that training on non-linguistic data with\nlatent structure (MIDI music or Java code) improves test performance on natural\nlanguage, despite no overlap in surface form or vocabulary. To pinpoint the\nkinds of abstract structure that models may be encoding to lead to this\nimprovement, we run similar experiments with two artificial parentheses\nlanguages: one which has a hierarchical recursive structure, and a control\nwhich has paired tokens but no recursion. Surprisingly, training a model on\neither of these artificial languages leads to the same substantial gains when\ntesting on natural language. Further experiments on transfer between natural\nlanguages controlling for vocabulary overlap show that zero-shot performance on\na test language is highly correlated with typological syntactic similarity to\nthe training language, suggesting that representations induced by pre-training\ncorrespond to the cross-linguistic syntactic properties. Our results provide\ninsights into the ways that neural models represent abstract syntactic\nstructure, and also about the kind of structural inductive biases which allow\nfor natural language acquisition.", "published": "2020-04-30 06:24:03", "link": "http://arxiv.org/abs/2004.14601v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Look at the First Sentence: Position Bias in Question Answering", "abstract": "Many extractive question answering models are trained to predict start and\nend positions of answers. The choice of predicting answers as positions is\nmainly due to its simplicity and effectiveness. In this study, we hypothesize\nthat when the distribution of the answer positions is highly skewed in the\ntraining set (e.g., answers lie only in the k-th sentence of each passage), QA\nmodels predicting answers as positions can learn spurious positional cues and\nfail to give answers in different positions. We first illustrate this position\nbias in popular extractive QA models such as BiDAF and BERT and thoroughly\nexamine how position bias propagates through each layer of BERT. To safely\ndeliver position information without position bias, we train models with\nvarious de-biasing methods including entropy regularization and bias\nensembling. Among them, we found that using the prior distribution of answer\npositions as a bias model is very effective at reducing position bias,\nrecovering the performance of BERT from 37.48% to 81.64% when trained on a\nbiased SQuAD dataset.", "published": "2020-04-30 06:25:16", "link": "http://arxiv.org/abs/2004.14602v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Your Context-Aware MT System Pass the DiP Benchmark Tests? :\n  Evaluation Benchmarks for Discourse Phenomena in Machine Translation", "abstract": "Despite increasing instances of machine translation (MT) systems including\ncontextual information, the evidence for translation quality improvement is\nsparse, especially for discourse phenomena. Popular metrics like BLEU are not\nexpressive or sensitive enough to capture quality improvements or drops that\nare minor in size but significant in perception. We introduce the first of\ntheir kind MT benchmark datasets that aim to track and hail improvements across\nfour main discourse phenomena: anaphora, lexical consistency, coherence and\nreadability, and discourse connective translation. We also introduce evaluation\nmethods for these tasks, and evaluate several baseline MT systems on the\ncurated datasets. Surprisingly, we find that existing context-aware models do\nnot improve discourse-related translations consistently across languages and\nphenomena.", "published": "2020-04-30 07:15:36", "link": "http://arxiv.org/abs/2004.14607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Injection into Dialogue Generation via Language Models", "abstract": "Dialogue generation has been successfully learned from scratch by neural\nnetworks, but tends to produce the same general response, e.g., \"what are you\ntalking about?\", in many conversations. To reduce this homogeneity, external\nknowledge such as the speaker's profile and domain knowledge is applied as an\nadditional condition to diversify a model's output. The required knowledge to\ndevelop an effective conversation, however, is not always available, which is\ndifferent from prior work's assumption that a model always has acquired\nsufficient knowledge before chatting. This problem can be detrimental when\napplying a dialogue model like this chatting online with unconstrained people\nand topics, because the model does not have the needed knowledge. To address\nthis problem, we propose InjK, which is a two-stage approach to inject\nknowledge into a dialogue generation model. First, we train a large-scale\nlanguage model and query it as textual knowledge. Second, we frame a dialogue\ngeneration model to sequentially generate textual knowledge and a corresponding\nresponse. Empirically, when a dialogue generation model can only access limited\nknowledge, our method outperforms prior work by producing more coherent and\ninformative responses.", "published": "2020-04-30 07:31:24", "link": "http://arxiv.org/abs/2004.14614v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Dependencies according to BERT: both more specific and more\n  general", "abstract": "This work focuses on analyzing the form and extent of syntactic abstraction\ncaptured by BERT by extracting labeled dependency trees from self-attentions.\n  Previous work showed that individual BERT heads tend to encode particular\ndependency relation types. We extend these findings by explicitly comparing\nBERT relations to Universal Dependencies (UD) annotations, showing that they\noften do not match one-to-one.\n  We suggest a method for relation identification and syntactic tree\nconstruction. Our approach produces significantly more consistent dependency\ntrees than previous work, showing that it better explains the syntactic\nabstractions in BERT. At the same time, it can be successfully applied with\nonly a minimal amount of supervision and generalizes well across languages.", "published": "2020-04-30 07:48:07", "link": "http://arxiv.org/abs/2004.14620v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Natural Language Inference Models Partially Embed Theories of\n  Lexical Entailment and Negation", "abstract": "We address whether neural models for Natural Language Inference (NLI) can\nlearn the compositional interactions between lexical entailment and negation,\nusing four methods: the behavioral evaluation methods of (1) challenge test\nsets and (2) systematic generalization tasks, and the structural evaluation\nmethods of (3) probes and (4) interventions. To facilitate this holistic\nevaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset\nfocused on lexical entailment and negation. In our behavioral evaluations, we\nfind that models trained on general-purpose NLI datasets fail systematically on\nMoNLI examples containing negation, but that MoNLI fine-tuning addresses this\nfailure. In our structural evaluations, we look for evidence that our\ntop-performing BERT-based model has learned to implement the monotonicity\nalgorithm behind MoNLI. Probes yield evidence consistent with this conclusion,\nand our intervention experiments bolster this, showing that the causal dynamics\nof the model mirror the causal dynamics of this algorithm on subsets of MoNLI.\nThis suggests that the BERT model at least partially embeds a theory of lexical\nentailment and negation at an algorithmic level.", "published": "2020-04-30 07:53:20", "link": "http://arxiv.org/abs/2004.14623v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks", "abstract": "Although coherence modeling has come a long way in developing novel models,\ntheir evaluation on downstream applications for which they are purportedly\ndeveloped has largely been neglected. With the advancements made by neural\napproaches in applications such as machine translation (MT), summarization and\ndialog systems, the need for coherence evaluation of these tasks is now more\ncrucial than ever. However, coherence models are typically evaluated only on\nsynthetic tasks, which may not be representative of their performance in\ndownstream applications. To investigate how representative the synthetic tasks\nare of downstream use cases, we conduct experiments on benchmarking well-known\ntraditional and neural coherence models on synthetic sentence ordering tasks,\nand contrast this with their performance on three downstream applications:\ncoherence evaluation for MT and summarization, and next utterance prediction in\nretrieval-based dialog. Our results demonstrate a weak correlation between the\nmodel performances in the synthetic tasks and the downstream applications,\n{motivating alternate training and evaluation methods for coherence models.", "published": "2020-04-30 08:00:42", "link": "http://arxiv.org/abs/2004.14626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capsule-Transformer for Neural Machine Translation", "abstract": "Transformer hugely benefits from its key design of the multi-head\nself-attention network (SAN), which extracts information from various\nperspectives through transforming the given input into different subspaces.\nHowever, its simple linear transformation aggregation strategy may still\npotentially fail to fully capture deeper contextualized information. In this\npaper, we thus propose the capsule-Transformer, which extends the linear\ntransformation into a more general capsule routing algorithm by taking SAN as a\nspecial case of capsule network. So that the resulted capsule-Transformer is\ncapable of obtaining a better attention distribution representation of the\ninput sequence via information aggregation among different heads and words.\nSpecifically, we see groups of attention weights in SAN as low layer capsules.\nBy applying the iterative capsule routing algorithm they can be further\naggregated into high layer capsules which contain deeper contextualized\ninformation. Experimental results on the widely-used machine translation\ndatasets show our proposed capsule-Transformer outperforms strong Transformer\nbaseline significantly.", "published": "2020-04-30 09:11:38", "link": "http://arxiv.org/abs/2004.14649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Word Alignment Outperforms GIZA++", "abstract": "Word alignment was once a core unsupervised learning task in natural language\nprocessing because of its essential role in training statistical machine\ntranslation (MT) models. Although unnecessary for training neural MT models,\nword alignment still plays an important role in interactive applications of\nneural machine translation, such as annotation transfer and lexicon injection.\nWhile statistical MT methods have been replaced by neural approaches with\nsuperior performance, the twenty-year-old GIZA++ toolkit remains a key\ncomponent of state-of-the-art word alignment systems. Prior work on neural word\nalignment has only been able to outperform GIZA++ by using its output during\ntraining. We present the first end-to-end neural word alignment method that\nconsistently outperforms GIZA++ on three data sets. Our approach repurposes a\nTransformer model trained for supervised translation to also serve as an\nunsupervised word alignment model in a manner that is tightly integrated and\ndoes not affect translation quality.", "published": "2020-04-30 10:29:37", "link": "http://arxiv.org/abs/2004.14675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Text Simplification with Back-Translation and Asymmetric\n  Denoising Autoencoders", "abstract": "Text simplification (TS) rephrases long sentences into simplified variants\nwhile preserving inherent semantics. Traditional sequence-to-sequence models\nheavily rely on the quantity and quality of parallel sentences, which limits\ntheir applicability in different languages and domains. This work investigates\nhow to leverage large amounts of unpaired corpora in TS task. We adopt the\nback-translation architecture in unsupervised machine translation (NMT),\nincluding denoising autoencoders for language modeling and automatic generation\nof parallel data by iterative back-translation. However, it is non-trivial to\ngenerate appropriate complex-simple pair if we directly treat the set of simple\nand complex corpora as two different languages, since the two types of\nsentences are quite similar and it is hard for the model to capture the\ncharacteristics in different types of sentences. To tackle this problem, we\npropose asymmetric denoising methods for sentences with separate complexity.\nWhen modeling simple and complex sentences with autoencoders, we introduce\ndifferent types of noise into the training process. Such a method can\nsignificantly improve the simplification performance. Our model can be trained\nin both unsupervised and semi-supervised manner. Automatic and human\nevaluations show that our unsupervised model outperforms the previous systems,\nand with limited supervision, our model can perform competitively with multiple\nstate-of-the-art simplification systems.", "published": "2020-04-30 11:19:04", "link": "http://arxiv.org/abs/2004.14693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Unsupervised Language Understanding and Generation by Joint Dual\n  Learning", "abstract": "In modular dialogue systems, natural language understanding (NLU) and natural\nlanguage generation (NLG) are two critical components, where NLU extracts the\nsemantics from the given texts and NLG is to construct corresponding natural\nlanguage sentences based on the input semantic representations. However, the\ndual property between understanding and generation has been rarely explored.\nThe prior work is the first attempt that utilized the duality between NLU and\nNLG to improve the performance via a dual supervised learning framework.\nHowever, the prior work still learned both components in a supervised manner,\ninstead, this paper introduces a general learning framework to effectively\nexploit such duality, providing flexibility of incorporating both supervised\nand unsupervised learning algorithms to train language understanding and\ngeneration models in a joint fashion. The benchmark experiments demonstrate\nthat the proposed approach is capable of boosting the performance of both NLU\nand NLG.", "published": "2020-04-30 12:02:33", "link": "http://arxiv.org/abs/2004.14710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Augmentation for Aspect Term Extraction via Masked\n  Sequence-to-Sequence Generation", "abstract": "Aspect term extraction aims to extract aspect terms from review texts as\nopinion targets for sentiment analysis. One of the big challenges with this\ntask is the lack of sufficient annotated data. While data augmentation is\npotentially an effective technique to address the above issue, it is\nuncontrollable as it may change aspect words and aspect labels unexpectedly. In\nthis paper, we formulate the data augmentation as a conditional generation\ntask: generating a new sentence while preserving the original opinion targets\nand labels. We propose a masked sequence-to-sequence method for conditional\naugmentation of aspect term extraction. Unlike existing augmentation\napproaches, ours is controllable and allows us to generate more diversified\nsentences. Experimental results confirm that our method alleviates the data\nscarcity problem significantly. It also effectively boosts the performances of\nseveral current models for aspect term extraction.", "published": "2020-04-30 13:34:04", "link": "http://arxiv.org/abs/2004.14769v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure-Augmented Text Representation Learning for Efficient Knowledge\n  Graph Completion", "abstract": "Human-curated knowledge graphs provide critical supportive information to\nvarious natural language processing tasks, but these graphs are usually\nincomplete, urging auto-completion of them. Prevalent graph embedding\napproaches, e.g., TransE, learn structured knowledge via representing graph\nelements into dense embeddings and capturing their triple-level relationship\nwith spatial distance. However, they are hardly generalizable to the elements\nnever visited in training and are intrinsically vulnerable to graph\nincompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort\nto graph triple's text and triple-level contextualized representations. They\nare generalizable enough and robust to the incompleteness, especially when\ncoupled with pre-trained encoders. But two major drawbacks limit the\nperformance: (1) high overheads due to the costly scoring of all possible\ntriples in inference, and (2) a lack of structured knowledge in the textual\nencoder. In this paper, we follow the textual encoding paradigm and aim to\nalleviate its drawbacks by augmenting it with graph embedding techniques -- a\ncomplementary hybrid of both paradigms. Specifically, we partition each triple\ninto two asymmetric parts as in translation-based graph embedding approach, and\nencode both parts into contextualized representations by a Siamese-style\ntextual encoder. Built upon the representations, our model employs both\ndeterministic classifier and spatial measurement for representation and\nstructure learning respectively. Moreover, we develop a self-adaptive ensemble\nscheme to further improve the performance by incorporating triple scores from\nan existing graph embedding model. In experiments, we achieve state-of-the-art\nperformance on three benchmarks and a zero-shot dataset for link prediction,\nwith highlights of inference costs reduced by 1-2 orders of magnitude compared\nto a textual encoding method.", "published": "2020-04-30 13:50:34", "link": "http://arxiv.org/abs/2004.14781v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting\n  BERT", "abstract": "By introducing a small set of additional parameters, a probe learns to solve\nspecific linguistic tasks (e.g., dependency parsing) in a supervised manner\nusing feature representations (e.g., contextualized embeddings). The\neffectiveness of such probing tasks is taken as evidence that the pre-trained\nmodel encodes linguistic knowledge. However, this approach of evaluating a\nlanguage model is undermined by the uncertainty of the amount of knowledge that\nis learned by the probe itself. Complementary to those works, we propose a\nparameter-free probing technique for analyzing pre-trained language models\n(e.g., BERT). Our method does not require direct supervision from the probing\ntasks, nor do we introduce additional parameters to the probing process. Our\nexperiments on BERT show that syntactic trees recovered from BERT using our\nmethod are significantly better than linguistically-uninformed baselines. We\nfurther feed the empirically induced dependency structures into a downstream\nsentiment classification task and find its improvement compatible with or even\nsuperior to a human-designed dependency schema.", "published": "2020-04-30 14:02:29", "link": "http://arxiv.org/abs/2004.14786v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Level Translation with Self-attention", "abstract": "We explore the suitability of self-attention models for character-level\nneural machine translation. We test the standard transformer model, as well as\na novel variant in which the encoder block combines information from nearby\ncharacters using convolutions. We perform extensive experiments on WMT and UN\ndatasets, testing both bilingual and multilingual translation to English using\nup to three input languages (French, Spanish, and Chinese). Our transformer\nvariant consistently outperforms the standard transformer at the\ncharacter-level and converges faster while learning more robust character-level\nalignments.", "published": "2020-04-30 14:05:26", "link": "http://arxiv.org/abs/2004.14788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STARC: Structured Annotations for Reading Comprehension", "abstract": "We present STARC (Structured Annotations for Reading Comprehension), a new\nannotation framework for assessing reading comprehension with multiple choice\nquestions. Our framework introduces a principled structure for the answer\nchoices and ties them to textual span annotations. The framework is implemented\nin OneStopQA, a new high-quality dataset for evaluation and analysis of reading\ncomprehension in English. We use this dataset to demonstrate that STARC can be\nleveraged for a key new application for the development of SAT-like reading\ncomprehension materials: automatic annotation quality probing via span ablation\nexperiments. We further show that it enables in-depth analyses and comparisons\nbetween machine and human reading comprehension behavior, including error\ndistributions and guessing ability. Our experiments also reveal that the\nstandard multiple choice dataset in NLP, RACE, is limited in its ability to\nmeasure reading comprehension. 47% of its questions can be guessed by machines\nwithout accessing the passage, and 18% are unanimously judged by humans as not\nhaving a unique correct answer. OneStopQA provides an alternative test set for\nreading comprehension which alleviates these shortcomings and has a\nsubstantially higher human ceiling performance.", "published": "2020-04-30 14:08:50", "link": "http://arxiv.org/abs/2004.14797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ENT-DESC: Entity Description Generation by Exploring Knowledge Graph", "abstract": "Previous works on knowledge-to-text generation take as input a few RDF\ntriples or key-value pairs conveying the knowledge of some entities to generate\na natural language description. Existing datasets, such as WIKIBIO, WebNLG, and\nE2E, basically have a good alignment between an input triple/pair set and its\noutput text. However, in practice, the input knowledge could be more than\nenough, since the output description may only cover the most significant\nknowledge. In this paper, we introduce a large-scale and challenging dataset to\nfacilitate the study of such a practical scenario in KG-to-text. Our dataset\ninvolves retrieving abundant knowledge of various types of main entities from a\nlarge knowledge graph (KG), which makes the current graph-to-sequence models\nseverely suffer from the problems of information loss and parameter explosion\nwhile generating the descriptions. We address these challenges by proposing a\nmulti-graph structure that is able to represent the original graph information\nmore comprehensively. Furthermore, we also incorporate aggregation methods that\nlearn to extract the rich graph information. Extensive experiments demonstrate\nthe effectiveness of our model architecture.", "published": "2020-04-30 14:16:19", "link": "http://arxiv.org/abs/2004.14813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocabulary Adaptation for Distant Domain Adaptation in Neural Machine\n  Translation", "abstract": "Neural network methods exhibit strong performance only in a few resource-rich\ndomains. Practitioners, therefore, employ domain adaptation from resource-rich\ndomains that are, in most cases, distant from the target domain. Domain\nadaptation between distant domains (e.g., movie subtitles and research papers),\nhowever, cannot be performed effectively due to mismatches in vocabulary; it\nwill encounter many domain-specific words (e.g., \"angstrom\") and words whose\nmeanings shift across domains(e.g., \"conductor\"). In this study, aiming to\nsolve these vocabulary mismatches in domain adaptation for neural machine\ntranslation (NMT), we propose vocabulary adaptation, a simple method for\neffective fine-tuning that adapts embedding layers in a given pre-trained NMT\nmodel to the target domain. Prior to fine-tuning, our method replaces the\nembedding layers of the NMT model by projecting general word embeddings induced\nfrom monolingual data in a target domain onto a source-domain embedding space.\nExperimental results indicate that our method improves the performance of\nconventional fine-tuning by 3.86 and 3.28 BLEU points in En-Ja and De-En\ntranslation, respectively.", "published": "2020-04-30 14:27:59", "link": "http://arxiv.org/abs/2004.14821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriched Pre-trained Transformers for Joint Slot Filling and Intent\n  Detection", "abstract": "Detecting the user's intent and finding the corresponding slots among the\nutterance's words are important tasks in natural language understanding. Their\ninterconnected nature makes their joint modeling a standard part of training\nsuch models. Moreover, data scarceness and specialized vocabularies pose\nadditional challenges. Recently, the advances in pre-trained language models,\nnamely contextualized models such as ELMo and BERT have revolutionized the\nfield by tapping the potential of training very large models with just a few\nsteps of fine-tuning on a task-specific dataset. Here, we leverage such models,\nnamely BERT and RoBERTa, and we design a novel architecture on top of them.\nMoreover, we propose an intent pooling attention mechanism, and we reinforce\nthe slot filling task by fusing intent distributions, word features, and token\nrepresentations. The experimental results on standard datasets show that our\nmodel outperforms both the current non-BERT state of the art as well as some\nstronger BERT-based baselines.", "published": "2020-04-30 15:00:21", "link": "http://arxiv.org/abs/2004.14848v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation\n  Extraction Task", "abstract": "TACRED (Zhang et al., 2017) is one of the largest, most widely used\ncrowdsourced datasets in Relation Extraction (RE). But, even with recent\nadvances in unsupervised pre-training and knowledge enhanced neural RE, models\nstill show a high error rate. In this paper, we investigate the questions: Have\nwe reached a performance ceiling or is there still room for improvement? And\nhow do crowd annotations, dataset, and models contribute to this error rate? To\nanswer these questions, we first validate the most challenging 5K examples in\nthe development and test sets using trained annotators. We find that label\nerrors account for 8% absolute F1 test error, and that more than 50% of the\nexamples need to be relabeled. On the relabeled test set the average F1 score\nof a large baseline model set improves from 62.1 to 70.1. After validation, we\nanalyze misclassifications on the challenging instances, categorize them into\nlinguistically motivated error groups, and verify the resulting error\nhypotheses on three state-of-the-art RE models. We show that two groups of\nambiguous relations are responsible for most of the remaining errors and that\nmodels may adopt shallow heuristics on the dataset when entities are not\nmasked.", "published": "2020-04-30 15:07:37", "link": "http://arxiv.org/abs/2004.14855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Spoken Language Understanding Using Domain- and Task-Aware\n  Parameterization", "abstract": "Spoken language understanding has been addressed as a supervised learning\nproblem, where a set of training data is available for each domain. However,\nannotating data for each domain is both financially costly and non-scalable so\nwe should fully utilize information across all domains. One existing approach\nsolves the problem by conducting multi-domain learning, using shared parameters\nfor joint training across domains. We propose to improve the parameterization\nof this method by using domain-specific and task-specific model parameters to\nimprove knowledge learning and transfer. Experiments on 5 domains show that our\nmodel is more effective for multi-domain SLU and obtain the best results. In\naddition, we show its transferability by outperforming the prior best model by\n12.4\\% when adapting to a new domain with little data.", "published": "2020-04-30 15:15:40", "link": "http://arxiv.org/abs/2004.14871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Surprising Variability in Word Embedding Stability Across\n  Languages", "abstract": "Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures, both in English and in other\nlanguages. To gain further insight into word embeddings, we explore their\nstability (e.g., overlap between the nearest neighbors of a word in different\nembedding spaces) in diverse languages. We discuss linguistic properties that\nare related to stability, drawing out insights about correlations with\naffixing, language gender systems, and other features. This has implications\nfor embedding use, particularly in research that uses them to study language\ntrends.", "published": "2020-04-30 15:24:43", "link": "http://arxiv.org/abs/2004.14876v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLSUM: The Multilingual Summarization Corpus", "abstract": "We present MLSUM, the first large-scale MultiLingual SUMmarization dataset.\nObtained from online newspapers, it contains 1.5M+ article/summary pairs in\nfive different languages -- namely, French, German, Spanish, Russian, Turkish.\nTogether with English newspapers from the popular CNN/Daily mail dataset, the\ncollected data form a large scale multilingual dataset which can enable new\nresearch directions for the text summarization community. We report\ncross-lingual comparative analyses based on state-of-the-art systems. These\nhighlight existing biases which motivate the use of a multi-lingual dataset.", "published": "2020-04-30 15:58:34", "link": "http://arxiv.org/abs/2004.14900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You are right. I am ALARMED -- But by Climate Change Counter Movement", "abstract": "The world is facing the challenge of climate crisis. Despite the consensus in\nscientific community about anthropogenic global warming, the web is flooded\nwith articles spreading climate misinformation. These articles are carefully\nconstructed by climate change counter movement (cccm) organizations to\ninfluence the narrative around climate change. We revisit the literature on\nclimate misinformation in social sciences and repackage it to introduce in the\ncommunity of NLP. Despite considerable work in detection of fake news, there is\nno misinformation dataset available that is specific to the domain.of climate\nchange. We try to bridge this gap by scraping and releasing articles with known\nclimate change misinformation.", "published": "2020-04-30 16:06:02", "link": "http://arxiv.org/abs/2004.14907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recipes for Adapting Pre-trained Monolingual and Multilingual Models to\n  Machine Translation", "abstract": "There has been recent success in pre-training on monolingual data and\nfine-tuning on Machine Translation (MT), but it remains unclear how to best\nleverage a pre-trained model for a given MT task. This paper investigates the\nbenefits and drawbacks of freezing parameters, and adding new ones, when\nfine-tuning a pre-trained model on MT. We focus on 1) Fine-tuning a model\ntrained only on English monolingual data, BART. 2) Fine-tuning a model trained\non monolingual data from 25 languages, mBART. For BART we get the best\nperformance by freezing most of the model parameters, and adding extra\npositional embeddings. For mBART we match or outperform the performance of\nnaive fine-tuning for most language pairs with the encoder, and most of the\ndecoder, frozen. The encoder-decoder attention parameters are most important to\nfine-tune. When constraining ourselves to an out-of-domain training set for\nVietnamese to English we see the largest improvements over the fine-tuning\nbaseline.", "published": "2020-04-30 16:09:22", "link": "http://arxiv.org/abs/2004.14911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for\n  Fast and Good Topics too!", "abstract": "Topic models are a useful analysis tool to uncover the underlying themes\nwithin document collections. The dominant approach is to use probabilistic\ntopic models that posit a generative story, but in this paper we propose an\nalternative way to obtain topics: clustering pre-trained word embeddings while\nincorporating document information for weighted clustering and reranking top\nwords. We provide benchmarks for the combination of different word embeddings\nand clustering algorithms, and analyse their performance under dimensionality\nreduction with PCA. The best performing combination for our approach performs\nas well as classical topic models, but with lower runtime and computational\ncomplexity.", "published": "2020-04-30 16:18:18", "link": "http://arxiv.org/abs/2004.14914v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Linguistic Typology and Multilingual Machine Translation with\n  Multi-View Language Representations", "abstract": "Sparse language vectors from linguistic typology databases and learned\nembeddings from tasks like multilingual machine translation have been\ninvestigated in isolation, without analysing how they could benefit from each\nother's language characterisation. We propose to fuse both views using singular\nvector canonical correlation analysis and study what kind of information is\ninduced from each source. By inferring typological features and language\nphylogenies, we observe that our representations embed typology and strengthen\ncorrelations with language relationships. We then take advantage of our\nmulti-view language vector space for multilingual machine translation, where we\nachieve competitive overall translation accuracy in tasks that require\ninformation about language similarities, such as language clustering and\nranking candidates for multilingual transfer. With our method, which is also\nreleased as a tool, we can easily project and assess new languages without\nexpensive retraining of massive multilingual or ranking models, which are major\ndisadvantages of related approaches.", "published": "2020-04-30 16:25:39", "link": "http://arxiv.org/abs/2004.14923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Zero-Resource Domains Using Document-Level Context in Neural\n  Machine Translation", "abstract": "Achieving satisfying performance in machine translation on domains for which\nthere is no training data is challenging. Traditional supervised domain\nadaptation is not suitable for addressing such zero-resource domains because it\nrelies on in-domain parallel data. We show that when in-domain parallel data is\nnot available, access to document-level context enables better capturing of\ndomain generalities compared to only having access to a single sentence. Having\naccess to more information provides a more reliable domain estimation. We\npresent two document-level Transformer models which are capable of using large\ncontext sizes and we compare these models against strong Transformer baselines.\nWe obtain improvements for the two zero resource domains we study. We\nadditionally provide an analysis where we vary the amount of context and look\nat the case where in-domain data is available.", "published": "2020-04-30 16:28:19", "link": "http://arxiv.org/abs/2004.14927v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Prior for Low-Resource Neural Machine Translation", "abstract": "The scarcity of large parallel corpora is an important obstacle for neural\nmachine translation. A common solution is to exploit the knowledge of language\nmodels (LM) trained on abundant monolingual data. In this work, we propose a\nnovel approach to incorporate a LM as prior in a neural translation model (TM).\nSpecifically, we add a regularization term, which pushes the output\ndistributions of the TM to be probable under the LM prior, while avoiding wrong\npredictions when the TM \"disagrees\" with the LM. This objective relates to\nknowledge distillation, where the LM can be viewed as teaching the TM about the\ntarget language. The proposed approach does not compromise decoding speed,\nbecause the LM is used only at training time, unlike previous work that\nrequires it during inference. We present an analysis of the effects that\ndifferent methods have on the distributions of the TM. Results on two\nlow-resource machine translation datasets show clear improvements even with\nlimited monolingual data.", "published": "2020-04-30 16:29:56", "link": "http://arxiv.org/abs/2004.14928v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Premise Selection: Finding Supporting Statements for\n  Mathematical Text", "abstract": "Mathematical text is written using a combination of words and mathematical\nexpressions. This combination, along with a specific way of structuring\nsentences makes it challenging for state-of-art NLP tools to understand and\nreason on top of mathematical discourse. In this work, we propose a new NLP\ntask, the natural premise selection, which is used to retrieve supporting\ndefinitions and supporting propositions that are useful for generating an\ninformal mathematical proof for a particular statement. We also make available\na dataset, NL-PS, which can be used to evaluate different approaches for the\nnatural premise selection task. Using different baselines, we demonstrate the\nunderlying interpretation challenges associated with the task.", "published": "2020-04-30 17:08:03", "link": "http://arxiv.org/abs/2004.14959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies", "abstract": "We describe a method for developing broad-coverage semantic dependency\nparsers for languages for which no semantically annotated resource is\navailable. We leverage a multitask learning framework coupled with an\nannotation projection method. We transfer supervised semantic dependency parse\nannotations from a rich-resource language to a low-resource language through\nparallel data, and train a semantic parser on projected data. We make use of\nsupervised syntactic parsing as an auxiliary task in a multitask learning\nframework, and show that with different multitask learning settings, we\nconsistently improve over the single-task baseline. In the setting in which\nEnglish is the source, and Czech is the target language, our best multitask\nmodel improves the labeled F1 score over the single-task baseline by 1.8 in the\nin-domain SemEval data (Oepen et al., 2015), as well as 2.5 in the\nout-of-domain test set. Moreover, we observe that syntactic and semantic\ndependency direction match is an important factor in improving the results.", "published": "2020-04-30 17:09:51", "link": "http://arxiv.org/abs/2004.14961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data and Representation for Turkish Natural Language Inference", "abstract": "Large annotated datasets in NLP are overwhelmingly in English. This is an\nobstacle to progress in other languages. Unfortunately, obtaining new annotated\nresources for each task in each language would be prohibitively expensive. At\nthe same time, commercial machine translation systems are now robust. Can we\nleverage these systems to translate English-language datasets automatically? In\nthis paper, we offer a positive response for natural language inference (NLI)\nin Turkish. We translated two large English NLI datasets into Turkish and had a\nteam of experts validate their translation quality and fidelity to the original\nlabels. Using these datasets, we address core issues of representation for\nTurkish NLI. We find that in-language embeddings are essential and that\nmorphological parsing can be avoided where the training set is large. Finally,\nwe show that models trained on our machine-translated datasets are successful\non human-translated evaluation sets. We share all code, models, and data\npublicly.", "published": "2020-04-30 17:12:52", "link": "http://arxiv.org/abs/2004.14963v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PlotMachines: Outline-Conditioned Generation with Dynamic Plot State\n  Tracking", "abstract": "We propose the task of outline-conditioned story generation: given an outline\nas a set of phrases that describe key characters and events to appear in a\nstory, the task is to generate a coherent narrative that is consistent with the\nprovided outline. This task is challenging as the input only provides a rough\nsketch of the plot, and thus, models need to generate a story by interweaving\nthe key points provided in the outline. This requires the model to keep track\nof the dynamic states of the latent plot, conditioning on the input outline\nwhile generating the full story. We present PlotMachines, a neural narrative\nmodel that learns to transform an outline into a coherent story by tracking the\ndynamic plot states. In addition, we enrich PlotMachines with high-level\ndiscourse structure so that the model can learn different writing styles\ncorresponding to different parts of the narrative. Comprehensive experiments\nover three fiction and non-fiction datasets demonstrate that large-scale\nlanguage models, such as GPT-2 and Grover, despite their impressive generation\nperformance, are not sufficient in generating coherent narratives for the given\noutline, and dynamic plot state tracking is important for composing narratives\nwith tighter, more consistent plots.", "published": "2020-04-30 17:16:31", "link": "http://arxiv.org/abs/2004.14967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact or Fiction: Verifying Scientific Claims", "abstract": "We introduce scientific claim verification, a new task to select abstracts\nfrom the research literature containing evidence that SUPPORTS or REFUTES a\ngiven scientific claim, and to identify rationales justifying each decision. To\nstudy this task, we construct SciFact, a dataset of 1.4K expert-written\nscientific claims paired with evidence-containing abstracts annotated with\nlabels and rationales. We develop baseline models for SciFact, and demonstrate\nthat simple domain adaptation techniques substantially improve performance\ncompared to models trained on Wikipedia or political news. We show that our\nsystem is able to verify claims related to COVID-19 by identifying evidence\nfrom the CORD-19 corpus. Our experiments indicate that SciFact will provide a\nchallenging testbed for the development of new systems designed to retrieve and\nreason over corpora containing specialized domain knowledge. Data and code for\nthis new task are publicly available at https://github.com/allenai/scifact. A\nleaderboard and COVID-19 fact-checking demo are available at\nhttps://scifact.apps.allenai.org.", "published": "2020-04-30 17:22:57", "link": "http://arxiv.org/abs/2004.14974v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrasing vs Coreferring: Two Sides of the Same Coin", "abstract": "We study the potential synergy between two different NLP tasks, both\nconfronting predicate lexical variability: identifying predicate paraphrases,\nand event coreference resolution. First, we used annotations from an event\ncoreference dataset as distant supervision to re-score heuristically-extracted\npredicate paraphrases. The new scoring gained more than 18 points in average\nprecision upon their ranking by the original scoring method. Then, we used the\nsame re-ranking features as additional inputs to a state-of-the-art event\ncoreference resolution model, which yielded modest but consistent improvements\nto the model's performance. The results suggest a promising direction to\nleverage data and models for each of the tasks to the benefit of the other.", "published": "2020-04-30 17:29:17", "link": "http://arxiv.org/abs/2004.14979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Control, Generate, Augment: A Scalable Framework for Multi-Attribute\n  Text Generation", "abstract": "We introduce CGA, a conditional VAE architecture, to control, generate, and\naugment text. CGA is able to generate natural English sentences controlling\nmultiple semantic and syntactic attributes by combining adversarial learning\nwith a context-aware loss and a cyclical word dropout routine. We demonstrate\nthe value of the individual model components in an ablation study. The\nscalability of our approach is ensured through a single discriminator,\nindependently of the number of attributes. We show high quality, diversity and\nattribute control in the generated sentences through a series of automatic and\nhuman assessments. As the main application of our work, we test the potential\nof this new NLG model in a data augmentation scenario. In a downstream NLP\ntask, the sentences generated by our CGA model show significant improvements\nover a strong baseline, and a classification performance often comparable to\nadding same amount of additional real data.", "published": "2020-04-30 17:31:16", "link": "http://arxiv.org/abs/2004.14983v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study in Improving BLEU Reference Coverage with Diverse Automatic\n  Paraphrasing", "abstract": "We investigate a long-perceived shortcoming in the typical use of BLEU: its\nreliance on a single reference. Using modern neural paraphrasing techniques, we\nstudy whether automatically generating additional diverse references can\nprovide better coverage of the space of valid translations and thereby improve\nits correlation with human judgments. Our experiments on the into-English\nlanguage directions of the WMT19 metrics task (at both the system and sentence\nlevel) show that using paraphrased references does generally improve BLEU, and\nwhen it does, the more diverse the better. However, we also show that better\nresults could be achieved if those paraphrases were to specifically target the\nparts of the space most relevant to the MT outputs being evaluated. Moreover,\nthe gains remain slight even when human paraphrases are used, suggesting\ninherent limitations to BLEU's capacity to correctly exploit multiple\nreferences. Surprisingly, we also find that adequacy appears to be less\nimportant, as shown by the high results of a strong sampling approach, which\neven beats human paraphrases when used with sentence-level BLEU.", "published": "2020-04-30 17:34:52", "link": "http://arxiv.org/abs/2004.14989v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Matter of Framing: The Impact of Linguistic Formalism on Probing\n  Results", "abstract": "Deep pre-trained contextualized encoders like BERT (Delvin et al., 2019)\ndemonstrate remarkable performance on a range of downstream tasks. A recent\nline of research in probing investigates the linguistic knowledge implicitly\nlearned by these models during pre-training. While most work in probing\noperates on the task level, linguistic tasks are rarely uniform and can be\nrepresented in a variety of formalisms. Any linguistics-based probing study\nthereby inevitably commits to the formalism used to annotate the underlying\ndata. Can the choice of formalism affect probing results? To investigate, we\nconduct an in-depth cross-formalism layer probing study in role semantics. We\nfind linguistically meaningful differences in the encoding of semantic role-\nand proto-role information by BERT depending on the formalism and demonstrate\nthat layer probing can detect subtle differences between the implementations of\nthe same linguistic formalism. Our results suggest that linguistic formalism is\nan important dimension in probing studies, along with the commonly used\ncross-task and cross-lingual experimental settings.", "published": "2020-04-30 17:45:16", "link": "http://arxiv.org/abs/2004.14999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Rotator's Distance", "abstract": "A key principle in assessing textual similarity is measuring the degree of\nsemantic overlap between two texts by considering the word alignment. Such\nalignment-based approaches are intuitive and interpretable; however, they are\nempirically inferior to the simple cosine similarity between general-purpose\nsentence vectors. To address this issue, we focus on and demonstrate the fact\nthat the norm of word vectors is a good proxy for word importance, and their\nangle is a good proxy for word similarity. Alignment-based approaches do not\ndistinguish them, whereas sentence-vector approaches automatically use the norm\nas the word importance. Accordingly, we propose a method that first decouples\nword vectors into their norm and direction, and then computes alignment-based\nsimilarity using earth mover's distance (i.e., optimal transport cost), which\nwe refer to as word rotator's distance. Besides, we find how to grow the norm\nand direction of word vectors (vector converter), which is a new systematic\napproach derived from sentence-vector estimation methods. On several textual\nsimilarity datasets, the combination of these simple proposed methods\noutperformed not only alignment-based approaches but also strong baselines. The\nsource code is available at https://github.com/eumesy/wrd", "published": "2020-04-30 17:48:42", "link": "http://arxiv.org/abs/2004.15003v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Template Guided Text Generation for Task-Oriented Dialogue", "abstract": "Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri\nenable users to interact with a large number of services and APIs on the web\nusing natural language. In this work, we investigate two methods for Natural\nLanguage Generation (NLG) using a single domain-independent model across a\nlarge number of APIs. First, we propose a schema-guided approach which\nconditions the generation on a schema describing the API in natural language.\nOur second method investigates the use of a small number of templates, growing\nlinearly in number of slots, to convey the semantics of the API. To generate\nutterances for an arbitrary slot combination, a few simple templates are first\nconcatenated to give a semantically correct, but possibly incoherent and\nungrammatical utterance. A pre-trained language model is subsequently employed\nto rewrite it into coherent, natural sounding text. Through automatic metrics\nand human evaluation, we show that our method improves over strong baselines,\nis robust to out-of-domain inputs and shows improved sample efficiency.", "published": "2020-04-30 17:51:08", "link": "http://arxiv.org/abs/2004.15006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Semantic Recognition", "abstract": "In lexical semantics, full-sentence segmentation and segment labeling of\nvarious phenomena are generally treated separately, despite their\ninterdependence. We hypothesize that a unified lexical semantic recognition\ntask is an effective way to encapsulate previously disparate styles of\nannotation, including multiword expression identification / classification and\nsupersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence\ntagger and evaluate its performance along various axes of annotation. As the\nlabel set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally\nevaluate how well the model generalizes to those test sets, finding that it\napproaches or surpasses existing models despite training only on STREUSLE. Our\nwork also establishes baseline models and evaluation metrics for integrated and\naccurate modeling of lexical semantics, facilitating future work in this area.", "published": "2020-04-30 17:52:11", "link": "http://arxiv.org/abs/2004.15008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TLDR: Extreme Summarization of Scientific Documents", "abstract": "We introduce TLDR generation, a new form of extreme summarization, for\nscientific papers. TLDR generation involves high source compression and\nrequires expert background knowledge and understanding of complex\ndomain-specific language. To facilitate study on this task, we introduce\nSciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR\ncontains both author-written and expert-derived TLDRs, where the latter are\ncollected using a novel annotation protocol that produces high-quality\nsummaries while minimizing annotation burden. We propose CATTS, a simple yet\neffective learning strategy for generating TLDRs that exploits titles as an\nauxiliary training signal. CATTS improves upon strong baselines under both\nautomated metrics and human evaluations. Data and code are publicly available\nat https://github.com/allenai/scitldr.", "published": "2020-04-30 17:56:18", "link": "http://arxiv.org/abs/2004.15011v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Data Augmentation Improve Generalization in NLP?", "abstract": "Neural models often exploit superficial features to achieve good performance,\nrather than deriving more general features. Overcoming this tendency is a\ncentral challenge in areas such as representation learning and ML fairness.\nRecent work has proposed using data augmentation, i.e., generating training\nexamples where the superficial features fail, as a means of encouraging models\nto prefer the stronger features. We design a series of toy learning problems to\ntest the hypothesis that data augmentation leads models to unlearn weaker\nheuristics, but not to learn stronger features in their place. We find partial\nsupport for this hypothesis: Data augmentation often hurts before it helps, and\nit is less effective when the preferred strong feature is much more difficult\nto extract than the competing weak feature.", "published": "2020-04-30 17:56:30", "link": "http://arxiv.org/abs/2004.15012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words\n  in Context", "abstract": "We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense\nDisambiguation. More specifically, we introduce a framework for Target Sense\nVerification of Words in Context which grounds its uniqueness in the\nformulation as a binary classification task thus being independent of external\nsense inventories, and the coverage of various domains. This makes the dataset\nhighly flexible for the evaluation of a diverse set of models and systems in\nand across domains. WiC-TSV provides three different evaluation settings,\ndepending on the input signals provided to the model. We set baseline\nperformance on the dataset using state-of-the-art language models. Experimental\nresults show that even though these models can perform decently on the task,\nthere remains a gap between machine and human performance, especially in\nout-of-domain settings. WiC-TSV data is available at\nhttps://competitions.codalab.org/competitions/23683", "published": "2020-04-30 17:57:27", "link": "http://arxiv.org/abs/2004.15016v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crisscrossed Captions: Extended Intramodal and Intermodal Semantic\n  Similarity Judgments for MS-COCO", "abstract": "By supporting multi-modal retrieval training and evaluation, image captioning\ndatasets have spurred remarkable progress on representation learning.\nUnfortunately, datasets have limited cross-modal associations: images are not\npaired with other images, captions are only paired with other captions of the\nsame image, there are no negative associations and there are missing positive\ncross-modal associations. This undermines research into how inter-modality\nlearning impacts intra-modality tasks. We address this gap with Crisscrossed\nCaptions (CxC), an extension of the MS-COCO dataset with human semantic\nsimilarity judgments for 267,095 intra- and inter-modality pairs. We report\nbaseline results on CxC for strong existing unimodal and multimodal models. We\nalso evaluate a multitask dual encoder trained on both image-caption and\ncaption-caption pairs that crucially demonstrates CxC's value for measuring the\ninfluence of intra- and inter-modality learning.", "published": "2020-04-30 17:59:17", "link": "http://arxiv.org/abs/2004.15020v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Method for Customizable Automated Tagging: Addressing the Problem of\n  Over-tagging and Under-tagging Text Documents", "abstract": "Using author provided tags to predict tags for a new document often results\nin the overgeneration of tags. In the case where the author doesn't provide any\ntags, our documents face the severe under-tagging issue. In this paper, we\npresent a method to generate a universal set of tags that can be applied widely\nto a large document corpus. Using IBM Watson's NLU service, first, we collect\nkeywords/phrases that we call \"complex document tags\" from 8,854 popular\nreports in the corpus. We apply LDA model over these complex document tags to\ngenerate a set of 765 unique \"simple tags\". In applying the tags to a corpus of\ndocuments, we run each document through the IBM Watson NLU and apply\nappropriate simple tags. Using only 765 simple tags, our method allows us to\ntag 87,397 out of 88,583 total documents in the corpus with at least one tag.\nAbout 92.1% of the total 87,397 documents are also determined to be\nsufficiently-tagged. In the end, we discuss the performance of our method and\nits limitations.", "published": "2020-04-30 18:28:42", "link": "http://arxiv.org/abs/2005.00042v1", "categories": ["cs.IR", "cs.CL", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Context based Text-generation using LSTM networks", "abstract": "Long short-term memory(LSTM) units on sequence-based models are being used in\ntranslation, question-answering systems, classification tasks due to their\ncapability of learning long-term dependencies. In Natural language generation,\nLSTM networks are providing impressive results on text generation models by\nlearning language models with grammatically stable syntaxes. But the downside\nis that the network does not learn about the context. The network only learns\nthe input-output function and generates text given a set of input words\nirrespective of pragmatics. As the model is trained without any such context,\nthere is no semantic consistency among the generated sentences. The proposed\nmodel is trained to generate text for a given set of input words along with a\ncontext vector. A context vector is similar to a paragraph vector that grasps\nthe semantic meaning(context) of the sentence. Several methods of extracting\nthe context vectors are proposed in this work. While training a language model,\nin addition to the input-output sequences, context vectors are also trained\nalong with the inputs. Due to this structure, the model learns the relation\namong the input words, context vector and the target word. Given a set of\ncontext terms, a well trained model will generate text around the provided\ncontext. Based on the nature of computing context vectors, the model has been\ntried out with two variations (word importance and word clustering). In the\nword clustering method, the suitable embeddings among various domains are also\nexplored. The results are evaluated based on the semantic closeness of the\ngenerated text to the given context.", "published": "2020-04-30 18:39:25", "link": "http://arxiv.org/abs/2005.00048v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attribution Analysis of Grammatical Dependencies in LSTMs", "abstract": "LSTM language models have been shown to capture syntax-sensitive grammatical\ndependencies such as subject-verb agreement with a high degree of accuracy\n(Linzen et al., 2016, inter alia). However, questions remain regarding whether\nthey do so using spurious correlations, or whether they are truly able to match\nverbs with their subjects. This paper argues for the latter hypothesis. Using\nlayer-wise relevance propagation (Bach et al., 2015), a technique that\nquantifies the contributions of input features to model behavior, we show that\nLSTM performance on number agreement is directly correlated with the model's\nability to distinguish subjects from other nouns. Our results suggest that LSTM\nlanguage models are able to infer robust representations of syntactic\ndependencies.", "published": "2020-04-30 19:19:37", "link": "http://arxiv.org/abs/2005.00062v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Linguistic Typology Features from Text: Inferring the Sparse Features of\n  World Atlas of Language Structures", "abstract": "The use of linguistic typological resources in natural language processing\nhas been steadily gaining more popularity. It has been observed that the use of\ntypological information, often combined with distributed language\nrepresentations, leads to significantly more powerful models. While linguistic\ntypology representations from various resources have mostly been used for\nconditioning the models, there has been relatively little attention on\npredicting features from these resources from the input data. In this paper we\ninvestigate whether the various linguistic features from World Atlas of\nLanguage Structures (WALS) can be reliably inferred from multi-lingual text.\nSuch a predictor can be used to infer structural features for a language never\nobserved in training data. We frame this task as a multi-label classification\ninvolving predicting the set of non-mutually exclusive and extremely sparse\nmulti-valued labels (WALS features). We construct a recurrent neural network\npredictor based on byte embeddings and convolutional layers and test its\nperformance on 556 languages, providing analysis for various linguistic types,\nmacro-areas, language families and individual features. We show that some\nfeatures from various linguistic types can be predicted reliably.", "published": "2020-04-30 21:00:53", "link": "http://arxiv.org/abs/2005.00100v2", "categories": ["cs.CL", "cs.LG", "I.2.7; I.5.4; J.5"], "primary_category": "cs.CL"}
{"title": "Structure-Tags Improve Text Classification for Scholarly Document\n  Quality Prediction", "abstract": "Training recurrent neural networks on long texts, in particular scholarly\ndocuments, causes problems for learning. While hierarchical attention networks\n(HANs) are effective in solving these problems, they still lose important\ninformation about the structure of the text. To tackle these problems, we\npropose the use of HANs combined with structure-tags which mark the role of\nsentences in the document. Adding tags to sentences, marking them as\ncorresponding to title, abstract or main body text, yields improvements over\nthe state-of-the-art for scholarly document quality prediction. The proposed\nsystem is applied to the task of accept/reject prediction on the PeerRead\ndataset and compared against a recent BiLSTM-based model and joint\ntextual+visual model as well as against plain HANs. Compared to plain HANs,\naccuracy increases on all three domains. On the computation and language domain\nour new model works best overall, and increases accuracy 4.7% over the best\nliterature result. We also obtain improvements when introducing the tags for\nprediction of the number of citations for 88k scientific publications that we\ncompiled from the Allen AI S2ORC dataset. For our HAN-system with\nstructure-tags we reach 28.5% explained variance, an improvement of 1.8% over\nour reimplementation of the BiLSTM-based model as well as 1.0% improvement over\nplain HANs.", "published": "2020-04-30 22:34:34", "link": "http://arxiv.org/abs/2005.00129v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contextual Text Style Transfer", "abstract": "We introduce a new task, Contextual Text Style Transfer - translating a\nsentence into a desired style with its surrounding context taken into account.\nThis brings two key challenges to existing style transfer approaches: ($i$) how\nto preserve the semantic meaning of target sentence and its consistency with\nsurrounding context during transfer; ($ii$) how to train a robust model with\nlimited labeled data accompanied with context. To realize high-quality style\ntransfer with natural context preservation, we propose a Context-Aware Style\nTransfer (CAST) model, which uses two separate encoders for each input sentence\nand its surrounding context. A classifier is further trained to ensure\ncontextual consistency of the generated sentence. To compensate for the lack of\nparallel data, additional self-reconstruction and back-translation losses are\nintroduced to leverage non-parallel data in a semi-supervised fashion. Two new\nbenchmarks, Enron-Context and Reddit-Context, are introduced for formality and\noffensiveness style transfer. Experimental results on these datasets\ndemonstrate the effectiveness of the proposed CAST model over state-of-the-art\nmethods across style accuracy, content preservation and contextual consistency\nmetrics.", "published": "2020-04-30 23:01:12", "link": "http://arxiv.org/abs/2005.00136v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Entity Representations through Large-Scale Typing", "abstract": "In standard methodology for natural language processing, entities in text are\ntypically embedded in dense vector spaces with pre-trained models. The\nembeddings produced this way are effective when fed into downstream models, but\nthey require end-task fine-tuning and are fundamentally difficult to interpret.\nIn this paper, we present an approach to creating entity representations that\nare human readable and achieve high performance on entity-related tasks out of\nthe box. Our representations are vectors whose values correspond to posterior\nprobabilities over fine-grained entity types, indicating the confidence of a\ntyping model's decision that the entity belongs to the corresponding type. We\nobtain these representations using a fine-grained entity typing model, trained\neither on supervised ultra-fine entity typing data (Choi et al. 2018) or\ndistantly-supervised examples from Wikipedia. On entity probing tasks involving\nrecognizing entity identity, our embeddings used in parameter-free downstream\nmodels achieve competitive performance with ELMo- and BERT-based embeddings in\ntrained models. We also show that it is possible to reduce the size of our type\nset in a learning-based way for particular domains. Finally, we show that these\nembeddings can be post-hoc modified through a small number of rules to\nincorporate domain knowledge and improve performance.", "published": "2020-04-30 23:58:03", "link": "http://arxiv.org/abs/2005.00147v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER\n  Identification", "abstract": "The PICO framework (Population, Intervention, Comparison, and Outcome) is\nusually used to formulate evidence in the medical domain. The major task of\nPICO extraction is to extract sentences from medical literature and classify\nthem into each class. However, in most circumstances, there will be more than\none evidences in an extracted sentence even it has been categorized to a\ncertain class. In order to address this problem, we propose a step-wise disease\nNamed Entity Recognition (DNER) extraction and PICO identification method. With\nour method, sentences in paper title and abstract are first classified into\ndifferent classes of PICO, and medical entities are then identified and\nclassified into P and O. Different kinds of deep learning frameworks are used\nand experimental results show that our method will achieve high performance and\nfine-grained extraction results comparing with conventional PICO extraction\nworks.", "published": "2020-04-30 03:09:17", "link": "http://arxiv.org/abs/2005.06601v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and\n  Context-Dependent Word Representations", "abstract": "This paper describes the winning contribution to SemEval-2020 Task 1:\nUnsupervised Lexical Semantic Change Detection (Subtask 2) handed in by team UG\nStudent Intern. We present an ensemble model that makes predictions based on\ncontext-free and context-dependent word representations. The key findings are\nthat (1) context-free word representations are a powerful and robust baseline,\n(2) a sentence classification objective can be used to obtain useful\ncontext-dependent word representations, and (3) combining those representations\nincreases performance on some datasets while decreasing performance on others.", "published": "2020-04-30 13:18:29", "link": "http://arxiv.org/abs/2005.06602v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WT5?! Training Text-to-Text Models to Explain their Predictions", "abstract": "Neural networks have recently achieved human-level performance on various\nchallenging natural language processing (NLP) tasks, but it is notoriously\ndifficult to understand why a neural network produced a particular prediction.\nIn this paper, we leverage the text-to-text framework proposed by Raffel et\nal.(2019) to train language models to output a natural text explanation\nalongside their prediction. Crucially, this requires no modifications to the\nloss function or training and decoding procedures -- we simply train the model\nto output the explanation after generating the (natural text) prediction. We\nshow that this approach not only obtains state-of-the-art results on\nexplainability benchmarks, but also permits learning from a limited set of\nlabeled explanations and transferring rationalization abilities across\ndatasets. To facilitate reproducibility and future work, we release our code\nuse to train the models.", "published": "2020-04-30 02:20:14", "link": "http://arxiv.org/abs/2004.14546v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Indirect Identification of Psychosocial Risks from Natural Language", "abstract": "During the perinatal period, psychosocial health risks, including depression\nand intimate partner violence, are associated with serious adverse health\noutcomes for parents and children. To appropriately intervene, healthcare\nprofessionals must first identify those at risk, yet stigma often prevents\npeople from directly disclosing the information needed to prompt an assessment.\nWe examine indirect methods of eliciting and analyzing information that could\nindicate psychosocial risks. Short diary entries by peripartum women exhibit\nthematic patterns, extracted by topic modeling, and emotional perspective,\ndrawn from dictionary-informed sentiment features. Using these features, we use\nregularized regression to predict screening measures of depression and\npsychological aggression by an intimate partner. Journal text entries\nquantified through topic models and sentiment features show promise for\ndepression prediction, with performance almost as good as closed-form\nquestions. Text-based features were less useful for prediction of intimate\npartner violence, but moderately indirect multiple-choice questioning allowed\nfor detection without explicit disclosure. Both methods may serve as an initial\nor complementary screening approach to detecting stigmatized risks.", "published": "2020-04-30 03:13:28", "link": "http://arxiv.org/abs/2004.14554v1", "categories": ["cs.CL", "cs.CY", "J.3; J.4; H.5.2"], "primary_category": "cs.CL"}
{"title": "Improved Natural Language Generation via Loss Truncation", "abstract": "Neural language models are usually trained to match the distributional\nproperties of a large-scale corpus by minimizing the log loss. While\nstraightforward to optimize, this approach forces the model to reproduce all\nvariations in the dataset, including noisy and invalid references (e.g.,\nmisannotation and hallucinated facts). Worse, the commonly used log loss is\noverly sensitive to such phenomena and even a small fraction of noisy data can\ndegrade performance. In this work, we show that the distinguishability of the\nmodels and reference serves as a principled and robust alternative for handling\ninvalid references. To optimize distinguishability, we propose loss truncation,\nwhich adaptively removes high loss examples during training. We show this is as\neasy to optimize as log loss and tightly bounds distinguishability under noise.\nEmpirically, we demonstrate that loss truncation outperforms existing baselines\non distinguishability on a summarization task, and show that samples generated\nby the loss truncation model have factual accuracy ratings that exceed those of\nbaselines and match human references.", "published": "2020-04-30 05:31:31", "link": "http://arxiv.org/abs/2004.14589v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EnsembleGAN: Adversarial Learning for Retrieval-Generation Ensemble\n  Model on Short-Text Conversation", "abstract": "Generating qualitative responses has always been a challenge for\nhuman-computer dialogue systems. Existing dialogue systems generally derive\nfrom either retrieval-based or generative-based approaches, both of which have\ntheir own pros and cons. Despite the natural idea of an ensemble model of the\ntwo, existing ensemble methods only focused on leveraging one approach to\nenhance another, we argue however that they can be further mutually enhanced\nwith a proper training strategy. In this paper, we propose ensembleGAN, an\nadversarial learning framework for enhancing a retrieval-generation ensemble\nmodel in open-domain conversation scenario. It consists of a\nlanguage-model-like generator, a ranker generator, and one ranker\ndiscriminator. Aiming at generating responses that approximate the ground-truth\nand receive high ranking scores from the discriminator, the two generators\nlearn to generate improved highly relevant responses and competitive unobserved\ncandidates respectively, while the discriminative ranker is trained to identify\ntrue responses from adversarial ones, thus featuring the merits of both\ngenerator counterparts. The experimental results on a large short-text\nconversation data demonstrate the effectiveness of the ensembleGAN by the\namelioration on both human and automatic evaluation metrics.", "published": "2020-04-30 05:59:12", "link": "http://arxiv.org/abs/2004.14592v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Robust Question Answering Through Sub-part Alignment", "abstract": "Current textual question answering models achieve strong performance on\nin-domain test sets, but often do so by fitting surface-level patterns in the\ndata, so they fail to generalize to out-of-distribution settings. To make a\nmore robust and understandable QA system, we model question answering as an\nalignment problem. We decompose both the question and context into smaller\nunits based on off-the-shelf semantic representations (here, semantic roles),\nand align the question to a subgraph of the context in order to find the\nanswer. We formulate our model as a structured SVM, with alignment scores\ncomputed via BERT, and we can train end-to-end despite using beam search for\napproximate inference. Our explicit use of alignments allows us to explore a\nset of constraints with which we can prohibit certain types of bad model\nbehavior arising in cross-domain settings. Furthermore, by investigating\ndifferences in scores across different potential answers, we can seek to\nunderstand what particular aspects of the input lead the model to choose the\nanswer without relying on post-hoc explanation techniques. We train our model\non SQuAD v1.1 and test it on several adversarial and out-of-domain datasets.\nThe results show that our model is more robust cross-domain than the standard\nBERT QA model, and constraints derived from alignment scores allow us to\neffectively trade off coverage and accuracy.", "published": "2020-04-30 09:10:57", "link": "http://arxiv.org/abs/2004.14648v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NUBIA: NeUral Based Interchangeability Assessor for Text Generation", "abstract": "We present NUBIA, a methodology to build automatic evaluation metrics for\ntext generation using only machine learning models as core components. A\ntypical NUBIA model is composed of three modules: a neural feature extractor,\nan aggregator and a calibrator. We demonstrate an implementation of NUBIA which\noutperforms metrics currently used to evaluate machine translation, summaries\nand slightly exceeds/matches state of the art metrics on correlation with human\njudgement on the WMT segment-level Direct Assessment task, sentence-level\nranking and image captioning evaluation. The model implemented is modular,\nexplainable and set to continuously improve over time.", "published": "2020-04-30 10:11:33", "link": "http://arxiv.org/abs/2004.14667v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMPERSAND: Argument Mining for PERSuAsive oNline Discussions", "abstract": "Argumentation is a type of discourse where speakers try to persuade their\naudience about the reasonableness of a claim by presenting supportive\narguments. Most work in argument mining has focused on modeling arguments in\nmonologues. We propose a computational model for argument mining in online\npersuasive discussion forums that brings together the micro-level (argument as\nproduct) and macro-level (argument as process) models of argumentation.\nFundamentally, this approach relies on identifying relations between components\nof arguments in a discussion thread. Our approach for relation prediction uses\ncontextual information in terms of fine-tuning a pre-trained language model and\nleveraging discourse relations based on Rhetorical Structure Theory. We\nadditionally propose a candidate selection method to automatically predict what\nparts of one's argument will be targeted by other participants in the\ndiscussion. Our models obtain significant improvements compared to recent\nstate-of-the-art approaches using pointer networks and a pre-trained language\nmodel.", "published": "2020-04-30 10:33:40", "link": "http://arxiv.org/abs/2004.14677v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Span-based Linearization for Constituent Trees", "abstract": "We propose a novel linearization of a constituent tree, together with a new\nlocally normalized model. For each split point in a sentence, our model\ncomputes the normalizer on all spans ending with that split point, and then\npredicts a tree span from them. Compared with global models, our model is fast\nand parallelizable. Different from previous local models, our linearization\nmethod is tied on the spans directly and considers more local features when\nperforming span prediction, which is more interpretable and effective.\nExperiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model\nsignificantly outperforms existing local models and efficiently achieves\ncompetitive results with global models.", "published": "2020-04-30 11:36:33", "link": "http://arxiv.org/abs/2004.14704v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised and Controlled Multi-Document Opinion Summarization", "abstract": "We address the problem of unsupervised abstractive summarization of\ncollections of user generated reviews with self-supervision and control. We\npropose a self-supervised setup that considers an individual document as a\ntarget summary for a set of similar documents. This setting makes training\nsimpler than previous approaches by relying only on standard log-likelihood\nloss. We address the problem of hallucinations through the use of control\ncodes, to steer the generation towards more coherent and relevant\nsummaries.Finally, we extend the Transformer architecture to allow for multiple\nreviews as input. Our benchmarks on two datasets against graph-based and recent\nneural abstractive unsupervised models show that our proposed method generates\nsummaries with a superior quality and relevance.This is confirmed in our human\nevaluation which focuses explicitly on the faithfulness of generated summaries\nWe also provide an ablation study, which shows the importance of the control\nsetup in controlling hallucinations and achieve high sentiment and topic\nalignment of the summaries with the input reviews.", "published": "2020-04-30 13:20:18", "link": "http://arxiv.org/abs/2004.14754v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accurate Word Alignment Induction from Neural Machine Translation", "abstract": "Despite its original goal to jointly learn to align and translate, prior\nresearches suggest that Transformer captures poor word alignments through its\nattention mechanism. In this paper, we show that attention weights DO capture\naccurate word alignments and propose two novel word alignment induction methods\nShift-Att and Shift-AET. The main idea is to induce alignments at the step when\nthe to-be-aligned target token is the decoder input rather than the decoder\noutput as in previous work. Shift-Att is an interpretation method that induces\nalignments from the attention weights of Transformer and does not require\nparameter update or architecture change. Shift-AET extracts alignments from an\nadditional alignment module which is tightly integrated into Transformer and\ntrained in isolation with supervision from symmetrized Shift-Att alignments.\nExperiments on three publicly available datasets demonstrate that both methods\nperform better than their corresponding neural baselines and Shift-AET\nsignificantly outperforms GIZA++ by 1.4-4.8 AER points.", "published": "2020-04-30 14:47:05", "link": "http://arxiv.org/abs/2004.14837v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Neural Models Learn Systematicity of Monotonicity Inference in\n  Natural Language?", "abstract": "Despite the success of language models using neural networks, it remains\nunclear to what extent neural models have the generalization ability to perform\ninferences. In this paper, we introduce a method for evaluating whether neural\nmodels can learn systematicity of monotonicity inference in natural language,\nnamely, the regularity for performing arbitrary inferences with generalization\non composition. We consider four aspects of monotonicity inferences and test\nwhether the models can systematically interpret lexical and logical phenomena\non different training/test splits. A series of experiments show that three\nneural models systematically draw inferences on unseen combinations of lexical\nand logical phenomena when the syntactic structures of the sentences are\nsimilar between the training and test sets. However, the performance of the\nmodels significantly decreases when the structures are slightly changed in the\ntest set while retaining all vocabularies and constituents already appearing in\nthe training set. This indicates that the generalization ability of neural\nmodels is limited to cases where the syntactic structures are nearly the same\nas those in the training set.", "published": "2020-04-30 14:48:39", "link": "http://arxiv.org/abs/2004.14839v2", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Embeddings and Explainable AI", "abstract": "Knowledge graph embeddings are now a widely adopted approach to knowledge\nrepresentation in which entities and relationships are embedded in vector\nspaces. In this chapter, we introduce the reader to the concept of knowledge\ngraph embeddings by explaining what they are, how they can be generated and how\nthey can be evaluated. We summarize the state-of-the-art in this field by\ndescribing the approaches that have been introduced to represent knowledge in\nthe vector space. In relation to knowledge representation, we consider the\nproblem of explainability, and discuss models and methods for explaining\npredictions obtained via knowledge graph embeddings.", "published": "2020-04-30 14:55:09", "link": "http://arxiv.org/abs/2004.14843v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Modelling Suspense in Short Stories as Uncertainty Reduction over Neural\n  Representation", "abstract": "Suspense is a crucial ingredient of narrative fiction, engaging readers and\nmaking stories compelling. While there is a vast theoretical literature on\nsuspense, it is computationally not well understood. We compare two ways for\nmodelling suspense: surprise, a backward-looking measure of how unexpected the\ncurrent state is given the story so far; and uncertainty reduction, a\nforward-looking measure of how unexpected the continuation of the story is.\nBoth can be computed either directly over story representations or over their\nprobability distributions. We propose a hierarchical language model that\nencodes stories and computes surprise and uncertainty reduction. Evaluating\nagainst short stories annotated with human suspense judgements, we find that\nuncertainty reduction over representations is the best predictor, resulting in\nnear-human accuracy. We also show that uncertainty reduction can be used to\npredict suspenseful events in movie synopses.", "published": "2020-04-30 16:03:06", "link": "http://arxiv.org/abs/2004.14905v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How do Decisions Emerge across Layers in Neural Models? Interpretation\n  with Differentiable Masking", "abstract": "Attribution methods assess the contribution of inputs to the model\nprediction. One way to do so is erasure: a subset of inputs is considered\nirrelevant if it can be removed without affecting the prediction. Though\nconceptually simple, erasure's objective is intractable and approximate search\nremains expensive with modern deep NLP models. Erasure is also susceptible to\nthe hindsight bias: the fact that an input can be dropped does not mean that\nthe model `knows' it can be dropped. The resulting pruning is over-aggressive\nand does not reflect how the model arrives at the prediction. To deal with\nthese challenges, we introduce Differentiable Masking. DiffMask learns to\nmask-out subsets of the input while maintaining differentiability. The decision\nto include or disregard an input token is made with a simple model based on\nintermediate hidden layers of the analyzed model. First, this makes the\napproach efficient because we predict rather than search. Second, as with\nprobing classifiers, this reveals what the network `knows' at the corresponding\nlayers. This lets us not only plot attribution heatmaps but also analyze how\ndecisions are formed across network layers. We use DiffMask to study BERT\nmodels on sentiment classification and question answering.", "published": "2020-04-30 17:36:14", "link": "http://arxiv.org/abs/2004.14992v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Segatron: Segment-Aware Transformer for Language Modeling and\n  Understanding", "abstract": "Transformers are powerful for sequence modeling. Nearly all state-of-the-art\nlanguage models and pre-trained language models are based on the Transformer\narchitecture. However, it distinguishes sequential tokens only with the token\nposition index. We hypothesize that better contextual representations can be\ngenerated from the Transformer with richer positional information. To verify\nthis, we propose a segment-aware Transformer (Segatron), by replacing the\noriginal token position encoding with a combined position encoding of\nparagraph, sentence, and token. We first introduce the segment-aware mechanism\nto Transformer-XL, which is a popular Transformer-based language model with\nmemory extension and relative position encoding. We find that our method can\nfurther improve the Transformer-XL base model and large model, achieving 17.1\nperplexity on the WikiText-103 dataset. We further investigate the pre-training\nmasked language modeling task with Segatron. Experimental results show that\nBERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla\nTransformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence\nrepresentation learning.", "published": "2020-04-30 17:38:27", "link": "http://arxiv.org/abs/2004.14996v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of\n  Contextual Embeddings", "abstract": "Multilingual contextual embeddings have demonstrated state-of-the-art\nperformance in zero-shot cross-lingual transfer learning, where multilingual\nBERT is fine-tuned on one source language and evaluated on a different target\nlanguage. However, published results for mBERT zero-shot accuracy vary as much\nas 17 points on the MLDoc classification task across four papers. We show that\nthe standard practice of using English dev accuracy for model selection in the\nzero-shot setting makes it difficult to obtain reproducible results on the\nMLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even\nanti-correlated) with target language accuracy, and zero-shot performance\nvaries greatly at different points in the same fine-tuning run and between\ndifferent fine-tuning runs. These reproducibility issues are also present for\nother tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We\nrecommend providing oracle scores alongside zero-shot results: still fine-tune\nusing English data, but choose a checkpoint with the target dev set. Reporting\nthis upper bound makes results more consistent by avoiding arbitrarily bad\ncheckpoints.", "published": "2020-04-30 17:47:17", "link": "http://arxiv.org/abs/2004.15001v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of\n  Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the\n  Society", "abstract": "With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Addressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus confirming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.", "published": "2020-04-30 18:04:20", "link": "http://arxiv.org/abs/2005.00033v5", "categories": ["cs.CL", "cs.CY", "cs.IR", "68T50", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "An Early Study on Intelligent Analysis of Speech under COVID-19:\n  Severity, Sleep Quality, Fatigue, and Anxiety", "abstract": "The COVID-19 outbreak was announced as a global pandemic by the World Health\nOrganisation in March 2020 and has affected a growing number of people in the\npast few weeks. In this context, advanced artificial intelligence techniques\nare brought to the fore in responding to fight against and reduce the impact of\nthis global health crisis. In this study, we focus on developing some potential\nuse-cases of intelligent speech analysis for COVID-19 diagnosed patients. In\nparticular, by analysing speech recordings from these patients, we construct\naudio-only-based models to automatically categorise the health state of\npatients from four aspects, including the severity of illness, sleep quality,\nfatigue, and anxiety. For this purpose, two established acoustic feature sets\nand support vector machines are utilised. Our experiments show that an average\naccuracy of .69 obtained estimating the severity of illness, which is derived\nfrom the number of days in hospitalisation. We hope that this study can foster\nan extremely fast, low-cost, and convenient way to automatically detect the\nCOVID-19 disease.", "published": "2020-04-30 20:47:05", "link": "http://arxiv.org/abs/2005.00096v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Spontaneous Emergence of Discrete and Compositional Signals", "abstract": "We propose a general framework to study language emergence through signaling\ngames with neural agents. Using a continuous latent space, we are able to (i)\ntrain using backpropagation, (ii) show that discrete messages nonetheless\nnaturally emerge. We explore whether categorical perception effects follow and\nshow that the messages are not compositional.", "published": "2020-04-30 21:15:19", "link": "http://arxiv.org/abs/2005.00110v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Learning to Faithfully Rationalize by Construction", "abstract": "In many settings it is important for one to be able to understand why a model\nmade a particular prediction. In NLP this often entails extracting snippets of\nan input text `responsible for' corresponding model output; when such a snippet\ncomprises tokens that indeed informed the model's prediction, it is a faithful\nexplanation. In some settings, faithfulness may be critical to ensure\ntransparency. Lei et al. (2016) proposed a model to produce faithful rationales\nfor neural text classification by defining independent snippet extraction and\nprediction modules. However, the discrete selection over input tokens performed\nby this method complicates training, leading to high variance and requiring\ncareful hyperparameter tuning. We propose a simpler variant of this approach\nthat provides faithful explanations by construction. In our scheme, named\nFRESH, arbitrary feature importance scores (e.g., gradients from a trained\nmodel) are used to induce binary labels over token inputs, which an extractor\ncan be trained to predict. An independent classifier module is then trained\nexclusively on snippets provided by the extractor; these snippets thus\nconstitute faithful explanations, even if the classifier is arbitrarily\ncomplex. In both automatic and manual evaluations we find that variants of this\nsimple framework yield predictive performance superior to `end-to-end'\napproaches, while being more general and easier to train. Code is available at\nhttps://github.com/successar/FRESH", "published": "2020-04-30 21:45:40", "link": "http://arxiv.org/abs/2005.00115v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Rank Intents in Voice Assistants", "abstract": "Voice Assistants aim to fulfill user requests by choosing the best intent\nfrom multiple options generated by its Automated Speech Recognition and Natural\nLanguage Understanding sub-systems. However, voice assistants do not always\nproduce the expected results. This can happen because voice assistants choose\nfrom ambiguous intents - user-specific or domain-specific contextual\ninformation reduces the ambiguity of the user request. Additionally the user\ninformation-state can be leveraged to understand how relevant/executable a\nspecific intent is for a user request. In this work, we propose a novel\nEnergy-based model for the intent ranking task, where we learn an affinity\nmetric and model the trade-off between extracted meaning from speech utterances\nand relevance/executability aspects of the intent. Furthermore we present a\nMultisource Denoising Autoencoder based pretraining that is capable of learning\nfused representations of data from multiple sources. We empirically show our\napproach outperforms existing state of the art methods by reducing the\nerror-rate by 3.8%, which in turn reduces ambiguity and eliminates undesired\ndead-ends leading to better user experience. Finally, we evaluate the\nrobustness of our algorithm on the intent ranking task and show our algorithm\nimproves the robustness by 33.3%.", "published": "2020-04-30 21:51:26", "link": "http://arxiv.org/abs/2005.00119v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Unsupervised Learning of KB Queries in Task-Oriented Dialogs", "abstract": "Task-oriented dialog (TOD) systems often need to formulate knowledge base\n(KB) queries corresponding to the user intent and use the query results to\ngenerate system responses. Existing approaches require dialog datasets to\nexplicitly annotate these KB queries -- these annotations can be time\nconsuming, and expensive. In response, we define the novel problems of\npredicting the KB query and training the dialog agent, without explicit KB\nquery annotation. For query prediction, we propose a reinforcement learning\n(RL) baseline, which rewards the generation of those queries whose KB results\ncover the entities mentioned in subsequent dialog. Further analysis reveals\nthat correlation among query attributes in KB can significantly confuse memory\naugmented policy optimization (MAPO), an existing state of the art RL agent. To\naddress this, we improve the MAPO baseline with simple but important\nmodifications suited to our task. To train the full TOD system for our setting,\nwe propose a pipelined approach: it independently predicts when to make a KB\nquery (query position predictor), then predicts a KB query at the predicted\nposition (query predictor), and uses the results of predicted query in\nsubsequent dialog (next response predictor). Overall, our work proposes first\nsolutions to our novel problem, and our analysis highlights the research\nchallenges in training TOD systems without query annotation.", "published": "2020-04-30 22:10:00", "link": "http://arxiv.org/abs/2005.00123v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On the Merging of Domain-Specific Heterogeneous Ontologies using Wordnet\n  and Web Pattern-based Queries", "abstract": "Ontologies form the basic interest in various computer science disciplines\nsuch as semantic web, information retrieval, database design, etc. They aim at\nproviding a formal, explicit and shared conceptualization and understanding of\ncommon domains between different communities. In addition, they allow for\nconcepts and their constraints of a specific domain to be explicitly defined.\nHowever, the distributed nature of ontology development and the differences in\nviewpoints of the ontology engineers have resulted in the so called \"semantic\nheterogeneity\" between ontologies. Semantic heterogeneity constitutes the major\nobstacle against achieving interoperability between ontologies. To overcome\nthis obstacle, we present a multi-purpose framework which exploits the WordNet\ngeneric knowledge base for: i) Discovering and correcting the incorrect\nsemantic relations between the concepts of the ontology in a specific domain.\nThis step is a primary step of ontology merging. ii) Merging domain-specific\nontologies through computing semantic relations between their concepts. iii)\nHandling the issue of missing concepts in WordNet through the acquisition of\nstatistical information on the Web. And iv) Enriching WordNet with these\nmissing concepts. An experimental instantiation of the framework and\ncomparisons with state-of-the-art syntactic and semantic-based systems validate\nour proposal.", "published": "2020-04-30 05:03:50", "link": "http://arxiv.org/abs/2005.00158v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Named Entity Recognition without Labelled Data: A Weak Supervision\n  Approach", "abstract": "Named Entity Recognition (NER) performance often degrades rapidly when\napplied to target domains that differ from the texts observed during training.\nWhen in-domain labelled data is available, transfer learning techniques can be\nused to adapt existing NER models to the target domain. But what should one do\nwhen there is no hand-labelled data for the target domain? This paper presents\na simple but powerful approach to learn NER models in the absence of labelled\ndata through weak supervision. The approach relies on a broad spectrum of\nlabelling functions to automatically annotate texts from the target domain.\nThese annotations are then merged together using a hidden Markov model which\ncaptures the varying accuracies and confusions of the labelling functions. A\nsequence labelling model can finally be trained on the basis of this unified\nannotation. We evaluate the approach on two English datasets (CoNLL 2003 and\nnews articles from Reuters and Bloomberg) and demonstrate an improvement of\nabout 7 percentage points in entity-level $F_1$ scores compared to an\nout-of-domain neural NER model.", "published": "2020-04-30 12:29:55", "link": "http://arxiv.org/abs/2004.14723v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Preventing Posterior Collapse with Levenshtein Variational Autoencoder", "abstract": "Variational autoencoders (VAEs) are a standard framework for inducing latent\nvariable models that have been shown effective in learning text representations\nas well as in text generation. The key challenge with using VAEs is the {\\it\nposterior collapse} problem: learning tends to converge to trivial solutions\nwhere the generators ignore latent variables. In our Levenstein VAE, we propose\nto replace the evidence lower bound (ELBO) with a new objective which is simple\nto optimize and prevents posterior collapse. Intuitively, it corresponds to\ngenerating a sequence from the autoencoder and encouraging the model to predict\nan optimal continuation according to the Levenshtein distance (LD) with the\nreference sentence at each time step in the generated sequence. We motivate the\nmethod from the probabilistic perspective by showing that it is closely related\nto optimizing a bound on the intractable Kullback-Leibler divergence of an\nLD-based kernel density estimator from the model distribution. With this\nobjective, any generator disregarding latent variables will incur large\npenalties and hence posterior collapse does not happen. We relate our approach\nto policy distillation \\cite{RossGB11} and dynamic oracles \\cite{GoldbergN12}.\nBy considering Yelp and SNLI benchmarks, we show that Levenstein VAE produces\nmore informative latent representations than alternative approaches to\npreventing posterior collapse.", "published": "2020-04-30 13:27:26", "link": "http://arxiv.org/abs/2004.14758v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The role of context in neural pitch accent detection in English", "abstract": "Prosody is a rich information source in natural language, serving as a marker\nfor phenomena such as contrast. In order to make this information available to\ndownstream tasks, we need a way to detect prosodic events in speech. We propose\na new model for pitch accent detection, inspired by the work of Stehwien et al.\n(2018), who presented a CNN-based model for this task. Our model makes greater\nuse of context by using full utterances as input and adding an LSTM layer. We\nfind that these innovations lead to an improvement from 87.5% to 88.7% accuracy\non pitch accent detection on American English speech in the Boston University\nRadio News Corpus, a state-of-the-art result. We also find that a simple\nbaseline that just predicts a pitch accent on every content word yields 82.2%\naccuracy, and we suggest that this is the appropriate baseline for this task.\nFinally, we conduct ablation tests that show pitch is the most important\nacoustic feature for this task and this corpus.", "published": "2020-04-30 14:59:05", "link": "http://arxiv.org/abs/2004.14846v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mind Your Inflections! Improving NLP for Non-Standard Englishes with\n  Base-Inflection Encoding", "abstract": "Inflectional variation is a common feature of World Englishes such as\nColloquial Singapore English and African American Vernacular English. Although\ncomprehension by human readers is usually unimpaired by non-standard\ninflections, current NLP systems are not yet robust. We propose Base-Inflection\nEncoding (BITE), a method to tokenize English text by reducing inflected words\nto their base forms before reinjecting the grammatical information as special\nsymbols. Fine-tuning pretrained NLP models for downstream tasks using our\nencoding defends against inflectional adversaries while maintaining performance\non clean data. Models using BITE generalize better to dialects with\nnon-standard inflections without explicit training and translation models\nconverge faster when trained with BITE. Finally, we show that our encoding\nimproves the vocabulary efficiency of popular data-driven subword tokenizers.\nSince there has been no prior work on quantitatively evaluating vocabulary\nefficiency, we propose metrics to do so.", "published": "2020-04-30 15:15:40", "link": "http://arxiv.org/abs/2004.14870v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Progressive Transformers for End-to-End Sign Language Production", "abstract": "The goal of automatic Sign Language Production (SLP) is to translate spoken\nlanguage to a continuous stream of sign language video at a level comparable to\na human translator. If this was achievable, then it would revolutionise Deaf\nhearing communications. Previous work on predominantly isolated SLP has shown\nthe need for architectures that are better suited to the continuous domain of\nfull sign sequences.\n  In this paper, we propose Progressive Transformers, a novel architecture that\ncan translate from discrete spoken language sentences to continuous 3D skeleton\npose outputs representing sign language. We present two model configurations,\nan end-to-end network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary.\n  Our transformer network architecture introduces a counter that enables\ncontinuous sequence generation at training and inference. We also provide\nseveral data augmentation processes to overcome the problem of drift and\nimprove the performance of SLP models. We propose a back translation evaluation\nmechanism for SLP, presenting benchmark quantitative results on the challenging\nRWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future\nresearch.", "published": "2020-04-30 15:20:25", "link": "http://arxiv.org/abs/2004.14874v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Few-Shot Learning for Opinion Summarization", "abstract": "Opinion summarization is the automatic creation of text reflecting subjective\ninformation expressed in multiple documents, such as user reviews of a product.\nThe task is practically important and has attracted a lot of attention.\nHowever, due to the high cost of summary production, datasets large enough for\ntraining supervised models are lacking. Instead, the task has been\ntraditionally approached with extractive methods that learn to select text\nfragments in an unsupervised or weakly-supervised way. Recently, it has been\nshown that abstractive summaries, potentially more fluent and better at\nreflecting conflicting information, can also be produced in an unsupervised\nfashion. However, these models, not being exposed to actual summaries, fail to\ncapture their essential properties. In this work, we show that even a handful\nof summaries is sufficient to bootstrap generation of the summary text with all\nexpected properties, such as writing style, informativeness, fluency, and\nsentiment preservation. We start by training a conditional Transformer language\nmodel to generate a new product review given other available reviews of the\nproduct. The model is also conditioned on review properties that are directly\nrelated to summaries; the properties are derived from reviews with no manual\neffort. In the second stage, we fine-tune a plug-in module that learns to\npredict property values on a handful of summaries. This lets us switch the\ngenerator to the summarization mode. We show on Amazon and Yelp datasets that\nour approach substantially outperforms previous extractive and abstractive\nmethods in automatic and human evaluation.", "published": "2020-04-30 15:37:38", "link": "http://arxiv.org/abs/2004.14884v3", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "abstract": "We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models.", "published": "2020-04-30 17:06:23", "link": "http://arxiv.org/abs/2004.14958v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Screening Questions for Job Postings", "abstract": "At LinkedIn, we want to create economic opportunity for everyone in the\nglobal workforce. A critical aspect of this goal is matching jobs with\nqualified applicants. To improve hiring efficiency and reduce the need to\nmanually screening each applicant, we develop a new product where recruiters\ncan ask screening questions online so that they can filter qualified candidates\neasily. To add screening questions to all $20$M active jobs at LinkedIn, we\npropose a new task that aims to automatically generate screening questions for\na given job posting. To solve the task of generating screening questions, we\ndevelop a two-stage deep learning model called Job2Questions, where we apply a\ndeep learning model to detect intent from the text description, and then rank\nthe detected intents by their importance based on other contextual features.\nSince this is a new product with no historical data, we employ deep transfer\nlearning to train complex models with limited training data. We launched the\nscreening question product and our AI models to LinkedIn users and observed\nsignificant impact in the job marketplace. During our online A/B test, we\nobserved $+53.10\\%$ screening question suggestion acceptance rate, $+22.17\\%$\njob coverage, $+190\\%$ recruiter-applicant interaction, and $+11$ Net Promoter\nScore. In sum, the deployed Job2Questions model helps recruiters to find\nqualified applicants and job seekers to find jobs they are qualified for.", "published": "2020-04-30 17:18:17", "link": "http://arxiv.org/abs/2004.14969v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the\n  Web", "abstract": "Following a navigation instruction such as 'Walk down the stairs and stop at\nthe brown sofa' requires embodied AI agents to ground scene elements referenced\nvia language (e.g. 'stairs') to visual content in the environment (pixels\ncorresponding to 'stairs').\n  We ask the following question -- can we leverage abundant 'disembodied'\nweb-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn\nvisual groundings (what do 'stairs' look like?) that improve performance on a\nrelatively data-starved embodied perception task (Vision-and-Language\nNavigation)? Specifically, we develop VLN-BERT, a visiolinguistic\ntransformer-based model for scoring the compatibility between an instruction\n('...stop at the brown sofa') and a sequence of panoramic RGB images captured\nby the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from\nthe web before fine-tuning on embodied path-instruction data significantly\nimproves performance on VLN -- outperforming the prior state-of-the-art in the\nfully-observed setting by 4 absolute percentage points on success rate.\nAblations of our pretraining curriculum show each stage to be impactful -- with\ntheir combination resulting in further positive synergistic effects.", "published": "2020-04-30 17:22:40", "link": "http://arxiv.org/abs/2004.14973v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Investigating Transferability in Pretrained Language Models", "abstract": "How does language model pretraining help transfer learning? We consider a\nsimple ablation technique for determining the impact of each pretrained layer\non transfer task performance. This method, partial reinitialization, involves\nreplacing different layers of a pretrained model with random weights, then\nfinetuning the entire model on the transfer task and observing the change in\nperformance. This technique reveals that in BERT, layers with high probing\nperformance on downstream GLUE tasks are neither necessary nor sufficient for\nhigh accuracy on those tasks. Furthermore, the benefit of using pretrained\nparameters for a layer varies dramatically with finetuning dataset size:\nparameters that provide tremendous performance improvement when data is\nplentiful may provide negligible benefits in data-scarce settings. These\nresults reveal the complexity of the transfer learning process, highlighting\nthe limitations of methods that operate on frozen models or single data\nsamples.", "published": "2020-04-30 17:23:19", "link": "http://arxiv.org/abs/2004.14975v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems", "abstract": "Adversaries may look to steal or attack black-box NLP systems, either for\nfinancial gain or to exploit model errors. One setting of particular interest\nis machine translation (MT), where models have high commercial value and errors\ncan be costly. We investigate possible exploits of black-box MT systems and\nexplore a preliminary defense against such threats. We first show that MT\nsystems can be stolen by querying them with monolingual sentences and training\nmodels to imitate their outputs. Using simulated experiments, we demonstrate\nthat MT model stealing is possible even when imitation models have different\ninput data or architectures than their target models. Applying these ideas, we\ntrain imitation models that reach within 0.6 BLEU of three production MT\nsystems on both high-resource and low-resource language pairs. We then leverage\nthe similarity of our imitation models to transfer adversarial examples to the\nproduction systems. We use gradient-based attacks that expose inputs which lead\nto semantically-incorrect translations, dropped content, and vulgar model\noutputs. To mitigate these vulnerabilities, we propose a defense that modifies\ntranslation outputs in order to misdirect the optimization of imitation models.\nThis defense degrades the adversary's BLEU score and attack success rate at\nsome cost in the defender's BLEU and inference speed.", "published": "2020-04-30 17:56:49", "link": "http://arxiv.org/abs/2004.15015v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MuSe 2020 -- The First International Multimodal Sentiment Analysis in\n  Real-life Media Challenge and Workshop", "abstract": "Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a\nChallenge-based Workshop focusing on the tasks of sentiment recognition, as\nwell as emotion-target engagement and trustworthiness detection by means of\nmore comprehensively integrating the audio-visual and language modalities. The\npurpose of MuSe 2020 is to bring together communities from different\ndisciplines; mainly, the audio-visual emotion recognition community\n(signal-based), and the sentiment analysis community (symbol-based). We present\nthree distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion\n(arousal and valence) prediction; MuSe-Topic, in which participants recognise\ndomain-specific topics as the target of 3-class (low, medium, high) emotions;\nand MuSe-Trust, in which the novel aspect of trustworthiness is to be\npredicted. In this paper, we provide detailed information on MuSe-CaR, the\nfirst of its kind in-the-wild database, which is utilised for the challenge, as\nwell as the state-of-the-art features and modelling approaches applied. For\neach sub-challenge, a competitive baseline for participants is set; namely, on\ntest we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for\nMuSe-Topic a score (computed as 0.34$\\cdot$ UAR + 0.66$\\cdot$F1) of 76.78 % on\nthe 10-class topic and 40.64 % on the 3-class emotion prediction, and for\nMuSe-Trust a CCC of .4359.", "published": "2020-04-30 15:54:22", "link": "http://arxiv.org/abs/2004.14858v3", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural\n  Text-to-Speech", "abstract": "Prosody Transfer (PT) is a technique that aims to use the prosody from a\nsource audio as a reference while synthesising speech. Fine-grained PT aims at\ncapturing prosodic aspects like rhythm, emphasis, melody, duration, and\nloudness, from a source audio at a very granular level and transferring them\nwhen synthesising speech in a different target speaker's voice. Current\napproaches for fine-grained PT suffer from source speaker leakage, where the\nsynthesised speech has the voice identity of the source speaker as opposed to\nthe target speaker. In order to mitigate this issue, they compromise on the\nquality of PT. In this paper, we propose CopyCat, a novel, many-to-many PT\nsystem that is robust to source speaker leakage, without using parallel data.\nWe achieve this through a novel reference encoder architecture capable of\ncapturing temporal prosodic representations which are robust to source speaker\nleakage. We compare CopyCat against a state-of-the-art fine-grained PT model\nthrough various subjective evaluations, where we show a relative improvement of\n$47\\%$ in the quality of prosody transfer and $14\\%$ in preserving the target\nspeaker identity, while still maintaining the same naturalness.", "published": "2020-04-30 07:42:29", "link": "http://arxiv.org/abs/2004.14617v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Domain Adaptation for Acoustic Scene Classification Using\n  Band-Wise Statistics Matching", "abstract": "The performance of machine learning algorithms is known to be negatively\naffected by possible mismatches between training (source) and test (target)\ndata distributions. In fact, this problem emerges whenever an acoustic scene\nclassification system which has been trained on data recorded by a given device\nis applied to samples acquired under different acoustic conditions or captured\nby mismatched recording devices. To address this issue, we propose an\nunsupervised domain adaptation method that consists of aligning the first- and\nsecond-order sample statistics of each frequency band of target-domain acoustic\nscenes to the ones of the source-domain training dataset. This model-agnostic\napproach is devised to adapt audio samples from unseen devices before they are\nfed to a pre-trained classifier, thus avoiding any further learning phase.\nUsing the DCASE 2018 Task 1-B development dataset, we show that the proposed\nmethod outperforms the state-of-the-art unsupervised methods found in the\nliterature in terms of both source- and target-domain classification accuracy.", "published": "2020-04-30 23:56:05", "link": "http://arxiv.org/abs/2005.00145v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Jukebox: A Generative Model for Music", "abstract": "We introduce Jukebox, a model that generates music with singing in the raw\naudio domain. We tackle the long context of raw audio using a multi-scale\nVQ-VAE to compress it to discrete codes, and modeling those using\nautoregressive Transformers. We show that the combined model at scale can\ngenerate high-fidelity and diverse songs with coherence up to multiple minutes.\nWe can condition on artist and genre to steer the musical and vocal style, and\non unaligned lyrics to make the singing more controllable. We are releasing\nthousands of non cherry-picked samples at https://jukebox.openai.com, along\nwith model weights and code at https://github.com/openai/jukebox", "published": "2020-04-30 09:02:45", "link": "http://arxiv.org/abs/2005.00341v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A convolutional neural-network model of human cochlear mechanics and\n  filter tuning for real-time applications", "abstract": "Auditory models are commonly used as feature extractors for automatic\nspeech-recognition systems or as front-ends for robotics, machine-hearing and\nhearing-aid applications. Although auditory models can capture the biophysical\nand nonlinear properties of human hearing in great detail, these biophysical\nmodels are computationally expensive and cannot be used in real-time\napplications. We present a hybrid approach where convolutional neural networks\nare combined with computational neuroscience to yield a real-time end-to-end\nmodel for human cochlear mechanics, including level-dependent filter tuning\n(CoNNear). The CoNNear model was trained on acoustic speech material and its\nperformance and applicability were evaluated using (unseen) sound stimuli\ncommonly employed in cochlear mechanics research. The CoNNear model accurately\nsimulates human cochlear frequency selectivity and its dependence on sound\nintensity, an essential quality for robust speech intelligibility at negative\nspeech-to-background-noise ratios. The CoNNear architecture is based on\nparallel and differentiable computations and has the power to achieve real-time\nhuman performance. These unique CoNNear features will enable the next\ngeneration of human-like machine-hearing applications.", "published": "2020-04-30 14:43:03", "link": "http://arxiv.org/abs/2004.14832v4", "categories": ["eess.AS", "cs.CE", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
