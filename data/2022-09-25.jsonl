{"title": "Neural inhibition during speech planning contributes to contrastive\n  hyperarticulation", "abstract": "Previous work has demonstrated that words are hyperarticulated on dimensions\nof speech that differentiate them from a minimal pair competitor. This\nphenomenon has been termed contrastive hyperarticulation (CH). We present a\ndynamic neural field (DNF) model of voice onset time (VOT) planning that\nderives CH from an inhibitory influence of the minimal pair competitor during\nplanning. We test some predictions of the model with a novel experiment\ninvestigating CH of voiceless stop consonant VOT in pseudowords. The results\ndemonstrate a CH effect in pseudowords, consistent with a basis for the effect\nin the real-time planning and production of speech. The scope and magnitude of\nCH in pseudowords was reduced compared to CH in real words, consistent with a\nrole for interactive activation between lexical and phonological levels of\nplanning. We discuss the potential of our model to unify an apparently\ndisparate set of phenomena, from CH to phonological neighborhood effects to\nphonetic trace effects in speech errors.", "published": "2022-09-25 17:54:59", "link": "http://arxiv.org/abs/2209.12278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can We Automate the Analysis of Online Child Sexual Exploitation\n  Discourse?", "abstract": "Social media's growing popularity raises concerns around children's online\nsafety. Interactions between minors and adults with predatory intentions is a\nparticularly grave concern. Research into online sexual grooming has often\nrelied on domain experts to manually annotate conversations, limiting both\nscale and scope. In this work, we test how well-automated methods can detect\nconversational behaviors and replace an expert human annotator. Informed by\npsychological theories of online grooming, we label $6772$ chat messages sent\nby child-sex offenders with one of eleven predatory behaviors. We train\nbag-of-words and natural language inference models to classify each behavior,\nand show that the best performing models classify behaviors in a manner that is\nconsistent, but not on-par, with human annotation.", "published": "2022-09-25 21:18:50", "link": "http://arxiv.org/abs/2209.12320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WinoDict: Probing language models for in-context word acquisition", "abstract": "We introduce a new in-context learning paradigm to measure Large Language\nModels' (LLMs) ability to learn novel words during inference. In particular, we\nrewrite Winograd-style co-reference resolution problems by replacing the key\nconcept word with a synthetic but plausible word that the model must understand\nto complete the task. Solving this task requires the model to make use of the\ndictionary definition of the new word given in the prompt. This benchmark\naddresses word acquisition, one important aspect of the diachronic degradation\nknown to afflict LLMs. As LLMs are frozen in time at the moment they are\ntrained, they are normally unable to reflect the way language changes over\ntime. We show that the accuracy of LLMs compared to the original Winograd tasks\ndecreases radically in our benchmark, thus identifying a limitation of current\nmodels and providing a benchmark to measure future improvements in LLMs ability\nto do in-context learning.", "published": "2022-09-25 05:30:13", "link": "http://arxiv.org/abs/2209.12153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Re-contextualizing Fairness in NLP: The Case of India", "abstract": "Recent research has revealed undesirable biases in NLP data and models.\nHowever, these efforts focus on social disparities in West, and are not\ndirectly portable to other geo-cultural contexts. In this paper, we focus on\nNLP fair-ness in the context of India. We start with a brief account of the\nprominent axes of social disparities in India. We build resources for fairness\nevaluation in the Indian context and use them to demonstrate prediction biases\nalong some of the axes. We then delve deeper into social stereotypes for Region\nandReligion, demonstrating its prevalence in corpora and models. Finally, we\noutline a holistic research agenda to re-contextualize NLP fairness research\nfor the Indian context, ac-counting for Indian societal context, bridging\ntechnological gaps in NLP capabilities and re-sources, and adapting to Indian\ncultural values. While we focus on India, this framework can be generalized to\nother geo-cultural contexts.", "published": "2022-09-25 13:56:13", "link": "http://arxiv.org/abs/2209.12226v5", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Diachronic Data Analysis Supports and Refines Conceptual Metaphor Theory", "abstract": "As a contribution to metaphor analysis, we introduce a statistical,\ndata-based investigation with empirical analysis of long-standing conjectures\nand a first-ever empirical exploration of the systematic features of metaphors.\nConversely, this also makes metaphor theory available as a basis of meaning\nemergence that can be quantitatively explored and integrated into the framework\nof NLP.", "published": "2022-09-25 14:39:09", "link": "http://arxiv.org/abs/2209.12234v2", "categories": ["cs.CL", "cs.DM", "91F20, 05C80, 05C90, 05C07, 91C20, 94C15, 68T50", "J.5; I.2.7; I.5.3; I.5.1"], "primary_category": "cs.CL"}
{"title": "Application of Deep Learning in Generating Structured Radiology Reports:\n  A Transformer-Based Technique", "abstract": "Since radiology reports needed for clinical practice and research are written\nand stored in free-text narrations, extraction of relative information for\nfurther analysis is difficult. In these circumstances, natural language\nprocessing (NLP) techniques can facilitate automatic information extraction and\ntransformation of free-text formats to structured data. In recent years, deep\nlearning (DL)-based models have been adapted for NLP experiments with promising\nresults. Despite the significant potential of DL models based on artificial\nneural networks (ANN) and convolutional neural networks (CNN), the models face\nsome limitations to implement in clinical practice. Transformers, another new\nDL architecture, have been increasingly applied to improve the process.\nTherefore, in this study, we propose a transformer-based fine-grained named\nentity recognition (NER) architecture for clinical information extraction. We\ncollected 88 abdominopelvic sonography reports in free-text formats and\nannotated them based on our developed information schema. The text-to-text\ntransfer transformer model (T5) and Scifive, a pre-trained domain-specific\nadaptation of the T5 model, were applied for fine-tuning to extract entities\nand relations and transform the input into a structured format. Our\ntransformer-based model in this study outperformed previously applied\napproaches such as ANN and CNN models based on ROUGE-1, ROUGE-2, ROUGE-L, and\nBLEU scores of 0.816, 0.668, 0.528, and 0.743, respectively, while providing an\ninterpretable structured report.", "published": "2022-09-25 08:03:15", "link": "http://arxiv.org/abs/2209.12177v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Cross-X Transfer for Legal Judgment Prediction", "abstract": "Cross-lingual transfer learning has proven useful in a variety of Natural\nLanguage Processing (NLP) tasks, but it is understudied in the context of legal\nNLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer\nlearning techniques on LJP using the trilingual Swiss-Judgment-Prediction\ndataset, including cases written in three languages. We find that cross-lingual\ntransfer improves the overall results across languages, especially when we use\nadapter-based fine-tuning. Finally, we further improve the model's performance\nby augmenting the training dataset with machine-translated versions of the\noriginal documents, using a 3x larger training corpus. Further on, we perform\nan analysis exploring the effect of cross-domain and cross-regional transfer,\ni.e., train a model across domains (legal areas), or regions. We find that in\nboth settings (legal areas, origin regions), models trained across all groups\nperform overall better, while they also have improved results in the worst-case\nscenarios. Finally, we report improved results when we ambitiously apply\ncross-jurisdiction transfer, where we further augment our dataset with Indian\nlegal cases.", "published": "2022-09-25 21:41:56", "link": "http://arxiv.org/abs/2209.12325v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Multimodal Exponentially Modified Gaussian Oscillators", "abstract": "Acoustic modeling serves audio processing tasks such as de-noising, data\nreconstruction, model-based testing and classification. Previous work dealt\nwith signal parameterization of wave envelopes either by multiple Gaussian\ndistributions or a single asymmetric Gaussian curve, which both fall short in\nrepresenting super-imposed echoes sufficiently well. This study presents a\nthree-stage Multimodal Exponentially Modified Gaussian (MEMG) model with an\noptional oscillating term that regards captured echoes as a superposition of\nunivariate probability distributions in the temporal domain. With this,\nsynthetic ultrasound signals suffering from artifacts can be fully recovered,\nwhich is backed by quantitative assessment. Real data experimentation is\ncarried out to demonstrate the classification capability of the acquired\nfeatures with object reflections being detected at different points in time.\nThe code is available at https://github.com/hahnec/multimodal_emg.", "published": "2022-09-25 11:48:09", "link": "http://arxiv.org/abs/2209.12202v6", "categories": ["cs.SD", "cs.CV", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
