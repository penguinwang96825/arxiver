{"title": "Language Tags Matter for Zero-Shot Neural Machine Translation", "abstract": "Multilingual Neural Machine Translation (MNMT) has aroused widespread\ninterest due to its efficiency. An exciting advantage of MNMT models is that\nthey could also translate between unsupervised (zero-shot) language directions.\nLanguage tag (LT) strategies are often adopted to indicate the translation\ndirections in MNMT. In this paper, we demonstrate that the LTs are not only\nindicators for translation directions but also crucial to zero-shot translation\nqualities. Unfortunately, previous work tends to ignore the importance of LT\nstrategies. We demonstrate that a proper LT strategy could enhance the\nconsistency of semantic representations and alleviate the off-target issue in\nzero-shot directions. Experimental results show that by ignoring the source\nlanguage tag (SLT) and adding the target language tag (TLT) to the encoder, the\nzero-shot translations could achieve a +8 BLEU score difference over other LT\nstrategies in IWSLT17, Europarl, TED talks translation tasks.", "published": "2021-06-15 07:32:36", "link": "http://arxiv.org/abs/2106.07930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT Embeddings for Automatic Readability Assessment", "abstract": "Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels with handcrafted linguistic features through a combined method for\nreadability assessment. Results show that the proposed method outperforms\nclassical approaches in readability assessment using English and Filipino\ndatasets, obtaining as high as 12.4% increase in F1 performance. We also show\nthat the general information encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.", "published": "2021-06-15 07:37:48", "link": "http://arxiv.org/abs/2106.07935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling morphology with Linear Discriminative Learning: considerations\n  and design choices", "abstract": "This study addresses a series of methodological questions that arise when\nmodeling inflectional morphology with Linear Discriminative Learning. Taking\nthe semi-productive German noun system as example, we illustrate how decisions\nmade about the representation of form and meaning influence model performance.\nWe clarify that for modeling frequency effects in learning, it is essential to\nmake use of incremental learning rather than the endstate of learning. We also\ndiscuss how the model can be set up to approximate the learning of inflected\nwords in context. In addition, we illustrate how in this approach the wug task\ncan be modeled in considerable detail. In general, the model provides an\nexcellent memory for known words, but appropriately shows more limited\nperformance for unseen data, in line with the semi-productivity of German noun\ninflection and generalization performance of native German speakers.", "published": "2021-06-15 07:37:52", "link": "http://arxiv.org/abs/2106.07936v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deriving Word Vectors from Contextualized Language Models using\n  Topic-Aware Mention Selection", "abstract": "One of the long-standing challenges in lexical semantics consists in learning\nrepresentations of words which reflect their semantic properties. The\nremarkable success of word embeddings for this purpose suggests that\nhigh-quality representations can be obtained by summarizing the sentence\ncontexts of word mentions. In this paper, we propose a method for learning word\nrepresentations that follows this basic strategy, but differs from standard\nword embeddings in two important ways. First, we take advantage of\ncontextualized language models (CLMs) rather than bags of word vectors to\nencode contexts. Second, rather than learning a word vector directly, we use a\ntopic model to partition the contexts in which words appear, and then learn\ndifferent topic-specific vectors for each word. Finally, we use a task-specific\nsupervision signal to make a soft selection of the resulting vectors. We show\nthat this simple strategy leads to high-quality word vectors, which are more\npredictive of semantic properties than word embeddings and existing CLM-based\nstrategies.", "published": "2021-06-15 08:02:42", "link": "http://arxiv.org/abs/2106.07947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARTA: Collection and Classification of Ambiguous Requests and Thoughtful\n  Actions", "abstract": "Human-assisting systems such as dialogue systems must take thoughtful,\nappropriate actions not only for clear and unambiguous user requests, but also\nfor ambiguous user requests, even if the users themselves are not aware of\ntheir potential requirements. To construct such a dialogue agent, we collected\na corpus and developed a model that classifies ambiguous user requests into\ncorresponding system actions. In order to collect a high-quality corpus, we\nasked workers to input antecedent user requests whose pre-defined actions could\nbe regarded as thoughtful. Although multiple actions could be identified as\nthoughtful for a single user request, annotating all combinations of user\nrequests and system actions is impractical. For this reason, we fully annotated\nonly the test data and left the annotation of the training data incomplete. In\norder to train the classification model on such training data, we applied the\npositive/unlabeled (PU) learning method, which assumes that only a part of the\ndata is labeled with positive examples. The experimental results show that the\nPU learning method achieved better performance than the general\npositive/negative (PN) learning method to classify thoughtful actions given an\nambiguous user request.", "published": "2021-06-15 09:28:39", "link": "http://arxiv.org/abs/2106.07999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Possible, the Plausible, and the Desirable: Event-Based Modality\n  Detection for Language Processing", "abstract": "Modality is the linguistic ability to describe events with added information\nsuch as how desirable, plausible, or feasible they are. Modality is important\nfor many NLP downstream tasks such as the detection of hedging, uncertainty,\nspeculation, and more. Previous studies that address modality detection in NLP\noften restrict modal expressions to a closed syntactic class, and the modal\nsense labels are vastly different across different studies, lacking an accepted\nstandard. Furthermore, these senses are often analyzed independently of the\nevents that they modify. This work builds on the theoretical foundations of the\nGeorgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to\npropose an event-based modality detection task where modal expressions can be\nwords of any syntactic class and sense labels are drawn from a comprehensive\ntaxonomy which harmonizes the modal concepts contributed by the different\nstudies. We present experiments on the GME corpus aiming to detect and classify\nfine-grained modal concepts and associate them with their modified events. We\nshow that detecting and classifying modal expressions is not only feasible, but\nalso improves the detection of modal events in their own right.", "published": "2021-06-15 10:47:57", "link": "http://arxiv.org/abs/2106.08037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Maximum Spanning Trees Are Invariant to Temperature Scaling in\n  Graph-based Dependency Parsing", "abstract": "Modern graph-based syntactic dependency parsers operate by predicting, for\neach token within a sentence, a probability distribution over its possible\nsyntactic heads (i.e., all other tokens) and then extracting a maximum spanning\ntree from the resulting log-probabilities. Nowadays, virtually all such parsers\nutilize deep neural networks and may thus be susceptible to miscalibration (in\nparticular, overconfident predictions). In this paper, we prove that\ntemperature scaling, a popular technique for post-hoc calibration of neural\nnetworks, cannot change the output of the aforementioned procedure. We conclude\nthat other techniques are needed to tackle miscalibration in graph-based\ndependency parsers in a way that improves parsing accuracy.", "published": "2021-06-15 13:57:24", "link": "http://arxiv.org/abs/2106.08159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direction is what you need: Improving Word Embedding Compression in\n  Large Language Models", "abstract": "The adoption of Transformer-based models in natural language processing (NLP)\nhas led to great success using a massive number of parameters. However, due to\ndeployment constraints in edge devices, there has been a rising interest in the\ncompression of these models to improve their inference time and memory\nfootprint. This paper presents a novel loss objective to compress token\nembeddings in the Transformer-based models by leveraging an AutoEncoder\narchitecture. More specifically, we emphasize the importance of the direction\nof compressed embeddings with respect to original uncompressed embeddings. The\nproposed method is task-agnostic and does not require further language modeling\npre-training. Our method significantly outperforms the commonly used SVD-based\nmatrix-factorization approach in terms of initial language model Perplexity.\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\nin most scenarios. Our code is public.", "published": "2021-06-15 14:28:00", "link": "http://arxiv.org/abs/2106.08181v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering Infused Pre-training of General-Purpose\n  Contextualized Representations", "abstract": "We propose a pre-training objective based on question answering (QA) for\nlearning general-purpose contextual representations, motivated by the intuition\nthat the representation of a phrase in a passage should encode all questions\nthat the phrase can answer in context. To this end, we train a bi-encoder QA\nmodel, which independently encodes passages and questions, to match the\npredictions of a more accurate cross-encoder model on 80 million synthesized QA\npairs. By encoding QA-relevant information, the bi-encoder's token-level\nrepresentations are useful for non-QA downstream tasks without extensive (or in\nsome cases, any) fine-tuning. We show large improvements over both\nRoBERTa-large and previous state-of-the-art results on zero-shot and few-shot\nparaphrase detection on four datasets, few-shot named entity recognition on two\ndatasets, and zero-shot sentiment analysis on three datasets.", "published": "2021-06-15 14:45:15", "link": "http://arxiv.org/abs/2106.08190v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency Regularization for Cross-Lingual Fine-Tuning", "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer\ntask-specific supervision from one language to the others. In this work, we\npropose to improve cross-lingual fine-tuning with consistency regularization.\nSpecifically, we use example consistency regularization to penalize the\nprediction sensitivity to four types of data augmentations, i.e., subword\nsampling, Gaussian noise, code-switch substitution, and machine translation. In\naddition, we employ model consistency to regularize the models trained with two\naugmented versions of the same training set. Experimental results on the XTREME\nbenchmark show that our method significantly improves cross-lingual fine-tuning\nacross various tasks, including text classification, question answering, and\nsequence labeling.", "published": "2021-06-15 15:35:44", "link": "http://arxiv.org/abs/2106.08226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three-part diachronic semantic change dataset for Russian", "abstract": "We present a manually annotated lexical semantic change dataset for Russian:\nRuShiftEval. Its novelty is ensured by a single set of target words annotated\nfor their diachronic semantic shifts across three time periods, while the\nprevious work either used only two time periods, or different sets of target\nwords. The paper describes the composition and annotation procedure for the\ndataset. In addition, it is shown how the ternary nature of RuShiftEval allows\nto trace specific diachronic trajectories: `changed at a particular time period\nand stable afterwards' or `was changing throughout all time periods'. Based on\nthe analysis of the submissions to the recent shared task on semantic change\ndetection for Russian, we argue that correctly identifying such trajectories\ncan be an interesting sub-task itself.", "published": "2021-06-15 17:12:25", "link": "http://arxiv.org/abs/2106.08294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Enrichment of Persona-grounded Dialog with Background\n  Stories", "abstract": "Humans often refer to personal narratives, life experiences, and events to\nmake a conversation more engaging and rich. While persona-grounded dialog\nmodels are able to generate responses that follow a given persona, they often\nmiss out on stating detailed experiences or events related to a persona, often\nleaving conversations shallow and dull. In this work, we equip dialog models\nwith 'background stories' related to a persona by leveraging fictional\nnarratives from existing story datasets (e.g. ROCStories). Since current dialog\ndatasets do not contain such narratives as responses, we perform an\nunsupervised adaptation of a retrieved story for generating a dialog response\nusing a gradient-based rewriting technique. Our proposed method encourages the\ngenerated response to be fluent (i.e., highly likely) with the dialog history,\nminimally different from the retrieved story to preserve event ordering and\nconsistent with the original persona. We demonstrate that our method can\ngenerate responses that are more diverse, and are rated more engaging and\nhuman-like by human evaluators, compared to outputs from existing dialog\nmodels.", "published": "2021-06-15 18:20:27", "link": "http://arxiv.org/abs/2106.08364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Context Features Can Transformer Language Models Use?", "abstract": "Transformer-based language models benefit from conditioning on contexts of\nhundreds to thousands of previous tokens. What aspects of these contexts\ncontribute to accurate model prediction? We describe a series of experiments\nthat measure usable information by selectively ablating lexical and structural\ninformation in transformer language models trained on English Wikipedia. In\nboth mid- and long-range contexts, we find that several extremely destructive\ncontext manipulations -- including shuffling word order within sentences and\ndeleting all words other than nouns -- remove less than 15% of the usable\ninformation. Our results suggest that long contexts, but not their detailed\nsyntactic and propositional content, are important for the low perplexity of\ncurrent transformer language models.", "published": "2021-06-15 18:38:57", "link": "http://arxiv.org/abs/2106.08367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges and Considerations with Code-Mixed NLP for Multilingual\n  Societies", "abstract": "Multilingualism refers to the high degree of proficiency in two or more\nlanguages in the written and oral communication modes. It often results in\nlanguage mixing, a.k.a. code-mixing, when a multilingual speaker switches\nbetween multiple languages in a single utterance of a text or speech. This\npaper discusses the current state of the NLP research, limitations, and\nforeseeable pitfalls in addressing five real-world applications for social good\ncrisis management, healthcare, political campaigning, fake news, and hate\nspeech for multilingual societies. We also propose futuristic datasets, models,\nand tools that can significantly advance the current research in multilingual\nNLP applications for the societal good. As a representative example, we\nconsider English-Hindi code-mixing but draw similar inferences for other\nlanguage pairs", "published": "2021-06-15 00:53:55", "link": "http://arxiv.org/abs/2106.07823v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilateral Personalized Dialogue Generation with Contrastive Learning", "abstract": "Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nBilateral Personalized Dialogue Generation (BPDG) method for dyadic\nconversation, which integrates user and robot personas into dialogue generation\nvia designing a dynamic persona-aware fusion method. To bridge the gap between\nthe learning objective function and evaluation metrics, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted with contrastive learning to\nselect the proper response from the generated candidates. Moreover, a bilateral\npersona accuracy metric is designed to measure the degree of bilateral\npersonalization. Experimental results demonstrate that, compared with several\nstate-of-the-art methods, the final results of the proposed method are more\npersonalized and consistent with bilateral personas in terms of both automatic\nand manual evaluations.", "published": "2021-06-15 03:21:19", "link": "http://arxiv.org/abs/2106.07857v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An enriched category theory of language: from syntax to semantics", "abstract": "State of the art language models return a natural language text continuation\nfrom any piece of input text. This ability to generate coherent text extensions\nimplies significant sophistication, including a knowledge of grammar and\nsemantics. In this paper, we propose a mathematical framework for passing from\nprobability distributions on extensions of given texts, such as the ones\nlearned by today's large language models, to an enriched category containing\nsemantic information. Roughly speaking, we model probability distributions on\ntexts as a category enriched over the unit interval. Objects of this category\nare expressions in language, and hom objects are conditional probabilities that\none expression is an extension of another. This category is syntactical -- it\ndescribes what goes with what. Then, via the Yoneda embedding, we pass to the\nenriched category of unit interval-valued copresheaves on this syntactical\ncategory. This category of enriched copresheaves is semantic -- it is where we\nfind meaning, logical operations such as entailment, and the building blocks\nfor more elaborate semantic concepts.", "published": "2021-06-15 05:40:51", "link": "http://arxiv.org/abs/2106.07890v2", "categories": ["math.CT", "cs.CL"], "primary_category": "math.CT"}
{"title": "An Automated Quality Evaluation Framework of Psychotherapy Conversations\n  with Local Quality Estimates", "abstract": "Text-based computational approaches for assessing the quality of\npsychotherapy are being developed to support quality assurance and clinical\ntraining. However, due to the long durations of typical conversation based\ntherapy sessions, and due to limited annotated modeling resources,\ncomputational methods largely rely on frequency-based lexical features or\ndialogue acts to assess the overall session level characteristics. In this\nwork, we propose a hierarchical framework to automatically evaluate the quality\nof transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the\nrichly dynamic nature of the spoken dialog within a talk therapy session, to\nevaluate the overall session level quality, we propose to consider modeling it\nas a function of local variations across the interaction. To implement that\nempirically, we divide each psychotherapy session into conversation segments\nand initialize the segment-level qualities with the session-level scores.\nFirst, we produce segment embeddings by fine-tuning a BERT-based model, and\npredict segment-level (local) quality scores. These embeddings are used as the\nlower-level input to a Bidirectional LSTM-based neural network to predict the\nsession-level (global) quality estimates. In particular, we model the global\nquality as a linear function of the local quality scores, which allows us to\nupdate the segment-level quality estimates based on the session-level quality\nprediction. These newly estimated segment-level scores benefit the BERT\nfine-tuning process, which in turn results in better segment embeddings. We\nevaluate the proposed framework on automatically derived transcriptions from\nreal-world CBT clinical recordings to predict session-level behavior codes. The\nresults indicate that our approach leads to improved evaluation accuracy for\nmost codes when used for both regression and classification tasks.", "published": "2021-06-15 07:18:30", "link": "http://arxiv.org/abs/2106.07922v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Incorporating Word Sense Disambiguation in Neural Language Models", "abstract": "We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.", "published": "2021-06-15 08:44:08", "link": "http://arxiv.org/abs/2106.07967v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CausalNLP: A Practical Toolkit for Causal Inference with Text", "abstract": "Causal inference is the process of estimating the effect or impact of a\ntreatment on an outcome with other covariates as potential confounders (and\nmediators) that may need to be controlled. The vast majority of existing\nmethods and systems for causal inference assume that all variables under\nconsideration are categorical or numerical (e.g., gender, price, enrollment).\nIn this paper, we present CausalNLP, a toolkit for inferring causality with\nobservational data that includes text in addition to traditional numerical and\ncategorical variables. CausalNLP employs the use of meta learners for treatment\neffect estimation and supports using raw text and its linguistic properties as\na treatment, an outcome, or a \"controlled-for\" variable (e.g., confounder). The\nlibrary is open source and available at: https://github.com/amaiya/causalnlp.", "published": "2021-06-15 10:57:44", "link": "http://arxiv.org/abs/2106.08043v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence-Level Training for Non-Autoregressive Neural Machine\n  Translation", "abstract": "In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this article, we propose using sequence-level training objectives\nto train NAT models, which evaluate the NAT outputs as a whole and correlates\nwell with the real translation quality. Firstly, we propose training NAT models\nto optimize sequence-level evaluation metrics (e.g., BLEU) based on several\nnovel reinforcement algorithms customized for NAT, which outperforms the\nconventional method by reducing the variance of gradient estimation. Secondly,\nwe introduce a novel training objective for NAT models, which aims to minimize\nthe Bag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks. The\nsource code is available at https://github.com/ictnlp/Seq-NAT.", "published": "2021-06-15 13:30:09", "link": "http://arxiv.org/abs/2106.08122v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PairConnect: A Compute-Efficient MLP Alternative to Attention", "abstract": "Transformer models have demonstrated superior performance in natural language\nprocessing. The dot product self-attention in Transformer allows us to model\ninteractions between words. However, this modeling comes with significant\ncomputational overhead. In this work, we revisit the memory-compute trade-off\nassociated with Transformer, particularly multi-head attention, and show a\nmemory-heavy but significantly more compute-efficient alternative to\nTransformer. Our proposal, denoted as PairConnect, a multilayer perceptron\n(MLP), models the pairwise interaction between words by explicit pairwise word\nembeddings. As a result, PairConnect substitutes self dot product with a simple\nembedding lookup. We show mathematically that despite being an MLP, our\ncompute-efficient PairConnect is strictly more expressive than Transformer. Our\nexperiment on language modeling tasks suggests that PairConnect could achieve\ncomparable results with Transformer while reducing the computational cost\nassociated with inference significantly.", "published": "2021-06-15 15:39:45", "link": "http://arxiv.org/abs/2106.08235v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interpretable Self-supervised Multi-task Learning for COVID-19\n  Information Retrieval and Extraction", "abstract": "The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.", "published": "2021-06-15 16:01:44", "link": "http://arxiv.org/abs/2106.08252v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Generative Conversational Networks", "abstract": "Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.", "published": "2021-06-15 23:19:37", "link": "http://arxiv.org/abs/2106.08484v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Teacher-Student MixIT for Unsupervised and Semi-supervised Speech\n  Separation", "abstract": "In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.", "published": "2021-06-15 02:26:42", "link": "http://arxiv.org/abs/2106.07843v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Margin Circle Loss for Speaker Verification", "abstract": "Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular\nsoftmax loss with margin penalties toenhance the intra-class compactness of\nspeaker embeddings,which achieved remarkable performance. In this paper, we\npro-pose a novel angular loss function called adaptive margin cir-cle loss for\nspeaker verification. The stage-based margin andchunk-based margin are applied\nto improve the angular discrim-ination of circle loss on the training set. The\nanalysis on gradi-ents shows that, compared with the previous angular loss\nlikeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization\nand definite convergence status. Experimentsare carried out on the Voxceleb and\nSITW. By applying adap-tive margin circle loss, our best system achieves\n1.31%EER onVoxceleb1 and 2.13% on SITW core-core.", "published": "2021-06-15 09:35:59", "link": "http://arxiv.org/abs/2106.08004v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Abstractive Opinion Summarization by Generating Sentences\n  with Tree-Structured Topic Guidance", "abstract": "This paper presents a novel unsupervised abstractive summarization method for\nopinionated texts. While the basic variational autoencoder-based models assume\na unimodal Gaussian prior for the latent code of sentences, we alternate it\nwith a recursive Gaussian mixture, where each mixture component corresponds to\nthe latent code of a topic sentence and is mixed by a tree-structured topic\ndistribution. By decoding each Gaussian component, we generate sentences with\ntree-structured topic guidance, where the root sentence conveys generic\ncontent, and the leaf sentences describe specific topics. Experimental results\ndemonstrate that the generated topic sentences are appropriate as a summary of\nopinionated texts, which are more informative and cover more input contents\nthan those generated by the recent unsupervised summarization model\n(Bra\\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of\nlatent Gaussians represents the granularity of sentences, analogous to Gaussian\nword embedding (Vilnis and McCallum, 2015).", "published": "2021-06-15 09:37:04", "link": "http://arxiv.org/abs/2106.08007v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SSMix: Saliency-Based Span Mixup for Text Classification", "abstract": "Data augmentation with mixup has shown to be effective on various computer\nvision tasks. Despite its great success, there has been a hurdle to apply mixup\nto NLP tasks since text consists of discrete tokens with variable length. In\nthis work, we propose SSMix, a novel mixup method where the operation is\nperformed on input text rather than on hidden vectors like previous approaches.\nSSMix synthesizes a sentence while preserving the locality of two original\ntexts by span-based mixing and keeping more tokens related to the prediction\nrelying on saliency information. With extensive experiments, we empirically\nvalidate that our method outperforms hidden-level mixup methods on a wide range\nof text classification benchmarks, including textual entailment, sentiment\nclassification, and question-type classification. Our code is available at\nhttps://github.com/clovaai/ssmix.", "published": "2021-06-15 11:40:23", "link": "http://arxiv.org/abs/2106.08062v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark", "abstract": "Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.", "published": "2021-06-15 12:25:30", "link": "http://arxiv.org/abs/2106.08087v6", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Representation and Inference for NLP", "abstract": "Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).", "published": "2021-06-15 13:22:48", "link": "http://arxiv.org/abs/2106.08117v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialectal Speech Recognition and Translation of Swiss German Speech to\n  Standard German Text: Microsoft's Submission to SwissText 2021", "abstract": "This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.", "published": "2021-06-15 13:34:02", "link": "http://arxiv.org/abs/2106.08126v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Graph-based Label Propagation for Semi-Supervised Speaker Identification", "abstract": "Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger set of\nunlabeled data, suggesting semisupervised learning to improve speaker profiles.\nWe propose a graph-based semi-supervised learning approach for speaker\nidentification in the household scenario, to leverage the unlabeled speech\nsamples. In contrast to most of the works in speaker recognition that focus on\nspeaker-discriminative embeddings, this work focuses on speaker label inference\n(scoring). Given a pre-trained embedding extractor, graph-based learning allows\nus to integrate information about both labeled and unlabeled utterances.\nConsidering each utterance as a graph node, we represent pairwise utterance\nsimilarity scores as edge weights. Graphs are constructed per household, and\nspeaker identities are propagated to unlabeled nodes to optimize a global\nconsistency criterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods as\nwell as their semi-supervised variants based on pseudo-labels.", "published": "2021-06-15 15:10:33", "link": "http://arxiv.org/abs/2106.08207v1", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "StockBabble: A Conversational Financial Agent to support Stock Market\n  Investors", "abstract": "We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.", "published": "2021-06-15 17:19:30", "link": "http://arxiv.org/abs/2106.08298v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Code to Comment Translation: A Comparative Study on Model Effectiveness\n  & Errors", "abstract": "Automated source code summarization is a popular software engineering\nresearch topic wherein machine translation models are employed to \"translate\"\ncode snippets into relevant natural language descriptions. Most evaluations of\nsuch models are conducted using automatic reference-based metrics. However,\ngiven the relatively large semantic gap between programming languages and\nnatural language, we argue that this line of research would benefit from a\nqualitative investigation into the various error modes of current\nstate-of-the-art models. Therefore, in this work, we perform both a\nquantitative and qualitative comparison of three recently proposed source code\nsummarization models. In our quantitative evaluation, we compare the models\nbased on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,\nand in our qualitative evaluation, we perform a manual open-coding of the most\ncommon errors committed by the models when compared to ground truth captions.\nOur investigation reveals new insights into the relationship between\nmetric-based performance and model prediction errors grounded in an empirically\nderived error taxonomy that can be used to drive future research efforts", "published": "2021-06-15 20:13:14", "link": "http://arxiv.org/abs/2106.08415v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Pathological voice adaptation with autoencoder-based voice conversion", "abstract": "In this paper, we propose a new approach to pathological speech synthesis.\nInstead of using healthy speech as a source, we customise an existing\npathological speech sample to a new speaker's voice characteristics. This\napproach alleviates the evaluation problem one normally has when converting\ntypical speech to pathological speech, as in our approach, the voice conversion\n(VC) model does not need to be optimised for speech degradation but only for\nthe speaker change. This change in the optimisation ensures that any\ndegradation found in naturalness is due to the conversion process and not due\nto the model exaggerating characteristics of a speech pathology. To show a\nproof of concept of this method, we convert dysarthric speech using the\nUASpeech database and an autoencoder-based VC technique. Subjective evaluation\nresults show reasonable naturalness for high intelligibility dysarthric\nspeakers, though lower intelligibility seems to introduce a marginal\ndegradation in naturalness scores for mid and low intelligibility speakers\ncompared to ground truth. Conversion of speaker characteristics for low and\nhigh intelligibility speakers is successful, but not for mid. Whether the\ndifferences in the results for the different intelligibility levels is due to\nthe intelligibility levels or due to the speakers needs to be further\ninvestigated.", "published": "2021-06-15 20:38:10", "link": "http://arxiv.org/abs/2106.08427v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis", "abstract": "This paper introduces RyanSpeech, a new speech corpus for research on\nautomated text-to-speech (TTS) systems. Publicly available TTS corpora are\noften noisy, recorded with multiple speakers, or lack quality male speech data.\nIn order to meet the need for a high quality, publicly available male speech\ncorpus within the field of speech recognition, we have designed and created\nRyanSpeech which contains textual materials from real-world conversational\nsettings. These materials contain over 10 hours of a professional male voice\nactor's speech recorded at 44.1 kHz. This corpus's design and pipeline make\nRyanSpeech ideal for developing TTS systems in real-world applications. To\nprovide a baseline for future research, protocols, and benchmarks, we trained 4\nstate-of-the-art speech models and a vocoder on RyanSpeech. The results show\n3.36 in mean opinion scores (MOS) in our best model. We have made both the\ncorpus and trained models for public use.", "published": "2021-06-15 22:24:38", "link": "http://arxiv.org/abs/2106.08468v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and\n  Multi-Task Language Modeling", "abstract": "Automatic Speech Recognition (ASR) robustness toward slot entities are\ncritical in e-commerce voice assistants that involve monetary transactions and\npurchases. Along with effective domain adaptation, it is intuitive that cross\nutterance contextual cues play an important role in disambiguating domain\nspecific content words from speech. In this paper, we investigate various\ntechniques to improve contextualization, content word robustness and domain\nadaptation of a Transformer-XL neural language model (NLM) to rescore ASR\nN-best hypotheses. To improve contextualization, we utilize turn level dialogue\nacts along with cross utterance context carry over. Additionally, to adapt our\ndomain-general NLM towards e-commerce on-the-fly, we use embeddings derived\nfrom a finetuned masked LM on in-domain data. Finally, to improve robustness\ntowards in-domain content words, we propose a multi-task model that can jointly\nperform content word detection and language modeling tasks. Compared to a\nnon-contextual LSTM LM baseline, our best performing NLM rescorer results in a\ncontent WER reduction of 19.2% on e-commerce audio test set and a slot labeling\nF1 improvement of 6.4%.", "published": "2021-06-15 21:27:34", "link": "http://arxiv.org/abs/2106.09532v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STAN: A stuttering therapy analysis helper", "abstract": "Stuttering is a complex speech disorder identified by repeti-tions,\nprolongations of sounds, syllables or words and blockswhile speaking. Specific\nstuttering behaviour differs strongly,thus needing personalized therapy.\nTherapy sessions requirea high level of concentration by the therapist. We\nintroduceSTAN, a system to aid speech therapists in stuttering therapysessions.\nSuch an automated feedback system can lower thecognitive load on the therapist\nand thereby enable a more con-sistent therapy as well as allowing analysis of\nstuttering overthe span of multiple therapy sessions.", "published": "2021-06-15 13:48:12", "link": "http://arxiv.org/abs/2106.09545v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Textual Data Distributions: Kullback Leibler Textual Distributions\n  Contrasts on GPT-2 Generated Texts, with Supervised, Unsupervised Learning on\n  Vaccine & Market Topics & Sentiment", "abstract": "Efficient textual data distributions (TDD) alignment and generation are open\nresearch problems in textual analytics and NLP. It is presently difficult to\nparsimoniously and methodologically confirm that two or more natural language\ndatasets belong to similar distributions, and to identify the extent to which\ntextual data possess alignment. This study focuses on addressing a segment of\nthe broader problem described above by applying multiple supervised and\nunsupervised machine learning (ML) methods to explore the behavior of TDD by\n(i) topical alignment, and (ii) by sentiment alignment. Furthermore we use\nmultiple text generation methods including fine-tuned GPT-2, to generate text\nby topic and by sentiment. Finally we develop a unique process driven variation\nof Kullback-Leibler divergence (KLD) application to TDD, named KL Textual\nDistributions Contrasts(KL-TDC) to identify the alignment of machine generated\ntextual corpora with naturally occurring textual corpora. This study thus\nidentifies a unique approach for generating and validating TDD by topic and\nsentiment, which can be used to help address sparse data problems and other\nresearch, practice and classroom situations in need of artificially generated\ntopic or sentiment aligned textual data.", "published": "2021-06-15 21:30:46", "link": "http://arxiv.org/abs/2107.02025v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Stable Classifiers by Transferring Unstable Features", "abstract": "While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask for both synthetically generated environments and real-world environments.\nOur code is available at https://github.com/YujiaBao/Tofu.", "published": "2021-06-15 02:41:12", "link": "http://arxiv.org/abs/2106.07847v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "E2E-based Multi-task Learning Approach to Joint Speech and Accent\n  Recognition", "abstract": "In this paper, we propose a single multi-task learning framework to perform\nEnd-to-End (E2E) speech recognition (ASR) and accent recognition (AR)\nsimultaneously. The proposed framework is not only more compact but can also\nyield comparable or even better results than standalone systems. Specifically,\nwe found that the overall performance is predominantly determined by the ASR\ntask, and the E2E-based ASR pretraining is essential to achieve improved\nperformance, particularly for the AR task. Additionally, we conduct several\nanalyses of the proposed method. First, though the objective loss for the AR\ntask is much smaller compared with its counterpart of ASR task, a smaller\nweighting factor with the AR task in the joint objective function is necessary\nto yield better results for each task. Second, we found that sharing only a few\nlayers of the encoder yields better AR results than sharing the overall\nencoder. Experimentally, the proposed method produces WER results close to the\nbest standalone E2E ASR ones, while it achieves 7.7% and 4.2% relative\nimprovement over standalone and single-task-based joint recognition methods on\ntest set for accent recognition respectively.", "published": "2021-06-15 15:17:08", "link": "http://arxiv.org/abs/2106.08211v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Study into Pre-training Strategies for Spoken Language Understanding\n  on Dysarthric Speech", "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems avoid an\nintermediate textual representation by mapping speech directly into intents\nwith slot values. This approach requires considerable domain-specific training\ndata. In low-resource scenarios this is a major concern, e.g., in the present\nstudy dealing with SLU for dysarthric speech. Pretraining part of the SLU model\nfor automatic speech recognition targets helps but no research has shown to\nwhich extent SLU on dysarthric speech benefits from knowledge transferred from\nother dysarthric speech tasks. This paper investigates the efficiency of\npre-training strategies for SLU tasks on dysarthric speech. The designed SLU\nsystem consists of a TDNN acoustic model for feature encoding and a capsule\nnetwork for intent and slot decoding. The acoustic model is pre-trained in two\nstages: initialization with a corpus of normal speech and finetuning on a\nmixture of dysarthric and normal speech. By introducing the intelligibility\nscore as a metric of the impairment severity, this paper quantitatively\nanalyzes the relation between generalization and pathology severity for\ndysarthric speech.", "published": "2021-06-15 17:43:30", "link": "http://arxiv.org/abs/2106.08313v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ADEPT: A Dataset for Evaluating Prosody Transfer", "abstract": "Text-to-speech is now able to achieve near-human naturalness and research\nfocus has shifted to increasing expressivity. One popular method is to transfer\nthe prosody from a reference speech sample. There have been considerable\nadvances in using prosody transfer to generate more expressive speech, but the\nfield lacks a clear definition of what successful prosody transfer means and a\nmethod for measuring it.\n  We introduce a dataset of prosodically-varied reference natural speech\nsamples for evaluating prosody transfer. The samples include global variations\nreflecting emotion and interpersonal attitude, and local variations reflecting\ntopical emphasis, propositional attitude, syntactic phrasing and marked\ntonicity. The corpus only includes prosodic variations that listeners are able\nto distinguish with reasonable accuracy, and we report these figures as a\nbenchmark against which text-to-speech prosody transfer can be compared.\n  We conclude the paper with a demonstration of our proposed evaluation\nmethodology, using the corpus to evaluate two text-to-speech models that\nperform prosody transfer.", "published": "2021-06-15 17:55:05", "link": "http://arxiv.org/abs/2106.08321v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards the Objective Speech Assessment of Smoking Status based on Voice\n  Features: A Review of the Literature", "abstract": "In smoking cessation clinical research and practice, objective validation of\nself-reported smoking status is crucial for ensuring the reliability of the\nprimary outcome, that is, smoking abstinence. Speech signals convey important\ninformation about a speaker, such as age, gender, body size, emotional state,\nand health state. We investigated (1) if smoking could measurably alter voice\nfeatures, (2) if smoking cessation could lead to changes in voice, and\ntherefore (3) if the voice-based smoking status assessment has the potential to\nbe used as an objective smoking cessation validation method.", "published": "2021-06-15 04:20:26", "link": "http://arxiv.org/abs/2106.07874v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram\n  Discriminators for High-Fidelity Waveform Generation", "abstract": "Most neural vocoders employ band-limited mel-spectrograms to generate\nwaveforms. If full-band spectral features are used as the input, the vocoder\ncan be provided with as much acoustic information as possible. However, in some\nmodels employing full-band mel-spectrograms, an over-smoothing problem occurs\nas part of which non-sharp spectrograms are generated. To address this problem,\nwe propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms\nin real time. Inspired by works in the field of voice activity detection, we\nadded a multi-resolution spectrogram discriminator that employs multiple linear\nspectrogram magnitudes computed using various parameter sets. Using full-band\nmel-spectrograms as input, we expect to generate high-resolution signals by\nadding a discriminator that employs spectrograms of multiple resolutions as the\ninput. In an evaluation on a dataset containing information on hundreds of\nspeakers, UnivNet obtained the best objective and subjective results among\ncompeting models for both seen and unseen speakers. These results, including\nthe best subjective score for text-to-speech, demonstrate the potential for\nfast adaptation to new speakers without a need for training from scratch.", "published": "2021-06-15 05:35:34", "link": "http://arxiv.org/abs/2106.07889v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SRIB Submission to Interspeech 2021 DiCOVA Challenge", "abstract": "The COVID-19 pandemic has resulted in more than 125 million infections and\nmore than 2.7 million casualties. In this paper, we attempt to classify covid\nvs non-covid cough sounds using signal processing and deep learning methods.\nAir turbulence, the vibration of tissues, movement of fluid through airways,\nopening, and closure of glottis are some of the causes for the production of\nthe acoustic sound signals during cough. Does the COVID-19 alter the acoustic\ncharacteristics of breath, cough, and speech sounds produced through the\nrespiratory system? This is an open question waiting for answers. In this\npaper, we incorporated novel data augmentation methods for cough sound\naugmentation and multiple deep neural network architectures and methods along\nwith handcrafted features. Our proposed system gives 14% absolute improvement\nin area under the curve (AUC). The proposed system is developed as part of\nInterspeech 2021 special sessions and challenges viz. diagnosing of COVID-19\nusing acoustics (DiCOVA). Our proposed method secured the 5th position on the\nleaderboard among 29 participants.", "published": "2021-06-15 08:50:19", "link": "http://arxiv.org/abs/2106.07972v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-channel Opus compression for far-field automatic speech\n  recognition with a fixed bitrate budget", "abstract": "Automatic speech recognition (ASR) in the cloud allows the use of larger\nmodels and more powerful multi-channel signal processing front-ends compared to\non-device processing. However, it also adds an inherent latency due to the\ntransmission of the audio signal, especially when transmitting multiple\nchannels of a microphone array. One way to reduce the network bandwidth\nrequirements is client-side compression with a lossy codec such as Opus.\nHowever, this compression can have a detrimental effect especially on\nmulti-channel ASR front-ends, due to the distortion and loss of spatial\ninformation introduced by the codec. In this publication, we propose an\nimproved approach for the compression of microphone array signals based on\nOpus, using a modified joint channel coding approach and additionally\nintroducing a multi-channel spatial decorrelating transform to reduce\nredundancy in the transmission. We illustrate the effect of the proposed\napproach on the spatial information retained in multi-channel signals after\ncompression, and evaluate the performance on far-field ASR with a multi-channel\nbeamforming front-end. We demonstrate that our approach can lead to a 37.5 %\nbitrate reduction or a 5.1 % relative word error rate reduction for a fixed\nbitrate budget in a seven channel setup.", "published": "2021-06-15 09:16:20", "link": "http://arxiv.org/abs/2106.07994v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voting for the right answer: Adversarial defense for speaker\n  verification", "abstract": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets.", "published": "2021-06-15 04:05:28", "link": "http://arxiv.org/abs/2106.07868v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Attention-based distributed speech enhancement for unconstrained\n  microphone arrays with varying number of nodes", "abstract": "Speech enhancement promises higher efficiency in ad-hoc microphone arrays\nthan in constrained microphone arrays thanks to the wide spatial coverage of\nthe devices in the acoustic scene. However, speech enhancement in ad-hoc\nmicrophone arrays still raises many challenges. In particular, the algorithms\nshould be able to handle a variable number of microphones, as some devices in\nthe array might appear or disappear. In this paper, we propose a solution that\ncan efficiently process the spatial information captured by the different\ndevices of the microphone array, while being robust to a link failure. To do\nthis, we use an attention mechanism in order to put more weight on the relevant\nsignals sent throughout the array and to neglect the redundant or empty\nchannels.", "published": "2021-06-15 07:42:29", "link": "http://arxiv.org/abs/2106.07939v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis", "abstract": "Text does not fully specify the spoken form, so text-to-speech models must be\nable to learn from speech data that vary in ways not explained by the\ncorresponding text. One way to reduce the amount of unexplained variation in\ntraining data is to provide acoustic information as an additional learning\nsignal. When generating speech, modifying this acoustic information enables\nmultiple distinct renditions of a text to be produced.\n  Since much of the unexplained variation is in the prosody, we propose a model\nthat generates speech explicitly conditioned on the three primary acoustic\ncorrelates of prosody: $F_{0}$, energy and duration. The model is flexible\nabout how the values of these features are specified: they can be externally\nprovided, or predicted from text, or predicted then subsequently modified.\n  Compared to a model that employs a variational auto-encoder to learn\nunsupervised latent features, our model provides more interpretable,\ntemporally-precise, and disentangled control. When automatically predicting the\nacoustic features from text, it generates speech that is more natural than that\nfrom a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop\nmodification of the predicted acoustic features can significantly further\nincrease naturalness.", "published": "2021-06-15 18:03:48", "link": "http://arxiv.org/abs/2106.08352v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
