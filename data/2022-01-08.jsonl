{"title": "Low-Rank Constraints for Fast Inference in Structured Models", "abstract": "Structured distributions, i.e. distributions over combinatorial spaces, are\ncommonly used to learn latent probabilistic representations from observed data.\nHowever, scaling these models is bottlenecked by the high computational and\nmemory complexity with respect to the size of the latent representations.\nCommon models such as Hidden Markov Models (HMMs) and Probabilistic\nContext-Free Grammars (PCFGs) require time and space quadratic and cubic in the\nnumber of hidden states respectively. This work demonstrates a simple approach\nto reduce the computational and memory complexity of a large class of\nstructured models. We show that by viewing the central inference step as a\nmatrix-vector product and using a low-rank constraint, we can trade off model\nexpressivity and speed via the rank. Experiments with neural parameterized\nstructured models for language modeling, polyphonic music modeling,\nunsupervised grammar induction, and video modeling show that our approach\nmatches the accuracy of standard models at large state spaces while providing\npractical speedups.", "published": "2022-01-08 00:47:50", "link": "http://arxiv.org/abs/2201.02715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defining maximum acceptable latency of AI-enhanced CAI tools", "abstract": "Recent years have seen an increasing number of studies around the design of\ncomputer-assisted interpreting tools with integrated automatic speech\nprocessing and their use by trainees and professional interpreters. This paper\ndiscusses the role of system latency of such tools and presents the results of\nan experiment designed to investigate the maximum system latency that is\ncognitively acceptable for interpreters working in the simultaneous modality.\nThe results show that interpreters can cope with a system latency of 3 seconds\nwithout any major impact in the rendition of the original text, both in terms\nof accuracy and fluency. This value is above the typical latency of available\nAI-based CAI tools and paves the way to experiment with larger context-based\nlanguage models and higher latencies.", "published": "2022-01-08 08:52:24", "link": "http://arxiv.org/abs/2201.02792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coherence-Based Distributed Document Representation Learning for\n  Scientific Documents", "abstract": "Distributed document representation is one of the basic problems in natural\nlanguage processing. Currently distributed document representation methods\nmainly consider the context information of words or sentences. These methods do\nnot take into account the coherence of the document as a whole, e.g., a\nrelation between the paper title and abstract, headline and description, or\nadjacent bodies in the document. The coherence shows whether a document is\nmeaningful, both logically and syntactically, especially in scientific\ndocuments (papers or patents, etc.). In this paper, we propose a coupled text\npair embedding (CTPE) model to learn the representation of scientific\ndocuments, which maintains the coherence of the document with coupled text\npairs formed by segmenting the document. First, we divide the document into two\nparts (e.g., title and abstract, etc) which construct a coupled text pair.\nThen, we adopt negative sampling to construct uncoupled text pairs whose two\nparts are from different documents. Finally, we train the model to judge\nwhether the text pair is coupled or uncoupled and use the obtained embedding of\ncoupled text pairs as the embedding of documents. We perform experiments on\nthree datasets for one information retrieval task and two recommendation tasks.\nThe experimental results verify the effectiveness of the proposed CTPE model.", "published": "2022-01-08 15:29:21", "link": "http://arxiv.org/abs/2201.02846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Review of Deep Learning for Automated Medical Coding", "abstract": "Automated medical coding, an essential task for healthcare operation and\ndelivery, makes unstructured data manageable by predicting medical codes from\nclinical documents. Recent advances in deep learning and natural language\nprocessing have been widely applied to this task. However, deep learning-based\nmedical coding lacks a unified view of the design of neural network\narchitectures. This review proposes a unified framework to provide a general\nunderstanding of the building blocks of medical coding models and summarizes\nrecent advanced models under the proposed framework. Our unified framework\ndecomposes medical coding into four main components, i.e., encoder modules for\ntext feature extraction, mechanisms for building deep encoder architectures,\ndecoder modules for transforming hidden representations into medical codes, and\nthe usage of auxiliary information. Finally, we introduce the benchmarks and\nreal-world usage and discuss key research challenges and future directions.", "published": "2022-01-08 09:37:23", "link": "http://arxiv.org/abs/2201.02797v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Beyond modeling: NLP Pipeline for efficient environmental policy\n  analysis", "abstract": "As we enter the UN Decade on Ecosystem Restoration, creating effective\nincentive structures for forest and landscape restoration has never been more\ncritical. Policy analysis is necessary for policymakers to understand the\nactors and rules involved in restoration in order to shift economic and\nfinancial incentives to the right places. Classical policy analysis is\nresource-intensive and complex, lacks comprehensive central information\nsources, and is prone to overlapping jurisdictions. We propose a Knowledge\nManagement Framework based on Natural Language Processing (NLP) techniques that\nwould tackle these challenges and automate repetitive tasks, reducing the\npolicy analysis process from weeks to minutes. Our framework was designed in\ncollaboration with policy analysis experts and made to be platform-, language-\nand policy-agnostic. In this paper, we describe the design of the NLP pipeline,\nreview the state-of-the-art methods for each of its components, and discuss the\nchallenges that rise when building a framework oriented towards policy\nanalysis.", "published": "2022-01-08 05:33:04", "link": "http://arxiv.org/abs/2201.07105v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Empirical Study of Vision-Language Pre-trained Model for\n  Supervised Cross-Modal Retrieval", "abstract": "Cross-Modal Retrieval (CMR) is an important research topic across multimodal\ncomputing and information retrieval, which takes one type of data as the query\nto retrieve relevant data of another type. It has been widely used in many\nreal-world applications. Recently, the vision-language pre-trained models\nrepresented by CLIP demonstrate its superiority in learning the visual and\ntextual representations and gain impressive performance on various vision and\nlanguage related tasks. Although CLIP as well as the previous pre-trained\nmodels have shown great performance improvement in the unsupervised CMR, the\nperformance and impact of these pre-trained models on the supervised CMR were\nrarely explored due to the lack of common representation for the multimodal\nclass-level associations. In this paper, we take CLIP as the current\nrepresentative vision-language pre-trained model to conduct a comprehensive\nempirical study. We evaluate its performance and impact on the supervised CMR,\nand attempt to answer several key research questions. To this end, we first\npropose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal\nRetrieval) that employs the pre-trained CLIP as backbone network to perform the\nsupervised CMR. Then by means of the CLIP4CMR framework, we revisit the design\nof different learning objectives in current CMR methods to provide new insights\non model design. Moreover, we investigate the most concerned aspects in\napplying CMR, including the robustness to modality imbalance and sensitivity to\nhyper-parameters, to provide new perspectives for practical applications.\nThrough extensive experiments, we show that CLIP4CMR achieves the SOTA results\nwith prominent improvements on the benchmark datasets, and can be used as a\nfundamental framework to empirically study the key research issues of the\nsupervised CMR, with significant implications for model design and practical\nconsiderations.", "published": "2022-01-08 06:00:22", "link": "http://arxiv.org/abs/2201.02772v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Clustering Text Using Attention", "abstract": "Clustering Text has been an important problem in the domain of Natural\nLanguage Processing. While there are techniques to cluster text based on using\nconventional clustering techniques on top of contextual or non-contextual\nvector space representations, it still remains a prevalent area of research\npossible to various improvements in performance and implementation of these\ntechniques. This paper discusses a novel technique to cluster text using\nattention mechanisms. Attention Mechanisms have proven to be highly effective\nin various NLP tasks in recent times. This paper extends the idea of attention\nmechanism in clustering space and sheds some light on a whole new area of\nresearch", "published": "2022-01-08 12:18:49", "link": "http://arxiv.org/abs/2201.02816v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Impact of Stop Sets on Stopping Active Learning for Text Classification", "abstract": "Active learning is an increasingly important branch of machine learning and a\npowerful technique for natural language processing. The main advantage of\nactive learning is its potential to reduce the amount of labeled data needed to\nlearn high-performing models. A vital aspect of an effective active learning\nalgorithm is the determination of when to stop obtaining additional labeled\ndata. Several leading state-of-the-art stopping methods use a stop set to help\nmake this decision. However, there has been relatively less attention given to\nthe choice of stop set than to the stopping algorithms that are applied on the\nstop set. Different choices of stop sets can lead to significant differences in\nstopping method performance. We investigate the impact of different stop set\nchoices on different stopping methods. This paper shows the choice of the stop\nset can have a significant impact on the performance of stopping methods and\nthe impact is different for stability-based methods from that on\nconfidence-based methods. Furthermore, the unbiased representative stop sets\nsuggested by original authors of methods work better than the systematically\nbiased stop sets used in recently published work, and stopping methods based on\nstabilizing predictions have stronger performance than confidence-based\nstopping methods when unbiased representative stop sets are used. We provide\nthe largest quantity of experimental results on the impact of stop sets to\ndate. The findings are important for helping to illuminate the impact of this\nimportant aspect of stopping methods that has been under-considered in recently\npublished work and that can have a large practical impact on the performance of\nstopping methods for important semantic computing applications such as\ntechnology assisted review and text classification more broadly.", "published": "2022-01-08 18:59:15", "link": "http://arxiv.org/abs/2201.05460v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7; I.5.4"], "primary_category": "cs.IR"}
{"title": "Effect of Toxic Review Content on Overall Product Sentiment", "abstract": "Toxic contents in online product review are a common phenomenon. A content is\nperceived to be toxic when it is rude, disrespectful, or unreasonable and make\nindividuals leave the discussion. Machine learning algorithms helps the sell\nside community to identify such toxic patterns and eventually moderate such\ninputs. Yet, the extant literature provides fewer information about the\nsentiment of a prospective consumer on the perception of a product after being\nexposed to such toxic review content. In this study, we collect a balanced data\nset of review comments from 18 different players segregated into three\ndifferent sectors from google play-store. Then we calculate the sentence-level\nsentiment and toxicity score of individual review content. Finally, we use\nstructural equation modelling to quantitatively study the influence of toxic\ncontent on overall product sentiment. We observe that comment toxicity\nnegatively influences overall product sentiment but do not exhibit a mediating\neffect over reviewer score to influence sector-wise relative rating.", "published": "2022-01-08 16:40:38", "link": "http://arxiv.org/abs/2201.02857v1", "categories": ["cs.HC", "cs.CL", "econ.GN", "q-fin.EC", "stat.AP", "I.2.7; J.4"], "primary_category": "cs.HC"}
{"title": "Two-Pass End-to-End ASR Model Compression", "abstract": "Speech recognition on smart devices is challenging owing to the small memory\nfootprint. Hence small size ASR models are desirable. With the use of popular\ntransducer-based models, it has become possible to practically deploy streaming\nspeech recognition models on small devices [1]. Recently, the two-pass model\n[2] combining RNN-T and LAS modules has shown exceptional performance for\nstreaming on-device speech recognition.\n  In this work, we propose a simple and effective approach to reduce the size\nof the two-pass model for memory-constrained devices. We employ a popular\nknowledge distillation approach in three stages using the Teacher-Student\ntraining technique. In the first stage, we use a trained RNN-T model as a\nteacher model and perform knowledge distillation to train the student RNN-T\nmodel. The second stage uses the shared encoder and trains a LAS rescorer for\nstudent model using the trained RNN-T+LAS teacher model. Finally, we perform\ndeep-finetuning for the student model with a shared RNN-T encoder, RNN-T\ndecoder, and LAS rescorer. Our experimental results on standard LibriSpeech\ndataset show that our system can achieve a high compression rate of 55% without\nsignificant degradation in the WER compared to the two-pass teacher model.", "published": "2022-01-08 02:19:22", "link": "http://arxiv.org/abs/2201.02741v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A novel audio representation using space filling curves", "abstract": "Since convolutional neural networks (CNNs) have revolutionized the image\nprocessing field, they have been widely applied in the audio context. A common\napproach is to convert the one-dimensional audio signal time series to\ntwo-dimensional images using a time-frequency decomposition method. Also it is\ncommon to discard the phase information. In this paper, we propose to map\none-dimensional audio waveforms to two-dimensional images using space filling\ncurves (SFCs). These mappings do not compress the input signal, while\npreserving its local structure. Moreover, the mappings benefit from progress\nmade in deep learning and the large collection of existing computer vision\nnetworks. We test eight SFCs on two keyword spotting problems. We show that the\nZ curve yields the best results due to its shift equivariance under convolution\noperations. Additionally, the Z curve produces comparable results to the widely\nused mel frequency cepstral coefficients across multiple CNNs.", "published": "2022-01-08 11:01:49", "link": "http://arxiv.org/abs/2201.02805v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks", "abstract": "State-of-the-art automatic speech recognition (ASR) system development is\ndata and computation intensive. The optimal design of deep neural networks\n(DNNs) for these systems often require expert knowledge and empirical\nevaluation. In this paper, a range of neural architecture search (NAS)\ntechniques are used to automatically learn two types of hyper-parameters of\nfactored time delay neural networks (TDNN-Fs): i) the left and right splicing\ncontext offsets; and ii) the dimensionality of the bottleneck linear projection\nat each hidden layer. These techniques include the differentiable neural\narchitecture search (DARTS) method integrating architecture learning with\nlattice-free MMI training; Gumbel-Softmax and pipelined DARTS methods reducing\nthe confusion over candidate architectures and improving the generalization of\narchitecture selection; and Penalized DARTS incorporating resource constraints\nto balance the trade-off between performance and system complexity. Parameter\nsharing among TDNN-F architectures allows an efficient search over up to 7^28\ndifferent systems. Statistically significant word error rate (WER) reductions\nof up to 1.2% absolute and relative model size reduction of 31% were obtained\nover a state-of-the-art 300-hour Switchboard corpus trained baseline LF-MMI\nTDNN-F system featuring speed perturbation, i-Vector and learning hidden unit\ncontribution (LHUC) based speaker adaptation as well as RNNLM rescoring.\nPerformance contrasts on the same task against recent end-to-end systems\nreported in the literature suggest the best NAS auto-configured system achieves\nstate-of-the-art WERs of 9.9% and 11.1% on the NIST Hub5' 00 and Rt03s test\nsets respectively with up to 96% model size reduction. Further analysis using\nBayesian learning shows that ...", "published": "2022-01-08 07:52:01", "link": "http://arxiv.org/abs/2201.03943v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
