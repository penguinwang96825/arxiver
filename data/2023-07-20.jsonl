{"title": "General Debiasing for Multimodal Sentiment Analysis", "abstract": "Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal\ninformation for prediction yet unavoidably suffers from fitting the spurious\ncorrelations between multimodal features and sentiment labels. For example, if\nmost videos with a blue background have positive labels in a dataset, the model\nwill rely on such correlations for prediction, while \"blue background\" is not a\nsentiment-related feature. To address this problem, we define a general\ndebiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD)\ngeneralization ability of MSA models by reducing their reliance on spurious\ncorrelations. To this end, we propose a general debiasing framework based on\nInverse Probability Weighting (IPW), which adaptively assigns small weights to\nthe samples with larger bias (i.e., the severer spurious correlations). The key\nto this debiasing framework is to estimate the bias of each sample, which is\nachieved by two steps: 1) disentangling the robust features and biased features\nin each modality, and 2) utilizing the biased features to estimate the bias.\nFinally, we employ IPW to reduce the effects of large-biased samples,\nfacilitating robust feature learning for sentiment prediction. To examine the\nmodel's generalization ability, we keep the original testing sets on two\nbenchmarks and additionally construct multiple unimodal and multimodal OOD\ntesting sets. The empirical results demonstrate the superior generalization\nability of our proposed framework. We have released the code and data to\nfacilitate the reproduction https://github.com/Teng-Sun/GEAR.", "published": "2023-07-20 00:36:41", "link": "http://arxiv.org/abs/2307.10511v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language\n  Models", "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models\n(PLMs) propagate societal biases from the large unmoderated pre-training\ncorpora. Existing solutions require debiasing training processes and datasets\nfor debiasing, which are resource-intensive and costly. Furthermore, these\nmethods hurt the PLMs' performance on downstream tasks. In this study, we\npropose Gender-tuning, which debiases the PLMs through fine-tuning on\ndownstream tasks' datasets. For this aim, Gender-tuning integrates Masked\nLanguage Modeling (MLM) training objectives into fine-tuning's training\nprocess. Comprehensive experiments show that Gender-tuning outperforms the\nstate-of-the-art baselines in terms of average gender bias scores in PLMs while\nimproving PLMs' performance on downstream tasks solely using the downstream\ntasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM\nthat works with original fine-tuning.", "published": "2023-07-20 01:48:51", "link": "http://arxiv.org/abs/2307.10522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction-following Evaluation through Verbalizer Manipulation", "abstract": "While instruction-tuned models have shown remarkable success in various\nnatural language processing tasks, accurately evaluating their ability to\nfollow instructions remains challenging. Existing benchmarks primarily focus on\ncommon instructions that align well with what the model learned during\ntraining. However, proficiency in responding to these instructions does not\nnecessarily imply strong ability in instruction following. In this paper, we\npropose a novel instruction-following evaluation protocol called verbalizer\nmanipulation. It instructs the model to verbalize the task label with words\naligning with model priors to different extents, adopting verbalizers from\nhighly aligned (e.g., outputting ``postive'' for positive sentiment), to\nminimally aligned (e.g., outputting ``negative'' for positive sentiment).\nVerbalizer manipulation can be seamlessly integrated with any classification\nbenchmark to examine the model's reliance on priors and its ability to override\nthem to accurately follow the instructions. We conduct a comprehensive\nevaluation of four major model families across nine datasets, employing twelve\nsets of verbalizers for each of them. We observe that the instruction-following\nabilities of models, across different families and scales, are significantly\ndistinguished by their performance on less natural verbalizers. Even the\nstrongest GPT-4 model struggles to perform better than random guessing on the\nmost challenging verbalizer, emphasizing the need for continued advancements to\nimprove their instruction-following abilities.", "published": "2023-07-20 03:54:24", "link": "http://arxiv.org/abs/2307.10558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Landscape of Natural Language Processing Research", "abstract": "As an efficient approach to understand, generate, and process natural\nlanguage texts, research in natural language processing (NLP) has exhibited a\nrapid spread and wide adoption in recent years. Given the increasing research\nwork in this area, several NLP-related approaches have been surveyed in the\nresearch community. However, a comprehensive study that categorizes established\ntopics, identifies trends, and outlines areas for future research remains\nabsent. Contributing to closing this gap, we have systematically classified and\nanalyzed research papers in the ACL Anthology. As a result, we present a\nstructured overview of the research landscape, provide a taxonomy of fields of\nstudy in NLP, analyze recent developments in NLP, summarize our findings, and\nhighlight directions for future work.", "published": "2023-07-20 07:33:30", "link": "http://arxiv.org/abs/2307.10652v5", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Dataset and Strong Baselines for Classification of Czech News Texts", "abstract": "Pre-trained models for Czech Natural Language Processing are often evaluated\non purely linguistic tasks (POS tagging, parsing, NER) and relatively simple\nclassification tasks such as sentiment classification or article classification\nfrom a single news source. As an alternative, we present\nCZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech\nclassification datasets, composed of news articles from various sources\nspanning over twenty years, which allows a more rigorous evaluation of such\nmodels. We define four classification tasks: news source, news category,\ninferred author's gender, and day of the week. To verify the task difficulty,\nwe conducted a human evaluation, which revealed that human performance lags\nbehind strong machine-learning baselines built upon pre-trained transformer\nmodels. Furthermore, we show that language-specific pre-trained encoder\nanalysis outperforms selected commercially available large-scale generative\nlanguage models.", "published": "2023-07-20 07:47:08", "link": "http://arxiv.org/abs/2307.10666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extreme Multi-Label Skill Extraction Training using Large Language\n  Models", "abstract": "Online job ads serve as a valuable source of information for skill\nrequirements, playing a crucial role in labor market analysis and e-recruitment\nprocesses. Since such ads are typically formatted in free text, natural\nlanguage processing (NLP) technologies are required to automatically process\nthem. We specifically focus on the task of detecting skills (mentioned\nliterally, or implicitly described) and linking them to a large skill ontology,\nmaking it a challenging case of extreme multi-label classification (XMLC).\nGiven that there is no sizable labeled (training) dataset are available for\nthis specific XMLC task, we propose techniques to leverage general Large\nLanguage Models (LLMs). We describe a cost-effective approach to generate an\naccurate, fully synthetic labeled dataset for skill extraction, and present a\ncontrastive learning strategy that proves effective in the task. Our results\nacross three skill extraction benchmarks show a consistent increase of between\n15 to 25 percentage points in \\textit{R-Precision@5} compared to previously\npublished results that relied solely on distant supervision through literal\nmatches.", "published": "2023-07-20 11:29:15", "link": "http://arxiv.org/abs/2307.10778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer-wise Representation Fusion for Compositional Generalization", "abstract": "Existing neural models are demonstrated to struggle with compositional\ngeneralization (CG), i.e., the ability to systematically generalize to unseen\ncompositions of seen components. A key reason for failure on CG is that the\nsyntactic and semantic representations of sequences in both the uppermost layer\nof the encoder and decoder are entangled. However, previous work concentrates\non separating the learning of syntax and semantics instead of exploring the\nreasons behind the representation entanglement (RE) problem to solve it. We\nexplain why it exists by analyzing the representation evolving mechanism from\nthe bottom to the top of the Transformer layers. We find that the ``shallow''\nresidual connections within each layer fail to fuse previous layers'\ninformation effectively, leading to information forgetting between layers and\nfurther the RE problems. Inspired by this, we propose LRF, a novel\n\\textbf{L}ayer-wise \\textbf{R}epresentation \\textbf{F}usion framework for CG,\nwhich learns to fuse previous layers' information back into the encoding and\ndecoding process effectively through introducing a \\emph{fuse-attention module}\nat each encoder and decoder layer. LRF achieves promising results on two\nrealistic benchmarks, empirically demonstrating the effectiveness of our\nproposal.", "published": "2023-07-20 12:01:40", "link": "http://arxiv.org/abs/2307.10799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Yelp Reviews and Food Types: A Comparative Analysis of Ratings,\n  Sentiments, and Topics", "abstract": "This study examines the relationship between Yelp reviews and food types,\ninvestigating how ratings, sentiments, and topics vary across different types\nof food. Specifically, we analyze how ratings and sentiments of reviews vary\nacross food types, cluster food types based on ratings and sentiments, infer\nreview topics using machine learning models, and compare topic distributions\namong different food types. Our analyses reveal that some food types have\nsimilar ratings, sentiments, and topics distributions, while others have\ndistinct patterns. We identify four clusters of food types based on ratings and\nsentiments and find that reviewers tend to focus on different topics when\nreviewing certain food types. These findings have important implications for\nunderstanding user behavior and cultural influence on digital media platforms\nand promoting cross-cultural understanding and appreciation.", "published": "2023-07-20 12:41:35", "link": "http://arxiv.org/abs/2307.10826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "L-Eval: Instituting Standardized Evaluation for Long Context Language\n  Models", "abstract": "Recently, there has been growing interest in extending the context length of\nlarge language models (LLMs), aiming to effectively process long inputs of one\nturn or conversations with more extensive histories. While proprietary models\nsuch as GPT-4 and Claude can largely preserve the reasoning ability in an\nextended context, open-source models are still progressing through the early\nstages of development. To bridge this gap, we propose L-Eval to institute a\nmore standardized evaluation for long context language models (LCLMs)\naddressing two key aspects: dataset construction and evaluation metrics. On the\none hand, we build a new evaluation suite containing 20 sub-tasks, 508 long\ndocuments, and over 2,000 human-labeled query-response pairs encompassing\ndiverse question styles, domains, and input length (3k$\\sim$200k tokens). On\nthe other hand, we investigate the effectiveness in evalution metrics for\nLCLMs. Results show that popular n-gram matching metrics generally can not\ncorrelate well with human judgment, and thus we strongly advocate for\nlength-instruction-enhanced (LIE) evaluation and employing LLM judges. We\nconducted a comprehensive study of 4 popular commercial LLMs and 12 open-source\ncounterparts using the L-Eval benchmark. Our empirical findings offer useful\ninsights into the study of LCLMs and lay the groundwork for the development of\nmore principled evaluation of these models.", "published": "2023-07-20 17:59:41", "link": "http://arxiv.org/abs/2307.11088v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for\n  Biomedical Entity Recognition", "abstract": "Pre-trained transformer language models (LMs) have in recent years become the\ndominant paradigm in applied NLP. These models have achieved state-of-the-art\nperformance on tasks such as information extraction, question answering,\nsentiment analysis, document classification and many others. In the biomedical\ndomain, significant progress has been made in adapting this paradigm to NLP\ntasks that require the integration of domain-specific knowledge as well as\nstatistical modelling of language. In particular, research in this area has\nfocused on the question of how best to construct LMs that take into account not\nonly the patterns of token distribution in medical text, but also the wealth of\nstructured information contained in terminology resources such as the UMLS.\nThis work contributes a data-centric paradigm for enriching the language\nrepresentations of biomedical transformer-encoder LMs by extracting text\nsequences from the UMLS. This allows for graph-based learning objectives to be\ncombined with masked-language pre-training. Preliminary results from\nexperiments in the extension of pre-trained LMs as well as training from\nscratch show that this framework improves downstream performance on multiple\nbiomedical and clinical Named Entity Recognition (NER) tasks.", "published": "2023-07-20 18:08:34", "link": "http://arxiv.org/abs/2307.11170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An In-Depth Evaluation of Federated Learning on Biomedical Natural\n  Language Processing", "abstract": "Language models (LMs) such as BERT and GPT have revolutionized natural\nlanguage processing (NLP). However, the medical field faces challenges in\ntraining LMs due to limited data access and privacy constraints imposed by\nregulations like the Health Insurance Portability and Accountability Act\n(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning\n(FL) offers a decentralized solution that enables collaborative learning while\nensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasks\nencompassing 8 corpora using 6 LMs. Our results show that: 1) FL models\nconsistently outperformed models trained on individual clients' data and\nsometimes performed comparably with models trained with polled data; 2) with\nthe fixed number of total data, FL models training with more clients produced\ninferior performance but pre-trained transformer-based models exhibited great\nresilience. 3) FL models significantly outperformed large language models using\nzero-/one-shot learning and offered lightning inference speed.", "published": "2023-07-20 22:10:04", "link": "http://arxiv.org/abs/2307.11254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain", "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable\nsuccess. However, such LLMs have not been widely adopted for medical purposes,\ndue to poor accuracy and inability to provide medical advice. We propose\nIvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality\nmedical question-answer (QA) instances and Reinforcement Learning from Human\nFeedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn\nconversation capabilities, but it cannot perform like a doctor in other\naspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output\nricher diagnosis and treatment answers that are closer to human. In the\ntraining, we used QLoRA to train 33 billion parameters on a small number of\nNVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed\nother medical GPT models.", "published": "2023-07-20 01:11:14", "link": "http://arxiv.org/abs/2307.10512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Deep Dive into the Disparity of Word Error Rates Across Thousands of\n  NPTEL MOOC Videos", "abstract": "Automatic speech recognition (ASR) systems are designed to transcribe spoken\nlanguage into written text and find utility in a variety of applications\nincluding voice assistants and transcription services. However, it has been\nobserved that state-of-the-art ASR systems which deliver impressive benchmark\nresults, struggle with speakers of certain regions or demographics due to\nvariation in their speech properties. In this work, we describe the curation of\na massive speech dataset of 8740 hours consisting of $\\sim9.8$K technical\nlectures in the English language along with their transcripts delivered by\ninstructors representing various parts of Indian demography. The dataset is\nsourced from the very popular NPTEL MOOC platform. We use the curated dataset\nto measure the existing disparity in YouTube Automatic Captions and OpenAI\nWhisper model performance across the diverse demographic traits of speakers in\nIndia. While there exists disparity due to gender, native region, age and\nspeech rate of speakers, disparity based on caste is non-existent. We also\nobserve statistically significant disparity across the disciplines of the\nlectures. These results indicate the need of more inclusive and robust ASR\nsystems and more representational datasets for disparity evaluation in them.", "published": "2023-07-20 05:03:00", "link": "http://arxiv.org/abs/2307.10587v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multi-Method Self-Training: Improving Code Generation With Text, And\n  Vice Versa", "abstract": "Large Language Models have many methods for solving the same problem. This\nintroduces novel strengths (different methods may work well for different\nproblems) and weaknesses (it may be difficult for users to know which method to\nuse). In this paper, we introduce Multi-Method Self-Training (MMST), where one\nmethod is trained on the filtered outputs of another, allowing us to augment\nthe strengths and ameliorate the weaknesses of each method. Using a 176B\nparameter model trained on both language and code, we show that MMST can 1)\nimprove the less performant method (up to 30%) making the model easier to use,\n2) improve the more performant method (up to 32.2%) making the model more\nperformant, and 3) improve the performance of related but distinct tasks (up to\n10.3%) by improving the ability of the model to generate rationales. We then\nconduct ablation analyses to explore why MMST works. We show that MMST\ngenerates more data than traditional self-training, but the improvement in\nperformance is driven by the use of multiple methods. We also analyze\nprompt-engineering and anti-correlated performance between methods as means of\nmaking MMST more effective. We hope the evidence from our paper motivates\nmachine learning researchers to explore ways in which advances in language\nmodels allow for new forms of training.", "published": "2023-07-20 06:58:55", "link": "http://arxiv.org/abs/2307.10633v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill\n  Sets", "abstract": "Evaluation of Large Language Models (LLMs) is challenging because\ninstruction-following necessitates alignment with human values and the required\nset of skills varies depending on the instruction. However, previous studies\nhave mainly focused on coarse-grained evaluation (i.e. overall preference-based\nevaluation), which limits interpretability since it does not consider the\nnature of user instructions that require instance-wise skill composition. In\nthis paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\nAlignment Skill Sets), a fine-grained evaluation protocol for both human-based\nand model-based evaluation which decomposes coarse-level scoring to a skill\nset-level scoring for each instruction. We experimentally observe that the\nfine-graininess of evaluation is crucial for attaining a holistic view of model\nperformance and increasing the reliability of the evaluation. Using FLASK, we\ncompare multiple open-source and proprietary LLMs and observe a high\ncorrelation between model-based and human-based evaluations. We publicly\nrelease the evaluation data and code implementation at\nhttps://github.com/kaistAI/FLASK.", "published": "2023-07-20 14:56:35", "link": "http://arxiv.org/abs/2307.10928v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MediaGPT : A Large Language Model For Chinese Media", "abstract": "Large language models (LLMs) have shown remarkable capabilities in generating\nhigh-quality text and making predictions based on large amounts of data,\nincluding the media domain. However, in practical applications, the differences\nbetween the media's use cases and the general-purpose applications of LLMs have\nbecome increasingly apparent, especially Chinese. This paper examines the\nunique characteristics of media-domain-specific LLMs compared to general LLMs,\ndesigned a diverse set of task instruction types to cater the specific\nrequirements of the domain and constructed unique datasets that are tailored to\nthe media domain. Based on these, we proposed MediaGPT, a domain-specific LLM\nfor the Chinese media domain, training by domain-specific data and experts SFT\ndata. By performing human experts evaluation and strong model evaluation on a\nvalidation set, this paper demonstrated that MediaGPT outperforms mainstream\nmodels on various Chinese media domain tasks and verifies the importance of\ndomain data and domain-defined prompt types for building an effective\ndomain-specific LLM.", "published": "2023-07-20 14:59:02", "link": "http://arxiv.org/abs/2307.10930v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identical and Fraternal Twins: Fine-Grained Semantic Contrastive\n  Learning of Sentence Representations", "abstract": "The enhancement of unsupervised learning of sentence representations has been\nsignificantly achieved by the utility of contrastive learning. This approach\nclusters the augmented positive instance with the anchor instance to create a\ndesired embedding space. However, relying solely on the contrastive objective\ncan result in sub-optimal outcomes due to its inability to differentiate subtle\nsemantic variations between positive pairs. Specifically, common data\naugmentation techniques frequently introduce semantic distortion, leading to a\nsemantic margin between the positive pair. While the InfoNCE loss function\noverlooks the semantic margin and prioritizes similarity maximization between\npositive pairs during training, leading to the insensitive semantic\ncomprehension ability of the trained model. In this paper, we introduce a novel\nIdentical and Fraternal Twins of Contrastive Learning (named IFTCL) framework,\ncapable of simultaneously adapting to various positive pairs generated by\ndifferent augmentation techniques. We propose a \\textit{Twins Loss} to preserve\nthe innate margin during training and promote the potential of data enhancement\nin order to overcome the sub-optimal issue. We also present proof-of-concept\nexperiments combined with the contrastive objective to prove the validity of\nthe proposed Twins Loss. Furthermore, we propose a hippocampus queue mechanism\nto restore and reuse the negative instances without additional calculation,\nwhich further enhances the efficiency and performance of the IFCL. We verify\nthe IFCL framework on nine semantic textual similarity tasks with both English\nand Chinese datasets, and the experimental results show that IFCL outperforms\nstate-of-the-art methods.", "published": "2023-07-20 15:02:42", "link": "http://arxiv.org/abs/2307.10932v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation", "abstract": "Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary.", "published": "2023-07-20 16:46:10", "link": "http://arxiv.org/abs/2307.11019v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot\n  Classification", "abstract": "Recent work has shown that language models' (LMs) prompt-based learning\ncapabilities make them well suited for automating data labeling in domains\nwhere manual annotation is expensive. The challenge is that while writing an\ninitial prompt is cheap, improving a prompt is costly -- practitioners often\nrequire significant labeled data in order to evaluate the impact of prompt\nmodifications. Our work asks whether it is possible to improve prompt-based\nlearning without additional labeled data. We approach this problem by\nattempting to modify the predictions of a prompt, rather than the prompt\nitself. Our intuition is that accurate predictions should also be consistent:\nsamples which are similar under some feature representation should receive the\nsame prompt prediction. We propose Embroid, a method which computes multiple\nrepresentations of a dataset under different embedding functions, and uses the\nconsistency between the LM predictions for neighboring samples to identify\nmispredictions. Embroid then uses these neighborhoods to create additional\npredictions for each sample, and combines these predictions with a simple\nlatent variable graphical model in order to generate a final corrected\nprediction. In addition to providing a theoretical analysis of Embroid, we\nconduct a rigorous empirical evaluation across six different LMs and up to 95\ndifferent tasks. We find that (1) Embroid substantially improves performance\nover original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also\nrealizes improvements for more sophisticated prompting strategies (e.g.,\nchain-of-thought), and (3) can be specialized to domains like law through the\nembedding functions.", "published": "2023-07-20 17:07:28", "link": "http://arxiv.org/abs/2307.11031v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Conversational Shaping for Intelligent Agents", "abstract": "The recent emergence of deep learning methods has enabled the research\ncommunity to achieve state-of-the art results in several domains including\nnatural language processing. However, the current robocall system remains\nunstable and inaccurate: text generator and chat-bots can be tedious and\nmisunderstand human-like dialogue. In this work, we study the performance of\ntwo models able to enhance an intelligent conversational agent through\nadversarial conversational shaping: a generative adversarial network with\npolicy gradient (GANPG) and a generative adversarial network with reward for\nevery generation step (REGS) based on the REGS model presented in Li et al.\n[18] . This model is able to assign rewards to both partially and fully\ngenerated text sequences. We discuss performance with different training\ndetails : seq2seq [ 36] and transformers [37 ] in a reinforcement learning\nframework.", "published": "2023-07-20 12:44:47", "link": "http://arxiv.org/abs/2307.11785v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building Socio-culturally Inclusive Stereotype Resources with Community\n  Engagement", "abstract": "With rapid development and deployment of generative language models in global\nsettings, there is an urgent need to also scale our measurements of harm, not\njust in the number and types of harms covered, but also how well they account\nfor local cultural contexts, including marginalized identities and the social\nbiases experienced by them. Current evaluation paradigms are limited in their\nabilities to address this, as they are not representative of diverse, locally\nsituated but global, socio-cultural perspectives. It is imperative that our\nevaluation resources are enhanced and calibrated by including people and\nexperiences from different cultures and societies worldwide, in order to\nprevent gross underestimations or skews in measurements of harm. In this work,\nwe demonstrate a socio-culturally aware expansion of evaluation resources in\nthe Indian societal context, specifically for the harm of stereotyping. We\ndevise a community engaged effort to build a resource which contains\nstereotypes for axes of disparity that are uniquely present in India. The\nresultant resource increases the number of stereotypes known for and in the\nIndian context by over 1000 stereotypes across many unique identities. We also\ndemonstrate the utility and effectiveness of such expanded resources for\nevaluations of language models. CONTENT WARNING: This paper contains examples\nof stereotypes that may be offensive.", "published": "2023-07-20 01:26:34", "link": "http://arxiv.org/abs/2307.10514v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Generative Language Models on Nucleotide Sequences of Human Genes", "abstract": "Language models, primarily transformer-based ones, obtained colossal success\nin NLP. To be more precise, studies like BERT in NLU and works such as GPT-3\nfor NLG are very crucial. DNA sequences are very close to natural language in\nterms of structure, so if the DNA-related bioinformatics domain is concerned,\ndiscriminative models, like DNABert, exist. Yet, the generative side of the\ncoin is mainly unexplored to the best of our knowledge. Consequently, we\nfocused on developing an autoregressive generative language model like GPT-3\nfor DNA sequences. Because working with whole DNA sequences is challenging\nwithout substantial computational resources, we decided to carry out our study\non a smaller scale, focusing on nucleotide sequences of human genes, unique\nparts in DNA with specific functionalities, instead of the whole DNA. This\ndecision did not change the problem structure a lot due to the fact that both\nDNA and genes can be seen as 1D sequences consisting of four different\nnucleotides without losing much information and making too much simplification.\nFirst of all, we systematically examined an almost entirely unexplored problem\nand observed that RNNs performed the best while simple techniques like N-grams\nwere also promising. Another beneficial point was learning how to work with\ngenerative models on languages we do not understand, unlike natural language.\nHow essential using real-life tasks beyond the classical metrics such as\nperplexity is observed. Furthermore, checking whether the data-hungry nature of\nthese models can be changed through selecting a language with minimal\nvocabulary size, four owing to four different types of nucleotides, is\nexamined. The reason for reviewing this was that choosing such a language might\nmake the problem easier. However, what we observed in this study was it did not\nprovide that much of a change in the amount of data needed.", "published": "2023-07-20 06:59:02", "link": "http://arxiv.org/abs/2307.10634v1", "categories": ["q-bio.GN", "cs.CL", "cs.LG"], "primary_category": "q-bio.GN"}
{"title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities\n  of Large Language Models", "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.", "published": "2023-07-20 07:01:57", "link": "http://arxiv.org/abs/2307.10635v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topics, Authors, and Institutions in Large Language Model Research:\n  Trends from 17K arXiv Papers", "abstract": "Large language models (LLMs) are dramatically influencing AI research,\nspurring discussions on what has changed so far and how to shape the field's\nfuture. To clarify such questions, we analyze a new dataset of 16,979\nLLM-related arXiv papers, focusing on recent trends in 2023 vs. 2018-2022.\nFirst, we study disciplinary shifts: LLM research increasingly considers\nsocietal impacts, evidenced by 20x growth in LLM submissions to the Computers\nand Society sub-arXiv. An influx of new authors -- half of all first authors in\n2023 -- are entering from non-NLP fields of CS, driving disciplinary expansion.\nSecond, we study industry and academic publishing trends. Surprisingly,\nindustry accounts for a smaller publication share in 2023, largely due to\nreduced output from Google and other Big Tech companies; universities in Asia\nare publishing more. Third, we study institutional collaboration: while\nindustry-academic collaborations are common, they tend to focus on the same\ntopics that industry focuses on rather than bridging differences. The most\nprolific institutions are all US- or China-based, but there is very little\ncross-country collaboration. We discuss implications around (1) how to support\nthe influx of new authors, (2) how industry trends may affect academics, and\n(3) possible effects of (the lack of) collaboration.", "published": "2023-07-20 08:45:00", "link": "http://arxiv.org/abs/2307.10700v4", "categories": ["cs.DL", "cs.CL", "cs.CY"], "primary_category": "cs.DL"}
{"title": "LLM Censorship: A Machine Learning Challenge or a Computer Security\n  Problem?", "abstract": "Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.", "published": "2023-07-20 09:25:02", "link": "http://arxiv.org/abs/2307.10719v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exploring Perspectives on the Impact of Artificial Intelligence on the\n  Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic\n  Parrots", "abstract": "Artificial Intelligence (AI), and in particular generative models, are\ntransformative tools for knowledge work. They problematise notions of\ncreativity, originality, plagiarism, the attribution of credit, and copyright\nownership. Critics of generative models emphasise the reliance on large amounts\nof training data, and view the output of these models as no more than\nrandomised plagiarism, remix, or collage of the source data. On these grounds,\nmany have argued for stronger regulations on the deployment, use, and\nattribution of the output of these models. However, these issues are not new or\nunique to artificial intelligence. In this position paper, using examples from\nliterary criticism, the history of art, and copyright law, I show how\ncreativity and originality resist definition as a notatable or\ninformation-theoretic property of an object, and instead can be seen as the\nproperty of a process, an author, or a viewer. Further alternative views hold\nthat all creative work is essentially reuse (mostly without attribution), or\nthat randomness itself can be creative. I suggest that creativity is ultimately\ndefined by communities of creators and receivers, and the deemed sources of\ncreativity in a workflow often depend on which parts of the workflow can be\nautomated. Using examples from recent studies of AI in creative knowledge work,\nI suggest that AI shifts knowledge work from material production to critical\nintegration. This position paper aims to begin a conversation around a more\nnuanced approach to the problems of creativity and credit assignment for\ngenerative models, one which more fully recognises the importance of the\ncreative and curatorial voice of the users of these models and moves away from\nsimpler notational or information-theoretic views.", "published": "2023-07-20 10:26:57", "link": "http://arxiv.org/abs/2307.10751v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Vesper: A Compact and Effective Pretrained Model for Speech Emotion\n  Recognition", "abstract": "This paper presents a paradigm that adapts general large-scale pretrained\nmodels (PTMs) to speech emotion recognition task. Although PTMs shed new light\non artificial general intelligence, they are constructed with general tasks in\nmind, and thus, their efficacy for specific tasks can be further improved.\nAdditionally, employing PTMs in practical applications can be challenging due\nto their considerable size. Above limitations spawn another research direction,\nnamely, optimizing large-scale PTMs for specific tasks to generate\ntask-specific PTMs that are both compact and effective. In this paper, we focus\non the speech emotion recognition task and propose an improved emotion-specific\npretrained encoder called Vesper. Vesper is pretrained on a speech dataset\nbased on WavLM and takes into account emotional characteristics. To enhance\nsensitivity to emotional information, Vesper employs an emotion-guided masking\nstrategy to identify the regions that need masking. Subsequently, Vesper\nemploys hierarchical and cross-layer self-supervision to improve its ability to\ncapture acoustic and semantic representations, both of which are crucial for\nemotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D\ndatasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12\nlayers, and the performance of Vesper with 12 layers surpasses that of WavLM\nLarge with 24 layers.", "published": "2023-07-20 10:42:16", "link": "http://arxiv.org/abs/2307.10757v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "\"It Felt Like Having a Second Mind\": Investigating Human-AI\n  Co-creativity in Prewriting with Large Language Models", "abstract": "Prewriting is the process of discovering and developing ideas before a first\ndraft, which requires divergent thinking and often implies unstructured\nstrategies such as diagramming, outlining, free-writing, etc. Although large\nlanguage models (LLMs) have been demonstrated to be useful for a variety of\ntasks including creative writing, little is known about how users would\ncollaborate with LLMs to support prewriting. The preferred collaborative role\nand initiative of LLMs during such a creativity process is also unclear. To\ninvestigate human-LLM collaboration patterns and dynamics during prewriting, we\nconducted a three-session qualitative study with 15 participants in two\ncreative tasks: story writing and slogan writing. The findings indicated that\nduring collaborative prewriting, there appears to be a three-stage iterative\nHuman-AI Co-creativity process that includes Ideation, Illumination, and\nImplementation stages. This collaborative process champions the human in a\ndominant role, in addition to mixed and shifting levels of initiative that\nexist between humans and LLMs. This research also reports on collaboration\nbreakdowns that occur during this process, user perceptions of using existing\nLLMs during Human-AI Co-creativity, and discusses design implications to\nsupport this co-creativity process.", "published": "2023-07-20 16:55:25", "link": "http://arxiv.org/abs/2307.10811v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.m; K.4.0"], "primary_category": "cs.HC"}
{"title": "Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other\n  Languages", "abstract": "In a conventional Speech emotion recognition (SER) task, a classifier for a\ngiven language is trained on a pre-existing dataset for that same language.\nHowever, where training data for a language does not exist, data from other\nlanguages can be used instead. We experiment with cross-lingual and\nmultilingual SER, working with Amharic, English, German and URDU. For Amharic,\nwe use our own publicly-available Amharic Speech Emotion Dataset (ASED). For\nEnglish, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.\nWe followed previous research in mapping labels for all datasets to just two\nclasses, positive and negative. Thus we can compare performance on different\nlanguages directly, and combine languages for training and testing. In\nExperiment 1, monolingual SER trials were carried out using three classifiers,\nAlexNet, VGGE (a proposed variant of VGG), and ResNet50. Results averaged for\nthe three models were very similar for ASED and RAVDESS, suggesting that\nAmharic and English SER are equally difficult. Similarly, German SER is more\ndifficult, and Urdu SER is easier. In Experiment 2, we trained on one language\nand tested on another, in both directions for each pair: Amharic<->German,\nAmharic<->English, and Amharic<->Urdu. Results with Amharic as target suggested\nthat using English or German as source will give the best result. In Experiment\n3, we trained on several non-Amharic languages and then tested on Amharic. The\nbest accuracy obtained was several percent greater than the best accuracy in\nExperiment 2, suggesting that a better result can be obtained when using two or\nthree non-Amharic languages for training than when using just one non-Amharic\nlanguage. Overall, the results suggest that cross-lingual and multilingual\ntraining can be an effective strategy for training a SER classifier when\nresources for a language are scarce.", "published": "2023-07-20 12:24:23", "link": "http://arxiv.org/abs/2307.10814v1", "categories": ["cs.CL", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Divide & Bind Your Attention for Improved Generative Semantic Nursing", "abstract": "Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., \"a cat and a dog\". However, its efficacy declines when dealing with more\ncomplex prompts, and it does not explicitly address the problem of improper\nattribute binding. To address the challenges posed by complex prompts or\nscenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.", "published": "2023-07-20 13:33:28", "link": "http://arxiv.org/abs/2307.10864v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with\n  Human Feedback", "abstract": "Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.", "published": "2023-07-20 13:40:22", "link": "http://arxiv.org/abs/2307.10867v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MASR: Multi-label Aware Speech Representation", "abstract": "In the recent years, speech representation learning is constructed primarily\nas a self-supervised learning (SSL) task, using the raw audio signal alone,\nwhile ignoring the side-information that is often available for a given speech\nrecording. In this paper, we propose MASR, a Multi-label Aware Speech\nRepresentation learning framework, which addresses the aforementioned\nlimitations. MASR enables the inclusion of multiple external knowledge sources\nto enhance the utilization of meta-data information. The external knowledge\nsources are incorporated in the form of sample-level pair-wise similarity\nmatrices that are useful in a hard-mining loss. A key advantage of the MASR\nframework is that it can be combined with any choice of SSL method. Using MASR\nrepresentations, we perform evaluations on several downstream tasks such as\nlanguage identification, speech recognition and other non-semantic tasks such\nas speaker and emotion recognition. In these experiments, we illustrate\nsignificant performance improvements for the MASR over other established\nbenchmarks. We perform a detailed analysis on the language identification task\nto provide insights on how the proposed loss function enables the\nrepresentations to separate closely related languages.", "published": "2023-07-20 16:09:57", "link": "http://arxiv.org/abs/2307.10982v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Pretrained ASR and LM to Perform Sequence Generation for\n  Spoken Language Understanding", "abstract": "There has been an increased interest in the integration of pretrained speech\nrecognition (ASR) and language models (LM) into the SLU framework. However,\nprior methods often struggle with a vocabulary mismatch between pretrained\nmodels, and LM cannot be directly utilized as they diverge from its NLU\nformulation. In this study, we propose a three-pass end-to-end (E2E) SLU system\nthat effectively integrates ASR and LM subnetworks into the SLU formulation for\nsequence generation tasks. In the first pass, our architecture predicts ASR\ntranscripts using the ASR subnetwork. This is followed by the LM subnetwork,\nwhich makes an initial SLU prediction. Finally, in the third pass, the\ndeliberation subnetwork conditions on representations from the ASR and LM\nsubnetworks to make the final prediction. Our proposed three-pass SLU system\nshows improved performance over cascaded and E2E SLU models on two benchmark\nSLU datasets, SLURP and SLUE, especially on acoustically challenging\nutterances.", "published": "2023-07-20 16:34:40", "link": "http://arxiv.org/abs/2307.11005v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding\n  Models", "abstract": "Jina Embeddings constitutes a set of high-performance sentence embedding\nmodels adept at translating textual inputs into numerical representations,\ncapturing the semantics of the text. These models excel in applications like\ndense retrieval and semantic textual similarity. This paper details the\ndevelopment of Jina Embeddings, starting with the creation of high-quality\npairwise and triplet datasets. It underlines the crucial role of data cleaning\nin dataset preparation, offers in-depth insights into the model training\nprocess, and concludes with a comprehensive performance evaluation using the\nMassive Text Embedding Benchmark (MTEB). Furthermore, to increase the model's\nawareness of grammatical negation, we construct a novel training and evaluation\ndataset of negated and non-negated statements, which we make publicly available\nto the community.", "published": "2023-07-20 20:37:24", "link": "http://arxiv.org/abs/2307.11224v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "H.3.1; H.3.3; I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "Transsion TSUP's speech recognition system for ASRU 2023 MADASR\n  Challenge", "abstract": "This paper presents a speech recognition system developed by the Transsion\nSpeech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge.\nThe system focuses on adapting ASR models for low-resource Indian languages and\ncovers all four tracks of the challenge. For tracks 1 and 2, the acoustic model\nutilized a squeezeformer encoder and bidirectional transformer decoder with\njoint CTC-Attention training loss. Additionally, an external KenLM language\nmodel was used during TLG beam search decoding. For tracks 3 and 4, pretrained\nIndicWhisper models were employed and finetuned on both the challenge dataset\nand publicly available datasets. The whisper beam search decoding was also\nmodified to support an external KenLM language model, which enabled better\nutilization of the additional text provided by the challenge. The proposed\nmethod achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97%\nfor Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and\n15.48% for Bhojpuri language in the four tracks. These results demonstrate the\neffectiveness of the proposed method.", "published": "2023-07-20 00:55:01", "link": "http://arxiv.org/abs/2307.11778v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Extractive-Abstractive Axis: Measuring Content \"Borrowing\" in\n  Generative Language Models", "abstract": "Generative language models produce highly abstractive outputs by design, in\ncontrast to extractive responses in search engines. Given this characteristic\nof LLMs and the resulting implications for content Licensing & Attribution, we\npropose the the so-called Extractive-Abstractive axis for benchmarking\ngenerative models and highlight the need for developing corresponding metrics,\ndatasets and annotation guidelines. We limit our discussion to the text\nmodality.", "published": "2023-07-20 02:12:00", "link": "http://arxiv.org/abs/2307.11779v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Cognitive Judgements Differ From Human", "abstract": "Large Language Models (LLMs) have lately been on the spotlight of\nresearchers, businesses, and consumers alike. While the linguistic capabilities\nof such models have been studied extensively, there is growing interest in\ninvestigating them as cognitive subjects. In the present work I examine GPT-3\nand ChatGPT capabilities on an limited-data inductive reasoning task from the\ncognitive science literature. The results suggest that these models' cognitive\njudgements are not human-like.", "published": "2023-07-20 16:22:36", "link": "http://arxiv.org/abs/2307.11787v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Applying QNLP to sentiment analysis in finance", "abstract": "As an application domain where the slightest qualitative improvements can\nyield immense value, finance is a promising candidate for early quantum\nadvantage. Focusing on the rapidly advancing field of Quantum Natural Language\nProcessing (QNLP), we explore the practical applicability of the two central\napproaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the\nproblem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data\ngeneration approach, we conduct a case study with more than 1000 realistic\nsentences and find that QLSTMs can be trained substantially faster than\nDisCoCat while also achieving close to classical results for their available\nsoftware implementations.", "published": "2023-07-20 18:30:35", "link": "http://arxiv.org/abs/2307.11788v3", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "What Twitter Data Tell Us about the Future?", "abstract": "Anticipation is a fundamental human cognitive ability that involves thinking\nabout and living towards the future. While language markers reflect\nanticipatory thinking, research on anticipation from the perspective of natural\nlanguage processing is limited. This study aims to investigate the futures\nprojected by futurists on Twitter and explore the impact of language cues on\nanticipatory thinking among social media users. We address the research\nquestions of what futures Twitter's futurists anticipate and share, and how\nthese anticipated futures can be modeled from social data. To investigate this,\nwe review related works on anticipation, discuss the influence of language\nmarkers and prestigious individuals on anticipatory thinking, and present a\ntaxonomy system categorizing futures into \"present futures\" and \"future\npresent\". This research presents a compiled dataset of over 1 million publicly\nshared tweets by future influencers and develops a scalable NLP pipeline using\nSOTA models. The study identifies 15 topics from the LDA approach and 100\ndistinct topics from the BERTopic approach within the futurists' tweets. These\nfindings contribute to the research on topic modelling and provide insights\ninto the futures anticipated by Twitter's futurists. The research demonstrates\nthe futurists' language cues signals futures-in-the-making that enhance social\nmedia users to anticipate their own scenarios and respond to them in present.\nThe fully open-sourced dataset, interactive analysis, and reproducible source\ncode are available for further exploration.", "published": "2023-07-20 14:02:47", "link": "http://arxiv.org/abs/2308.02035v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Dynamic Large Language Models on Blockchains", "abstract": "Training and deploying the large language models requires a large mount of\ncomputational resource because the language models contain billions of\nparameters and the text has thousands of tokens. Another problem is that the\nlarge language models are static. They are fixed after the training process. To\ntackle these issues, in this paper, we propose to train and deploy the dynamic\nlarge language model on blockchains, which have high computation performance\nand are distributed across a network of computers. A blockchain is a secure,\ndecentralized, and transparent system that allows for the creation of a\ntamper-proof ledger for transactions without the need for intermediaries. The\ndynamic large language models can continuously learn from the user input after\nthe training process. Our method provides a new way to develop the large\nlanguage models and also sheds a light on the next generation artificial\nintelligence systems.", "published": "2023-07-20 03:26:57", "link": "http://arxiv.org/abs/2307.10549v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CV"}
{"title": "Meta-Transformer: A Unified Framework for Multimodal Learning", "abstract": "Multimodal learning aims to build models that can process and relate\ninformation from multiple modalities. Despite years of development in this\nfield, it still remains challenging to design a unified network for processing\nvarious modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point\nclouds, audio, video, time series, tabular data) due to the inherent gaps among\nthem. In this work, we propose a framework, named Meta-Transformer, that\nleverages a $\\textbf{frozen}$ encoder to perform multimodal perception without\nany paired multimodal training data. In Meta-Transformer, the raw input data\nfrom various modalities are mapped into a shared token space, allowing a\nsubsequent encoder with frozen parameters to extract high-level semantic\nfeatures of the input data. Composed of three main components: a unified data\ntokenizer, a modality-shared encoder, and task-specific heads for downstream\ntasks, Meta-Transformer is the first framework to perform unified learning\nacross 12 modalities with unpaired data. Experiments on different benchmarks\nreveal that Meta-Transformer can handle a wide range of tasks including\nfundamental perception (text, image, point cloud, audio, video), practical\napplication (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,\ntabular, and time-series). Meta-Transformer indicates a promising future for\ndeveloping unified multimodal intelligence with transformers. Code will be\navailable at https://github.com/invictus717/MetaTransformer", "published": "2023-07-20 12:10:29", "link": "http://arxiv.org/abs/2307.10802v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "PAS: Partial Additive Speech Data Augmentation Method for Noise Robust\n  Speaker Verification", "abstract": "Background noise reduces speech intelligibility and quality, making speaker\nverification (SV) in noisy environments a challenging task. To improve the\nnoise robustness of SV systems, additive noise data augmentation method has\nbeen commonly used. In this paper, we propose a new additive noise method,\npartial additive speech (PAS), which aims to train SV systems to be less\naffected by noisy environments. The experimental results demonstrate that PAS\noutperforms traditional additive noise in terms of equal error rates (EER),\nwith relative improvements of 4.64% and 5.01% observed in SE-ResNet34 and\nECAPA-TDNN. We also show the effectiveness of proposed method by analyzing\nattention modules and visualizing speaker embeddings.", "published": "2023-07-20 06:50:56", "link": "http://arxiv.org/abs/2307.10628v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning and Bias Correction with Pre-trained Audio Embeddings", "abstract": "Deep neural network models have become the dominant approach to a large\nvariety of tasks within music information retrieval (MIR). These models\ngenerally require large amounts of (annotated) training data to achieve high\naccuracy. Because not all applications in MIR have sufficient quantities of\ntraining data, it is becoming increasingly common to transfer models across\ndomains. This approach allows representations derived for one task to be\napplied to another, and can result in high accuracy with less stringent\ntraining data requirements for the downstream task. However, the properties of\npre-trained audio embeddings are not fully understood. Specifically, and unlike\ntraditionally engineered features, the representations extracted from\npre-trained deep networks may embed and propagate biases from the model's\ntraining regime. This work investigates the phenomenon of bias propagation in\nthe context of pre-trained audio representations for the task of instrument\nrecognition. We first demonstrate that three different pre-trained\nrepresentations (VGGish, OpenL3, and YAMNet) exhibit comparable performance\nwhen constrained to a single dataset, but differ in their ability to generalize\nacross datasets (OpenMIC and IRMAS). We then investigate dataset identity and\ngenre distribution as potential sources of bias. Finally, we propose and\nevaluate post-processing countermeasures to mitigate the effects of bias, and\nimprove generalization across datasets.", "published": "2023-07-20 12:53:18", "link": "http://arxiv.org/abs/2307.10834v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer", "abstract": "Expressive speech synthesis models are trained by adding corpora with diverse\nspeakers, various emotions, and different speaking styles to the dataset, in\norder to control various characteristics of speech and generate the desired\nvoice. In this paper, we propose a style control (SC) VALL-E model based on the\nneural codec language model (called VALL-E), which follows the structure of the\ngenerative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input\nfrom text sentences and prompt audio and is designed to generate controllable\nspeech by not simply mimicking the characteristics of the prompt audio but by\ncontrolling the attributes to produce diverse voices. We identify tokens in the\nstyle embedding matrix of the newly designed style network that represent\nattributes such as emotion, speaking rate, pitch, and voice intensity, and\ndesign a model that can control these attributes. To evaluate the performance\nof SC VALL-E, we conduct comparative experiments with three representative\nexpressive speech synthesis models: global style token (GST) Tacotron2,\nvariational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word\nerror rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as\nevaluation metrics to assess the accuracy of generated sentences. For comparing\nthe quality of synthesized speech, we measure comparative mean option score\n(CMOS) and similarity mean option score (SMOS). To evaluate the style control\nability of the generated speech, we observe the changes in F0 and\nmel-spectrogram by modifying the trained tokens. When using prompt audio that\nis not present in the training data, SC VALL-E generates a variety of\nexpressive sounds and demonstrates competitive performance compared to the\nexisting models. Our implementation, pretrained models, and audio samples are\nlocated on GitHub.", "published": "2023-07-20 03:28:06", "link": "http://arxiv.org/abs/2307.10550v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Genre Classification with ResNet and Bi-GRU Using Visual\n  Spectrograms", "abstract": "Music recommendation systems have emerged as a vital component to enhance\nuser experience and satisfaction for the music streaming services, which\ndominates music consumption. The key challenge in improving these recommender\nsystems lies in comprehending the complexity of music data, specifically for\nthe underpinning music genre classification. The limitations of manual genre\nclassification have highlighted the need for a more advanced system, namely the\nAutomatic Music Genre Classification (AMGC) system. While traditional machine\nlearning techniques have shown potential in genre classification, they heavily\nrely on manually engineered features and feature selection, failing to capture\nthe full complexity of music data. On the other hand, deep learning\nclassification architectures like the traditional Convolutional Neural Networks\n(CNN) are effective in capturing the spatial hierarchies but struggle to\ncapture the temporal dynamics inherent in music data. To address these\nchallenges, this study proposes a novel approach using visual spectrograms as\ninput, and propose a hybrid model that combines the strength of the Residual\nneural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is\ndesigned to provide a more comprehensive analysis of music data, offering the\npotential to improve the music recommender systems through achieving a more\ncomprehensive analysis of music data and hence potentially more accurate genre\nclassification.", "published": "2023-07-20 11:10:06", "link": "http://arxiv.org/abs/2307.10773v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Quality Assessment of Omnidirectional Audio-visual Signals", "abstract": "Omnidirectional videos (ODVs) play an increasingly important role in the\napplication fields of medical, education, advertising, tourism, etc. Assessing\nthe quality of ODVs is significant for service-providers to improve the user's\nQuality of Experience (QoE). However, most existing quality assessment studies\nfor ODVs only focus on the visual distortions of videos, while ignoring that\nthe overall QoE also depends on the accompanying audio signals. In this paper,\nwe first establish a large-scale audio-visual quality assessment dataset for\nomnidirectional videos, which includes 375 distorted omnidirectional\naudio-visual (A/V) sequences generated from 15 high-quality pristine\nomnidirectional A/V contents, and the corresponding perceptual audio-visual\nquality scores. Then, we design three baseline methods for full-reference\nomnidirectional audio-visual quality assessment (OAVQA), which combine existing\nstate-of-the-art single-mode audio and video QA models via multimodal fusion\nstrategies. We validate the effectiveness of the A/V multimodal fusion method\nfor OAVQA on our dataset, which provides a new benchmark for omnidirectional\nQoE evaluation. Our dataset is available at https://github.com/iamazxl/OAVQA.", "published": "2023-07-20 12:21:26", "link": "http://arxiv.org/abs/2307.10813v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV", "I.4.0; I.5.4"], "primary_category": "cs.CV"}
{"title": "Globally Normalising the Transducer for Streaming Speech Recognition", "abstract": "The Transducer (e.g. RNN-Transducer or Conformer-Transducer) generates an\noutput label sequence as it traverses the input sequence. It is straightforward\nto use in streaming mode, where it generates partial hypotheses before the\ncomplete input has been seen. This makes it popular in speech recognition.\nHowever, in streaming mode the Transducer has a mathematical flaw which, simply\nput, restricts the model's ability to change its mind. The fix is to replace\nlocal normalisation (e.g. a softmax) with global normalisation, but then the\nloss function becomes impossible to evaluate exactly. A recent paper proposes\nto solve this by approximating the model, severely degrading performance.\nInstead, this paper proposes to approximate the loss function, allowing global\nnormalisation to apply to a state-of-the-art streaming model. Global\nnormalisation reduces its word error rate by 9-11% relative, closing almost\nhalf the gap between streaming and lookahead mode.", "published": "2023-07-20 16:04:07", "link": "http://arxiv.org/abs/2307.10975v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "68T10"], "primary_category": "eess.AS"}
{"title": "Progressive distillation diffusion for raw music generation", "abstract": "This paper aims to apply a new deep learning approach to the task of\ngenerating raw audio files. It is based on diffusion models, a recent type of\ndeep generative model. This new type of method has recently shown outstanding\nresults with image generation. A lot of focus has been given to those models by\nthe computer vision community. On the other hand, really few have been given\nfor other types of applications such as music generation in waveform domain.\n  In this paper the model for unconditional generating applied to music is\nimplemented: Progressive distillation diffusion with 1D U-Net. Then, a\ncomparison of different parameters of diffusion and their value in a full\nresult is presented. One big advantage of the methods implemented through this\nwork is the fact that the model is able to deal with progressing audio\nprocessing and generating , using transformation from 1-channel 128 x 384 to\n3-channel 128 x 128 mel-spectrograms and looped generation. The empirical\ncomparisons are realized across different self-collected datasets.", "published": "2023-07-20 16:25:00", "link": "http://arxiv.org/abs/2307.10994v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Brain2Music: Reconstructing Music from Human Brain Activity", "abstract": "The process of reconstructing experiences from human brain activity offers a\nunique lens into how the brain interprets and represents the world. In this\npaper, we introduce a method for reconstructing music from brain activity,\ncaptured using functional magnetic resonance imaging (fMRI). Our approach uses\neither music retrieval or the MusicLM music generation model conditioned on\nembeddings derived from fMRI data. The generated music resembles the musical\nstimuli that human subjects experienced, with respect to semantic properties\nlike genre, instrumentation, and mood. We investigate the relationship between\ndifferent components of MusicLM and brain activity through a voxel-wise\nencoding modeling analysis. Furthermore, we discuss which brain regions\nrepresent information derived from purely textual descriptions of music\nstimuli. We provide supplementary material including examples of the\nreconstructed music at https://google-research.github.io/seanet/brain2music", "published": "2023-07-20 17:55:17", "link": "http://arxiv.org/abs/2307.11078v1", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
