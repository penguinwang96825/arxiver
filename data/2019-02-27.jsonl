{"title": "A Framework for Decoding Event-Related Potentials from Text", "abstract": "We propose a novel framework for modeling event-related potentials (ERPs)\ncollected during reading that couples pre-trained convolutional decoders with a\nlanguage model. Using this framework, we compare the abilities of a variety of\nexisting and novel sentence processing models to reconstruct ERPs. We find that\nmodern contextual word embeddings underperform surprisal-based models but that,\ncombined, the two outperform either on its own.", "published": "2019-02-27 01:43:48", "link": "http://arxiv.org/abs/1902.10296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CN-Probase: A Data-driven Approach for Large-scale Chinese Taxonomy\n  Construction", "abstract": "Taxonomies play an important role in machine intelligence. However, most\nwell-known taxonomies are in English, and non-English taxonomies, especially\nChinese ones, are still very rare. In this paper, we focus on automatic Chinese\ntaxonomy construction and propose an effective generation and verification\nframework to build a large-scale and high-quality Chinese taxonomy. In the\ngeneration module, we extract isA relations from multiple sources of Chinese\nencyclopedia, which ensures the coverage. To further improve the precision of\ntaxonomy, we apply three heuristic approaches in verification module. As a\nresult, we construct the largest Chinese taxonomy with high precision about 95%\ncalled CN-Probase. Our taxonomy has been deployed on Aliyun, with over 82\nmillion API calls in six months.", "published": "2019-02-27 04:28:33", "link": "http://arxiv.org/abs/1902.10326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Constrained Advertising Keyword Generation", "abstract": "Advertising (ad for short) keyword suggestion is important for sponsored\nsearch to improve online advertising and increase search revenue. There are two\ncommon challenges in this task. First, the keyword bidding problem: hot ad\nkeywords are very expensive for most of the advertisers because more\nadvertisers are bidding on more popular keywords, while unpopular keywords are\ndifficult to discover. As a result, most ads have few chances to be presented\nto the users. Second, the inefficient ad impression issue: a large proportion\nof search queries, which are unpopular yet relevant to many ad keywords, have\nno ads presented on their search result pages. Existing retrieval-based or\nmatching-based methods either deteriorate the bidding competition or are unable\nto suggest novel keywords to cover more queries, which leads to inefficient ad\nimpressions. To address the above issues, this work investigates to use\ngenerative neural networks for keyword generation in sponsored search. Given a\npurchased keyword (a word sequence) as input, our model can generate a set of\nkeywords that are not only relevant to the input but also satisfy the domain\nconstraint which enforces that the domain category of a generated keyword is as\nexpected. Furthermore, a reinforcement learning algorithm is proposed to\nadaptively utilize domain-specific information in keyword generation. Offline\nevaluation shows that the proposed model can generate keywords that are\ndiverse, novel, relevant to the source keyword, and accordant with the domain\nconstraint. Online evaluation shows that generative models can improve coverage\n(COV), click-through rate (CTR), and revenue per mille (RPM) substantially in\nsponsored search.", "published": "2019-02-27 07:54:54", "link": "http://arxiv.org/abs/1902.10374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Questions by Learning What not to Generate", "abstract": "Automatic question generation is an important technique that can improve the\ntraining of question answering, help chatbots to start or continue a\nconversation with humans, and provide assessment materials for educational\npurposes. Existing neural question generation models are not sufficient mainly\ndue to their inability to properly model the process of how each word in the\nquestion is selected, i.e., whether repeating the given passage or being\ngenerated from a vocabulary. In this paper, we propose our Clue Guided Copy\nNetwork for Question Generation (CGC-QG), which is a sequence-to-sequence\ngenerative model with copying mechanism, yet employing a variety of novel\ncomponents and techniques to boost the performance of question generation. In\nCGC-QG, we design a multi-task labeling strategy to identify whether a question\nword should be copied from the input passage or be generated instead, guiding\nthe model to learn the accurate boundaries between copying and generation.\nFurthermore, our input passage encoder takes as input, among a diverse range of\nother features, the prediction made by a clue word predictor, which helps\nidentify whether each word in the input passage is a potential clue to be\ncopied into the target question. The clue word predictor is designed based on a\nnovel application of Graph Convolutional Networks onto a syntactic dependency\ntree representation of each passage, thus being able to predict clue words only\nbased on their context in the passage and their relative positions to the\nanswer in the tree. We jointly train the clue prediction as well as question\ngeneration with multi-task learning and a number of practical strategies to\nreduce the complexity. Extensive evaluations show that our model significantly\nimproves the performance of question generation and out-performs all previous\nstate-of-the-art neural question generation models by a substantial margin.", "published": "2019-02-27 09:56:19", "link": "http://arxiv.org/abs/1902.10418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation with Knowledge Distillation", "abstract": "Multilingual machine translation, which translates multiple languages with a\nsingle model, has attracted much attention due to its efficiency of offline\ntraining and online serving. However, traditional multilingual translation\nusually yields inferior accuracy compared with the counterpart using individual\nmodels for each language pair, due to language diversity and model capacity\nlimitations. In this paper, we propose a distillation-based approach to boost\nthe accuracy of multilingual machine translation. Specifically, individual\nmodels are first trained and regarded as teachers, and then the multilingual\nmodel is trained to fit the training data and match the outputs of individual\nmodels simultaneously through knowledge distillation. Experiments on IWSLT, WMT\nand Ted talk translation datasets demonstrate the effectiveness of our method.\nParticularly, we show that one model is enough to handle multiple languages (up\nto 44 languages in our experiment), with comparable or even better accuracy\nthan individual models.", "published": "2019-02-27 11:14:16", "link": "http://arxiv.org/abs/1902.10461v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Induction Networks for Few-Shot Text Classification", "abstract": "Text classification tends to struggle when data is deficient or when it needs\nto adapt to unseen classes. In such challenging scenarios, recent studies have\nused meta-learning to simulate the few-shot task, in which new queries are\ncompared to a small support set at the sample-wise level. However, this\nsample-wise comparison may be severely disturbed by the various expressions in\nthe same class. Therefore, we should be able to learn a general representation\nof each class in the support set and then compare it to new queries. In this\npaper, we propose a novel Induction Network to learn such a generalized\nclass-wise representation, by innovatively leveraging the dynamic routing\nalgorithm in meta-learning. In this way, we find the model is able to induce\nand generalize better. We evaluate the proposed model on a well-studied\nsentiment classification dataset (English) and a real-world dialogue intent\nclassification dataset (Chinese). Experiment results show that on both\ndatasets, the proposed model significantly outperforms the existing\nstate-of-the-art approaches, proving the effectiveness of class-wise\ngeneralization in few-shot text classification.", "published": "2019-02-27 12:16:55", "link": "http://arxiv.org/abs/1902.10482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion", "abstract": "Sentence fusion is the task of joining several independent sentences into a\nsingle coherent text. Current datasets for sentence fusion are small and\ninsufficient for training modern neural models. In this paper, we propose a\nmethod for automatically-generating fusion examples from raw text and present\nDiscoFuse, a large scale dataset for discourse-based sentence fusion. We author\na set of rules for identifying a diverse set of discourse phenomena in raw\ntext, and decomposing the text into two independent sentences. We apply our\napproach on two document collections: Wikipedia and Sports articles, yielding\n60 million fusion examples annotated with discourse information required to\nreconstruct the fused text. We develop a sequence-to-sequence model on\nDiscoFuse and thoroughly analyze its strengths and weaknesses with respect to\nthe various discourse phenomena, using both automatic as well as human\nevaluation. Finally, we conduct transfer learning experiments with WebSplit, a\nrecent dataset for text simplification. We show that pretraining on DiscoFuse\nsubstantially improves performance on WebSplit when viewed as a sentence fusion\ntask.", "published": "2019-02-27 13:48:59", "link": "http://arxiv.org/abs/1902.10526v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiresolution Graph Attention Networks for Relevance Matching", "abstract": "A large number of deep learning models have been proposed for the text\nmatching problem, which is at the core of various typical natural language\nprocessing (NLP) tasks. However, existing deep models are mainly designed for\nthe semantic matching between a pair of short texts, such as paraphrase\nidentification and question answering, and do not perform well on the task of\nrelevance matching between short-long text pairs. This is partially due to the\nfact that the essential characteristics of short-long text matching have not\nbeen well considered in these deep models. More specifically, these methods\nfail to handle extreme length discrepancy between text pieces and neither can\nthey fully characterize the underlying structural information in long text\ndocuments. In this paper, we are especially interested in relevance matching\nbetween a piece of short text and a long document, which is critical to\nproblems like query-document matching in information retrieval and web\nsearching. To extract the structural information of documents, an undirected\ngraph is constructed, with each vertex representing a keyword and the weight of\nan edge indicating the degree of interaction between keywords. Based on the\nkeyword graph, we further propose a Multiresolution Graph Attention Network to\nlearn multi-layered representations of vertices through a Graph Convolutional\nNetwork (GCN), and then match the short text snippet with the graphical\nrepresentation of the document with the attention mechanisms applied over each\nlayer of the GCN. Experimental results on two datasets demonstrate that our\ngraph approach outperforms other state-of-the-art deep matching models.", "published": "2019-02-27 15:19:36", "link": "http://arxiv.org/abs/1902.10580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When a Tweet is Actually Sexist. A more Comprehensive Classification of\n  Different Online Harassment Categories and The Challenges in NLP", "abstract": "Sexism is very common in social media and makes the boundaries of freedom\ntighter for feminist and female users. There is still no comprehensive\nclassification of sexism attracting natural language processing techniques.\nCategorizing sexism in social media in the categories of hostile or benevolent\nsexism are so general that simply ignores the other types of sexism happening\nin these media. This paper proposes a more comprehensive and in-depth\ncategories of online harassment in social media e.g. twitter into the following\ncategories, \"Indirect harassment\", \"Information threat\", \"sexual harassment\",\n\"Physical harassment\" and \"Not sexist\" and address the challenge of labeling\nthem along with presenting the classification result of the categories. It is\npreliminary work applying machine learning to learn the concept of sexism and\ndistinguishes itself by looking at more precise categories of sexism in social\nmedia.", "published": "2019-02-27 15:26:45", "link": "http://arxiv.org/abs/1902.10584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical\n  Composition", "abstract": "Building meaningful phrase representations is challenging because phrase\nmeanings are not simply the sum of their constituent meanings. Lexical\ncomposition can shift the meanings of the constituent words and introduce\nimplicit information. We tested a broad range of textual representations for\ntheir capacity to address these issues. We found that as expected,\ncontextualized word representations perform better than static word embeddings,\nmore so on detecting meaning shift than in recovering implicit information, in\nwhich their performance is still far from that of humans. Our evaluation suite,\nincluding 5 tasks related to lexical composition effects, can serve future\nresearch aiming to improve such representations.", "published": "2019-02-27 16:16:37", "link": "http://arxiv.org/abs/1902.10618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zoho at SemEval-2019 Task 9: Semi-supervised Domain Adaptation using\n  Tri-training for Suggestion Mining", "abstract": "This paper describes our submission for the SemEval-2019 Suggestion Mining\ntask. A simple Convolutional Neural Network (CNN) classifier with contextual\nword representations from a pre-trained language model was used for sentence\nclassification. The model is trained using tri-training, a semi-supervised\nbootstrapping mechanism for labelling unseen data. Tri-training proved to be an\neffective technique to accommodate domain shift for cross-domain suggestion\nmining (Subtask B) where there is no hand labelled training data. For in-domain\nevaluation (Subtask A), we use the same technique to augment the training set.\nOur system ranks thirteenth in Subtask A with an $F_1$-score of 68.07 and third\nin Subtask B with an $F_1$-score of 81.94.", "published": "2019-02-27 16:33:32", "link": "http://arxiv.org/abs/1902.10623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F10-SGD: Fast Training of Elastic-net Linear Models for Text\n  Classification and Named-entity Recognition", "abstract": "Voice-assistants text classification and named-entity recognition (NER)\nmodels are trained on millions of example utterances. Because of the large\ndatasets, long training time is one of the bottlenecks for releasing improved\nmodels. In this work, we develop F10-SGD, a fast optimizer for text\nclassification and NER elastic-net linear models. On internal datasets, F10-SGD\nprovides 4x reduction in training time compared to the OWL-QN optimizer without\nloss of accuracy or increase in model size. Furthermore, we incorporate biased\nsampling that prioritizes harder examples towards the end of the training. As a\nresult, in addition to faster training, we were able to obtain statistically\nsignificant accuracy improvements for NER.\n  On public datasets, F10-SGD obtains 22% faster training time compared to\nFastText for text classification. And, 4x reduction in training time compared\nto CRFSuite OWL-QN for NER.", "published": "2019-02-27 17:32:15", "link": "http://arxiv.org/abs/1902.10649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Machine Learning Approach to Comment Toxicity Classification", "abstract": "Now-a-days, derogatory comments are often made by one another, not only in\noffline environment but also immensely in online environments like social\nnetworking websites and online communities. So, an Identification combined with\nPrevention System in all social networking websites and applications, including\nall the communities, existing in the digital world is a necessity. In such a\nsystem, the Identification Block should identify any negative online behaviour\nand should signal the Prevention Block to take action accordingly. This study\naims to analyse any piece of text and detecting different types of toxicity\nlike obscenity, threats, insults and identity-based hatred. The labelled\nWikipedia Comment Dataset prepared by Jigsaw is used for the purpose. A\n6-headed Machine Learning tf-idf Model has been made and trained separately,\nyielding a Mean Validation Accuracy of 98.08% and Absolute Validation Accuracy\nof 91.61%. Such an Automated System should be deployed for enhancing healthy\nonline conversation", "published": "2019-02-27 07:21:44", "link": "http://arxiv.org/abs/1903.06765v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Large a Vocabulary Does Text Classification Need? A Variational\n  Approach to Vocabulary Selection", "abstract": "With the rapid development in deep learning, deep neural networks have been\nwidely adopted in many real-life natural language applications. Under deep\nneural networks, a pre-defined vocabulary is required to vectorize text inputs.\nThe canonical approach to select pre-defined vocabulary is based on the word\nfrequency, where a threshold is selected to cut off the long tail distribution.\nHowever, we observed that such simple approach could easily lead to under-sized\nvocabulary or over-sized vocabulary issues. Therefore, we are interested in\nunderstanding how the end-task classification accuracy is related to the\nvocabulary size and what is the minimum required vocabulary size to achieve a\nspecific performance. In this paper, we provide a more sophisticated\nvariational vocabulary dropout (VVD) based on variational dropout to perform\nvocabulary selection, which can intelligently select the subset of the\nvocabulary to achieve the required performance. To evaluate different\nalgorithms on the newly proposed vocabulary selection problem, we propose two\nnew metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X\\% Accuracy\nDrop. Through extensive experiments on various NLP classification tasks, our\nvariational framework is shown to significantly outperform the frequency-based\nand other selection baselines on these metrics.", "published": "2019-02-27 05:57:13", "link": "http://arxiv.org/abs/1902.10339v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Editorial Network for Enhanced Document Summarization", "abstract": "We suggest a new idea of Editorial Network - a mixed extractive-abstractive\nsummarization approach, which is applied as a post-processing step over a given\nsequence of extracted sentences. Our network tries to imitate the decision\nprocess of a human editor during summarization. Within such a process, each\nextracted sentence may be either kept untouched, rephrased or completely\nrejected. We further suggest an effective way for training the \"editor\" based\non a novel soft-labeling approach. Using the CNN/DailyMail dataset we\ndemonstrate the effectiveness of our approach compared to state-of-the-art\nextractive-only or abstractive-only baseline methods.", "published": "2019-02-27 07:04:47", "link": "http://arxiv.org/abs/1902.10360v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Viable Dependency Parsing as Sequence Labeling", "abstract": "We recast dependency parsing as a sequence labeling problem, exploring\nseveral encodings of dependency trees as labels. While dependency parsing by\nmeans of sequence labeling had been attempted in existing work, results\nsuggested that the technique was impractical. We show instead that with a\nconventional BiLSTM-based model it is possible to obtain fast and accurate\nparsers. These parsers are conceptually simple, not needing traditional parsing\nalgorithms or auxiliary structures. However, experiments on the PTB and a\nsample of UD treebanks show that they provide a good speed-accuracy tradeoff,\nwith results competitive with more complex approaches.", "published": "2019-02-27 13:08:27", "link": "http://arxiv.org/abs/1902.10505v2", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained\n  Language Models", "abstract": "A growing number of state-of-the-art transfer learning methods employ\nlanguage models pretrained on large generic corpora. In this paper we present a\nconceptually simple and effective transfer learning approach that addresses the\nproblem of catastrophic forgetting. Specifically, we combine the task-specific\noptimization function with an auxiliary language model objective, which is\nadjusted during the training process. This preserves language regularities\ncaptured by language models, while enabling sufficient adaptation for solving\nthe target task. Our method does not require pretraining or finetuning separate\ncomponents of the network and we train our models end-to-end in a single step.\nWe present results on a variety of challenging affective and text\nclassification tasks, surpassing well established transfer learning methods\nwith greater level of complexity.", "published": "2019-02-27 14:17:12", "link": "http://arxiv.org/abs/1902.10547v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Attending to Discontinuity in Identification of\n  Multiword Expressions", "abstract": "We introduce a new method to tag Multiword Expressions (MWEs) using a\nlinguistically interpretable language-independent deep learning architecture.\nWe specifically target discontinuity, an under-explored aspect that poses a\nsignificant challenge to computational treatment of MWEs. Two neural\narchitectures are explored: Graph Convolutional Network (GCN) and multi-head\nself-attention. GCN leverages dependency parse information, and self-attention\nattends to long-range relations. We finally propose a combined model that\nintegrates complementary information from both through a gating mechanism. The\nexperiments on a standard multilingual dataset for verbal MWEs show that our\nmodel outperforms the baselines not only in the case of discontinuous MWEs but\nalso in overall F-score.", "published": "2019-02-27 18:01:53", "link": "http://arxiv.org/abs/1902.10667v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing the Perceived Severity of Cybersecurity Threats Reported on\n  Social Media", "abstract": "Breaking cybersecurity events are shared across a range of websites,\nincluding security blogs (FireEye, Kaspersky, etc.), in addition to social\nmedia platforms such as Facebook and Twitter. In this paper, we investigate\nmethods to analyze the severity of cybersecurity threats based on the language\nthat is used to describe them online. A corpus of 6,000 tweets describing\nsoftware vulnerabilities is annotated with authors' opinions toward their\nseverity. We show that our corpus supports the development of automatic\nclassifiers with high precision for this task. Furthermore, we demonstrate the\nvalue of analyzing users' opinions about the severity of threats reported\nonline as an early indicator of important software vulnerabilities. We present\na simple, yet effective method for linking software vulnerabilities reported in\ntweets to Common Vulnerabilities and Exposures (CVEs) in the National\nVulnerability Database (NVD). Using our predicted severity scores, we show that\nit is possible to achieve a Precision@50 of 0.86 when forecasting high severity\nvulnerabilities, significantly outperforming a baseline that is based on tweet\nvolume. Finally we show how reports of severe vulnerabilities online are\npredictive of real-world exploits.", "published": "2019-02-27 18:45:09", "link": "http://arxiv.org/abs/1902.10680v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Learning Hierarchical Discourse-level Structure for Fake News Detection", "abstract": "On the one hand, nowadays, fake news articles are easily propagated through\nvarious online media platforms and have become a grand threat to the\ntrustworthiness of information. On the other hand, our understanding of the\nlanguage of fake news is still minimal. Incorporating hierarchical\ndiscourse-level structure of fake and real news articles is one crucial step\ntoward a better understanding of how these articles are structured.\nNevertheless, this has rarely been investigated in the fake news detection\ndomain and faces tremendous challenges. First, existing methods for capturing\ndiscourse-level structure rely on annotated corpora which are not available for\nfake news datasets. Second, how to extract out useful information from such\ndiscovered structures is another challenge. To address these challenges, we\npropose Hierarchical Discourse-level Structure for Fake news detection. HDSF\nlearns and constructs a discourse-level structure for fake/real news articles\nin an automated and data-driven manner. Moreover, we identify insightful\nstructure-related properties, which can explain the discovered structures and\nboost our understating of fake news. Conducted experiments show the\neffectiveness of the proposed approach. Further structural analysis suggests\nthat real and fake news present substantial differences in the hierarchical\ndiscourse-level structures.", "published": "2019-02-27 00:03:17", "link": "http://arxiv.org/abs/1903.07389v6", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The VOiCES from a Distance Challenge 2019 Evaluation Plan", "abstract": "The \"VOiCES from a Distance Challenge 2019\" is designed to foster research in\nthe area of speaker recognition and automatic speech recognition (ASR) with the\nspecial focus on single channel distant/far-field audio, under noisy\nconditions. The main objectives of this challenge are to: (i) benchmark\nstate-of-the-art technology in the area of speaker recognition and automatic\nspeech recognition (ASR), (ii) support the development of new ideas and\ntechnologies in speaker recognition and ASR, (iii) support new research groups\nentering the field of distant/far-field speech processing, and (iv) provide a\nnew, publicly available dataset to the community that exhibits realistic\ndistance characteristics.", "published": "2019-02-27 23:22:39", "link": "http://arxiv.org/abs/1902.10828v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time detection, classification and DOA estimation of Unmanned\n  Aerial Vehicle", "abstract": "The present work deals with a new passive system for real-time detection,\nclassification and direction of arrival estimator of Unmanned Aerial Vehicles\n(UAVs). The proposed system composed of a very low cost hardware components,\ncomprises two different arrays of three or six-microphones, non-linear\namplification and filtering of the analog acoustic signal, avoiding also the\nsaturation effect in case where the UAV is located nearby to the microphones.\nAdvance array processing methods are used to detect and locate the wide-band\nsources in the near and far-field including array calibration and energy based\nbeamforming techniques. Moreover, oversampling techniques are adopted to\nincrease the acquired signals accuracy and to also decrease the quantization\nnoise. The classifier is based on the nearest neighbor rule of a normalized\nPower Spectral Density, the acoustic signature of the UAV spectrum in short\nperiods of time. The low-cost, low-power and high efficiency embedded processor\nSTM32F405RG is used for system implementation. Preliminary experimental results\nhave shown the effectiveness of the proposed approach.", "published": "2019-02-27 11:41:39", "link": "http://arxiv.org/abs/1902.11130v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Accurate Target Localization by using Artificial Pinnae of brown\n  long-eared bat", "abstract": "Echolocating bats locate the targets by echolocation. Many theoretical\nframeworks have been suggested the abilities of bats are related to the shapes\nof bats ears, but few artificial bat-like ears have been made to mimic the\nabilities, the difficulty of which lies in the determination of the elevation\nangle of the target. In this study, we present a device with artificial bat\npinnae modeling by the ears of brown long-eared bat (Plecotus auritus) which\ncan accurately estimate the elevation angle of the aerial target by virtue of\nactive sonar. An artificial neural-network with the labeled data obtained from\nechoes as the trained and tested data is used and optimized by a tenfold\ncross-validation technique. A decision method we named sliding window averaging\nalgorithm is designed for getting the estimation results of elevation. At last,\na right-angle pinnae construction is designed for determining direction of the\ntarget. The results show a higher accuracy for the direction determination of\nthe single target. The results also demonstrate that for the Plecotus auritus\nbat, not only the binaural shapes, but the binaural relative orientations also\nplay important roles in the target localization.", "published": "2019-02-27 01:15:54", "link": "http://arxiv.org/abs/1902.10291v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "eess.SP"}
