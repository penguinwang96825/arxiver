{"title": "Joint multifractality in the cross-correlations between grains \\& oilseeds indices and external uncertainties", "abstract": "This study investigates the relationships between agricultural spot markets\nand external uncertainties via the multifractal detrending moving-average\ncross-correlation analysis (MF-X-DMA). The dataset contains the Grains \\&\nOilseeds Index (GOI) and its five sub-indices of wheat, maize, soyabeans, rice,\nand barley. Moreover, we use three uncertainty proxies, namely, economic policy\nuncertainty (EPU), geopolitical risk (GPR), and volatility Index (VIX). We\nobserve the presence of multifractal cross-correlations between agricultural\nmarkets and uncertainties. Further, statistical tests show that maize has\nintrinsic joint multifractality with all the uncertainty proxies, exhibiting a\nhigh degree of sensitivity. Additionally, intrinsic multifractality among\nGOI-GPR, wheat-GPR and soyabeans-VIX is illustrated. However, other series have\napparent multifractal cross-correlations with high possibilities. Moreover, our\nanalysis suggests that among the three kinds of external uncertainties,\ngeopolitical risk has a relatively stronger association with grain prices.", "published": "2024-09-18 14:27:26", "link": "http://arxiv.org/abs/2410.02798v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "How does liquidity shape the yield curve?", "abstract": "The phenomenology of the forward rate curve (FRC) can be accurately\nunderstood by the fluctuations of a stiff elastic string (Le Coz and Bouchaud,\n2024). By relating the exogenous shocks driving such fluctuations to the\nsurprises in the order flows, we elevate the model from purely describing price\nvariations to a microstructural model that incorporates the joint dynamics of\nprices and order flows, accounting for both impact and cross-impact effects.\nRemarkably, this framework allows for at least the same explanatory power as\nexisting cross-impact models, while using significantly fewer parameters. In\naddition, our model generates liquidity-dependent correlations between the\nforward rate of one tenor and the order flow of another, consistent with recent\nempirical findings. We show that the model also account for the non-martingale\nbehavior of prices at short timescales.", "published": "2024-09-18 19:33:02", "link": "http://arxiv.org/abs/2409.12282v2", "categories": ["q-fin.TR", "cond-mat.stat-mech"], "primary_category": "q-fin.TR"}
{"title": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of\n  Personas on Cultural Norm Interpretation in LLMs", "abstract": "As the deployment of large language models (LLMs) expands, there is an\nincreasing demand for personalized LLMs. One method to personalize and guide\nthe outputs of these models is by assigning a persona -- a role that describes\nthe expected behavior of the LLM (e.g., a man, a woman, an engineer). This\nstudy investigates whether an LLM's understanding of social norms varies across\nassigned personas. Ideally, the perception of a social norm should remain\nconsistent regardless of the persona, since acceptability of a social norm\nshould be determined by the region the norm originates from, rather than by\nindividual characteristics such as gender, body size, or race. A norm is\nuniversal within its cultural context. In our research, we tested 36 distinct\npersonas from 12 sociodemographic categories (e.g., age, gender, beauty) across\nfour different LLMs. We find that LLMs' cultural norm interpretation varies\nbased on the persona used and the norm interpretation also varies within a\nsociodemographic category (e.g., a fat person and a thin person as in physical\nappearance group) where an LLM with the more socially desirable persona (e.g.,\na thin person) interprets social norms more accurately than with the less\nsocially desirable persona (e.g., a fat person). We also discuss how different\ntypes of social biases may contribute to the results that we observe.", "published": "2024-09-18 01:56:34", "link": "http://arxiv.org/abs/2409.11636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs\n  for Bangla", "abstract": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies.", "published": "2024-09-18 02:02:30", "link": "http://arxiv.org/abs/2409.11638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model", "abstract": "Unified information extraction (UIE) aims to extract diverse structured\ninformation from unstructured text. While large language models (LLMs) have\nshown promise for UIE, they require significant computational resources and\noften struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based\nUnified Information Extraction), a framework that leverages in-context learning\nfor efficient task generalization. RUIE introduces a novel demonstration\nselection mechanism combining LLM preferences with a keyword-enhanced reward\nmodel, and employs a bi-encoder retriever trained through contrastive learning\nand knowledge distillation. As the first trainable retrieval framework for UIE,\nRUIE serves as a universal plugin for various LLMs. Experimental results on\neight held-out datasets demonstrate RUIE's effectiveness, with average F1-score\nimprovements of 19.22 and 3.22 compared to instruction-tuning methods and other\nretrievers, respectively.", "published": "2024-09-18 03:20:04", "link": "http://arxiv.org/abs/2409.11673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Complex Formula Recognition with Hierarchical Detail-Focused\n  Network", "abstract": "Hierarchical and complex Mathematical Expression Recognition (MER) is\nchallenging due to multiple possible interpretations of a formula, complicating\nboth parsing and evaluation. In this paper, we introduce the Hierarchical\nDetail-Focused Recognition dataset (HDR), the first dataset specifically\ndesigned to address these issues. It consists of a large-scale training set,\nHDR-100M, offering an unprecedented scale and diversity with one hundred\nmillion training instances. And the test set, HDR-Test, includes multiple\ninterpretations of complex hierarchical formulas for comprehensive model\nperformance evaluation. Additionally, the parsing of complex formulas often\nsuffers from errors in fine-grained details. To address this, we propose the\nHierarchical Detail-Focused Recognition Network (HDNet), an innovative\nframework that incorporates a hierarchical sub-formula module, focusing on the\nprecise handling of formula details, thereby significantly enhancing MER\nperformance. Experimental results demonstrate that HDNet outperforms existing\nMER models across various datasets.", "published": "2024-09-18 03:32:25", "link": "http://arxiv.org/abs/2409.11677v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing LLMs for API Interactions: A Framework for Classification and\n  Synthetic Data Generation", "abstract": "As Large Language Models (LLMs) advance in natural language processing, there\nis growing interest in leveraging their capabilities to simplify software\ninteractions. In this paper, we propose a novel system that integrates LLMs for\nboth classifying natural language inputs into corresponding API calls and\nautomating the creation of sample datasets tailored to specific API functions.\nBy classifying natural language commands, our system allows users to invoke\ncomplex software functionalities through simple inputs, improving interaction\nefficiency and lowering the barrier to software utilization. Our dataset\ngeneration approach also enables the efficient and systematic evaluation of\ndifferent LLMs in classifying API calls, offering a practical tool for\ndevelopers or business owners to assess the suitability of LLMs for customized\nAPI management. We conduct experiments on several prominent LLMs using\ngenerated sample datasets for various API functions. The results show that\nGPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B\nperforms much worse at 0.759. These findings highlight the potential of LLMs to\ntransform API management and validate the effectiveness of our system in\nguiding model testing and selection across diverse applications.", "published": "2024-09-18 04:56:52", "link": "http://arxiv.org/abs/2409.11703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning", "abstract": "Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.", "published": "2024-09-18 06:19:59", "link": "http://arxiv.org/abs/2409.11724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Real-Time Conversations with Minimal Training Costs", "abstract": "Large language models (LLMs) have demonstrated the ability to improve human\nefficiency through conversational interactions. Conventional LLM-powered\ndialogue systems, operating on a turn-based paradigm, preclude real-time\ninteraction during response generation. To address this limitation, researchers\nhave proposed duplex models. These models can dynamically adapt to user input,\nfacilitating real-time interactive feedback. However, these methods typically\nrequire substantial computational resources to acquire the ability. To reduce\noverhead, this paper presents a new duplex decoding approach that enhances LLMs\nwith duplex ability, requiring minimal additional training. Specifically, our\nmethod employs parallel decoding of queries and responses in conversations,\neffectively implementing a channel-division-multiplexing decoding strategy.\nExperimental results indicate that our proposed method significantly enhances\nthe naturalness and human-likeness of user-AI interactions with minimal\ntraining costs.", "published": "2024-09-18 06:27:26", "link": "http://arxiv.org/abs/2409.11727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human-like Affective Cognition in Foundation Models", "abstract": "Understanding emotions is fundamental to human interaction and experience.\nHumans easily infer emotions from situations or facial expressions, situations\nfrom emotions, and do a variety of other affective cognition. How adept is\nmodern AI at these inferences? We introduce an evaluation framework for testing\naffective cognition in foundation models. Starting from psychological theory,\nwe generate 1,280 diverse scenarios exploring relationships between appraisals,\nemotions, expressions, and outcomes. We evaluate the abilities of foundation\nmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully\nselected conditions. Our results show foundation models tend to agree with\nhuman intuitions, matching or exceeding interparticipant agreement. In some\nconditions, models are ``superhuman'' -- they better predict modal human\njudgements than the average human. All models benefit from chain-of-thought\nreasoning. This suggests foundation models have acquired a human-like\nunderstanding of emotions and their influence on beliefs and behavior.", "published": "2024-09-18 06:42:13", "link": "http://arxiv.org/abs/2409.11733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development and bilingual evaluation of Japanese medical large language\n  model within reasonably low computational resources", "abstract": "The recent success of large language models (LLMs) and the scaling law has\nled to a widespread adoption of larger models. Particularly in the healthcare\nindustry, there is an increasing demand for locally operated LLMs due to\nsecurity concerns. However, the majority of high quality open-source LLMs have\na size of 70B parameters, imposing significant financial burdens on users for\nGPU preparation and operation. To overcome these issues, we present a medical\nadaptation based on the recent 7B models, which enables the operation in low\ncomputational resources. We compare the performance on medical\nquestion-answering benchmarks in two languages (Japanese and English),\ndemonstrating that its scores reach parity with or surpass those of currently\nexisting medical LLMs that are ten times larger. We find that fine-tuning an\nEnglish-centric base model on Japanese medical dataset improves the score in\nboth language, supporting the effect of cross-lingual knowledge transfer. We\nhope that this study will alleviate financial challenges, serving as a stepping\nstone for clinical institutions to practically utilize LLMs locally. Our\nevaluation code is available at\nhttps://github.com/stardust-coder/japanese-lm-med-harness.", "published": "2024-09-18 08:07:37", "link": "http://arxiv.org/abs/2409.11783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extract-and-Abstract: Unifying Extractive and Abstractive Summarization\n  within Single Encoder-Decoder Framework", "abstract": "Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive\nsummarization with the help of salient information identified by the extractive\nmodel. Previous works that adopt this paradigm train the extractor and\nabstractor separately and introduce extra parameters to highlight the extracted\nsalients to the abstractor, which results in error accumulation and additional\ntraining costs. In this paper, we first introduce a parameter-free highlight\nmethod into the encoder-decoder framework: replacing the encoder attention mask\nwith a saliency mask in the cross-attention module to force the decoder to\nfocus only on salient parts of the input. A preliminary analysis compares\ndifferent highlight methods, demonstrating the effectiveness of our saliency\nmask. We further propose the novel extract-and-abstract paradigm, ExtAbs, which\njointly and seamlessly performs Extractive and Abstractive summarization tasks\nwithin single encoder-decoder model to reduce error accumulation. In ExtAbs,\nthe vanilla encoder is augmented to extract salients, and the vanilla decoder\nis modified with the proposed saliency mask to generate summaries. Built upon\nBART and PEGASUS, experiments on three datasets show that ExtAbs can achieve\nsuperior performance than baselines on the extractive task and performs\ncomparable, or even better than the vanilla models on the abstractive task.", "published": "2024-09-18 09:21:25", "link": "http://arxiv.org/abs/2409.11827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs + Persona-Plug = Personalized LLMs", "abstract": "Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches.", "published": "2024-09-18 11:54:45", "link": "http://arxiv.org/abs/2409.11901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs in Education: Novel Perspectives, Challenges, and Opportunities", "abstract": "The role of large language models (LLMs) in education is an increasing area\nof interest today, considering the new opportunities they offer for teaching,\nlearning, and assessment. This cutting-edge tutorial provides an overview of\nthe educational applications of NLP and the impact that the recent advances in\nLLMs have had on this field. We will discuss the key challenges and\nopportunities presented by LLMs, grounding them in the context of four major\neducational applications: reading, writing, and speaking skills, and\nintelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for\nresearchers and practitioners interested in the educational applications of NLP\nand the role LLMs have to play in this area. It is the first of its kind to\naddress this timely topic.", "published": "2024-09-18 12:29:22", "link": "http://arxiv.org/abs/2409.11917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models to Generate Clinical Trial Tables and\n  Figures", "abstract": "Tables, figures, and listings (TFLs) are essential tools for summarizing\nclinical trial data. Creation of TFLs for reporting activities is often a\ntime-consuming task encountered routinely during the execution of clinical\ntrials. This study explored the use of large language models (LLMs) to automate\nthe generation of TFLs through prompt engineering and few-shot transfer\nlearning. Using public clinical trial data in ADaM format, our results\ndemonstrated that LLMs can efficiently generate TFLs with prompt instructions,\nshowcasing their potential in this domain. Furthermore, we developed a\nconservational agent named Clinical Trial TFL Generation Agent: An app that\nmatches user queries to predefined prompts that produce customized programs to\ngenerate specific predefined TFLs.", "published": "2024-09-18 15:16:37", "link": "http://arxiv.org/abs/2409.12046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguini: A benchmark for language-agnostic linguistic reasoning", "abstract": "We propose a new benchmark to measure a language model's linguistic reasoning\nskills without relying on pre-existing language-specific knowledge. The test\ncovers 894 questions grouped in 160 problems across 75 (mostly) extremely\nlow-resource languages, extracted from the International Linguistic Olympiad\ncorpus. To attain high accuracy on this benchmark, models don't need previous\nknowledge of the tested language, as all the information needed to solve the\nlinguistic puzzle is presented in the context. We find that, while all analyzed\nmodels rank below 25% accuracy, there is a significant gap between open and\nclosed models, with the best-performing proprietary model at 24.05% and the\nbest-performing open model at 8.84%.", "published": "2024-09-18 16:51:02", "link": "http://arxiv.org/abs/2409.12126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for\n  Reasoning", "abstract": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication.", "published": "2024-09-18 17:12:41", "link": "http://arxiv.org/abs/2409.12147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Only Read Once (YORO): Learning to Internalize Database Knowledge\n  for Text-to-SQL", "abstract": "While significant progress has been made on the text-to-SQL task, recent\nsolutions repeatedly encode the same database schema for every question,\nresulting in unnecessary high inference cost and often overlooking crucial\ndatabase knowledge. To address these issues, we propose You Only Read Once\n(YORO), a novel paradigm that directly internalizes database knowledge into the\nparametric knowledge of a text-to-SQL model during training and eliminates the\nneed for schema encoding during inference. YORO significantly reduces the input\ntoken length by 66%-98%. Despite its shorter inputs, our empirical results\ndemonstrate YORO's competitive performances with traditional systems on three\nbenchmarks as well as its significant outperformance on large databases.\nFurthermore, YORO excels in handling questions with challenging value\nretrievals such as abbreviation.", "published": "2024-09-18 17:38:25", "link": "http://arxiv.org/abs/2409.12172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qwen2.5-Coder Technical Report", "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade\nfrom its predecessor, CodeQwen1.5. This series includes six models:\nQwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,\nQwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained\non a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,\nscalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder\ndemonstrates impressive code generation capabilities while retaining general\nand math skills. These models have been evaluated on a wide range of\ncode-related tasks, achieving state-of-the-art (SOTA) performance across more\nthan 10 benchmarks, including code generation, completion, reasoning, and\nrepair, consistently outperforming larger models of the same model size. We\nbelieve that the release of the Qwen2.5-Coder series will advance research in\ncode intelligence and, with its permissive licensing, support wider adoption by\ndevelopers in real-world applications.", "published": "2024-09-18 17:57:57", "link": "http://arxiv.org/abs/2409.12186v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for\n  Arabic Language", "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities\nacross numerous application domains. A key challenge is to keep these models\nupdated with latest available information, which limits the true potential of\nthese models for the end-applications. Although, there have been numerous\nattempts for LLMs Knowledge Editing (KE), i.e., to edit the LLMs prior\nknowledge and in turn test it via Multi-hop Question Answering (MQA), yet so\nfar these studies are primarily focused on English language. To bridge this\ngap, in this paper we propose: Multi-hop Questioning Answering under Knowledge\nEditing for Arabic Language (MQA-KEAL). MQA-KEAL stores knowledge edits as\nstructured knowledge units in the external memory. In order to solve multi-hop\nquestion, it first uses task-decomposition to decompose the question into\nsmaller sub-problems. Later for each sub-problem, it iteratively queries the\nexternal memory and/or target LLM in order to generate the final response. In\naddition, we also contribute MQUAKE-AR (Arabic translation of English benchmark\nMQUAKE), as well as a new benchmark MQA-AEVAL for rigorous performance\nevaluation of MQA under KE for Arabic language. Experimentation evaluation\nreveals MQA-KEAL outperforms the baseline models by a significant margin.", "published": "2024-09-18 18:40:02", "link": "http://arxiv.org/abs/2409.12257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Large Language Models into World Models with Precondition and\n  Effect Knowledge", "abstract": "World models, which encapsulate the dynamics of how actions affect\nenvironments, are foundational to the functioning of intelligent agents. In\nthis work, we explore the potential of Large Language Models (LLMs) to operate\nas world models. Although LLMs are not inherently designed to model real-world\ndynamics, we show that they can be induced to perform two critical world model\nfunctions: determining the applicability of an action based on a given world\nstate, and predicting the resulting world state upon action execution. This is\nachieved by fine-tuning two separate LLMs-one for precondition prediction and\nanother for effect prediction-while leveraging synthetic data generation\ntechniques. Through human-participant studies, we validate that the\nprecondition and effect knowledge generated by our models aligns with human\nunderstanding of world dynamics. We also analyze the extent to which the world\nmodel trained on our synthetic data results in an inferred state space that\nsupports the creation of action chains, a necessary property for planning.", "published": "2024-09-18 19:28:04", "link": "http://arxiv.org/abs/2409.12278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FLARE: Fusing Language Models and Collaborative Architectures for\n  Recommender Enhancement", "abstract": "Recent proposals in recommender systems represent items with their textual\ndescription, using a large language model. They show better results on standard\nbenchmarks compared to an item ID-only model, such as Bert4Rec. In this work,\nwe revisit the often-used Bert4Rec baseline and show that with further tuning,\nBert4Rec significantly outperforms previously reported numbers, and in some\ndatasets, is competitive with state-of-the-art models.\n  With revised baselines for item ID-only models, this paper also establishes\nnew competitive results for architectures that combine IDs and textual\ndescriptions. We demonstrate this with Flare (Fusing Language models and\ncollaborative Architectures for Recommender Enhancement). Flare is a novel\nhybrid sequence recommender that integrates a language model with a\ncollaborative filtering model using a Perceiver network.\n  Prior studies focus evaluation on datasets with limited-corpus size, but many\ncommercially-applicable recommender systems common on the web must handle\nlarger corpora. We evaluate Flare on a more realistic dataset with a\nsignificantly larger item vocabulary, introducing new baselines for this\nsetting. This paper also showcases Flare's inherent ability to support\ncritiquing, enabling users to provide feedback and refine recommendations. We\nleverage critiquing as an evaluation method to assess the model's language\nunderstanding and its transferability to the recommendation task.", "published": "2024-09-18 04:43:41", "link": "http://arxiv.org/abs/2409.11699v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From Lists to Emojis: How Format Bias Affects Model Alignment", "abstract": "In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.", "published": "2024-09-18 05:13:18", "link": "http://arxiv.org/abs/2409.11704v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing the Challenge of Detecting Character Knowledge Errors in LLM\n  Role-Playing", "abstract": "Large language model (LLM) role-playing has gained widespread attention,\nwhere the authentic character knowledge is crucial for constructing realistic\nLLM role-playing agents. However, existing works usually overlook the\nexploration of LLMs' ability to detect characters' known knowledge errors (KKE)\nand unknown knowledge errors (UKE) while playing roles, which would lead to\nlow-quality automatic construction of character trainable corpus. In this\npaper, we propose a probing dataset to evaluate LLMs' ability to detect errors\nin KKE and UKE. The results indicate that even the latest LLMs struggle to\neffectively detect these two types of errors, especially when it comes to\nfamiliar knowledge. We experimented with various reasoning strategies and\npropose an agent-based reasoning method, Self-Recollection and Self-Doubt\n(S2RD), to further explore the potential for improving error detection\ncapabilities. Experiments show that our method effectively improves the LLMs'\nability to detect error character knowledge, but it remains an issue that\nrequires ongoing attention.", "published": "2024-09-18 06:21:44", "link": "http://arxiv.org/abs/2409.11726v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "abstract": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.", "published": "2024-09-18 09:55:48", "link": "http://arxiv.org/abs/2409.11844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocMamba: Efficient Document Pre-training with State Space Model", "abstract": "In recent years, visually-rich document understanding has attracted\nincreasing attention. Transformer-based pre-trained models have become the\nmainstream approach, yielding significant performance gains in this field.\nHowever, the self-attention mechanism's quadratic computational complexity\nhinders their efficiency and ability to process long documents. In this paper,\nwe present DocMamba, a novel framework based on the state space model. It is\ndesigned to reduce computational complexity to linear while preserving global\nmodeling capabilities. To further enhance its effectiveness in document\nprocessing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture\ncontiguous semantic information. Experimental results demonstrate that DocMamba\nachieves new state-of-the-art results on downstream datasets such as FUNSD,\nCORD, and SORIE, while significantly improving speed and reducing memory usage.\nNotably, experiments on the HRDoc confirm DocMamba's potential for length\nextrapolation.", "published": "2024-09-18 11:34:28", "link": "http://arxiv.org/abs/2409.11887v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficacy of Synthetic Data as a Benchmark", "abstract": "Large language models (LLMs) have enabled a range of applications in\nzero-shot and few-shot learning settings, including the generation of synthetic\ndatasets for training and testing. However, to reliably use these synthetic\ndatasets, it is essential to understand how representative they are of\nreal-world data. We investigate this by assessing the effectiveness of\ngenerating synthetic data through LLM and using it as a benchmark for various\nNLP tasks. Our experiments across six datasets, and three different tasks, show\nthat while synthetic data can effectively capture performance of various\nmethods for simpler tasks, such as intent classification, it falls short for\nmore complex tasks like named entity recognition. Additionally, we propose a\nnew metric called the bias factor, which evaluates the biases introduced when\nthe same LLM is used to both generate benchmarking data and to perform the\ntasks. We find that smaller LLMs exhibit biases towards their own generated\ndata, whereas larger models do not. Overall, our findings suggest that the\neffectiveness of synthetic data as a benchmark varies depending on the task,\nand that practitioners should rely on data generated from multiple larger\nmodels whenever possible.", "published": "2024-09-18 13:20:23", "link": "http://arxiv.org/abs/2409.11968v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sampling Latent Material-Property Information From LLM-Derived Embedding\n  Representations", "abstract": "Vector embeddings derived from large language models (LLMs) show promise in\ncapturing latent information from the literature. Interestingly, these can be\nintegrated into material embeddings, potentially useful for data-driven\npredictions of materials properties. We investigate the extent to which\nLLM-derived vectors capture the desired information and their potential to\nprovide insights into material properties without additional training. Our\nfindings indicate that, although LLMs can be used to generate representations\nreflecting certain property information, extracting the embeddings requires\nidentifying the optimal contextual clues and appropriate comparators. Despite\nthis restriction, it appears that LLMs still have the potential to be useful in\ngenerating meaningful materials-science representations.", "published": "2024-09-18 13:22:04", "link": "http://arxiv.org/abs/2409.11971v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase\n  Detection Models", "abstract": "The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus", "published": "2024-09-18 15:33:48", "link": "http://arxiv.org/abs/2409.12060v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models", "abstract": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.", "published": "2024-09-18 16:26:22", "link": "http://arxiv.org/abs/2409.12106v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework", "abstract": "In tackling the challenge of Multi-Document Summarization (MDS), numerous\nmethods have been proposed, spanning both extractive and abstractive\nsummarization techniques. However, each approach has its own limitations,\nmaking it less effective to rely solely on either one. An emerging and\npromising strategy involves a synergistic fusion of extractive and abstractive\nsummarization methods. Despite the plethora of studies in this domain, research\non the combined methodology remains scarce, particularly in the context of\nVietnamese language processing. This paper presents a novel Vietnamese MDS\nframework leveraging a two-component pipeline architecture that integrates\nextractive and abstractive techniques. The first component employs an\nextractive approach to identify key sentences within each document. This is\nachieved by a modification of the pre-trained BERT network, which derives\nsemantically meaningful phrase embeddings using siamese and triplet network\nstructures. The second component utilizes the VBD-LLaMA2-7B-50b model for\nabstractive summarization, ultimately generating the final summary document.\nOur proposed framework demonstrates a positive performance, attaining ROUGE-2\nscores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art\nbaselines.", "published": "2024-09-18 16:56:06", "link": "http://arxiv.org/abs/2409.12134v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty", "abstract": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers.", "published": "2024-09-18 17:52:53", "link": "http://arxiv.org/abs/2409.12180v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Controlled Study on Long Context Extension and Generalization in LLMs", "abstract": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.", "published": "2024-09-18 17:53:17", "link": "http://arxiv.org/abs/2409.12181v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender Representation and Bias in Indian Civil Service Mock Interviews", "abstract": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies.", "published": "2024-09-18 17:59:52", "link": "http://arxiv.org/abs/2409.12194v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ARTICLE: Annotator Reliability Through In-Context Learning", "abstract": "Ensuring annotator quality in training and evaluation data is a key piece of\nmachine learning in NLP. Tasks such as sentiment analysis and offensive speech\ndetection are intrinsically subjective, creating a challenging scenario for\ntraditional quality assessment approaches because it is hard to distinguish\ndisagreement due to poor work from that due to differences of opinions between\nsincere annotators. With the goal of increasing diverse perspectives in\nannotation while ensuring consistency, we propose \\texttt{ARTICLE}, an\nin-context learning (ICL) framework to estimate annotation quality through\nself-consistency. We evaluate this framework on two offensive speech datasets\nusing multiple LLMs and compare its performance with traditional methods. Our\nfindings indicate that \\texttt{ARTICLE} can be used as a robust method for\nidentifying reliable annotators, hence improving data quality.", "published": "2024-09-18 17:59:32", "link": "http://arxiv.org/abs/2409.12218v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Local Explanations and Self-Explanations for Assessing Faithfulness in\n  black-box LLMs", "abstract": "This paper introduces a novel task to assess the faithfulness of large\nlanguage models (LLMs) using local perturbations and self-explanations. Many\nLLMs often require additional context to answer certain questions correctly.\nFor this purpose, we propose a new efficient alternative explainability\ntechnique, inspired by the commonly used leave-one-out approach. Using this\napproach, we identify the sufficient and necessary parts for the LLM to\ngenerate correct answers, serving as explanations. We propose a metric for\nassessing faithfulness that compares these crucial parts with the\nself-explanations of the model. Using the Natural Questions dataset, we\nvalidate our approach, demonstrating its effectiveness in explaining model\ndecisions and assessing faithfulness.", "published": "2024-09-18 10:16:45", "link": "http://arxiv.org/abs/2409.13764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Characterization of the Effectiveness of Alignment in Large\n  Language Models for Categorical Decisions", "abstract": "As large language models (LLMs) are deployed in high-stakes domains like\nhealthcare, understanding how well their decision-making aligns with human\npreferences and values becomes crucial, especially when we recognize that there\nis no single gold standard for these preferences. This paper applies a\nsystematic methodology for evaluating preference alignment in LLMs on\ncategorical decision-making with medical triage as a domain-specific use case.\nIt also measures how effectively an alignment procedure will change the\nalignment of a specific model. Key to this methodology is a novel simple\nmeasure, the Alignment Compliance Index (ACI), that quantifies how effectively\na LLM can be aligned to a given preference function or gold standard. Since the\nACI measures the effect rather than the process of alignment, it is applicable\nto alignment methods beyond the in-context learning used in this study.\n  Using a dataset of simulated patient pairs, three frontier LLMs (GPT4o,\nClaude 3.5 Sonnet, and Gemini Advanced) were assessed on their ability to make\ntriage decisions consistent with an expert clinician's preferences. The models'\nperformance before and after alignment attempts was evaluated using various\nprompting strategies. The results reveal significant variability in alignment\neffectiveness across models and alignment approaches. Notably, models that\nperformed well, as measured by ACI, pre-alignment sometimes degraded\npost-alignment, and small changes in the target preference function led to\nlarge shifts in model rankings. The implicit ethical principles, as understood\nby humans, underlying the LLMs' decisions were also explored through targeted\nquestioning.\n  This study motivates the use of a practical set of methods and the ACI, in\nthe near term, to understand the correspondence between the variety of human\nand LLM decision-making values in categorical decision-making such as triage.", "published": "2024-09-18 19:03:04", "link": "http://arxiv.org/abs/2409.18995v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Factuality of Large Language Models in the Legal Domain", "abstract": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.", "published": "2024-09-18 08:30:20", "link": "http://arxiv.org/abs/2409.11798v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASR Benchmarking: Need for a More Representative Conversational Dataset", "abstract": "Automatic Speech Recognition (ASR) systems have achieved remarkable\nperformance on widely used benchmarks such as LibriSpeech and Fleurs. However,\nthese benchmarks do not adequately reflect the complexities of real-world\nconversational environments, where speech is often unstructured and contains\ndisfluencies such as pauses, interruptions, and diverse accents. In this study,\nwe introduce a multilingual conversational dataset, derived from TalkBank,\nconsisting of unstructured phone conversation between adults. Our results show\na significant performance drop across various state-of-the-art ASR models when\ntested in conversational settings. Furthermore, we observe a correlation\nbetween Word Error Rate and the presence of speech disfluencies, highlighting\nthe critical need for more realistic, conversational ASR benchmarks.", "published": "2024-09-18 15:03:04", "link": "http://arxiv.org/abs/2409.12042v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer\n  Thinking, Decoding and Bootstrapping Reasoning", "abstract": "Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.", "published": "2024-09-18 15:32:48", "link": "http://arxiv.org/abs/2409.12059v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skill matching at scale: freelancer-project alignment for efficient\n  multilingual candidate retrieval", "abstract": "Finding the perfect match between a job proposal and a set of freelancers is\nnot an easy task to perform at scale, especially in multiple languages. In this\npaper, we propose a novel neural retriever architecture that tackles this\nproblem in a multilingual setting. Our method encodes project descriptions and\nfreelancer profiles by leveraging pre-trained multilingual language models. The\nlatter are used as backbone for a custom transformer architecture that aims to\nkeep the structure of the profiles and project. This model is trained with a\ncontrastive loss on historical data. Thanks to several experiments, we show\nthat this approach effectively captures skill matching similarity and\nfacilitates efficient matching, outperforming traditional methods.", "published": "2024-09-18 16:15:18", "link": "http://arxiv.org/abs/2409.12097v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality\n  Speech LLM Training and Inference", "abstract": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels.", "published": "2024-09-18 16:39:10", "link": "http://arxiv.org/abs/2409.12117v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via\n  Self-Improvement", "abstract": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.", "published": "2024-09-18 16:45:37", "link": "http://arxiv.org/abs/2409.12122v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRIN: GRadient-INformed MoE", "abstract": "Mixture-of-Experts (MoE) models scale more effectively than dense models due\nto sparse computation through expert routing, selectively activating only a\nsmall subset of expert modules. However, sparse computation challenges\ntraditional training practices, as discrete expert routing hinders standard\nbackpropagation and thus gradient-based optimization, which are the cornerstone\nof deep learning. To better pursue the scaling power of MoE, we introduce GRIN\n(GRadient-INformed MoE training), which incorporates sparse gradient estimation\nfor expert routing and configures model parallelism to avoid token dropping.\nApplying GRIN to autoregressive language modeling, we develop a top-2\n16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters,\noutperforms a 7B dense model and matches the performance of a 14B dense model\ntrained on the same data. Extensive evaluations across diverse tasks\ndemonstrate the potential of GRIN to significantly enhance MoE efficacy,\nachieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.", "published": "2024-09-18 17:00:20", "link": "http://arxiv.org/abs/2409.12136v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic\n  reasoning", "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.", "published": "2024-09-18 17:55:00", "link": "http://arxiv.org/abs/2409.12183v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution", "abstract": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .", "published": "2024-09-18 17:59:32", "link": "http://arxiv.org/abs/2409.12191v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and\n  Language Models", "abstract": "Large language models (LLMs) have recently emerged as promising tools for\nsolving challenging robotic tasks, even in the presence of action and\nobservation uncertainties. Recent LLM-based decision-making methods (also\nreferred to as LLM-based agents), when paired with appropriate critics, have\ndemonstrated potential in solving complex, long-horizon tasks with relatively\nfew interactions. However, most existing LLM-based agents lack the ability to\nretain and learn from past interactions - an essential trait of learning-based\nrobotic systems. We propose RAG-Modulo, a framework that enhances LLM-based\nagents with a memory of past interactions and incorporates critics to evaluate\nthe agents' decisions. The memory component allows the agent to automatically\nretrieve and incorporate relevant past experiences as in-context examples,\nproviding context-aware feedback for more informed decision-making. Further by\nupdating its memory, the agent improves its performance over time, thereby\nexhibiting learning. Through experiments in the challenging BabyAI and AlfWorld\ndomains, we demonstrate significant improvements in task success rates and\nefficiency, showing that the proposed RAG-Modulo framework outperforms\nstate-of-the-art baselines.", "published": "2024-09-18 20:03:32", "link": "http://arxiv.org/abs/2409.12294v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Measuring Sound Symbolism in Audio-visual Models", "abstract": "Audio-visual pre-trained models have gained substantial attention recently\nand demonstrated superior performance on various audio-visual tasks. This study\ninvestigates whether pre-trained audio-visual models demonstrate non-arbitrary\nassociations between sounds and visual representations$\\unicode{x2013}$known as\nsound symbolism$\\unicode{x2013}$which is also observed in humans. We developed\na specialized dataset with synthesized images and audio samples and assessed\nthese models using a non-parametric approach in a zero-shot setting. Our\nfindings reveal a significant correlation between the models' outputs and\nestablished patterns of sound symbolism, particularly in models trained on\nspeech data. These results suggest that such models can capture sound-meaning\nconnections akin to human language processing, providing insights into both\ncognitive architectures and machine learning strategies.", "published": "2024-09-18 20:33:54", "link": "http://arxiv.org/abs/2409.12306v3", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reward-Robust RLHF in LLMs", "abstract": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.", "published": "2024-09-18 02:35:41", "link": "http://arxiv.org/abs/2409.15360v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs\n  Fine-tuning", "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to their\nadoption across a wide range of tasks, ranging from code generation to machine\ntranslation and sentiment analysis, etc. Red teaming/Safety alignment efforts\nshow that fine-tuning models on benign (non-harmful) data could compromise\nsafety. However, it remains unclear to what extent this phenomenon is\ninfluenced by different variables, including fine-tuning task, model\ncalibrations, etc. This paper explores the task-wise safety degradation due to\nfine-tuning on downstream tasks such as summarization, code generation,\ntranslation, and classification across various calibration. Our results reveal\nthat: 1) Fine-tuning LLMs for code generation and translation leads to the\nhighest degradation in safety guardrails. 2) LLMs generally have weaker\nguardrails for translation and classification, with 73-92% of harmful prompts\nanswered, across baseline and other calibrations, falling into one of two\nconcern categories. 3) Current solutions, including guards and safety tuning\ndatasets, lack cross-task robustness. To address these issues, we developed a\nnew multitask safety dataset effectively reducing attack success rates across a\nrange of tasks without compromising the model's overall helpfulness. Our work\nunderscores the need for generalized alignment measures to ensure safer and\nmore robust models.", "published": "2024-09-18 08:04:24", "link": "http://arxiv.org/abs/2409.15361v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VERA: Validation and Enhancement for Retrieval Augmented systems", "abstract": "Large language models (LLMs) exhibit remarkable capabilities but often\nproduce inaccurate responses, as they rely solely on their embedded knowledge.\nRetrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external\ninformation retrieval system, supplying additional context along with the query\nto mitigate inaccuracies for a particular context. However, accuracy issues\nstill remain, as the model may rely on irrelevant documents or extrapolate\nincorrectly from its training knowledge. To assess and improve the performance\nof both the retrieval system and the LLM in a RAG framework, we propose\n\\textbf{VERA} (\\textbf{V}alidation and \\textbf{E}nhancement for\n\\textbf{R}etrieval \\textbf{A}ugmented systems), a system designed to: 1)\nEvaluate and enhance the retrieved context before response generation, and 2)\nEvaluate and refine the LLM-generated response to ensure precision and minimize\nerrors. VERA employs an evaluator-cum-enhancer LLM that first checks if\nexternal retrieval is necessary, evaluates the relevance and redundancy of the\nretrieved context, and refines it to eliminate non-essential information.\nPost-response generation, VERA splits the response into atomic statements,\nassesses their relevance to the query, and ensures adherence to the context.\nOur experiments demonstrate VERA's remarkable efficacy not only in improving\nthe performance of smaller open-source models, but also larger state-of-the art\nmodels. These enhancements underscore VERA's potential to produce accurate and\nrelevant responses, advancing the state-of-the-art in retrieval-augmented\nlanguage modeling. VERA's robust methodology, combining multiple evaluation and\nrefinement steps, effectively mitigates hallucinations and improves retrieval\nand response processes, making it a valuable tool for applications demanding\nhigh accuracy and reliability in information generation. .", "published": "2024-09-18 16:10:47", "link": "http://arxiv.org/abs/2409.15364v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss", "abstract": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.", "published": "2024-09-18 18:36:18", "link": "http://arxiv.org/abs/2409.15367v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Navigation with VLM framework: Go to Any Language", "abstract": "Navigating towards fully open language goals and exploring open scenes in a\nmanner akin to human exploration have always posed significant challenges.\nRecently, Vision Large Language Models (VLMs) have demonstrated remarkable\ncapabilities in reasoning with both language and visual data. While many works\nhave focused on leveraging VLMs for navigation in open scenes and with open\nvocabularies, these efforts often fall short of fully utilizing the potential\nof VLMs or require substantial computational resources. We introduce Navigation\nwith VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable\nagents to navigate towards any language goal specific or non-specific in open\nscenes, emulating human exploration behaviors without any prior training. The\nagent leverages the VLM as its cognitive core to perceive environmental\ninformation based on any language goal and constantly provides exploration\nguidance during navigation until it reaches the target location or area. Our\nframework not only achieves state-of-the-art performance in Success Rate (SR)\nand Success weighted by Path Length (SPL) in traditional specific goal settings\nbut also extends the navigation capabilities to any open-set language goal. We\nevaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D),\nHabitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator.\nWith the power of VLMs, navigation has entered a new era.", "published": "2024-09-18 02:29:00", "link": "http://arxiv.org/abs/2410.02787v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TaCIE: Enhancing Instruction Comprehension in Large Language Models\n  through Task-Centred Instruction Evolution", "abstract": "Large Language Models (LLMs) require precise alignment with complex\ninstructions to optimize their performance in real-world applications. As the\ndemand for refined instruction tuning data increases, traditional methods that\nevolve simple seed instructions often struggle to effectively enhance\ncomplexity or manage difficulty scaling across various domains. Our innovative\napproach, Task-Centered Instruction Evolution (TaCIE), addresses these\nshortcomings by redefining instruction evolution from merely evolving seed\ninstructions to a more dynamic and comprehensive combination of elements. TaCIE\nstarts by deconstructing complex instructions into their fundamental\ncomponents. It then generates and integrates new elements with the original\nones, reassembling them into more sophisticated instructions that progressively\nincrease in difficulty, diversity, and complexity. Applied across multiple\ndomains, LLMs fine-tuned with these evolved instructions have substantially\noutperformed those tuned with conventional methods, marking a significant\nadvancement in instruction-based model fine-tuning.", "published": "2024-09-18 10:06:28", "link": "http://arxiv.org/abs/2410.02795v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "BodyShapeGPT: SMPL Body Shape Manipulation with LLMs", "abstract": "Generative AI models provide a wide range of tools capable of performing\ncomplex tasks in a fraction of the time it would take a human. Among these,\nLarge Language Models (LLMs) stand out for their ability to generate diverse\ntexts, from literary narratives to specialized responses in different fields of\nknowledge. This paper explores the use of fine-tuned LLMs to identify physical\ndescriptions of people, and subsequently create accurate representations of\navatars using the SMPL-X model by inferring shape parameters. We demonstrate\nthat LLMs can be trained to understand and manipulate the shape space of SMPL,\nallowing the control of 3D human shapes through natural language. This approach\npromises to improve human-machine interaction and opens new avenues for\ncustomization and simulation in virtual environments.", "published": "2024-09-18 16:55:23", "link": "http://arxiv.org/abs/2410.03556v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation", "abstract": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.", "published": "2024-09-18 10:30:50", "link": "http://arxiv.org/abs/2409.11860v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.ET", "cs.HC"], "primary_category": "cs.IR"}
{"title": "MedCodER: A Generative AI Assistant for Medical Coding", "abstract": "Medical coding is essential for standardizing clinical data and communication\nbut is often time-consuming and prone to errors. Traditional Natural Language\nProcessing (NLP) methods struggle with automating coding due to the large label\nspace, lengthy text inputs, and the absence of supporting evidence annotations\nthat justify code selection. Recent advancements in Generative Artificial\nIntelligence (AI) offer promising solutions to these challenges. In this work,\nwe introduce MedCodER, a Generative AI framework for automatic medical coding\nthat leverages extraction, retrieval, and re-ranking techniques as core\ncomponents. MedCodER achieves a micro-F1 score of 0.60 on International\nClassification of Diseases (ICD) code prediction, significantly outperforming\nstate-of-the-art methods. Additionally, we present a new dataset containing\nmedical records annotated with disease diagnoses, ICD codes, and supporting\nevidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests\nconfirm that MedCodER's performance depends on the integration of each of its\naforementioned components, as performance declines when these components are\nevaluated in isolation.", "published": "2024-09-18 19:36:33", "link": "http://arxiv.org/abs/2409.15368v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring an Inter-Pausal Unit (IPU) based Approach for Indic End-to-End\n  TTS Systems", "abstract": "Sentences in Indian languages are generally longer than those in English.\nIndian languages are also considered to be phrase-based, wherein semantically\ncomplete phrases are concatenated to make up sentences. Long utterances lead to\npoor training of text-to-speech models and result in poor prosody during\nsynthesis. In this work, we explore an inter-pausal unit (IPU) based approach\nin the end-to-end (E2E) framework, focusing on synthesising\nconversational-style text. We consider both autoregressive Tacotron2 and\nnon-autoregressive FastSpeech2 architectures in our study and perform\nexperiments with three Indian languages, namely, Hindi, Tamil and Telugu. With\nthe IPU-based Tacotron2 approach, we see a reduction in insertion and deletion\nerrors in the synthesised audio, providing an alternative approach to the\nFastSpeech(2) network in terms of error reduction. The IPU-based approach\nrequires less computational resources and produces prosodically richer\nsynthesis compared to conventional sentence-based systems.", "published": "2024-09-18 12:25:41", "link": "http://arxiv.org/abs/2409.11915v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via\n  Multi-Scale Speech Coding and Generation", "abstract": "The neural codec language model (CLM) has demonstrated remarkable performance\nin text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM\nlacks sufficient attention to coarse-grained information at a higher temporal\nscale, often producing unnatural or even unintelligible speech. This work\nproposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale\nspeech coding and generation to address this issue. We train a multi-scale\nneural codec, CoFi-Codec, to encode speech into a multi-scale discrete\nrepresentation, comprising multiple token sequences with different time\nresolutions. Then, we propose CoFi-LM that can generate this representation in\ntwo modes: the single-LM-based chain-of-scale generation and the\nmultiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech\nsignificantly outperforms single-scale baseline systems on naturalness and\nspeaker similarity in zero-shot TTS. The analysis of multi-scale coding\ndemonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete\nspeech representations while keeping high-quality speech reconstruction. The\ncoarse-to-fine multi-scale generation, especially for the stack-of-scale\napproach, is also validated as a crucial approach in pursuing a high-quality\nneural codec language model for TTS.", "published": "2024-09-18 01:31:19", "link": "http://arxiv.org/abs/2409.11630v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dense-TSNet: Dense Connected Two-Stage Structure for Ultra-Lightweight\n  Speech Enhancement", "abstract": "Speech enhancement aims to improve speech quality and intelligibility in\nnoisy environments. Recent advancements have concentrated on deep neural\nnetworks, particularly employing the Two-Stage (TS) architecture to enhance\nfeature extraction. However, the complexity and size of these models remain\nsignificant, which limits their applicability in resource-constrained\nscenarios. Designing models suitable for edge devices presents its own set of\nchallenges. Narrow lightweight models often encounter performance bottlenecks\ndue to uneven loss landscapes. Additionally, advanced operators such as\nTransformers or Mamba may lack the practical adaptability and efficiency that\nconvolutional neural networks (CNNs) offer in real-world deployments. To\naddress these challenges, we propose Dense-TSNet, an innovative\nultra-lightweight speech enhancement network. Our approach employs a novel\nDense Two-Stage (Dense-TS) architecture, which, compared to the classic\nTwo-Stage architecture, ensures more robust refinement of the objective\nfunction in the later training stages. This leads to improved final\nperformance, addressing the early convergence limitations of the baseline\nmodel. We also introduce the Multi-View Gaze Block (MVGB), which enhances\nfeature extraction by incorporating global, channel, and local perspectives\nthrough convolutional neural networks (CNNs). Furthermore, we discuss how the\nchoice of loss function impacts perceptual quality. Dense-TSNet demonstrates\npromising performance with a compact model size of around 14K parameters,\nmaking it particularly well-suited for deployment in resource-constrained\nenvironments.", "published": "2024-09-18 06:21:36", "link": "http://arxiv.org/abs/2409.11725v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance and Robustness of Signal-Dependent vs. Signal-Independent\n  Binaural Signal Matching with Wearable Microphone Arrays", "abstract": "The increasing popularity of spatial audio in applications such as\nteleconferencing, entertainment, and virtual reality has led to the recent\ndevelopments of binaural reproduction methods. However, only a few of these\nmethods are well-suited for wearable and mobile arrays, which typically consist\nof a small number of microphones. One such method is binaural signal matching\n(BSM), which has been shown to produce high-quality binaural signals for\nwearable arrays. However, BSM may be suboptimal in cases of high\ndirect-to-reverberant ratio (DRR) as it is based on the diffuse sound field\nassumption. To overcome this limitation, previous studies incorporated\nsound-field models other than diffuse. However, performance may be sensitive to\nsignal estimation errors. This paper aims to provide a systematic and\ncomprehensive analysis of signal-dependent vs. signal-independent BSM, so that\nthe benefits and limitations of the methods become clearer. Two\nsignal-dependent BSM-based methods designed for high DRR scenarios that\nincorporate a sound field model composed of direct and reverberant components\nare investigated mathematically, using simulations, and finally validated by a\nlistening test, and compared to the signal-independent BSM. The results show\nthat signal-dependent BSM can significantly improve performance, in particular\nin the direction of the source, while presenting only a negligible degradation\nin other directions. Furthermore, when source direction estimation is\ninaccurate, performance of of the signal-dependent BSM degrade to equal that of\nthe signal-independent BSM, presenting a desired robustness quality.", "published": "2024-09-18 06:40:12", "link": "http://arxiv.org/abs/2409.11731v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simulating Native Speaker Shadowing for Nonnative Speech Assessment with\n  Latent Speech Representations", "abstract": "Evaluating speech intelligibility is a critical task in computer-aided\nlanguage learning systems. Traditional methods often rely on word error rates\n(WER) provided by automatic speech recognition (ASR) as intelligibility scores.\nHowever, this approach has significant limitations due to notable differences\nbetween human speech recognition (HSR) and ASR. A promising alternative is to\ninvolve a native (L1) speaker in shadowing what nonnative (L2) speakers say.\nBreakdowns or mispronunciations in the L1 speaker's shadowing utterance can\nserve as indicators for assessing L2 speech intelligibility. In this study, we\npropose a speech generation system that simulates the L1 shadowing process\nusing voice conversion (VC) techniques and latent speech representations. Our\nexperimental results demonstrate that this method effectively replicates the L1\nshadowing process, offering an innovative tool to evaluate L2 speech\nintelligibility. Notably, systems that utilize self-supervised speech\nrepresentations (S3R) show a higher degree of similarity to real L1 shadowing\nutterances in both linguistic accuracy and naturalness.", "published": "2024-09-18 06:54:46", "link": "http://arxiv.org/abs/2409.11742v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SALT: Standardized Audio event Label Taxonomy", "abstract": "Machine listening systems often rely on fixed taxonomies to organize and\nlabel audio data, key for training and evaluating deep neural networks (DNNs)\nand other supervised algorithms. However, such taxonomies face significant\nconstraints: they are composed of application-dependent predefined categories,\nwhich hinders the integration of new or varied sounds, and exhibits limited\ncross-dataset compatibility due to inconsistent labeling standards. To overcome\nthese limitations, we introduce SALT: Standardized Audio event Label Taxonomy.\nBuilding upon the hierarchical structure of AudioSet's ontology, our taxonomy\nextends and standardizes labels across 24 publicly available environmental\nsound datasets, allowing the mapping of class labels from diverse datasets to a\nunified system. Our proposal comes with a new Python package designed for\nnavigating and utilizing this taxonomy, easing cross-dataset label searching\nand hierarchical exploration. Notably, our package allows effortless data\naggregation from diverse sources, hence easy experimentation with combined\ndatasets.", "published": "2024-09-18 06:59:47", "link": "http://arxiv.org/abs/2409.11746v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music\n  Generation", "abstract": "Western music is often characterized by a homophonic texture, in which the\nmusical content can be organized into a melody and an accompaniment. In\norchestral music, in particular, the composer can select specific\ncharacteristics for each instrument's part within the accompaniment, while also\nneeding to adapt the melody to suit the capabilities of the instruments\nperforming it. In this work, we propose METEOR, a model for Melody-aware\nTexture-controllable Orchestral music generation. This model performs symbolic\nmulti-track music style transfer with a focus on melodic fidelity. We allow\nbar- and track-level controllability of the accompaniment with various textural\nattributes while keeping a homophonic texture. We show that the model can\nachieve controllability performances similar to strong baselines while greatly\nimprove melodic fidelity.", "published": "2024-09-18 07:15:11", "link": "http://arxiv.org/abs/2409.11753v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spin Detection Using Racket Bounce Sounds in Table Tennis", "abstract": "While table tennis players primarily rely on visual cues, sound provides\nvaluable information. The sound generated when the ball strikes the racket can\nassist in predicting the ball's trajectory, especially in determining the spin.\nWhile professional players can distinguish spin through these auditory cues,\nthey often go unnoticed by untrained players. In this paper, we demonstrate\nthat different rackets produce distinct sounds, which can be used to identify\nthe racket type. In addition, we show that the sound generated by the racket\ncan indicate whether spin was applied to the ball, or not. To achieve this, we\ncreated a comprehensive dataset featuring bounce sounds from 10 racket\nconfigurations, each applying various spins to the ball. To achieve millisecond\nlevel temporal accuracy, we first detect high frequency peaks that may\ncorrespond to table tennis ball bounces. We then refine these results using a\nCNN based classifier that accurately predicts both the type of racket used and\nwhether spin was applied.", "published": "2024-09-18 07:36:24", "link": "http://arxiv.org/abs/2409.11760v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conformal Prediction for Manifold-based Source Localization with\n  Gaussian Processes", "abstract": "We address the problem of uncertainty quantification (UQ) in the localization\nof a sound source within adverse acoustic environments. Estimating the position\nof the source is influenced by various factors, such as noise and\nreverberation, leading to significant uncertainty. Quantifying this uncertainty\nis essential, particularly when localization outcomes impact critical\ndecision-making processes, such as in robot audition, where the accuracy of\nlocation estimates directly influences subsequent actions. Despite this, common\nlocalization methods offer point estimates without quantifying the estimation\nuncertainty. To address this, we employ conformal prediction (CP)-a framework\nthat delivers statistically valid prediction intervals (PIs) with finite-sample\nguarantees, independent of the data distribution. However, commonly used\nInductive CP (ICP) methods require a large amount of labeled data, which can be\ndifficult to obtain in the localization setting. To mitigate this limitation,\nwe incorporate a semi-supervised manifold-based localization method using\nGaussian process regression (GPR), with an efficient Transductive CP (TCP)\ntechnique, specifically designed for GPR. We demonstrate that our method\ngenerates statistically valid PIs across different acoustic conditions, while\nproducing smaller intervals compared to baselines.", "published": "2024-09-18 08:41:14", "link": "http://arxiv.org/abs/2409.11804v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for\n  Enhancing Whisper", "abstract": "State-of-the-art models like OpenAI's Whisper exhibit strong performance in\nmultilingual automatic speech recognition (ASR), but they still face challenges\nin accurately recognizing diverse subdialects. In this paper, we propose\nM2R-whisper, a novel multi-stage and multi-scale retrieval augmentation\napproach designed to enhance ASR performance in low-resource settings. Building\non the principles of in-context learning (ICL) and retrieval-augmented\ntechniques, our method employs sentence-level ICL in the pre-processing stage\nto harness contextual information, while integrating token-level k-Nearest\nNeighbors (kNN) retrieval as a post-processing step to further refine the final\noutput distribution. By synergistically combining sentence-level and\ntoken-level retrieval strategies, M2R-whisper effectively mitigates various\ntypes of recognition errors. Experiments conducted on Mandarin and subdialect\ndatasets, including AISHELL-1 and KeSpeech, demonstrate substantial\nimprovements in ASR accuracy, all achieved without any parameter updates.", "published": "2024-09-18 11:35:55", "link": "http://arxiv.org/abs/2409.11889v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mixture of Experts Fusion for Fake Audio Detection Using Frozen wav2vec\n  2.0", "abstract": "Speech synthesis technology has posed a serious threat to speaker\nverification systems.\n  Currently, the most effective fake audio detection methods utilize pretrained\nmodels, and integrating features from various layers of pretrained model\nfurther enhances detection performance.\n  However, most of the previously proposed fusion methods require fine-tuning\nthe pretrained models, resulting in excessively long training times and\nhindering model iteration when facing new speech synthesis technology.\n  To address this issue, this paper proposes a feature fusion method based on\nthe Mixture of Experts, which extracts and integrates features relevant to fake\naudio detection from layer features, guided by a gating network based on the\nlast layer feature, while freezing the pretrained model.\n  Experiments conducted on the ASVspoof2019 and ASVspoof2021 datasets\ndemonstrate that the proposed method achieves competitive performance compared\nto those requiring fine-tuning.", "published": "2024-09-18 12:10:04", "link": "http://arxiv.org/abs/2409.11909v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for\n  Authenticity Verification", "abstract": "Recent advances in speech spoofing necessitate stronger verification\nmechanisms in neural speech codecs to ensure authenticity. Current methods\nembed numerical watermarks before compression and extract them from\nreconstructed speech for verification, but face limitations such as separate\ntraining processes for the watermark and codec, and insufficient cross-modal\ninformation integration, leading to reduced watermark imperceptibility,\nextraction accuracy, and capacity. To address these issues, we propose WMCodec,\nthe first neural speech codec to jointly train compression-reconstruction and\nwatermark embedding-extraction in an end-to-end manner, optimizing both\nimperceptibility and extractability of the watermark. Furthermore, We design an\niterative Attention Imprint Unit (AIU) for deeper feature integration of\nwatermark and speech, reducing the impact of quantization noise on the\nwatermark. Experimental results show WMCodec outperforms AudioSeal with Encodec\nin most quality metrics for watermark imperceptibility and consistently exceeds\nboth AudioSeal with Encodec and reinforced TraceableSpeech in extraction\naccuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16\nbps, WMCodec maintains over 99% extraction accuracy under common attacks,\ndemonstrating strong robustness.", "published": "2024-09-18 16:45:09", "link": "http://arxiv.org/abs/2409.12121v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simultaneous Music Separation and Generation Using Multi-Track Latent\n  Diffusion Models", "abstract": "Diffusion models have recently shown strong potential in both music\ngeneration and music source separation tasks. Although in early stages, a trend\nis emerging towards integrating these tasks into a single framework, as both\ninvolve generating musically aligned parts and can be seen as facets of the\nsame generative process. In this work, we introduce a latent diffusion-based\nmulti-track generation model capable of both source separation and multi-track\nmusic synthesis by learning the joint probability distribution of tracks\nsharing a musical context. Our model also enables arrangement generation by\ncreating any subset of tracks given the others. We trained our model on the\nSlakh2100 dataset, compared it with an existing simultaneous generation and\nseparation model, and observed significant improvements across objective\nmetrics for source separation, music, and arrangement generation tasks. Sound\nexamples are available at https://msg-ld.github.io/.", "published": "2024-09-18 22:35:26", "link": "http://arxiv.org/abs/2409.12346v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "META-CAT: Speaker-Informed Speech Embeddings via Meta Information\n  Concatenation for Multi-talker ASR", "abstract": "We propose a novel end-to-end multi-talker automatic speech recognition (ASR)\nframework that enables both multi-speaker (MS) ASR and target-speaker (TS) ASR.\nOur proposed model is trained in a fully end-to-end manner, incorporating\nspeaker supervision from a pre-trained speaker diarization module. We introduce\nan intuitive yet effective method for masking ASR encoder activations using\noutput from the speaker supervision module, a technique we term Meta-Cat\n(meta-information concatenation), that can be applied to both MS-ASR and\nTS-ASR. Our results demonstrate that the proposed architecture achieves\ncompetitive performance in both MS-ASR and TS-ASR tasks, without the need for\ntraditional methods, such as neural mask estimation or masking at the audio or\nfeature level. Furthermore, we demonstrate a glimpse of a unified dual-task\nmodel which can efficiently handle both MS-ASR and TS-ASR tasks. Thus, this\nwork illustrates that a robust end-to-end multi-talker ASR framework can be\nimplemented with a streamlined architecture, obviating the need for the complex\nspeaker filtering mechanisms employed in previous studies.", "published": "2024-09-18 23:16:23", "link": "http://arxiv.org/abs/2409.12352v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A microscopic investigation of the effect of random envelope\n  fluctuations on phoneme-in-noise perception", "abstract": "In this study, we investigated the effect of specific noise realizations on\nthe discrimination of two consonants, /b/ and /d/. For this purpose, we\ncollected data from twelve participants, who listened to the words /aba/ or\n/ada/ embedded in one of three background noises. All noises had the same\nlong-term spectrum but differed in the amount of random envelope fluctuations.\nThe data were analyzed on a trial-by-trial basis using the reverse-correlation\nmethod. The results revealed that it is possible to predict the categorical\nresponses with better-than-chance accuracy purely based on the spectro-temporal\ndistribution of the random envelope fluctuations of the corresponding noises,\nwithout taking into account the actual targets or the signal-to-noise ratios\nused in the trials. The effect of the noise fluctuations explained on average\n8.1% of the participants' responses in white noise, a proportion that increased\nup to 13.3% for noises with a larger amount of fluctuations. The estimated\ntime-frequency weights revealed that the measured effect originated from\nconfusions between noise fluctuations and relevant acoustic cues from the\ntarget words. Substantially similar conclusions were obtained from simulations\nusing an artificial listener. We argue that this token-specific effect of noise\nis a form of informational masking.", "published": "2024-09-18 11:47:13", "link": "http://arxiv.org/abs/2409.13765v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DETECLAP: Enhancing Audio-Visual Representation Learning with Object\n  Information", "abstract": "Current audio-visual representation learning can capture rough object\ncategories (e.g., ``animals'' and ``instruments''), but it lacks the ability to\nrecognize fine-grained details, such as specific categories like ``dogs'' and\n``flutes'' within animals and instruments. To address this issue, we introduce\nDETECLAP, a method to enhance audio-visual representation learning with object\ninformation. Our key idea is to introduce an audio-visual label prediction loss\nto the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its\nobject awareness. To avoid costly manual annotations, we prepare object labels\nfrom both audio and visual inputs using state-of-the-art language-audio models\nand object detectors. We evaluate the method of audio-visual retrieval and\nclassification using the VGGSound and AudioSet20K datasets. Our method achieves\nimprovements in recall@10 of +1.5% and +1.2% for audio-to-visual and\nvisual-to-audio retrieval, respectively, and an improvement in accuracy of\n+0.6% for audio-visual classification.", "published": "2024-09-18 06:38:48", "link": "http://arxiv.org/abs/2409.11729v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style\n  Temporal Modeling in Text-to-Speech", "abstract": "In recent years, speech diffusion models have advanced rapidly. Alongside the\nwidely used U-Net architecture, transformer-based models such as the Diffusion\nTransformer (DiT) have also gained attention. However, current DiT speech\nmodels treat Mel spectrograms as general images, which overlooks the specific\nacoustic properties of speech. To address these limitations, we propose a\nmethod called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which\nbuilds on DiT and achieves fast training without compromising accuracy.\nNotably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive\ninference approach that aligns more closely with acoustic properties, enhancing\nthe naturalness of the generated speech. Additionally, we introduce a\nfine-grained style temporal modeling method that further improves speaker style\nsimilarity. Experimental results demonstrate that our method increases the\ntraining speed by nearly 2 times and significantly outperforms the baseline\nmodels.", "published": "2024-09-18 09:36:55", "link": "http://arxiv.org/abs/2409.11835v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Efficient Acoustic Scene Classification using Teacher-Informed\n  Confusing Class Instruction", "abstract": "In this technical report, we describe the SNTL-NTU team's submission for Task\n1 Data-Efficient Low-Complexity Acoustic Scene Classification of the detection\nand classification of acoustic scenes and events (DCASE) 2024 challenge. Three\nsystems are introduced to tackle training splits of different sizes. For small\ntraining splits, we explored reducing the complexity of the provided baseline\nmodel by reducing the number of base channels. We introduce data augmentation\nin the form of mixup to increase the diversity of training samples. For the\nlarger training splits, we use FocusNet to provide confusing class information\nto an ensemble of multiple Patchout faSt Spectrogram Transformer (PaSST) models\nand baseline models trained on the original sampling rate of 44.1 kHz. We use\nKnowledge Distillation to distill the ensemble model to the baseline student\nmodel. Training the systems on the TAU Urban Acoustic Scene 2022 Mobile\ndevelopment dataset yielded the highest average testing accuracy of (62.21,\n59.82, 56.81, 53.03, 47.97)% on split (100, 50, 25, 10, 5)% respectively over\nthe three systems.", "published": "2024-09-18 13:16:00", "link": "http://arxiv.org/abs/2409.11964v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pareto Data Framework: Steps Towards Resource-Efficient Decision Making\n  Using Minimum Viable Data (MVD)", "abstract": "This paper introduces the Pareto Data Framework, an approach for identifying\nand selecting the Minimum Viable Data (MVD) required for enabling machine\nlearning applications on constrained platforms such as embedded systems, mobile\ndevices, and Internet of Things (IoT) devices. We demonstrate that strategic\ndata reduction can maintain high performance while significantly reducing\nbandwidth, energy, computation, and storage costs. The framework identifies\nMinimum Viable Data (MVD) to optimize efficiency across resource-constrained\nenvironments without sacrificing performance. It addresses common inefficient\npractices in an IoT application such as overprovisioning of sensors and\noverprecision, and oversampling of signals, proposing scalable solutions for\noptimal sensor selection, signal extraction and transmission, and data\nrepresentation. An experimental methodology demonstrates effective acoustic\ndata characterization after downsampling, quantization, and truncation to\nsimulate reduced-fidelity sensors and network and storage constraints; results\nshows that performance can be maintained up to 95\\% with sample rates reduced\nby 75\\% and bit depths and clip length reduced by 50\\% which translates into\nsubstantial cost and resource reduction. These findings have implications on\nthe design and development of constrained systems. The paper also discusses\nbroader implications of the framework, including the potential to democratize\nadvanced AI technologies across IoT applications and sectors such as\nagriculture, transportation, and manufacturing to improve access and multiply\nthe benefits of data-driven insights.", "published": "2024-09-18 16:31:19", "link": "http://arxiv.org/abs/2409.12112v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models", "abstract": "With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to\nhttps://everest-ai.github.io/takinaudiollm/.", "published": "2024-09-18 17:03:12", "link": "http://arxiv.org/abs/2409.12139v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Language Models are Strong Audio-Visual Speech Recognition\n  Learners", "abstract": "Multimodal large language models (MLLMs) have recently become a focal point\nof research due to their formidable multimodal understanding capabilities. For\nexample, in the audio and speech domains, an LLM can be equipped with\n(automatic) speech recognition (ASR) abilities by just concatenating the audio\ntokens, computed with an audio encoder, and the text tokens to achieve\nstate-of-the-art results. On the contrary, tasks like visual and audio-visual\nspeech recognition (VSR/AVSR), which also exploit noise-invariant lip movement\ninformation, have received little or no attention. To bridge this gap, we\npropose Llama-AVSR, a new MLLM with strong audio-visual speech recognition\ncapabilities. It leverages pre-trained audio and video encoders to produce\nmodality-specific tokens which, together with the text tokens, are processed by\na pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an\nauto-regressive fashion. Llama-AVSR requires a small number of trainable\nparameters as only modality-specific projectors and LoRA modules are trained\nwhereas the multi-modal encoders and LLM are kept frozen. We evaluate our\nproposed approach on LRS3, the largest public AVSR benchmark, and we achieve\nnew state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79%\nand 0.77%, respectively. To bolster our results, we investigate the key factors\nthat underpin the effectiveness of Llama-AVSR: the choice of the pre-trained\nencoders and LLM, the efficient integration of LoRA modules, and the optimal\nperformance-efficiency trade-off obtained via modality-aware compression rates.", "published": "2024-09-18 21:17:27", "link": "http://arxiv.org/abs/2409.12319v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SpoofCeleb: Speech Deepfake Detection and SASV In The Wild", "abstract": "This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake\nDetection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV),\nutilizing source data from real-world conditions and spoofing attacks generated\nby Text-To-Speech (TTS) systems also trained on the same real-world data.\nRobust recognition systems require speech data recorded in varied acoustic\nenvironments with different levels of noise to be trained. However, existing\ndatasets typically include clean, high-quality recordings (bona fide data) due\nto the requirements for TTS training; studio-quality or well-recorded read\nspeech is typically necessary to train TTS models. Existing SDD datasets also\nhave limited usefulness for training SASV models due to insufficient speaker\ndiversity. We present SpoofCeleb, which leverages a fully automated pipeline\nthat processes the VoxCeleb1 dataset, transforming it into a suitable form for\nTTS training. We subsequently train 23 contemporary TTS systems. The resulting\nSpoofCeleb dataset comprises over 2.5 million utterances from 1,251 unique\nspeakers, collected under natural, real-world conditions. The dataset includes\ncarefully partitioned training, validation, and evaluation sets with\nwell-controlled experimental protocols. We provide baseline results for both\nSDD and SASV tasks. All data, protocols, and baselines are publicly available\nat https://jungjee.github.io/spoofceleb.", "published": "2024-09-18 23:17:02", "link": "http://arxiv.org/abs/2409.17285v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
