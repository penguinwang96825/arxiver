{"title": "Scalable Sentiment for Sequence-to-sequence Chatbot Response with\n  Performance Analysis", "abstract": "Conventional seq2seq chatbot models only try to find the sentences with the\nhighest probabilities conditioned on the input sequences, without considering\nthe sentiment of the output sentences. Some research works trying to modify the\nsentiment of the output sequences were reported. In this paper, we propose five\nmodels to scale or adjust the sentiment of the chatbot response: persona-based\nmodel, reinforcement learning, plug and play model, sentiment transformation\nnetwork and cycleGAN, all based on the conventional seq2seq model. We also\ndevelop two evaluation metrics to estimate if the responses are reasonable\ngiven the input. These metrics together with other two popularly used metrics\nwere used to analyze the performance of the five proposed models on different\naspects, and reinforcement learning and cycleGAN were shown to be very\nattractive. The evaluation metrics were also found to be well correlated with\nhuman evaluation.", "published": "2018-04-07 03:56:55", "link": "http://arxiv.org/abs/1804.02504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating historical text normalization systems: How well do they\n  generalize?", "abstract": "We highlight several issues in the evaluation of historical text\nnormalization systems that make it hard to tell how well these systems would\nactually work in practice---i.e., for new datasets or languages; in comparison\nto more na\\\"ive systems; or as a preprocessing step for downstream NLP tools.\nWe illustrate these issues and exemplify our proposed evaluation practices by\ncomparing two neural models against a na\\\"ive baseline system. We show that the\nneural models generalize well to unseen words in tests on five languages;\nnevertheless, they provide no clear benefit over the na\\\"ive baseline for\ndownstream POS tagging of an English historical collection. We conclude that\nfuture work should include more rigorous evaluation, including both intrinsic\nand extrinsic measures where possible.", "published": "2018-04-07 11:06:26", "link": "http://arxiv.org/abs/1804.02545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Neural Machine Translation with Retrieved Translation Pieces", "abstract": "One of the difficulties of neural machine translation (NMT) is the recall and\nappropriate translation of low-frequency words or phrases. In this paper, we\npropose a simple, fast, and effective method for recalling previously seen\ntranslation examples and incorporating them into the NMT decoding process.\nSpecifically, for an input sentence, we use a search engine to retrieve\nsentence pairs whose source sides are similar with the input sentence, and then\ncollect $n$-grams that are both in the retrieved target sentences and aligned\nwith words that match in the source sentences, which we call \"translation\npieces\". We compute pseudo-probabilities for each retrieved sentence based on\nsimilarities between the input sentence and the retrieved source sentences, and\nuse these to weight the retrieved translation pieces. Finally, an existing NMT\nmodel is used to translate the input sentence, with an additional bonus given\nto outputs that contain the collected translation pieces. We show our method\nimproves NMT translation results up to 6 BLEU points on three narrow domain\ntranslation tasks where repetitiveness of the target sentences is particularly\nsalient. It also causes little increase in the translation time, and compares\nfavorably to another alternative retrieval-based method with respect to\naccuracy, speed, and simplicity of implementation.", "published": "2018-04-07 13:20:24", "link": "http://arxiv.org/abs/1804.02559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Models for Word Formation in English Slang", "abstract": "We propose generative models for three types of extra-grammatical word\nformation phenomena abounding in English slang: Blends, Clippings, and\nReduplicatives. Adopting a data-driven approach coupled with linguistic\nknowledge, we propose simple models with state of the art performance on human\nannotated gold standard datasets. Overall, our models reveal insights into the\ngenerative processes of word formation in slang -- insights which are\nincreasingly relevant in the context of the rising prevalence of slang and\nnon-standard varieties on the Internet.", "published": "2018-04-07 21:59:46", "link": "http://arxiv.org/abs/1804.02596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quootstrap: Scalable Unsupervised Extraction of Quotation-Speaker Pairs\n  from Large News Corpora via Bootstrapping", "abstract": "We propose Quootstrap, a method for extracting quotations, as well as the\nnames of the speakers who uttered them, from large news corpora. Whereas prior\nwork has addressed this problem primarily with supervised machine learning, our\napproach follows a fully unsupervised bootstrapping paradigm. It leverages the\nredundancy present in large news corpora, more precisely, the fact that the\nsame quotation often appears across multiple news articles in slightly\ndifferent contexts. Starting from a few seed patterns, such as [\"Q\", said S.],\nour method extracts a set of quotation-speaker pairs (Q, S), which are in turn\nused for discovering new patterns expressing the same quotations; the process\nis then repeated with the larger pattern set. Our algorithm is highly scalable,\nwhich we demonstrate by running it on the large ICWSM 2011 Spinn3r corpus.\nValidating our results against a crowdsourced ground truth, we obtain 90%\nprecision at 40% recall using a single seed pattern, with significantly higher\nrecall values for more frequently reported (and thus likely more interesting)\nquotations. Finally, we showcase the usefulness of our algorithm's output for\ncomputational social science by analyzing the sentiment expressed in our\nextracted quotations.", "published": "2018-04-07 07:50:50", "link": "http://arxiv.org/abs/1804.02525v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "A comparison of recent waveform generation and acoustic modeling methods\n  for neural-network-based speech synthesis", "abstract": "Recent advances in speech synthesis suggest that limitations such as the\nlossy nature of the amplitude spectrum with minimum phase approximation and the\nover-smoothing effect in acoustic modeling can be overcome by using advanced\nmachine learning approaches. In this paper, we build a framework in which we\ncan fairly compare new vocoding and acoustic modeling techniques with\nconventional approaches by means of a large scale crowdsourced evaluation.\nResults on acoustic models showed that generative adversarial networks and an\nautoregressive (AR) model performed better than a normal recurrent network and\nthe AR model performed best. Evaluation on vocoders by using the same AR\nacoustic model demonstrated that a Wavenet vocoder outperformed classical\nsource-filter-based vocoders. Particularly, generated speech waveforms from the\ncombination of AR acoustic model and Wavenet vocoder achieved a similar score\nof speech quality to vocoded speech.", "published": "2018-04-07 12:16:50", "link": "http://arxiv.org/abs/1804.02549v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
