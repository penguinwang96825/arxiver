{"title": "STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation\n  Learning", "abstract": "In this paper, we present a novel multi-modal deep neural network\narchitecture that uses speech and text entanglement for learning phonetically\nsound spoken-word representations. STEPs-RL is trained in a supervised manner\nto predict the phonetic sequence of a target spoken-word using its contextual\nspoken word's speech and text, such that the model encodes its meaningful\nlatent representations. Unlike existing work, we have used text along with\nspeech for auditory representation learning to capture semantical and\nsyntactical information along with the acoustic and temporal information. The\nlatent representations produced by our model were not only able to predict the\ntarget phonetic sequences with an accuracy of 89.47% but were also able to\nachieve competitive results to textual word representation models, Word2Vec &\nFastText (trained on textual transcripts), when evaluated on four widely used\nword similarity benchmark datasets. In addition, investigation of the generated\nvector space also demonstrated the capability of the proposed model to capture\nthe phonetic structure of the spoken-words. To the best of our knowledge, none\nof the existing works use speech and text entanglement for learning spoken-word\nrepresentation, which makes this work first of its kind.", "published": "2020-11-23 13:29:16", "link": "http://arxiv.org/abs/2011.11387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-ISCA: Bidirectional Inter-Sentence Contextual Attention Mechanism for\n  Detecting Sarcasm in User Generated Noisy Short Text", "abstract": "Many online comments on social media platforms are hateful, humorous, or\nsarcastic. The sarcastic nature of these comments (especially the short ones)\nalters their actual implied sentiments, which leads to misinterpretations by\nthe existing sentiment analysis models. A lot of research has already been done\nto detect sarcasm in the text using user-based, topical, and conversational\ninformation but not much work has been done to use inter-sentence contextual\ninformation for detecting the same. This paper proposes a new state-of-the-art\ndeep learning architecture that uses a novel Bidirectional Inter-Sentence\nContextual Attention mechanism (Bi-ISCA) to capture inter-sentence dependencies\nfor detecting sarcasm in the user-generated short text using only the\nconversational context. The proposed deep learning model demonstrates the\ncapability to capture explicit, implicit, and contextual incongruous words &\nphrases responsible for invoking sarcasm. Bi-ISCA generates state-of-the-art\nresults on two widely used benchmark datasets for the sarcasm detection task\n(Reddit and Twitter). To the best of our knowledge, none of the existing\nstate-of-the-art models use an inter-sentence contextual attention mechanism to\ndetect sarcasm in the user-generated short text using only conversational\ncontext.", "published": "2020-11-23 15:24:27", "link": "http://arxiv.org/abs/2011.11465v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language\n  Model", "abstract": "Recent research indicates that pretraining cross-lingual language models on\nlarge-scale unlabeled texts yields significant performance improvements over\nvarious cross-lingual and low-resource tasks. Through training on one hundred\nlanguages and terabytes of texts, cross-lingual language models have proven to\nbe effective in leveraging high-resource languages to enhance low-resource\nlanguage processing and outperform monolingual models. In this paper, we\nfurther investigate the cross-lingual and cross-domain (CLCD) setting when a\npretrained cross-lingual language model needs to adapt to new domains.\nSpecifically, we propose a novel unsupervised feature decomposition method that\ncan automatically extract domain-specific features and domain-invariant\nfeatures from the entangled pretrained cross-lingual representations, given\nunlabeled raw texts in the source language. Our proposed model leverages mutual\ninformation estimation to decompose the representations computed by a\ncross-lingual model into domain-invariant and domain-specific parts.\nExperimental results show that our proposed method achieves significant\nperformance improvements over the state-of-the-art pretrained cross-lingual\nlanguage model in the CLCD setting. The source code of this paper is publicly\navailable at https://github.com/lijuntaopku/UFD.", "published": "2020-11-23 16:00:42", "link": "http://arxiv.org/abs/2011.11499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Online Multilingual Hate speech Recognition System", "abstract": "The exponential increase in the use of the Internet and social media over the\nlast two decades has changed human interaction. This has led to many positive\noutcomes, but at the same time it has brought risks and harms. While the volume\nof harmful content online, such as hate speech, is not manageable by humans,\ninterest in the academic community to investigate automated means for hate\nspeech detection has increased. In this study, we analyse six publicly\navailable datasets by combining them into a single homogeneous dataset and\nclassify them into three classes, abusive, hateful or neither. We create a\nbaseline model and we improve model performance scores using various\noptimisation techniques. After attaining a competitive performance score, we\ncreate a tool which identifies and scores a page with effective metric in\nnear-real time and uses the same as feedback to re-train our model. We prove\nthe competitive performance of our multilingual model on two langauges, English\nand Hindi, leading to comparable or superior performance to most monolingual\nmodels.", "published": "2020-11-23 16:33:48", "link": "http://arxiv.org/abs/2011.11523v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Studying Taxonomy Enrichment on Diachronic WordNet Versions", "abstract": "Ontologies, taxonomies, and thesauri are used in many NLP tasks. However,\nmost studies are focused on the creation of these lexical resources rather than\nthe maintenance of the existing ones. Thus, we address the problem of taxonomy\nenrichment. We explore the possibilities of taxonomy extension in a\nresource-poor setting and present methods which are applicable to a large\nnumber of languages. We create novel English and Russian datasets for training\nand evaluating taxonomy enrichment models and describe a technique of creating\nsuch datasets for other languages.", "published": "2020-11-23 16:49:37", "link": "http://arxiv.org/abs/2011.11536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does BERT Understand Sentiment? Leveraging Comparisons Between\n  Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment\n  Models", "abstract": "When performing Polarity Detection for different words in a sentence, we need\nto look at the words around to understand the sentiment. Massively pretrained\nlanguage models like BERT can encode not only just the words in a document but\nalso the context around the words along with them. This begs the questions,\n\"Does a pretrain language model also automatically encode sentiment information\nabout each word?\" and \"Can it be used to infer polarity towards different\naspects?\". In this work we try to answer this question by showing that training\na comparison of a contextual embedding from BERT and a generic word embedding\ncan be used to infer sentiment. We also show that if we finetune a subset of\nweights the model built on comparison of BERT and generic word embedding, it\ncan get state of the art results for Polarity Detection in Aspect Based\nSentiment Classification datasets.", "published": "2020-11-23 19:12:31", "link": "http://arxiv.org/abs/2011.11673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Machine Learning and Natural Language Processing Techniques to\n  Analyze and Support Moderation of Student Book Discussions", "abstract": "The increasing adoption of technology to augment or even replace traditional\nface-to-face learning has led to the development of a myriad of tools and\nplatforms aimed at engaging the students and facilitating the teacher's ability\nto present new information. The IMapBook project aims at improving the literacy\nand reading comprehension skills of elementary school-aged children by\npresenting them with interactive e-books and letting them take part in\nmoderated book discussions. This study aims to develop and illustrate a machine\nlearning-based approach to message classification that could be used to\nautomatically notify the discussion moderator of a possible need for an\nintervention and also to collect other useful information about the ongoing\ndiscussion. We aim to predict whether a message posted in the discussion is\nrelevant to the discussed book, whether the message is a statement, a question,\nor an answer, and in which broad category it can be classified. We\nincrementally enrich our used feature subsets and compare them using standard\nclassification algorithms as well as the novel Feature stacking method. We use\nstandard classification performance metrics as well as the Bayesian correlated\nt-test to show that the use of described methods in discussion moderation is\nfeasible. Moving forward, we seek to attain better performance by focusing on\nextracting more of the significant information found in the strong temporal\ninterdependence of the messages.", "published": "2020-11-23 20:33:09", "link": "http://arxiv.org/abs/2011.11712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Humor-Focused Sentiment Analysis through Improved\n  Contextualized Embeddings and Model Architecture", "abstract": "Humor is a natural and fundamental component of human interactions. When\ncorrectly applied, humor allows us to express thoughts and feelings\nconveniently and effectively, increasing interpersonal affection, likeability,\nand trust. However, understanding the use of humor is a computationally\nchallenging task from the perspective of humor-aware language processing\nmodels. As language models become ubiquitous through virtual-assistants and IOT\ndevices, the need to develop humor-aware models rises exponentially. To further\nimprove the state-of-the-art capacity to perform this particular\nsentiment-analysis task we must explore models that incorporate contextualized\nand nonverbal elements in their design. Ideally, we seek architectures\naccepting non-verbal elements as additional embedded inputs to the model,\nalongside the original sentence-embedded input. This survey thus analyses the\ncurrent state of research in techniques for improved contextualized embedding\nincorporating nonverbal information, as well as newly proposed deep\narchitectures to improve context retention on top of popular word-embeddings\nmethods.", "published": "2020-11-23 22:30:32", "link": "http://arxiv.org/abs/2011.11773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Input Representation for Language Identification in\n  Hindi-English Code Mixed Text", "abstract": "Natural language processing (NLP) techniques have become mainstream in the\nrecent decade. Most of these advances are attributed to the processing of a\nsingle language. More recently, with the extensive growth of social media\nplatforms focus has shifted to code-mixed text. The code-mixed text comprises\ntext written in more than one language. People naturally tend to combine local\nlanguage with global languages like English. To process such texts, current NLP\ntechniques are not sufficient. As a first step, the text is processed to\nidentify the language of the words in the text. In this work, we focus on\nlanguage identification in code-mixed sentences for Hindi-English mixed text.\nThe task of language identification is formulated as a token classification\ntask. In the supervised setting, each word in the sentence has an associated\nlanguage label. We evaluate different deep learning models and input\nrepresentation combinations for this task. Mainly, character, sub-word, and\nword embeddings are considered in combination with CNN and LSTM based models.\nWe show that sub-word representation along with the LSTM model gives the best\nresults. In general sub-word representations perform significantly better than\nother input representations. We report the best accuracy of 94.52% using a\nsingle layer LSTM model on the standard SAIL ICON 2017 test set.", "published": "2020-11-23 08:08:09", "link": "http://arxiv.org/abs/2011.11263v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conformance Checking of Mixed-paradigm Process Models", "abstract": "Mixed-paradigm process models integrate strengths of procedural and\ndeclarative representations like Petri nets and Declare. They are specifically\ninteresting for process mining because they allow capturing complex behaviour\nin a compact way. A key research challenge for the proliferation of\nmixed-paradigm models for process mining is the lack of corresponding\nconformance checking techniques. In this paper, we address this problem by\ndevising the first approach that works with intertwined state spaces of\nmixed-paradigm models. More specifically, our approach uses an alignment-based\nreplay to explore the state space and compute trace fitness in a procedural\nway. In every state, the declarative constraints are separately updated, such\nthat violations disable the corresponding activities. Our technique provides\nfor an efficient replay towards an optimal alignment by respecting all\northogonal Declare constraints. We have implemented our technique in ProM and\ndemonstrate its performance in an evaluation with real-world event logs.", "published": "2020-11-23 17:04:33", "link": "http://arxiv.org/abs/2011.11551v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Effect of Word Embedding Models on Hate and Offensive Speech Detection", "abstract": "Deep neural networks have been adopted successfully in hate speech detection\nproblems. Nevertheless, the effect of the word embedding models on the neural\nnetwork's performance has not been appropriately examined in the literature. In\nour study, through different detection tasks, 2-class, 3-class, and 6-class\nclassification, we investigate the impact of both word embedding models and\nneural network architectures on the predictive accuracy. Our focus is on the\nArabic language. We first train several word embedding models on a large-scale\nunlabelled Arabic text corpus. Next, based on a dataset of Arabic hate and\noffensive speech, for each detection task, we train several neural network\nclassifiers using the pre-trained word embedding models. This task yields a\nlarge number of various learned models, which allows conducting an exhaustive\ncomparison. The empirical analysis demonstrates, on the one hand, the\nsuperiority of the skip-gram models and, on the other hand, the superiority of\nthe CNN network across the three detection tasks.", "published": "2020-11-23 02:43:45", "link": "http://arxiv.org/abs/2012.07534v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language guided machine action", "abstract": "Here we build a hierarchical modular network called Language guided machine\naction (LGMA), whose modules process information stream mimicking human\ncortical network that allows to achieve multiple general tasks such as language\nguided action, intention decomposition and mental simulation before action\nexecution etc. LGMA contains 3 main systems: (1) primary sensory system that\nmultimodal sensory information of vision, language and sensorimotor. (2)\nassociation system involves and Broca modules to comprehend and synthesize\nlanguage, BA14/40 module to translate between sensorimotor and language,\nmidTemporal module to convert between language and vision, and superior\nparietal lobe to integrate attended visual object and arm state into cognitive\nmap for future spatial actions. Pre-supplementary motor area (pre-SMA) can\nconverts high level intention into sequential atomic actions, while SMA can\nintegrate these atomic actions, current arm and attended object state into\nsensorimotor vector to apply corresponding torques on arm via pre-motor and\nprimary motor of arm to achieve the intention. The high-level executive system\ncontains PFC that does explicit inference and guide voluntary action based on\nlanguage, while BG is the habitual action control center.", "published": "2020-11-23 13:49:02", "link": "http://arxiv.org/abs/2011.11400v1", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "Speech Command Recognition in Computationally Constrained Environments\n  with a Quadratic Self-organized Operational Layer", "abstract": "Automatic classification of speech commands has revolutionized human computer\ninteractions in robotic applications. However, employed recognition models\nusually follow the methodology of deep learning with complicated networks which\nare memory and energy hungry. So, there is a need to either squeeze these\ncomplicated models or use more efficient light-weight models in order to be\nable to implement the resulting classifiers on embedded devices. In this paper,\nwe pick the second approach and propose a network layer to enhance the speech\ncommand recognition capability of a lightweight network and demonstrate the\nresult via experiments. The employed method borrows the ideas of Taylor\nexpansion and quadratic forms to construct a better representation of features\nin both input and hidden layers. This richer representation results in\nrecognition accuracy improvement as shown by extensive experiments on Google\nspeech commands (GSC) and synthetic speech commands (SSC) datasets.", "published": "2020-11-23 14:40:18", "link": "http://arxiv.org/abs/2011.11436v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for\n  unsupervised spoken language modeling", "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning\nof linguistic representations from raw audio signals without any labels, along\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\nmetrics probing for the quality of the learned models at 4 linguistic levels:\nphonetics, lexicon, syntax and semantics. We present the results and analyses\nof a composite baseline made of the concatenation of three unsupervised\nsystems: self-supervised contrastive representation learning (CPC), clustering\n(k-means) and language modeling (LSTM or BERT). The language models learn on\nthe basis of the pseudo-text derived from clustering the learned\nrepresentations. This simple pipeline shows better than chance performance on\nall four metrics, demonstrating the feasibility of spoken language modeling\nfrom raw speech. It also yields worse performance compared to text-based\n'topline' systems trained on the same data, delineating the space to be\nexplored by more sophisticated end-to-end models.", "published": "2020-11-23 18:01:37", "link": "http://arxiv.org/abs/2011.11588v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interpretable Visual Reasoning via Induced Symbolic Space", "abstract": "We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Our model design\nmakes this an easy adaption via first predicting the concepts of objects and\nrelations and then projecting the predicted concepts back to the visual feature\nspace so the compositional reasoning module can process normally. Experiments\non the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of\nthe art without human-annotated functional programs; 2) our induced concepts\nare both accurate and sufficient as OCCAM achieves an on-par performance on\nobjects represented either in visual features or in the induced symbolic\nconcept space.", "published": "2020-11-23 18:21:49", "link": "http://arxiv.org/abs/2011.11603v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Streaming Multi-speaker ASR with RNN-T", "abstract": "Recent research shows end-to-end ASR systems can recognize overlapped speech\nfrom multiple speakers. However, all published works have assumed no latency\nconstraints during inference, which does not hold for most voice assistant\ninteractions. This work focuses on multi-speaker speech recognition based on a\nrecurrent neural network transducer (RNN-T) that has been shown to provide high\nrecognition accuracy at a low latency online recognition regime. We investigate\ntwo approaches to multi-speaker model training of the RNN-T: deterministic\noutput-target assignment and permutation invariant training. We show that\nguiding separation with speaker order labels in the former case enhances the\nhigh-level speaker tracking capability of RNN-T. Apart from that, with\nmultistyle training on single- and multi-speaker utterances, the resulting\nmodels gain robustness against ambiguous numbers of speakers during inference.\nOur best model achieves a WER of 10.2% on simulated 2-speaker LibriSpeech data,\nwhich is competitive with the previously reported state-of-the-art nonstreaming\nmodel (10.3%), while the proposed model could be directly applied for streaming\napplications.", "published": "2020-11-23 19:10:40", "link": "http://arxiv.org/abs/2011.11671v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-task Language Modeling for Improving Speech Recognition of Rare\n  Words", "abstract": "End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.", "published": "2020-11-23 20:40:44", "link": "http://arxiv.org/abs/2011.11715v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-end Silent Speech Recognition with Acoustic Sensing", "abstract": "Silent speech interfaces (SSI) has been an exciting area of recent interest.\nIn this paper, we present a non-invasive silent speech interface that uses\ninaudible acoustic signals to capture people's lip movements when they speak.\nWe exploit the speaker and microphone of the smartphone to emit signals and\nlisten to their reflections, respectively. The extracted phase features of\nthese reflections are fed into the deep learning networks to recognize speech.\nAnd we also propose an end-to-end recognition framework, which combines the CNN\nand attention-based encoder-decoder network. Evaluation results on a limited\nvocabulary (54 sentences) yield word error rates of 8.4% in speaker-independent\nand environment-independent settings, and 8.1% for unseen sentence testing.", "published": "2020-11-23 10:29:23", "link": "http://arxiv.org/abs/2011.11315v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Synthetic Audio to Improve The Recognition of Out-Of-Vocabulary\n  Words in End-To-End ASR Systems", "abstract": "Today, many state-of-the-art automatic speech recognition (ASR) systems apply\nall-neural models that map audio to word sequences trained end-to-end along one\nglobal optimisation criterion in a fully data driven fashion. These models\nallow high precision ASR for domains and words represented in the training\nmaterial but have difficulties recognising words that are rarely or not at all\nrepresented during training, i.e. trending words and new named entities. In\nthis paper, we use a text-to-speech (TTS) engine to provide synthetic audio for\nout-of-vocabulary (OOV) words. We aim to boost the recognition accuracy of a\nrecurrent neural network transducer (RNN-T) on OOV words by using the extra\naudio-text pairs, while maintaining the performance on the non-OOV words.\nDifferent regularisation techniques are explored and the best performance is\nachieved by fine-tuning the RNN-T on both original training data and extra\nsynthetic data with elastic weight consolidation (EWC) applied on the encoder.\nThis yields a 57% relative word error rate (WER) reduction on utterances\ncontaining OOV words without any degradation on the whole test set.", "published": "2020-11-23 17:19:10", "link": "http://arxiv.org/abs/2011.11564v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
