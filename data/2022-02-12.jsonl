{"title": "Improving Contextual Coherence in Variational Personalized and\n  Empathetic Dialogue Agents", "abstract": "In recent years, latent variable models, such as the Conditional Variational\nAuto Encoder (CVAE), have been applied to both personalized and empathetic\ndialogue generation. Prior work have largely focused on generating diverse\ndialogue responses that exhibit persona consistency and empathy. However, when\nit comes to the contextual coherence of the generated responses, there is still\nroom for improvement. Hence, to improve the contextual coherence, we propose a\nnovel Uncertainty Aware CVAE (UA-CVAE) framework. The UA-CVAE framework\ninvolves approximating and incorporating the aleatoric uncertainty during\nresponse generation. We apply our framework to both personalized and empathetic\ndialogue generation. Empirical results show that our framework significantly\nimproves the contextual coherence of the generated response. Additionally, we\nintroduce a novel automatic metric for measuring contextual coherence, which\nwas found to correlate positively with human judgement.", "published": "2022-02-12 03:57:14", "link": "http://arxiv.org/abs/2202.05971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models", "abstract": "Due to the high costs associated with finetuning large language models,\nvarious recent works propose to adapt them to specific tasks without any\nparameter updates through in-context learning. Unfortunately, for in-context\nlearning there is currently no way to leverage unlabeled data, which is often\nmuch easier to obtain in large quantities than labeled examples. In this work,\nwe therefore investigate ways to make use of unlabeled examples to improve the\nzero-shot performance of pretrained language models without any finetuning: We\nintroduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies\nexamples by retrieving semantically similar unlabeled examples, assigning\nlabels to them in a zero-shot fashion, and then using them for in-context\nlearning. We also propose bag-of-contexts priming, a new priming strategy that\nis more suitable for our setting and enables the usage of more examples than\nfit into the context window.", "published": "2022-02-12 19:50:59", "link": "http://arxiv.org/abs/2202.06133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-supervised New Event Type Induction and Description via Contrastive\n  Loss-Enforced Batch Attention", "abstract": "Most event extraction methods have traditionally relied on an annotated set\nof event types. However, creating event ontologies and annotating supervised\ntraining data are expensive and time-consuming. Previous work has proposed\nsemi-supervised approaches which leverage seen (annotated) types to learn how\nto automatically discover new event types. State-of-the-art methods, both\nsemi-supervised or fully unsupervised, use a form of reconstruction loss on\nspecific tokens in a context. In contrast, we present a novel approach to\nsemi-supervised new event type induction using a masked contrastive loss, which\nlearns similarities between event mentions by enforcing an attention mechanism\nover the data minibatch. We further disentangle the discovered clusters by\napproximating the underlying manifolds in the data, which allows us to increase\nnormalized mutual information and Fowlkes-Mallows scores by over 20% absolute.\nBuilding on these clustering results, we extend our approach to two new tasks:\npredicting the type name of the discovered clusters and linking them to\nFrameNet frames.", "published": "2022-02-12 00:32:22", "link": "http://arxiv.org/abs/2202.05943v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "A multi-task semi-supervised framework for Text2Graph & Graph2Text", "abstract": "The Artificial Intelligence industry regularly develops applications that\nmostly rely on Knowledge Bases, a data repository about specific, or general,\ndomains, usually represented in a graph shape. Similar to other databases, they\nface two main challenges: information ingestion and information retrieval. We\napproach these challenges by jointly learning graph extraction from text and\ntext generation from graphs. The proposed solution, a T5 architecture, is\ntrained in a multi-task semi-supervised environment, with our collected\nnon-parallel data, following a cycle training regime. Experiments on WebNLG\ndataset show that our approach surpasses unsupervised state-of-the-art results\nin text-to-graph and graph-to-text. More relevantly, our framework is more\nconsistent across seen and unseen domains than supervised models. The resulting\nmodel can be easily trained in any new domain with non-parallel data, by simply\nadding text and graphs about it, in our cycle framework.", "published": "2022-02-12 11:02:17", "link": "http://arxiv.org/abs/2202.06041v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Ultra-fine Entity Typing with Indirect Supervision from Natural Language\n  Inference", "abstract": "The task of ultra-fine entity typing (UFET) seeks to predict diverse and\nfree-form words or phrases that describe the appropriate types of entities\nmentioned in sentences. A key challenge for this task lies in the large amount\nof types and the scarcity of annotated data per type. Existing systems\nformulate the task as a multi-way classification problem and train directly or\ndistantly supervised classifiers. This causes two issues: (i) the classifiers\ndo not capture the type semantics since types are often converted into indices;\n(ii) systems developed in this way are limited to predicting within a\npre-defined type set, and often fall short of generalizing to types that are\nrarely seen or unseen in training. This work presents LITE, a new approach that\nformulates entity typing as a natural language inference (NLI) problem, making\nuse of (i) the indirect supervision from NLI to infer type information\nmeaningfully represented as textual hypotheses and alleviate the data scarcity\nissue, as well as (ii) a learning-to-rank objective to avoid the pre-defining\nof a type set. Experiments show that, with limited training data, LITE obtains\nstate-of-the-art performance on the UFET task. In addition, LITE demonstrates\nits strong generalizability, by not only yielding best results on other\nfine-grained entity typing benchmarks, more importantly, a pre-trained LITE\nsystem works well on new data containing unseen types.", "published": "2022-02-12 23:56:26", "link": "http://arxiv.org/abs/2202.06167v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Double-Barreled Question Detection at Momentive", "abstract": "Momentive offers solutions in market research, customer experience, and\nenterprise feedback. The technology is gleaned from the billions of real\nresponses to questions asked on the platform. However, people may create biased\nquestions. A double-barreled question (DBQ) is a common type of biased question\nthat asks two aspects in one question. For example, \"Do you agree with the\nstatement: The food is yummy, and the service is great.\". This DBQ confuses\nsurvey respondents because there are two parts in a question. DBQs impact both\nthe survey respondents and the survey owners. Momentive aims to detect DBQs and\nrecommend survey creators to make a change towards gathering high quality\nunbiased survey data. Previous research work has suggested detecting DBQs by\nchecking the existence of grammatical conjunction. While this is a simple\nrule-based approach, this method is error-prone because conjunctions can also\nexist in properly constructed questions. We present an end-to-end machine\nlearning approach for DBQ classification in this work. We handled this\nimbalanced data using active learning, and compared state-of-the-art embedding\nalgorithms to transform text data into vectors. Furthermore, we proposed a\nmodel interpretation technique propagating the vector-level SHAP values to a\nSHAP value for each word in the questions. We concluded that the word2vec\nsubword embedding with maximum pooling is the optimal word embedding\nrepresentation in terms of precision and running time in the offline\nexperiments using the survey data at Momentive. The A/B test and production\nmetrics indicate that this model brings a positive change to the business. To\nthe best of our knowledge, this is the first machine learning framework for DBQ\ndetection, and it successfully differentiates Momentive from the competitors.\nWe hope our work sheds light on machine learning approaches for bias question\ndetection.", "published": "2022-02-12 00:04:24", "link": "http://arxiv.org/abs/2203.03545v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder", "abstract": "Improving end-to-end speech recognition by incorporating external text data\nhas been a longstanding research topic. There has been a recent focus on\ntraining E2E ASR models that get the performance benefits of external text data\nwithout incurring the extra cost of evaluating an external language model at\ninference time. In this work, we propose training ASR model jointly with a set\nof text-to-text auxiliary tasks with which it shares a decoder and parts of the\nencoder. When we jointly train ASR and masked language model with the 960-hour\nLibrispeech and Opensubtitles data respectively, we observe WER reductions of\n16% and 20% on test-other and test-clean respectively over an ASR-only baseline\nwithout any extra cost at inference time, and reductions of 6% and 8% compared\nto a stronger MUTE-L baseline which trains the decoder with the same text data\nas our model. We achieve further improvements when we train masked language\nmodel on Librispeech data or when we use machine translation as the auxiliary\ntask, without significantly sacrificing performance on the task itself.", "published": "2022-02-12 11:35:59", "link": "http://arxiv.org/abs/2202.06045v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Wav2Vec2.0 on the Edge: Performance Evaluation", "abstract": "Wav2Vec2.0 is a state-of-the-art model which learns speech representations\nthrough unlabeled speech data, aka, self supervised learning. The pretrained\nmodel is then fine tuned on small amounts of labeled data to use it for\nspeech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformative\nsolution for low resource languages as it is mainly developed using unlabeled\naudio data. Getting large amounts of labeled data is resource intensive and\nespecially challenging to do for low resource languages such as Swahilli,\nTatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpasses\nthe very recent supervised learning algorithms while using 100x less labeled\ndata. Given its importance and enormous potential in enabling speech based\ntasks on world's 7000 languages, it is key to evaluate the accuracy, latency\nand efficiency of this model on low resource and low power edge devices and\ninvestigate the feasibility of using it in such devices for private, secure and\nreliable speech based tasks. On-device speech tasks preclude sending audio data\nto the server hence inherently providing privacy, reduced latency and enhanced\nreliability. In this paper, Wav2Vec2.0 model's accuracy and latency has been\nevaluated on Raspberry Pi along with the KenLM language model for speech\nrecognition tasks. How to tune certain parameters to achieve desired level of\nWER rate and latency while meeting the CPU, memory and energy budgets of the\nproduct has been discussed.", "published": "2022-02-12 05:49:49", "link": "http://arxiv.org/abs/2202.05993v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Performer: Score-to-Audio Music Performance Synthesis", "abstract": "Music performance synthesis aims to synthesize a musical score into a natural\nperformance. In this paper, we borrow recent advances in text-to-speech\nsynthesis and present the Deep Performer -- a novel system for score-to-audio\nmusic performance synthesis. Unlike speech, music often contains polyphony and\nlong notes. Hence, we propose two new techniques for handling polyphonic inputs\nand providing a fine-grained conditioning in a transformer encoder-decoder\nmodel. To train our proposed system, we present a new violin dataset consisting\nof paired recordings and scores along with estimated alignments between them.\nWe show that our proposed model can synthesize music with clear polyphony and\nharmonic structures. In a listening test, we achieve competitive quality\nagainst the baseline model, a conditional generative audio model, in terms of\npitch accuracy, timbre and noise level. Moreover, our proposed model\nsignificantly outperforms the baseline on an existing piano dataset in overall\nquality.", "published": "2022-02-12 10:36:52", "link": "http://arxiv.org/abs/2202.06034v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
