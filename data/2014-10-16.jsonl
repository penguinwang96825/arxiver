{"title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks\n  for Large Vocabulary Speech Recognition", "abstract": "Long short-term memory (LSTM) based acoustic modeling methods have recently\nbeen shown to give state-of-the-art performance on some speech recognition\ntasks. To achieve a further performance improvement, in this research, deep\nextensions on LSTM are investigated considering that deep hierarchical model\nhas turned out to be more efficient than a shallow one. Motivated by previous\nresearch on constructing deep recurrent neural networks (RNNs), alternative\ndeep LSTM architectures are proposed and empirically evaluated on a large\nvocabulary conversational telephone speech recognition task. Meanwhile,\nregarding to multi-GPU devices, the training process for LSTM networks is\nintroduced and discussed. Experimental results demonstrate that the deep LSTM\nnetworks benefit from the depth and yield the state-of-the-art performance on\nthis task.", "published": "2014-10-16 02:44:41", "link": "http://arxiv.org/abs/1410.4281v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Patterns in the English Language: Phonological Networks, Percolation and\n  Assembly Models", "abstract": "In this paper we provide a quantitative framework for the study of\nphonological networks (PNs) for the English language by carrying out principled\ncomparisons to null models, either based on site percolation, randomization\ntechniques, or network growth models. In contrast to previous work, we mainly\nfocus on null models that reproduce lower order characteristics of the\nempirical data. We find that artificial networks matching connectivity\nproperties of the English PN are exceedingly rare: this leads to the hypothesis\nthat the word repertoire might have been assembled over time by preferentially\nintroducing new words which are small modifications of old words. Our null\nmodels are able to explain the \"power-law-like\" part of the degree\ndistributions and generally retrieve qualitative features of the PN such as\nhigh clustering, high assortativity coefficient, and small-world\ncharacteristics. However, the detailed comparison to expectations from null\nmodels also points out significant differences, suggesting the presence of\nadditional constraints in word assembly. Key constraints we identify are the\navoidance of large degrees, the avoidance of triadic closure, and the avoidance\nof large non-percolating clusters.", "published": "2014-10-16 14:25:01", "link": "http://arxiv.org/abs/1410.4445v3", "categories": ["cs.CL", "cond-mat.stat-mech"], "primary_category": "cs.CL"}
{"title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "abstract": "Originally designed to model text, topic modeling has become a powerful tool\nfor uncovering latent structure in domains including medicine, finance, and\nvision. The goals for the model vary depending on the application: in some\ncases, the discovered topics may be used for prediction or some other\ndownstream task. In other cases, the content of the topic itself may be of\nintrinsic scientific interest.\n  Unfortunately, even using modern sparse techniques, the discovered topics are\noften difficult to interpret due to the high dimensionality of the underlying\nspace. To improve topic interpretability, we introduce Graph-Sparse LDA, a\nhierarchical topic model that leverages knowledge of relationships between\nwords (e.g., as encoded by an ontology). In our model, topics are summarized by\na few latent concept-words from the underlying graph that explain the observed\nwords. Graph-Sparse LDA recovers sparse, interpretable summaries on two\nreal-world biomedical datasets while matching state-of-the-art prediction\nperformance.", "published": "2014-10-16 17:35:31", "link": "http://arxiv.org/abs/1410.4510v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
