{"title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix\n  Language Model", "abstract": "We describe Sparse Non-negative Matrix (SNM) language model estimation using\nmultinomial loss on held-out data.\n  Being able to train on held-out data is important in practical situations\nwhere the training data is usually mismatched from the held-out/test data. It\nis also less constrained than the previous training algorithm using\nleave-one-out on training data: it allows the use of richer meta-features in\nthe adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing\nwhich would be difficult to deal with correctly in leave-one-out training.\n  In experiments on the one billion words language modeling benchmark, we are\nable to slightly improve on our previous results which use a different loss\nfunction, and employ leave-one-out training on a subset of the main training\nset. Surprisingly, an adjustment model with meta-features that discard all\nlexical information can perform as well as lexicalized meta-features. We find\nthat fairly small amounts of held-out data (on the order of 30-70 thousand\nwords) are sufficient for training the adjustment model.\n  In a real-life scenario where the training data is a mix of data sources that\nare imbalanced in size, and of different degrees of relevance to the held-out\nand test data, taking into account the data source for a given skip-/n-gram\nfeature and combining them for best performance on held-out/test data improves\nover skip-/n-gram SNM models trained on pooled data by about 8% in the SMT\nsetup, or as much as 15% in the ASR/IME setup.\n  The ability to mix various data sources based on how relevant they are to a\nmismatched held-out set is probably the most attractive feature of the new\nestimation method for SNM LM.", "published": "2015-11-05 01:45:29", "link": "http://arxiv.org/abs/1511.01574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Sentiment Classification of Chinese Review using\n  Word Embedding", "abstract": "In this article, how word embeddings can be used as features in Chinese\nsentiment classification is presented. Firstly, a Chinese opinion corpus is\nbuilt with a million comments from hotel review websites. Then the word\nembeddings which represent each comment are used as input in different machine\nlearning methods for sentiment classification, including SVM, Logistic\nRegression, Convolutional Neural Network (CNN) and ensemble methods. These\nmethods get better performance compared with N-gram models using Naive Bayes\n(NB) and Maximum Entropy (ME). Finally, a combination of machine learning\nmethods is proposed which presents an outstanding performance in precision,\nrecall and F1 score. After selecting the most useful methods to construct the\ncombinational model and testing over the corpus, the final F1 score is 0.920.", "published": "2015-11-05 09:25:21", "link": "http://arxiv.org/abs/1511.01665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping", "abstract": "The development of plot or story in novels is reflected in the content and\nthe words used. The flow of sentiments, which is one aspect of writing style,\ncan be quantified by analyzing the flow of words. This study explores literary\nworks as signals in word embedding space and tries to compare writing styles of\npopular classic novels using dynamic time warping.", "published": "2015-11-05 09:25:41", "link": "http://arxiv.org/abs/1511.01666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Pale as death\" or \"p\u00e2le comme la mort\" : Frozen similes used as\n  literary clich\u00e9s", "abstract": "The present study is focused on the automatic identification and description\nof frozen similes in British and French novels written between the 19 th\ncentury and the beginning of the 20 th century. Two main patterns of frozen\nsimiles were considered: adjectival ground + simile marker + nominal vehicle\n(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.\nsleep like a top). All potential similes and their components were first\nextracted using a rule-based algorithm. Then, frozen similes were identified\nbased on reference lists of existing similes and semantic distance between the\ntenor and the vehicle. The results obtained tend to confirm the fact that\nfrozen similes are not used haphazardly in literary texts. In addition,\ncontrary to how they are often presented, frozen similes often go beyond the\nground or the eventuality and the vehicle to also include the tenor.", "published": "2015-11-05 14:20:01", "link": "http://arxiv.org/abs/1511.01756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Color Aesthetics and Social Networks in Complete Tang Poems:\n  Explorations and Discoveries", "abstract": "The Complete Tang Poems (CTP) is the most important source to study Tang\npoems. We look into CTP with computational tools from specific linguistic\nperspectives, including distributional semantics and collocational analysis.\nFrom such quantitative viewpoints, we compare the usage of \"wind\" and \"moon\" in\nthe poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,\nand play a crucial role in the imageries of poems. Thus, words for colors are\nstudied, and \"white\" is the main focus because it is the most frequent color in\nCTP. We also explore some cases of using colored words in antithesis pairs that\nwere central for fostering the imageries of the poems. CTP also contains useful\nhistorical information, and we extract person names in CTP to study the social\nnetworks of the Tang poets. Such information can then be integrated with the\nChina Biographical Database of Harvard University.", "published": "2015-11-05 00:21:40", "link": "http://arxiv.org/abs/1511.01559v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
