{"title": "Price impact and long-term profitability of energy storage", "abstract": "We study the price impact of storage facilities in electricity markets and\nanalyze the long-term profitability of these facilities in prospective\nscenarios of energy transition. To this end, we begin by characterizing the\noptimal operating strategy for a stylized storage system, assuming an arbitrary\nexogenous price process. Following this, we determine the equilibrium price in\na market comprising storage systems (acting as price takers), renewable energy\nproducers, and conventional producers with a defined supply function, all\ndriven by an exogenous demand process. The price process is characterized as a\nsolution to a fully coupled system of forward-backward stochastic differential\nequations, for which we establish existence and uniqueness under appropriate\nassumptions. We finally illustrate the impact of storage on intraday\nelectricity prices through numerical examples and show how the revenues of\nstorage agents may evolve in prospective energy transition scenarios from RTE,\nthe French energy electricity network operator, taking into account both the\nincreasing penetration of renewable energies and the self-cannibalization\neffect of growing storage capacity. We find that both the average revenues and\nthe interquantile ranges increase in all scenarios, highlighting higher\nexpected profits and higher risk for storage assets.", "published": "2024-10-16 12:16:22", "link": "http://arxiv.org/abs/2410.12495v1", "categories": ["q-fin.MF", "91A80, 91A06"], "primary_category": "q-fin.MF"}
{"title": "TradExpert: Revolutionizing Trading with Mixture of Expert LLMs", "abstract": "The integration of Artificial Intelligence (AI) in the financial domain has\nopened new avenues for quantitative trading, particularly through the use of\nLarge Language Models (LLMs). However, the challenge of effectively\nsynthesizing insights from diverse data sources and integrating both structured\nand unstructured data persists. This paper presents TradeExpert, a novel\nframework that employs a mix of experts (MoE) approach, using four specialized\nLLMs, each analyzing distinct sources of financial data, including news\narticles, market data, alpha factors, and fundamental data. The insights of\nthese expert LLMs are further synthesized by a General Expert LLM to make a\nfinal prediction or decision. With specific prompts, TradeExpert can be\nswitched between the prediction mode and the ranking mode for stock movement\nprediction and quantitative stock trading, respectively. In addition to\nexisting benchmarks, we also release a large-scale financial dataset to\ncomprehensively evaluate TradeExpert's effectiveness. Our experimental results\ndemonstrate TradeExpert's superior performance across all trading scenarios.", "published": "2024-10-16 20:24:16", "link": "http://arxiv.org/abs/2411.00782v1", "categories": ["cs.AI", "q-fin.ST"], "primary_category": "cs.AI"}
{"title": "Exploring Large Language Models for Hate Speech Detection in Rioplatense\n  Spanish", "abstract": "Hate speech detection deals with many language variants, slang, slurs,\nexpression modalities, and cultural nuances. This outlines the importance of\nworking with specific corpora, when addressing hate speech within the scope of\nNatural Language Processing, recently revolutionized by the irruption of Large\nLanguage Models. This work presents a brief analysis of the performance of\nlarge language models in the detection of Hate Speech for Rioplatense Spanish.\nWe performed classification experiments leveraging chain-of-thought reasoning\nwith ChatGPT 3.5, Mixtral, and Aya, comparing their results with those of a\nstate-of-the-art BERT classifier. These experiments outline that, even if large\nlanguage models show a lower precision compared to the fine-tuned BERT\nclassifier and, in some cases, they find hard-to-get slurs or colloquialisms,\nthey still are sensitive to highly nuanced cases (particularly,\nhomophobic/transphobic hate speech). We make our code and models publicly\navailable for future research.", "published": "2024-10-16 02:32:12", "link": "http://arxiv.org/abs/2410.12174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negative-Prompt-driven Alignment for Generative Language Model", "abstract": "Large language models have achieved remarkable capabilities, but aligning\ntheir outputs with human values and preferences remains a significant\nchallenge. Existing alignment methods primarily focus on positive examples\nwhile overlooking the importance of negative responses in guiding models away\nfrom undesirable behaviors. For instance, the widely-used alignment datasets\nreveals a scarcity of explicit negative examples that contradict human values,\nhindering its ability to discourage harmful or biased outputs during training.\nTo address this limitation, we propose NEAT, i.e., NEgative-prompt-driven\nAlignmenT, to introduce negative prompts to generate undesirable responses\nalongside positive examples during the optimization process. NEAT explicitly\npenalizes the model for producing harmful outputs, guiding it not only toward\ndesirable behaviors but also steering it away from generating undesirable,\nbiased responses. This dual feedback mechanism enables better alignment with\nhuman preferences, crucial in contexts where avoiding harm is paramount.\nStarting from a pre-trained language model, NEAT performs online alignment by\nincorporating a ranking loss derived from an expanded preference dataset\ncontaining both positive and negative examples. Extensive experiments validate\nNEAT's effectiveness in significantly enhancing language models' alignment with\nhuman values and preferences.", "published": "2024-10-16 03:30:09", "link": "http://arxiv.org/abs/2410.12194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree", "abstract": "When annotators disagree, predicting the labels given by individual\nannotators can capture nuances overlooked by traditional label aggregation. We\nintroduce three approaches to predicting individual annotator ratings on the\ntoxicity of text by incorporating individual annotator-specific information: a\nneural collaborative filtering (NCF) approach, an in-context learning (ICL)\napproach, and an intermediate embedding-based architecture. We also study the\nutility of demographic information for rating prediction. NCF showed limited\nutility; however, integrating annotator history, demographics, and survey\ninformation permits both the embedding-based architecture and ICL to\nsubstantially improve prediction accuracy, with the embedding-based\narchitecture outperforming the other methods. We also find that, if\ndemographics are predicted from survey information, using these imputed\ndemographics as features performs comparably to using true demographic data.\nThis suggests that demographics may not provide substantial information for\nmodeling ratings beyond what is captured in survey responses. Our findings\nraise considerations about the relative utility of different types of annotator\ninformation and provide new approaches for modeling annotators in subjective\nNLP tasks.", "published": "2024-10-16 04:26:40", "link": "http://arxiv.org/abs/2410.12217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for\n  Retrieval-Augmented Generation with Enhanced Data Diversity", "abstract": "Retrieval-Augmented Generation (RAG) aims to enhance large language models\n(LLMs) to generate more accurate and reliable answers with the help of the\nretrieved context from external knowledge sources, thereby reducing the\nincidence of hallucinations. Despite the advancements, evaluating these systems\nremains a crucial research area due to the following issues: (1) Limited data\ndiversity: The insufficient diversity of knowledge sources and query types\nconstrains the applicability of RAG systems; (2) Obscure problems location:\nExisting evaluation methods have difficulty in locating the stage of the RAG\npipeline where problems occur; (3) Unstable retrieval evaluation: These methods\noften fail to effectively assess retrieval performance, particularly when the\nchunking strategy changes. To tackle these challenges, we propose a\nComprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough\nevaluation across the entire RAG pipeline, including chunking, retrieval,\nreranking, and generation. To effectively evaluate the first three phases, we\nintroduce multi-granularity keywords, including coarse-grained and fine-grained\nkeywords, to assess the retrieved context instead of relying on the annotation\nof golden chunks. Moreover, we release a holistic benchmark dataset tailored\nfor diverse data scenarios covering a wide range of document formats and query\ntypes. We demonstrate the utility of the CoFE-RAG framework by conducting\nexperiments to evaluate each stage of RAG systems. Our evaluation method\nprovides unique insights into the effectiveness of RAG systems in handling\ndiverse data scenarios, offering a more nuanced understanding of their\ncapabilities and limitations.", "published": "2024-10-16 05:20:32", "link": "http://arxiv.org/abs/2410.12248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic and Cost-Efficient Peer-Review Framework for Language\n  Generation Evaluation", "abstract": "With the rapid development of large language models (LLMs), how to\nefficiently evaluate them has become an important research question. Existing\nevaluation methods often suffer from high costs, limited test formats, the need\nof human references, and systematic evaluation biases. To address these\nlimitations, our study introduces the Auto-PRE, an automatic LLM evaluation\nframework based on peer review. In contrast to previous studies that rely on\nhuman annotations, Auto-PRE selects evaluator LLMs automatically based on their\ninherent traits including consistency, self-confidence, and pertinence. We\nconduct extensive experiments on three tasks: summary generation, non-factoid\nquestion-answering, and dialogue generation. Experimental results indicate our\nAuto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our\nstudy highlights the impact of prompt strategies and evaluation formats on\nevaluation performance, offering guidance for method optimization in the\nfuture.", "published": "2024-10-16 06:06:06", "link": "http://arxiv.org/abs/2410.12265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much do contextualized representations encode long-range context?", "abstract": "We analyze contextual representations in neural autoregressive language\nmodels, emphasizing long-range contexts that span several thousand tokens. Our\nmethodology employs a perturbation setup and the metric\n\\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of\ncontextualization of long-range patterns from the perspective of representation\ngeometry. We begin the analysis with a case study on standard decoder-only\nTransformers, demonstrating that similar perplexity can exhibit markedly\ndifferent downstream task performance, which can be explained by the difference\nin contextualization of long-range content. Next, we extend the analysis to\nother models, covering recent novel architectural designs and various training\nconfigurations. The representation-level results illustrate a reduced capacity\nfor high-complexity (i.e., less compressible) sequences across architectures,\nand that fully recurrent models rely heavily on local context, whereas hybrid\nmodels more effectively encode the entire sequence structure. Finally,\npreliminary analysis of model size and training configurations on the encoding\nof long-range context suggest potential directions for improving existing\nlanguage models.", "published": "2024-10-16 06:49:54", "link": "http://arxiv.org/abs/2410.12292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering\n  Vectors", "abstract": "Large language models (LLMs) have achieved remarkable performance across many\ntasks, yet aligning them with desired behaviors remains challenging. Activation\nintervention has emerged as an effective and economical method to modify the\nbehavior of LLMs. Despite considerable interest in this area, current\nintervention methods exclusively employ a fixed steering vector to modify model\nactivations, lacking adaptability to diverse input semantics. To address this\nlimitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel\nmethod that constructs a dynamic steering vector to intervene model activations\nat inference time. More specifically, SADI utilizes activation differences in\ncontrastive pairs to precisely identify critical elements of an LLM (i.e.,\nattention heads, hidden states, and neurons) for targeted intervention. During\ninference, SADI dynamically steers model behavior by scaling element-wise\nactivations based on the directions of input semantics. Experimental results\nshow that SADI outperforms established baselines by substantial margins,\nimproving task performance without training. SADI's cost-effectiveness and\ngeneralizability across various LLM backbones and tasks highlight its potential\nas a versatile alignment technique.", "published": "2024-10-16 06:58:49", "link": "http://arxiv.org/abs/2410.12299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Low-Resource Language Model Training: Comprehensive Analysis\n  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches", "abstract": "In this paper, we address the challenge of optimizing training setups for\nLarge Language Models (LLMs) of low-resource language with a limited amount of\ncorpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training\nto utilize the limited target language corpus efficiently. However, there is\nstill a lack of understanding about the optimal hyperparameter setups for\ncombining these three approaches to train LLMs. We exhaustively explore\ntraining setups for low-resource language LLM, combining these three\napproaches, and found the following insights for efficiently reducing the cost\nof hyperparameter search: (1) As the amount of target language corpus\ndecreases, the optimal training approach shifts from monolingual single-stage\ntraining to multi-lingual two-stage training at a compute budget dependent\nthreshold. (2) The optimal model scale remains stable regardless of the amount\nof target language corpus, allowing the use of the compute-optimal scale of\nmonolingual training. (3) The optimal number of epochs can be extrapolated from\nsmaller-scale experiments to larger scale using our proposed model. Also, we\nprovide evidence that, in single-stage training, the target language validation\nloss follows a power law with respect to the target language ratio, with an\nexponent independent of the amount of data, model scale, and language pair.", "published": "2024-10-16 07:45:56", "link": "http://arxiv.org/abs/2410.12325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuron-based Personality Trait Induction in Large Language Models", "abstract": "Large language models (LLMs) have become increasingly proficient at\nsimulating various personality traits, an important capability for supporting\nrelated applications (e.g., role-playing). To further improve this capacity, in\nthis paper, we present a neuron-based approach for personality trait induction\nin LLMs, with three major technical contributions. First, we construct\nPersonalityBench, a large-scale dataset for identifying and evaluating\npersonality traits in LLMs. This dataset is grounded in the Big Five\npersonality traits from psychology and is designed to assess the generative\ncapabilities of LLMs towards specific personality traits. Second, by leveraging\nPersonalityBench, we propose an efficient method for identifying\npersonality-related neurons within LLMs by examining the opposite aspects of a\ngiven trait. Third, we develop a simple yet effective induction method that\nmanipulates the values of these identified personality-related neurons. This\nmethod enables fine-grained control over the traits exhibited by LLMs without\ntraining and modifying model parameters. Extensive experiments validate the\nefficacy of our neuron identification and trait induction methods. Notably, our\napproach achieves comparable performance as fine-tuned models, offering a more\nefficient and flexible solution for personality trait induction in LLMs. We\nprovide access to all the mentioned resources at\nhttps://github.com/RUCAIBox/NPTI.", "published": "2024-10-16 07:47:45", "link": "http://arxiv.org/abs/2410.12327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models", "abstract": "Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.", "published": "2024-10-16 08:55:49", "link": "http://arxiv.org/abs/2410.12380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Compression for Large Language Models: A Survey", "abstract": "Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.", "published": "2024-10-16 09:13:23", "link": "http://arxiv.org/abs/2410.12388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .", "published": "2024-10-16 09:38:13", "link": "http://arxiv.org/abs/2410.12405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nominal Class Assignment in Swahili: A Computational Account", "abstract": "We discuss the open question of the relation between semantics and nominal\nclass assignment in Swahili. We approach the problem from a computational\nperspective, aiming first to quantify the extent of this relation, and then to\nexplicate its nature, taking extra care to suppress morphosyntactic confounds.\nOur results are the first of their kind, providing a quantitative evaluation of\nthe semantic cohesion of each nominal class, as well as a nuanced taxonomic\ndescription of its semantic content.", "published": "2024-10-16 09:41:48", "link": "http://arxiv.org/abs/2410.12406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical Analysis of Hierarchical Language Recognition and Generation\n  by Transformers without Positional Encoding", "abstract": "In this study, we provide constructive proof that Transformers can recognize\nand generate hierarchical language efficiently with respect to model size, even\nwithout the need for a specific positional encoding. Specifically, we show that\ncausal masking and a starting token enable Transformers to compute positional\ninformation and depth within hierarchical structures. We demonstrate that\nTransformers without positional encoding can generate hierarchical languages.\nFurthermore, we suggest that explicit positional encoding might have a\ndetrimental effect on generalization with respect to sequence length.", "published": "2024-10-16 09:56:01", "link": "http://arxiv.org/abs/2410.12413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models", "abstract": "Service chatbots play an important role in enhancing customer support by\ndelivering timely responses to diverse queries. Traditionally, these chatbots\nrely on retrieval-based methods constrained by a predefined knowledge base of\nquestion-answer (QA) pairs to guarantee reliable responses. To effectively\nhandle varied customer inquiries, augmenting the knowledge base with similar\nquestions that maintain semantic consistency and linguistic variability is\ncrucial. This paper presents methodologies for a novel approach that utilizes\nLarge Language Models (LLMs) for generating similar questions and selecting an\noptimal subset of questions for knowledge base augmentation in industrial\nchatbots. Specifically, we define the SQG task in the context of LLM training\nand propose a one-to-many objective that incorporates contextual information.\nWe also introduce an optimization framework that selects a diverse subset of\nsimilar questions within predefined resource constraints. Experimental results\ndemonstrate significant improvements over traditional methods, achieving\ngreater semantic diversity while aligning with source QA pairs, with over 120%\nrelative improvement in meeting business-specific requirements with human\nevaluation. Combined with several best practices, we provide a robust,\napplication-driven solution for enhancing chatbot performance and improving\ncustomer service satisfaction.", "published": "2024-10-16 10:48:14", "link": "http://arxiv.org/abs/2410.12444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph", "abstract": "The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.", "published": "2024-10-16 11:16:34", "link": "http://arxiv.org/abs/2410.12458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.", "published": "2024-10-16 11:23:03", "link": "http://arxiv.org/abs/2410.12462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels", "abstract": "Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.", "published": "2024-10-16 11:34:33", "link": "http://arxiv.org/abs/2410.12470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models", "abstract": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.", "published": "2024-10-16 11:46:55", "link": "http://arxiv.org/abs/2410.12478v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning", "abstract": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85\\% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.", "published": "2024-10-16 12:14:25", "link": "http://arxiv.org/abs/2410.12491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "With a Grain of SALT: Are LLMs Fair Across Social Dimensions?", "abstract": "This paper presents a systematic analysis of biases in open-source Large\nLanguage Models (LLMs), across gender, religion, and race. Our study evaluates\nbias in smaller-scale Llama and Gemma models using the SALT ($\\textbf{S}$ocial\n$\\textbf{A}$ppropriateness in $\\textbf{L}$LM-Generated $\\textbf{T}$ext)\ndataset, which incorporates five distinct bias triggers: General Debate,\nPositioned Debate, Career Advice, Problem Solving, and CV Generation. To\nquantify bias, we measure win rates in General Debate and the assignment of\nnegative roles in Positioned Debate. For real-world use cases, such as Career\nAdvice, Problem Solving, and CV Generation, we anonymize the outputs to remove\nexplicit demographic identifiers and use DeepSeek-R1 as an automated evaluator.\nWe also address inherent biases in LLM-based evaluation, including evaluation\nbias, positional bias, and length bias, and validate our results through human\nevaluations. Our findings reveal consistent polarization across models, with\ncertain demographic groups receiving systematically favorable or unfavorable\ntreatment. By introducing SALT, we provide a comprehensive benchmark for bias\nanalysis and underscore the need for robust bias mitigation strategies in the\ndevelopment of equitable AI systems.", "published": "2024-10-16 12:22:47", "link": "http://arxiv.org/abs/2410.12499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Fairness in Natural Language Processing: From Traditional\n  Methods to Explainability", "abstract": "The burgeoning field of Natural Language Processing (NLP) stands at a\ncritical juncture where the integration of fairness within its frameworks has\nbecome an imperative. This PhD thesis addresses the need for equity and\ntransparency in NLP systems, recognizing that fairness in NLP is not merely a\ntechnical challenge but a moral and ethical necessity, requiring a rigorous\nexamination of how these technologies interact with and impact diverse human\npopulations. Through this lens, this thesis undertakes a thorough investigation\ninto the development of equitable NLP methodologies and the evaluation of\nbiases that prevail in current systems.\n  First, it introduces an innovative algorithm to mitigate biases in\nmulti-class classifiers, tailored for high-risk NLP applications, surpassing\ntraditional methods in both bias mitigation and prediction accuracy. Then, an\nanalysis of the Bios dataset reveals the impact of dataset size on\ndiscriminatory biases and the limitations of standard fairness metrics. This\nawareness has led to explorations in the field of explainable AI, aiming for a\nmore complete understanding of biases where traditional metrics are limited.\nConsequently, the thesis presents COCKATIEL, a model-agnostic explainability\nmethod that identifies and ranks concepts in Transformer models, outperforming\nprevious approaches in sentiment analysis tasks. Finally, the thesis\ncontributes to bridging the gap between fairness and explainability by\nintroducing TaCo, a novel method to neutralize bias in Transformer model\nembeddings.\n  In conclusion, this thesis constitutes a significant interdisciplinary\nendeavor that intertwines explicability and fairness to challenge and reshape\ncurrent NLP paradigms. The methodologies and critiques presented contribute to\nthe ongoing discourse on fairness in machine learning, offering actionable\nsolutions for more equitable and responsible AI systems.", "published": "2024-10-16 12:38:58", "link": "http://arxiv.org/abs/2410.12511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction", "abstract": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.", "published": "2024-10-16 12:45:35", "link": "http://arxiv.org/abs/2410.12513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration", "abstract": "Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.", "published": "2024-10-16 13:10:27", "link": "http://arxiv.org/abs/2410.12532v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Make LLMs Forget: On Reversing In-Context Knowledge Edits", "abstract": "In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.", "published": "2024-10-16 14:04:26", "link": "http://arxiv.org/abs/2410.12586v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs", "abstract": "Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.", "published": "2024-10-16 14:17:53", "link": "http://arxiv.org/abs/2410.12600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization", "abstract": "To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.", "published": "2024-10-16 14:21:52", "link": "http://arxiv.org/abs/2410.12601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning", "abstract": "Large language models (LLMs) have shown increasing competence in solving\nmathematical reasoning problems. However, many open-source LLMs still struggle\nwith errors in calculation and semantic understanding during intermediate\nreasoning steps. In this work, we introduce Prove, a simple yet effective\nframework that leverages translated programs derived from natural language\nsolutions as a verification mechanism to filter out potentially incorrect\nreasoning paths before aggregating final answers. Unlike vanilla majority\nvoting, our approach filters out solutions whose corresponding program output\nis inconsistent with the generated solution, aggregating only those that pass\nverification. We conducted extensive experiments using 13 open-source LLMs from\nvarious model families and sizes, ranging from 0.5B to 13B parameters, across\neight mathematical benchmarks. Our results show that Prove consistently\noutperforms vanilla majority voting as a heuristic for solving mathematical\nreasoning tasks across all model sizes and datasets, achieving improvements of\nup to 18% on GSM8K and 8% on MATH-500. Our codes are available at\nhttps://github.com/declare-lab/prove.", "published": "2024-10-16 14:24:55", "link": "http://arxiv.org/abs/2410.12608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing Akkadian Verbs with Prolog", "abstract": "This paper describes a parsing/generation system for finite verbal forms in\nAkkadian, with the possible addition of suffixes, implemented in Prolog. The\nwork described provides the framework and engine to interpret the D, N, and G\nstems along with accusative, dative and ventive endings.", "published": "2024-10-16 14:34:30", "link": "http://arxiv.org/abs/2410.12617v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for\n  multimodal language models evaluation", "abstract": "Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.", "published": "2024-10-16 16:31:24", "link": "http://arxiv.org/abs/2410.12722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Extrinsic Factors for NER in French", "abstract": "Named entity recognition (NER) is a crucial task that aims to identify\nstructured information, which is often replete with complex, technical terms\nand a high degree of variability. Accurate and reliable NER can facilitate the\nextraction and analysis of important information. However, NER for other than\nEnglish is challenging due to limited data availability, as the high expertise,\ntime, and expenses are required to annotate its data. In this paper, by using\nthe limited data, we explore various factors including model structure, corpus\nannotation scheme and data augmentation techniques to improve the performance\nof a NER model for French. Our experiments demonstrate that these approaches\ncan significantly improve the model's F1 score from original CRF score of 62.41\nto 79.39. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance where the size of data is limited.", "published": "2024-10-16 17:12:06", "link": "http://arxiv.org/abs/2410.12750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception", "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking.", "published": "2024-10-16 17:59:32", "link": "http://arxiv.org/abs/2410.12788v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context is Key(NMF): Modelling Topical Information Dynamics in Chinese\n  Diaspora Media", "abstract": "Does the People's Republic of China (PRC) interfere with European elections\nthrough ethnic Chinese diaspora media? This question forms the basis of an\nongoing research project exploring how PRC narratives about European elections\nare represented in Chinese diaspora media, and thus the objectives of PRC news\nmedia manipulation. In order to study diaspora media efficiently and at scale,\nit is necessary to use techniques derived from quantitative text analysis, such\nas topic modelling. In this paper, we present a pipeline for studying\ninformation dynamics in Chinese media. Firstly, we present KeyNMF, a new\napproach to static and dynamic topic modelling using transformer-based\ncontextual embedding models. We provide benchmark evaluations to demonstrate\nthat our approach is competitive on a number of Chinese datasets and metrics.\nSecondly, we integrate KeyNMF with existing methods for describing information\ndynamics in complex systems. We apply this pipeline to data from five news\nsites, focusing on the period of time leading up to the 2024 European\nparliamentary elections. Our methods and results demonstrate the effectiveness\nof KeyNMF for studying information dynamics in Chinese media and lay groundwork\nfor further work addressing the broader research questions.", "published": "2024-10-16 17:59:52", "link": "http://arxiv.org/abs/2410.12791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Data Synthesis and Augmentation for Large Language Models", "abstract": "The success of Large Language Models (LLMs) is inherently linked to the\navailability of vast, diverse, and high-quality data for training and\nevaluation. However, the growth rate of high-quality data is significantly\noutpaced by the expansion of training datasets, leading to a looming data\nexhaustion crisis. This underscores the urgent need to enhance data efficiency\nand explore new data sources. In this context, synthetic data has emerged as a\npromising solution. Currently, data generation primarily consists of two major\napproaches: data augmentation and synthesis. This paper comprehensively reviews\nand summarizes data generation techniques throughout the lifecycle of LLMs,\nincluding data preparation, pre-training, fine-tuning, instruction-tuning,\npreference alignment, and applications. Furthermore, We discuss the current\nconstraints faced by these methods and investigate potential pathways for\nfuture development and research. Our aspiration is to equip researchers with a\nclear understanding of these methodologies, enabling them to swiftly identify\nappropriate data generation strategies in the construction of LLMs, while\nproviding valuable insights for future exploration.", "published": "2024-10-16 16:12:39", "link": "http://arxiv.org/abs/2410.12896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL\n  Translation", "abstract": "Text-to-SQL generation enables non-experts to interact with databases via\nnatural language. Recent advances rely on large closed-source models like GPT-4\nthat present challenges in accessibility, privacy, and latency. To address\nthese issues, we focus on developing small, efficient, and open-source\ntext-to-SQL models. We demonstrate the benefits of sampling multiple candidate\nSQL generations and propose our method, MSc-SQL, to critique them using\nassociated metadata. Our sample critiquing model evaluates multiple outputs\nsimultaneously, achieving state-of-the-art performance compared to other\nopen-source models while remaining competitive with larger models at a much\nlower cost. Full code can be found at https://github.com/layer6ai-labs/msc-sql.", "published": "2024-10-16 18:03:24", "link": "http://arxiv.org/abs/2410.12916v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting token compositionality in LLMs: A robustness analysis", "abstract": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges.", "published": "2024-10-16 18:10:50", "link": "http://arxiv.org/abs/2410.12924v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Mathematical Reasoning in LLMs by Stepwise Correction", "abstract": "Best-of-N decoding methods instruct large language models (LLMs) to generate\nmultiple solutions, score each using a scoring function, and select the highest\nscored as the final answer to mathematical reasoning problems. However, this\nrepeated independent process often leads to the same mistakes, making the\nselected solution still incorrect. We propose a novel prompting method named\nStepwise Correction (StepCo) that helps LLMs identify and revise incorrect\nsteps in their generated reasoning paths. It iterates verification and revision\nphases that employ a process-supervised verifier. The verify-then-revise\nprocess not only improves answer correctness but also reduces token consumption\nwith fewer paths needed to generate. With StepCo, a series of LLMs demonstrate\nexceptional performance. Notably, using GPT-4o as the backend LLM, StepCo\nachieves an average accuracy of 94.1 across eight datasets, significantly\noutperforming the state-of-the-art Best-of-N method by +2.4, while reducing\ntoken consumption by 77.8%.", "published": "2024-10-16 18:18:42", "link": "http://arxiv.org/abs/2410.12934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Multi-turn Function Calling for LLMs via Compositional\n  Instruction Tuning", "abstract": "Large Language Models (LLMs) have exhibited significant potential in\nperforming diverse tasks, including the ability to call functions or use\nexternal tools to enhance their performance. While current research on function\ncalling by LLMs primarily focuses on single-turn interactions, this paper\naddresses the overlooked necessity for LLMs to engage in multi-turn function\ncalling--critical for handling compositional, real-world queries that require\nplanning with functions but not only use functions. To facilitate this, we\nintroduce an approach, BUTTON, which generates synthetic compositional\ninstruction tuning data via bottom-up instruction construction and top-down\ntrajectory generation. In the bottom-up phase, we generate simple atomic tasks\nbased on real-world scenarios and build compositional tasks using heuristic\nstrategies based on atomic tasks. Corresponding function definitions are then\nsynthesized for these compositional tasks. The top-down phase features a\nmulti-agent environment where interactions among simulated humans, assistants,\nand tools are utilized to gather multi-turn function calling trajectories. This\napproach ensures task compositionality and allows for effective function and\ntrajectory generation by examining atomic tasks within compositional tasks. We\nproduce a dataset BUTTONInstruct comprising 8k data points and demonstrate its\neffectiveness through extensive experiments across various LLMs.", "published": "2024-10-16 18:40:26", "link": "http://arxiv.org/abs/2410.12952v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Pluralising Culture Alignment for Large Language Models", "abstract": "As large language models (LLMs) become increasingly accessible in many\ncountries, it is essential to align them to serve pluralistic human values\nacross cultures. However, pluralistic culture alignment in LLMs remain an open\nproblem. In this paper, we propose CultureSPA, a Self-Pluralising Culture\nAlignment framework that allows LLMs to simultaneously align to pluralistic\ncultures. The framework first generates questions on various culture topics,\nthen yields LLM outputs in response to these generated questions under both\nculture-aware and culture-unaware settings. By comparing culture-aware/unaware\noutputs, we are able to detect and collect culture-related instances. These\ninstances are employed to fine-tune LLMs to serve pluralistic cultures in\neither a culture-joint or culture-specific way. Extensive experiments\ndemonstrate that CultureSPA significantly improves the alignment of LLMs to\ndiverse cultures without compromising general abilities. And further\nimprovements can be achieved if CultureSPA is combined with advanced prompt\nengineering techniques. Comparisons between culture-joint and culture-specific\ntuning strategies, along with variations in data quality and quantity,\nillustrate the robustness of our method. We also explore the mechanisms\nunderlying CultureSPA and the relations between different cultures it reflects.", "published": "2024-10-16 19:06:08", "link": "http://arxiv.org/abs/2410.12971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks", "abstract": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers).", "published": "2024-10-16 19:07:37", "link": "http://arxiv.org/abs/2410.12972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BenchmarkCards: Large Language Model and Risk Reporting", "abstract": "Large language models (LLMs) offer powerful capabilities but also introduce\nsignificant risks. One way to mitigate these risks is through comprehensive\npre-deployment evaluations using benchmarks designed to test for specific\nvulnerabilities. However, the rapidly expanding body of LLM benchmark\nliterature lacks a standardized method for documenting crucial benchmark\ndetails, hindering consistent use and informed selection. BenchmarkCards\naddresses this gap by providing a structured framework specifically for\ndocumenting LLM benchmark properties rather than defining the entire evaluation\nprocess itself. BenchmarkCards do not prescribe how to measure or interpret\nbenchmark results (e.g., defining ``correctness'') but instead offer a\nstandardized way to capture and report critical characteristics like targeted\nrisks and evaluation methodologies, including properties such as bias and\nfairness. This structured metadata facilitates informed benchmark selection,\nenabling researchers to choose appropriate benchmarks and promoting\ntransparency and reproducibility in LLM evaluation.", "published": "2024-10-16 19:09:02", "link": "http://arxiv.org/abs/2410.12974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging LLMs for Translating and Classifying Mental Health Data", "abstract": "Large language models (LLMs) are increasingly used in medical fields. In\nmental health support, the early identification of linguistic markers\nassociated with mental health conditions can provide valuable support to mental\nhealth professionals, and reduce long waiting times for patients. Despite the\nbenefits of LLMs for mental health support, there is limited research on their\napplication in mental health systems for languages other than English. Our\nstudy addresses this gap by focusing on the detection of depression severity in\nGreek through user-generated posts which are automatically translated from\nEnglish. Our results show that GPT3.5-turbo is not very successful in\nidentifying the severity of depression in English, and it has a varying\nperformance in Greek as well. Our study underscores the necessity for further\nresearch, especially in languages with less resources. Also, careful\nimplementation is necessary to ensure that LLMs are used effectively in mental\nhealth platforms, and human supervision remains crucial to avoid misdiagnosis.", "published": "2024-10-16 19:30:11", "link": "http://arxiv.org/abs/2410.12985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Let's Argue Both Sides\": Argument Generation Can Force Small Models to\n  Utilize Previously Inaccessible Reasoning Capabilities", "abstract": "Large Language Models (LLMs), despite achieving state-of-the-art results in a\nnumber of evaluation tasks, struggle to maintain their performance when logical\nreasoning is strictly required to correctly infer a prediction. In this work,\nwe propose Argument Generation as a method of forcing models to utilize their\nreasoning capabilities when other approaches such as chain-of-thought reasoning\nprove insufficient. Our method involves the generation of arguments for each\npossible inference result, and asking the end model to rank the generated\narguments. We show that Argument Generation can serve as an appropriate\nsubstitute for zero-shot prompting techniques without the requirement to add\nlayers of complexity. Furthermore, we argue that knowledge-probing techniques\nsuch as chain-of-thought reasoning and Argument Generation are only useful when\nfurther reasoning is required to infer a prediction, making them auxiliary to\nmore common zero-shot approaches. Finally, we demonstrate that our approach\nforces larger gains in smaller language models, showcasing a complex\nrelationship between model size and prompting methods in foundation models.", "published": "2024-10-16 19:49:30", "link": "http://arxiv.org/abs/2410.12997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "POROver: Improving Safety and Reducing Overrefusal in Large Language\n  Models with Overgeneration and Preference Optimization", "abstract": "Balancing safety and usefulness in large language models has become a\ncritical challenge in recent years. Models often exhibit unsafe behavior or\nadopt an overly cautious approach, leading to frequent overrefusal of benign\nprompts, which reduces their usefulness. Addressing these issues requires\nmethods that maintain safety while avoiding overrefusal. In this work, we\nexamine how the overgeneration of training data using advanced teacher models\n(e.g., GPT-4o), including responses to both general-purpose and toxic prompts,\ninfluences the safety and overrefusal balance of instruction-following language\nmodels. Additionally, we present POROver, a strategy to use preference\noptimization methods in order to reduce overrefusal, via employing a superior\nteacher model's completions. Our results show that overgenerating completions\nfor general-purpose prompts significantly improves the balance between safety\nand usefulness. Specifically, the F1 score calculated between safety and\nusefulness increases from 70.8% to 88.3%. Moreover, overgeneration for toxic\nprompts substantially reduces overrefusal, decreasing it from 94.4% to 45.2%.\nFurthermore, preference optimization algorithms, when applied with carefully\ncurated preference data, can effectively reduce a model's overrefusal from\n45.2% to 15.0% while maintaining comparable safety levels. Our code and data\nare available at https://github.com/batuhankmkaraman/POROver.", "published": "2024-10-16 19:56:22", "link": "http://arxiv.org/abs/2410.12999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptExp: Multi-granularity Prompt Explanation of Large Language Models", "abstract": "Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nPromptExp , a framework for multi-granularity prompt explanations by\naggregating token-level insights. PromptExp introduces two token-level\nexplanation approaches: 1. an aggregation-based approach combining local\nexplanation techniques, and 2. a perturbation-based approach with novel\ntechniques to evaluate token masking impact. PromptExp supports both white-box\nand black-box explanations and extends explanations to higher granularity\nlevels, enabling flexible analysis. We evaluate PromptExp in case studies such\nas sentiment analysis, showing the perturbation-based approach performs best\nusing semantic similarity to assess perturbation impact. Furthermore, we\nconducted a user study to confirm PromptExp's accuracy and practical value, and\ndemonstrate its potential to enhance LLM interpretability.", "published": "2024-10-16 22:25:15", "link": "http://arxiv.org/abs/2410.13073v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with\n  Large Language Models", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning\nabilities, but they still struggle with faithful reasoning due to knowledge\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\nbeen utilized to enhance LLM reasoning through their structured knowledge.\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\nencounter difficulties in accurately retrieving knowledge and efficiently\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\n(GCR), a novel framework that bridges structured knowledge in KGs with\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\nreason on graphs and generate faithful reasoning paths grounded in KGs.\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\ngraph-constrained reasoning alongside a powerful general LLM for inductive\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\nzero-shot generalizability to unseen KGs without additional training.", "published": "2024-10-16 22:55:17", "link": "http://arxiv.org/abs/2410.13080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iter-AHMCL: Alleviate Hallucination for Large Language Model via\n  Iterative Model-level Contrastive Learning", "abstract": "The development of Large Language Models (LLMs) has significantly advanced\nvarious AI applications in commercial and scientific research fields, such as\nscientific literature summarization, writing assistance, and knowledge graph\nconstruction. However, a significant challenge is the high risk of\nhallucination during LLM inference, which can lead to security concerns like\nfactual inaccuracies, inconsistent information, and fabricated content. To\ntackle this issue, it is essential to develop effective methods for reducing\nhallucination while maintaining the original capabilities of the LLM. This\npaper introduces a novel approach called Iterative Model-level Contrastive\nLearning (Iter-AHMCL) to address hallucination. This method modifies the\nrepresentation layers of pre-trained LLMs by using contrastive `positive' and\n`negative' models, trained on data with and without hallucinations. By\nleveraging the differences between these two models, we create a more\nstraightforward pathway to eliminate hallucinations, and the iterative nature\nof contrastive learning further enhances performance. Experimental validation\non four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen)\nfinetuning with a specially designed dataset shows that our approach achieves\nan average improvement of 10.1 points on the TruthfulQA benchmark.\nComprehensive experiments demonstrate the effectiveness of Iter-AHMCL in\nreducing hallucination while maintaining the general capabilities of LLMs.", "published": "2024-10-16 00:15:40", "link": "http://arxiv.org/abs/2410.12130v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Preference Optimization with Multi-Sample Comparisons", "abstract": "Recent advancements in generative models, particularly large language models\n(LLMs) and diffusion models, have been driven by extensive pretraining on large\ndatasets followed by post-training. However, current post-training methods such\nas reinforcement learning from human feedback (RLHF) and direct alignment from\npreference methods (DAP) primarily utilize single-sample comparisons. These\napproaches often fail to capture critical characteristics such as generative\ndiversity and bias, which are more accurately assessed through multiple\nsamples. To address these limitations, we introduce a novel approach that\nextends post-training to include multi-sample comparisons. To achieve this, we\npropose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample\nIdentity Preference Optimization (mIPO). These methods improve traditional DAP\nmethods by focusing on group-wise characteristics. Empirically, we demonstrate\nthat multi-sample comparison is more effective in optimizing collective\ncharacteristics~(e.g., diversity and bias) for generative models than\nsingle-sample comparison. Additionally, our findings suggest that multi-sample\ncomparisons provide a more robust optimization framework, particularly for\ndataset with label noise.", "published": "2024-10-16 00:59:19", "link": "http://arxiv.org/abs/2410.12138v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Layer-of-Thoughts Prompting (LoT): Leveraging LLM-Based Retrieval with\n  Constraint Hierarchies", "abstract": "This paper presents a novel approach termed Layer-of-Thoughts Prompting\n(LoT), which utilizes constraint hierarchies to filter and refine candidate\nresponses to a given query. By integrating these constraints, our method\nenables a structured retrieval process that enhances explainability and\nautomation. Existing methods have explored various prompting techniques but\noften present overly generalized frameworks without delving into the nuances of\nprompts in multi-turn interactions. Our work addresses this gap by focusing on\nthe hierarchical relationships among prompts. We demonstrate that the efficacy\nof thought hierarchy plays a critical role in developing efficient and\ninterpretable retrieval algorithms. Leveraging Large Language Models (LLMs),\nLoT significantly improves the accuracy and comprehensibility of information\nretrieval tasks.", "published": "2024-10-16 01:20:44", "link": "http://arxiv.org/abs/2410.12153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting LLMs' Reasoning Capability to Infer Implicit Concepts in\n  Legal Information Retrieval", "abstract": "Statutory law retrieval is a typical problem in legal language processing,\nthat has various practical applications in law engineering. Modern deep\nlearning-based retrieval methods have achieved significant results for this\nproblem. However, retrieval systems relying on semantic and lexical\ncorrelations often exhibit limitations, particularly when handling queries that\ninvolve real-life scenarios, or use the vocabulary that is not specific to the\nlegal domain. In this work, we focus on overcoming this weaknesses by utilizing\nthe logical reasoning capabilities of large language models (LLMs) to identify\nrelevant legal terms and facts related to the situation mentioned in the query.\nThe proposed retrieval system integrates additional information from the\nterm--based expansion and query reformulation to improve the retrieval\naccuracy. The experiments on COLIEE 2022 and COLIEE 2023 datasets show that\nextra knowledge from LLMs helps to improve the retrieval result of both lexical\nand semantic ranking models. The final ensemble retrieval system outperformed\nthe highest results among all participating teams in the COLIEE 2022 and 2023\ncompetitions.", "published": "2024-10-16 01:34:14", "link": "http://arxiv.org/abs/2410.12154v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation", "abstract": "Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in poor data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubric template and used large language models (LLMs) to score the\ngeneration on quantifiable scales. We compared popular LLMs as well as widely\nadopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods for the generation of synthetic\nunfaithful data, as well as heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation of whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non the latency and cost of deploying such a system.", "published": "2024-10-16 04:36:17", "link": "http://arxiv.org/abs/2410.12222v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference", "abstract": "The Mixture-of-Experts (MoE) model has emerged as a prominent architecture in\nthe field of Large Language Models (LLMs), providing a better balance between\nmodel performance and computational efficiency. However the General Matrix\nMultiply (GEMM) operations and large parameters introduce challenges related to\ncomputational efficiency and communication overhead, which become throughput\nbottlenecks during inference. Applying a single parallelism strategy like EP,\nDP, TP or a straightforward combination of them to MoE usually achieves\nsub-optimal inference throughput. This paper introduces EPS-MoE, a novel expert\npipeline scheduler for MoE that surpasses the existing parallelism schemes. Our\napproach optimizes the computation of MoE FeedForward Network (FFN) modules by\ndynamically selecting the best kernel implementation of GroupGemm and DenseGemm\nfor different loads and adaptively overlapping these computations with\ncommunication, leading to a substantial increase in throughput. Our\nexperimental results demonstrate at most 52.4\\% improvement in prefill\nthroughput compared to existing parallel inference methods. Specifically, our\nmethod accelerated the highly optimized DeepSeekV2 model from a claimed 100K\ntokens per second to at least 120K tokens per second.", "published": "2024-10-16 05:17:49", "link": "http://arxiv.org/abs/2410.12247v2", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Kallini et al. (2024) do not compare impossible languages with\n  constituency-based ones", "abstract": "A central goal of linguistic theory is to find a precise characterization of\nthe notion \"possible human language\", in the form of a computational device\nthat is capable of describing all and only the languages that can be acquired\nby a typically developing human child. The success of recent large language\nmodels (LLMs) in NLP applications arguably raises the possibility that LLMs\nmight be computational devices that meet this goal. This would only be the case\nif, in addition to succeeding in learning human languages, LLMs struggle to\nlearn \"impossible\" human languages. Kallini et al. (2024; \"Mission: Impossible\nLanguage Models\", Proc. ACL) conducted experiments aiming to test this by\ntraining GPT-2 on a variety of synthetic languages, and found that it learns\nsome more successfully than others. They present these asymmetries as support\nfor the idea that LLMs' inductive biases align with what is regarded as\n\"possible\" for human languages, but the most significant comparison has a\nconfound that makes this conclusion unwarranted. In this paper I explain the\nconfound and suggest some ways forward towards constructing a comparison that\nappropriately tests the underlying issue.", "published": "2024-10-16 06:16:30", "link": "http://arxiv.org/abs/2410.12271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context\n  Reasoning", "abstract": "Extensive knowledge graphs (KGs) have been constructed to facilitate\nknowledge-driven tasks across various scenarios. However, existing work usually\ndevelops separate reasoning models for different KGs, lacking the ability to\ngeneralize and transfer knowledge across diverse KGs and reasoning settings. In\nthis paper, we propose a prompt-based KG foundation model via in-context\nlearning, namely KG-ICL, to achieve a universal reasoning ability.\nSpecifically, we introduce a prompt graph centered with a query-related example\nfact as context to understand the query relation. To encode prompt graphs with\nthe generalization ability to unseen entities and relations in queries, we\nfirst propose a unified tokenizer that maps entities and relations in prompt\ngraphs to predefined tokens. Then, we propose two message passing neural\nnetworks to perform prompt encoding and KG reasoning, respectively. We conduct\nevaluation on 43 different KGs in both transductive and inductive settings.\nResults indicate that the proposed KG-ICL outperforms baselines on most\ndatasets, showcasing its outstanding generalization and universal reasoning\ncapabilities. The source code is accessible on GitHub:\nhttps://github.com/nju-websoft/KG-ICL.", "published": "2024-10-16 06:47:18", "link": "http://arxiv.org/abs/2410.12288v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs", "abstract": "Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.", "published": "2024-10-16 06:57:18", "link": "http://arxiv.org/abs/2410.12298v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open Domain Question Answering with Conflicting Contexts", "abstract": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.", "published": "2024-10-16 07:24:28", "link": "http://arxiv.org/abs/2410.12311v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reversal of Thought: Enhancing Large Language Models with\n  Preference-Guided Reverse Reasoning Warm-up", "abstract": "Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a plug-and-play and cost-effective reasoning\nframework designed to enhance the logical reasoning abilities of LLMs during\nthe warm-up phase prior to batch inference. RoT utilizes a Preference-Guided\nReverse Reasoning warm-up strategy, which integrates logical symbols for\npseudocode planning through meta-cognitive mechanisms and pairwise preference\nself-evaluation to generate task-specific prompts solely through\ndemonstrations, aligning with LLMs' cognitive preferences shaped by RLHF.\nThrough reverse reasoning, we utilize a Cognitive Preference Manager to assess\nknowledge boundaries and further expand LLMs' reasoning capabilities by\naggregating solution logic for known tasks and stylistic templates for unknown\ntasks. Experiments across various tasks demonstrate that RoT surpasses existing\nbaselines in both reasoning accuracy and efficiency.", "published": "2024-10-16 07:44:28", "link": "http://arxiv.org/abs/2410.12323v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding the Role of LLMs in Multimodal Evaluation Benchmarks", "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has been\naccompanied by the development of various benchmarks to evaluate their\ncapabilities. However, the true nature of these evaluations and the extent to\nwhich they assess multimodal reasoning versus merely leveraging the underlying\nLarge Language Model (LLM) backbone remain unclear. This paper presents a\ncomprehensive investigation into the role of LLM backbones in MLLM evaluation,\nfocusing on two critical aspects: the degree to which current benchmarks truly\nassess multimodal reasoning and the influence of LLM prior knowledge on\nperformance. Specifically, we introduce a modified evaluation protocol to\ndisentangle the contributions of the LLM backbone from multimodal integration,\nand an automatic knowledge identification technique for diagnosing whether LLMs\nequip the necessary knowledge for corresponding multimodal questions. Our study\nencompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key\nfindings reveal that some benchmarks allow high performance even without visual\ninputs and up to 50\\% of error rates can be attributed to insufficient world\nknowledge in the LLM backbone, indicating a heavy reliance on language\ncapabilities. To address knowledge deficiencies, we propose a knowledge\naugmentation pipeline that achieves significant performance gains, with\nimprovements of up to 60\\% on certain datasets, resulting in a approximately 4x\nincrease in performance. Our work provides crucial insights into the role of\nthe LLM backbone in MLLMs, and highlights the need for more nuanced\nbenchmarking approaches.", "published": "2024-10-16 07:49:13", "link": "http://arxiv.org/abs/2410.12329v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing Model Collapse in Large Language Models Using Semantic\n  Networks and Next-Token Probability", "abstract": "As synthetic content increasingly infiltrates the web, generative AI models\nmay experience an autophagy process, where they are fine-tuned using their own\noutputs. This autophagy could lead to a phenomenon known as model collapse,\nwhich entails a degradation in the performance and diversity of generative AI\nmodels over successive generations. Recent studies have explored the emergence\nof model collapse across various generative AI models and types of data.\nHowever, the current characterizations of model collapse tend to be simplistic\nand lack comprehensive evaluation. In this article, we conduct a thorough\ninvestigation of model collapse across three text datasets, utilizing semantic\nnetworks to analyze text repetitiveness and diversity, while employing\nnext-token probabilities to quantify the loss of diversity. We also examine how\nthe proportions of synthetic tokens affect the severity of model collapse and\nperform cross-dataset evaluations to identify domain-specific variations. By\nproposing metrics and strategies for a more detailed assessment of model\ncollapse, our study provides new insights for the development of robust\ngenerative AI systems.", "published": "2024-10-16 08:02:48", "link": "http://arxiv.org/abs/2410.12341v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GECTurk WEB: An Explainable Online Platform for Turkish Grammatical\n  Error Detection and Correction", "abstract": "Sophisticated grammatical error detection/correction tools are available for\na small set of languages such as English and Chinese. However, it is not\nstraightforward -- if not impossible -- to adapt them to morphologically rich\nlanguages with complex writing rules like Turkish which has more than 80\nmillion speakers. Even though several tools exist for Turkish, they primarily\nfocus on spelling errors rather than grammatical errors and lack features such\nas web interfaces, error explanations and feedback mechanisms. To fill this\ngap, we introduce GECTurk WEB, a light, open-source, and flexible web-based\nsystem that can detect and correct the most common forms of Turkish writing\nerrors, such as the misuse of diacritics, compound and foreign words, pronouns,\nlight verbs along with spelling mistakes. Our system provides native speakers\nand second language learners an easily accessible tool to detect/correct such\nmistakes and also to learn from their mistakes by showing the explanation for\nthe violated rule(s). The proposed system achieves 88,3 system usability score,\nand is shown to help learn/remember a grammatical rule (confirmed by 80% of the\nparticipants). The GECTurk WEB is available both as an offline tool at\nhttps://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.", "published": "2024-10-16 08:13:54", "link": "http://arxiv.org/abs/2410.12350v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance", "abstract": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.", "published": "2024-10-16 08:24:09", "link": "http://arxiv.org/abs/2410.12361v3", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims", "abstract": "To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). For evidence retrieval, a language model is\nused to enhance a query by generating hypothetical fact-checking documents. We\nprompt pretrained and fine-tuned LLMs for question generation and veracity\nprediction by crafting prompts with retrieved in-context samples. HerO achieved\n2nd place on the leaderboard with the AVeriTeC score of 0.57, suggesting the\npotential of open LLMs for verifying real-world claims. For future research, we\nmake our code publicly available at https://github.com/ssu-humane/HerO.", "published": "2024-10-16 08:49:17", "link": "http://arxiv.org/abs/2410.12377v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Tracking Universal Features Through Fine-Tuning and Model Merging", "abstract": "We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.", "published": "2024-10-16 09:18:39", "link": "http://arxiv.org/abs/2410.12391v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing the Barriers of Language Agents in Planning", "abstract": "Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.", "published": "2024-10-16 09:44:38", "link": "http://arxiv.org/abs/2410.12409v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Conformity in Large Language Models", "abstract": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.", "published": "2024-10-16 10:16:34", "link": "http://arxiv.org/abs/2410.12428v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs", "abstract": "The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.", "published": "2024-10-16 10:49:22", "link": "http://arxiv.org/abs/2410.12445v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation", "abstract": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.", "published": "2024-10-16 11:46:32", "link": "http://arxiv.org/abs/2410.12476v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-end Planner Training for Language Modeling", "abstract": "Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.", "published": "2024-10-16 12:14:29", "link": "http://arxiv.org/abs/2410.12492v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-based Translation Inference with Iterative Bilingual Understanding", "abstract": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).", "published": "2024-10-16 13:21:46", "link": "http://arxiv.org/abs/2410.12543v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Claim Decomposition Benchmark for Long-form Answer Verification", "abstract": "The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.", "published": "2024-10-16 13:34:51", "link": "http://arxiv.org/abs/2410.12558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STRUX: An LLM for Decision-Making with Structured Explanations", "abstract": "Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.", "published": "2024-10-16 14:01:22", "link": "http://arxiv.org/abs/2410.12583v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning", "abstract": "As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.", "published": "2024-10-16 14:40:32", "link": "http://arxiv.org/abs/2410.12621v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic\n  Training Data for Classifying Social Constructs", "abstract": "Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.", "published": "2024-10-16 14:42:23", "link": "http://arxiv.org/abs/2410.12622v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models", "abstract": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.", "published": "2024-10-16 15:17:20", "link": "http://arxiv.org/abs/2410.12656v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce", "abstract": "Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.", "published": "2024-10-16 15:51:18", "link": "http://arxiv.org/abs/2410.12691v5", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine", "abstract": "Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.", "published": "2024-10-16 15:54:11", "link": "http://arxiv.org/abs/2410.12694v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sarcasm Detection in a Less-Resourced Language", "abstract": "The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.", "published": "2024-10-16 16:10:59", "link": "http://arxiv.org/abs/2410.12704v1", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe consistency of rewards across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.", "published": "2024-10-16 16:51:01", "link": "http://arxiv.org/abs/2410.12735v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples", "abstract": "Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .", "published": "2024-10-16 17:25:25", "link": "http://arxiv.org/abs/2410.12757v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unitary Multi-Margin BERT for Robust Natural Language Processing", "abstract": "Recent developments in adversarial attacks on deep learning leave many\nmission-critical natural language processing (NLP) systems at risk of\nexploitation. To address the lack of computationally efficient adversarial\ndefense methods, this paper reports a novel, universal technique that\ndrastically improves the robustness of Bidirectional Encoder Representations\nfrom Transformers (BERT) by combining the unitary weights with the multi-margin\nloss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin\nBERT (UniBERT), boosts post-attack classification accuracies significantly by\n5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,\nthe pre-attack and post-attack accuracy tradeoff can be adjusted via a single\nscalar parameter to best fit the design requirements for the target\napplications.", "published": "2024-10-16 17:30:58", "link": "http://arxiv.org/abs/2410.12759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information", "abstract": "The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.", "published": "2024-10-16 17:49:45", "link": "http://arxiv.org/abs/2410.12774v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Learning Enables Robot Action Prediction in LLMs", "abstract": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt.", "published": "2024-10-16 17:56:49", "link": "http://arxiv.org/abs/2410.12782v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via\n  Model Fusion of Embedding Models", "abstract": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks\nsuch as question-answering (QA), relying on retrieving relevant documents from\na vector store computed using a pretrained embedding model. However, if the\nretrieved context is inaccurate, the answers generated using the large language\nmodel (LLM) may contain errors or hallucinations. Although pretrained embedding\nmodels have advanced, adapting them to new domains remains challenging.\nFine-tuning is a potential solution, but industry settings often lack the\nnecessary fine-tuning data. To address these challenges, we propose REFINE, a\nnovel technique that generates synthetic data from available documents and then\nuses a model fusion approach to fine-tune embeddings for improved retrieval\nperformance in new domains, while preserving out-of-domain capability. We\nconducted experiments on the two public datasets: SQUAD and RAG-12000 and a\nproprietary TOURISM dataset. Results demonstrate that even the standard\nfine-tuning with the proposed data augmentation technique outperforms the\nvanilla pretrained model. Furthermore, when combined with model fusion, the\nproposed approach achieves superior performance, with a 5.76% improvement in\nrecall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and\nRAG-12000 respectively.", "published": "2024-10-16 08:43:39", "link": "http://arxiv.org/abs/2410.12890v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation", "abstract": "Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.", "published": "2024-10-16 12:24:42", "link": "http://arxiv.org/abs/2410.12893v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models and the Rationalist Empiricist Debate", "abstract": "To many Chomsky's debates with Quine and Skinner are an updated version of\nthe Rationalist Empiricist debates of the 17th century. The consensus being\nthat Chomsky's Rationalism was victorious. This dispute has reemerged with the\nadvent of Large Language Models. With some arguing that LLMs vindicate\nrationalism because of the necessity of building in innate biases to make them\nwork. The necessity of building in innate biases is taken to prove that\nempiricism hasn't got the conceptual resources to explain linguistic\ncompetence. Such claims depend on the nature of the empiricism one is\nendorsing. Externalized Empiricism has no difficulties with innate apparatus\nonce they are determined empirically (Quine 1969). Thus, externalized\nempiricism is not refuted because of the need to build in innate biases in\nLLMs. Furthermore, the relevance of LLMs to the rationalist empiricist debate\nin relation to humans is dubious. For any claim about whether LLMs learn in an\nempiricist manner to be relevant to humans it needs to be shown that LLMs and\nhumans learn in the same way. Two key features distinguish humans and LLMs.\nHumans learn despite a poverty of stimulus and LLMs learn because of an\nincredibly rich stimulus. Human linguistic outputs are grounded in sensory\nexperience and LLMs are not. These differences in how the two learn indicates\nthat they both use different underlying competencies to produce their output.\nTherefore, any claims about whether LLMs learn in an empiricist manner are not\nrelevant to whether humans learn in an empiricist manner.", "published": "2024-10-16 15:49:33", "link": "http://arxiv.org/abs/2410.12895v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merge to Learn: Efficiently Adding Skills to Language Models with Model\n  Merging", "abstract": "Adapting general-purpose language models to new skills is currently an\nexpensive process that must be repeated as new instruction datasets targeting\nnew skills are created, or can cause the models to forget older skills. In this\nwork, we investigate the effectiveness of adding new skills to preexisting\nmodels by training on the new skills in isolation and later merging with the\ngeneral model (e.g. using task vectors). In experiments focusing on scientific\nliterature understanding, safety, and coding, we find that the\nparallel-train-then-merge procedure, which is significantly cheaper than\nretraining the models on updated data mixtures, is often comparably effective.\nOur experiments also show that parallel training is especially well-suited for\nenabling safety features in LMs relative to continued finetuning and\nretraining, as it dramatically improves model compliance with safe prompts\nwhile preserving its ability to refuse dangerous or harmful prompts.", "published": "2024-10-16 18:23:50", "link": "http://arxiv.org/abs/2410.12937v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via\n  Mechanistic Localization", "abstract": "Methods for knowledge editing and unlearning in large language models seek to\nedit or remove undesirable knowledge or capabilities without compromising\ngeneral language modeling performance. This work investigates how mechanistic\ninterpretability -- which, in part, aims to identify model components\n(circuits) associated to specific interpretable mechanisms that make up a model\ncapability -- can improve the precision and effectiveness of editing and\nunlearning. We find a stark difference in unlearning and edit robustness when\ntraining components localized by different methods. We highlight an important\ndistinction between methods that localize components based primarily on\npreserving outputs, and those finding high level mechanisms with predictable\nintermediate states. In particular, localizing edits/unlearning to components\nassociated with the lookup-table mechanism for factual recall 1) leads to more\nrobust edits/unlearning across different input/output formats, and 2) resists\nattempts to relearn the unwanted information, while also reducing unintended\nside effects compared to baselines, on both a sports facts dataset and the\nCounterFact dataset across multiple models. We also find that certain localized\nedits disrupt the latent knowledge in the model more than any other baselines,\nmaking unlearning more robust to various attacks.", "published": "2024-10-16 18:35:02", "link": "http://arxiv.org/abs/2410.12949v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models as a Tool for Mining Object Knowledge", "abstract": "Commonsense knowledge is essential for machines to reason about the world.\nLarge language models (LLMs) have demonstrated their ability to perform almost\nhuman-like text generation. Despite this success, they fall short as\ntrustworthy intelligent systems, due to the opacity of the basis for their\nanswers and a tendency to confabulate facts when questioned about obscure\nentities or technical domains. We hypothesize, however, that their general\nknowledge about objects in the everyday world is largely sound. Based on that\nhypothesis, this paper investigates LLMs' ability to formulate explicit\nknowledge about common physical artifacts, focusing on their parts and\nmaterials. Our work distinguishes between the substances that comprise an\nentire object and those that constitute its parts$\\unicode{x2014}$a previously\nunderexplored distinction in knowledge base construction. Using few-shot with\nfive in-context examples and zero-shot multi-step prompting, we produce a\nrepository of data on the parts and materials of about 2,300 objects and their\nsubtypes. Our evaluation demonstrates LLMs' coverage and soundness in\nextracting knowledge. This contribution to knowledge mining should prove useful\nto AI research on reasoning about object structure and composition and serve as\nan explicit knowledge source (analogous to knowledge graphs) for LLMs\nperforming multi-hop question answering.", "published": "2024-10-16 18:46:02", "link": "http://arxiv.org/abs/2410.12959v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Qtok: A Comprehensive Framework for Evaluating Multilingual Tokenizer\n  Quality in Large Language Models", "abstract": "In the development of Large Language Models (LLMs), considerable attention\nhas been given to the quality of training datasets. However, the role of\ntokenizers in the LLM training pipeline, particularly for multilingual models,\nhas received less focus. The quality of tokenization can significantly impact a\nmodel's ability to handle diverse languages effectively. We introduce Qtok, a\ntool designed to assess tokenizer quality with a specific emphasis on their\nperformance in multilingual contexts.\n  Our research proposes a set of metrics for evaluating tokenizer quality,\nincluding measures of language coverage, token completeness, and distribution\nacross languages and linguistic categories. Qtok applies these metrics to\nevaluate 13 distinct tokenizers from 58 publicly available models, analyzing\ntheir output across different linguistic contexts. Our analysis revealed\nsignificant variations in token distribution across languages and categories,\nhighlighting potential biases and areas for improvement in current tokenization\nstrategies.\n  This research contributes to the field of tokenizer evaluation within\nmultilingual LLM development by providing a systematic approach to assessing\ntokenizer quality. Our findings highlight the critical role of tokenization in\nmultilingual LLM capability. The Qtok tool and our analysis methodology offer\npractical means for researchers to evaluate and improve tokenization strategies\nfor multilingual applications. We offer a method to compare tokenizer quality\nacross these metrics, which may be useful when selecting or adjusting\ntokenizers for specific multilingual LLM applications.", "published": "2024-10-16 19:34:34", "link": "http://arxiv.org/abs/2410.12989v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "primary_category": "cs.CL"}
{"title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks", "abstract": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups", "published": "2024-10-16 20:33:06", "link": "http://arxiv.org/abs/2410.13025v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Not to Answer: Evaluating Prompts on GPT Models for Effective\n  Abstention in Unanswerable Math Word Problems", "abstract": "Large language models (LLMs) are increasingly relied upon to solve complex\nmathematical word problems. However, being susceptible to hallucination, they\nmay generate inaccurate results when presented with unanswerable questions,\nraising concerns about their potential harm. While GPT models are now widely\nused and trusted, the exploration of how they can effectively abstain from\nanswering unanswerable math problems and the enhancement of their abstention\ncapabilities has not been rigorously investigated. In this paper, we\ninvestigate whether GPTs can appropriately respond to unanswerable math word\nproblems by applying prompts typically used in solvable mathematical scenarios.\nOur experiments utilize the Unanswerable Word Math Problem (UWMP) dataset,\ndirectly leveraging GPT model APIs. Evaluation metrics are introduced, which\nintegrate three key factors: abstention, correctness and confidence. Our\nfindings reveal critical gaps in GPT models and the hallucination it suffers\nfrom for unsolvable problems, highlighting the need for improved models capable\nof better managing uncertainty and complex reasoning in math word\nproblem-solving contexts.", "published": "2024-10-16 20:40:50", "link": "http://arxiv.org/abs/2410.13029v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Channel-Wise Mixed-Precision Quantization for Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of language tasks, but their deployment on edge devices remains\nchallenging due to the substantial memory requirements imposed by their large\nparameter sizes. Weight-only quantization presents a promising solution to\nreduce the memory footprint of LLMs. However, existing approaches primarily\nfocus on integer-bit quantization, limiting their adaptability to\nfractional-bit quantization tasks and preventing the full utilization of\navailable storage space on devices. In this paper, we introduce Channel-Wise\nMixed-Precision Quantization (CMPQ), a novel mixed-precision quantization\nmethod that allocates quantization precision in a channel-wise pattern based on\nactivation distributions. By assigning different precision levels to different\nweight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a\nnon-uniform quantization strategy and incorporates two outlier extraction\ntechniques that collaboratively preserve the critical information, thereby\nminimizing the quantization loss. Experiments on different sizes of LLMs\ndemonstrate that CMPQ not only enhances performance in integer-bit quantization\ntasks but also achieves significant performance gains with a modest increase in\nmemory usage. CMPQ thus represents an adaptive and effective approach to LLM\nquantization, offering substantial benefits across diverse device capabilities.", "published": "2024-10-16 21:34:41", "link": "http://arxiv.org/abs/2410.13056v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological\n  Garden Path Errors", "abstract": "In languages without orthographic word boundaries, NLP models perform word\nsegmentation, either as an explicit preprocessing step or as an implicit step\nin an end-to-end computation. This paper shows that Chinese NLP models are\nvulnerable to morphological garden path errors: errors caused by a failure to\nresolve local word segmentation ambiguities using sentence-level\nmorphosyntactic context. We propose a benchmark, ERAS, that tests a model's\nvulnerability to morphological garden path errors by comparing its behavior on\nsentences with and without local segmentation ambiguities. Using ERAS, we show\nthat word segmentation models make garden path errors on locally ambiguous\nsentences, but do not make equivalent errors on unambiguous sentences. We\nfurther show that sentiment analysis models with character-level tokenization\nmake implicit garden path errors, even without an explicit word segmentation\nstep in the pipeline. Our results indicate that models' segmentation of Chinese\ntext often fails to account for morphosyntactic context.", "published": "2024-10-16 21:35:20", "link": "http://arxiv.org/abs/2410.13057v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models as Semiotic Machines: Reconceptualizing AI Language\n  Systems through Structuralist and Post-Structuralist Theories of Language", "abstract": "This paper proposes a novel framework for understanding large language models\n(LLMs) by reconceptualizing them as semiotic machines rather than as imitations\nof human cognition. Drawing from structuralist and post-structuralist theories\nof language-specifically the works of Ferdinand de Saussure and Jacques\nDerrida-I argue that LLMs should be understood as models of language itself,\naligning with Derrida's concept of 'writing' (l'ecriture). The paper is\nstructured into three parts. First, I lay the theoretical groundwork by\nexplaining how the word2vec embedding algorithm operates within Saussure's\nframework of language as a relational system of signs. Second, I apply\nDerrida's critique of Saussure to position 'writing' as the object modeled by\nLLMs, offering a view of the machine's 'mind' as a statistical approximation of\nsign behavior. Finally, the third section addresses how modern LLMs reflect\npost-structuralist notions of unfixed meaning, arguing that the \"next token\ngeneration\" mechanism effectively captures the dynamic nature of meaning. By\nreconceptualizing LLMs as semiotic machines rather than cognitive models, this\nframework provides an alternative lens through which to assess the strengths\nand limitations of LLMs, offering new avenues for future research.", "published": "2024-10-16 21:45:54", "link": "http://arxiv.org/abs/2410.13065v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Is Semantic Chunking Worth the Computational Cost?", "abstract": "Recent advances in Retrieval-Augmented Generation (RAG) systems have\npopularized semantic chunking, which aims to improve retrieval performance by\ndividing documents into semantically coherent segments. Despite its growing\nadoption, the actual benefits over simpler fixed-size chunking, where documents\nare split into consecutive, fixed-size segments, remain unclear. This study\nsystematically evaluates the effectiveness of semantic chunking using three\ncommon retrieval-related tasks: document retrieval, evidence retrieval, and\nretrieval-based answer generation. The results show that the computational\ncosts associated with semantic chunking are not justified by consistent\nperformance gains. These findings challenge the previous assumptions about\nsemantic chunking and highlight the need for more efficient chunking strategies\nin RAG systems.", "published": "2024-10-16 21:53:48", "link": "http://arxiv.org/abs/2410.13070v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on\nfinal-layer loss for training and final-layer representations for predictions,\npotentially overlooking the predictive power embedded in intermediate layers.\nSurprisingly, we find that focusing training efforts on these intermediate\nlayers can yield training losses comparable to those of final layers, with\ncomplementary test-time performance. We introduce a novel tuning framework,\nMixture-of-Depths (MoD), which trains late layers as ensembles contributing to\nthe final logits through learned routing weights. With the auxiliary\ndistillation loss and additional normalization modules, we ensure that the\noutputs of the late layers adapt to language modeling. Our MoD framework, which\ncan be integrated with any existing tuning method, shows consistent improvement\non various language modelling tasks. Furthermore, by replacing traditional\ntrainable modules with MoD, our approach achieves similar performance with\nsignificantly fewer trainable parameters, demonstrating the potential of\nleveraging predictive power from intermediate representations during training.", "published": "2024-10-16 22:51:45", "link": "http://arxiv.org/abs/2410.13077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Communication-Efficient and Tensorized Federated Fine-Tuning of Large\n  Language Models", "abstract": "Parameter-efficient fine-tuning (PEFT) methods typically assume that Large\nLanguage Models (LLMs) are trained on data from a single device or client.\nHowever, real-world scenarios often require fine-tuning these models on private\ndata distributed across multiple devices. Federated Learning (FL) offers an\nappealing solution by preserving user privacy, as sensitive data remains on\nlocal devices during training. Nonetheless, integrating PEFT methods into FL\nintroduces two main challenges: communication overhead and data heterogeneity.\nIn this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by\nintegrating tensorized adapters into client-side models' encoder/decoder\nblocks. FedTT is versatile and can be applied to both cross-silo FL and\nlarge-scale cross-device FL. FedTT+, an extension of FedTT tailored for\ncross-silo FL, enhances robustness against data heterogeneity by adaptively\nfreezing portions of tensor factors, further reducing the number of trainable\nparameters. Experiments on BERT and LLaMA models demonstrate that our proposed\nmethods successfully address data heterogeneity challenges and perform on par\nor even better than existing federated PEFT approaches while achieving up to\n10$\\times$ reduction in communication cost.", "published": "2024-10-16 23:50:39", "link": "http://arxiv.org/abs/2410.13097v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Rethinking Token Reduction for State Space Models", "abstract": "Recent advancements in State Space Models (SSMs) have attracted significant\ninterest, particularly in models optimized for parallel training and handling\nlong-range dependencies. Architectures like Mamba have scaled to billions of\nparameters with selective SSM. To facilitate broader applications using Mamba,\nexploring its efficiency is crucial. While token reduction techniques offer a\nstraightforward post-training strategy, we find that applying existing methods\ndirectly to SSMs leads to substantial performance drops. Through insightful\nanalysis, we identify the reasons for this failure and the limitations of\ncurrent techniques. In response, we propose a tailored, unified post-training\ntoken reduction method for SSMs. Our approach integrates token importance and\nsimilarity, thus taking advantage of both pruning and merging, to devise a\nfine-grained intra-layer token reduction strategy. Extensive experiments show\nthat our method improves the average accuracy by 5.7% to 13.1% on six\nbenchmarks with Mamba-2 compared to existing methods, while significantly\nreducing computational demands and memory requirements.", "published": "2024-10-16 00:06:13", "link": "http://arxiv.org/abs/2410.14725v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Table-LLM-Specialist: Language Model Specialists for Tables using\n  Iterative Generator-Validator Fine-tuning", "abstract": "In this work, we propose Table-LLM-Specialist, or Table-Specialist for short,\nas a new self-trained fine-tuning paradigm specifically designed for table\ntasks. Our insight is that for each table task, there often exist two dual\nversions of the same task, one generative and one classification in nature.\nLeveraging their duality, we propose a Generator-Validator paradigm, to\niteratively generate-then-validate training data from language-models, to\nfine-tune stronger \\sys models that can specialize in a given task, without\nrequiring manually-labeled data.\n  Our extensive evaluations suggest that our Table-Specialist has (1)\n\\textit{strong performance} on diverse table tasks over vanilla language-models\n-- for example, Table-Specialist fine-tuned on GPT-3.5 not only outperforms\nvanilla GPT-3.5, but can often match or surpass GPT-4 level quality, (2)\n\\textit{lower cost} to deploy, because when Table-Specialist fine-tuned on\nGPT-3.5 achieve GPT-4 level quality, it becomes possible to deploy smaller\nmodels with lower latency and inference cost, with comparable quality, and (3)\n\\textit{better generalizability} when evaluated across multiple benchmarks,\nsince \\sys is fine-tuned on a broad range of training data systematically\ngenerated from diverse real tables. Our code and data will be available at\nhttps://github.com/microsoft/Table-LLM-Specialist.", "published": "2024-10-16 02:04:17", "link": "http://arxiv.org/abs/2410.12164v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities", "abstract": "We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.", "published": "2024-10-16 04:29:46", "link": "http://arxiv.org/abs/2410.12219v1", "categories": ["cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations", "abstract": "Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.", "published": "2024-10-16 04:44:15", "link": "http://arxiv.org/abs/2410.12228v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection", "abstract": "We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.", "published": "2024-10-16 06:31:59", "link": "http://arxiv.org/abs/2410.12278v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CV"}
{"title": "Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech\n  Synthesis in ASR", "abstract": "Synthetically generated speech has rapidly approached human levels of\nnaturalness. However, the paradox remains that ASR systems, when trained on TTS\noutput that is judged as natural by humans, continue to perform badly on real\nspeech. In this work, we explore whether this phenomenon is due to the\noversmoothing behaviour of models commonly used in TTS, with a particular focus\non the behaviour of TTS-for-ASR as the amount of TTS training data is scaled\nup. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)\nto Mean Squared Error (MSE) based models for TTS, when used for ASR model\ntraining. We test the scalability of the two approaches, varying both the\nnumber hours, and the number of different speakers. We find that for a given\nmodel size, DDPM can make better use of more data, and a more diverse set of\nspeakers, than MSE models. We achieve the best reported ratio between real and\nsynthetic speech WER to date (1.46), but also find that a large gap remains.", "published": "2024-10-16 06:35:56", "link": "http://arxiv.org/abs/2410.12279v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting", "abstract": "The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.", "published": "2024-10-16 06:43:02", "link": "http://arxiv.org/abs/2410.12284v2", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "LLM-based Cognitive Models of Students with Misconceptions", "abstract": "Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.", "published": "2024-10-16 06:51:09", "link": "http://arxiv.org/abs/2410.12294v2", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Beyond Coarse-Grained Matching in Video-Text Retrieval", "abstract": "Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.", "published": "2024-10-16 09:42:29", "link": "http://arxiv.org/abs/2410.12407v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs", "abstract": "Schema matching (SM) and entity matching (EM) tasks are crucial for data\nintegration. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. This study presents the Knowledge-Compliant Matching Framework\n(KcMF), an LLM-based approach that addresses these issues without the need for\ndomain-specific fine-tuning. KcMF employs a once-and-for-all pseudo-code-based\ntask decomposition strategy to adopt natural language statements that guide LLM\nreasoning and reduce confusion across various task types. We also propose two\nmechanisms, Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build\ndomain knowledge sets when unstructured domain knowledge is lacking. Moreover,\nwe introduce a result-ensemble strategy to leverage multiple knowledge sources\nand suppress badly formatted outputs. Extensive evaluations confirm that KcMF\nclearly enhances five LLM backbones in both SM and EM tasks while outperforming\nthe non-LLM competitors by an average F1-score of 17.93%.", "published": "2024-10-16 11:50:02", "link": "http://arxiv.org/abs/2410.12480v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models", "abstract": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).", "published": "2024-10-16 15:20:08", "link": "http://arxiv.org/abs/2410.12662v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines", "abstract": "Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.", "published": "2024-10-16 16:11:49", "link": "http://arxiv.org/abs/2410.12705v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts", "abstract": "With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.", "published": "2024-10-16 17:51:25", "link": "http://arxiv.org/abs/2410.12777v1", "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges", "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench.", "published": "2024-10-16 17:58:19", "link": "http://arxiv.org/abs/2410.12784v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exploiting Longitudinal Speech Sessions via Voice Assistant Systems for\n  Early Detection of Cognitive Decline", "abstract": "Mild Cognitive Impairment (MCI) is an early stage of Alzheimer's disease\n(AD), a form of neurodegenerative disorder. Early identification of MCI is\ncrucial for delaying its progression through timely interventions. Existing\nresearch has demonstrated the feasibility of detecting MCI using speech\ncollected from clinical interviews or digital devices. However, these\napproaches typically analyze data collected at limited time points, limiting\ntheir ability to identify cognitive changes over time. This paper presents a\nlongitudinal study using voice assistant systems (VAS) to remotely collect\nseven-session speech data at three-month intervals across 18 months. We propose\ntwo methods to improve MCI detection and the prediction of cognitive changes.\nThe first method incorporates historical data, while the second predicts\ncognitive changes at two time points. Our results indicate improvements when\nincorporating historical data: the average F1-score for MCI detection improves\nfrom 58.6% to 71.2% (by 12.6%) in the case of acoustic features and from 62.1%\nto 75.1% (by 13.0%) in the case of linguistic features. Additionally, the\nprediction of cognitive changes achieves an F1-score of 73.7% in the case of\nacoustic features. These results confirm the potential of VAS-based speech\nsessions for early detection of cognitive decline.", "published": "2024-10-16 01:10:21", "link": "http://arxiv.org/abs/2410.12885v1", "categories": ["eess.AS", "cs.CL", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic\n  Filtering and Iterative Reasoning", "abstract": "Recent advancements in QA with LLM, like GPT-4, have shown limitations in\nhandling complex multi-hop queries. We propose AT-RAG, a novel multistep RAG\nincorporating topic modeling for efficient document retrieval and reasoning.\nUsing BERTopic, our model dynamically assigns topics to queries, improving\nretrieval accuracy and efficiency. We evaluated AT-RAG on multihop benchmark\ndatasets QA and a medical case study QA. Results show significant improvements\nin correctness, completeness, and relevance compared to existing methods.\nAT-RAG reduces retrieval time while maintaining high precision, making it\nsuitable for general tasks QA and complex domain-specific challenges such as\nmedical QA. The integration of topic filtering and iterative reasoning enables\nour model to handle intricate queries efficiently, which makes it suitable for\napplications that require nuanced information retrieval and decision-making.", "published": "2024-10-16 01:57:56", "link": "http://arxiv.org/abs/2410.12886v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-trait User Simulation with Adaptive Decoding for Conversational\n  Task Assistants", "abstract": "Conversational systems must be robust to user interactions that naturally\nexhibit diverse conversational traits. Capturing and simulating these diverse\ntraits coherently and efficiently presents a complex challenge. This paper\nintroduces Multi-Trait Adaptive Decoding (mTAD), a method that generates\ndiverse user profiles at decoding-time by sampling from various trait-specific\nLanguage Models (LMs). mTAD provides an adaptive and scalable approach to user\nsimulation, enabling the creation of multiple user profiles without the need\nfor additional fine-tuning. By analyzing real-world dialogues from the\nConversational Task Assistant (CTA) domain, we identify key conversational\ntraits and developed a framework to generate profile-aware dialogues that\nenhance conversational diversity. Experimental results validate the\neffectiveness of our approach in modeling single-traits using specialized LMs,\nwhich can capture less common patterns, even in out-of-domain tasks.\nFurthermore, the results demonstrate that mTAD is a robust and flexible\nframework for combining diverse user simulators.", "published": "2024-10-16 09:40:34", "link": "http://arxiv.org/abs/2410.12891v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7"], "primary_category": "cs.CL"}
{"title": "What Do Speech Foundation Models Not Learn About Speech?", "abstract": "Understanding how speech foundation models capture non-verbal cues is crucial\nfor improving their interpretability and adaptability across diverse tasks. In\nour work, we analyze several prominent models such as Whisper, Seamless,\nWav2Vec, HuBERT, and Qwen2-Audio focusing on their learned representations in\nboth paralinguistic and non-paralinguistic tasks from the Dynamic-SUPERB\nbenchmark. Our study addresses three key questions: (1) What non-verbal cues\n(e.g., speaker intent, emotion, environmental context) are captured? (2) How\nare these cues represented across different layers of the models? and (3) To\nwhat extent can these representations be effectively adapted to downstream\ntasks? To answer these questions, we first evaluate the models in a zero-shot\nsetting, followed by fine-tuning on layer-wise features extracted from these\nmodels. Our results provide insights into the models' capacity for\ngeneralization, the characteristics of their layer-wise representations, and\nthe degree of transformation required for downstream task adaptation. Our\nfindings suggest that some of these models perform well on various tasks in\nzero-shot settings, despite not being explicitly trained for those tasks. We\nalso observe that zero-shot performance correlates with better-learned\nrepresentations. The analysis of layer-wise features demonstrates that some\nmodels exhibit a convex relationship between the separability of the learned\nrepresentations and model depth, with different layers capturing task-specific\nfeatures.", "published": "2024-10-16 18:34:07", "link": "http://arxiv.org/abs/2410.12948v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question\n  Answering", "abstract": "We present LEGAL-UQA, the first Urdu legal question-answering dataset derived\nfrom Pakistan's constitution. This parallel English-Urdu dataset includes 619\nquestion-answer pairs, each with corresponding legal article contexts,\naddressing the need for domain-specific NLP resources in low-resource\nlanguages. We describe the dataset creation process, including OCR extraction,\nmanual refinement, and GPT-4-assisted translation and generation of QA pairs.\nOur experiments evaluate the latest generalist language and embedding models on\nLEGAL-UQA, with Claude-3.5-Sonnet achieving 99.19% human-evaluated accuracy. We\nfine-tune mt5-large-UQA-1.0, highlighting the challenges of adapting\nmultilingual models to specialized domains. Additionally, we assess retrieval\nperformance, finding OpenAI's text-embedding-3-large outperforms Mistral's\nmistral-embed. LEGAL-UQA bridges the gap between global NLP advancements and\nlocalized applications, particularly in constitutional law, and lays the\nfoundation for improved legal information access in Pakistan.", "published": "2024-10-16 20:14:45", "link": "http://arxiv.org/abs/2410.13013v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "Learning Representations for Reasoning: Generalizing Across Diverse\n  Structures", "abstract": "Reasoning, the ability to logically draw conclusions from existing knowledge,\nis a hallmark of human. Together with perception, they constitute the two major\nthemes of artificial intelligence. While deep learning has pushed the limit of\nperception beyond human-level performance, the progress in reasoning domains is\nway behind. One fundamental reason is that reasoning problems usually have\nflexible structures for both knowledge and queries, and many existing models\nonly perform well on structures seen during training. Here we aim to push the\nboundary of reasoning models by devising algorithms that generalize across\nknowledge and query structures, as well as systems that accelerate development\non structured data. This thesis consists of three parts. In Part I, we study\nmodels that can inductively generalize to unseen knowledge graphs with new\nentity and relation vocabularies. For new entities, we propose a framework that\nlearns neural operators in a dynamic programming algorithm computing path\nrepresentations. For relations, we construct a relation graph to capture the\ninteractions between relations, thereby converting new relations into new\nentities. In Part II, we propose two solutions for generalizing across\nmulti-step queries on knowledge graphs and text respectively. For knowledge\ngraphs, we show that multi-step queries can be solved by multiple calls of\ngraph neural networks and fuzzy logic operations. For text, we devise an\nalgorithm to learn explicit knowledge as textual rules to improve large\nlanguage models on multi-step queries. In Part III, we propose two systems to\nfacilitate machine learning development on structured data. Our library treats\nstructured data as first-class citizens and removes the barrier for developing\nalgorithms on structured data. Our node embedding system solves the GPU memory\nbottleneck of embedding matrices and scales to graphs with billion nodes.", "published": "2024-10-16 20:23:37", "link": "http://arxiv.org/abs/2410.13018v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Sensitivity of Generative VLMs to Semantically and Lexically Altered\n  Prompts", "abstract": "Despite the significant influx of prompt-tuning techniques for generative\nvision-language models (VLMs), it remains unclear how sensitive these models\nare to lexical and semantic alterations in prompts. In this paper, we evaluate\nthe ability of generative VLMs to understand lexical and semantic changes in\ntext using the SugarCrepe++ dataset. We analyze the sensitivity of VLMs to\nlexical alterations in prompts without corresponding semantic changes. Our\nfindings demonstrate that generative VLMs are highly sensitive to such\nalterations. Additionally, we show that this vulnerability affects the\nperformance of techniques aimed at achieving consistency in their outputs.", "published": "2024-10-16 20:41:32", "link": "http://arxiv.org/abs/2410.13030v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLM Confidence Evaluation Measures in Zero-Shot CSS Classification", "abstract": "Assessing classification confidence is critical for leveraging large language\nmodels (LLMs) in automated labeling tasks, especially in the sensitive domains\npresented by Computational Social Science (CSS) tasks. In this paper, we make\nthree key contributions: (1) we propose an uncertainty quantification (UQ)\nperformance measure tailored for data annotation tasks, (2) we compare, for the\nfirst time, five different UQ strategies across three distinct LLMs and CSS\ndata annotation tasks, (3) we introduce a novel UQ aggregation strategy that\neffectively identifies low-confidence LLM annotations and disproportionately\nuncovers data incorrectly labeled by the LLMs. Our results demonstrate that our\nproposed UQ aggregation strategy improves upon existing methods andcan be used\nto significantly improve human-in-the-loop data annotation processes.", "published": "2024-10-16 21:17:18", "link": "http://arxiv.org/abs/2410.13047v2", "categories": ["cs.HC", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
{"title": "Supply Chain Network Extraction and Entity Classification Leveraging\n  Large Language Models", "abstract": "Supply chain networks are critical to the operational efficiency of\nindustries, yet their increasing complexity presents significant challenges in\nmapping relationships and identifying the roles of various entities.\nTraditional methods for constructing supply chain networks rely heavily on\nstructured datasets and manual data collection, limiting their scope and\nefficiency. In contrast, recent advancements in Natural Language Processing\n(NLP) and large language models (LLMs) offer new opportunities for discovering\nand analyzing supply chain networks using unstructured text data. This paper\nproposes a novel approach that leverages LLMs to extract and process raw\ntextual information from publicly available sources to construct a\ncomprehensive supply chain graph. We focus on the civil engineering sector as a\ncase study, demonstrating how LLMs can uncover hidden relationships among\ncompanies, projects, and other entities. Additionally, we fine-tune an LLM to\nclassify entities within the supply chain graph, providing detailed insights\ninto their roles and relationships. The results show that domain-specific\nfine-tuning improves classification accuracy, highlighting the potential of\nLLMs for industry-specific supply chain analysis. Our contributions include the\ndevelopment of a supply chain graph for the civil engineering sector, as well\nas a fine-tuned LLM model that enhances entity classification and understanding\nof supply chain networks.", "published": "2024-10-16 21:24:13", "link": "http://arxiv.org/abs/2410.13051v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language\n  Models", "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in\nhealthcare, particularly in disease diagnosis and treatment planning. Recent\nprogress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\npossibilities for interactive diagnostic tools. However, these models often\nsuffer from factual hallucination, which can lead to incorrect diagnoses.\nFine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\naddress these issues. However, the amount of high-quality data and distribution\nshifts between training data and deployment data limit the application of\nfine-tuning methods. Although RAG is lightweight and effective, existing\nRAG-based approaches are not sufficiently general to different medical domains\nand can potentially cause misalignment issues, both between modalities and\nbetween the model and the ground truth. In this paper, we propose a versatile\nmultimodal RAG system, MMed-RAG, designed to enhance the factuality of\nMed-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\nadaptive retrieved contexts selection method, and a provable RAG-based\npreference fine-tuning strategy. These innovations make the RAG process\nsufficiently general and reliable, significantly improving alignment when\nintroducing retrieved contexts. Experimental results across five medical\ndatasets (involving radiology, ophthalmology, pathology) on medical VQA and\nreport generation demonstrate that MMed-RAG can achieve an average improvement\nof 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\nin https://github.com/richard-peng-xia/MMed-RAG.", "published": "2024-10-16 23:03:27", "link": "http://arxiv.org/abs/2410.13085v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Reverse-Engineering the Reader", "abstract": "Numerous previous studies have sought to determine to what extent language\nmodels, pretrained on natural language text, can serve as useful models of\nhuman cognition. In this paper, we are interested in the opposite question:\nwhether we can directly optimize a language model to be a useful cognitive\nmodel by aligning it to human psychometric data. To achieve this, we introduce\na novel alignment technique in which we fine-tune a language model to\nimplicitly optimize the parameters of a linear regressor that directly predicts\nhumans' reading times of in-context linguistic units, e.g., phonemes,\nmorphemes, or words, using surprisal estimates derived from the language model.\nUsing words as a test case, we evaluate our technique across multiple model\nsizes and datasets and find that it improves language models' psychometric\npredictive power. However, we find an inverse relationship between psychometric\npower and a model's performance on downstream NLP tasks as well as its\nperplexity on held-out test data. While this latter trend has been observed\nbefore (Oh et al., 2022; Shain et al., 2024), we are the first to induce it by\nmanipulating a model's alignment to psychometric data.", "published": "2024-10-16 23:05:01", "link": "http://arxiv.org/abs/2410.13086v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Comparison for Dataset-Level Membership Inference in Large\n  (Vision-)Language Models", "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have made\nsignificant advancements in a wide range of natural language processing and\nvision-language tasks. Access to large web-scale datasets has been a key factor\nin their success. However, concerns have been raised about the unauthorized use\nof copyrighted materials and potential copyright infringement. Existing\nmethods, such as sample-level Membership Inference Attacks (MIA) and\ndistribution-based dataset inference, distinguish member data (data used for\ntraining) and non-member data by leveraging the common observation that models\ntend to memorize and show greater confidence in member data. Nevertheless,\nthese methods face challenges when applied to LLMs and VLMs, such as the\nrequirement for ground-truth member data or non-member data that shares the\nsame distribution as the test data. In this paper, we propose a novel\ndataset-level membership inference method based on Self-Comparison. We find\nthat a member prefix followed by a non-member suffix (paraphrased from a member\nsuffix) can further trigger the model's memorization on training data. Instead\nof directly comparing member and non-member data, we introduce paraphrasing to\nthe second half of the sequence and evaluate how the likelihood changes before\nand after paraphrasing. Unlike prior approaches, our method does not require\naccess to ground-truth member data or non-member data in identical\ndistribution, making it more practical. Extensive experiments demonstrate that\nour proposed method outperforms traditional MIA and dataset inference\ntechniques across various datasets and models, including including public\nmodels, fine-tuned models, and API-based commercial models.", "published": "2024-10-16 23:05:59", "link": "http://arxiv.org/abs/2410.13088v1", "categories": ["cs.LG", "cs.CL", "cs.MM"], "primary_category": "cs.LG"}
{"title": "SoK: Prompt Hacking of Large Language Models", "abstract": "The safety and robustness of large language models (LLMs) based applications\nremain critical challenges in artificial intelligence. Among the key threats to\nthese applications are prompt hacking attacks, which can significantly\nundermine the security and reliability of LLM-based systems. In this work, we\noffer a comprehensive and systematic overview of three distinct types of prompt\nhacking: jailbreaking, leaking, and injection, addressing the nuances that\ndifferentiate them despite their overlapping characteristics. To enhance the\nevaluation of LLM-based applications, we propose a novel framework that\ncategorizes LLM responses into five distinct classes, moving beyond the\ntraditional binary classification. This approach provides more granular\ninsights into the AI's behavior, improving diagnostic precision and enabling\nmore targeted enhancements to the system's safety and robustness.", "published": "2024-10-16 01:30:41", "link": "http://arxiv.org/abs/2410.13901v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.CR"}
{"title": "NSmark: Null Space Based Black-box Watermarking Defense Framework for\n  Language Models", "abstract": "Language models (LMs) have emerged as critical intellectual property (IP)\nassets that necessitate protection. Although various watermarking strategies\nhave been proposed, they remain vulnerable to Linear Functionality Equivalence\nAttack (LFEA), which can invalidate most existing white-box watermarks without\nprior knowledge of the watermarking scheme or training data. This paper\nanalyzes and extends the attack scenarios of LFEA to the commonly employed\nblack-box settings for LMs by considering Last-Layer outputs (dubbed LL-LFEA).\nWe discover that the null space of the output matrix remains invariant against\nLL-LFEA attacks. Based on this finding, we propose NSmark, a black-box\nwatermarking scheme that is task-agnostic and capable of resisting LL-LFEA\nattacks. NSmark consists of three phases: (i) watermark generation using the\ndigital signature of the owner, enhanced by spread spectrum modulation for\nincreased robustness; (ii) watermark embedding through an output mapping\nextractor that preserves the LM performance while maximizing watermark\ncapacity; (iii) watermark verification, assessed by extraction rate and null\nspace conformity. Extensive experiments on both pre-training and downstream\ntasks confirm the effectiveness, scalability, reliability, fidelity, and\nrobustness of our approach. Code is available at\nhttps://github.com/dongdongzhaoUP/NSmark.", "published": "2024-10-16 14:45:27", "link": "http://arxiv.org/abs/2410.13907v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Is Less More? Exploring Token Condensation as Training-free Test-time\n  Adaptation", "abstract": "Contrastive Language-Image Pretraining (CLIP) excels at learning\ngeneralizable image representations but often falls short in zero-shot\ninference on certain downstream datasets. Test-time adaptation (TTA) mitigates\nthis issue by adjusting components like normalization layers or context\nprompts, yet it typically requires large batch sizes and extensive\naugmentations, leading to high computational costs. This raises a key question:\nCan VLMs' performance drop in specific test cases be mitigated through\nefficient, training-free approaches? To explore the solution, we investigate\ntoken condensation (TC) techniques, originally designed to enhance vision\ntransformer efficiency by refining token usage during inference. We observe\nthat informative tokens improve visual-text alignment in VLMs like CLIP on\nunseen datasets. However, existing TC methods often fail to maintain\nin-distribution performance when reducing tokens, prompting us to ask: How can\nwe transform TC into an effective ``free-lunch'' adaptation strategy for VLMs?\nTo address this, we propose Token Condensation as Adaptation (TCA), a\ntraining-free adaptation method that takes a step beyond standard TC. Rather\nthan passively discarding tokens, TCA condenses token representation by\nintroducing reservoir-based domain anchor tokens for information-preserving\ntoken reduction and logits correction. TCA achieves up to a 21.4% performance\nimprovement over the strongest baseline on cross-dataset benchmark and the\nCIFAR-100-Corrupted dataset while reducing GFLOPs by 12.2% to 48.9%, with\nminimal hyperparameter dependency on both CLIP and SigLIP series.", "published": "2024-10-16 07:13:35", "link": "http://arxiv.org/abs/2410.14729v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection", "abstract": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.", "published": "2024-10-16 08:34:51", "link": "http://arxiv.org/abs/2410.14731v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge Graph Embeddings: A Comprehensive Survey on Capturing Relation\n  Properties", "abstract": "Knowledge Graph Embedding (KGE) techniques play a pivotal role in\ntransforming symbolic Knowledge Graphs (KGs) into numerical representations,\nthereby enhancing various deep learning models for knowledge-augmented\napplications. Unlike entities, relations in KGs are the carriers of semantic\nmeaning, and their accurate modeling is crucial for the performance of KGE\nmodels. Firstly, we address the complex mapping properties inherent in\nrelations, such as one-to-one, one-to-many, many-to-one, and many-to-many\nmappings. We provide a comprehensive summary of relation-aware mapping-based\nmodels, models that utilize specific representation spaces, tensor\ndecomposition-based models, and neural network-based models. Next, focusing on\ncapturing various relation patterns like symmetry, asymmetry, inversion, and\ncomposition, we review models that employ modified tensor decomposition, those\nbased on modified relation-aware mappings, and those that leverage rotation\noperations. Subsequently, considering the implicit hierarchical relations among\nentities, we introduce models that incorporate auxiliary information, models\nbased on hyperbolic spaces, and those that utilize the polar coordinate system.\nFinally, in response to more complex scenarios such as sparse and dynamic KGs,\nthis paper discusses potential future research directions. We explore\ninnovative ideas such as integrating multimodal information into KGE, enhancing\nrelation pattern modeling with rules, and developing models to capture relation\ncharacteristics in dynamic KGE settings.", "published": "2024-10-16 08:54:52", "link": "http://arxiv.org/abs/2410.14733v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Agent Skill Acquisition for Large Language Models via CycleQD", "abstract": "Training large language models to acquire specific skills remains a\nchallenging endeavor. Conventional training approaches often struggle with data\ndistribution imbalances and inadequacies in objective functions that do not\nalign well with task-specific performance. To address these challenges, we\nintroduce CycleQD, a novel approach that leverages the Quality Diversity\nframework through a cyclic adaptation of the algorithm, along with a model\nmerging based crossover and an SVD-based mutation. In CycleQD, each task's\nperformance metric is alternated as the quality measure while the others serve\nas the behavioral characteristics. This cyclic focus on individual tasks allows\nfor concentrated effort on one task at a time, eliminating the need for data\nratio tuning and simplifying the design of the objective function. Empirical\nresults from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT\nbased models not only enables them to surpass traditional fine-tuning methods\nin coding, operating systems, and database tasks, but also achieves performance\non par with GPT-3.5-TURBO, which potentially contains much more parameters,\nacross these domains. Crucially, this enhanced performance is achieved while\nretaining robust language capabilities, as evidenced by its performance on\nwidely adopted language benchmark tasks. We highlight the key design choices in\nCycleQD, detailing how these contribute to its effectiveness. Furthermore, our\nmethod is general and can be applied to image segmentation models, highlighting\nits applicability across different domains.", "published": "2024-10-16 20:27:15", "link": "http://arxiv.org/abs/2410.14735v4", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "First-Person Fairness in Chatbots", "abstract": "Evaluating chatbot fairness is crucial given their rapid proliferation, yet\ntypical chatbot tasks (e.g., resume writing, entertainment) diverge from the\ninstitutional decision-making tasks (e.g., resume screening) which have\ntraditionally been central to discussion of algorithmic fairness. The\nopen-ended nature and diverse use-cases of chatbots necessitate novel methods\nfor bias assessment. This paper addresses these challenges by introducing a\nscalable counterfactual approach to evaluate \"first-person fairness,\" meaning\nfairness toward chatbot users based on demographic characteristics. Our method\nemploys a Language Model as a Research Assistant (LMRA) to yield quantitative\nmeasures of harmful stereotypes and qualitative analyses of demographic\ndifferences in chatbot responses. We apply this approach to assess biases in\nsix of our language models across millions of interactions, covering sixty-six\ntasks in nine domains and spanning two genders and four races. Independent\nhuman annotations corroborate the LMRA-generated bias evaluations. This study\nrepresents the first large-scale fairness evaluation based on real-world chat\ndata. We highlight that post-training reinforcement learning techniques\nsignificantly mitigate these biases. This evaluation provides a practical\nmethodology for ongoing bias monitoring and mitigation.", "published": "2024-10-16 17:59:47", "link": "http://arxiv.org/abs/2410.19803v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve\n  Anomalies", "abstract": "Existing household robots have made significant progress in performing\nroutine tasks, such as cleaning floors or delivering objects. However, a key\nlimitation of these robots is their inability to recognize potential problems\nor dangers in home environments. For example, a child may pick up and ingest\nmedication that has fallen on the floor, posing a serious risk. We argue that\nhousehold robots should proactively detect such hazards or anomalies within the\nhome, and propose the task of anomaly scenario generation. We leverage\nfoundational models instead of relying on manually labeled data to build\nsimulated environments. Specifically, we introduce a multi-agent brainstorming\napproach, where agents collaborate and generate diverse scenarios covering\nhousehold hazards, hygiene management, and child safety. These textual task\ndescriptions are then integrated with designed 3D assets to simulate realistic\nenvironments. Within these constructed environments, the robotic agent learns\nthe necessary skills to proactively discover and handle the proposed anomalies\nthrough task decomposition, and optimal learning approach selection. We\ndemonstrate that our generated environment outperforms others in terms of task\ndescription and scene diversity, ultimately enabling robotic agents to better\naddress potential household hazards.", "published": "2024-10-16 19:29:14", "link": "http://arxiv.org/abs/2411.00781v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking", "abstract": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.", "published": "2024-10-16 08:46:26", "link": "http://arxiv.org/abs/2410.12375v1", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring Model Kinship for Merging Large Language Models", "abstract": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.", "published": "2024-10-16 14:29:29", "link": "http://arxiv.org/abs/2410.12613v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "LFOSum: Summarizing Long-form Opinions with Large Language Models", "abstract": "Online reviews play a pivotal role in influencing consumer decisions across\nvarious domains, from purchasing products to selecting hotels or restaurants.\nHowever, the sheer volume of reviews -- often containing repetitive or\nirrelevant content -- leads to information overload, making it challenging for\nusers to extract meaningful insights. Traditional opinion summarization models\nface challenges in handling long inputs and large volumes of reviews, while\nnewer Large Language Model (LLM) approaches often fail to generate accurate and\nfaithful summaries. To address those challenges, this paper introduces (1) a\nnew dataset of long-form user reviews, each entity comprising over a thousand\nreviews, (2) two training-free LLM-based summarization approaches that scale to\nlong inputs, and (3) automatic evaluation metrics. Our dataset of user reviews\nis paired with in-depth and unbiased critical summaries by domain experts,\nserving as a reference for evaluation. Additionally, our novel reference-free\nevaluation metrics provide a more granular, context-sensitive assessment of\nsummary faithfulness. We benchmark several open-source and closed-source LLMs\nusing our methods. Our evaluation reveals that LLMs still face challenges in\nbalancing sentiment and format adherence in long-form summaries, though\nopen-source models can narrow the gap when relevant information is retrieved in\na focused manner.", "published": "2024-10-16 20:52:39", "link": "http://arxiv.org/abs/2410.13037v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ERVQ: Enhanced Residual Vector Quantization with\n  Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ)\nto discretize speech signals. However, they often experience codebook collapse,\nwhich reduces the effective codebook size and leads to suboptimal performance.\nTo address this problem, we introduce ERVQ, Enhanced Residual Vector\nQuantization, a novel enhancement strategy for the RVQ framework in neural\naudio codecs. ERVQ mitigates codebook collapse and boosts codec performance\nthrough both intra- and inter-codebook optimization. Intra-codebook\noptimization incorporates an online clustering strategy and a code balancing\nloss to ensure balanced and efficient codebook utilization. Inter-codebook\noptimization improves the diversity of quantized features by minimizing the\nsimilarity between successive quantizations. Our experiments show that ERVQ\nsignificantly enhances audio codec performance across different models,\nsampling rates, and bitrates, achieving superior quality and generalization\ncapabilities. It also achieves 100% codebook utilization on one of the most\nadvanced neural audio codecs. Further experiments indicate that audio codecs\nimproved by the ERVQ strategy can improve unified speech-and-text large\nlanguage models (LLMs). Specifically, there is a notable improvement in the\nnaturalness of generated speech in downstream zero-shot text-to-speech tasks.\nAudio samples are available here.", "published": "2024-10-16 08:21:37", "link": "http://arxiv.org/abs/2410.12359v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SWIM: An Attention-Only Model for Speech Quality Assessment Under\n  Subjective Variance", "abstract": "Speech quality is best evaluated by human feedback using mean opinion scores\n(MOS). However, variance in ratings between listeners can introduce noise in\nthe true quality label of an utterance. Currently, deep learning networks\nincluding convolutional, recurrent, and attention-based architectures have been\nexplored for quality estimation. This paper proposes an exclusively\nattention-based model involving a Swin Transformer for MOS estimation (SWIM).\nOur network captures local and global dependencies that reflect the acoustic\nproperties of an utterance. To counteract subjective variance in MOS labels, we\npropose a normal distance-based objective that accounts for standard deviation\nin each label, and we avail a multistage self-teaching strategy to improve\ngeneralization further. Our model is significantly more compact than existing\nattention-based networks for quality estimation. Finally, our experiments on\nthe Samsung Open Mean Opinion Score (SOMOS) dataset show improvement over\nexisting baseline models when trained from scratch.", "published": "2024-10-16 15:40:34", "link": "http://arxiv.org/abs/2410.12675v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AI-Enhanced Acoustic Analysis for Comprehensive Biodiversity Monitoring\n  and Assessment", "abstract": "This project proposes the development of a comprehensive real-time\nbiodiversity monitoring system that harnesses sound data through a network of\nacoustic sensors and advanced artificial intelligence algorithms. The system\nanalyzes sound recordings from various ecosystems to identify and classify\ndifferent species, providing valuable insights into ecosystem health and\nbiodiversity patterns while facilitating the detection of subtle changes in\nspecies presence and behavior over time. By addressing critical challenges such\nas noise pollution and species overlap, the system employs sophisticated\nfiltering and classification techniques to ensure accurate and reliable\nmonitoring, distinguishing between natural sounds and anthropogenic noise.\nUltimately, this initiative aims to enhance our understanding of biodiversity\ndynamics and provide essential information to support effective conservation\nstrategies and inform policy decisions, empowering stakeholders with actionable\ninsights to protect and preserve vital ecosystems.", "published": "2024-10-16 16:42:33", "link": "http://arxiv.org/abs/2410.12897v1", "categories": ["eess.AS", "68T05, 68U15, 92D20, 62H30, 68W20, 68Q32", "H.5.1; I.2.9; I.5.2; J.2; K.4.3; I.3.8"], "primary_category": "eess.AS"}
{"title": "Guided Speaker Embedding", "abstract": "This paper proposes a guided speaker embedding extraction system, which\nextracts speaker embeddings of the target speaker using speech activities of\ntarget and interference speakers as clues. Several methods for long-form\noverlapped multi-speaker audio processing are typically two-staged: i)\nsegment-level processing and ii) inter-segment speaker matching. Speaker\nembeddings are often used for the latter purpose. Typical speaker embedding\nextraction approaches only use single-speaker intervals to avoid corrupting the\nembeddings with speech from interference speakers. However, this often makes\nspeaker embeddings impossible to extract because sufficiently long\nnon-overlapping intervals are not always available. In this paper, we propose\nusing speaker activities as clues to extract the embedding of the\nspeaker-of-interest directly from overlapping speech. Specifically, we\nconcatenate the activity of target and non-target speakers to acoustic features\nbefore being fed to the model. We also condition the attention weights used for\npooling so that the attention weights of the intervals in which the target\nspeaker is inactive are zero. The effectiveness of the proposed method is\ndemonstrated in speaker verification and speaker diarization.", "published": "2024-10-16 03:01:05", "link": "http://arxiv.org/abs/2410.12182v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FlashAudio: Rectified Flows for Fast and High-Fidelity Text-to-Audio\n  Generation", "abstract": "Recent advancements in latent diffusion models (LDMs) have markedly enhanced\ntext-to-audio generation, yet their iterative sampling processes impose\nsubstantial computational demands, limiting practical deployment. While recent\nmethods utilizing consistency-based distillation aim to achieve few-step or\nsingle-step inference, their one-step performance is constrained by curved\ntrajectories, preventing them from surpassing traditional diffusion models. In\nthis work, we introduce FlashAudio with rectified flows to learn straight flow\nfor fast simulation. To alleviate the inefficient timesteps allocation and\nsuboptimal distribution of noise, FlashAudio optimizes the time distribution of\nrectified flow with Bifocal Samplers and proposes immiscible flow to minimize\nthe total distance of data-noise pairs in a batch vias assignment. Furthermore,\nto address the amplified accumulation error caused by the classifier-free\nguidance (CFG), we propose Anchored Optimization, which refines the guidance\nscale by anchoring it to a reference trajectory. Experimental results on\ntext-to-audio generation demonstrate that FlashAudio's one-step generation\nperformance surpasses the diffusion-based models with hundreds of sampling\nsteps on audio quality and enables a sampling speed of 400x faster than\nreal-time on a single NVIDIA 4090Ti GPU.", "published": "2024-10-16 06:06:30", "link": "http://arxiv.org/abs/2410.12266v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SF-Speech: Straightened Flow for Zero-Shot Voice Clone", "abstract": "Recently, neural ordinary differential equations (ODE) models trained with\nflow matching have achieved impressive performance on the zero-shot voice clone\ntask. Nevertheless, postulating standard Gaussian noise as the initial\ndistribution of ODE gives rise to numerous intersections within the fitted\ntargets of flow matching, which presents challenges to model training and\nenhances the curvature of the learned generated trajectories. These curved\ntrajectories restrict the capacity of ODE models for generating desirable\nsamples with a few steps. This paper proposes SF-Speech, a novel voice clone\nmodel based on ODE and in-context learning. Unlike the previous works,\nSF-Speech adopts a lightweight multi-stage module to generate a more\ndeterministic initial distribution for ODE. Without introducing any additional\nloss function, we effectively straighten the curved reverse trajectories of the\nODE model by jointly training it with the proposed module. Experiment results\non datasets of various scales show that SF-Speech outperforms the\nstate-of-the-art zero-shot TTS methods and requires only a quarter of the\nsolver steps, resulting in a generation speed approximately 3.7 times that of\nVoicebox and E2 TTS. Audio samples are available at the demo\npage\\footnote{[Online] Available: https://lixuyuan102.github.io/Demo/}.", "published": "2024-10-16 09:27:25", "link": "http://arxiv.org/abs/2410.12399v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SeQuiFi: Mitigating Catastrophic Forgetting in Speech Emotion\n  Recognition with Sequential Class-Finetuning", "abstract": "In this work, we introduce SeQuiFi, a novel approach for mitigating\ncatastrophic forgetting (CF) in speech emotion recognition (SER). SeQuiFi\nadopts a sequential class-finetuning strategy, where the model is fine-tuned\nincrementally on one emotion class at a time, preserving and enhancing\nretention for each class. While various state-of-the-art (SOTA) methods, such\nas regularization-based, memory-based, and weight-averaging techniques, have\nbeen proposed to address CF, it still remains a challenge, particularly with\ndiverse and multilingual datasets. Through extensive experiments, we\ndemonstrate that SeQuiFi significantly outperforms both vanilla fine-tuning and\nSOTA continual learning techniques in terms of accuracy and F1 scores on\nmultiple benchmark SER datasets, including CREMA-D, RAVDESS, Emo-DB, MESD, and\nSHEMO, covering different languages.", "published": "2024-10-16 13:41:18", "link": "http://arxiv.org/abs/2410.12567v1", "categories": ["eess.AS", "cs.SD", "68T45", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Beyond Speech and More: Investigating the Emergent Ability of Speech\n  Foundation Models for Classifying Physiological Time-Series Signals", "abstract": "Despite being trained exclusively on speech data, speech foundation models\n(SFMs) like Whisper have shown impressive performance in non-speech tasks such\nas audio classification. This is partly because speech shares some common\ntraits with audio, enabling SFMs to transfer effectively. In this study, we\npush the boundaries by evaluating SFMs on a more challenging out-of-domain\n(OOD) task: classifying physiological time-series signals. We test two key\nhypotheses: first, that SFMs can generalize to physiological signals by\ncapturing shared temporal patterns; second, that multilingual SFMs will\noutperform others due to their exposure to greater variability during\npre-training, leading to more robust, generalized representations. Our\nexperiments, conducted for stress recognition using ECG (Electrocardiogram),\nEMG (Electromyography), and EDA (Electrodermal Activity) signals, reveal that\nmodels trained on SFM-derived representations outperform those trained on raw\nphysiological signals. Among all models, multilingual SFMs achieve the highest\naccuracy, supporting our hypothesis and demonstrating their OOD capabilities.\nThis work positions SFMs as promising tools for new uncharted domains beyond\nspeech.", "published": "2024-10-16 15:04:30", "link": "http://arxiv.org/abs/2410.12645v1", "categories": ["eess.AS", "eess.SP", "68T45", "I.2.7"], "primary_category": "eess.AS"}
{"title": "HeightCeleb - an enrichment of VoxCeleb dataset with speaker height\n  information", "abstract": "Prediction of speaker's height is of interest for voice forensics,\nsurveillance, and automatic speaker profiling. Until now, TIMIT has been the\nmost popular dataset for training and evaluation of the height estimation\nmethods. In this paper, we introduce HeightCeleb, an extension to VoxCeleb,\nwhich is the dataset commonly used in speaker recognition tasks. This\nenrichment consists in adding information about the height of all 1251 speakers\nfrom VoxCeleb that has been extracted with an automated method from publicly\navailable sources. Such annotated data will enable the research community to\nutilize freely available speaker embedding extractors, pre-trained on VoxCeleb,\nto build more efficient speaker height estimators. In this work, we describe\nthe creation of the HeightCeleb dataset and show that using it enables to\nachieve state-of-the-art results on the TIMIT test set by using simple\nstatistical regression methods and embeddings obtained with a popular speaker\nmodel (without any additional fine-tuning).", "published": "2024-10-16 15:30:52", "link": "http://arxiv.org/abs/2410.12668v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-View Multi-Task Modeling with Speech Foundation Models for Speech\n  Forensic Tasks", "abstract": "Speech forensic tasks (SFTs), such as automatic speaker recognition (ASR),\nspeech emotion recognition (SER), gender recognition (GR), and age estimation\n(AE), find use in different security and biometric applications. Previous works\nhave applied various techniques, with recent studies focusing on applying\nspeech foundation models (SFMs) for improved performance. However, most prior\nefforts have centered on building individual models for each task separately,\ndespite the inherent similarities among these tasks. This isolated approach\nresults in higher computational resource requirements, increased costs, time\nconsumption, and maintenance challenges. In this study, we address these\nchallenges by employing a multi-task learning strategy. Firstly, we explore the\nvarious state-of-the-art (SOTA) SFMs by extracting their representations for\nlearning these SFTs and investigating their effectiveness at each task\nspecifically. Secondly, we analyze the performance of the extracted\nrepresentations on the SFTs in a multi-task learning framework. We observe a\ndecline in performance when SFTs are modeled together compared to individual\ntask-specific models, and as a remedy, we propose multi-view learning (MVL).\nViews are representations from different SFMs transformed into distinct\nabstract spaces by characteristics unique to each SFM. By leveraging MVL, we\nintegrate these diverse representations to capture complementary information\nacross tasks, enhancing the shared learning process. We introduce a new\nframework called TANGO (Task Alignment with iNter-view Gated Optimal transport)\nto implement this approach. With TANGO, we achieve the topmost performance in\ncomparison to individual SFM representations as well as baseline fusion\ntechniques across benchmark datasets such as CREMA-D, emo-DB, and BAVED.", "published": "2024-10-16 18:34:06", "link": "http://arxiv.org/abs/2410.12947v1", "categories": ["eess.AS", "cs.SD", "68T45", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Enhancing Speech Emotion Recognition through Segmental Average Pooling\n  of Self-Supervised Learning Features", "abstract": "Speech Emotion Recognition (SER) analyzes human emotions expressed through\nspeech. Self-supervised learning (SSL) offers a promising approach to SER by\nlearning meaningful representations from a large amount of unlabeled audio\ndata. However, existing SSL-based methods rely on Global Average Pooling (GAP)\nto represent audio signals, treating speech and non-speech segments equally.\nThis can lead to dilution of informative speech features by irrelevant\nnon-speech information. To address this, the paper proposes Segmental Average\nPooling (SAP), which selectively focuses on informative speech segments while\nignoring non-speech segments. By applying both GAP and SAP to SSL features, our\napproach utilizes overall speech signal information from GAP and specific\ninformation from SAP, leading to improved SER performance. Experiments show\nstate-of-the-art results on the IEMOCAP for English and superior performance on\nKEMDy19 for Korean datasets in both unweighted and weighted accuracies.", "published": "2024-10-16 10:00:57", "link": "http://arxiv.org/abs/2410.12416v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SiFiSinger: A High-Fidelity End-to-End Singing Voice Synthesizer based\n  on Source-filter Model", "abstract": "This paper presents an advanced end-to-end singing voice synthesis (SVS)\nsystem based on the source-filter mechanism that directly translates lyrical\nand melodic cues into expressive and high-fidelity human-like singing.\nSimilarly to VISinger 2, the proposed system also utilizes training paradigms\nevolved from VITS and incorporates elements like the fundamental pitch (F0)\npredictor and waveform generation decoder. To address the issue that the\ncoupling of mel-spectrogram features with F0 information may introduce errors\nduring F0 prediction, we consider two strategies. Firstly, we leverage\nmel-cepstrum (mcep) features to decouple the intertwined mel-spectrogram and F0\ncharacteristics. Secondly, inspired by the neural source-filter models, we\nintroduce source excitation signals as the representation of F0 in the SVS\nsystem, aiming to capture pitch nuances more accurately. Meanwhile,\ndifferentiable mcep and F0 losses are employed as the waveform decoder\nsupervision to fortify the prediction accuracy of speech envelope and pitch in\nthe generated speech. Experiments on the Opencpop dataset demonstrate efficacy\nof the proposed model in synthesis quality and intonation accuracy.", "published": "2024-10-16 13:18:45", "link": "http://arxiv.org/abs/2410.12536v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Computational Analysis of Pansori Singing", "abstract": "Pansori is one of the most representative vocal genres of Korean traditional\nmusic, which has an elaborated vocal melody line with strong vibrato. Although\nthe music is transmitted orally without any music notation, transcribing\npansori music in Western staff notation has been introduced for several\npurposes, such as documentation of music, education, or research. In this\npaper, we introduce computational analysis of pansori based on both audio and\ncorresponding transcription, how modern Music Information Retrieval tasks can\nbe used in analyzing traditional music and how it revealed different audio\ncharacteristics of what pansori contains.", "published": "2024-10-16 18:44:28", "link": "http://arxiv.org/abs/2410.12956v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic\n  Synchronization", "abstract": "Generating music that aligns with the visual content of a video has been a\nchallenging task, as it requires a deep understanding of visual semantics and\ninvolves generating music whose melody, rhythm, and dynamics harmonize with the\nvisual narratives. This paper presents MuVi, a novel framework that effectively\naddresses these challenges to enhance the cohesion and immersive experience of\naudio-visual content. MuVi analyzes video content through a specially designed\nvisual adaptor to extract contextually and temporally relevant features. These\nfeatures are used to generate music that not only matches the video's mood and\ntheme but also its rhythm and pacing. We also introduce a contrastive\nmusic-visual pre-training scheme to ensure synchronization, based on the\nperiodicity nature of music phrases. In addition, we demonstrate that our\nflow-matching-based music generator has in-context learning ability, allowing\nus to control the style and genre of the generated music. Experimental results\nshow that MuVi demonstrates superior performance in both audio quality and\ntemporal synchronization. The generated music video samples are available at\nhttps://muvi-v2m.github.io.", "published": "2024-10-16 18:44:56", "link": "http://arxiv.org/abs/2410.12957v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AADNet: An End-to-End Deep Learning Model for Auditory Attention\n  Decoding", "abstract": "Auditory attention decoding (AAD) is the process of identifying the attended\nspeech in a multi-talker environment using brain signals, typically recorded\nthrough electroencephalography (EEG). Over the past decade, AAD has undergone\ncontinuous development, driven by its promising application in neuro-steered\nhearing devices. Most AAD algorithms are relying on the increase in neural\nentrainment to the envelope of attended speech, as compared to unattended\nspeech, typically using a two-step approach. First, the algorithm predicts\nrepresentations of the attended speech signal envelopes; second, it identifies\nthe attended speech by finding the highest correlation between the predictions\nand the representations of the actual speech signals. In this study, we\nproposed a novel end-to-end neural network architecture, named AADNet, which\ncombines these two stages into a direct approach to address the AAD problem. We\ncompare the proposed network against the traditional approaches, including\nlinear stimulus reconstruction, canonical correlation analysis, and an\nalternative non-linear stimulus reconstruction using two different datasets.\nAADNet shows a significant performance improvement for both subject-specific\nand subject-independent models. Notably, the average subject-independent\nclassification accuracies from 56.1 % to 82.7 % with analysis window lengths\nranging from 1 to 40 seconds, respectively, show a significantly improved\nability to generalize to data from unseen subjects. These results highlight the\npotential of deep learning models for advancing AAD, with promising\nimplications for future hearing aids, assistive devices, and clinical\nassessments.", "published": "2024-10-16 21:39:45", "link": "http://arxiv.org/abs/2410.13059v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
