{"title": "Scaling Neural Machine Translation", "abstract": "Sequence to sequence learning models still require several days to reach\nstate of the art performance on large benchmark datasets using a single\nmachine. This paper shows that reduced precision and large batch training can\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\nimplementation. On WMT'14 English-German translation, we match the accuracy of\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\nfurther improve these results to 29.8 BLEU by training on the much larger\nParacrawl dataset. On the WMT'14 English-French task, we obtain a\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.", "published": "2018-06-01 04:33:16", "link": "http://arxiv.org/abs/1806.00187v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Some of Them Can be Guessed! Exploring the Effect of Linguistic Context\n  in Predicting Quantifiers", "abstract": "We study the role of linguistic context in predicting quantifiers (`few',\n`all'). We collect crowdsourced data from human participants and test various\nmodels in a local (single-sentence) and a global context (multi-sentence)\ncondition. Models significantly out-perform humans in the former setting and\nare only slightly better in the latter. While human performance improves with\nmore linguistic context (especially on proportional quantifiers), model\nperformance suffers. Models are very effective in exploiting lexical and\nmorpho-syntactic patterns; humans are better at genuinely understanding the\nmeaning of the (global) context.", "published": "2018-06-01 14:02:39", "link": "http://arxiv.org/abs/1806.00354v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7", "abstract": "Scene-aware dialog systems will be able to have conversations with users\nabout the objects and events around them. Progress on such systems can be made\nby integrating state-of-the-art technologies from multiple research areas\nincluding end-to-end dialog systems visual dialog, and video description. We\nintroduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. In\nthis challenge, which is one track of the 7th Dialog System Technology\nChallenges (DSTC7) workshop1, the task is to build a system that generates\nresponses in a dialog about an input video", "published": "2018-06-01 19:51:58", "link": "http://arxiv.org/abs/1806.00525v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Survey of Domain Adaptation for Neural Machine Translation", "abstract": "Neural machine translation (NMT) is a deep learning based approach for\nmachine translation, which yields the state-of-the-art translation performance\nin scenarios where large-scale parallel corpora are available. Although the\nhigh-quality and domain-specific translation is crucial in the real world,\ndomain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT\nperforms poorly in such scenarios. Domain adaptation that leverages both\nout-of-domain parallel corpora as well as monolingual corpora for in-domain\ntranslation, is very important for domain-specific translation. In this paper,\nwe give a comprehensive survey of the state-of-the-art domain adaptation\ntechniques for NMT.", "published": "2018-06-01 09:54:32", "link": "http://arxiv.org/abs/1806.00258v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Classification of Knowledge, Reasoning, and Context within\n  the ARC Dataset", "abstract": "The recent work of Clark et al. introduces the AI2 Reasoning Challenge (ARC)\nand the associated ARC dataset that partitions open domain, complex science\nquestions into an Easy Set and a Challenge Set. That paper includes an analysis\nof 100 questions with respect to the types of knowledge and reasoning required\nto answer them; however, it does not include clear definitions of these types,\nnor does it offer information about the quality of the labels. We propose a\ncomprehensive set of definitions of knowledge and reasoning types necessary for\nanswering the questions in the ARC dataset. Using ten annotators and a\nsophisticated annotation interface, we analyze the distribution of labels\nacross the Challenge Set and statistics related to them. Additionally, we\ndemonstrate that although naive information retrieval methods return sentences\nthat are irrelevant to answering the query, sufficient supporting text is often\npresent in the (ARC) corpus. Evaluating with human-selected relevant sentences\nimproves the performance of a neural machine comprehension model by 42 points.", "published": "2018-06-01 14:06:45", "link": "http://arxiv.org/abs/1806.00358v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Structurally Sparsified Backward Propagation for Faster Long Short-Term\n  Memory Training", "abstract": "Exploiting sparsity enables hardware systems to run neural networks faster\nand more energy-efficiently. However, most prior sparsity-centric optimization\ntechniques only accelerate the forward pass of neural networks and usually\nrequire an even longer training process with iterative pruning and retraining.\nWe observe that artificially inducing sparsity in the gradients of the gates in\nan LSTM cell has little impact on the training quality. Further, we can enforce\nstructured sparsity in the gate gradients to make the LSTM backward pass up to\n45% faster than the state-of-the-art dense approach and 168% faster than the\nstate-of-the-art sparsifying method on modern GPUs. Though the structured\nsparsifying method can impact the accuracy of a model, this performance gap can\nbe eliminated by mixing our sparse training method and the standard dense\ntraining method. Experimental results show that the mixed method can achieve\ncomparable results in a shorter time span than using purely dense training.", "published": "2018-06-01 19:08:30", "link": "http://arxiv.org/abs/1806.00512v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "OpenTag: Open Attribute Value Extraction from Product Profiles [Deep\n  Learning, Active Learning, Named Entity Recognition]", "abstract": "Extraction of missing attribute values is to find values describing an\nattribute of interest from a free text input. Most past related work on\nextraction of missing attribute values work with a closed world assumption with\nthe possible set of values known beforehand, or use dictionaries of values and\nhand-crafted features. How can we discover new attribute values that we have\nnever seen before? Can we do this with limited human annotation or supervision?\nWe study this problem in the context of product catalogs that often have\nmissing values for many attributes of interest.\n  In this work, we leverage product profile information such as titles and\ndescriptions to discover missing values of product attributes. We develop a\nnovel deep tagging model OpenTag for this extraction problem with the following\ncontributions: (1) we formalize the problem as a sequence tagging task, and\npropose a joint model exploiting recurrent neural networks (specifically,\nbidirectional LSTM) to capture context and semantics, and Conditional Random\nFields (CRF) to enforce tagging consistency, (2) we develop a novel attention\nmechanism to provide interpretable explanation for our model's decisions, (3)\nwe propose a novel sampling strategy exploring active learning to reduce the\nburden of human annotation. OpenTag does not use any dictionary or hand-crafted\nfeatures as in prior works. Extensive experiments in real-life datasets in\ndifferent domains show that OpenTag with our active learning strategy discovers\nnew attribute values from as few as 150 annotated samples (reduction in 3.3x\namount of annotation effort) with a high F-score of 83%, outperforming\nstate-of-the-art models.", "published": "2018-06-01 19:41:07", "link": "http://arxiv.org/abs/1806.01264v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Document Chunking and Learning Objective Generation for Instruction\n  Design", "abstract": "Instructional Systems Design is the practice of creating of instructional\nexperiences that make the acquisition of knowledge and skill more efficient,\neffective, and appealing. Specifically in designing courses, an hour of\ntraining material can require between 30 to 500 hours of effort in sourcing and\norganizing reference data for use in just the preparation of course material.\nIn this paper, we present the first system of its kind that helps reduce the\neffort associated with sourcing reference material and course creation. We\npresent algorithms for document chunking and automatic generation of learning\nobjectives from content, creating descriptive content metadata to improve\ncontent-discoverability. Unlike existing methods, the learning objectives\ngenerated by our system incorporate pedagogically motivated Bloom's verbs. We\ndemonstrate the usefulness of our methods using real world data from the\nbanking industry and through a live deployment at a large pharmaceutical\ncompany.", "published": "2018-06-01 06:47:28", "link": "http://arxiv.org/abs/1806.01351v2", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation for Electronic Health Records", "abstract": "A variety of methods existing for generating synthetic electronic health\nrecords (EHRs), but they are not capable of generating unstructured text, like\nemergency department (ED) chief complaints, history of present illness or\nprogress notes. Here, we use the encoder-decoder model, a deep learning\nalgorithm that features in many contemporary machine translation systems, to\ngenerate synthetic chief complaints from discrete variables in EHRs, like age\ngroup, gender, and discharge diagnosis. After being trained end-to-end on\nauthentic records, the model can generate realistic chief complaint text that\npreserves much of the epidemiological information in the original data. As a\nside effect of the model's optimization goal, these synthetic chief complaints\nare also free of relatively uncommon abbreviation and misspellings, and they\ninclude none of the personally-identifiable information (PII) that was in the\ntraining data, suggesting it may be used to support the de-identification of\ntext in EHRs. When combined with algorithms like generative adversarial\nnetworks (GANs), our model could be used to generate fully-synthetic EHRs,\nfacilitating data sharing between healthcare providers and researchers and\nimproving our ability to develop machine learning methods tailored to the\ninformation in healthcare data.", "published": "2018-06-01 12:01:48", "link": "http://arxiv.org/abs/1806.01353v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DNN Based Speech Enhancement for Unseen Noises Using Monte Carlo Dropout", "abstract": "In this work, we propose the use of dropouts as a Bayesian estimator for\nincreasing the generalizability of a deep neural network (DNN) for speech\nenhancement. By using Monte Carlo (MC) dropout, we show that the DNN performs\nbetter enhancement in unseen noise and SNR conditions. The DNN is trained on\nspeech corrupted with Factory2, M109, Babble, Leopard and Volvo noises at SNRs\nof 0, 5 and 10 dB and tested on speech with white, pink and factory1 noises.\nSpeech samples are obtained from the TIMIT database and noises from NOISEX-92.\nIn another experiment, we train five DNN models separately on speech corrupted\nwith Factory2, M109, Babble, Leopard and Volvo noises, at 0, 5 and 10 dB SNRs.\nThe model precision (estimated using MC dropout) is used as a proxy for squared\nerror to dynamically select the best of the DNN models based on their\nperformance on each frame of test data.", "published": "2018-06-01 19:21:05", "link": "http://arxiv.org/abs/1806.00516v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning a Latent Space of Multitrack Measures", "abstract": "Discovering and exploring the underlying structure of multi-instrumental\nmusic using learning-based approaches remains an open problem. We extend the\nrecent MusicVAE model to represent multitrack polyphonic measures as vectors in\na latent space. Our approach enables several useful operations such as\ngenerating plausible measures from scratch, interpolating between measures in a\nmusically meaningful way, and manipulating specific musical attributes. We also\nintroduce chord conditioning, which allows all of these operations to be\nperformed while keeping harmony fixed, and allows chords to be changed while\nmaintaining musical \"style\". By generating a sequence of measures over a\npredefined chord progression, our model can produce music with convincing\nlong-term structure. We demonstrate that our latent space model makes it\npossible to intuitively control and generate musical sequences with rich\ninstrumentation (see https://goo.gl/s2N7dV for generated audio).", "published": "2018-06-01 04:59:05", "link": "http://arxiv.org/abs/1806.00195v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Sparse Pursuit and Dictionary Learning for Blind Source Separation in\n  Polyphonic Music Recordings", "abstract": "We propose an algorithm for the blind separation of single-channel audio\nsignals. It is based on a parametric model that describes the spectral\nproperties of the sounds of musical instruments independently of pitch. We\ndevelop a novel sparse pursuit algorithm that can match the discrete frequency\nspectra from the recorded signal with the continuous spectra delivered by the\nmodel. We first use this algorithm to convert an STFT spectrogram from the\nrecording into a novel form of log-frequency spectrogram whose resolution\nexceeds that of the mel spectrogram. We then make use of the pitch-invariant\nproperties of that representation in order to identify the sounds of the\ninstruments via the same sparse pursuit method. As the model parameters which\ncharacterize the musical instruments are not known beforehand, we train a\ndictionary that contains them, using a modified version of Adam. Applying the\nalgorithm on various audio samples, we find that it is capable of producing\nhigh-quality separation results when the model assumptions are satisfied and\nthe instruments are clearly distinguishable, but combinations of instruments\nwith similar spectral characteristics pose a conceptual difficulty. While a key\nfeature of the model is that it explicitly models inharmonicity, its presence\ncan also still impede performance of the sparse pursuit algorithm. In general,\ndue to its pitch-invariance, our method is especially suitable for dealing with\nspectra from acoustic instruments, requiring only a minimal number of\nhyperparameters to be preset. Additionally, we demonstrate that the dictionary\nthat is constructed for one recording can be applied to a different recording\nwith similar instruments without additional training.", "published": "2018-06-01 10:28:50", "link": "http://arxiv.org/abs/1806.00273v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Performance Based Cost Functions for End-to-End Speech Separation", "abstract": "Recent neural network strategies for source separation attempt to model audio\nsignals by processing their waveforms directly. Mean squared error (MSE) that\nmeasures the Euclidean distance between waveforms of denoised speech and the\nground-truth speech, has been a natural cost-function for these approaches.\nHowever, MSE is not a perceptually motivated measure and may result in large\nperceptual discrepancies. In this paper, we propose and experiment with new\nloss functions for end-to-end source separation. These loss functions are\nmotivated by BSS\\_Eval and perceptual metrics like source to distortion ratio\n(SDR), source to interference ratio (SIR), source to artifact ratio (SAR) and\nshort-time objective intelligibility ratio (STOI). This enables the flexibility\nto mix and match these loss functions depending upon the requirements of the\ntask. Subjective listening tests reveal that combinations of the proposed cost\nfunctions help achieve superior separation performance as compared to\nstand-alone MSE and SDR costs.", "published": "2018-06-01 19:04:48", "link": "http://arxiv.org/abs/1806.00511v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Machines hear better when they have ears", "abstract": "Deep-neural-network (DNN) based noise suppression systems yield significant\nimprovements over conventional approaches such as spectral subtraction and\nnon-negative matrix factorization, but do not generalize well to noise\nconditions they were not trained for. In comparison to DNNs, humans show\nremarkable noise suppression capabilities that yield successful speech\nintelligibility under various adverse listening conditions and negative\nsignal-to-noise ratios (SNRs). Motivated by the excellent human performance,\nthis paper explores whether numerical models that simulate human cochlear\nsignal processing can be combined with DNNs to improve the robustness of DNN\nbased noise suppression systems. Five cochlear models were coupled to\nfully-connected and recurrent NN-based noise suppression systems and were\ntrained and evaluated for a variety of noise conditions using objective\nmetrics: perceptual speech quality (PESQ), segmental SNR and cepstral distance.\nThe simulations show that biophysically-inspired cochlear models improve the\ngeneralizability of DNN-based noise suppression systems for unseen noise and\nnegative SNRs. This approach thus leads to robust noise suppression systems\nthat are less sensitive to the noise type and noise level. Because cochlear\nmodels capture the intrinsic nonlinearities and dynamics of peripheral auditory\nprocessing, it is shown here that accounting for their deterministic signal\nprocessing improves machine hearing and avoids overtraining of multi-layer\nDNNs. We hence conclude that machines hear better when realistic cochlear\nmodels are used at the input of DNNs.", "published": "2018-06-01 15:44:35", "link": "http://arxiv.org/abs/1806.01145v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
