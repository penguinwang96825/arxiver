{"title": "Pivot Through English: Reliably Answering Multilingual Questions without\n  Document Retrieval", "abstract": "Existing methods for open-retrieval question answering in lower resource\nlanguages (LRLs) lag significantly behind English. They not only suffer from\nthe shortcomings of non-English document retrieval, but are reliant on\nlanguage-specific supervision for either the task or translation. We formulate\na task setup more realistic to available resources, that circumvents document\nretrieval to reliably transfer knowledge from English to lower resource\nlanguages. Assuming a strong English question answering model or database, we\ncompare and analyze methods that pivot through English: to map foreign queries\nto English and then English answers back to target language answers. Within\nthis task setup we propose Reranked Multilingual Maximal Inner Product Search\n(RM-MIPS), akin to semantic similarity retrieval over the English training set\nwith reranking, which outperforms the strongest baselines by 2.7% on XQuAD and\n6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy\nover state-of-the-art alternatives in challenging settings: low-resource\nlanguages, with extensive distractor data and query distribution misalignment.\nCircumventing retrieval, our analysis shows this approach offers rapid answer\ngeneration to almost any language off-the-shelf, without the need for any\nadditional training data in the target language.", "published": "2020-12-28 04:38:45", "link": "http://arxiv.org/abs/2012.14094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-Enhanced Pre-trained Model", "abstract": "We study the problem of leveraging the syntactic structure of text to enhance\npre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of\ntext either in the pre-training stage or in the fine-tuning stage, so that they\nsuffer from discrepancy between the two stages. Such a problem would lead to\nthe necessity of having human-annotated syntactic information, which limits the\napplication of existing methods to broader scenarios. To address this, we\npresent a model that utilizes the syntax of text in both pre-training and\nfine-tuning stages. Our model is based on Transformer with a syntax-aware\nattention layer that considers the dependency tree of the text. We further\nintroduce a new pre-training task of predicting the syntactic distance among\ntokens in the dependency tree. We evaluate the model on three downstream tasks,\nincluding relation classification, entity typing, and question answering.\nResults show that our model achieves state-of-the-art performance on six public\nbenchmark datasets. We have two major findings. First, we demonstrate that\ninfusing automatically produced syntax of text improves pre-trained models.\nSecond, global syntactic distances among tokens bring larger performance gains\ncompared to local head relations between contiguous tokens.", "published": "2020-12-28 06:48:04", "link": "http://arxiv.org/abs/2012.14116v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Generating Extended Summaries of Long Documents", "abstract": "Prior work in document summarization has mainly focused on generating short\nsummaries of a document. While this type of summary helps get a high-level view\nof a given document, it is desirable in some cases to know more detailed\ninformation about its salient points that can't fit in a short summary. This is\ntypically the case for longer documents such as a research paper, legal\ndocument, or a book. In this paper, we present a new method for generating\nextended summaries of long papers. Our method exploits hierarchical structure\nof the documents and incorporates it into an extractive summarization model\nthrough a multi-task learning approach. We then present our results on three\nlong summarization datasets, arXiv-Long, PubMed-Long, and Longsumm. Our method\noutperforms or matches the performance of strong baselines. Furthermore, we\nperform a comprehensive analysis over the generated results, shedding insights\non future research for long-form summary generation task. Our analysis shows\nthat our multi-tasking approach can adjust extraction probability distribution\nto the favor of summary-worthy sentences across diverse sections. Our datasets,\nand codes are publicly available at\nhttps://github.com/Georgetown-IR-Lab/ExtendedSumm", "published": "2020-12-28 08:10:28", "link": "http://arxiv.org/abs/2012.14136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fully Automated Manga Translation", "abstract": "We tackle the problem of machine translation of manga, Japanese comics. Manga\ntranslation involves two important problems in machine translation:\ncontext-aware and multimodal translation. Since text and images are mixed up in\nan unstructured fashion in Manga, obtaining context from the image is essential\nfor manga translation. However, it is still an open problem how to extract\ncontext from image and integrate into MT models. In addition, corpus and\nbenchmarks to train and evaluate such model is currently unavailable. In this\npaper, we make the following four contributions that establishes the foundation\nof manga translation research. First, we propose multimodal context-aware\ntranslation framework. We are the first to incorporate context information\nobtained from manga image. It enables us to translate texts in speech bubbles\nthat cannot be translated without using context information (e.g., texts in\nother speech bubbles, gender of speakers, etc.). Second, for training the\nmodel, we propose the approach to automatic corpus construction from pairs of\noriginal manga and their translations, by which large parallel corpus can be\nconstructed without any manual labeling. Third, we created a new benchmark to\nevaluate manga translation. Finally, on top of our proposed methods, we devised\na first comprehensive system for fully automated manga translation.", "published": "2020-12-28 15:20:52", "link": "http://arxiv.org/abs/2012.14271v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Panarchy: ripples of a boundary concept", "abstract": "How do social-ecological systems change over time? In 2002 Holling and\ncolleagues proposed the concept of Panarchy, which presented social-ecological\nsystems as an interacting set of adaptive cycles, each of which is produced by\nthe dynamic tensions between novelty and efficiency at multiple scales.\nInitially introduced as a conceptual framework and set of metaphors, panarchy\nhas gained the attention of scholars across many disciplines and its ideas\ncontinue to inspire further conceptual developments. Almost twenty years after\nthis concept was introduced we review how it has been used, tested, extended\nand revised. We do this by combining qualitative methods and machine learning.\nDocument analysis was used to code panarchy features that are commonly used in\nthe scientific literature (N = 42), a qualitative analysis that was\ncomplemented with topic modeling of 2177 documents. We find that the adaptive\ncycle is the feature of panarchy that has attracted the most attention.\nChallenges remain in empirically grounding the metaphor, but recent theoretical\nand empirical work offers some avenues for future research.", "published": "2020-12-28 15:47:45", "link": "http://arxiv.org/abs/2012.14312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BURT: BERT-inspired Universal Representation from Learning Meaningful\n  Segment", "abstract": "Although pre-trained contextualized language models such as BERT achieve\nsignificant performance on various downstream tasks, current language\nrepresentation still only focuses on linguistic objective at a specific\ngranularity, which may not applicable when multiple levels of linguistic units\nare involved at the same time. Thus this work introduces and explores the\nuniversal representation learning, i.e., embeddings of different levels of\nlinguistic unit in a uniform vector space. We present a universal\nrepresentation model, BURT (BERT-inspired Universal Representation from\nlearning meaningful segmenT), to encode different levels of linguistic unit\ninto the same vector space. Specifically, we extract and mask meaningful\nsegments based on point-wise mutual information (PMI) to incorporate different\ngranular objectives into the pre-training stage. We conduct experiments on\ndatasets for English and Chinese including the GLUE and CLUE benchmarks, where\nour model surpasses its baselines and alternatives on a wide range of\ndownstream tasks. We present our approach of constructing analogy datasets in\nterms of words, phrases and sentences and experiment with multiple\nrepresentation models to examine geometric properties of the learned vector\nspace through a task-independent evaluation. Finally, we verify the\neffectiveness of our unified pre-training strategy in two real-world text\nmatching scenarios. As a result, our model significantly outperforms existing\ninformation retrieval (IR) methods and yields universal representations that\ncan be directly applied to retrieval-based question-answering and natural\nlanguage generation tasks.", "published": "2020-12-28 16:02:28", "link": "http://arxiv.org/abs/2012.14320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Sentence Representation Learning with Conditional Masked\n  Language Model", "abstract": "This paper presents a novel training method, Conditional Masked Language\nModeling (CMLM), to effectively learn sentence representations on large scale\nunlabeled corpora. CMLM integrates sentence representation learning into MLM\ntraining by conditioning on the encoded vectors of adjacent sentences. Our\nEnglish CMLM model achieves state-of-the-art performance on SentEval, even\noutperforming models learned using supervised signals. As a fully unsupervised\nlearning method, CMLM can be conveniently extended to a broad range of\nlanguages and domains. We find that a multilingual CMLM model co-trained with\nbitext retrieval (BR) and natural language inference (NLI) tasks outperforms\nthe previous state-of-the-art multilingual models by a large margin, e.g. 10%\nimprovement upon baseline models on cross-lingual semantic search. We explore\nthe same language bias of the learned representations, and propose a simple,\npost-training and model agnostic approach to remove the language identifying\ninformation from the representation while still retaining sentence semantics.", "published": "2020-12-28 18:06:37", "link": "http://arxiv.org/abs/2012.14388v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Paragraph-level Multi-task Learning Model for Scientific\n  Fact-Verification", "abstract": "Even for domain experts, it is a non-trivial task to verify a scientific\nclaim by providing supporting or refuting evidence rationales. The situation\nworsens as misinformation is proliferated on social media or news websites,\nmanually or programmatically, at every moment. As a result, an automatic\nfact-verification tool becomes crucial for combating the spread of\nmisinformation. In this work, we propose a novel, paragraph-level, multi-task\nlearning model for the SciFact task by directly computing a sequence of\ncontextualized sentence embeddings from a BERT model and jointly training the\nmodel on rationale selection and stance prediction.", "published": "2020-12-28 21:51:31", "link": "http://arxiv.org/abs/2012.14500v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advanced Machine Learning Techniques for Fake News (Online\n  Disinformation) Detection: A Systematic Mapping Study", "abstract": "Fake news has now grown into a big problem for societies and also a major\nchallenge for people fighting disinformation. This phenomenon plagues\ndemocratic elections, reputations of individual persons or organizations, and\nhas negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US\nor Brazil). Hence, developing effective tools to fight this phenomenon by\nemploying advanced Machine Learning (ML) methods poses a significant challenge.\nThe following paper displays the present body of knowledge on the application\nof such intelligent tools in the fight against disinformation. It starts by\nshowing the historical perspective and the current role of fake news in the\ninformation war. Proposed solutions based solely on the work of experts are\nanalysed and the most important directions of the application of intelligent\nsystems in the detection of misinformation sources are pointed out.\nAdditionally, the paper presents some useful resources (mainly datasets useful\nwhen assessing ML solutions for fake news detection) and provides a short\noverview of the most important R&D projects related to this subject. The main\npurpose of this work is to analyse the current state of knowledge in detecting\nfake news; on the one hand to show possible solutions, and on the other hand to\nidentify the main challenges and methodological gaps to motivate future\nresearch.", "published": "2020-12-28 13:07:42", "link": "http://arxiv.org/abs/2101.01142v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Automatic Curriculum Learning With Over-repetition Penalty for Dialogue\n  Policy Learning", "abstract": "Dialogue policy learning based on reinforcement learning is difficult to be\napplied to real users to train dialogue agents from scratch because of the high\ncost. User simulators, which choose random user goals for the dialogue agent to\ntrain on, have been considered as an affordable substitute for real users.\nHowever, this random sampling method ignores the law of human learning, making\nthe learned dialogue policy inefficient and unstable. We propose a novel\nframework, Automatic Curriculum Learning-based Deep Q-Network (ACL-DQN), which\nreplaces the traditional random sampling method with a teacher policy model to\nrealize the dialogue policy for automatic curriculum learning. The teacher\nmodel arranges a meaningful ordered curriculum and automatically adjusts it by\nmonitoring the learning progress of the dialogue agent and the over-repetition\npenalty without any requirement of prior knowledge. The learning progress of\nthe dialogue agent reflects the relationship between the dialogue agent's\nability and the sampled goals' difficulty for sample efficiency. The\nover-repetition penalty guarantees the sampled diversity. Experiments show that\nthe ACL-DQN significantly improves the effectiveness and stability of dialogue\ntasks with a statistically significant margin. Furthermore, the framework can\nbe further improved by equipping with different curriculum schedules, which\ndemonstrates that the framework has strong generalizability.", "published": "2020-12-28 02:44:49", "link": "http://arxiv.org/abs/2012.14072v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Text Generation with Artificial Negative Examples", "abstract": "Neural text generation models conditioning on given input (e.g. machine\ntranslation and image captioning) are usually trained by maximum likelihood\nestimation of target text. However, the trained models suffer from various\ntypes of errors at inference time. In this paper, we propose to suppress an\narbitrary type of errors by training the text generation model in a\nreinforcement learning framework, where we use a trainable reward function that\nis capable of discriminating between references and sentences containing the\ntargeted type of errors. We create such negative examples by artificially\ninjecting the targeted errors to the references. In experiments, we focus on\ntwo error types, repeated and dropped tokens in model-generated text. The\nexperimental results show that our method can suppress the generation errors\nand achieve significant improvements on two machine translation and two image\ncaptioning tasks.", "published": "2020-12-28 07:25:10", "link": "http://arxiv.org/abs/2012.14124v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index\n  Sizes", "abstract": "Information Retrieval using dense low-dimensional representations recently\nbecame popular and showed out-performance to traditional sparse-representations\nlike BM25. However, no previous work investigated how dense representations\nperform with large index sizes. We show theoretically and empirically that the\nperformance for dense representations decreases quicker than sparse\nrepresentations for increasing index sizes. In extreme cases, this can even\nlead to a tipping point where at a certain index size sparse representations\noutperform dense representations. We show that this behavior is tightly\nconnected to the number of dimensions of the representations: The lower the\ndimension, the higher the chance for false positives, i.e. returning irrelevant\ndocuments.", "published": "2020-12-28 12:25:25", "link": "http://arxiv.org/abs/2012.14210v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced\n  Bengali Language", "abstract": "The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behaviour like online\nharassment, cyberbullying, and hate speech. Numerous works have been proposed\nto utilize textual data for social and anti-social behaviour analysis, by\npredicting the contexts mostly for highly-resourced languages like English.\nHowever, some languages are under-resourced, e.g., South Asian languages like\nBengali, that lack computational resources for accurate natural language\nprocessing (NLP). In this paper, we propose an explainable approach for hate\nspeech detection from the under-resourced Bengali language, which we called\nDeepHateExplainer. Bengali texts are first comprehensively preprocessed, before\nclassifying them into political, personal, geopolitical, and religious hates\nusing a neural ensemble method of transformer-based neural architectures (i.e.,\nmonolingual Bangla BERT-base, multilingual BERT-cased/uncased, and\nXLM-RoBERTa). Important(most and least) terms are then identified using\nsensitivity analysis and layer-wise relevance propagation(LRP), before\nproviding human-interpretable explanations. Finally, we compute\ncomprehensiveness and sufficiency scores to measure the quality of explanations\nw.r.t faithfulness. Evaluations against machine learning~(linear and tree-based\nmodels) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word\nembeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,\npersonal, geopolitical, and religious hates, respectively, outperforming both\nML and DNN baselines.", "published": "2020-12-28 16:46:03", "link": "http://arxiv.org/abs/2012.14353v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved\n  Transformer for Multi-Hop Explanation Ranking", "abstract": "Explainable question answering for science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. To\ncounter the limitations of methods that view each query-document pair in\nisolation, we propose the LSTM-Interleaved Transformer which incorporates\ncross-document interactions for improved multi-hop ranking. The LIT\narchitecture can leverage prior ranking positions in the re-ranking setting.\nOur model is competitive on the current leaderboard for the TextGraphs 2020\nshared task, achieving a test-set MAP of 0.5607, and would have gained third\nplace had we submitted before the competition deadline. Our code implementation\nis made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2020", "published": "2020-12-28 09:54:00", "link": "http://arxiv.org/abs/2012.14164v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "General Mechanism of Evolution Shared by Proteins and Words", "abstract": "Complex systems, such as life and languages, are governed by principles of\nevolution. The analogy and comparison between biology and\nlinguistics\\cite{alphafold2, RoseTTAFold, lang_virus, cell language, faculty1,\nlanguage of gene, Protein linguistics, dictionary, Grammar of pro_dom,\ncomplexity, genomics_nlp, InterPro, language modeling, Protein language\nmodeling} provide a computational foundation for characterizing and analyzing\nprotein sequences, human corpora, and their evolution. However, no general\nmathematical formula has been proposed so far to illuminate the origin of\nquantitative hallmarks shared by life and language. Here we show several new\nstatistical relationships shared by proteins and words, which inspire us to\nestablish a general mechanism of evolution with explicit formulations that can\nincorporate both old and new characteristics. We found natural selection can be\nquantified via the entropic formulation by the principle of least effort to\ndetermine the sequence variation that survives in evolution. Besides, the\norigin of power law behavior and how changes in the environment stimulate the\nemergence of new proteins and words can also be explained via the introduction\nof function connection network. Our results demonstrate not only the\ncorrespondence between genetics and linguistics over their different\nhierarchies but also new fundamental physical properties for the evolution of\ncomplex adaptive systems. We anticipate our statistical tests can function as\nquantitative criteria to examine whether an evolution theory of sequence is\nconsistent with the regularity of real data. In the meantime, their\ncorrespondence broadens the bridge to exchange existing knowledge, spurs new\ninterpretations, and opens Pandora's box to release several potentially\nrevolutionary challenges. For example, does linguistic arbitrariness conflict\nwith the dogma that structure determines function?", "published": "2020-12-28 15:46:19", "link": "http://arxiv.org/abs/2012.14309v2", "categories": ["q-bio.PE", "cond-mat.soft", "cs.CL", "physics.bio-ph"], "primary_category": "q-bio.PE"}
{"title": "Building Multi lingual TTS using Cross Lingual Voice Conversion", "abstract": "In this paper we propose a new cross-lingual Voice Conversion (VC) approach\nwhich can generate all speech parameters (MCEP, LF0, BAP) from one DNN model\nusing PPGs (Phonetic PosteriorGrams) extracted from inputted speech using\nseveral ASR acoustic models. Using the proposed VC method, we tried three\ndifferent approaches to build a multilingual TTS system without recording a\nmultilingual speech corpus. A listening test was carried out to evaluate both\nspeech quality (naturalness) and voice similarity between converted speech and\ntarget speech. The results show that Approach 1 achieved the highest level of\nnaturalness (3.28 MOS on a 5-point scale) and similarity (2.77 MOS).", "published": "2020-12-28 00:28:14", "link": "http://arxiv.org/abs/2012.14039v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lattice-Free MMI Adaptation Of Self-Supervised Pretrained Acoustic\n  Models", "abstract": "In this work, we propose lattice-free MMI (LFMMI) for supervised adaptation\nof self-supervised pretrained acoustic model. We pretrain a Transformer model\non thousand hours of untranscribed Librispeech data followed by supervised\nadaptation with LFMMI on three different datasets. Our results show that\nfine-tuning with LFMMI, we consistently obtain relative WER improvements of 10%\nand 35.3% on the clean and other test sets of Librispeech (100h), 10.8% on\nSwitchboard (300h), and 4.3% on Swahili (38h) and 4.4% on Tagalog (84h)\ncompared to the baseline trained only with supervised data.", "published": "2020-12-28 14:53:28", "link": "http://arxiv.org/abs/2012.14252v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
