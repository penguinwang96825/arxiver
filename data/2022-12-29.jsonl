{"title": "Sequence Generation with Label Augmentation for Relation Extraction", "abstract": "Sequence generation demonstrates promising performance in recent information\nextraction efforts, by incorporating large-scale pre-trained Seq2Seq models.\nThis paper investigates the merits of employing sequence generation in relation\nextraction, finding that with relation names or synonyms as generation targets,\ntheir textual semantics and the correlation (in terms of word sequence pattern)\namong them affect model performance. We then propose Relation Extraction with\nLabel Augmentation (RELA), a Seq2Seq model with automatic label augmentation\nfor RE. By saying label augmentation, we mean prod semantically synonyms for\neach relation name as the generation target. Besides, we present an in-depth\nanalysis of the Seq2Seq model's behavior when dealing with RE. Experimental\nresults show that RELA achieves competitive results compared with previous\nmethods on four RE datasets.", "published": "2022-12-29 11:28:05", "link": "http://arxiv.org/abs/2212.14266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reviewing Labels: Label Graph Network with Top-k Prediction Set for\n  Relation Extraction", "abstract": "The typical way for relation extraction is fine-tuning large pre-trained\nlanguage models on task-specific datasets, then selecting the label with the\nhighest probability of the output distribution as the final prediction.\nHowever, the usage of the Top-k prediction set for a given sample is commonly\noverlooked. In this paper, we first reveal that the Top-k prediction set of a\ngiven sample contains useful information for predicting the correct label. To\neffectively utilizes the Top-k prediction set, we propose Label Graph Network\nwith Top-k Prediction Set, termed as KLG. Specifically, for a given sample, we\nbuild a label graph to review candidate labels in the Top-k prediction set and\nlearn the connections between them. We also design a dynamic $k$-selection\nmechanism to learn more powerful and discriminative relation representation.\nOur experiments show that KLG achieves the best performances on three relation\nextraction datasets. Moreover, we observe that KLG is more effective in dealing\nwith long-tailed classes.", "published": "2022-12-29 11:44:42", "link": "http://arxiv.org/abs/2212.14270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error syntax aware augmentation of feedback comment generation dataset", "abstract": "This paper presents a solution to the GenChal 2022 shared task dedicated to\nfeedback comment generation for writing learning. In terms of this task given a\ntext with an error and a span of the error, a system generates an explanatory\nnote that helps the writer (language learner) to improve their writing skills.\nOur solution is based on fine-tuning the T5 model on the initial dataset\naugmented according to syntactical dependencies of the words located within\nindicated error span. The solution of our team \"nigula\" obtained second place\naccording to manual evaluation by the organizers.", "published": "2022-12-29 12:57:23", "link": "http://arxiv.org/abs/2212.14293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Political Rhetoric with Epistemic Stance Detection", "abstract": "Participants in political discourse employ rhetorical strategies -- such as\nhedging, attributions, or denials -- to display varying degrees of belief\ncommitments to claims proposed by themselves or others. Traditionally,\npolitical scientists have studied these epistemic phenomena through\nlabor-intensive manual content analysis. We propose to help automate such work\nthrough epistemic stance prediction, drawn from research in computational\nsemantics, to distinguish at the clausal level what is asserted, denied, or\nonly ambivalently suggested by the author or other mentioned entities (belief\nholders). We first develop a simple RoBERTa-based model for multi-source stance\npredictions that outperforms more complex state-of-the-art modeling. Then we\ndemonstrate its novel application to political science by conducting a\nlarge-scale analysis of the Mass Market Manifestos corpus of U.S. political\nopinion books, where we characterize trends in cited belief holders --\nrespected allies and opposed bogeymen -- across U.S. political ideologies.", "published": "2022-12-29 23:47:14", "link": "http://arxiv.org/abs/2212.14486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality\n  Hybrid", "abstract": "Multi-modal entity alignment (MMEA) aims to discover identical entities\nacross different knowledge graphs (KGs) whose entities are associated with\nrelevant images. However, current MMEA algorithms rely on KG-level modality\nfusion strategies for multi-modal entity representation, which ignores the\nvariations of modality preferences of different entities, thus compromising\nrobustness against noise in modalities such as blurry images and relations.\nThis paper introduces MEAformer, a multi-modal entity alignment transformer\napproach for meta modality hybrid, which dynamically predicts the mutual\ncorrelation coefficients among modalities for more fine-grained entity-level\nmodality fusion and alignment. Experimental results demonstrate that our model\nnot only achieves SOTA performance in multiple training scenarios, including\nsupervised, unsupervised, iterative, and low-resource settings, but also has a\nlimited number of parameters, efficient runtime, and interpretability. Our code\nis available at https://github.com/zjukg/MEAformer.", "published": "2022-12-29 20:49:58", "link": "http://arxiv.org/abs/2212.14454v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Macro-block dropout for improved regularization in training end-to-end\n  speech recognition models", "abstract": "This paper proposes a new regularization algorithm referred to as macro-block\ndropout. The overfitting issue has been a difficult problem in training large\nneural network models. The dropout technique has proven to be simple yet very\neffective for regularization by preventing complex co-adaptations during\ntraining. In our work, we define a macro-block that contains a large number of\nunits from the input to a Recurrent Neural Network (RNN). Rather than applying\ndropout to each unit, we apply random dropout to each macro-block. This\nalgorithm has the effect of applying different drop out rates for each layer\neven if we keep a constant average dropout rate, which has better\nregularization effects. In our experiments using Recurrent Neural\nNetwork-Transducer (RNN-T), this algorithm shows relatively 4.30 % and 6.13 %\nWord Error Rates (WERs) improvement over the conventional dropout on\nLibriSpeech test-clean and test-other. With an Attention-based Encoder-Decoder\n(AED) model, this algorithm shows relatively 4.36 % and 5.85 % WERs improvement\nover the conventional dropout on the same test sets.", "published": "2022-12-29 02:09:49", "link": "http://arxiv.org/abs/2212.14149v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Maximizing Use-Case Specificity through Precision Model Tuning", "abstract": "Language models have become increasingly popular in recent years for tasks\nlike information retrieval. As use-cases become oriented toward specific\ndomains, fine-tuning becomes default for standard performance. To fine-tune\nthese models for specific tasks and datasets, it is necessary to carefully tune\nthe model's hyperparameters and training techniques. In this paper, we present\nan in-depth analysis of the performance of four transformer-based language\nmodels on the task of biomedical information retrieval. The models we consider\nare DeepMind's RETRO (7B parameters), GPT-J (6B parameters), GPT-3 (175B\nparameters), and BLOOM (176B parameters). We compare their performance on the\nbasis of relevance, accuracy, and interpretability, using a large corpus of\n480000 research papers on protein structure/function prediction as our dataset.\nOur findings suggest that smaller models, with <10B parameters and fine-tuned\non domain-specific datasets, tend to outperform larger language models on\nhighly specific questions in terms of accuracy, relevancy, and interpretability\nby a significant margin (+50% on average). However, larger models do provide\ngenerally better results on broader prompts.", "published": "2022-12-29 07:50:14", "link": "http://arxiv.org/abs/2212.14206v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3"], "primary_category": "cs.CL"}
{"title": "GPT Takes the Bar Exam", "abstract": "Nearly all jurisdictions in the United States require a professional license\nexam, commonly referred to as \"the Bar Exam,\" as a precondition for law\npractice. To even sit for the exam, most jurisdictions require that an\napplicant completes at least seven years of post-secondary education, including\nthree years at an accredited law school. In addition, most test-takers also\nundergo weeks to months of further, exam-specific preparation. Despite this\nsignificant investment of time and capital, approximately one in five\ntest-takers still score under the rate required to pass the exam on their first\ntry. In the face of a complex task that requires such depth of knowledge, what,\nthen, should we expect of the state of the art in \"AI?\" In this research, we\ndocument our experimental evaluation of the performance of OpenAI's\n`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate\nmultiple choice (MBE) section of the exam. While we find no benefit in\nfine-tuning over GPT-3.5's zero-shot performance at the scale of our training\ndata, we do find that hyperparameter optimization and prompt engineering\npositively impacted GPT-3.5's zero-shot performance. For best prompt and\nparameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete\nNCBE MBE practice exam, significantly in excess of the 25% baseline guessing\nrate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's\nranking of responses is also highly-correlated with correctness; its top two\nand top three choices are correct 71% and 88% of the time, respectively,\nindicating very strong non-entailment performance. While our ability to\ninterpret these results is limited by nascent scientific understanding of LLMs\nand the proprietary nature of GPT, we believe that these results strongly\nsuggest that an LLM will pass the MBE component of the Bar Exam in the near\nfuture.", "published": "2022-12-29 18:19:43", "link": "http://arxiv.org/abs/2212.14402v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Multimodal Data Augmentation in Feature Space", "abstract": "The ability to jointly learn from multiple modalities, such as text, audio,\nand visual data, is a defining feature of intelligent systems. While there have\nbeen promising advances in designing neural networks to harness multimodal\ndata, the enormous success of data augmentation currently remains limited to\nsingle-modality tasks like image classification. Indeed, it is particularly\ndifficult to augment each modality while preserving the overall semantic\nstructure of the data; for example, a caption may no longer be a good\ndescription of an image after standard augmentations have been applied, such as\ntranslation. Moreover, it is challenging to specify reasonable transformations\nthat are not tailored to a particular modality. In this paper, we introduce\nLeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that\nautomatically learns to jointly augment multimodal data in feature space, with\nno constraints on the identities of the modalities or the relationship between\nmodalities. We show that LeMDA can (1) profoundly improve the performance of\nmultimodal deep learning architectures, (2) apply to combinations of modalities\nthat have not been previously considered, and (3) achieve state-of-the-art\nresults on a wide range of applications comprised of image, text, and tabular\ndata.", "published": "2022-12-29 20:39:36", "link": "http://arxiv.org/abs/2212.14453v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Political representation bias in DBpedia and Wikidata as a challenge for\n  downstream processing", "abstract": "Diversity Searcher is a tool originally developed to help analyse diversity\nin news media texts. It relies on a form of automated content analysis and thus\nrests on prior assumptions and depends on certain design choices related to\ndiversity and fairness. One such design choice is the external knowledge\nsource(s) used. In this article, we discuss implications that these sources can\nhave on the results of content analysis. We compare two data sources that\nDiversity Searcher has worked with - DBpedia and Wikidata - with respect to\ntheir ontological coverage and diversity, and describe implications for the\nresulting analyses of text corpora. We describe a case study of the relative\nover- or under-representation of Belgian political parties between 1990 and\n2020 in the English-language DBpedia, the Dutch-language DBpedia, and Wikidata,\nand highlight the many decisions needed with regard to the design of this data\nanalysis and the assumptions behind it, as well as implications from the\nresults. In particular, we came across a staggering over-representation of the\npolitical right in the English-language DBpedia.", "published": "2022-12-29 18:21:09", "link": "http://arxiv.org/abs/2301.00671v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Multimodal Sequential Generative Models for Semi-Supervised Language\n  Instruction Following", "abstract": "Agents that can follow language instructions are expected to be useful in a\nvariety of situations such as navigation. However, training neural\nnetwork-based agents requires numerous paired trajectories and languages. This\npaper proposes using multimodal generative models for semi-supervised learning\nin the instruction following tasks. The models learn a shared representation of\nthe paired data, and enable semi-supervised learning by reconstructing unpaired\ndata through the representation. Key challenges in applying the models to\nsequence-to-sequence tasks including instruction following are learning a\nshared representation of variable-length mulitimodal data and incorporating\nattention mechanisms. To address the problems, this paper proposes a novel\nnetwork architecture to absorb the difference in the sequence lengths of the\nmultimodal data. In addition, to further improve the performance, this paper\nshows how to incorporate the generative model-based approach with an existing\nsemi-supervised method called a speaker-follower model, and proposes a\nregularization term that improves inference using unpaired trajectories.\nExperiments on BabyAI and Room-to-Room (R2R) environments show that the\nproposed method improves the performance of instruction following by leveraging\nunpaired data, and improves the performance of the speaker-follower model by\n2\\% to 4\\% in R2R.", "published": "2022-12-29 03:23:43", "link": "http://arxiv.org/abs/2301.00676v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from\n  Style-Based TTS Models", "abstract": "One-shot voice conversion (VC) aims to convert speech from any source speaker\nto an arbitrary target speaker with only a few seconds of reference speech from\nthe target speaker. This relies heavily on disentangling the speaker's identity\nand speech content, a task that still remains challenging. Here, we propose a\nnovel approach to learning disentangled speech representation by transfer\nlearning from style-based text-to-speech (TTS) models. With cycle consistent\nand adversarial training, the style-based TTS models can perform\ntranscription-guided one-shot VC with high fidelity and similarity. By learning\nan additional mel-spectrogram encoder through a teacher-student knowledge\ntransfer and novel data augmentation scheme, our approach results in\ndisentangled speech representation without needing the input text. The\nsubjective evaluation shows that our approach can significantly outperform the\nprevious state-of-the-art one-shot voice conversion models in both naturalness\nand similarity.", "published": "2022-12-29 08:56:20", "link": "http://arxiv.org/abs/2212.14227v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
