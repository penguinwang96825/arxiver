{"title": "Automatic Related Work Generation: A Meta Study", "abstract": "Academic research is an exploration activity to solve problems that have\nnever been resolved before. By this nature, each academic research work is\nrequired to perform a literature review to distinguish its novelties that have\nnot been addressed by prior works. In natural language processing, this\nliterature review is usually conducted under the \"Related Work\" section. The\ntask of automatic related work generation aims to automatically generate the\n\"Related Work\" section given the rest of the research paper and a list of cited\npapers. Although this task was proposed over 10 years ago, it received little\nattention until very recently, when it was cast as a variant of the scientific\nmulti-document summarization problem. However, even today, the problems of\nautomatic related work and citation text generation are not yet standardized.\nIn this survey, we conduct a meta-study to compare the existing literature on\nrelated work generation from the perspectives of problem formulation, dataset\ncollection, methodological approach, performance evaluation, and future\nprospects to provide the reader insight into the progress of the\nstate-of-the-art studies, as well as and how future studies can be conducted.\nWe also survey relevant fields of study that we suggest future work to consider\nintegrating.", "published": "2022-01-06 01:16:38", "link": "http://arxiv.org/abs/2201.01880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAEG: Phrase-level Adversarial Example Generation for Neural Machine\n  Translation", "abstract": "While end-to-end neural machine translation (NMT) has achieved impressive\nprogress, noisy input usually leads models to become fragile and unstable.\nGenerating adversarial examples as the augmented data has been proved to be\nuseful to alleviate this problem. Existing methods for adversarial example\ngeneration (AEG) are word-level or character-level, which ignore the ubiquitous\nphrase structure. In this paper, we propose a Phrase-level Adversarial Example\nGeneration (PAEG) framework to enhance the robustness of the translation model.\nOur method further improves the gradient-based word-level AEG method by\nadopting a phrase-level substitution strategy. We verify our method on three\nbenchmarks, including LDC Chinese-English, IWSLT14 German-English, and WMT14\nEnglish-German tasks. Experimental results demonstrate that our approach\nsignificantly improves translation performance and robustness to noise compared\nto previous strong baselines.", "published": "2022-01-06 11:00:49", "link": "http://arxiv.org/abs/2201.02009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fortunately, Discourse Markers Can Enhance Language Models for Sentiment\n  Analysis", "abstract": "In recent years, pretrained language models have revolutionized the NLP\nworld, while achieving state of the art performance in various downstream\ntasks. However, in many cases, these models do not perform well when labeled\ndata is scarce and the model is expected to perform in the zero or few shot\nsetting. Recently, several works have shown that continual pretraining or\nperforming a second phase of pretraining (inter-training) which is better\naligned with the downstream task, can lead to improved results, especially in\nthe scarce data setting. Here, we propose to leverage sentiment-carrying\ndiscourse markers to generate large-scale weakly-labeled data, which in turn\ncan be used to adapt language models for sentiment analysis. Extensive\nexperimental results show the value of our approach on various benchmark\ndatasets, including the finance domain. Code, models and data are available at\nhttps://github.com/ibm/tslm-discourse-markers.", "published": "2022-01-06 12:33:47", "link": "http://arxiv.org/abs/2201.02026v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERN2: an advanced neural biomedical named entity recognition and\n  normalization tool", "abstract": "In biomedical natural language processing, named entity recognition (NER) and\nnamed entity normalization (NEN) are key tasks that enable the automatic\nextraction of biomedical entities (e.g. diseases and drugs) from the\never-growing biomedical literature. In this article, we present BERN2 (Advanced\nBiomedical Entity Recognition and Normalization), a tool that improves the\nprevious neural network-based NER tool by employing a multi-task NER model and\nneural network-based NEN models to achieve much faster and more accurate\ninference. We hope that our tool can help annotate large-scale biomedical texts\nfor various tasks such as biomedical knowledge graph construction.", "published": "2022-01-06 14:53:30", "link": "http://arxiv.org/abs/2201.02080v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConTrip: Consensus Sentiment review Analysis and Platform ratings in a\n  single score", "abstract": "People unequivocally employ reviews to decide on purchasing an item or an\nexperience on the internet. In that regard, the growing significance and number\nof opinions have led to the development of methods to assess their sentiment\ncontent automatically. However, it is not straightforward for the models to\ncreate a consensus value that embodies the agreement of the different reviews\nand differentiates across equal ratings for an item. Based on the approach\nproposed by Nguyen et al. in 2020, we derive a novel consensus value named\nConTrip that merges their consensus score and the overall rating of a platform\nfor an item. ConTrip lies in the rating range values, which makes it more\ninterpretable while maintaining the ability to differentiate across equally\nrated experiences. ConTrip is implemented and freely available under MIT\nlicense at https://github.com/pepebonet/contripscore", "published": "2022-01-06 15:50:34", "link": "http://arxiv.org/abs/2201.02113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Object Grounding Using Scene Graphs", "abstract": "Object grounding tasks aim to locate the target object in an image through\nverbal communications. Understanding human command is an important process\nneeded for effective human-robot communication. However, this is challenging\nbecause human commands can be ambiguous and erroneous. This paper aims to\ndisambiguate the human's referring expressions by allowing the agent to ask\nrelevant questions based on semantic data obtained from scene graphs. We test\nif our agent can use relations between objects from a scene graph to ask\nsemantically relevant questions that can disambiguate the original user\ncommand. In this paper, we present Incremental Grounding using Scene Graphs\n(IGSG), a disambiguation model that uses semantic data from an image scene\ngraph and linguistic structures from a language scene graph to ground objects\nbased on human command. Compared to the baseline, IGSG shows promising results\nin complex real-world scenes where there are multiple identical target objects.\nIGSG can effectively disambiguate ambiguous or wrong referring expressions by\nasking disambiguating questions back to the user.", "published": "2022-01-06 02:55:34", "link": "http://arxiv.org/abs/2201.01901v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HuSpaCy: an industrial-strength Hungarian natural language processing\n  toolkit", "abstract": "Although there are a couple of open-source language processing pipelines\navailable for Hungarian, none of them satisfies the requirements of today's NLP\napplications. A language processing pipeline should consist of close to\nstate-of-the-art lemmatization, morphosyntactic analysis, entity recognition\nand word embeddings. Industrial text processing applications have to satisfy\nnon-functional software quality requirements, what is more, frameworks\nsupporting multiple languages are more and more favored. This paper introduces\nHuSpaCy, an industry-ready Hungarian language processing toolkit. The presented\ntool provides components for the most important basic linguistic analysis\ntasks. It is open-source and is available under a permissive license. Our\nsystem is built upon spaCy's NLP components resulting in an easily usable, fast\nyet accurate application. Experiments confirm that HuSpaCy has high accuracy\nwhile maintaining resource-efficient prediction capabilities.", "published": "2022-01-06 07:49:45", "link": "http://arxiv.org/abs/2201.01956v2", "categories": ["cs.CL", "stat.ML", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Compact Bidirectional Transformer for Image Captioning", "abstract": "Most current image captioning models typically generate captions from left to\nright. This unidirectional property makes them can only leverage past context\nbut not future context. Though recent refinement-based models can exploit both\npast and future context by generating a new caption in the second stage based\non pre-retrieved or pre-generated captions in the first stage, the decoder of\nthese models generally consists of two networks~(i.e. a retriever or captioner\nin the first stage and a refiner in the second stage), which can only be\nexecuted sequentially. In this paper, we introduce a Compact Bidirectional\nTransformer model for image captioning that can leverage bidirectional context\nimplicitly and explicitly while the decoder can be executed parallelly.\nSpecifically, it is implemented by tightly coupling left-to-right(L2R) and\nright-to-left(R2L) flows into a single compact model~(i.e. implicitly) and\noptionally allowing interaction of the two flows(i.e. explicitly), while the\nfinal caption is chosen from either L2R or R2L flow in a sentence-level\nensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark\nand find that the compact architecture, which serves as a regularization for\nimplicitly exploiting bidirectional context, and the sentence-level ensemble\nplay more important roles than the explicit interaction mechanism. By combining\nwith word-level ensemble seamlessly, the effect of the sentence-level ensemble\nis further enlarged. We further extend the conventional one-flow self-critical\ntraining to the two-flows version under this architecture and achieve new\nstate-of-the-art results in comparison with non-vision-language-pretraining\nmodels. Source code is available at\n{\\color{magenta}\\url{https://github.com/YuanEZhou/CBTrans}}.", "published": "2022-01-06 09:23:18", "link": "http://arxiv.org/abs/2201.01984v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An exploratory experiment on Hindi, Bengali hate-speech detection and\n  transfer learning using neural networks", "abstract": "This work presents our approach to train a neural network to detect\nhate-speech texts in Hindi and Bengali. We also explore how transfer learning\ncan be applied to learning these languages, given that they have the same\norigin and thus, are similar to some extend. Even though the whole experiment\nwas conducted with low computational power, the obtained result is comparable\nto the results of other, more expensive, models. Furthermore, since the\ntraining data in use is relatively small and the two languages are almost\nentirely unknown to us, this work can be generalized as an effort to demystify\nlost or alien languages that no human is capable of understanding.", "published": "2022-01-06 10:13:28", "link": "http://arxiv.org/abs/2201.01997v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Training Vision Language BERTs with a Unified Conditional Model", "abstract": "Natural language BERTs are trained with language corpus in a self-supervised\nmanner. Unlike natural language BERTs, vision language BERTs need paired data\nto train, which restricts the scale of VL-BERT pretraining. We propose a\nself-training approach that allows training VL-BERTs from unlabeled image data.\nThe proposed method starts with our unified conditional model -- a vision\nlanguage BERT model that can perform zero-shot conditional generation. Given\ndifferent conditions, the unified conditional model can generate captions,\ndense captions, and even questions. We use the labeled image data to train a\nteacher model and use the trained model to generate pseudo captions on\nunlabeled image data. We then combine the labeled data and pseudo labeled data\nto train a student model. The process is iterated by putting the student model\nas a new teacher. By using the proposed self-training approach and only 300k\nunlabeled extra data, we are able to get competitive or even better\nperformances compared to the models of similar model size trained with 3\nmillion extra image data.", "published": "2022-01-06 11:00:52", "link": "http://arxiv.org/abs/2201.02010v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sales Time Series Analytics Using Deep Q-Learning", "abstract": "The article describes the use of deep Q-learning models in the problems of\nsales time series analytics. In contrast to supervised machine learning which\nis a kind of passive learning using historical data, Q-learning is a kind of\nactive learning with goal to maximize a reward by optimal sequence of actions.\nModel free Q-learning approach for optimal pricing strategies and supply-demand\nproblems was considered in the work. The main idea of the study is to show that\nusing deep Q-learning approach in time series analytics, the sequence of\nactions can be optimized by maximizing the reward function when the environment\nfor learning agent interaction can be modeled using the parametric model and in\nthe case of using the model which is based on the historical data. In the\npricing optimizing case study environment was modeled using sales dependence on\nextras price and randomly simulated demand. In the pricing optimizing case\nstudy, the environment was modeled using sales dependence on extra price and\nrandomly simulated demand. In the supply-demand case study, it was proposed to\nuse historical demand time series for environment modeling, agent states were\nrepresented by promo actions, previous demand values and weekly seasonality\nfeatures. Obtained results show that using deep Q-learning, we can optimize the\ndecision making process for price optimization and supply-demand problems.\nEnvironment modeling using parametric models and historical data can be used\nfor the cold start of learning agent. On the next steps, after the cold start,\nthe trained agent can be used in real business environment.", "published": "2022-01-06 13:48:40", "link": "http://arxiv.org/abs/2201.02058v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign\n  Language", "abstract": "Sign language is an essential resource enabling access to communication and\nproper socioemotional development for individuals suffering from disabling\nhearing loss. As this population is expected to reach 700 million by 2050, the\nimportance of the language becomes even more essential as it plays a critical\nrole to ensure the inclusion of such individuals in society. The Sign Language\nRecognition field aims to bridge the gap between users and non-users of sign\nlanguages. However, the scarcity in quantity and quality of datasets is one of\nthe main challenges limiting the exploration of novel approaches that could\nlead to significant advancements in this research area. Thus, this paper\ncontributes by introducing two new datasets for the American Sign Language: the\nfirst is composed of the three-dimensional representation of the signers and,\nthe second, by an unprecedented linguistics-based representation containing a\nset of phonological attributes of the signs.", "published": "2022-01-06 14:10:03", "link": "http://arxiv.org/abs/2201.02065v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Opinion Mining of Text in COVID-19 Issues along with Comparative\n  Study in ML, BERT & RNN", "abstract": "The global world is crossing a pandemic situation where this is a\ncatastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a\nglobal threat all over the 212 countries that people every day meet with mighty\nsituations. On the contrary, thousands of infected people live rich in\nmountains. Mental health is also affected by this worldwide coronavirus\nsituation. Due to this situation online sources made a communicative place that\ncommon people shares their opinion in any agenda. Such as affected news related\npositive and negative, financial issues, country and family crisis, lack of\nimport and export earning system etc. different kinds of circumstances are\nrecent trendy news in anywhere. Thus, vast amounts of text are produced within\nmoments therefore, in subcontinent areas the same as situation in other\ncountries and peoples opinion of text and situation also same but the language\nis different. This article has proposed some specific inputs along with Bangla\ntext comments from individual sources which can assure the goal of illustration\nthat machine learning outcome capable of building an assistive system. Opinion\nmining assistive system can be impactful in all language preferences possible.\nTo the best of our knowledge, the article predicted the Bangla input text on\nCOVID-19 issues proposed ML algorithms and deep learning models analysis also\ncheck the future reachability with a comparative analysis. Comparative analysis\nstates a report on text prediction accuracy is 91% along with ML algorithms and\n79% along with Deep Learning Models.", "published": "2022-01-06 15:59:20", "link": "http://arxiv.org/abs/2201.02119v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Large-scale protein-protein post-translational modification extraction\n  with distant supervision and confidence calibrated BioBERT", "abstract": "Protein-protein interactions (PPIs) are critical to normal cellular function\nand are related to many disease pathways. However, only 4% of PPIs are\nannotated with PTMs in biological knowledge databases such as IntAct, mainly\nperformed through manual curation, which is neither time nor cost-effective. We\nuse the IntAct PPI database to create a distant supervised dataset annotated\nwith interacting protein pairs, their corresponding PTM type, and associated\nabstracts from the PubMed database. We train an ensemble of BioBERT models -\ndubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of\nensemble average confidence approach with confidence variation to counteract\nthe effects of class imbalance to extract high confidence predictions. The\nPPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro\n41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low\nvariation to identify high quality predictions, tuning the predictions for\nprecision, we retained 19% of the test predictions with 100% precision. We\nevaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6\nmillion (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700\n(4584 unique) high confidence predictions. Of the 5700, human evaluation on a\nsmall randomly sampled subset shows that the precision drops to 33.7% despite\nconfidence calibration and highlights the challenges of generalisability beyond\nthe test set even with confidence calibration. We circumvent the problem by\nonly including predictions associated with multiple papers, improving the\nprecision to 58.8%. In this work, we highlight the benefits and challenges of\ndeep learning-based text mining in practice, and the need for increased\nemphasis on confidence calibration to facilitate human curation efforts.", "published": "2022-01-06 19:59:14", "link": "http://arxiv.org/abs/2201.02229v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Applying Word Embeddings to Measure Valence in Information Operations\n  Targeting Journalists in Brazil", "abstract": "Among the goals of information operations are to change the overall\ninformation environment vis-\\'a-vis specific actors. For example, \"trolling\ncampaigns\" seek to undermine the credibility of specific public figures,\nleading others to distrust them and intimidating these figures into silence. To\naccomplish these aims, information operations frequently make use of \"trolls\"\n-- malicious online actors who target verbal abuse at these figures. In Brazil,\nin particular, allies of Brazil's current president have been accused of\noperating a \"hate cabinet\" -- a trolling operation that targets journalists who\nhave alleged corruption by this politician and other members of his regime.\nLeading approaches to detecting harmful speech, such as Google's Perspective\nAPI, seek to identify specific messages with harmful content. While this\napproach is helpful in identifying content to downrank, flag, or remove, it is\nknown to be brittle, and may miss attempts to introduce more subtle biases into\nthe discourse. Here, we aim to develop a measure that might be used to assess\nhow targeted information operations seek to change the overall valence, or\nappraisal, of specific actors. Preliminary results suggest known campaigns\ntarget female journalists more so than male journalists, and that these\ncampaigns may leave detectable traces in overall Twitter discourse.", "published": "2022-01-06 21:52:20", "link": "http://arxiv.org/abs/2201.02257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Mandarin End-to-End Speech Recognition with Word N-gram\n  Language Model", "abstract": "Despite the rapid progress of end-to-end (E2E) automatic speech recognition\n(ASR), it has been shown that incorporating external language models (LMs) into\nthe decoding can further improve the recognition performance of E2E ASR\nsystems. To align with the modeling units adopted in E2E ASR systems,\nsubword-level (e.g., characters, BPE) LMs are usually used to cooperate with\ncurrent E2E ASR systems. However, the use of subword-level LMs will ignore the\nword-level information, which may limit the strength of the external LMs in E2E\nASR. Although several methods have been proposed to incorporate word-level\nexternal LMs in E2E ASR, these methods are mainly designed for languages with\nclear word boundaries such as English and cannot be directly applied to\nlanguages like Mandarin, in which each character sequence can have multiple\ncorresponding word sequences. To this end, we propose a novel decoding\nalgorithm where a word-level lattice is constructed on-the-fly to consider all\npossible word sequences for each partial hypothesis. Then, the LM score of the\nhypothesis is obtained by intersecting the generated lattice with an external\nword N-gram LM. The proposed method is examined on both Attention-based\nEncoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments\nsuggest that our method consistently outperforms subword-level LMs, including\nN-gram LM and neural network LM. We achieve state-of-the-art results on both\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by\n14.8% relatively on a 21K-hour Mandarin dataset.", "published": "2022-01-06 10:04:56", "link": "http://arxiv.org/abs/2201.01995v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Forming Predictive Features of Tweets for Decision-Making Support", "abstract": "The article describes the approaches for forming different predictive\nfeatures of tweet data sets and using them in the predictive analysis for\ndecision-making support. The graph theory as well as frequent itemsets and\nassociation rules theory is used for forming and retrieving different features\nfrom these datasests. The use of these approaches makes it possible to reveal a\nsemantic structure in tweets related to a specified entity. It is shown that\nquantitative characteristics of semantic frequent itemsets can be used in\npredictive regression models with specified target variables.", "published": "2022-01-06 13:25:57", "link": "http://arxiv.org/abs/2201.02049v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bayesian Regression Approach for Building and Stacking Predictive Models\n  in Time Series Analytics", "abstract": "The paper describes the use of Bayesian regression for building time series\nmodels and stacking different predictive models for time series. Using Bayesian\nregression for time series modeling with nonlinear trend was analyzed. This\napproach makes it possible to estimate an uncertainty of time series prediction\nand calculate value at risk characteristics. A hierarchical model for time\nseries using Bayesian regression has been considered. In this approach, one set\nof parameters is the same for all data samples, other parameters can be\ndifferent for different groups of data samples. Such an approach allows using\nthis model in the case of short historical data for specified time series, e.g.\nin the case of new stores or new products in the sales prediction problem. In\nthe study of predictive models stacking, the models ARIMA, Neural Network,\nRandom Forest, Extra Tree were used for the prediction on the first level of\nmodel ensemble. On the second level, time series predictions of these models on\nthe validation set were used for stacking by Bayesian regression. This approach\ngives distributions for regression coefficients of these models. It makes it\npossible to estimate the uncertainty contributed by each model to stacking\nresult. The information about these distributions allows us to select an\noptimal set of stacking models, taking into account the domain knowledge. The\nprobabilistic approach for stacking predictive models allows us to make risk\nassessment for the predictions that are important in a decision-making process.", "published": "2022-01-06 12:58:23", "link": "http://arxiv.org/abs/2201.02034v1", "categories": ["stat.AP", "cs.AI", "cs.CL", "cs.LG", "cs.NA", "math.NA"], "primary_category": "stat.AP"}
{"title": "Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization", "abstract": "Augmented reality devices have the potential to enhance human perception and\nenable other assistive functionalities in complex conversational environments.\nEffectively capturing the audio-visual context necessary for understanding\nthese social interactions first requires detecting and localizing the voice\nactivities of the device wearer and the surrounding people. These tasks are\nchallenging due to their egocentric nature: the wearer's head motion may cause\nmotion blur, surrounding people may appear in difficult viewing angles, and\nthere may be occlusions, visual clutter, audio noise, and bad lighting. Under\nthese conditions, previous state-of-the-art active speaker detection methods do\nnot give satisfactory results. Instead, we tackle the problem from a new\nsetting using both video and multi-channel microphone array audio. We propose a\nnovel end-to-end deep learning approach that is able to give robust voice\nactivity detection and localization results. In contrast to previous methods,\nour method localizes active speakers from all possible directions on the\nsphere, even outside the camera's field of view, while simultaneously detecting\nthe device wearer's own voice activity. Our experiments show that the proposed\nmethod gives superior results, can run in real time, and is robust against\nnoise and clutter.", "published": "2022-01-06 05:40:16", "link": "http://arxiv.org/abs/2201.01928v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Implementing simple spectral denoising for environmental audio\n  recordings", "abstract": "This technical report details changes applied to a noise filter to facilitate\nits application and improve its results. The filter is applied to denoise\nnatural sounds recorded in the wild and to generate an acoustic index used in\nsoundscape analysis.", "published": "2022-01-06 15:30:33", "link": "http://arxiv.org/abs/2201.02099v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
