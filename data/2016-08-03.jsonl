{"title": "Morphological Priors for Probabilistic Neural Word Embeddings", "abstract": "Word embeddings allow natural language processing systems to share\nstatistical information across related words. These embeddings are typically\nbased on distributional statistics, making it difficult for them to generalize\nto rare or unseen words. We propose to improve word embeddings by incorporating\nmorphological information, capturing shared sub-word features. Unlike previous\nwork that constructs word embeddings directly from morphemes, we combine\nmorphological and distributional information in a unified probabilistic\nframework, in which the word embedding is a latent variable. The morphological\ninformation provides a prior distribution on the latent word embeddings, which\nin turn condition a likelihood function over an observed corpus. This approach\nyields improvements on intrinsic word similarity evaluations, and also in the\ndownstream task of part-of-speech tagging.", "published": "2016-08-03 02:21:16", "link": "http://arxiv.org/abs/1608.01056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering\n  in Statistical Machine Translation", "abstract": "Reordering poses a major challenge in machine translation (MT) between two\nlanguages with significant differences in word order. In this paper, we present\na novel reordering approach utilizing sparse features based on dependency word\npairs. Each instance of these features captures whether two words, which are\nrelated by a dependency link in the source sentence dependency parse tree,\nfollow the same order or are swapped in the translation output. Experiments on\nChinese-to-English translation show a statistically significant improvement of\n1.21 BLEU point using our approach, compared to a state-of-the-art statistical\nMT system that incorporates prior reordering approaches.", "published": "2016-08-03 06:24:01", "link": "http://arxiv.org/abs/1608.01084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Quality of Hierarchical Clustering for Large Data Series", "abstract": "Brown clustering is a hard, hierarchical, bottom-up clustering of words in a\nvocabulary. Words are assigned to clusters based on their usage pattern in a\ngiven corpus. The resulting clusters and hierarchical structure can be used in\nconstructing class-based language models and for generating features to be used\nin NLP tasks. Because of its high computational cost, the most-used version of\nBrown clustering is a greedy algorithm that uses a window to restrict its\nsearch space. Like other clustering algorithms, Brown clustering finds a\nsub-optimal, but nonetheless effective, mapping of words to clusters. Because\nof its ability to produce high-quality, human-understandable cluster, Brown\nclustering has seen high uptake the NLP research community where it is used in\nthe preprocessing and feature generation steps.\n  Little research has been done towards improving the quality of Brown\nclusters, despite the greedy and heuristic nature of the algorithm. The\napproaches tried so far have focused on: studying the effect of the\ninitialisation in a similar algorithm; tuning the parameters used to define the\ndesired number of clusters and the behaviour of the algorithm; and including a\nseparate parameter to differentiate the window from the desired number of\nclusters. However, some of these approaches have not yielded significant\nimprovements in cluster quality.\n  In this thesis, a close analysis of the Brown algorithm is provided,\nrevealing important under-specifications and weaknesses in the original\nalgorithm. These have serious effects on cluster quality and reproducibility of\nresearch using Brown clustering. In the second part of the thesis, two\nmodifications are proposed. Finally, a thorough evaluation is performed,\nconsidering both the optimization criterion of Brown clustering and the\nperformance of the resulting class-based language models.", "published": "2016-08-03 16:12:23", "link": "http://arxiv.org/abs/1608.01238v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Query Clustering using Segment Specific Context Embeddings", "abstract": "This paper presents a novel query clustering approach to capture the broad\ninterest areas of users querying search engines. We make use of recent advances\nin NLP - word2vec and extend it to get query2vec, vector representations of\nqueries, based on query contexts, obtained from the top search results for the\nquery and use a highly scalable Divide & Merge clustering algorithm on top of\nthe query vectors, to get the clusters. We have tried this approach on a\nvariety of segments, including Retail, Travel, Health, Phones and found the\nclusters to be effective in discovering user's interest areas which have high\nmonetization potential.", "published": "2016-08-03 16:33:32", "link": "http://arxiv.org/abs/1608.01247v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning Online Alignments with Continuous Rewards Policy Gradient", "abstract": "Sequence-to-sequence models with soft attention had significant success in\nmachine translation, speech recognition, and question answering. Though capable\nand easy to use, they require that the entirety of the input sequence is\navailable at the beginning of inference, an assumption that is not valid for\ninstantaneous translation and speech recognition. To address this problem, we\npresent a new method for solving sequence-to-sequence problems using hard\nonline alignments instead of soft offline alignments. The online alignments\nmodel is able to start producing outputs without the need to first process the\nentire input sequence. A highly accurate online sequence-to-sequence model is\nuseful because it can be used to build an accurate voice-based instantaneous\ntranslator. Our model uses hard binary stochastic decisions to select the\ntimesteps at which outputs will be produced. The model is trained to produce\nthese stochastic decisions using a standard policy gradient method. In our\nexperiments, we show that this model achieves encouraging performance on TIMIT\nand Wall Street Journal (WSJ) speech recognition datasets.", "published": "2016-08-03 18:35:12", "link": "http://arxiv.org/abs/1608.01281v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Physical Metaphor to Study Semantic Drift", "abstract": "In accessibility tests for digital preservation, over time we experience\ndrifts of localized and labelled content in statistical models of evolving\nsemantics represented as a vector field. This articulates the need to detect,\nmeasure, interpret and model outcomes of knowledge dynamics. To this end we\nemploy a high-performance machine learning algorithm for the training of\nextremely large emergent self-organizing maps for exploratory data analysis.\nThe working hypothesis we present here is that the dynamics of semantic drifts\ncan be modeled on a relaxed version of Newtonian mechanics called social\nmechanics. By using term distances as a measure of semantic relatedness vs.\ntheir PageRank values indicating social importance and applied as variable\n`term mass', gravitation as a metaphor to express changes in the semantic\ncontent of a vector field lends a new perspective for experimentation. From\n`term gravitation' over time, one can compute its generating potential whose\nfluctuations manifest modifications in pairwise term similarity vs. social\nimportance, thereby updating Osgood's semantic differential. The dataset\nexamined is the public catalog metadata of Tate Galleries, London.", "published": "2016-08-03 19:34:13", "link": "http://arxiv.org/abs/1608.01298v1", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
