{"title": "Incorporating Both Distributional and Relational Semantics in Word\n  Representations", "abstract": "We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases.", "published": "2014-12-14 15:18:18", "link": "http://arxiv.org/abs/1412.4369v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent-Neural-Network for Language Detection on Twitter\n  Code-Switching Corpus", "abstract": "Mixed language data is one of the difficult yet less explored domains of\nnatural language processing. Most research in fields like machine translation\nor sentiment analysis assume monolingual input. However, people who are capable\nof using more than one language often communicate using multiple languages at\nthe same time. Sociolinguists believe this \"code-switching\" phenomenon to be\nsocially motivated. For example, to express solidarity or to establish\nauthority. Most past work depend on external tools or resources, such as\npart-of-speech tagging, dictionary look-up, or named-entity recognizers to\nextract rich features for training machine learning models. In this paper, we\ntrain recurrent neural networks with only raw features, and use word embedding\nto automatically learn meaningful representations. Using the same\nmixed-language Twitter corpus, our system is able to outperform the best\nSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in\naccuracy, or by 17% in error rate reduction.", "published": "2014-12-14 05:34:25", "link": "http://arxiv.org/abs/1412.4314v2", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Unsupervised Domain Adaptation with Feature Embeddings", "abstract": "Representation learning is the dominant technique for unsupervised domain\nadaptation, but existing approaches often require the specification of \"pivot\nfeatures\" that generalize across domains, which are selected by task-specific\nheuristics. We show that a novel but simple feature embedding approach provides\nbetter performance, by exploiting the feature template structure common in NLP\nproblems.", "published": "2014-12-14 17:44:58", "link": "http://arxiv.org/abs/1412.4385v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tools for Terminology Processing", "abstract": "Automatic terminology processing appeared 10 years ago when electronic\ncorpora became widely available. Such processing may be statistically or\nlinguistically based and produces terminology resources that can be used in a\nnumber of applications : indexing, information retrieval, technology watch,\netc. We present the tools that have been developed in the IRIN Institute. They\nall take as input texts (or collection of texts) and reflect different states\nof terminology processing: term acquisition, term recognition and term\nstructuring.", "published": "2014-12-14 20:03:36", "link": "http://arxiv.org/abs/1412.4401v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
