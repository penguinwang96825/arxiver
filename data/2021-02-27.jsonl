{"title": "N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking", "abstract": "Augmentation of task-oriented dialogues has followed standard methods used\nfor plain-text such as back-translation, word-level manipulation, and\nparaphrasing despite its richly annotated structure. In this work, we introduce\nan augmentation framework that utilizes belief state annotations to match turns\nfrom various dialogues and form new synthetic dialogues in a bottom-up manner.\nUnlike other augmentation strategies, it operates with as few as five examples.\nOur augmentation strategy yields significant improvements when both adapting a\nDST model to a new domain, and when adapting a language model to the DST task,\non evaluations with TRADE and TOD-BERT models. Further analysis shows that our\nmodel performs better on seen values during training, and it is also more\nrobust to unseen values. We conclude that exploiting belief state annotations\nenhances dialogue augmentation and results in improved models in n-shot\ntraining scenarios.", "published": "2021-02-27 18:55:12", "link": "http://arxiv.org/abs/2103.00293v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Harmful Content On Online Platforms: What Platforms Need Vs.\n  Where Research Efforts Go", "abstract": "The proliferation of harmful content on online platforms is a major societal\nproblem, which comes in many different forms including hate speech, offensive\nlanguage, bullying and harassment, misinformation, spam, violence, graphic\ncontent, sexual abuse, self harm, and many other. Online platforms seek to\nmoderate such content to limit societal harm, to comply with legislation, and\nto create a more inclusive environment for their users. Researchers have\ndeveloped different methods for automatically detecting harmful content, often\nfocusing on specific sub-problems or on narrow communities, as what is\nconsidered harmful often depends on the platform and on the context. We argue\nthat there is currently a dichotomy between what types of harmful content\nonline platforms seek to curb, and what research efforts there are to\nautomatically detect such content. We thus survey existing methods as well as\ncontent moderation policies by online platforms in this light and we suggest\ndirections for future work.", "published": "2021-02-27 08:01:10", "link": "http://arxiv.org/abs/2103.00153v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "EDS-MEMBED: Multi-sense embeddings based on enhanced distributional\n  semantic structures via a graph walk over word senses", "abstract": "Several language applications often require word semantics as a core part of\ntheir processing pipeline, either as precise meaning inference or semantic\nsimilarity. Multi-sense embeddings (M-SE) can be exploited for this important\nrequirement. M-SE seeks to represent each word by their distinct senses in\norder to resolve the conflation of meanings of words as used in different\ncontexts. Previous works usually approach this task by training a model on a\nlarge corpus and often ignore the effect and usefulness of the semantic\nrelations offered by lexical resources. However, even with large training data,\ncoverage of all possible word senses is still an issue. In addition, a\nconsiderable percentage of contextual semantic knowledge are never learned\nbecause a huge amount of possible distributional semantic structures are never\nexplored. In this paper, we leverage the rich semantic structures in WordNet\nusing a graph-theoretic walk technique over word senses to enhance the quality\nof multi-sense embeddings. This algorithm composes enriched texts from the\noriginal texts. Furthermore, we derive new distributional semantic similarity\nmeasures for M-SE from prior ones. We adapt these measures to word sense\ndisambiguation (WSD) aspect of our experiment. We report evaluation results on\n11 benchmark datasets involving WSD and Word Similarity tasks and show that our\nmethod for enhancing distributional semantic structures improves embeddings\nquality on the baselines. Despite the small training data, it achieves\nstate-of-the-art performance on some of the datasets.", "published": "2021-02-27 14:36:55", "link": "http://arxiv.org/abs/2103.00232v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey on Stance Detection for Mis- and Disinformation Identification", "abstract": "Understanding attitudes expressed in texts, also known as stance detection,\nplays an important role in systems for detecting false information online, be\nit misinformation (unintentionally false) or disinformation (intentionally\nfalse information). Stance detection has been framed in different ways,\nincluding (a) as a component of fact-checking, rumour detection, and detecting\npreviously fact-checked claims, or (b) as a task in its own right. While there\nhave been prior efforts to contrast stance detection with other related tasks\nsuch as argumentation mining and sentiment analysis, there is no existing\nsurvey on examining the relationship between stance detection and mis- and\ndisinformation detection. Here, we aim to bridge this gap by reviewing and\nanalysing existing work in this area, with mis- and disinformation in focus,\nand discussing lessons learnt and future challenges.", "published": "2021-02-27 15:27:22", "link": "http://arxiv.org/abs/2103.00242v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improving Longer-range Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) is a pivotal component in task-oriented\ndialogue systems. While it is relatively easy for a DST model to capture belief\nstates in short conversations, the task of DST becomes more challenging as the\nlength of a dialogue increases due to the injection of more distracting\ncontexts. In this paper, we aim to improve the overall performance of DST with\na special focus on handling longer dialogues. We tackle this problem from three\nperspectives: 1) A model designed to enable hierarchical slot status\nprediction; 2) Balanced training procedure for generic and task-specific\nlanguage understanding; 3) Data perturbation which enhances the model's ability\nin handling longer conversations. We conduct experiments on the MultiWOZ\nbenchmark, and demonstrate the effectiveness of each component via a set of\nablation tests, especially on longer conversations.", "published": "2021-02-27 02:44:28", "link": "http://arxiv.org/abs/2103.00109v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COVID-19 Tweets Analysis through Transformer Language Models", "abstract": "Understanding the public sentiment and perception in a healthcare crisis is\nessential for developing appropriate crisis management techniques. While some\nstudies have used Twitter data for predictive modelling during COVID-19,\nfine-grained sentiment analysis of the opinion of people on social media during\nthis pandemic has not yet been done. In this study, we perform an in-depth,\nfine-grained sentiment analysis of tweets in COVID-19. For this purpose, we\nperform supervised training of four transformer language models on the\ndownstream task of multi-label classification of tweets into seven tone\nclasses: [confident, anger, fear, joy, sadness, analytical, tentative]. We\nachieve a LRAP (Label Ranking Average Precision) score of 0.9267 through\nRoBERTa. This trained transformer model is able to correctly predict, with high\naccuracy, the tone of a tweet. We then leverage this model for predicting tones\nfor 200,000 tweets on COVID-19. We then perform a country-wise analysis of the\ntone of tweets, and extract useful indicators of the psychological condition\nabout the people in this pandemic.", "published": "2021-02-27 12:06:33", "link": "http://arxiv.org/abs/2103.00199v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting ultrasound tongue imaging for the automatic detection of\n  speech articulation errors", "abstract": "Speech sound disorders are a common communication impairment in childhood.\nBecause speech disorders can negatively affect the lives and the development of\nchildren, clinical intervention is often recommended. To help with diagnosis\nand treatment, clinicians use instrumented methods such as spectrograms or\nultrasound tongue imaging to analyse speech articulations. Analysis with these\nmethods can be laborious for clinicians, therefore there is growing interest in\nits automation. In this paper, we investigate the contribution of ultrasound\ntongue imaging for the automatic detection of speech articulation errors. Our\nsystems are trained on typically developing child speech and augmented with a\ndatabase of adult speech using audio and ultrasound. Evaluation on typically\ndeveloping speech indicates that pre-training on adult speech and jointly using\nultrasound and audio gives the best results with an accuracy of 86.9%. To\nevaluate on disordered speech, we collect pronunciation scores from experienced\nspeech and language therapists, focusing on cases of velar fronting and gliding\nof /r/. The scores show good inter-annotator agreement for velar fronting, but\nnot for gliding errors. For automatic velar fronting error detection, the best\nresults are obtained when jointly using ultrasound and audio. The best system\ncorrectly detects 86.6% of the errors identified by experienced clinicians. Out\nof all the segments identified as errors by the best system, 73.2% match errors\nidentified by clinicians. Results on automatic gliding detection are harder to\ninterpret due to poor inter-annotator agreement, but appear promising. Overall\nfindings suggest that automatic detection of speech articulation errors has\npotential to be integrated into ultrasound intervention software for\nautomatically quantifying progress during speech therapy.", "published": "2021-02-27 21:16:45", "link": "http://arxiv.org/abs/2103.00324v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "Silent versus modal multi-speaker speech recognition from ultrasound and\n  video", "abstract": "We investigate multi-speaker speech recognition from ultrasound images of the\ntongue and video images of the lips. We train our systems on imaging data from\nmodal speech, and evaluate on matched test sets of two speaking modes: silent\nand modal speech. We observe that silent speech recognition from imaging data\nunderperforms compared to modal speech recognition, likely due to a\nspeaking-mode mismatch between training and testing. We improve silent speech\nrecognition performance using techniques that address the domain mismatch, such\nas fMLLR and unsupervised model adaptation. We also analyse the properties of\nsilent and modal speech in terms of utterance duration and the size of the\narticulatory space. To estimate the articulatory space, we compute the convex\nhull of tongue splines, extracted from ultrasound tongue images. Overall, we\nobserve that the duration of silent speech is longer than that of modal speech,\nand that silent speech covers a smaller articulatory space than modal speech.\nAlthough these two properties are statistically significant across speaking\nmodes, they do not directly correlate with word error rates from speech\nrecognition.", "published": "2021-02-27 21:34:48", "link": "http://arxiv.org/abs/2103.00333v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network", "abstract": "Mean opinion score (MOS) is a popular subjective metric to assess the quality\nof synthesized speech, and usually involves multiple human judges to evaluate\neach speech utterance. To reduce the labor cost in MOS test, multiple methods\nhave been proposed to automatically predict MOS scores. To our knowledge, for a\nspeech utterance, all previous works only used the average of multiple scores\nfrom different judges as the training target and discarded the score of each\nindividual judge, which did not well exploit the precious MOS training data. In\nthis paper, we propose MBNet, a MOS predictor with a mean subnet and a bias\nsubnet to better utilize every judge score in MOS datasets, where the mean\nsubnet is used to predict the mean score of each utterance similar to that in\nprevious works, and the bias subnet to predict the bias score (the difference\nbetween the mean score and each individual judge score) and capture the\npersonal preference of individual judges. Experiments show that compared with\nMOSNet baseline that only leverages mean score for training, MBNet improves the\nsystem-level spearmans rank correlation co-efficient (SRCC) by 2.9% on VCC 2018\ndataset and 6.7% on VCC 2016 dataset.", "published": "2021-02-27 02:48:26", "link": "http://arxiv.org/abs/2103.00110v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Genre Bars", "abstract": "Music Genres, as a popular meta-data of music, are very useful to organize,\nexplore or search music datasets. Soft music genres are weighted multiple-genre\nannotations to songs. In this initial work, we propose horizontally stacked bar\ncharts to represent a music dataset annotated by these soft music genres. For\nthis purpose, we take an example of a toy dataset consisting of songs labelled\nwith help of three music genres; Blues, Jazz and Country. We demonstrate how\nsuch a stacked bar chart can be used as a slider for user-input in an\ninterface. We implement this by embedding this genre bar in a streaming\napplication prototype and show its utility in choosing playlists. We finally\nconclude by proposing further work and future explorations on our proposed\npreliminary research.", "published": "2021-02-27 05:05:12", "link": "http://arxiv.org/abs/2103.00129v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Expert Decision Support System for aeroacoustic source type\n  identification using clustering", "abstract": "This paper presents an Expert Decision Support System for the identification\nof time-invariant, aeroacoustic source types. The system comprises two steps:\nfirst, acoustic properties are calculated based on spectral and spatial\ninformation. Second, clustering is performed based on these properties. The\nclustering aims at helping and guiding an expert for quick identification of\ndifferent source types, providing an understanding of how sources differ. This\nsupports the expert in determining similar or atypical behavior. A variety of\nfeatures are proposed for capturing the characteristics of the sources. These\nfeatures represent aeroacoustic properties that can be interpreted by both the\nmachine and by experts. The features are independent of the absolute Mach\nnumber which enables the proposed method to cluster data measured at different\nflow configurations. The method is evaluated on deconvolved beamforming data\nfrom two scaled airframe half-model measurements. For this exemplary data, the\nproposed support system method results in clusters that mostly correspond to\nthe source types identified by the authors. The clustering also provides the\nmean feature values and the cluster hierarchy for each cluster and for each\ncluster member a clustering confidence. This additional information makes the\nresults transparent and allows the expert to understand the clustering choices.", "published": "2021-02-27 15:47:59", "link": "http://arxiv.org/abs/2103.00255v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
