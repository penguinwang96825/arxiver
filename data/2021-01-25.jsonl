{"title": "MadDog: A Web-based System for Acronym Identification and Disambiguation", "abstract": "Acronyms and abbreviations are the short-form of longer phrases and they are\nubiquitously employed in various types of writing. Despite their usefulness to\nsave space in writing and reader's time in reading, they also provide\nchallenges for understanding the text especially if the acronym is not defined\nin the text or if it is used far from its definition in long texts. To\nalleviate this issue, there are considerable efforts both from the research\ncommunity and software developers to build systems for identifying acronyms and\nfinding their correct meanings in the text. However, none of the existing works\nprovide a unified solution capable of processing acronyms in various domains\nand to be publicly available. Thus, we provide the first web-based acronym\nidentification and disambiguation system which can process acronyms from\nvarious domains including scientific, biomedical, and general domains. The\nweb-based system is publicly available at http://iq.cs.uoregon.edu:5000 and a\ndemo video is available at https://youtu.be/IkSh7LqI42M. The system source code\nis also available at https://github.com/amirveyseh/MadDog.", "published": "2021-01-25 04:49:25", "link": "http://arxiv.org/abs/2101.09893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GP: Context-free Grammar Pre-training for Text-to-SQL Parsers", "abstract": "A new method for Text-to-SQL parsing, Grammar Pre-training (GP), is proposed\nto decode deep relations between question and database. Firstly, to better\nutilize the information of databases, a random value is added behind a question\nword which is recognized as a column, and the new sentence serves as the model\ninput. Secondly, initialization of vectors for decoder part is optimized, with\nreference to the former encoding so that question information can be concerned.\nFinally, a new approach called flooding level is adopted to get the non-zero\ntraining loss which can generalize better results. By encoding the sentence\nwith GRAPPA and RAT-SQL model, we achieve better performance on spider, a\ncross-DB Text-to-SQL dataset (72.8 dev, 69.8 test). Experiments show that our\nmethod is easier to converge during training and has excellent robustness.", "published": "2021-01-25 05:41:31", "link": "http://arxiv.org/abs/2101.09901v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EGFI: Drug-Drug Interaction Extraction and Generation with Fusion of\n  Enriched Entity and Sentence Information", "abstract": "The rapid growth in literature accumulates diverse and yet comprehensive\nbiomedical knowledge hidden to be mined such as drug interactions. However, it\nis difficult to extract the heterogeneous knowledge to retrieve or even\ndiscover the latest and novel knowledge in an efficient manner. To address such\na problem, we propose EGFI for extracting and consolidating drug interactions\nfrom large-scale medical literature text data. Specifically, EGFI consists of\ntwo parts: classification and generation. In the classification part, EGFI\nencompasses the language model BioBERT which has been comprehensively\npre-trained on biomedical corpus. In particular, we propose the multi-head\nattention mechanism and pack BiGRU to fuse multiple semantic information for\nrigorous context modeling. In the generation part, EGFI utilizes another\npre-trained language model BioGPT-2 where the generation sentences are selected\nbased on filtering rules. We evaluated the classification part on \"DDIs 2013\"\ndataset and \"DTIs\" dataset, achieving the FI score of 0.842 and 0.720\nrespectively. Moreover, we applied the classification part to distinguish\nhigh-quality generated sentences and verified with the exiting growth truth to\nconfirm the filtered sentences. The generated sentences that are not recorded\nin DrugBank and DDIs 2013 dataset also demonstrate the potential of EGFI to\nidentify novel drug relationships.", "published": "2021-01-25 06:52:29", "link": "http://arxiv.org/abs/2101.09914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and\n  Wikidata", "abstract": "In this paper, we propose CHOLAN, a modular approach to target end-to-end\nentity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two\ntransformer-based models integrated sequentially to accomplish the EL task. The\nfirst transformer model identifies surface forms (entity mentions) in a given\ntext. For each mention, a second transformer model is employed to classify the\ntarget entity among a predefined candidates list. The latter transformer is fed\nby an enriched context captured from the sentence (i.e. local context), and\nentity description gained from Wikipedia. Such external contexts have not been\nused in the state of the art EL approaches. Our empirical study was conducted\non two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical\nresults suggest that CHOLAN outperforms state-of-the-art approaches on standard\ndatasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx.", "published": "2021-01-25 09:19:58", "link": "http://arxiv.org/abs/2101.09969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Key-phrase Extraction and Clustering for Classification\n  Scheme in Scientific Publications", "abstract": "Several methods have been explored for automating parts of Systematic Mapping\n(SM) and Systematic Review (SR) methodologies. Challenges typically evolve\naround the gaps in semantic understanding of text, as well as lack of domain\nand background knowledge necessary to bridge that gap. In this paper we\ninvestigate possible ways of automating parts of the SM/SR process, i.e. that\nof extracting keywords and key-phrases from scientific documents using\nunsupervised methods, which are then used as a basis to construct the\ncorresponding Classification Scheme using semantic key-phrase clustering\ntechniques. Specifically, we explore the effect of ensemble scores measure in\nkey-phrase extraction, we explore semantic network based word embedding in\nembedding representation of phrase semantics and finally we also explore how\nclustering can be used to group related key-phrases. The evaluation is\nconducted on a dataset of publications pertaining the domain of \"Explainable\nAI\" which we constructed using standard publicly available digital libraries\nand sets of indexing terms (keywords). Results shows that: ensemble ranking\nscore does improve the key-phrase extraction performance. Semantic-network\nbased word embedding based on the ConceptNet Semantic Network has similar\nperformance with contextualized word embedding, however the former are\ncomputationally more efficient. Finally Semantic key-phrase clustering at\nterm-level can group similar terms together that can be suitable for\nclassification scheme.", "published": "2021-01-25 10:17:33", "link": "http://arxiv.org/abs/2101.09990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Disaster-Related Knowledge Base for Intelligent Agents", "abstract": "In this paper, we describe our efforts in establishing a simple knowledge\nbase by building a semantic network composed of concepts and word relationships\nin the context of disasters in the Philippines. Our primary source of data is a\ncollection of news articles scraped from various Philippine news websites.\nUsing word embeddings, we extract semantically similar and co-occurring words\nfrom an initial seed words list. We arrive at an expanded ontology with a total\nof 450 word assertions. We let experts from the fields of linguistics,\ndisasters, and weather science evaluate our knowledge base and arrived at an\nagreeability rate of 64%. We then perform a time-based analysis of the\nassertions to identify important semantic changes captured by the knowledge\nbase such as the (a) trend of roles played by human entities, (b) memberships\nof human entities, and (c) common association of disaster-related words. The\ncontext-specific knowledge base developed from this study can be adapted by\nintelligent agents such as chat bots integrated in platforms such as Facebook\nMessenger for answering disaster-related queries.", "published": "2021-01-25 11:31:05", "link": "http://arxiv.org/abs/2101.10014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Terminology Translation with Target Lemma Annotations", "abstract": "Most of the recent work on terminology integration in machine translation has\nassumed that terminology translations are given already inflected in forms that\nare suitable for the target language sentence. In day-to-day work of\nprofessional translators, however, it is seldom the case as translators work\nwith bilingual glossaries where terms are given in their dictionary forms;\nfinding the right target language form is part of the translation process. We\nargue that the requirement for apriori specified target language forms is\nunrealistic and impedes the practical applicability of previous work. In this\nwork, we propose to train machine translation systems using a source-side data\naugmentation method that annotates randomly selected source language words with\ntheir target language lemmas. We show that systems trained on such augmented\ndata are readily usable for terminology integration in real-life translation\nscenarios. Our experiments on terminology translation into the morphologically\ncomplex Baltic and Uralic languages show an improvement of up to 7 BLEU points\nover baseline systems with no means for terminology integration and an average\nimprovement of 4 BLEU points over the previous work. Results of the human\nevaluation indicate a 47.7% absolute improvement over the previous work in term\ntranslation accuracy when translating into Latvian.", "published": "2021-01-25 12:07:20", "link": "http://arxiv.org/abs/2101.10035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpanEmo: Casting Multi-label Emotion Classification as Span-prediction", "abstract": "Emotion recognition (ER) is an important task in Natural Language Processing\n(NLP), due to its high impact in real-world applications from health and\nwell-being to author profiling, consumer analysis and security. Current\napproaches to ER, mainly classify emotions independently without considering\nthat emotions can co-exist. Such approaches overlook potential ambiguities, in\nwhich multiple emotions overlap. We propose a new model \"SpanEmo\" casting\nmulti-label emotion classification as span-prediction, which can aid ER models\nto learn associations between labels and words in a sentence. Furthermore, we\nintroduce a loss function focused on modelling multiple co-existing emotions in\nthe input sentence. Experiments performed on the SemEval2018 multi-label\nemotion data over three language sets (i.e., English, Arabic and Spanish)\ndemonstrate our method's effectiveness. Finally, we present different analyses\nthat illustrate the benefits of our method in terms of improving the model\nperformance and learning meaningful associations between emotion classes and\nwords in the sentence.", "published": "2021-01-25 12:11:04", "link": "http://arxiv.org/abs/2101.10038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelWalk A Latent Variable Model Approach to Knowledge Graph Embedding", "abstract": "Embedding entities and relations of a knowledge graph in a low-dimensional\nspace has shown impressive performance in predicting missing links between\nentities. Although progresses have been achieved, existing methods are\nheuristically motivated and theoretical understanding of such embeddings is\ncomparatively underdeveloped. This paper extends the random walk model (Arora\net al., 2016a) of word embeddings to Knowledge Graph Embeddings (KGEs) to\nderive a scoring function that evaluates the strength of a relation R between\ntwo entities h (head) and t (tail). Moreover, we show that marginal loss\nminimisation, a popular objective used in much prior work in KGE, follows\nnaturally from the log-likelihood ratio maximisation under the probabilities\nestimated from the KGEs according to our theoretical relationship. We propose a\nlearning objective motivated by the theoretical analysis to learn KGEs from a\ngiven knowledge graph. Using the derived objective, accurate KGEs are learnt\nfrom FB15K237 and WN18RR benchmark datasets, providing empirical evidence in\nsupport of the theory.", "published": "2021-01-25 13:31:29", "link": "http://arxiv.org/abs/2101.10070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "With Measured Words: Simple Sentence Selection for Black-Box\n  Optimization of Sentence Compression Algorithms", "abstract": "Sentence Compression is the task of generating a shorter, yet grammatical\nversion of a given sentence, preserving the essence of the original sentence.\nThis paper proposes a Black-Box Optimizer for Compression (B-BOC): given a\nblack-box compression algorithm and assuming not all sentences need be\ncompressed -- find the best candidates for compression in order to maximize\nboth compression rate and quality. Given a required compression ratio, we\nconsider two scenarios: (i) single-sentence compression, and (ii)\nsentences-sequence compression. In the first scenario, our optimizer is trained\nto predict how well each sentence could be compressed while meeting the\nspecified ratio requirement. In the latter, the desired compression ratio is\napplied to a sequence of sentences (e.g., a paragraph) as a whole, rather than\non each individual sentence. To achieve that, we use B-BOC to assign an optimal\ncompression ratio to each sentence, then cast it as a Knapsack problem, which\nwe solve using bounded dynamic programming. We evaluate B-BOC on both scenarios\non three datasets, demonstrating that our optimizer improves both accuracy and\nRouge-F1-score compared to direct application of other compression algorithms.", "published": "2021-01-25 14:00:56", "link": "http://arxiv.org/abs/2101.10096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Mindedness and Style Coordination in Argumentative Discussions", "abstract": "Linguistic accommodation is the process in which speakers adjust their\naccent, diction, vocabulary, and other aspects of language according to the\ncommunication style of one another. Previous research has shown how linguistic\naccommodation correlates with gaps in the power and status of the speakers and\nthe way it promotes approval and discussion efficiency. In this work, we\nprovide a novel perspective on the phenomena, exploring its correlation with\nthe open-mindedness of a speaker, rather than to her social status. We process\nthousands of unstructured argumentative discussions that took place in Reddit's\nChange My View (CMV) subreddit, demonstrating that open-mindedness relates to\nthe assumed role of a speaker in different contexts. On the discussion level,\nwe surprisingly find that discussions that reach agreement present lower levels\nof accommodation.", "published": "2021-01-25 15:24:55", "link": "http://arxiv.org/abs/2101.10164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach to Measure Semantic Relatedness in Biomedical Concepts", "abstract": "Objective: This work aimed to demonstrate the effectiveness of a hybrid\napproach based on Sentence BERT model and retrofitting algorithm to compute\nrelatedness between any two biomedical concepts. Materials and Methods: We\ngenerated concept vectors by encoding concept preferred terms using ELMo, BERT,\nand Sentence BERT models. We used BioELMo and Clinical ELMo. We used Ontology\nKnowledge Free (OKF) models like PubMedBERT, BioBERT, BioClinicalBERT, and\nOntology Knowledge Injected (OKI) models like SapBERT, CoderBERT, KbBERT, and\nUmlsBERT. We trained all the BERT models using Siamese network on SNLI and STSb\ndatasets to allow the models to learn more semantic information at the phrase\nor sentence level so that they can represent multi-word concepts better.\nFinally, to inject ontology relationship knowledge into concept vectors, we\nused retrofitting algorithm and concepts from various UMLS relationships. We\nevaluated our hybrid approach on four publicly available datasets which also\nincludes the recently released EHR-RelB dataset. EHR-RelB is the largest\npublicly available relatedness dataset in which 89% of terms are multi-word\nwhich makes it more challenging. Results: Sentence BERT models mostly\noutperformed corresponding BERT models. The concept vectors generated using the\nSentence BERT model based on SapBERT and retrofitted using UMLS-related\nconcepts achieved the best results on all four datasets. Conclusions: Sentence\nBERT models are more effective compared to BERT models in computing relatedness\nscores in most of the cases. Injecting ontology knowledge into concept vectors\nfurther enhances their quality and contributes to better relatedness scores.", "published": "2021-01-25 16:01:27", "link": "http://arxiv.org/abs/2101.10196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Trigger-Sense Memory Flow Framework for Joint Entity and Relation\n  Extraction", "abstract": "Joint entity and relation extraction framework constructs a unified model to\nperform entity recognition and relation extraction simultaneously, which can\nexploit the dependency between the two tasks to mitigate the error propagation\nproblem suffered by the pipeline model. Current efforts on joint entity and\nrelation extraction focus on enhancing the interaction between entity\nrecognition and relation extraction through parameter sharing, joint decoding,\nor other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast\nas a multi-round reading comprehension task). However, there are still two\nissues on the table. First, the interaction utilized by most methods is still\nweak and uni-directional, which is unable to model the mutual dependency\nbetween the two tasks. Second, relation triggers are ignored by most methods,\nwhich can help explain why humans would extract a relation in the sentence.\nThey're essential for relation extraction but overlooked. To this end, we\npresent a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and\nrelation extraction. We build a memory module to remember category\nrepresentations learned in entity recognition and relation extraction tasks.\nAnd based on it, we design a multi-level memory flow attention mechanism to\nenhance the bi-directional interaction between entity recognition and relation\nextraction. Moreover, without any human annotations, our model can enhance\nrelation trigger information in a sentence through a trigger sensor module,\nwhich improves the model performance and makes model predictions with better\ninterpretation. Experiment results show that our proposed framework achieves\nstate-of-the-art results by improves the relation F1 to 52.44% (+3.2%) on\nSciERC, 66.49% (+4.9%) on ACE05, 72.35% (+0.6%) on CoNLL04 and 80.66% (+2.3%)\non ADE.", "published": "2021-01-25 16:24:04", "link": "http://arxiv.org/abs/2101.10213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Process-Level Representation of Scientific Protocols with Interactive\n  Annotation", "abstract": "We develop Process Execution Graphs (PEG), a document-level representation of\nreal-world wet lab biochemistry protocols, addressing challenges such as\ncross-sentence relations, long-range coreference, grounding, and implicit\narguments. We manually annotate PEGs in a corpus of complex lab protocols with\na novel interactive textual simulator that keeps track of entity traits and\nsemantic constraints during annotation. We use this data to develop\ngraph-prediction models, finding them to be good at entity identification and\nlocal relation extraction, while our corpus facilitates further exploration of\nchallenging long-range relations.", "published": "2021-01-25 17:18:20", "link": "http://arxiv.org/abs/2101.10244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning From Revisions: Quality Assessment of Claims in Argumentation\n  at Scale", "abstract": "Assessing the quality of arguments and of the claims the arguments are\ncomposed of has become a key task in computational argumentation. However, even\nif different claims share the same stance on the same topic, their assessment\ndepends on the prior perception and weighting of the different aspects of the\ntopic being discussed. This renders it difficult to learn topic-independent\nquality indicators. In this paper, we study claim quality assessment\nirrespective of discussed aspects by comparing different revisions of the same\nclaim. We compile a large-scale corpus with over 377k claim revision pairs of\nvarious types from kialo.com, covering diverse topics from politics, ethics,\nentertainment, and others. We then propose two tasks: (a) assessing which claim\nof a revision pair is better, and (b) ranking all versions of a claim by\nquality. Our first experiments with embedding-based logistic regression and\ntransformer-based neural networks show promising results, suggesting that\nlearned indicators generalize well across topics. In a detailed error analysis,\nwe give insights into what quality dimensions of claims can be assessed\nreliably. We provide the data and scripts needed to reproduce all results.", "published": "2021-01-25 17:32:04", "link": "http://arxiv.org/abs/2101.10250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of\n  Tasks Datasets and Metrics", "abstract": "Tasks, Datasets and Evaluation Metrics are important concepts for\nunderstanding experimental scientific papers. However, most previous work on\ninformation extraction for scientific literature mainly focuses on the\nabstracts only, and does not treat datasets as a separate type of entity (Zadeh\nand Schumann, 2016; Luan et al., 2018). In this paper, we present a new corpus\nthat contains domain expert annotations for Task (T), Dataset (D), Metric (M)\nentities on 2,000 sentences extracted from NLP papers. We report experiment\nresults on TDM extraction using a simple data augmentation strategy and apply\nour tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is\nmade publicly available to the community for fostering research on scientific\npublication summarization (Erera et al., 2019) and knowledge discovery.", "published": "2021-01-25 17:54:06", "link": "http://arxiv.org/abs/2101.10273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAWLS: PDF Annotation With Labels and Structure", "abstract": "Adobe's Portable Document Format (PDF) is a popular way of distributing\nview-only documents with a rich visual markup. This presents a challenge to NLP\npractitioners who wish to use the information contained within PDF documents\nfor training models or data analysis, because annotating these documents is\ndifficult. In this paper, we present PDF Annotation with Labels and Structure\n(PAWLS), a new annotation tool designed specifically for the PDF document\nformat. PAWLS is particularly suited for mixed-mode annotation and scenarios in\nwhich annotators require extended context to annotate accurately. PAWLS\nsupports span-based textual annotation, N-ary relations and freeform,\nnon-textual bounding boxes, all of which can be exported in convenient formats\nfor training multi-modal machine learning models. A read-only PAWLS server is\navailable at https://pawls.apps.allenai.org/ and the source code is available\nat https://github.com/allenai/pawls.", "published": "2021-01-25 18:02:43", "link": "http://arxiv.org/abs/2101.10281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Effective Multi-task and Multilingual Modelling", "abstract": "Natural language processing (NLP) tasks (e.g. question-answering in English)\nbenefit from knowledge of other tasks (e.g. named entity recognition in\nEnglish) and knowledge of other languages (e.g. question-answering in Spanish).\nSuch shared representations are typically learned in isolation, either across\ntasks or across languages. In this work, we propose a meta-learning approach to\nlearn the interactions between both tasks and languages. We also investigate\nthe role of different sampling strategies used during meta-learning. We present\nexperiments on five different tasks and six different languages from the XTREME\nmultilingual benchmark dataset. Our meta-learned model clearly improves in\nperformance compared to competitive baseline models that also include\nmulti-task baselines. We also present zero-shot evaluations on unseen target\nlanguages to demonstrate the utility of our proposed model.", "published": "2021-01-25 19:30:26", "link": "http://arxiv.org/abs/2101.10368v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Power of Language: Understanding Sentiment Towards the Climate\n  Emergency using Twitter Data", "abstract": "Understanding how attitudes towards the Climate Emergency vary can hold the\nkey to driving policy changes for effective action to mitigate climate related\nrisk. The Oil and Gas industry account for a significant proportion of global\nemissions and so it could be speculated that there is a relationship between\nCrude Oil Futures and sentiment towards the Climate Emergency. Using Latent\nDirichlet Allocation for Topic Modelling on a bespoke Twitter dataset, this\nstudy shows that it is possible to split the conversation surrounding the\nClimate Emergency into 3 distinct topics. Forecasting Crude Oil Futures using\nSeasonal AutoRegressive Integrated Moving Average Modelling gives promising\nresults with a root mean squared error of 0.196 and 0.209 on the training and\ntesting data respectively. Understanding variation in attitudes towards climate\nemergency provides inconclusive results which could be improved using\nspatial-temporal analysis methods such as Density Based Clustering (DBSCAN).", "published": "2021-01-25 19:51:10", "link": "http://arxiv.org/abs/2101.10376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "English Machine Reading Comprehension Datasets: A Survey", "abstract": "This paper surveys 60 English Machine Reading Comprehension datasets, with a\nview to providing a convenient resource for other researchers interested in\nthis problem. We categorize the datasets according to their question and answer\nform and compare them across various dimensions including size, vocabulary,\ndata source, method of creation, human performance level, and first question\nword. Our analysis reveals that Wikipedia is by far the most common data source\nand that there is a relative lack of why, when, and where questions across\ndatasets.", "published": "2021-01-25 21:15:06", "link": "http://arxiv.org/abs/2101.10421v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PolyLM: Learning about Polysemy through Language Modeling", "abstract": "To avoid the \"meaning conflation deficiency\" of word embeddings, a number of\nmodels have aimed to embed individual word senses. These methods at one time\nperformed well on tasks such as word sense induction (WSI), but they have since\nbeen overtaken by task-specific techniques which exploit contextualized\nembeddings. However, sense embeddings and contextualization need not be\nmutually exclusive. We introduce PolyLM, a method which formulates the task of\nlearning sense embeddings as a language modeling problem, allowing\ncontextualization techniques to be applied. PolyLM is based on two underlying\nassumptions about word senses: firstly, that the probability of a word\noccurring in a given context is equal to the sum of the probabilities of its\nindividual senses occurring; and secondly, that for a given occurrence of a\nword, one of its senses tends to be much more plausible in the context than the\nothers. We evaluate PolyLM on WSI, showing that it performs considerably better\nthan previous sense embedding techniques, and matches the current\nstate-of-the-art specialized WSI method despite having six times fewer\nparameters. Code and pre-trained models are available at\nhttps://github.com/AlanAnsell/PolyLM.", "published": "2021-01-25 22:09:12", "link": "http://arxiv.org/abs/2101.10448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Trends in Named Entity Recognition (NER)", "abstract": "The availability of large amounts of computer-readable textual data and\nhardware that can process the data has shifted the focus of knowledge projects\ntowards deep learning architecture. Natural Language Processing, particularly\nthe task of Named Entity Recognition is no exception. The bulk of the learning\nmethods that have produced state-of-the-art results have changed the deep\nlearning model, the training method used, the training data itself or the\nencoding of the output of the NER system. In this paper, we review significant\nlearning methods that have been employed for NER in the recent past and how\nthey came about from the linear learning methods of the past. We also cover the\nprogress of related tasks that are upstream or downstream to NER, e.g.,\nsequence tagging, entity linking, etc., wherever the processes in question have\nalso improved NER results.", "published": "2021-01-25 14:18:24", "link": "http://arxiv.org/abs/2101.11420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECOL-R: Encouraging Copying in Novel Object Captioning with\n  Reinforcement Learning", "abstract": "Novel Object Captioning is a zero-shot Image Captioning task requiring\ndescribing objects not seen in the training captions, but for which information\nis available from external object detectors. The key challenge is to select and\ndescribe all salient detected novel objects in the input images. In this paper,\nwe focus on this challenge and propose the ECOL-R model (Encouraging Copying of\nObject Labels with Reinforced Learning), a copy-augmented transformer model\nthat is encouraged to accurately describe the novel object labels. This is\nachieved via a specialised reward function in the SCST reinforcement learning\nframework (Rennie et al., 2017) that encourages novel object mentions while\nmaintaining the caption quality. We further restrict the SCST training to the\nimages where detected objects are mentioned in reference captions to train the\nECOL-R model. We additionally improve our copy mechanism via Abstract Labels,\nwhich transfer knowledge from known to novel object types, and a Morphological\nSelector, which determines the appropriate inflected forms of novel object\nlabels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et\nal., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.", "published": "2021-01-25 02:41:02", "link": "http://arxiv.org/abs/2101.09865v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-lingual Visual Pre-training for Multimodal Machine Translation", "abstract": "Pre-trained language models have been shown to improve performance in many\nnatural language tasks substantially. Although the early focus of such models\nwas single language pre-training, recent advances have resulted in\ncross-lingual and visual pre-training methods. In this paper, we combine these\ntwo approaches to learn visually-grounded cross-lingual representations.\nSpecifically, we extend the translation language modelling (Lample and Conneau,\n2019) with masked region classification and perform pre-training with three-way\nparallel vision & language corpora. We show that when fine-tuned for multimodal\nmachine translation, these models obtain state-of-the-art performance. We also\nprovide qualitative insights into the usefulness of the learned grounded\nrepresentations.", "published": "2021-01-25 12:46:41", "link": "http://arxiv.org/abs/2101.10044v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MICROS: Mixed-Initiative ConveRsatiOnal Systems Workshop", "abstract": "The 1st edition of the workshop on Mixed-Initiative ConveRsatiOnal Systems\n(MICROS@ECIR2021) aims at investigating and collecting novel ideas and\ncontributions in the field of conversational systems. Oftentimes, the users\nfulfill their information need using smartphones and home assistants. This has\nrevolutionized the way users access online information, thus posing new\nchallenges compared to traditional search and recommendation. The first edition\nof MICROS will have a particular focus on mixed-initiative conversational\nsystems. Indeed, conversational systems need to be proactive, proposing not\nonly answers but also possible interpretations for ambiguous or vague requests.", "published": "2021-01-25 16:31:40", "link": "http://arxiv.org/abs/2101.10219v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Re-imagining Algorithmic Fairness in India and Beyond", "abstract": "Conventional algorithmic fairness is West-centric, as seen in its sub-groups,\nvalues, and methods. In this paper, we de-center algorithmic fairness and\nanalyse AI power in India. Based on 36 qualitative interviews and a discourse\nanalysis of algorithmic deployments in India, we find that several assumptions\nof algorithmic fairness are challenged. We find that in India, data is not\nalways reliable due to socio-economic factors, ML makers appear to follow\ndouble standards, and AI evokes unquestioning aspiration. We contend that\nlocalising model fairness alone can be window dressing in India, where the\ndistance between models and oppressed communities is large. Instead, we\nre-imagine algorithmic fairness in India and provide a roadmap to\nre-contextualise data and models, empower oppressed communities, and enable\nFair-ML ecosystems.", "published": "2021-01-25 10:20:57", "link": "http://arxiv.org/abs/2101.09995v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "The emergence of visual semantics through communication games", "abstract": "The emergence of communication systems between agents which learn to play\nreferential signalling games with realistic images has attracted a lot of\nattention recently. The majority of work has focused on using fixed, pretrained\nimage feature extraction networks which potentially bias the information the\nagents learn to communicate. In this work, we consider a signalling game\nsetting in which a `sender' agent must communicate the information about an\nimage to a `receiver' who must select the correct image from many distractors.\nWe investigate the effect of the feature extractor's weights and of the task\nbeing solved on the visual semantics learned by the models. We first\ndemonstrate to what extent the use of pretrained feature extraction networks\ninductively bias the visual semantics conveyed by emergent communication\nchannel and quantify the visual semantics that are induced.\n  We then go on to explore ways in which inductive biases can be introduced to\nencourage the emergence of semantically meaningful communication without the\nneed for any form of supervised pretraining of the visual feature extractor. We\nimpose various augmentations to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game. Our work\nbridges a gap between emergent communication research and self-supervised\nfeature learning.", "published": "2021-01-25 17:43:37", "link": "http://arxiv.org/abs/2101.10253v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Curriculum Learning: A Survey", "abstract": "Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.", "published": "2021-01-25 20:08:32", "link": "http://arxiv.org/abs/2101.10382v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Randomized Deep Structured Prediction for Discourse-Level Processing", "abstract": "Expressive text encoders such as RNNs and Transformer Networks have been at\nthe center of NLP models in recent work. Most of the effort has focused on\nsentence-level tasks, capturing the dependencies between words in a single\nsentence, or pairs of sentences. However, certain tasks, such as argumentation\nmining, require accounting for longer texts and complicated structural\ndependencies between them. Deep structured prediction is a general framework to\ncombine the complementary strengths of expressive neural encoders and\nstructured inference for highly structured domains. Nevertheless, when the need\narises to go beyond sentences, most work relies on combining the output scores\nof independently trained classifiers. One of the main reasons for this is that\nconstrained inference comes at a high computational cost. In this paper, we\nexplore the use of randomized inference to alleviate this concern and show that\nwe can efficiently leverage deep structured prediction and expressive neural\nencoders for a set of tasks involving complicated argumentative structures.", "published": "2021-01-25 21:49:32", "link": "http://arxiv.org/abs/2101.10435v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Learning Approach for Detecting Psychological Distress in\n  Brexit Tweets", "abstract": "In 2016, United Kingdom (UK) citizens voted to leave the European Union (EU),\nwhich was officially implemented in 2020. During this period, UK residents\nexperienced a great deal of uncertainty around the UK's continued relationship\nwith the EU. Many people have used social media platforms to express their\nemotions about this critical event. Sentiment analysis has been recently\nconsidered as an important tool for detecting mental well-being in Twitter\ncontents. However, detecting the psychological distress status in\npolitical-related tweets is a challenging task due to the lack of explicit\nsentences describing the depressive or anxiety status. To address this problem,\nthis paper leverages a transfer learning approach for sentiment analysis to\nmeasure the non-clinical psychological distress status in Brexit tweets. The\nframework transfers the knowledge learnt from self-reported psychological\ndistress tweets (source domain) to detect the distress status in Brexit tweets\n(target domain). The framework applies a domain adaptation technique to\ndecrease the impact of negative transfer between source and target domains. The\npaper also introduces a Brexit distress index that can be used to detect levels\nof psychological distress of individuals in Brexit tweets. We design an\nexperiment that includes data from both domains. The proposed model is able to\ndetect the non-clinical psychological distress status in Brexit tweets with an\naccuracy of 66% and 62% on the source and target domains, respectively.", "published": "2021-01-25 16:54:37", "link": "http://arxiv.org/abs/2102.00912v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unanswerable Questions about Images and Texts", "abstract": "Questions about a text or an image that cannot be answered raise distinctive\nissues for an AI. This note discusses the problem of unanswerable questions in\nVQA (visual question answering), in QA (visual question answering), and in AI\ngenerally.", "published": "2021-01-25 17:56:15", "link": "http://arxiv.org/abs/2102.06793v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Supervised and Unsupervised Approaches for Controlling Narrow Lexical\n  Focus in Sequence-to-Sequence Speech Synthesis", "abstract": "Although Sequence-to-Sequence (S2S) architectures have become\nstate-of-the-art in speech synthesis, capable of generating outputs that\napproach the perceptual quality of natural samples, they are limited by a lack\nof flexibility when it comes to controlling the output. In this work we present\na framework capable of controlling the prosodic output via a set of concise,\ninterpretable, disentangled parameters. We apply this framework to the\nrealization of emphatic lexical focus, proposing a variety of architectures\ndesigned to exploit different levels of supervision based on the availability\nof labeled resources. We evaluate these approaches via listening tests that\ndemonstrate we are able to successfully realize controllable focus while\nmaintaining the same, or higher, naturalness over an established baseline, and\nwe explore how the different approaches compare when synthesizing in a target\nvoice with or without labeled data.", "published": "2021-01-25 08:02:42", "link": "http://arxiv.org/abs/2101.09940v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "High-Quality Vocoding Design with Signal Processing for Speech Synthesis\n  and Voice Conversion", "abstract": "This Ph.D. thesis focuses on developing a system for high-quality speech\nsynthesis and voice conversion. Vocoder-based speech analysis, manipulation,\nand synthesis plays a crucial role in various kinds of statistical parametric\nspeech research. Although there are vocoding methods which yield close to\nnatural synthesized speech, they are typically computationally expensive, and\nare thus not suitable for real-time implementation, especially in embedded\nenvironments. Therefore, there is a need for simple and computationally\nfeasible digital signal processing algorithms for generating high-quality and\nnatural-sounding synthesized speech. In this dissertation, I propose a solution\nto extract optimal acoustic features and a new waveform generator to achieve\nhigher sound quality and conversion accuracy by applying advances in deep\nlearning. The approach remains computationally efficient. This challenge\nresulted in five thesis groups, which are briefly summarized below.", "published": "2021-01-25 17:58:47", "link": "http://arxiv.org/abs/2101.10278v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lexical and syntactic gemination in Italian consonants -- Does a\n  geminate Italian consonant consist of a repeated or a strengthened consonant?", "abstract": "Two types of consonant gemination characterize Italian: lexical and\nsyntactic. Italian lexical gemination is contrastive, so that two words may\ndiffer by only one geminated consonant. In contrast, syntactic gemination\noccurs across word boundaries, and affects the initial consonant of a word in\nspecific contexts, such as the presence of a monosyllabic morpheme before the\nword. This study investigates the acoustic correlates of Italian lexical and\nsyntactic gemination, asking if the correlates for the two types are similar in\nthe case of stop consonants. Results confirmed previous studies showing that\nduration is a prominent gemination cue, with a lengthened consonant closure and\na shortened pre-consonant vowel for both types. Results also revealed the\npresence, in about 10-12% of instances, of a double stop-release burst,\nproviding strong support for the biphonematic nature of Italian geminated stop\nconsonants. Moreover, the timing of these bursts suggests a different planning\nprocess for lexical vs. syntactic geminates. The second burst, when present, is\naccommodated within the closure interval in syntactic geminates, while lexical\ngeminates are lengthened by the extra burst. This suggests that syntactic\ngemination occurs during a post-lexical phase of production planning, after\ntiming has already been established.", "published": "2021-01-25 16:07:45", "link": "http://arxiv.org/abs/2102.03166v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain-Dependent Speaker Diarization for the Third DIHARD Challenge", "abstract": "This report presents the system developed by the ABSP Laboratory team for the\nthird DIHARD speech diarization challenge. Our main contribution in this work\nis to develop a simple and efficient solution for acoustic domain dependent\nspeech diarization. We explore speaker embeddings for \\emph{acoustic domain\nidentification} (ADI) task. Our study reveals that i-vector based method\nachieves considerably better performance than x-vector based approach in the\nthird DIHARD challenge dataset. Next, we integrate the ADI module with the\ndiarization framework. The performance substantially improved over that of the\nbaseline when we optimized the thresholds for agglomerative hierarchical\nclustering and the parameters for dimensionality reduction during scoring for\nindividual acoustic domains. We achieved a relative improvement of $9.63\\%$ and\n$10.64\\%$ in DER for core and full conditions, respectively, for Track 1 of the\nDIHARD III evaluation set.", "published": "2021-01-25 04:01:13", "link": "http://arxiv.org/abs/2101.09884v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Angle of Arrival for Improving Indoor Localization", "abstract": "In this paper, we primarily explore the improvement of single stream audio\nsystems using Angle of Arrival calculations in both simulation and real life\ngathered data. We wanted to learn how to discern the direction of an audio\nsource from gathered signal data to ultimately incorporate into a multi modal\nsecurity system. We focused on the MUSIC algorithm for the estimation of the\nangle of arrival but briefly experimented with other techniques such as\nBartlett and Capo. We were able to implement our own MUSIC algorithm on\nstimulated data from Cornell. In addition, we demonstrated how we are able to\ncalculate the angle of arrival over time in a real life scene. Finally, we are\nable to detect the direction of arrival for two separate and simultaneous audio\nsources in a real life scene. Eventually, we could incorporate this tracking\ninto a multi modal system combined with video. Overall, we are able to produce\ncompelling results for angle of arrival calculations that could be the stepping\nstones for a better system to detect events in a scene.", "published": "2021-01-25 05:52:19", "link": "http://arxiv.org/abs/2101.09904v1", "categories": ["cs.SD", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Novel Recording Studio Features for Music Information Retrieval", "abstract": "In the recording studio, producers of Electronic Dance Music (EDM) spend more\ntime creating, shaping, mixing and mastering sounds, than with compositional\naspects or arrangement. They tune the sound by close listening and by\nleveraging audio metering and audio analysis tools, until they successfully\ncreat the desired sound aesthetics. DJs of EDM tend to play sets of songs that\nmeet their sound ideal. We therefore suggest using audio metering and\nmonitoring tools from the recording studio to analyze EDM, instead of relying\non conventional low-level audio features. We test our novel set of features by\na simple classification task. We attribute songs to DJs who would play the\nspecific song. This new set of features and the focus on DJ sets is targeted at\nEDM as it takes the producer and DJ culture into account. With simple\ndimensionality reduction and machine learning these features enable us to\nattribute a song to a DJ with an accuracy of 63%. The features from the audio\nmetering and monitoring tools in the recording studio could serve for many\napplications in Music Information Retrieval, such as genre, style and era\nclassification and music recommendation for both DJs and consumers of\nelectronic dance music.", "published": "2021-01-25 16:09:25", "link": "http://arxiv.org/abs/2101.10201v1", "categories": ["cs.SD", "cs.IR", "eess.AS", "68Txx", "I.2.6; J.5"], "primary_category": "cs.SD"}
