{"title": "Moments by Integrating the Moment-Generating Function", "abstract": "We introduce a novel method for obtaining a wide variety of moments of a\nrandom variable with a well-defined moment-generating function (MGF). We derive\nnew expressions for fractional moments and fractional absolute moments, both\ncentral and non-central moments. The new moment expressions are relatively\nsimple integrals that involve the MGF, but do not require its derivatives. We\nlabel the new method CMGF because it uses a complex extension of the MGF and\ncan be used to obtain complex moments. We illustrate the new method with three\napplications where the MGF is available in closed-form, while the corresponding\ndensities and the derivatives of the MGF are either unavailable or very\ndifficult to obtain.", "published": "2024-10-31 02:58:56", "link": "http://arxiv.org/abs/2410.23587v3", "categories": ["econ.EM", "q-fin.CP", "stat.CO"], "primary_category": "econ.EM"}
{"title": "On Cost-Sensitive Distributionally Robust Log-Optimal Portfolio", "abstract": "This paper addresses a novel \\emph{cost-sensitive} distributionally robust\nlog-optimal portfolio problem, where the investor faces \\emph{ambiguous} return\ndistributions, and a general convex transaction cost model is incorporated. The\nuncertainty in the return distribution is quantified using the\n\\emph{Wasserstein} metric, which captures distributional ambiguity. We\nestablish conditions that ensure robustly survivable trades for all\ndistributions in the Wasserstein ball under convex transaction costs. By\nleveraging duality theory, we approximate the infinite-dimensional\ndistributionally robust optimization problem with a finite convex program,\nenabling computational tractability for mid-sized portfolios. Empirical studies\nusing S\\&P 500 data validate our theoretical framework: without transaction\ncosts, the optimal portfolio converges to an equal-weighted allocation, while\nwith transaction costs, the portfolio shifts slightly towards the risk-free\nasset, reflecting the trade-off between cost considerations and optimal\nallocation.", "published": "2024-10-31 00:56:47", "link": "http://arxiv.org/abs/2410.23536v1", "categories": ["math.OC", "cs.SY", "eess.SY", "q-fin.CP", "q-fin.PM", "91G10, 93E03, 90C17, 90C46, 90C25"], "primary_category": "math.OC"}
{"title": "A dynamic programming principle for multiperiod control problems with bicausal constraints", "abstract": "We consider multiperiod stochastic control problems with non-parametric\nuncertainty on the underlying probabilistic model. We derive a new metric on\nthe space of probability measures, called the adapted $(p,\n\\infty)$--Wasserstein distance $\\mathcal{AW}_p^\\infty$ with the following\nproperties: (1) the adapted $(p, \\infty)$--Wasserstein distance generates a\ntopology that guarantees continuity of stochastic control problems and (2) the\ncorresponding $\\mathcal{AW}_p^\\infty$-distributionally robust optimization\n(DRO) problem can be computed via a dynamic programming principle involving\none-step Wasserstein-DRO problems. If the cost function is semi-separable, then\nwe further show that a minimax theorem holds, even though balls with respect to\n$\\mathcal{AW}_p^\\infty$ are neither convex nor compact in general. We also\nderive first-order sensitivity results.", "published": "2024-10-31 13:35:33", "link": "http://arxiv.org/abs/2410.23927v1", "categories": ["math.OC", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "Deep Learning in Long-Short Stock Portfolio Allocation: An Empirical Study", "abstract": "This paper provides an empirical study explores the application of deep\nlearning algorithms-Multilayer Perceptron (MLP), Convolutional Neural Networks\n(CNN), Long Short-Term Memory (LSTM), and Transformer-in constructing\nlong-short stock portfolios. Two datasets comprising randomly selected stocks\nfrom the S&P500 and NASDAQ indices, each spanning a decade of daily data, are\nutilized. The models predict daily stock returns based on historical features\nsuch as past returns,Relative Strength Index (RSI), trading volume, and\nvolatility. Portfolios are dynamically adjusted by longing stocks with positive\npredicted returns and shorting those with negative predictions, with equal\nasset weights. Performance is evaluated over a two-year testing period,\nfocusing on return, Sharpe ratio, and maximum drawdown metrics. The results\ndemonstrate the efficacy of deep learning models in enhancing long-short stock\nportfolio performance.", "published": "2024-10-31 10:48:18", "link": "http://arxiv.org/abs/2411.13555v3", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Large Language Models for Patient Comments Multi-Label Classification", "abstract": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.", "published": "2024-10-31 00:29:52", "link": "http://arxiv.org/abs/2410.23528v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context\n  Learning for Long-Tail Knowledge in LLMs", "abstract": "Large language models (LLMs) can learn vast amounts of knowledge from diverse\ndomains during pre-training. However, long-tail knowledge from specialized\ndomains is often scarce and underrepresented, rarely appearing in the models'\nmemorization. Prior work has shown that in-context learning (ICL) with\nretriever augmentation can help LLMs better capture long-tail knowledge,\nreducing their reliance on pre-trained data. Despite these advances, we observe\nthat LLM predictions for long-tail questions remain uncertain to variations in\nretrieved samples. To take advantage of the uncertainty in ICL for guiding LLM\npredictions toward correct answers on long-tail samples, we propose a\nreinforcement learning-based dynamic uncertainty ranking method for ICL that\naccounts for the varying impact of each retrieved sample on LLM predictions.\nOur approach prioritizes more informative and stable samples while demoting\nmisleading ones, updating rankings based on the feedback from the LLM w.r.t.\neach retrieved sample. To enhance training efficiency and reduce query costs,\nwe introduce a learnable dynamic ranking threshold, adjusted when the model\nencounters negative prediction shifts. Experimental results on various\nquestion-answering datasets from different domains show that our method\noutperforms the best baseline by $2.76\\%$, with a notable $5.96\\%$ boost in\naccuracy on long-tail questions that elude zero-shot inference.", "published": "2024-10-31 03:42:17", "link": "http://arxiv.org/abs/2410.23605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Positional Bias of Faithfulness for Long-form Summarization", "abstract": "Large Language Models (LLMs) often exhibit positional bias in long-context\nsettings, under-attending to information in the middle of inputs. We\ninvestigate the presence of this bias in long-form summarization, its impact on\nfaithfulness, and various techniques to mitigate this bias. To consistently\nevaluate faithfulness, we first compile a benchmark of eight human-annotated\nlong-form summarization datasets and perform a meta-evaluation of faithfulness\nmetrics. We show that LLM-based faithfulness metrics, though effective with\nfull-context inputs, remain sensitive to document order, indicating positional\nbias. Analyzing LLM-generated summaries across six datasets, we find a\n\"U-shaped\" trend in faithfulness, where LLMs faithfully summarize the beginning\nand end of documents but neglect middle content. Perturbing document order\nsimilarly reveals models are less faithful when important documents are placed\nin the middle of the input. We find that this behavior is partly due to\nshifting focus with context length: as context increases, summaries become less\nfaithful, but beyond a certain length, faithfulness improves as the model\nfocuses on the end. Finally, we experiment with different generation techniques\nto reduce positional bias and find that prompting techniques effectively direct\nmodel attention to specific positions, whereas more sophisticated approaches\noffer limited improvements. Our data and code are available in\nhttps://github.com/meetdavidwan/longformfact.", "published": "2024-10-31 03:50:15", "link": "http://arxiv.org/abs/2410.23609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Typology in BPE Subword Productivity and Language Modeling", "abstract": "This study investigates the impact of morphological typology on tokenization\nand language modeling performance. We focus on languages with synthetic and\nanalytical morphological structures and examine their productivity when\ntokenized using the byte-pair encoding (BPE) algorithm. We compare the\nperformance of models trained with similar amounts of data in different\nlanguages. Our experiments reveal that languages with synthetic features\nexhibit greater subword regularity and productivity with BPE tokenization and\nachieve better results in language modeling tasks. We also observe that the\ntypological continuum from linguistic theory is reflected in several\nexperiments. These findings suggest a correlation between morphological\ntypology and BPE tokenization efficiency.", "published": "2024-10-31 06:13:29", "link": "http://arxiv.org/abs/2410.23656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo-Conversation Injection for LLM Goal Hijacking", "abstract": "Goal hijacking is a type of adversarial attack on Large Language Models\n(LLMs) where the objective is to manipulate the model into producing a\nspecific, predetermined output, regardless of the user's original input. In\ngoal hijacking, an attacker typically appends a carefully crafted malicious\nsuffix to the user's prompt, which coerces the model into ignoring the user's\noriginal input and generating the target response. In this paper, we introduce\na novel goal hijacking attack method called Pseudo-Conversation Injection,\nwhich leverages the weaknesses of LLMs in role identification within\nconversation contexts. Specifically, we construct the suffix by fabricating\nresponses from the LLM to the user's initial prompt, followed by a prompt for a\nmalicious new task. This leads the model to perceive the initial prompt and\nfabricated response as a completed conversation, thereby executing the new,\nfalsified prompt. Following this approach, we propose three Pseudo-Conversation\nconstruction strategies: Targeted Pseudo-Conversation, Universal\nPseudo-Conversation, and Robust Pseudo-Conversation. These strategies are\ndesigned to achieve effective goal hijacking across various scenarios. Our\nexperiments, conducted on two mainstream LLM platforms including ChatGPT and\nQwen, demonstrate that our proposed method significantly outperforms existing\napproaches in terms of attack effectiveness.", "published": "2024-10-31 06:58:34", "link": "http://arxiv.org/abs/2410.23678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in\n  Byte-Level Tokenizers", "abstract": "Tokenization is a crucial step that bridges human-readable text with\nmodel-readable discrete tokens. However, recent studies have revealed that\ntokenizers can be exploited to elicit unwanted model behaviors. In this work,\nwe investigate incomplete tokens, i.e., undecodable tokens with stray bytes\nresulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize\nthat such tokens are heavily reliant on their adjacent tokens and are fragile\nwhen paired with unfamiliar tokens. To demonstrate this vulnerability, we\nintroduce improbable bigrams: out-of-distribution combinations of incomplete\ntokens designed to exploit their dependency. Our experiments show that\nimprobable bigrams are significantly prone to hallucinatory behaviors.\nSurprisingly, alternative tokenizations of the same phrases result in\ndrastically lower rates of hallucination (93% reduction in Llama3.1). We\ncaution against the potential vulnerabilities introduced by byte-level BPE\ntokenizers, which may impede the development of trustworthy language models.", "published": "2024-10-31 07:19:44", "link": "http://arxiv.org/abs/2410.23684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GigaCheck: Detecting LLM-generated Content", "abstract": "With the increasing quality and spread of LLM-based assistants, the amount of\nLLM-generated content is growing rapidly. In many cases and tasks, such texts\nare already indistinguishable from those written by humans, and the quality of\ngeneration tends to only increase. At the same time, detection methods are\ndeveloping more slowly, making it challenging to prevent misuse of generative\nAI technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize AI-generated intervals within text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets.", "published": "2024-10-31 08:30:55", "link": "http://arxiv.org/abs/2410.23728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Automated Verification of Textual Claims (AVeriTeC) Shared Task", "abstract": "The Automated Verification of Textual Claims (AVeriTeC) shared task asks\nparticipants to retrieve evidence and predict veracity for real-world claims\nchecked by fact-checkers. Evidence can be found either via a search engine, or\nvia a knowledge store provided by the organisers. Submissions are evaluated\nusing AVeriTeC score, which considers a claim to be accurately verified if and\nonly if both the verdict is correct and retrieved evidence is considered to\nmeet a certain quality threshold. The shared task received 21 submissions, 18\nof which surpassed our baseline. The winning team was TUDA_MAI with an AVeriTeC\nscore of 63%. In this paper we describe the shared task, present the full\nresults, and highlight key takeaways from the shared task.", "published": "2024-10-31 12:01:12", "link": "http://arxiv.org/abs/2410.23850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Responsible Retrieval Augmented Generation for Climate Decision Making\n  from Documents", "abstract": "Climate decision making is constrained by the complexity and inaccessibility\nof key information within lengthy, technical, and multi-lingual documents.\nGenerative AI technologies offer a promising route for improving the\naccessibility of information contained within these documents, but suffer from\nlimitations. These include (1) a tendency to hallucinate or mis-represent\ninformation, (2) difficulty in steering or guaranteeing properties of generated\noutput, and (3) reduced performance in specific technical domains. To address\nthese challenges, we introduce a novel evaluation framework with\ndomain-specific dimensions tailored for climate-related documents. We then\napply this framework to evaluate Retrieval-Augmented Generation (RAG)\napproaches and assess retrieval- and generation-quality within a prototype tool\nthat answers questions about individual climate law and policy documents. In\naddition, we publish a human-annotated dataset and scalable automated\nevaluation tools, with the aim of facilitating broader adoption and robust\nassessment of these systems in the climate domain. Our findings highlight the\nkey components of responsible deployment of RAG to enhance decision-making,\nwhile also providing insights into user experience (UX) considerations for\nsafely deploying such systems to build trust with users in high-risk domains.", "published": "2024-10-31 13:05:39", "link": "http://arxiv.org/abs/2410.23902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models can Self-Lengthen to Generate Long Texts", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.", "published": "2024-10-31 13:47:10", "link": "http://arxiv.org/abs/2410.23933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language", "abstract": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.", "published": "2024-10-31 14:09:50", "link": "http://arxiv.org/abs/2410.23956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting text level intellectual influence with knowledge graph\n  embeddings", "abstract": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.", "published": "2024-10-31 15:21:27", "link": "http://arxiv.org/abs/2410.24021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs", "abstract": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.", "published": "2024-10-31 15:45:23", "link": "http://arxiv.org/abs/2410.24049v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-environment Topic Models", "abstract": "Probabilistic topic models are a powerful tool for extracting latent themes\nfrom large text datasets. In many text datasets, we also observe per-document\ncovariates (e.g., source, style, political affiliation) that act as\nenvironments that modulate a \"global\" (environment-agnostic) topic\nrepresentation. Accurately learning these representations is important for\nprediction on new documents in unseen environments and for estimating the\ncausal effect of topics on real-world outcomes. To this end, we introduce the\nMulti-environment Topic Model (MTM), an unsupervised probabilistic model that\nseparates global and environment-specific terms. Through experimentation on\nvarious political content, from ads to tweets and speeches, we show that the\nMTM produces interpretable global topics with distinct environment-specific\nwords. On multi-environment data, the MTM outperforms strong baselines in and\nout-of-distribution. It also enables the discovery of accurate causal effects.", "published": "2024-10-31 16:50:39", "link": "http://arxiv.org/abs/2410.24126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Touch My Diacritics", "abstract": "The common practice of preprocessing text before feeding it into NLP models\nintroduces many decision points which have unintended consequences on model\nperformance. In this opinion piece, we focus on the handling of diacritics in\ntexts originating in many languages and scripts. We demonstrate, through\nseveral case studies, the adverse effects of inconsistent encoding of\ndiacritized characters and of removing diacritics altogether. We call on the\ncommunity to adopt simple but necessary steps across all models and toolkits in\norder to improve handling of diacritized text and, by extension, increase\nequity in multilingual NLP.", "published": "2024-10-31 17:03:44", "link": "http://arxiv.org/abs/2410.24140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Blind Spot Navigation in LLM Reasoning with Thought Space Explorer", "abstract": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.", "published": "2024-10-31 17:12:14", "link": "http://arxiv.org/abs/2410.24155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT or BERT: why not both?", "abstract": "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.", "published": "2024-10-31 17:18:11", "link": "http://arxiv.org/abs/2410.24159v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation", "abstract": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes.", "published": "2024-10-31 17:55:27", "link": "http://arxiv.org/abs/2410.24199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Masking: Power Law Masking Improves Multi-attribute Controlled\n  Generation", "abstract": "We introduce LingGen, a novel approach for controlled text generation that\noffers precise control over a wide array of linguistic attributes, even as the\nnumber of attributes varies. LingGen employs a dynamic P-MASKING strategy,\nwhich samples masking rates from a power law distribution during training. This\ninnovative approach enables the model to develop robust representations and\nadapt its attribute control capabilities across a variable number of\nattributes, from a single attribute to multiple complex configurations. The\nP-MASKING technique enhances LingGen's ability to manage different levels of\nattribute visibility, resulting in superior performance in multi-attribute\ngeneration tasks. Our experiments demonstrate that LingGen surpasses current\nstate-of-the-art models in both attribute control accuracy and text fluency,\nparticularly excelling in scenarios with varying attribute demands.\nAdditionally, our ablation studies highlight the effectiveness of P-MASKING and\nthe influence of different base language models on performance. These findings\ndemonstrate LingGen's potential for applications requiring precise and\nadaptable control over multiple linguistic attributes in text generation.", "published": "2024-10-31 17:55:45", "link": "http://arxiv.org/abs/2410.24201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RESTOR: Knowledge Recovery through Machine Unlearning", "abstract": "Large language models trained on web-scale corpora can memorize undesirable\ndatapoints such as incorrect facts, copyrighted content or sensitive data.\nRecently, many machine unlearning algorithms have been proposed that aim to\n`erase' these datapoints from trained models -- that is, revert model behavior\nto be similar to a model that had never been trained on these datapoints.\nHowever, evaluating the success of unlearning algorithms remains an open\nchallenge. In this work, we propose the RESTOR framework for machine\nunlearning, which evaluates the ability of unlearning algorithms to perform\ntargeted data erasure from models, by evaluating the ability of models to\nforget the knowledge introduced in these data points, while simultaneously\nrecovering the model's knowledge state had it not encountered these datapoints.\nRESTOR helps uncover several novel insights about popular unlearning\nalgorithms, and the mechanisms through which they operate -- for instance,\nidentifying that some algorithms merely emphasize forgetting, and that\nlocalizing unlearning targets can enhance unlearning performance.", "published": "2024-10-31 20:54:35", "link": "http://arxiv.org/abs/2411.00204v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making", "abstract": "Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods.", "published": "2024-10-31 22:58:08", "link": "http://arxiv.org/abs/2411.00248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve\n  Factualness in Large Language Models", "abstract": "Large language models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks, yet they often struggle with maintaining\nfactual accuracy, particularly in knowledge-intensive domains like healthcare.\nThis study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,\na novel approach designed to enhance the factual reliability of LLMs, with a\nfocus on medical question answering (QA). LEAF utilizes a dual strategy to\nenhance the factual accuracy of responses from models such as Llama 3 70B\nInstruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,\nimproves Retrieval-Augmented Generation (RAG) by incorporating fact-checking\nresults to guide the retrieval process without updating model parameters. The\nsecond strategy, Learning from Fact-Checks via Self-Training, involves\nsupervised fine-tuning (SFT) on fact-checked responses or applying Simple\nPreference Optimization (SimPO) with fact-checking as a ranking mechanism, both\nupdating LLM parameters from supervision. These findings suggest that\nintegrating fact-checked responses whether through RAG enhancement or\nself-training enhances the reliability and factual correctness of LLM outputs,\noffering a promising solution for applications where information accuracy is\ncrucial.", "published": "2024-10-31 00:18:05", "link": "http://arxiv.org/abs/2410.23526v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioNCERE: Non-Contrastive Enhancement For Relation Extraction In\n  Biomedical Texts", "abstract": "State-of-the-art models for relation extraction (RE) in the biomedical domain\nconsider finetuning BioBERT using classification, but they may suffer from the\nanisotropy problem. Contrastive learning methods can reduce this anisotropy\nphenomena, and also help to avoid class collapse in any classification problem.\nIn the present paper, a new training method called biological non-contrastive\nrelation extraction (BioNCERE) is introduced for relation extraction without\nusing any named entity labels for training to reduce annotation costs. BioNCERE\nuses transfer learning and non-contrastive learning to avoid full or\ndimensional collapse as well as bypass overfitting. It resolves RE in three\nstages by leveraging transfer learning two times. By freezing the weights\nlearned in previous stages in the proposed pipeline and by leveraging\nnon-contrastive learning in the second stage, the model predicts relations\nwithout any knowledge of named entities. Experiments have been done on SemMedDB\nthat are almost similar to State-of-the-art performance on RE without using the\ninformation of named entities.", "published": "2024-10-31 02:51:56", "link": "http://arxiv.org/abs/2410.23583v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Ontology Learning with Large Language Models", "abstract": "Ontologies are useful for automatic machine processing of domain knowledge as\nthey represent it in a structured format. Yet, constructing ontologies requires\nsubstantial manual effort. To automate part of this process, large language\nmodels (LLMs) have been applied to solve various subtasks of ontology learning.\nHowever, this partial ontology learning does not capture the interactions\nbetween subtasks. We address this gap by introducing OLLM, a general and\nscalable method for building the taxonomic backbone of an ontology from\nscratch. Rather than focusing on subtasks, like individual relations between\nentities, we model entire subcomponents of the target ontology by finetuning an\nLLM with a custom regulariser that reduces overfitting on high-frequency\nconcepts. We introduce a novel suite of metrics for evaluating the quality of\nthe generated ontology by measuring its semantic and structural similarity to\nthe ground truth. In contrast to standard metrics, our metrics use deep\nlearning techniques to define more robust distance measures between graphs.\nBoth our quantitative and qualitative results on Wikipedia show that OLLM\noutperforms subtask composition methods, producing more semantically accurate\nontologies while maintaining structural integrity. We further demonstrate that\nour model can be effectively adapted to new domains, like arXiv, needing only a\nsmall number of training examples. Our source code and datasets are available\nat https://github.com/andylolu2/ollm.", "published": "2024-10-31 02:52:39", "link": "http://arxiv.org/abs/2410.23584v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Using Multimodal Deep Neural Networks to Disentangle Language from\n  Visual Aesthetics", "abstract": "When we experience a visual stimulus as beautiful, how much of that\nexperience derives from perceptual computations we cannot describe versus\nconceptual knowledge we can readily translate into natural language?\nDisentangling perception from language in visually-evoked affective and\naesthetic experiences through behavioral paradigms or neuroimaging is often\nempirically intractable. Here, we circumnavigate this challenge by using linear\ndecoding over the learned representations of unimodal vision, unimodal\nlanguage, and multimodal (language-aligned) deep neural network (DNN) models to\npredict human beauty ratings of naturalistic images. We show that unimodal\nvision models (e.g. SimCLR) account for the vast majority of explainable\nvariance in these ratings. Language-aligned vision models (e.g. SLIP) yield\nsmall gains relative to unimodal vision. Unimodal language models (e.g. GPT2)\nconditioned on visual embeddings to generate captions (via CLIPCap) yield no\nfurther gains. Caption embeddings alone yield less accurate predictions than\nimage and caption embeddings combined (concatenated). Taken together, these\nresults suggest that whatever words we may eventually find to describe our\nexperience of beauty, the ineffable computations of feedforward perception may\nprovide sufficient foundation for that experience.", "published": "2024-10-31 03:37:21", "link": "http://arxiv.org/abs/2410.23603v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction", "abstract": "Human mobility prediction plays a critical role in applications such as\ndisaster response, urban planning, and epidemic forecasting. Traditional\nmethods often rely on designing crafted, domain-specific models, and typically\nfocus on short-term predictions, which struggle to generalize across diverse\nurban environments. In this study, we introduce Llama-3-8B-Mob, a large\nlanguage model fine-tuned with instruction tuning, for long-term citywide\nmobility prediction -- in a Q&A manner. We validate our approach using\nlarge-scale human mobility data from four metropolitan areas in Japan, focusing\non predicting individual trajectories over the next 15 days. The results\ndemonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility --\nsurpassing the state-of-the-art on multiple prediction metrics. It also\ndisplays strong zero-shot generalization capabilities -- effectively\ngeneralizing to other cities even when fine-tuned only on limited samples from\na single city. Source codes are available at\nhttps://github.com/TANGHULU6/Llama3-8B-Mob.", "published": "2024-10-31 07:30:38", "link": "http://arxiv.org/abs/2410.23692v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large\n  Language Models", "abstract": "Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge.", "published": "2024-10-31 07:48:44", "link": "http://arxiv.org/abs/2410.23703v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios", "abstract": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL.", "published": "2024-10-31 09:01:25", "link": "http://arxiv.org/abs/2410.23746v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams", "abstract": "In this work, we leverage LLMs to produce medical qualification exam\nquestions and the corresponding answers through few-shot prompts, investigating\nin-depth how LLMs meet the requirements in terms of coherence, evidence of\nstatement, factual consistency, and professionalism etc. Utilizing a\nmulticenter bidirectional anonymized database with respect to comorbid chronic\ndiseases, named Elderly Comorbidity Medical Database (CECMed), we tasked LLMs\nwith generating open-ended questions and answers based on a subset of sampled\nadmission reports. For CECMed, the retrospective cohort includes patients\nenrolled from January 2010 to January 2022 while the prospective cohort from\nJanuary 2023 to November 2023, with participants sourced from selected tertiary\nand community hospitals across the southern, northern, and central regions of\nChina. A total of 8 widely used LLMs were used, including ERNIE 4, ChatGLM 4,\nDoubao, Hunyuan, Spark 4, Qwen,\n  Conventional medical education requires sophisticated clinicians to formulate\nquestions and answers based on prototypes from EHRs, which is heuristic and\ntime-consuming. We found that mainstream LLMs could generate questions and\nanswers with real-world EHRs at levels close to clinicians. Although current\nLLMs performed dissatisfactory in some aspects, medical students, interns and\nresidents could reasonably make use of LLMs to facilitate understanding.", "published": "2024-10-31 09:33:37", "link": "http://arxiv.org/abs/2410.23769v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What is Wrong with Perplexity for Long-context Language Modeling?", "abstract": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.", "published": "2024-10-31 09:39:28", "link": "http://arxiv.org/abs/2410.23771v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for\n  Minority Languages", "abstract": "The need for large text corpora has increased with the advent of pretrained\nlanguage models and, in particular, the discovery of scaling laws for these\nmodels. Most available corpora have sufficient data only for languages with\nlarge dominant communities. However, there is no corpus available that (i)\ncovers a wide range of minority languages; (ii) is generated by an open-source\nreproducible pipeline; and (iii) is rigorously cleaned from noise, making it\ntrustworthy to use. We present GlotCC, a clean, document-level, 2TB general\ndomain corpus derived from CommonCrawl, covering more than 1000 languages. We\nmake GlotCC and the system used to generate it - including the pipeline,\nlanguage identification model, and filters - available to the research\ncommunity. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,\nPipeline v. 3.0 https://github.com/cisnlp/GlotCC.", "published": "2024-10-31 11:14:12", "link": "http://arxiv.org/abs/2410.23825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasons and Solutions for the Decline in Model Performance after Editing", "abstract": "Knowledge editing technology has received widespread attention for low-cost\nupdates of incorrect or outdated knowledge in large-scale language models.\nHowever, recent research has found that edited models often exhibit varying\ndegrees of performance degradation. The reasons behind this phenomenon and\npotential solutions have not yet been provided. In order to investigate the\nreasons for the performance decline of the edited model and optimize the\nediting method, this work explores the underlying reasons from both data and\nmodel perspectives. Specifically, 1) from a data perspective, to clarify the\nimpact of data on the performance of editing models, this paper first\nconstructs a Multi-Question Dataset (MQD) to evaluate the impact of different\ntypes of editing data on model performance. The performance of the editing\nmodel is mainly affected by the diversity of editing targets and sequence\nlength, as determined through experiments. 2) From a model perspective, this\narticle explores the factors that affect the performance of editing models. The\nresults indicate a strong correlation between the L1-norm of the editing model\nlayer and the editing accuracy, and clarify that this is an important factor\nleading to the bottleneck of editing performance. Finally, in order to improve\nthe performance of the editing model, this paper further proposes a Dump for\nSequence (D4S) method, which successfully overcomes the previous editing\nbottleneck by reducing the L1-norm of the editing layer, allowing users to\nperform multiple effective edits and minimizing model damage. Our code is\navailable at https://github.com/nlpkeg/D4S.", "published": "2024-10-31 11:49:44", "link": "http://arxiv.org/abs/2410.23843v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Commonsense Knowledge Editing Based on Free-Text in LLMs", "abstract": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.", "published": "2024-10-31 11:50:24", "link": "http://arxiv.org/abs/2410.23844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?", "abstract": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.", "published": "2024-10-31 12:07:44", "link": "http://arxiv.org/abs/2410.23856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Failure Modes of LLMs for Causal Reasoning on Narratives", "abstract": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.", "published": "2024-10-31 12:48:58", "link": "http://arxiv.org/abs/2410.23884v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages", "abstract": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.", "published": "2024-10-31 12:52:26", "link": "http://arxiv.org/abs/2410.23890v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Training for Selective Prediction", "abstract": "Classifier models are prevalent in natural language processing (NLP), often\nwith high accuracy. Yet in real world settings, human-in-the-loop systems can\nfoster trust in model outputs and even higher performance. Selective Prediction\n(SP) methods determine when to adopt a classifier's output versus defer to a\nhuman. Previous SP approaches have addressed how to improve softmax as a\nmeasure of model confidence, or have developed separate confidence estimators.\nOne previous method involves learning a deferral model based on engineered\nfeatures. We introduce a novel joint-training approach that simultaneously\noptimizes learned representations used by the classifier module and a learned\ndeferral policy. Our results on four classification tasks demonstrate that\njoint training not only leads to better SP outcomes over two strong baselines,\nbut also improves the performance of both modules.", "published": "2024-10-31 15:28:26", "link": "http://arxiv.org/abs/2410.24029v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Concept With Text-Guided Diffusion Models", "abstract": "Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal.", "published": "2024-10-31 17:09:55", "link": "http://arxiv.org/abs/2410.24151v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Redefining <Creative> in Dictionary: Towards an Enhanced Semantic\n  Understanding of Creative Generation", "abstract": "``Creative'' remains an inherently abstract concept for both humans and\ndiffusion models. While text-to-image (T2I) diffusion models can easily\ngenerate out-of-distribution concepts like ``a blue banana'', they struggle\nwith generating combinatorial objects such as ``a creative mixture that\nresembles a lettuce and a mantis'', due to difficulties in understanding the\nsemantic depth of ``creative''. Current methods rely heavily on synthesizing\nreference prompts or images to achieve a creative effect, typically requiring\nretraining for each unique creative output-a process that is computationally\nintensive and limits practical applications. To address this, we introduce\nCreTok, which brings meta-creativity to diffusion models by redefining\n``creative'' as a new token, \\texttt{<CreTok>}, thus enhancing models' semantic\nunderstanding for combinatorial creativity. CreTok achieves such redefinition\nby iteratively sampling diverse text pairs from our proposed CangJie dataset to\nform adaptive prompts and restrictive prompts, and then optimizing the\nsimilarity between their respective text embeddings. Extensive experiments\ndemonstrate that <CreTok> enables the universal and direct generation of\ncombinatorial creativity across diverse concepts without additional training,\nachieving state-of-the-art performance with improved text-image alignment and\nhigher human preference ratings. Code will be made available at\nhttps://github.com/fu-feng/CreTok.", "published": "2024-10-31 17:19:03", "link": "http://arxiv.org/abs/2410.24160v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models", "abstract": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.", "published": "2024-10-31 17:42:26", "link": "http://arxiv.org/abs/2410.24175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "abstract": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while raising the question of\nwhether such neutrality is truly the path forward.", "published": "2024-10-31 17:51:00", "link": "http://arxiv.org/abs/2410.24190v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive\n  Reranking", "abstract": "Accurate document retrieval is crucial for the success of retrieval-augmented\ngeneration (RAG) applications, including open-domain question answering and\ncode completion. While large language models (LLMs) have been employed as dense\nencoders or listwise rerankers in RAG systems, they often struggle with\nreasoning-intensive tasks because they lack nuanced analysis when judging\ndocument relevance. To address this limitation, we introduce JudgeRank, a novel\nagentic reranker that emulates human cognitive processes when assessing\ndocument relevance. Our approach consists of three key steps: (1) query\nanalysis to identify the core problem, (2) document analysis to extract a\nquery-aware summary, and (3) relevance judgment to provide a concise assessment\nof document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT\nbenchmark, demonstrating substantial performance improvements over first-stage\nretrieval methods and outperforming other popular reranking approaches. In\naddition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers\non the popular BEIR benchmark, validating its zero-shot generalization\ncapability. Through comprehensive ablation studies, we demonstrate that\nJudgeRank's performance generalizes well across LLMs of various sizes while\nensembling them yields even more accurate reranking than individual models.", "published": "2024-10-31 18:43:12", "link": "http://arxiv.org/abs/2411.00142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State\n  Tracking", "abstract": "Zero-shot domain adaptation for dialogue state tracking (DST) remains a\nchallenging problem in task-oriented dialogue (TOD) systems, where models must\ngeneralize to target domains unseen at training time. Current large language\nmodel approaches for zero-shot domain adaptation rely on prompting to introduce\nknowledge pertaining to the target domains. However, their efficacy strongly\ndepends on prompt engineering, as well as the zero-shot ability of the\nunderlying language model. In this work, we devise a novel data augmentation\napproach, Schema Augmentation, that improves the zero-shot domain adaptation of\nlanguage models through fine-tuning. Schema Augmentation is a simple but\neffective technique that enhances generalization by introducing variations of\nslot names within the schema provided in the prompt. Experiments on MultiWOZ\nand SpokenWOZ showed that the proposed approach resulted in a substantial\nimprovement over the baseline, in some experiments achieving over a twofold\naccuracy gain over unseen domains while maintaining equal or superior\nperformance over all domains.", "published": "2024-10-31 18:57:59", "link": "http://arxiv.org/abs/2411.00150v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Label Attention: Transparency in Language Models for Automated\n  Medical Coding via Dictionary Learning", "abstract": "Medical coding, the translation of unstructured clinical text into\nstandardized medical codes, is a crucial but time-consuming healthcare\npractice. Though large language models (LLM) could automate the coding process\nand improve the efficiency of such tasks, interpretability remains paramount\nfor maintaining patient trust. Current efforts in interpretability of medical\ncoding applications rely heavily on label attention mechanisms, which often\nleads to the highlighting of extraneous tokens irrelevant to the ICD code. To\nfacilitate accurate interpretability in medical language models, this paper\nleverages dictionary learning that can efficiently extract sparsely activated\nrepresentations from dense language model embeddings in superposition. Compared\nwith common label attention mechanisms, our model goes beyond token-level\nrepresentations by building an interpretable dictionary which enhances the\nmechanistic-based explanations for each ICD code prediction, even when the\nhighlighted tokens are medically irrelevant. We show that dictionary features\ncan steer model behavior, elucidate the hidden meanings of upwards of 90% of\nmedically irrelevant tokens, and are human interpretable.", "published": "2024-10-31 19:39:40", "link": "http://arxiv.org/abs/2411.00173v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction", "abstract": "Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction.", "published": "2024-10-31 19:48:12", "link": "http://arxiv.org/abs/2411.00177v3", "categories": ["cond-mat.mtrl-sci", "cs.CL"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in\n  Small Models Fine-tuned on Data from Larger Models", "abstract": "Recently, there has been an explosion of large language models created\nthrough fine-tuning with data from larger models. These small models able to\nproduce outputs that appear qualitatively similar to significantly larger\nmodels. However, one of the key limitations that have been observed with these\nmodels is their propensity to hallucinate significantly more often than larger\nmodels. In particular, they have been observed to generate coherent outputs\nthat involve factually incorrect information and spread misinformation,\ntoxicity, and stereotypes. There are many potential causes of hallucination, of\nwhich, one hypothesis is that fine-tuning a model on data produced by a larger\nmodel leads to a knowledge mismatch which contributes to hallucination. In\nparticular, it is hypothesized that there is a mismatch between the knowledge\nthat is fed to the model to fine-tune it and the knowledge that is already\npresent in the graph. Fine-tuning the model on data that has such mismatch\ncould contribute to an increased propensity to hallucinate. We show that on an\nunseen test set, a smaller model fine-tuned on data generated from a larger\nmodel produced more wrong answers when compared to models fine-tuned on data\ncreated by the small model, which confirms the hypothesis.", "published": "2024-10-31 13:01:46", "link": "http://arxiv.org/abs/2411.00878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simulating User Agents for Embodied Conversational-AI", "abstract": "Embodied agents designed to assist users with tasks must engage in natural\nlanguage interactions, interpret instructions, execute actions, and communicate\neffectively to resolve issues. However, collecting large-scale, diverse\ndatasets of situated human-robot dialogues to train and evaluate such agents is\nexpensive, labor-intensive, and time-consuming. To address this challenge, we\npropose building a large language model (LLM)-based user agent that can\nsimulate user behavior during interactions with an embodied agent in a virtual\nenvironment. Given a user goal (e.g., make breakfast), at each time step, the\nuser agent may observe\" the robot actions or speak\" to either intervene with\nthe robot or answer questions. Such a user agent assists in improving the\nscalability and efficiency of embodied dialogues dataset generation and is\ncritical for enhancing and evaluating the robot's interaction and task\ncompletion ability, as well as for research in reinforcement learning using AI\nfeedback. We evaluate our user agent's ability to generate human-like behaviors\nby comparing its simulated dialogues with the TEACh dataset. We perform three\nexperiments: zero-shot prompting to predict dialogue acts, few-shot prompting,\nand fine-tuning on the TEACh training subset. Results show the LLM-based user\nagent achieves an F-measure of 42% with zero-shot prompting and 43.4% with\nfew-shot prompting in mimicking human speaking behavior. Through fine-tuning,\nperformance in deciding when to speak remained stable, while deciding what to\nsay improved from 51.1% to 62.5%. These findings showcase the feasibility of\nthe proposed approach for assessing and enhancing the effectiveness of robot\ntask completion through natural language communication.", "published": "2024-10-31 00:56:08", "link": "http://arxiv.org/abs/2410.23535v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "From Context to Action: Analysis of the Impact of State Representation\n  and Context on the Generalization of Multi-Turn Web Navigation Agents", "abstract": "Recent advancements in Large Language Model (LLM)-based frameworks have\nextended their capabilities to complex real-world applications, such as\ninteractive web navigation. These systems, driven by user commands, navigate\nweb browsers to complete tasks through multi-turn dialogues, offering both\ninnovative opportunities and significant challenges. Despite the introduction\nof benchmarks for conversational web navigation, a detailed understanding of\nthe key contextual components that influence the performance of these agents\nremains elusive. This study aims to fill this gap by analyzing the various\ncontextual elements crucial to the functioning of web navigation agents. We\ninvestigate the optimization of context management, focusing on the influence\nof interaction history and web page representation. Our work highlights\nimproved agent performance across out-of-distribution scenarios, including\nunseen websites, categories, and geographic locations through effective context\nmanagement. These findings provide insights into the design and optimization of\nLLM-based agents, enabling more accurate and effective web navigation in\nreal-world applications.", "published": "2024-10-31 01:51:41", "link": "http://arxiv.org/abs/2410.23555v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Kernel Looping: Eliminating Synchronization Boundaries for Peak\n  Inference Performance", "abstract": "Token generation speed is critical to power the next wave of AI inference\napplications. GPUs significantly underperform during token generation due to\nsynchronization overheads at kernel boundaries, utilizing only 21% of their\npeak memory bandwidth. While recent dataflow architectures mitigate these\noverheads by enabling aggressive fusion of decoder layers into a single kernel,\nthey too leave performance on the table due to synchronization penalties at\nlayer boundaries.\n  This paper presents kernel looping, a specialized global optimization\ntechnique which exploits an optimization opportunity brought by combining the\nunique layer-level fusion possible in modern dataflow architectures with the\nrepeated layer structure found in language models. Kernel looping eliminates\nsynchronization costs between consecutive calls to the same kernel by\ntransforming these calls into a single call to a modified kernel containing a\npipelined outer loop. We evaluate kernel looping on the SambaNova SN40L\nReconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.\nExperiments demonstrate that kernel looping speeds up the decode phase of a\nwide array of powerful open-source models by up to 2.2$\\times$ on SN40L. Kernel\nlooping allows scaling of decode performance over multiple SN40L sockets,\nachieving speedups of up to 2.5$\\times$. Finally, kernel looping enables SN40L\nto achieve over 90% of peak performance on 8 and 16 sockets and achieve a\nspeedup of up to 3.7$\\times$ over DGX H100. Kernel looping, as well as the\nmodels evaluated in this paper, are deployed in production in a commercial AI\ninference cloud.", "published": "2024-10-31 06:32:47", "link": "http://arxiv.org/abs/2410.23668v1", "categories": ["cs.CL", "cs.AI", "cs.AR", "D.3.4; C.1.3"], "primary_category": "cs.CL"}
{"title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A\n  Gradient Perspective", "abstract": "What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.", "published": "2024-10-31 08:58:06", "link": "http://arxiv.org/abs/2410.23743v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models", "abstract": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.", "published": "2024-10-31 12:11:17", "link": "http://arxiv.org/abs/2410.23861v1", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue", "abstract": "Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.", "published": "2024-10-31 12:45:54", "link": "http://arxiv.org/abs/2410.23883v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "BitStack: Any-Size Compression of Large Language Models in Variable\n  Memory Environments", "abstract": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.", "published": "2024-10-31 13:26:11", "link": "http://arxiv.org/abs/2410.23918v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech is More Than Words: Do Speech-to-Text Translation Systems\n  Leverage Prosody?", "abstract": "The prosody of a spoken utterance, including features like stress, intonation\nand rhythm, can significantly affect the underlying semantics, and as a\nconsequence can also affect its textual translation. Nevertheless, prosody is\nrarely studied within the context of speech-to-text translation (S2TT) systems.\nIn particular, end-to-end (E2E) systems have been proposed as well-suited for\nprosody-aware translation because they have direct access to the speech signal\nwhen making translation decisions, but the understanding of whether this is\nsuccessful in practice is still limited. A main challenge is the difficulty of\nevaluating prosody awareness in translation. To address this challenge, we\nintroduce an evaluation methodology and a focused benchmark (named ContraProST)\naimed at capturing a wide range of prosodic phenomena. Our methodology uses\nlarge language models and controllable text-to-speech (TTS) to generate\ncontrastive examples. Through experiments in translating English speech into\nGerman, Spanish, and Japanese, we find that (a) S2TT models possess some\ninternal representation of prosody, but the prosody signal is often not strong\nenough to affect the translations, (b) E2E systems outperform cascades of\nspeech recognition and text translation systems, confirming their theoretical\nadvantage in this regard, and (c) certain cascaded systems also capture\nprosodic information in the translation, but only to a lesser extent that\ndepends on the particulars of the transcript's surface form.", "published": "2024-10-31 15:20:50", "link": "http://arxiv.org/abs/2410.24019v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced\n  Protein Sequence Representation", "abstract": "Proteins, essential to biological systems, perform functions intricately\nlinked to their three-dimensional structures. Understanding the relationship\nbetween protein structures and their amino acid sequences remains a core\nchallenge in protein modeling. While traditional protein foundation models\nbenefit from pre-training on vast unlabeled datasets, they often struggle to\ncapture critical co-evolutionary information, which evolutionary-based methods\nexcel at. In this study, we introduce a novel pre-training strategy for protein\nfoundation models that emphasizes the interactions among amino acid residues to\nenhance the extraction of both short-range and long-range co-evolutionary\nfeatures from sequence data. Trained on a large-scale protein sequence dataset,\nour model demonstrates superior generalization ability, outperforming\nestablished baselines of similar size, including the ESM model, across diverse\ndownstream tasks. Experimental results confirm the model's effectiveness in\nintegrating co-evolutionary information, marking a significant step forward in\nprotein sequence-based modeling.", "published": "2024-10-31 15:22:03", "link": "http://arxiv.org/abs/2410.24022v1", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks", "abstract": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.", "published": "2024-10-31 15:30:55", "link": "http://arxiv.org/abs/2410.24032v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "In-Context Fine-Tuning for Time-Series Foundation Models", "abstract": "Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.", "published": "2024-10-31 16:20:04", "link": "http://arxiv.org/abs/2410.24087v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Nearest Neighbor Normalization Improves Multimodal Retrieval", "abstract": "Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.", "published": "2024-10-31 16:44:10", "link": "http://arxiv.org/abs/2410.24114v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices", "abstract": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.", "published": "2024-10-31 17:41:14", "link": "http://arxiv.org/abs/2410.24174v1", "categories": ["cs.CE", "cs.CL", "cs.DC"], "primary_category": "cs.CE"}
{"title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models", "abstract": "Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.", "published": "2024-10-31 17:43:13", "link": "http://arxiv.org/abs/2410.24177v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SelfCodeAlign: Self-Alignment for Code Generation", "abstract": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.", "published": "2024-10-31 17:55:13", "link": "http://arxiv.org/abs/2410.24198v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Length-Induced Embedding Collapse in Transformer-based Models", "abstract": "Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.", "published": "2024-10-31 17:55:36", "link": "http://arxiv.org/abs/2410.24200v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Scalable Reinforcement Post-Training Beyond Static Human Prompts:\n  Evolving Alignment via Asymmetric Self-Play", "abstract": "Current reinforcement learning (RL) frameworks for large language models\n(LLM) post-training typically assume a fixed prompt distribution, which is\nsub-optimal and bottlenecks scalability. Prior works have explored prompt\nevolving, but are often limited to the supervised fine-tuning stage, and\nprompts are sampled and evolved uniformly without signals. This empirical work\npresents a paradigm shift: Evolving Alignment via Asymmetric Self-Play (eva),\nthat casts post-training as an infinite game with regret-based signals for 2\nplayers: (i) a creator, who strategically samples and creates new informative\nprompts and (ii) a solver, who learns to produce preferred responses. eva is\nthe first method that allows language models to adaptively create training\nprompts in both offline and online RL post-training. The design is simple,\neasy-to-use yet remarkably effective: eva sets a new SOTA on challenging\nbenchmarks, without any extra human prompts, e.g. it boosts the win-rate of\ngemma-2-9b-it on Arena-Hard by 51.6% -> 60.1% for DPO and 52.6% -> 62.4% for\nRLOO, surpassing claude-3-opus and catching up to gemini-1.5-pro, both of which\nare orders of magnitude larger. Extensive experiments show eva can create\neffective RL curricula and is robust across ablations. We believe adaptively\nevolving prompts are key to designing the next-generation RL post-training\nscheme.", "published": "2024-10-31 08:15:32", "link": "http://arxiv.org/abs/2411.00062v3", "categories": ["cs.CL", "cs.AI", "physics.data-an", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Interpretable Language Modeling via Induction-head Ngram Models", "abstract": "Recent large language models (LLMs) have excelled across a wide range of\ntasks, but their use in high-stakes and compute-limited settings has\nintensified the demand for interpretability and efficiency. We address this\nneed by proposing Induction-head ngram models (Induction-Gram), a method that\nbuilds an efficient, interpretable LM by bolstering modern ngram models with a\nhand-engineered \"induction head\". This induction head uses a custom neural\nsimilarity metric to efficiently search the model's input context for potential\nnext-word completions. This process enables Induction-Gram to provide\nngram-level grounding for each generated token. Moreover, experiments show that\nthis simple method significantly improves next-word prediction over baseline\ninterpretable models (up to 26%p) and can be used to speed up LLM inference for\nlarge models through speculative decoding. We further study Induction-Gram in a\nnatural-language neuroscience setting, where the goal is to predict the next\nfMRI response in a sequence. It again provides a significant improvement over\ninterpretable models (20% relative increase in the correlation of predicted\nfMRI responses), potentially enabling deeper scientific investigation of\nlanguage selectivity in the brain. The code is available at\nhttps://github.com/ejkim47/induction-gram.", "published": "2024-10-31 12:33:26", "link": "http://arxiv.org/abs/2411.00066v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation", "abstract": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL.", "published": "2024-10-31 16:22:26", "link": "http://arxiv.org/abs/2411.00073v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models", "abstract": "Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.", "published": "2024-10-31 18:59:46", "link": "http://arxiv.org/abs/2411.00154v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional Automata Embeddings for Goal-Conditioned Reinforcement\n  Learning", "abstract": "Goal-conditioned reinforcement learning is a powerful way to control an AI\nagent's behavior at runtime. That said, popular goal representations, e.g.,\ntarget states or natural language, are either limited to Markovian tasks or\nrely on ambiguous task semantics. We propose representing temporal goals using\ncompositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL\nagents. cDFAs balance the need for formal temporal semantics with ease of\ninterpretation: if one can understand a flow chart, one can understand a cDFA.\nOn the other hand, cDFAs form a countably infinite concept class with Boolean\nsemantics, and subtle changes to the automaton can result in very different\ntasks, making them difficult to condition agent behavior on. To address this,\nwe observe that all paths through a DFA correspond to a series of reach-avoid\ntasks and propose pre-training graph neural network embeddings on \"reach-avoid\nderived\" DFAs. Through empirical evaluation, we demonstrate that the proposed\npre-training method enables zero-shot generalization to various cDFA task\nclasses and accelerated policy specialization without the myopic suboptimality\nof hierarchical methods.", "published": "2024-10-31 20:56:07", "link": "http://arxiv.org/abs/2411.00205v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in\n  Large-Scale Reproducible Social Science Research", "abstract": "Large Language Models (LLMs) are distinguished by their architecture, which\ndictates their parameter size and performance capabilities. Social scientists\nhave increasingly adopted LLMs for text classification tasks, which are\ndifficult to scale with human coders. While very large, closed-source models\noften deliver superior performance, their use presents significant risks. These\ninclude lack of transparency, potential exposure of sensitive data, challenges\nto replicability, and dependence on proprietary systems. Additionally, their\nhigh costs make them impractical for large-scale research projects.\n  In contrast, open-source models, although available in various sizes, may\nunderperform compared to commercial alternatives if used without further\nfine-tuning. However, open-source models offer distinct advantages: they can be\nrun locally (ensuring data privacy), fine-tuned for specific tasks, shared\nwithin the research community, and integrated into reproducible workflows.\n  This study demonstrates that small, fine-tuned open-source LLMs can achieve\nequal or superior performance to models such as ChatGPT-4. We further explore\nthe relationship between training set size and fine-tuning efficacy in\nopen-source models. Finally, we propose a hybrid workflow that leverages the\nstrengths of both open and closed models, offering a balanced approach to\nperformance, transparency, and reproducibility.", "published": "2024-10-31 20:26:30", "link": "http://arxiv.org/abs/2411.00890v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "IdeaBench: Benchmarking Large Language Models for Research Idea\n  Generation", "abstract": "Large Language Models (LLMs) have transformed how people interact with\nartificial intelligence (AI) systems, achieving state-of-the-art results in\nvarious tasks, including scientific discovery and hypothesis generation.\nHowever, the lack of a comprehensive and systematic evaluation framework for\ngenerating research ideas using LLMs poses a significant obstacle to\nunderstanding and assessing their generative capabilities in scientific\ndiscovery. To address this gap, we propose IdeaBench, a benchmark system that\nincludes a comprehensive dataset and an evaluation framework for standardizing\nthe assessment of research idea generation using LLMs. Our dataset comprises\ntitles and abstracts from a diverse range of influential papers, along with\ntheir referenced works. To emulate the human process of generating research\nideas, we profile LLMs as domain-specific researchers and ground them in the\nsame context considered by human researchers. This maximizes the utilization of\nthe LLMs' parametric knowledge to dynamically generate new research ideas. We\nalso introduce an evaluation framework for assessing the quality of generated\nresearch ideas. Our evaluation framework is a two-stage process: first, using\nGPT-4o to rank ideas based on user-specified quality indicators such as novelty\nand feasibility, enabling scalable personalization; and second, calculating\nrelative ranking based \"Insight Score\" to quantify the chosen quality\nindicator. The proposed benchmark system will be a valuable asset for the\ncommunity to measure and compare different LLMs, ultimately advancing the\nautomation of the scientific discovery process.", "published": "2024-10-31 17:04:59", "link": "http://arxiv.org/abs/2411.02429v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Artificial intelligence to improve clinical coding practice in\n  Scandinavia: a crossover randomized controlled trial", "abstract": "\\textbf{Trial design} Crossover randomized controlled trial. \\textbf{Methods}\nAn AI tool, Easy-ICD, was developed to assist clinical coders and was tested\nfor improving both accuracy and time in a user study in Norway and Sweden.\nParticipants were randomly assigned to two groups, and crossed over between\ncoding complex (longer) texts versus simple (shorter) texts, while using our\ntool versus not using our tool. \\textbf{Results} Based on Mann-Whitney U test,\nthe median coding time difference for complex clinical text sequences was 123\nseconds (\\emph{P}\\textless.001, 95\\% CI: 81 to 164), representing a 46\\%\nreduction in median coding time when our tool is used. There was no significant\ntime difference for simpler text sequences. For coding accuracy, the\nimprovement we noted for both complex and simple texts was not significant.\n\\textbf{Conclusions} This study demonstrates the potential of AI to transform\ncommon tasks in clinical workflows, with ostensible positive impacts on work\nefficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly\nunderstood.", "published": "2024-10-31 08:24:37", "link": "http://arxiv.org/abs/2410.23725v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Representative Social Choice: From Learning Theory to AI Alignment", "abstract": "Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.", "published": "2024-10-31 14:07:26", "link": "http://arxiv.org/abs/2410.23953v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.GT"], "primary_category": "cs.LG"}
{"title": "Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use", "abstract": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL", "published": "2024-10-31 17:59:52", "link": "http://arxiv.org/abs/2410.24218v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Novel View Acoustic Parameter Estimation", "abstract": "The task of Novel View Acoustic Synthesis (NVAS) - generating Room Impulse\nResponses (RIRs) for unseen source and receiver positions in a scene - has\nrecently gained traction, especially given its relevance to Augmented Reality\n(AR) and Virtual Reality (VR) development. However, many of these efforts\nsuffer from similar limitations: they infer RIRs in the time domain, which\nprove challenging to optimize; they focus on scenes with simple, single-room\ngeometries; they infer only single-channel, directionally-independent acoustic\ncharacteristics; and they require inputs, such as 3D geometry meshes with\nmaterial properties, that may be impractical to obtain for on-device\napplications. On the other hand, research suggests that sample-wise accuracy of\nRIRs is not required for perceptual plausibility in AR and VR. Standard\nacoustic parameters like Clarity Index (C50) or Reverberation Time (T60) have\nbeen shown to capably describe pertinent characteristics of the RIRs,\nespecially late reverberation. To address these gaps, this paper introduces a\nnew task centered on estimating spatially distributed acoustic parameters that\ncan be then used to condition a simple reverberator for arbitrary source and\nreceiver positions. The approach is modelled as an image-to-image translation\ntask, which translates 2D floormaps of a scene into 2D heatmaps of acoustic\nparameters. We introduce a new, large-scale dataset of 1000 scenes consisting\nof complex, multi-room apartment conditions, and show that our method\noutperforms statistical baselines significantly. Moreover, we show that the\nmethod also works for directionally-dependent (i.e. beamformed) parameter\nprediction. Finally, the proposed method operates on very limited information,\nrequiring only a broad outline of the scene and a single RIR at inference time.", "published": "2024-10-31 00:09:13", "link": "http://arxiv.org/abs/2410.23523v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An Empirical Analysis of Speech Self-Supervised Learning at Multiple\n  Resolutions", "abstract": "Self-supervised learning (SSL) models have become crucial in speech\nprocessing, with recent advancements concentrating on developing architectures\nthat capture representations across multiple timescales. The primary goal of\nthese multi-scale architectures is to exploit the hierarchical nature of\nspeech, where lower-resolution components aim to capture representations that\nalign with increasingly abstract concepts (e.g., from phones to words to\nsentences). Although multi-scale approaches have demonstrated some improvements\nover single-scale models, the precise reasons for these enhancements have poor\nempirical support. In this study, we present an initial analysis of layer-wise\nrepresentations in multi-scale architectures, with a focus on Canonical\nCorrelation Analysis (CCA) and Mutual Information (MI). We apply this analysis\nto Multi-Resolution HuBERT (MR-HuBERT) and find that (1) the improved\nperformance on SUPERB tasks is primarily due to the auxiliary low-resolution\nloss rather than the downsampling itself, and (2) downsampling to lower\nresolutions neither improves downstream performance nor correlates with\nhigher-level information (e.g., words), though it does improve computational\nefficiency. These findings challenge assumptions about the multi-scale nature\nof MR-HuBERT and motivate the importance of disentangling computational\nefficiency from learning better representations.", "published": "2024-10-31 14:09:05", "link": "http://arxiv.org/abs/2410.23955v1", "categories": ["eess.AS", "cs.LG", "I.2.0"], "primary_category": "eess.AS"}
{"title": "Task-Aware Unified Source Separation", "abstract": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts.", "published": "2024-10-31 14:40:48", "link": "http://arxiv.org/abs/2410.23987v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cough-E: A multimodal, privacy-preserving cough detection algorithm for\n  the edge", "abstract": "Continuous cough monitors can greatly aid doctors in home monitoring and\ntreatment of respiratory diseases. Although many algorithms have been proposed,\nthey still face limitations in data privacy and short-term monitoring. Edge-AI\noffers a promising solution by processing privacy-sensitive data near the\nsource, but challenges arise in deploying resource-intensive algorithms on\nconstrained devices. From a suitable selection of audio and kinematic signals,\nour methodology aims at the optimal selection of features via Recursive Feature\nElimination with Cross-Validation (RFECV), which exploits the explainability of\nthe selected XGB model. Additionally, it analyzes the use of Mel spectrogram\nfeatures, instead of the more common MFCC. Moreover, a set of hyperparameters\nfor a multimodal implementation of the classifier is explored. Finally, it\nevaluates the performance based on clinically relevant event-based metrics. We\napply our methodology to develop Cough-E, an energy-efficient, multimodal and\nedge AI cough detection algorithm. It exploits audio and kinematic data in two\ndistinct classifiers, jointly cooperating for a balanced energy and performance\ntrade-off. We demonstrate that our algorithm can be executed in real-time on an\nARM Cortex M33 microcontroller. Cough-E achieves a 70.56\\% energy saving when\ncompared to the audio-only approach, at the cost of a 1.26\\% relative\nperformance drop, resulting in a 0.78 F1-score. Both Cough-E and the edge-aware\nmodel optimization methodology are publicly available as open-source code. This\napproach demonstrates the benefits of the proposed hardware-aware methodology\nto enable privacy-preserving cough monitors on the edge, paving the way to\nefficient cough monitoring.", "published": "2024-10-31 16:00:19", "link": "http://arxiv.org/abs/2410.24066v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Angular Distance Distribution Loss for Audio Classification", "abstract": "Classification is a pivotal task in deep learning not only because of its\nintrinsic importance, but also for providing embeddings with desirable\nproperties in other tasks. To optimize these properties, a wide variety of loss\nfunctions have been proposed that attempt to minimize the intra-class distance\nand maximize the inter-class distance in the embeddings space. In this paper we\nargue that, in addition to these two, eliminating hierarchies within and among\nclasses are two other desirable properties for classification embeddings.\nFurthermore, we propose the Angular Distance Distribution (ADD) Loss, which\naims to enhance the four previous properties jointly. For this purpose, it\nimposes conditions on the first and second order statistical moments of the\nangular distance between embeddings. Finally, we perform experiments showing\nthat our loss function improves all four properties and, consequently, performs\nbetter than other loss functions in audio classification tasks.", "published": "2024-10-31 18:59:34", "link": "http://arxiv.org/abs/2411.00153v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neurobench: DCASE 2020 Acoustic Scene Classification benchmark on\n  XyloAudio 2", "abstract": "XyloAudio is a line of ultra-low-power audio inference chips, designed for\nin- and near-microphone analysis of audio in real-time energy-constrained\nscenarios. Xylo is designed around a highly efficient integer-logic processor\nwhich simulates parameter- and activity-sparse spiking neural networks (SNNs)\nusing a leaky integrate-and-fire (LIF) neuron model. Neurons on Xylo are\nquantised integer devices operating in synchronous digital CMOS, with neuron\nand synapse state quantised to 16 bit, and weight parameters quantised to 8\nbit. Xylo is tailored for real-time streaming operation, as opposed to\naccelerated-time operation in the case of an inference accelerator. XyloAudio\nincludes a low-power audio encoding interface for direct connection to a\nmicrophone, designed for sparse encoding of incident audio for further\nprocessing by the inference core. In this report we present the results of\nDCASE 2020 acoustic scene classification audio benchmark dataset deployed to\nXyloAudio 2. We describe the benchmark dataset; the audio preprocessing\napproach; and the network architecture and training approach. We present the\nperformance of the trained model, and the results of power and latency\nmeasurements performed on the XyloAudio 2 development kit. This benchmark is\nconducted as part of the Neurobench project.", "published": "2024-10-31 09:48:12", "link": "http://arxiv.org/abs/2410.23776v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing\n  Audio Generation Challenge", "abstract": "This paper presents the NPU-HWC system submitted to the ISCSLP 2024\nInspirational and Convincing Audio Generation Challenge 2024 (ICAGC). Our\nsystem consists of two modules: a speech generator for Track 1 and a background\naudio generator for Track 2. In Track 1, we employ Single-Codec to tokenize the\nspeech into discrete tokens and use a language-model-based approach to achieve\nzero-shot speaking style cloning. The Single-Codec effectively decouples timbre\nand speaking style at the token level, reducing the acoustic modeling burden on\nthe autoregressive language model. Additionally, we use DSPGAN to upsample 16\nkHz mel-spectrograms to high-fidelity 48 kHz waveforms. In Track 2, we propose\na background audio generator based on large language models (LLMs). This system\nproduces scene-appropriate accompaniment descriptions, synthesizes background\naudio with Tango 2, and integrates it with the speech generated by our Track 1\nsystem. Our submission achieves the second place and the first place in Track 1\nand Track 2 respectively.", "published": "2024-10-31 10:58:59", "link": "http://arxiv.org/abs/2410.23815v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "I Can Hear You: Selective Robust Training for Deepfake Audio Detection", "abstract": "Recent advances in AI-generated voices have intensified the challenge of\ndetecting deepfake audio, posing risks for scams and the spread of\ndisinformation. To tackle this issue, we establish the largest public voice\ndataset to date, named DeepFakeVox-HQ, comprising 1.3 million samples,\nincluding 270,000 high-quality deepfake samples from 14 diverse sources.\nDespite previously reported high accuracy, existing deepfake voice detectors\nstruggle with our diversely collected dataset, and their detection success\nrates drop even further under realistic corruptions and adversarial attacks. We\nconduct a holistic investigation into factors that enhance model robustness and\nshow that incorporating a diversified set of voice augmentations is beneficial.\nMoreover, we find that the best detection models often rely on high-frequency\nfeatures, which are imperceptible to humans and can be easily manipulated by an\nattacker. To address this, we propose the F-SAT: Frequency-Selective\nAdversarial Training method focusing on high-frequency components. Empirical\nresults demonstrate that using our training dataset boosts baseline model\nperformance (without robust training) by 33%, and our robust training further\nimproves accuracy by 7.7% on clean samples and by 29.3% on corrupted and\nattacked samples, over the state-of-the-art RawNet3 model.", "published": "2024-10-31 18:21:36", "link": "http://arxiv.org/abs/2411.00121v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Machine Learning Framework for Audio-Based Content Evaluation using\n  MFCC, Chroma, Spectral Contrast, and Temporal Feature Engineering", "abstract": "This study presents a machine learning framework for assessing similarity\nbetween audio content and predicting sentiment score. We construct a dataset\ncontaining audio samples from music covers on YouTube along with the audio of\nthe original song, and sentiment scores derived from user comments, serving as\nproxy labels for content quality. Our approach involves extensive\npre-processing, segmenting audio signals into 30-second windows, and extracting\nhigh-dimensional feature representations through Mel-Frequency Cepstral\nCoefficients (MFCC), Chroma, Spectral Contrast, and Temporal characteristics.\nLeveraging these features, we train regression models to predict sentiment\nscores on a 0-100 scale, achieving root mean square error (RMSE) values of\n3.420, 5.482, 2.783, and 4.212, respectively. Improvements over a baseline\nmodel based on absolute difference metrics are observed. These results\ndemonstrate the potential of machine learning to capture sentiment and\nsimilarity in audio, offering an adaptable framework for AI applications in\nmedia analysis.", "published": "2024-10-31 20:26:26", "link": "http://arxiv.org/abs/2411.00195v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.6; I.5.4; H.5.5"], "primary_category": "cs.SD"}
{"title": "Improving snore detection under limited dataset through\n  harmonic/percussive source separation and convolutional neural networks", "abstract": "Snoring, an acoustic biomarker commonly observed in individuals with\nObstructive Sleep Apnoea Syndrome (OSAS), holds significant potential for\ndiagnosing and monitoring this recognized clinical disorder. Irrespective of\nsnoring types, most snoring instances exhibit identifiable harmonic patterns\nmanifested through distinctive energy distributions over time. In this work, we\npropose a novel method to differentiate monaural snoring from non-snoring\nsounds by analyzing the harmonic content of the input sound using\nharmonic/percussive sound source separation (HPSS). The resulting feature,\nbased on the harmonic spectrogram from HPSS, is employed as input data for\nconventional neural network architectures, aiming to enhance snoring detection\nperformance even under a limited data learning framework. To evaluate the\nperformance of our proposal, we studied two different scenarios: 1) using a\nlarge dataset of snoring and interfering sounds, and 2) using a reduced\ntraining set composed of around 1% of the data material. In the former\nscenario, the proposed HPSS-based feature provides competitive results compared\nto other input features from the literature. However, the key advantage of the\nproposed method lies in the superior performance of the harmonic spectrogram\nderived from HPSS in a limited data learning context. In this particular\nscenario, using the proposed harmonic feature significantly enhances the\nperformance of all the studied architectures in comparison to the classical\ninput features documented in the existing literature. This finding clearly\ndemonstrates that incorporating harmonic content enables more reliable learning\nof the essential time-frequency characteristics that are prevalent in most\nsnoring sounds, even in scenarios where the amount of training data is limited.", "published": "2024-10-31 10:27:48", "link": "http://arxiv.org/abs/2410.23796v1", "categories": ["cs.SD", "cs.AI", "cs.ET", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
