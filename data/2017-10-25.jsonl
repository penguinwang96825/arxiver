{"title": "Linking Tweets with Monolingual and Cross-Lingual News using Transformed\n  Word Embeddings", "abstract": "Social media platforms have grown into an important medium to spread\ninformation about an event published by the traditional media, such as news\narticles. Grouping such diverse sources of information that discuss the same\ntopic in varied perspectives provide new insights. But the gap in word usage\nbetween informal social media content such as tweets and diligently written\ncontent (e.g. news articles) make such assembling difficult. In this paper, we\npropose a transformation framework to bridge the word usage gap between tweets\nand online news articles across languages by leveraging their word embeddings.\nUsing our framework, word embeddings extracted from tweets and news articles\nare aligned closer to each other across languages, thus facilitating the\nidentification of similarity between news articles and tweets. Experimental\nresults show a notable improvement over baselines for monolingual tweets and\nnews articles comparison, while new findings are reported for cross-lingual\ncomparison.", "published": "2017-10-25 09:45:58", "link": "http://arxiv.org/abs/1710.09137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Use of Text Classification in the Legal Domain", "abstract": "In this paper, we investigate the application of text classification methods\nto support law professionals. We present several experiments applying machine\nlearning techniques to predict with high accuracy the ruling of the French\nSupreme Court and the law area to which a case belongs to. We also investigate\nthe influence of the time period in which a ruling was made on the form of the\ncase description and the extent to which we need to mask information in a full\ncase ruling to automatically obtain training and test data that resembles case\ndescriptions. We developed a mean probability ensemble system combining the\noutput of multiple SVM classifiers. We report results of 98% average F1 score\nin predicting a case ruling, 96% F1 score for predicting the law area of a\ncase, and 87.07% F1 score on estimating the date of a ruling.", "published": "2017-10-25 15:34:52", "link": "http://arxiv.org/abs/1710.09306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Projective Dependency Parsing with Non-Local Transitions", "abstract": "We present a novel transition system, based on the Covington non-projective\nparser, introducing non-local transitions that can directly create arcs\ninvolving nodes to the left of the current focus positions. This avoids the\nneed for long sequences of No-Arc transitions to create long-distance arcs,\nthus alleviating error propagation. The resulting parser outperforms the\noriginal version and achieves the best accuracy on the Stanford Dependencies\nconversion of the Penn Treebank among greedy transition-based algorithms.", "published": "2017-10-25 16:57:51", "link": "http://arxiv.org/abs/1710.09340v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Re-evaluating the need for Modelling Term-Dependence in Text\n  Classification Problems", "abstract": "A substantial amount of research has been carried out in developing machine\nlearning algorithms that account for term dependence in text classification.\nThese algorithms offer acceptable performance in most cases but they are\nassociated with a substantial cost. They require significantly greater\nresources to operate. This paper argues against the justification of the higher\ncosts of these algorithms, based on their performance in text classification\nproblems. In order to prove the conjecture, the performance of one of the best\ndependence models is compared to several well established algorithms in text\nclassification. A very specific collection of datasets have been designed,\nwhich would best reflect the disparity in the nature of text data, that are\npresent in real world applications. The results show that even one of the best\nterm dependence models, performs decent at best when compared to other\nindependence models. Coupled with their substantially greater requirement for\nhardware resources for operation, this makes them an impractical choice for\nbeing used in real world scenarios.", "published": "2017-10-25 06:26:28", "link": "http://arxiv.org/abs/1710.09085v1", "categories": ["cs.IR", "cs.CL", "68P20"], "primary_category": "cs.IR"}
{"title": "Trace norm regularization and faster inference for embedded speech\n  recognition RNNs", "abstract": "We propose and evaluate new techniques for compressing and speeding up dense\nmatrix multiplications as found in the fully connected and recurrent layers of\nneural networks for embedded large vocabulary continuous speech recognition\n(LVCSR). For compression, we introduce and study a trace norm regularization\ntechnique for training low rank factored versions of matrix multiplications.\nCompared to standard low rank training, we show that our method leads to good\naccuracy versus number of parameter trade-offs and can be used to speed up\ntraining of large models. For speedup, we enable faster inference on ARM\nprocessors through new open sourced kernels optimized for small batch sizes,\nresulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond\nLVCSR, we expect our techniques and kernels to be more generally applicable to\nembedded neural networks with large fully connected or recurrent layers.", "published": "2017-10-25 00:20:55", "link": "http://arxiv.org/abs/1710.09026v2", "categories": ["cs.LG", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Relative Transfer Function Inverse Regression from Low Dimensional\n  Manifold", "abstract": "In room acoustic environments, the Relative Transfer Functions (RTFs) are\ncontrolled by few underlying modes of variability. Accordingly, they are\nconfined to a low-dimensional manifold. In this letter, we investigate a RTF\ninverse regression problem, the task of which is to generate the\nhigh-dimensional responses from their low-dimensional representations. The\nproblem is addressed from a pure data-driven perspective and a supervised Deep\nNeural Network (DNN) model is applied to learn a mapping from the\nsource-receiver poses (positions and orientations) to the frequency domain RTF\nvectors. The experiments show promising results: the model achieves lower\nprediction error of the RTF than the free field assumption. However, it fails\nto compete with the linear interpolation technique in small sampling distances.", "published": "2017-10-25 07:02:02", "link": "http://arxiv.org/abs/1710.09091v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "abstract": "Modern compression algorithms are often the result of laborious\ndomain-specific research; industry standards such as MP3, JPEG, and AMR-WB took\nyears to develop and were largely hand-designed. We present a deep neural\nnetwork model which optimizes all the steps of a wideband speech coding\npipeline (compression, quantization, entropy coding, and decompression)\nend-to-end directly from raw speech data -- no manual feature engineering\nnecessary, and it trains in hours. In testing, our DNN-based coder performs on\npar with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).\nIt also runs in realtime on a 3.8GhZ Intel CPU.", "published": "2017-10-25 03:21:44", "link": "http://arxiv.org/abs/1710.09064v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
