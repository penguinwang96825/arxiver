{"title": "A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing", "abstract": "This work systematically analyzes the smoothing effect of vocabulary\nreduction for phrase translation models. We extensively compare various\nword-level vocabularies to show that the performance of smoothing is not\nsignificantly affected by the choice of vocabulary. This result provides\nempirical evidence that the standard phrase translation model is extremely\nsparse. Our experiments also reveal that vocabulary reduction is more effective\nfor smoothing large-scale phrase tables.", "published": "2019-01-06 17:20:24", "link": "http://arxiv.org/abs/1901.01574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Training for Large Vocabulary Translation Using Sparse\n  Lexicon and Word Classes", "abstract": "We address for the first time unsupervised training for a translation task\nwith hundreds of thousands of vocabulary words. We scale up the\nexpectation-maximization (EM) algorithm to learn a large translation table\nwithout any parallel text or seed lexicon. First, we solve the memory\nbottleneck and enforce the sparsity with a simple thresholding scheme for the\nlexicon. Second, we initialize the lexicon training with word classes, which\nefficiently boosts the performance. Our methods produced promising results on\ntwo large-scale unsupervised translation tasks.", "published": "2019-01-06 17:29:24", "link": "http://arxiv.org/abs/1901.01577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Unsupervised Word-by-Word Translation with Language Model and\n  Denoising Autoencoder", "abstract": "Unsupervised learning of cross-lingual word embedding offers elegant matching\nof words across languages, but has fundamental limitations in translating\nsentences. In this paper, we propose simple yet effective methods to improve\nword-by-word translation of cross-lingual embeddings, using only monolingual\ncorpora but without any back-translation. We integrate a language model for\ncontext-aware search, and use a novel denoising autoencoder to handle\nreordering. Our system surpasses state-of-the-art unsupervised neural\ntranslation systems without costly iterative training. We also analyze the\neffect of vocabulary size and denoising type on the translation performance,\nwhich provides better understanding of learning the cross-lingual word\nembedding and its usage in translation.", "published": "2019-01-06 18:30:50", "link": "http://arxiv.org/abs/1901.01590v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition in Electronic Health Records Using Transfer\n  Learning Bootstrapped Neural Networks", "abstract": "Neural networks (NNs) have become the state of the art in many machine\nlearning applications, especially in image and sound processing [1]. The same,\nalthough to a lesser extent [2,3], could be said in natural language processing\n(NLP) tasks, such as named entity recognition. However, the success of NNs\nremains dependent on the availability of large labelled datasets, which is a\nsignificant hurdle in many important applications. One such case are electronic\nhealth records (EHRs), which are arguably the largest source of medical data,\nmost of which lies hidden in natural text [4,5]. Data access is difficult due\nto data privacy concerns, and therefore annotated datasets are scarce. With\nscarce data, NNs will likely not be able to extract this hidden information\nwith practical accuracy. In our study, we develop an approach that solves these\nproblems for named entity recognition, obtaining 94.6 F1 score in I2B2 2009\nMedical Extraction Challenge [6], 4.3 above the architecture that won the\ncompetition. Beyond the official I2B2 challenge, we further achieve 82.4 F1 on\nextracting relationships between medical terms. To reach this state-of-the-art\naccuracy, our approach applies transfer learning to leverage on datasets\nannotated for other I2B2 tasks, and designs and trains embeddings that\nspecially benefit from such transfer.", "published": "2019-01-06 18:53:12", "link": "http://arxiv.org/abs/1901.01592v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enhancing Sound Texture in CNN-Based Acoustic Scene Classification", "abstract": "Acoustic scene classification is the task of identifying the scene from which\nthe audio signal is recorded. Convolutional neural network (CNN) models are\nwidely adopted with proven successes in acoustic scene classification. However,\nthere is little insight on how an audio scene is perceived in CNN, as what have\nbeen demonstrated in image recognition research. In the present study, the\nClass Activation Mapping (CAM) is utilized to analyze how the log-magnitude\nMel-scale filter-bank (log-Mel) features of different acoustic scenes are\nlearned in a CNN classifier. It is noted that distinct high-energy\ntime-frequency components of audio signals generally do not correspond to\nstrong activation on CAM, while the background sound texture are well learned\nin CNN. In order to make the sound texture more salient, we propose to apply\nthe Difference of Gaussian (DoG) and Sobel operator to process the log-Mel\nfeatures and enhance edge information of the time-frequency image. Experimental\nresults on the DCASE 2017 ASC challenge show that using edge enhanced log-Mel\nimages as input feature of CNN significantly improves the performance of audio\nscene classification.", "published": "2019-01-06 05:21:41", "link": "http://arxiv.org/abs/1901.01502v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
