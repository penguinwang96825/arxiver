{"title": "Selective Demonstrations for Cross-domain Text-to-SQL", "abstract": "Large language models (LLMs) with in-context learning have demonstrated\nimpressive generalization capabilities in the cross-domain text-to-SQL task,\nwithout the use of in-domain annotations. However, incorporating in-domain\ndemonstration examples has been found to greatly enhance LLMs' performance. In\nthis paper, we delve into the key factors within in-domain examples that\ncontribute to the improvement and explore whether we can harness these benefits\nwithout relying on in-domain annotations. Based on our findings, we propose a\ndemonstration selection framework ODIS which utilizes both out-of-domain\nexamples and synthetically generated in-domain examples to construct\ndemonstrations. By retrieving demonstrations from hybrid sources, ODIS\nleverages the advantages of both, showcasing its effectiveness compared to\nbaseline methods that rely on a single data source. Furthermore, ODIS\noutperforms state-of-the-art approaches on two cross-domain text-to-SQL\ndatasets, with improvements of 1.1 and 11.8 points in execution accuracy,\nrespectively.", "published": "2023-10-10 04:31:41", "link": "http://arxiv.org/abs/2310.06302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfoCL: Alleviating Catastrophic Forgetting in Continual Text\n  Classification from An Information Theoretic Perspective", "abstract": "Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. We focus on continual text\nclassification under the class-incremental setting. Recent CL studies have\nidentified the severe performance decrease on analogous classes as a key factor\nfor catastrophic forgetting. In this paper, through an in-depth exploration of\nthe representation learning process in CL, we discover that the compression\neffect of the information bottleneck leads to confusion on analogous classes.\nTo enable the model learn more sufficient representations, we propose a novel\nreplay-based continual text classification method, InfoCL. Our approach\nutilizes fast-slow and current-past contrastive learning to perform mutual\ninformation maximization and better recover the previously learned\nrepresentations. In addition, InfoCL incorporates an adversarial memory\naugmentation strategy to alleviate the overfitting problem of replay.\nExperimental results demonstrate that InfoCL effectively mitigates forgetting\nand achieves state-of-the-art performance on three text classification tasks.\nThe code is publicly available at https://github.com/Yifan-Song793/InfoCL.", "published": "2023-10-10 07:00:13", "link": "http://arxiv.org/abs/2310.06362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity\n  Alignment", "abstract": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify\nequivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,\nthis task faces challenges due to the presence of different types of\ninformation, including neighboring entities, multi-modal attributes, and entity\ntypes. Directly incorporating the above information (e.g., concatenation or\nattention) can lead to an unaligned information space. To address these\nchallenges, we propose a novel MMEA transformer, called MoAlign, that\nhierarchically introduces neighbor features, multi-modal attributes, and entity\ntypes to enhance the alignment task. Taking advantage of the transformer's\nability to better integrate multiple information, we design a hierarchical\nmodifiable self-attention block in a transformer encoder to preserve the unique\nsemantics of different information. Furthermore, we design two entity-type\nprefix injection methods to integrate entity-type information using type\nprefixes, which help to restrict the global information of entities not present\nin the MMKGs. Our extensive experiments on benchmark datasets demonstrate that\nour approach outperforms strong competitors and achieves excellent entity\nalignment performance.", "published": "2023-10-10 07:06:06", "link": "http://arxiv.org/abs/2310.06365v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Model Selection and Decoding for Keyphrase Generation with\n  Pre-trained Sequence-to-Sequence Models", "abstract": "Keyphrase Generation (KPG) is a longstanding task in NLP with widespread\napplications. The advent of sequence-to-sequence (seq2seq) pre-trained language\nmodels (PLMs) has ushered in a transformative era for KPG, yielding promising\nperformance improvements. However, many design decisions remain unexplored and\nare often made arbitrarily. This paper undertakes a systematic analysis of the\ninfluence of model selection and decoding strategies on PLM-based KPG. We begin\nby elucidating why seq2seq PLMs are apt for KPG, anchored by an\nattention-driven hypothesis. We then establish that conventional wisdom for\nselecting seq2seq PLMs lacks depth: (1) merely increasing model size or\nperforming task-specific adaptation is not parameter-efficient; (2) although\ncombining in-domain pre-training with task adaptation benefits KPG, it does\npartially hinder generalization. Regarding decoding, we demonstrate that while\ngreedy search achieves strong F1 scores, it lags in recall compared with\nsampling-based methods. Based on these insights, we propose DeSel, a\nlikelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves\ngreedy search by an average of 4.7% semantic F1 across five datasets. Our\ncollective findings pave the way for deeper future investigations into\nPLM-based KPG.", "published": "2023-10-10 07:34:45", "link": "http://arxiv.org/abs/2310.06374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Humans and language models diverge when predicting repeating text", "abstract": "Language models that are trained on the next-word prediction task have been\nshown to accurately model human behavior in word prediction and reading speed.\nIn contrast with these findings, we present a scenario in which the performance\nof humans and LMs diverges. We collected a dataset of human next-word\npredictions for five stimuli that are formed by repeating spans of text. Human\nand GPT-2 LM predictions are strongly aligned in the first presentation of a\ntext span, but their performance quickly diverges when memory (or in-context\nlearning) begins to play a role. We traced the cause of this divergence to\nspecific attention heads in a middle layer. Adding a power-law recency bias to\nthese attention heads yielded a model that performs much more similarly to\nhumans. We hope that this scenario will spur future work in bringing LMs closer\nto human behavior.", "published": "2023-10-10 08:24:28", "link": "http://arxiv.org/abs/2310.06408v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer\n  for Document Question Answering", "abstract": "We introduce MemSum-DQA, an efficient system for document question answering\n(DQA) that leverages MemSum, a long document extractive summarizer. By\nprefixing each text block in the parsed document with the provided question and\nquestion type, MemSum-DQA selectively extracts text blocks as answers from\ndocuments. On full-document answering tasks, this approach yields a 9%\nimprovement in exact match accuracy over prior state-of-the-art baselines.\nNotably, MemSum-DQA excels in addressing questions related to\nchild-relationship understanding, underscoring the potential of extractive\nsummarization techniques for DQA tasks.", "published": "2023-10-10 09:06:08", "link": "http://arxiv.org/abs/2310.06436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Compass: Predicting Transfer Learning Success in Offensive\n  Language Detection with Cultural Features", "abstract": "The increasing ubiquity of language technology necessitates a shift towards\nconsidering cultural diversity in the machine learning realm, particularly for\nsubjective tasks that rely heavily on cultural nuances, such as Offensive\nLanguage Detection (OLD). Current understanding underscores that these tasks\nare substantially influenced by cultural values, however, a notable gap exists\nin determining if cultural features can accurately predict the success of\ncross-cultural transfer learning for such subjective tasks. Addressing this,\nour study delves into the intersection of cultural features and transfer\nlearning effectiveness. The findings reveal that cultural value surveys indeed\npossess a predictive power for cross-cultural transfer learning success in OLD\ntasks and that it can be further improved using offensive word distance. Based\non these results, we advocate for the integration of cultural information into\ndatasets. Additionally, we recommend leveraging data sources rich in cultural\ninformation, such as surveys, to enhance cultural adaptability. Our research\nsignifies a step forward in the quest for more inclusive, culturally sensitive\nlanguage technologies.", "published": "2023-10-10 09:29:38", "link": "http://arxiv.org/abs/2310.06458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Jailbreak Challenges in Large Language Models", "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.", "published": "2023-10-10 09:44:06", "link": "http://arxiv.org/abs/2310.06474v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Benchmark and Reverse Validation Method for Passage-level\n  Hallucination Detection", "abstract": "Large Language Models (LLMs) have shown their ability to collaborate\neffectively with humans in real-world scenarios. However, LLMs are apt to\ngenerate hallucinations, i.e., makeup incorrect text and unverified\ninformation, which can cause significant damage when deployed for\nmission-critical tasks. In this paper, we propose a self-check approach based\non reverse validation to detect factual errors automatically in a zero-resource\nfashion. To facilitate future studies and assess different methods, we\nconstruct a hallucination detection benchmark named PHD, which is generated by\nChatGPT and annotated by human annotators. Contrasting previous studies of\nzero-resource hallucination detection, our method and benchmark concentrate on\npassage-level detection instead of sentence-level. We empirically evaluate our\nmethod and existing zero-resource detection methods on two datasets. The\nexperimental results demonstrate that the proposed method considerably\noutperforms the baselines while costing fewer tokens and less time.\nFurthermore, we manually analyze some hallucination cases that LLM failed to\ncapture, revealing the shared limitation of zero-resource methods.", "published": "2023-10-10 10:14:59", "link": "http://arxiv.org/abs/2310.06498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment\n  Quadruples: A Comparative Analysis", "abstract": "Recently, ChatGPT has attracted great attention from both industry and\nacademia due to its surprising abilities in natural language understanding and\ngeneration. We are particularly curious about whether it can achieve promising\nperformance on one of the most complex tasks in aspect-based sentiment\nanalysis, i.e., extracting aspect-category-opinion-sentiment quadruples from\ntexts. To this end, in this paper we develop a specialized prompt template that\nenables ChatGPT to effectively tackle this complex quadruple extraction task.\nFurther, we propose a selection method on few-shot examples to fully exploit\nthe in-context learning ability of ChatGPT and uplift its effectiveness on this\ncomplex task. Finally, we provide a comparative evaluation on ChatGPT against\nexisting state-of-the-art quadruple extraction models based on four public\ndatasets and highlight some important findings regarding the capability\nboundaries of ChatGPT in the quadruple extraction.", "published": "2023-10-10 10:19:58", "link": "http://arxiv.org/abs/2310.06502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer\n  Service Dialogues on Twitter", "abstract": "Due to the rise of user-generated content, social media is increasingly\nadopted as a channel to deliver customer service. Given the public character of\nthese online platforms, the automatic detection of emotions forms an important\napplication in monitoring customer satisfaction and preventing negative\nword-of-mouth. This paper introduces EmoTwiCS, a corpus of 9,489 Dutch customer\nservice dialogues on Twitter that are annotated for emotion trajectories. In\nour business-oriented corpus, we view emotions as dynamic attributes of the\ncustomer that can change at each utterance of the conversation. The term\n`emotion trajectory' refers therefore not only to the fine-grained emotions\nexperienced by customers (annotated with 28 labels and\nvalence-arousal-dominance scores), but also to the event happening prior to the\nconversation and the responses made by the human operator (both annotated with\n8 categories). Inter-annotator agreement (IAA) scores on the resulting dataset\nare substantial and comparable with related research, underscoring its high\nquality. Given the interplay between the different layers of annotated\ninformation, we perform several in-depth analyses to investigate (i) static\nemotions in isolated tweets, (ii) dynamic emotions and their shifts in\ntrajectory, and (iii) the role of causes and response strategies in emotion\ntrajectories. We conclude by listing the advantages and limitations of our\ndataset, after which we give some suggestions on the different types of\npredictive modelling tasks and open research questions to which EmoTwiCS can be\napplied. The dataset is available upon request and will be made publicly\navailable upon acceptance of the paper.", "published": "2023-10-10 11:31:11", "link": "http://arxiv.org/abs/2310.06536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech\n  Recognition through Pitch Manipulation", "abstract": "Automatic speech recognition (ASR) systems are known to be sensitive to the\nsociolinguistic variability of speech data, in which gender plays a crucial\nrole. This can result in disparities in recognition accuracy between male and\nfemale speakers, primarily due to the under-representation of the latter group\nin the training data. While in the context of hybrid ASR models several\nsolutions have been proposed, the gender bias issue has not been explicitly\naddressed in end-to-end neural architectures. To fill this gap, we propose a\ndata augmentation technique that manipulates the fundamental frequency (f0) and\nformants. This technique reduces the data unbalance among genders by simulating\nvoices of the under-represented female speakers and increases the variability\nwithin each gender group. Experiments on spontaneous English speech show that\nour technique yields a relative WER improvement up to 9.87% for utterances by\nfemale speakers, with larger gains for the least-represented f0 ranges.", "published": "2023-10-10 12:55:22", "link": "http://arxiv.org/abs/2310.06590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Large Language Models Perform Better in Knowledge Graph\n  Completion", "abstract": "Large language model (LLM) based knowledge graph completion (KGC) aims to\npredict the missing triples in the KGs with LLMs. However, research about\nLLM-based KGC fails to sufficiently harness LLMs' inference proficiencies,\noverlooking critical structural information integral to KGs. In this paper, we\nexplore methods to incorporate structural information into the LLMs, with the\noverarching goal of facilitating structure-aware reasoning. We first discuss on\nthe existing LLM paradigms like in-context learning and instruction tuning,\nproposing basic structural information injection approaches. Then we propose a\nKnowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a\nstructural pre-training phase to comprehend the intricate entities and\nrelations within KGs, representing them as structural embeddings. Then KoPA\ncommunicates such cross-modal structural information understanding to the LLMs\nthrough a knowledge prefix adapter which projects the structural embeddings\ninto the textual space and obtains virtual knowledge tokens positioned as a\nprefix of the input prompt. We conduct comprehensive experiments and provide\nincisive analysis concerning how the introduction of cross-modal structural\ninformation would be better for LLM's factual knowledge reasoning ability. Our\ncode and data are available at https://github.com/zjukg/KoPA .", "published": "2023-10-10 14:47:09", "link": "http://arxiv.org/abs/2310.06671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA", "abstract": "Question answering over hybrid contexts is a complex task, which requires the\ncombination of information extracted from unstructured texts and structured\ntables in various ways. Recently, In-Context Learning demonstrated significant\nperformance advances for reasoning tasks. In this paradigm, a large language\nmodel performs predictions based on a small set of supporting exemplars. The\nperformance of In-Context Learning depends heavily on the selection procedure\nof the supporting exemplars, particularly in the case of HybridQA, where\nconsidering the diversity of reasoning chains and the large size of the hybrid\ncontexts becomes crucial. In this work, we present Selection of ExEmplars for\nhybrid Reasoning (SEER), a novel method for selecting a set of exemplars that\nis both representative and diverse. The key novelty of SEER is that it\nformulates exemplar selection as a Knapsack Integer Linear Program. The\nKnapsack framework provides the flexibility to incorporate diversity\nconstraints that prioritize exemplars with desirable attributes, and capacity\nconstraints that ensure that the prompt size respects the provided capacity\nbudgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two\nreal-world benchmarks for HybridQA, where it outperforms previous exemplar\nselection methods.", "published": "2023-10-10 14:50:20", "link": "http://arxiv.org/abs/2310.06675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large\n  Language Models", "abstract": "Aligned large language models (LLMs) demonstrate exceptional capabilities in\ntask-solving, following instructions, and ensuring safety. However, the\ncontinual learning aspect of these aligned LLMs has been largely overlooked.\nExisting continual learning benchmarks lack sufficient challenge for leading\naligned LLMs, owing to both their simplicity and the models' potential exposure\nduring instruction tuning. In this paper, we introduce TRACE, a novel benchmark\ndesigned to evaluate continual learning in LLMs. TRACE consists of 8 distinct\ndatasets spanning challenging tasks including domain-specific tasks,\nmultilingual capabilities, code generation, and mathematical reasoning. All\ndatasets are standardized into a unified format, allowing for effortless\nautomatic evaluation of LLMs. Our experiments show that after training on\nTRACE, aligned LLMs exhibit significant declines in both general ability and\ninstruction-following capabilities. For example, the accuracy of llama2-chat\n13B on gsm8k dataset declined precipitously from 28.8\\% to 2\\% after training\non our datasets. This highlights the challenge of finding a suitable tradeoff\nbetween achieving performance on specific tasks while preserving the original\nprowess of LLMs. Empirical findings suggest that tasks inherently equipped with\nreasoning paths contribute significantly to preserving certain capabilities of\nLLMs against potential declines. Motivated by this, we introduce the\nReasoning-augmented Continual Learning (RCL) approach. RCL integrates\ntask-specific cues with meta-rationales, effectively reducing catastrophic\nforgetting in LLMs while expediting convergence on novel tasks.", "published": "2023-10-10 16:38:49", "link": "http://arxiv.org/abs/2310.06762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OmniLingo: Listening- and speaking-based language learning", "abstract": "In this demo paper we present OmniLingo, an architecture for distributing\ndata for listening- and speaking-based language learning applications and a\ndemonstration client built using the architecture. The architecture is based on\nthe Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty\nover data.", "published": "2023-10-10 16:40:00", "link": "http://arxiv.org/abs/2310.06764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lemur: Harmonizing Natural Language and Code for Language Agents", "abstract": "We introduce Lemur and Lemur-Chat, openly accessible language models\noptimized for both natural language and coding capabilities to serve as the\nbackbone of versatile language agents. The evolution from language chat models\nto functional language agents demands that models not only master human\ninteraction, reasoning, and planning but also ensure grounding in the relevant\nenvironments. This calls for a harmonious blend of language and coding\ncapabilities in the models. Lemur and Lemur-Chat are proposed to address this\nnecessity, demonstrating balanced proficiencies in both domains, unlike\nexisting open-source models that tend to specialize in either. Through\nmeticulous pre-training using a code-intensive corpus and instruction\nfine-tuning on text and code data, our models achieve state-of-the-art averaged\nperformance across diverse text and coding benchmarks among open-source models.\nComprehensive experiments demonstrate Lemur's superiority over existing\nopen-source models and its proficiency across various agent tasks involving\nhuman communication, tool usage, and interaction under fully- and partially-\nobservable environments. The harmonization between natural and programming\nlanguages enables Lemur-Chat to significantly narrow the gap with proprietary\nmodels on agent abilities, providing key insights into developing advanced\nopen-source agents adept at reasoning, planning, and operating seamlessly\nacross environments. https://github.com/OpenLemur/Lemur", "published": "2023-10-10 17:57:45", "link": "http://arxiv.org/abs/2310.06830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Supervision for Multi-Aspect Sentiment Analysis Without\n  Fine-grained Labels", "abstract": "Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often\ntrained through supervision from human annotations of opinionated texts. These\nfine-grained annotations include identifying aspects towards which a user\nexpresses their sentiment, and their associated polarities (aspect-based\nsentiments). Such fine-grained annotations can be expensive and often\ninfeasible to obtain in real-world settings. There is, however, an abundance of\nscenarios where user-generated text contains an overall sentiment, such as a\nrating of 1-5 in user reviews or user-generated feedback, which may be\nleveraged for this task. In this paper, we propose a VAE-based topic modeling\napproach that performs ABSA using document-level supervision and without\nrequiring fine-grained labels for either aspects or sentiments. Our approach\nallows for the detection of multiple aspects in a document, thereby allowing\nfor the possibility of reasoning about how sentiment expressed through multiple\naspects comes together to form an observable overall document-level sentiment.\nWe demonstrate results on two benchmark datasets from two different domains,\nsignificantly outperforming a state-of-the-art baseline.", "published": "2023-10-10 18:53:21", "link": "http://arxiv.org/abs/2310.06940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why bother with geometry? On the relevance of linear decompositions of\n  Transformer embeddings", "abstract": "A recent body of work has demonstrated that Transformer embeddings can be\nlinearly decomposed into well-defined sums of factors, that can in turn be\nrelated to specific network inputs or components. There is however still a\ndearth of work studying whether these mathematical reformulations are\nempirically meaningful. In the present work, we study representations from\nmachine-translation decoders using two of such embedding decomposition methods.\nOur results indicate that, while decomposition-derived indicators effectively\ncorrelate with model performance, variation across different runs suggests a\nmore nuanced take on this question. The high variability of our measurements\nindicate that geometry reflects model-specific characteristics more than it\ndoes sentence-specific computations, and that similar training conditions do\nnot guarantee similar vector spaces.", "published": "2023-10-10 19:56:10", "link": "http://arxiv.org/abs/2310.06977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crossing the Threshold: Idiomatic Machine Translation through Retrieval\n  Augmentation and Loss Weighting", "abstract": "Idioms are common in everyday language, but often pose a challenge to\ntranslators because their meanings do not follow from the meanings of their\nparts. Despite significant advances, machine translation systems still struggle\nto translate idiomatic expressions. We provide a simple characterization of\nidiomatic translation and related issues. This allows us to conduct a synthetic\nexperiment revealing a tipping point at which transformer-based machine\ntranslation models correctly default to idiomatic translations. To expand\nmultilingual resources, we compile a dataset of ~4k natural sentences\ncontaining idiomatic expressions in French, Finnish, and Japanese. To improve\ntranslation of natural idioms, we introduce two straightforward yet effective\ntechniques: the strategic upweighting of training loss on potentially idiomatic\nsentences, and using retrieval-augmented models. This not only improves the\naccuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in\nabsolute accuracy, but also holds potential benefits for non-idiomatic\nsentences.", "published": "2023-10-10 23:47:25", "link": "http://arxiv.org/abs/2310.07081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp", "abstract": "In the era of digital transformation, customer service is of paramount\nimportance to the success of organizations, and to meet the growing demand for\nimmediate responses and personalized assistance 24 hours a day, chatbots have\nbecome a promising tool to solve these problems. Currently, there are many\ncompanies that need to provide these solutions to their customers, which\nmotivates us to study this problem and offer a suitable solution. The objective\nof this study is to develop a chatbot based on natural language processing to\nimprove customer satisfaction and improve the quality of service provided by\nthe company through WhatsApp. The solution focuses on creating a chatbot that\nefficiently and effectively handles user queries. A literature review related\nto existing chatbots has been conducted, analyzing methodological approaches,\nartificial intelligence techniques and quality attributes used in the\nimplementation of chatbots. The results found highlight that chatbots based on\nnatural language processing enable fast and accurate responses, which improves\nthe efficiency of customer service, as chatbots contribute to customer\nsatisfaction by providing accurate answers and quick solutions to their queries\nat any time. Some authors point out that artificial intelligence techniques,\nsuch as machine learning, improve the learning and adaptability of chatbots as\nuser interactions occur, so a good choice of appropriate natural language\nunderstanding technologies is essential for optimal chatbot performance. The\nresults of this study will provide a solid foundation for the design and\ndevelopment of effective chatbots for customer service, ensuring a satisfactory\nuser experience and thus meeting the needs of the organization.", "published": "2023-10-10 18:54:15", "link": "http://arxiv.org/abs/2310.10675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs as Potential Brainstorming Partners for Math and Science Problems", "abstract": "With the recent rise of widely successful deep learning models, there is\nemerging interest among professionals in various math and science communities\nto see and evaluate the state-of-the-art models' abilities to collaborate on\nfinding or solving problems that often require creativity and thus\nbrainstorming. While a significant chasm still exists between current\nhuman-machine intellectual collaborations and the resolution of complex math\nand science problems, such as the six unsolved Millennium Prize Problems, our\ninitial investigation into this matter reveals a promising step towards\nbridging the divide. This is due to the recent advancements in Large Language\nModels (LLMs). More specifically, we conduct comprehensive case studies to\nexplore both the capabilities and limitations of the current state-of-the-art\nLLM, notably GPT-4, in collective brainstorming with humans.", "published": "2023-10-10 21:16:35", "link": "http://arxiv.org/abs/2310.10677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models", "abstract": "The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well. Code is available on the\nproject website: https://rohinmanvi.github.io/GeoLLM", "published": "2023-10-10 00:03:23", "link": "http://arxiv.org/abs/2310.06213v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evolution of Natural Language Processing Technology: Not Just Language\n  Processing Towards General Purpose AI", "abstract": "Since the invention of computers, communication through natural language\n(actual human language) has been a dream technology. However, natural language\nis extremely difficult to mathematically formulate, making it difficult to\nrealize as an algorithm without considering programming. While there have been\nnumerous technological developments, one cannot say that any results allowing\nfree utilization have been achieved thus far. In the case of language learning\nin humans, for instance when learning one's mother tongue or foreign language,\none must admit that this process is similar to the adage \"practice makes\nperfect\" in principle, even though the learning method is significant up to a\npoint. Deep learning has played a central role in contemporary AI technology in\nrecent years. When applied to natural language processing (NLP), this produced\nunprecedented results. Achievements exceeding the initial predictions have been\nreported from the results of learning vast amounts of textual data using deep\nlearning. For instance, four arithmetic operations could be performed without\nexplicit learning, thereby enabling the explanation of complex images and the\ngeneration of images from corresponding explanatory texts. It is an accurate\nexample of the learner embodying the concept of \"practice makes perfect\" by\nusing vast amounts of textual data. This report provides a technological\nexplanation of how cutting-edge NLP has made it possible to realize the\n\"practice makes perfect\" principle. Additionally, examples of how this can be\napplied to business are provided. We reported in June 2022 in Japanese on the\nNLP movement from late 2021 to early 2022. We would like to summarize this as a\nmemorandum since this is just the initial movement leading to the current large\nlanguage models (LLMs).", "published": "2023-10-10 00:41:38", "link": "http://arxiv.org/abs/2310.06228v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Model Tuning or Prompt Tuning? A Study of Large Language Models for\n  Clinical Concept and Relation Extraction", "abstract": "Objective To develop soft prompt-based learning algorithms for large language\nmodels (LLMs), examine the shape of prompts, prompt-tuning using\nfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.\nMethods We developed a soft prompt-based LLM model and compared 4 training\nstrategies including (1) fine-tuning without prompts; (2) hard-prompt with\nunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with\nfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for\nclinical concept and relation extraction on two benchmark datasets. We\nevaluated the transfer learning ability of the prompt-based learning algorithms\nin a cross-institution setting. We also assessed the few-shot learning ability.\nResults and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft\nprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept\nextraction, outperforming the traditional fine-tuning and hard prompt-based\nmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft\nprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end\nrelation extraction, outperforming the other two models by 0.2~2% and\n0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million\nparameters) LLMs have a big gap to be competitive with unfrozen models; scaling\nLLMs up to billions of parameters makes frozen LLMs competitive with unfrozen\nLLMs. For cross-institute evaluation, soft prompting with a frozen\nGatorTron-8.9B model achieved the best performance. This study demonstrates\nthat (1) machines can learn soft prompts better than humans, (2) frozen LLMs\nhave better few-shot learning ability and transfer learning ability to\nfacilitate muti-institution applications, and (3) frozen LLMs require large\nmodels.", "published": "2023-10-10 01:27:08", "link": "http://arxiv.org/abs/2310.06239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "We are what we repeatedly do: Inducing and deploying habitual schemas in\n  persona-based responses", "abstract": "Many practical applications of dialogue technology require the generation of\nresponses according to a particular developer-specified persona. While a\nvariety of personas can be elicited from recent large language models, the\nopaqueness and unpredictability of these models make it desirable to be able to\nspecify personas in an explicit form. In previous work, personas have typically\nbeen represented as sets of one-off pieces of self-knowledge that are retrieved\nby the dialogue system for use in generation. However, in realistic human\nconversations, personas are often revealed through story-like narratives that\ninvolve rich habitual knowledge -- knowledge about kinds of events that an\nagent often participates in (e.g., work activities, hobbies, sporting\nactivities, favorite entertainments, etc.), including typical goals,\nsub-events, preconditions, and postconditions of those events. We capture such\nhabitual knowledge using an explicit schema representation, and propose an\napproach to dialogue generation that retrieves relevant schemas to condition a\nlarge language model to generate persona-based responses. Furthermore, we\ndemonstrate a method for bootstrapping the creation of such schemas by first\ngenerating generic passages from a set of simple facts, and then inducing\nschemas from the generated passages.", "published": "2023-10-10 01:44:47", "link": "http://arxiv.org/abs/2310.06245v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Get the gist? Using large language models for few-shot\n  decontextualization", "abstract": "In many NLP applications that involve interpreting sentences within a rich\ncontext -- for instance, information retrieval systems or dialogue systems --\nit is desirable to be able to preserve the sentence in a form that can be\nreadily understood without context, for later reuse -- a process known as\n``decontextualization''. While previous work demonstrated that generative\nSeq2Seq models could effectively perform decontextualization after being\nfine-tuned on a specific dataset, this approach requires expensive human\nannotations and may not transfer to other domains. We propose a few-shot method\nof decontextualization using a large language model, and present preliminary\nresults showing that this method achieves viable performance on multiple\ndomains using only a small set of examples.", "published": "2023-10-10 02:00:00", "link": "http://arxiv.org/abs/2310.06254v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Mitigating Hallucination in Large Language Models via\n  Self-Reflection", "abstract": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.", "published": "2023-10-10 03:05:44", "link": "http://arxiv.org/abs/2310.06271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Semantic Invariant Robust Watermark for Large Language Models", "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely\nhigh accuracy in detecting text generated by LLMs. Such algorithms typically\ninvolve adding extra watermark logits to the LLM's logits at each generation\nstep. However, prior algorithms face a trade-off between attack robustness and\nsecurity robustness. This is because the watermark logits for a token are\ndetermined by a certain number of preceding tokens; a small number leads to low\nsecurity robustness, while a large number results in insufficient attack\nrobustness. In this work, we propose a semantic invariant watermarking method\nfor LLMs that provides both attack robustness and security robustness. The\nwatermark logits in our work are determined by the semantics of all preceding\ntokens. Specifically, we utilize another embedding LLM to generate semantic\nembeddings for all preceding tokens, and then these semantic embeddings are\ntransformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our\nmethod in semantically invariant settings: synonym substitution and text\nparaphrasing settings. Finally, we also show that our watermark possesses\nadequate security robustness. Our code and data are available at\n\\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}.\nAdditionally, our algorithm could also be accessed through MarkLLM\n\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.", "published": "2023-10-10 06:49:43", "link": "http://arxiv.org/abs/2310.06356v3", "categories": ["cs.CR", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CR"}
{"title": "Large Language Models for Propaganda Detection", "abstract": "The prevalence of propaganda in our digital society poses a challenge to\nsocietal harmony and the dissemination of truth. Detecting propaganda through\nNLP in text is challenging due to subtle manipulation techniques and contextual\ndependencies. To address this issue, we investigate the effectiveness of modern\nLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.\nWe conduct experiments using the SemEval-2020 task 11 dataset, which features\nnews articles labeled with 14 propaganda techniques as a multi-label\nclassification problem. Five variations of GPT-3 and GPT-4 are employed,\nincorporating various prompt engineering and fine-tuning strategies across the\ndifferent models. We evaluate the models' performance by assessing metrics such\nas $F1$ score, $Precision$, and $Recall$, comparing the results with the\ncurrent state-of-the-art approach using RoBERTa. Our findings demonstrate that\nGPT-4 achieves comparable results to the current state-of-the-art. Further,\nthis study analyzes the potential and challenges of LLMs in complex tasks like\npropaganda detection.", "published": "2023-10-10 08:46:10", "link": "http://arxiv.org/abs/2310.06422v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constructive Large Language Models Alignment with Diverse Feedback", "abstract": "In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.", "published": "2023-10-10 09:20:14", "link": "http://arxiv.org/abs/2310.06450v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion", "abstract": "Since its launch in November 2022, ChatGPT has had a transformative effect on\neducation where students are using it to help with homework assignments and\nteachers are actively employing it in their teaching practices. This includes\nusing ChatGPT as a tool for writing teachers to grade and generate feedback on\nstudents' essays. In this study, we evaluated the quality of the feedback\ngenerated by ChatGPT regarding the coherence and cohesion of the essays written\nby English Language Learners (ELLs) students. We selected 50 argumentative\nessays and generated feedback on coherence and cohesion using the ELLIPSE\nrubric. During the feedback evaluation, we used a two-step approach: first,\neach sentence in the feedback was classified into subtypes based on its\nfunction (e.g., positive reinforcement, problem statement). Next, we evaluated\nits accuracy and usability according to these types. Both the analysis of\nfeedback types and the evaluation of accuracy and usability revealed that most\nfeedback sentences were highly abstract and generic, failing to provide\nconcrete suggestions for improvement. The accuracy in detecting major problems,\nsuch as repetitive ideas and the inaccurate use of cohesive devices, depended\non superficial linguistic features and was often incorrect. In conclusion,\nChatGPT, without specific training for the feedback generation task, does not\noffer effective feedback on ELL students' coherence and cohesion.", "published": "2023-10-10 10:25:56", "link": "http://arxiv.org/abs/2310.06505v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rationale-Enhanced Language Models are Better Continual Relation\n  Learners", "abstract": "Continual relation extraction (CRE) aims to solve the problem of catastrophic\nforgetting when learning a sequence of newly emerging relations. Recent CRE\nstudies have found that catastrophic forgetting arises from the model's lack of\nrobustness against future analogous relations. To address the issue, we\nintroduce rationale, i.e., the explanations of relation classification results\ngenerated by large language models (LLM), into CRE task. Specifically, we\ndesign the multi-task rationale tuning strategy to help the model learn current\nrelations robustly. We also conduct contrastive rationale replay to further\ndistinguish analogous relations. Experimental results on two standard\nbenchmarks demonstrate that our method outperforms the state-of-the-art CRE\nmodels.", "published": "2023-10-10 11:50:27", "link": "http://arxiv.org/abs/2310.06547v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated clinical coding using off-the-shelf large language models", "abstract": "The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.", "published": "2023-10-10 11:56:48", "link": "http://arxiv.org/abs/2310.06552v3", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.8"], "primary_category": "cs.AI"}
{"title": "FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics", "abstract": "Despite the massive success of fine-tuning Pre-trained Language Models\n(PLMs), they remain susceptible to out-of-distribution input. Dataset\ncartography is a simple yet effective dual-model approach that improves the\nrobustness of fine-tuned PLMs. It involves fine-tuning a model on the original\ntraining set (i.e. reference model), selecting a subset of important training\ninstances based on the training dynamics, and fine-tuning again only on these\nselected examples (i.e. main model). However, this approach requires\nfine-tuning the same model twice, which is computationally expensive for large\nPLMs. In this paper, we show that (1) training dynamics are highly transferable\nacross model sizes and pre-training methods, and that (2) fine-tuning main\nmodels using these selected training instances achieves higher training\nefficiency than empirical risk minimization (ERM). Building on these\nobservations, we propose a novel fine-tuning approach: Fine-Tuning by\ntransFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT\nuses more efficient reference models and aggressive early stopping. FTFT\nachieves robustness improvements over ERM while lowering the training cost by\nup to $\\sim 50\\%$.", "published": "2023-10-10 12:53:48", "link": "http://arxiv.org/abs/2310.06588v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic-DPR: Topic-based Prompts for Dense Passage Retrieval", "abstract": "Prompt-based learning's efficacy across numerous natural language processing\ntasks has led to its integration into dense passage retrieval. Prior research\nhas mainly focused on enhancing the semantic understanding of pre-trained\nlanguage models by optimizing a single vector as a continuous prompt. This\napproach, however, leads to a semantic space collapse; identical semantic\ninformation seeps into all representations, causing their distributions to\nconverge in a restricted region. This hinders differentiation between relevant\nand irrelevant passages during dense retrieval. To tackle this issue, we\npresent Topic-DPR, a dense passage retrieval model that uses topic-based\nprompts. Unlike the single prompt method, multiple topic-based prompts are\nestablished over a probabilistic simplex and optimized simultaneously through\ncontrastive learning. This encourages representations to align with their topic\ndistributions, improving space uniformity. Furthermore, we introduce a novel\npositive and negative sampling strategy, leveraging semi-structured data to\nboost dense retrieval efficiency. Experimental results from two datasets affirm\nthat our method surpasses previous state-of-the-art retrieval techniques.", "published": "2023-10-10 13:45:24", "link": "http://arxiv.org/abs/2310.06626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Representation Learning for Online Handwriting Text\n  Classification", "abstract": "Self-supervised learning offers an efficient way of extracting rich\nrepresentations from various types of unlabeled data while avoiding the cost of\nannotating large-scale datasets. This is achievable by designing a pretext task\nto form pseudo labels with respect to the modality and domain of the data.\nGiven the evolving applications of online handwritten texts, in this study, we\npropose the novel Part of Stroke Masking (POSM) as a pretext task for\npretraining models to extract informative representations from the online\nhandwriting of individuals in English and Chinese languages, along with two\nsuggested pipelines for fine-tuning the pretrained models. To evaluate the\nquality of the extracted representations, we use both intrinsic and extrinsic\nevaluation methods. The pretrained models are fine-tuned to achieve\nstate-of-the-art results in tasks such as writer identification, gender\nclassification, and handedness classification, also highlighting the\nsuperiority of utilizing the pretrained models over the models trained from\nscratch.", "published": "2023-10-10 14:07:49", "link": "http://arxiv.org/abs/2310.06645v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Multiplex Representations on Text-Attributed Graphs with One\n  Language Model Encoder", "abstract": "In real-world scenarios, texts in a graph are often linked by multiple\nsemantic relations (e.g., papers in an academic graph are referenced by other\npublications, written by the same author, or published in the same venue),\nwhere text documents and their relations form a multiplex text-attributed\ngraph. Mainstream text representation learning methods use pretrained language\nmodels (PLMs) to generate one embedding for each text unit, expecting that all\ntypes of relations between texts can be captured by these single-view\nembeddings. However, this presumption does not hold particularly in multiplex\ntext-attributed graphs. Along another line of work, multiplex graph neural\nnetworks (GNNs) directly initialize node attributes as a feature vector for\nnode representation learning, but they cannot fully capture the semantics of\nthe nodes' associated texts. To bridge these gaps, we propose METAG, a new\nframework for learning Multiplex rEpresentations on Text-Attributed Graphs. In\ncontrast to existing methods, METAG uses one text encoder to model the shared\nknowledge across relations and leverages a small number of parameters per\nrelation to derive relation-specific representations. This allows the encoder\nto effectively capture the multiplex structures in the graph while also\npreserving parameter efficiency. We conduct experiments on nine downstream\ntasks in five graphs from both academic and e-commerce domains, where METAG\noutperforms baselines significantly and consistently. The code is available at\nhttps://github.com/PeterGriffinJin/METAG.", "published": "2023-10-10 14:59:22", "link": "http://arxiv.org/abs/2310.06684v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with\n  Large Language Models", "abstract": "Large language models (LLMs) have unveiled remarkable reasoning capabilities\nby exploiting chain-of-thought (CoT) prompting, which generates intermediate\nreasoning chains to serve as the rationale for deriving the answer. However,\ncurrent CoT methods either simply employ general prompts such as Let's think\nstep by step, or heavily rely on pre-defined task-specific demonstrations to\nattain preferable performances, thereby engendering an inescapable gap between\nperformance and generalization. To bridge this gap, we propose GeM-CoT, a\nGeneralizable CoT prompting mechanism in Mixed-task scenarios where the type of\ninput questions is unknown. GeM-CoT first categorizes the question type and\nsubsequently samples or constructs demonstrations from the corresponding data\npool in an automatic pattern. With this technical design, GeM-CoT\nsimultaneously enjoys superior generalization capabilities and remarkable\nperformances on 10 public reasoning tasks and 23 BBH tasks.", "published": "2023-10-10 15:10:03", "link": "http://arxiv.org/abs/2310.06692v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quality-Aware Translation Models: Efficient Generation and Quality\n  Estimation in a Single Model", "abstract": "Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy\nfor neural machine translation (NMT) models. The underlying assumption is that\nmodel probability correlates well with human judgment, with better translations\ngetting assigned a higher score by the model. However, research has shown that\nthis assumption does not always hold, and generation quality can be improved by\ndecoding to optimize a utility function backed by a metric or\nquality-estimation signal, as is done by Minimum Bayes Risk (MBR) or\nquality-aware decoding. The main disadvantage of these approaches is that they\nrequire an additional model to calculate the utility function during decoding,\nsignificantly increasing the computational cost. In this paper, we propose to\nmake the NMT models themselves quality-aware by training them to estimate the\nquality of their own output. Using this approach for MBR decoding we can\ndrastically reduce the size of the candidate list, resulting in a speed-up of\ntwo-orders of magnitude. When applying our method to MAP decoding we obtain\nquality gains similar or even superior to quality reranking approaches, but\nwith the efficiency of single pass decoding.", "published": "2023-10-10 15:33:51", "link": "http://arxiv.org/abs/2310.06707v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uni3D: Exploring Unified 3D Representation at Scale", "abstract": "Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.", "published": "2023-10-10 16:49:21", "link": "http://arxiv.org/abs/2310.06773v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Text Embeddings Reveal (Almost) As Much As Text", "abstract": "How much private information do text embeddings reveal about the original\ntext? We investigate the problem of embedding \\textit{inversion},\nreconstructing the full text represented in dense text embeddings. We frame the\nproblem as controlled generation: generating text that, when reembedded, is\nclose to a fixed point in latent space. We find that although a na\\\"ive model\nconditioned on the embedding performs poorly, a multi-step method that\niteratively corrects and re-embeds text is able to recover $92\\%$ of\n$32\\text{-token}$ text inputs exactly. We train our model to decode text\nembeddings from two state-of-the-art embedding models, and also show that our\nmodel can recover important personal information (full names) from a dataset of\nclinical notes. Our code is available on Github:\n\\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.", "published": "2023-10-10 17:39:03", "link": "http://arxiv.org/abs/2310.06816v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks", "abstract": "Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.", "published": "2023-10-10 17:57:00", "link": "http://arxiv.org/abs/2310.06827v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating and Evaluating Tests for K-12 Students with Language Model\n  Simulations: A Case Study on Sentence Reading Efficiency", "abstract": "Developing an educational test can be expensive and time-consuming, as each\nitem must be written by experts and then evaluated by collecting hundreds of\nstudent responses. Moreover, many tests require multiple distinct sets of\nquestions administered throughout the school year to closely monitor students'\nprogress, known as parallel tests. In this study, we focus on tests of silent\nsentence reading efficiency, used to assess students' reading ability over\ntime. To generate high-quality parallel tests, we propose to fine-tune large\nlanguage models (LLMs) to simulate how previous students would have responded\nto unseen items. With these simulated responses, we can estimate each item's\ndifficulty and ambiguity. We first use GPT-4 to generate new test items\nfollowing a list of expert-developed rules and then apply a fine-tuned LLM to\nfilter the items based on criteria from psychological measurements. We also\npropose an optimal-transport-inspired technique for generating parallel tests\nand show the generated tests closely correspond to the original test's\ndifficulty and reliability based on crowdworker responses. Our evaluation of a\ngenerated test with 234 students from grades 2 to 8 produces test scores highly\ncorrelated (r=0.93) to those of a standard test form written by human experts\nand evaluated across thousands of K-12 students.", "published": "2023-10-10 17:59:51", "link": "http://arxiv.org/abs/2310.06837v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression", "abstract": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational cost, performance reduction, and position\nbias. Research indicates that LLM performance hinges on the density and\nposition of key information in the input prompt. Inspired by these findings, we\npropose LongLLMLingua for prompt compression towards improving LLMs' perception\nof the key information to simultaneously address the three challenges. Our\nextensive evaluation across various long context scenarios demonstrates that\nLongLLMLingua not only enhances performance but also significantly reduces\ncosts and latency. For instance, in the NaturalQuestions benchmark,\nLongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in\nGPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost\nreduction in the LooGLE benchmark. Moreover, when compressing prompts of about\n10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency\nby 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.", "published": "2023-10-10 17:59:58", "link": "http://arxiv.org/abs/2310.06839v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE", "abstract": "The recent success of SimCSE has greatly advanced state-of-the-art sentence\nrepresentations. However, the original formulation of SimCSE does not fully\nexploit the potential of hard negative samples in contrastive learning. This\nstudy introduces an unsupervised contrastive learning framework that combines\nSimCSE with hard negative mining, aiming to enhance the quality of sentence\nembeddings. The proposed focal-InfoNCE function introduces self-paced\nmodulation terms in the contrastive objective, downweighting the loss\nassociated with easy negatives and encouraging the model focusing on hard\nnegatives. Experimentation on various STS benchmarks shows that our method\nimproves sentence embeddings in terms of Spearman's correlation and\nrepresentation alignment and uniformity.", "published": "2023-10-10 18:15:24", "link": "http://arxiv.org/abs/2310.06918v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Fine-tuning for Inference Acceleration of Large Language Models", "abstract": "We consider the problem of accurate sparse fine-tuning of large language\nmodels (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while\ninducing sparsity in their weights. On the accuracy side, we observe that\nstandard loss-based fine-tuning may fail to recover accuracy, especially at\nhigh sparsities. To address this, we perform a detailed study of\ndistillation-type losses, determining an L2-based distillation approach we term\nSquareHead which enables accurate recovery even at higher sparsities, across\nall model types. On the practical efficiency side, we show that sparse LLMs can\nbe executed with speedups by taking advantage of sparsity, for both CPU and GPU\nruntimes. While the standard approach is to leverage sparsity for computational\nreduction, we observe that in the case of memory-bound LLMs sparsity can also\nbe leveraged for reducing memory bandwidth. We exhibit end-to-end results\nshowing speedups due to sparsity, while recovering accuracy, on T5 (language\ntranslation), Whisper (speech translation), and open GPT-type (MPT for text\ngeneration). For MPT text generation, we show for the first time that sparse\nfine-tuning can reach 75% sparsity without accuracy drops, provide notable\nend-to-end speedups for both CPU and GPU inference, and highlight that sparsity\nis also compatible with quantization approaches. Models and software for\nreproducing our results are provided in Section 6.", "published": "2023-10-10 18:28:38", "link": "http://arxiv.org/abs/2310.06927v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jaynes Machine: The universal microstructure of deep neural networks", "abstract": "We present a novel theory of the microstructure of deep neural networks.\nUsing a theoretical framework called statistical teleodynamics, which is a\nconceptual synthesis of statistical thermodynamics and potential game theory,\nwe predict that all highly connected layers of deep neural networks have a\nuniversal microstructure of connection strengths that is distributed\nlognormally ($LN({\\mu}, {\\sigma})$). Furthermore, under ideal conditions, the\ntheory predicts that ${\\mu}$ and ${\\sigma}$ are the same for all layers in all\nnetworks. This is shown to be the result of an arbitrage equilibrium where all\nconnections compete and contribute the same effective utility towards the\nminimization of the overall loss function. These surprising predictions are\nshown to be supported by empirical data from six large-scale deep neural\nnetworks in real life. We also discuss how these results can be exploited to\nreduce the amount of data, time, and computational resources needed to train\nlarge deep neural networks.", "published": "2023-10-10 19:22:01", "link": "http://arxiv.org/abs/2310.06960v1", "categories": ["cond-mat.stat-mech", "cs.CL"], "primary_category": "cond-mat.stat-mech"}
{"title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of\n  Mind Prediction Error in Large Language Models", "abstract": "Recent research shows that Large Language Models (LLMs) exhibit a compelling\nlevel of proficiency in Theory of Mind (ToM) tasks. This ability to impute\nunobservable mental states to others is vital to human social cognition and may\nprove equally important in principal-agent relations between individual humans\nand Artificial Intelligences (AIs). In this paper, we explore how a mechanism\nstudied in developmental psychology known as Violation of Expectation (VoE) can\nbe implemented to reduce errors in LLM prediction about users by leveraging\nemergent ToM affordances. And we introduce a \\textit{metacognitive prompting}\nframework to apply VoE in the context of an AI tutor. By storing and retrieving\nfacts derived in cases where LLM expectation about the user was violated, we\nfind that LLMs are able to learn about users in ways that echo theories of\nhuman learning. Finally, we discuss latent hazards and augmentative\nopportunities associated with modeling user psychology and propose ways to\nmitigate risk along with possible directions for future inquiry.", "published": "2023-10-10 20:05:13", "link": "http://arxiv.org/abs/2310.06983v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models can Learn Rules", "abstract": "When prompted with a few examples and intermediate steps, large language\nmodels (LLMs) have demonstrated impressive performance in various reasoning\ntasks. However, prompting methods that rely on implicit knowledge in an LLM\noften generate incorrect answers when the implicit knowledge is wrong or\ninconsistent with the task. To tackle this problem, we present\nHypotheses-to-Theories (HtT), a framework that learns a rule library for\nreasoning with LLMs. HtT contains two stages, an induction stage and a\ndeduction stage. In the induction stage, an LLM is first asked to generate and\nverify rules over a set of training examples. Rules that appear and lead to\ncorrect answers sufficiently often are collected to form a rule library. In the\ndeduction stage, the LLM is then prompted to employ the learned rule library to\nperform reasoning to answer test questions. Experiments on relational\nreasoning, numerical reasoning and concept learning problems show that HtT\nimproves existing prompting methods, with an absolute gain of 10-30% in\naccuracy. The learned rules are also transferable to different models and to\ndifferent forms of the same problem.", "published": "2023-10-10 23:07:01", "link": "http://arxiv.org/abs/2310.07064v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained\n  Decoding", "abstract": "Instruction-tuned large language models (LLMs) excel at many tasks but often\nfail to use external tools due to complicated and unfamiliar syntax\nconstraints. While extensive fine-tuning and prompting can mitigate the issue,\nthese approaches are expensive and hard to generalize. Furthermore, because\nsyntax constraints are only learned implicitly during fine-tuning, models still\nmake frequent syntax errors. Motivated by the fact that these constraints can\nbe better satisfied explicitly with constrained decoding, we propose TOOLDEC, a\ndecoding algorithm using finite state machines to force LLMs to follow tool\nsyntax. Our experiments show that TOOLDEC eliminates all syntax errors,\nachieving significantly better performance on various base models and\nbenchmarks. More surprisingly, when applied to generalist out-of-the-box LLMs\nsuch as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from the\ninitial 0% to an impressive 52%, matching the performance of specialized\nfine-tuned models such as ToolLLM.", "published": "2023-10-10 23:37:53", "link": "http://arxiv.org/abs/2310.07075v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An experiment on an automated literature survey of data-driven speech\n  enhancement methods", "abstract": "The increasing number of scientific publications in acoustics, in general,\npresents difficulties in conducting traditional literature surveys. This work\nexplores the use of a generative pre-trained transformer (GPT) model to\nautomate a literature survey of 116 articles on data-driven speech enhancement\nmethods. The main objective is to evaluate the capabilities and limitations of\nthe model in providing accurate responses to specific queries about the papers\nselected from a reference human-based survey. While we see great potential to\nautomate literature surveys in acoustics, improvements are needed to address\ntechnical questions more clearly and accurately.", "published": "2023-10-10 02:07:24", "link": "http://arxiv.org/abs/2310.06260v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings", "abstract": "Discussion and debate among Large Language Models (LLMs) have gained\nconsiderable attention due to their potential to enhance the reasoning ability\nof LLMs. Although natural language is an obvious choice for communication due\nto LLM's language understanding capability, the token sampling step needed when\ngenerating natural language poses a potential risk of information loss, as it\nuses only one token to represent the model's belief across the entire\nvocabulary. In this paper, we introduce a communication regime named CIPHER\n(Communicative Inter-Model Protocol Through Embedding Representation) to\naddress this issue. Specifically, we remove the token sampling step from LLMs\nand let them communicate their beliefs across the vocabulary through the\nexpectation of the raw transformer output embeddings. Remarkably, by deviating\nfrom natural language, CIPHER offers an advantage of encoding a broader\nspectrum of information without any modification to the model weights,\noutperforming the state-of-the-art LLM debate methods using natural language by\n0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying\nsizes. This showcases the superiority and robustness of embeddings as an\nalternative \"language\" for communication among LLMs. We anticipate that CIPHER\nwill inspire further exploration for the design of interactions within LLM\nagent systems, offering a new direction that could significantly influence\nfuture developments in the field.", "published": "2023-10-10 03:06:38", "link": "http://arxiv.org/abs/2310.06272v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context\n  Demonstrations", "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks,\nyet their safety and the risk of generating harmful content remain pressing\nconcerns. In this paper, we delve into the potential of In-Context Learning\n(ICL) to modulate the alignment of LLMs. Specifically, we propose the\nIn-Context Attack (ICA) which employs harmful demonstrations to subvert LLMs,\nand the In-Context Defense (ICD) which bolsters model resilience through\nexamples that demonstrate refusal to produce harmful responses. We offer\ntheoretical insights to elucidate how a limited set of in-context\ndemonstrations can pivotally influence the safety alignment of LLMs. Through\nextensive experiments, we demonstrate the efficacy of ICA and ICD in\nrespectively elevating and mitigating the success rates of jailbreaking\nprompts. Our findings illuminate the profound influence of ICL on LLM behavior,\nopening new avenues for improving the safety of LLMs.", "published": "2023-10-10 07:50:29", "link": "http://arxiv.org/abs/2310.06387v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection", "abstract": "The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.", "published": "2023-10-10 07:53:36", "link": "http://arxiv.org/abs/2310.06390v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improved prompting and process for writing user personas with LLMs,\n  using qualitative interviews: Capturing behaviour and personality traits of\n  users", "abstract": "This draft paper presents a workflow for creating User Personas with Large\nLanguage Models, using the results of a Thematic Analysis of qualitative\ninterviews. The proposed workflow uses improved prompting and a larger pool of\nThemes, compared to previous work conducted by the author for the same task.\nThis is possible due to the capabilities of a recently released LLM which\nallows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to\nthe possibility to offer a refined prompting for the creation of Personas. The\npaper offers details of performing Phase 2 and 3 of Thematic Analysis, and then\ndiscusses the improved workflow for creating Personas. The paper also offers\nsome reflections on the relationship between the proposed process and existing\napproaches to Personas such as the data-driven and qualitative Personas.\nMoreover, the paper offers reflections on the capacity of LLMs to capture user\nbehaviours and personality traits, from the underlying dataset of qualitative\ninterviews used for the analysis.", "published": "2023-10-10 07:54:24", "link": "http://arxiv.org/abs/2310.06391v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Hexa: Self-Improving for Knowledge-Grounded Dialogue System", "abstract": "A common practice in knowledge-grounded dialogue generation is to explicitly\nutilize intermediate steps (e.g., web-search, memory retrieval) with modular\napproaches. However, data for such steps are often inaccessible compared to\nthose of dialogue responses as they are unobservable in an ordinary dialogue.\nTo fill in the absence of these data, we develop a self-improving method to\nimprove the generative performances of intermediate steps without the ground\ntruth data. In particular, we propose a novel bootstrapping scheme with a\nguided prompt and a modified loss function to enhance the diversity of\nappropriate self-generated responses. Through experiments on various benchmark\ndatasets, we empirically demonstrate that our method successfully leverages a\nself-improving mechanism in generating intermediate and final responses and\nimproves the performances on the task of knowledge-grounded dialogue\ngeneration.", "published": "2023-10-10 08:15:24", "link": "http://arxiv.org/abs/2310.06404v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retromorphic Testing: A New Approach to the Test Oracle Problem", "abstract": "A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.", "published": "2023-10-10 09:03:01", "link": "http://arxiv.org/abs/2310.06433v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV", "D.3.0; I.2.7; I.4.0"], "primary_category": "cs.SE"}
{"title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity", "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has\nbeen significant work developing these methods, our understanding of the\nbenefits and downsides of each stage in RLHF is still limited. To fill this\ngap, we present an extensive analysis of how each stage of the process (i.e.\nsupervised fine-tuning (SFT), reward modelling, and RLHF) affects two key\nproperties: out-of-distribution (OOD) generalisation and output diversity. OOD\ngeneralisation is crucial given the wide range of real-world scenarios in which\nthese models are being used, while output diversity refers to the model's\nability to generate varied outputs and is important for a variety of use cases.\nWe perform our analysis across two base models on both summarisation and\ninstruction following tasks, the latter being highly relevant for current LLM\nuse cases. We find that RLHF generalises better than SFT to new inputs,\nparticularly as the distribution shift between train and test becomes larger.\nHowever, RLHF significantly reduces output diversity compared to SFT across a\nvariety of measures, implying a tradeoff in current LLM fine-tuning methods\nbetween generalisation and diversity. Our results provide guidance on which\nfine-tuning method should be used depending on the application, and show that\nmore research is needed to improve the tradeoff between generalisation and\ndiversity.", "published": "2023-10-10 09:25:44", "link": "http://arxiv.org/abs/2310.06452v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network", "abstract": "Spiking Neural Networks (SNNs) have emerged as a promising alternative to\nconventional Artificial Neural Networks (ANNs), demonstrating comparable\nperformance in both visual and linguistic tasks while offering the advantage of\nimproved energy efficiency. Despite these advancements, the integration of\nlinguistic and visual features into a unified representation through spike\ntrains poses a significant challenge, and the application of SNNs to multimodal\nscenarios remains largely unexplored. This paper presents SpikeCLIP, a novel\nframework designed to bridge the modality gap in spike-based computation. Our\napproach employs a two-step recipe: an ``alignment pre-training'' to align\nfeatures across modalities, followed by a ``dual-loss fine-tuning'' to refine\nthe model's performance. Extensive experiments reveal that SNNs achieve results\non par with ANNs while substantially reducing energy consumption across various\ndatasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP\nmaintains robust image classification capabilities, even when dealing with\nclasses that fall outside predefined categories. This study marks a significant\nadvancement in the development of energy-efficient and biologically plausible\nmultimodal learning systems.", "published": "2023-10-10 09:57:17", "link": "http://arxiv.org/abs/2310.06488v3", "categories": ["cs.NE", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Revisit Input Perturbation Problems for LLMs: A Unified Robustness\n  Evaluation Framework for Noisy Slot Filling Task", "abstract": "With the increasing capabilities of large language models (LLMs), these\nhigh-performance models have achieved state-of-the-art results on a wide range\nof natural language processing (NLP) tasks. However, the models' performance on\ncommonly-used benchmark datasets often fails to accurately reflect their\nreliability and robustness when applied to real-world noisy data. To address\nthese challenges, we propose a unified robustness evaluation framework based on\nthe slot-filling task to systematically evaluate the dialogue understanding\ncapability of LLMs in diverse input perturbation scenarios. Specifically, we\nconstruct a input perturbation evaluation dataset, Noise-LLM, which contains\nfive types of single perturbation and four types of mixed perturbation data.\nFurthermore, we utilize a multi-level data augmentation method (character,\nword, and sentence levels) to construct a candidate data pool, and carefully\ndesign two ways of automatic task demonstration construction strategies\n(instance-level and entity-level) with various prompt templates. Our aim is to\nassess how well various robustness methods of LLMs perform in real-world noisy\nscenarios. The experiments have demonstrated that the current open-source LLMs\ngenerally achieve limited perturbation robustness performance. Based on these\nexperimental observations, we make some forward-looking suggestions to fuel the\nresearch in this direction.", "published": "2023-10-10 10:22:05", "link": "http://arxiv.org/abs/2310.06504v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward Semantic Publishing in Non-Invasive Brain Stimulation: A\n  Comprehensive Analysis of rTMS Studies", "abstract": "Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation\ntechniques that can influence brain excitability. These techniques have the\npotential to treat conditions like depression, anxiety, and chronic pain, and\nto provide insights into brain function. However, a lack of standardized\nreporting practices limits its reproducibility and full clinical potential.\nThis paper aims to foster interinterdisciplinarity toward adopting Computer\nScience Semantic reporting methods for the standardized documentation of\nNeuroscience NIBS studies making them explicitly Findable, Accessible,\nInteroperable, and Reusable (FAIR).\n  In a large-scale systematic review of 600 repetitive transcranial magnetic\nstimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that\nallow for structured descriptions and comparisons of the studies. This paper\nshowcases the semantic publishing of NIBS in the ecosphere of\nknowledge-graph-based next-generation scholarly digital libraries.\nSpecifically, the FAIR Semantic Web resource(s)-based publishing paradigm is\nimplemented for the 600 reviewed rTMS studies in the Open Research Knowledge\nGraph.", "published": "2023-10-10 11:00:23", "link": "http://arxiv.org/abs/2310.06517v1", "categories": ["cs.DL", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.DL"}
{"title": "A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo:\n  A Romanian Clickbait Corpus of News Articles", "abstract": "To increase revenue, news websites often resort to using deceptive news\ntitles, luring users into clicking on the title and reading the full news.\nClickbait detection is the task that aims to automatically detect this form of\nfalse advertisement and avoid wasting the precious time of online users.\nDespite the importance of the task, to the best of our knowledge, there is no\npublicly available clickbait corpus for the Romanian language. To this end, we\nintroduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news\nsamples which are manually annotated with clickbait and non-clickbait labels.\nFurthermore, we conduct experiments with four machine learning methods, ranging\nfrom handcrafted models to recurrent and transformer-based neural networks, to\nestablish a line-up of competitive baselines. We also carry out experiments\nwith a weighted voting ensemble. Among the considered baselines, we propose a\nnovel BERT-based contrastive learning model that learns to encode news titles\nand contents into a deep metric space such that titles and contents of\nnon-clickbait news have high cosine similarity, while titles and contents of\nclickbait news have low cosine similarity. Our data set and code to reproduce\nthe baselines are publicly available for download at\nhttps://github.com/dariabroscoteanu/RoCliCo.", "published": "2023-10-10 11:38:16", "link": "http://arxiv.org/abs/2310.06540v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual\n  Voice Conversion", "abstract": "This paper proposes a simple and robust zero-shot voice conversion system\nwith a cycle structure and mel-spectrogram pre-processing. Previous works\nsuffer from information loss and poor synthesis quality due to their reliance\non a carefully designed bottleneck structure. Moreover, models relying solely\non self-reconstruction loss struggled with reproducing different speakers'\nvoices. To address these issues, we suggested a cycle-consistency loss that\nconsiders conversion back and forth between target and source speakers.\nAdditionally, stacked random-shuffled mel-spectrograms and a label smoothing\nmethod are utilized during speaker encoder training to extract a\ntime-independent global speaker representation from speech, which is the key to\na zero-shot conversion. Our model outperforms existing state-of-the-art results\nin both subjective and objective evaluations. Furthermore, it facilitates\ncross-lingual voice conversions and enhances the quality of synthesized speech.", "published": "2023-10-10 11:50:16", "link": "http://arxiv.org/abs/2310.06546v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "It's About Time: Temporal References in Emergent Communication", "abstract": "Emergent communication studies the development of language between autonomous\nagents, aiming to improve understanding of natural language evolution and\nincrease communication efficiency. While temporal aspects of language have been\nconsidered in computational linguistics, there has been no research on temporal\nreferences in emergent communication. This paper addresses this gap, by\nexploring how agents communicate about temporal relationships. We analyse three\npotential influences for the emergence of temporal references: environmental,\nexternal, and architectural changes. Our experiments demonstrate that altering\nthe loss function is insufficient for temporal references to emerge; rather,\narchitectural changes are necessary. However, a minimal change in agent\narchitecture, using a different batching method, allows the emergence of\ntemporal references. This modified design is compared with the standard\narchitecture in a temporal referential games environment, which emphasises\ntemporal relationships. The analysis indicates that over 95\\% of the agents\nwith the modified batching method develop temporal references, without changes\nto their loss function. We consider temporal referencing necessary for future\nimprovements to the agents' communication efficiency, yielding a closer to\noptimal coding as compared to purely compositional languages. Our readily\ntransferable architectural insights provide the basis for their incorporation\ninto other emergent communication settings.", "published": "2023-10-10 12:10:40", "link": "http://arxiv.org/abs/2310.06555v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models", "abstract": "Counterfactual reasoning, a fundamental aspect of human cognition, involves\ncontemplating alternatives to established facts or past events, significantly\nenhancing our abilities in planning and decision-making. In light of the\nadvancements in current multi-modal large language models, we explore their\neffectiveness in counterfactual reasoning. To facilitate this investigation, we\nintroduce a novel dataset, C-VQA, specifically designed to test the\ncounterfactual reasoning capabilities of modern multi-modal large language\nmodels. This dataset is constructed by infusing original questions with\ncounterfactual presuppositions, spanning various types such as numerical and\nboolean queries. It encompasses a mix of real and synthetic data, representing\na wide range of difficulty levels. Our thorough evaluations of contemporary\nvision-language models using this dataset have revealed substantial performance\ndrops, with some models showing up to a 40% decrease, highlighting a\nsignificant gap between current models and human-like vision reasoning\ncapabilities. We hope our dataset will serve as a vital benchmark for\nevaluating the counterfactual reasoning capabilities of models. Code and\ndataset are publicly available at https://bzhao.me/C-VQA/.", "published": "2023-10-10 13:45:59", "link": "http://arxiv.org/abs/2310.06627v4", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unlock the Potential of Counterfactually-Augmented Data in\n  Out-Of-Distribution Generalization", "abstract": "Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip\nthe corresponding labels -- has the potential to improve the\nOut-Of-Distribution (OOD) generalization capability of language models, as CAD\ninduces language models to exploit domain-independent causal features and\nexclude spurious correlations. However, the empirical results of CAD's OOD\ngeneralization are not as efficient as anticipated. In this study, we attribute\nthe inefficiency to the myopia phenomenon caused by CAD: language models only\nfocus on causal features that are edited in the augmentation operation and\nexclude other non-edited causal features. Therefore, the potential of CAD is\nnot fully exploited. To address this issue, we analyze the myopia phenomenon in\nfeature space from the perspective of Fisher's Linear Discriminant, then we\nintroduce two additional constraints based on CAD's structural properties\n(dataset-level and sentence-level) to help language models extract more\ncomplete causal features in CAD, thereby mitigating the myopia phenomenon and\nimproving OOD generalization capability. We evaluate our method on two tasks:\nSentiment Analysis and Natural Language Inference, and the experimental results\ndemonstrate that our method could unlock the potential of CAD and improve the\nOOD generalization performance of language models by 1.0% to 5.9%.", "published": "2023-10-10 14:41:38", "link": "http://arxiv.org/abs/2310.06666v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured\n  Pruning", "abstract": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\nthe concurrent TinyLlama models, on a wide range of downstream and instruction\ntuning evaluations, while requiring only 3% of compute compared to training\nsuch models from scratch. This work provides compelling evidence that\nleveraging existing LLMs with structured pruning is a far more cost-effective\napproach for building competitive small-scale LLMs", "published": "2023-10-10 15:13:30", "link": "http://arxiv.org/abs/2310.06694v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Temporally Aligning Long Audio Interviews with Questions: A Case Study\n  in Multimodal Data Integration", "abstract": "The problem of audio-to-text alignment has seen significant amount of\nresearch using complete supervision during training. However, this is typically\nnot in the context of long audio recordings wherein the text being queried does\nnot appear verbatim within the audio file. This work is a collaboration with a\nnon-governmental organization called CARE India that collects long audio health\nsurveys from young mothers residing in rural parts of Bihar, India. Given a\nquestion drawn from a questionnaire that is used to guide these surveys, we aim\nto locate where the question is asked within a long audio recording. This is of\ngreat value to African and Asian organizations that would otherwise have to\npainstakingly go through long and noisy audio recordings to locate questions\n(and answers) of interest. Our proposed framework, INDENT, uses a\ncross-attention-based model and prior information on the temporal ordering of\nsentences to learn speech embeddings that capture the semantics of the\nunderlying spoken text. These learnt embeddings are used to retrieve the\ncorresponding audio segment based on text queries at inference time. We\nempirically demonstrate the significant effectiveness (improvement in R-avg of\nabout 3%) of our model over those obtained using text-based heuristics. We also\nshow how noisy ASR, generated using state-of-the-art ASR models for Indian\nlanguages, yields better results when used in place of speech. INDENT, trained\nonly on Hindi data is able to cater to all languages supported by the\n(semantically) shared text space. We illustrate this empirically on 11 Indic\nlanguages.", "published": "2023-10-10 15:25:33", "link": "http://arxiv.org/abs/2310.06702v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Memorization in Fine-tuned Language Models", "abstract": "Large language models (LLMs) have shown great capabilities in various tasks\nbut also exhibited memorization of training data, raising tremendous privacy\nand copyright concerns. While prior works have studied memorization during\npre-training, the exploration of memorization during fine-tuning is rather\nlimited. Compared to pre-training, fine-tuning typically involves more\nsensitive data and diverse objectives, thus may bring distinct privacy risks\nand unique memorization behaviors. In this work, we conduct the first\ncomprehensive analysis to explore language models' (LMs) memorization during\nfine-tuning across tasks. Our studies with open-sourced and our own fine-tuned\nLMs across various tasks indicate that memorization presents a strong disparity\namong different fine-tuning tasks. We provide an intuitive explanation of this\ntask disparity via sparse coding theory and unveil a strong correlation between\nmemorization and attention score distribution.", "published": "2023-10-10 15:41:26", "link": "http://arxiv.org/abs/2310.06714v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "abstract": "Language models have outpaced our ability to evaluate them effectively, but\nfor their future development it is essential to study the frontier of their\ncapabilities. We find real-world software engineering to be a rich,\nsustainable, and challenging testbed for evaluating the next generation of\nlanguage models. To this end, we introduce SWE-bench, an evaluation framework\nconsisting of $2,294$ software engineering problems drawn from real GitHub\nissues and corresponding pull requests across $12$ popular Python repositories.\nGiven a codebase along with a description of an issue to be resolved, a\nlanguage model is tasked with editing the codebase to address the issue.\nResolving issues in SWE-bench frequently requires understanding and\ncoordinating changes across multiple functions, classes, and even files\nsimultaneously, calling for models to interact with execution environments,\nprocess extremely long contexts and perform complex reasoning that goes far\nbeyond traditional code generation tasks. Our evaluations show that both\nstate-of-the-art proprietary models and our fine-tuned model SWE-Llama can\nresolve only the simplest issues. The best-performing model, Claude 2, is able\nto solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps\ntowards LMs that are more practical, intelligent, and autonomous.", "published": "2023-10-10 16:47:29", "link": "http://arxiv.org/abs/2310.06770v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text", "abstract": "There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.", "published": "2023-10-10 16:57:28", "link": "http://arxiv.org/abs/2310.06786v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Advancing Transformer's Capabilities in Commonsense Reasoning", "abstract": "Recent advances in general purpose pre-trained language models have shown\ngreat potential in commonsense reasoning. However, current works still perform\npoorly on standard commonsense reasoning benchmarks including the Com2Sense\nDataset. We argue that this is due to a disconnect with current cutting-edge\nmachine learning methods. In this work, we aim to bridge the gap by introducing\ncurrent ML-based methods to improve general purpose pre-trained language models\nin the task of commonsense reasoning. Specifically, we experiment with and\nsystematically evaluate methods including knowledge transfer, model ensemble,\nand introducing an additional pairwise contrastive objective. Our best model\noutperforms the strongest previous works by ~15\\% absolute gains in Pairwise\nAccuracy and ~8.7\\% absolute gains in Standard Accuracy.", "published": "2023-10-10 17:21:03", "link": "http://arxiv.org/abs/2310.06803v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mistral 7B", "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.", "published": "2023-10-10 17:54:58", "link": "http://arxiv.org/abs/2310.06825v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Transformer-based Neural Text Representation\n  Techniques on Bug Triaging", "abstract": "Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.", "published": "2023-10-10 18:09:32", "link": "http://arxiv.org/abs/2310.06913v1", "categories": ["cs.SE", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation", "abstract": "The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.", "published": "2023-10-10 20:15:54", "link": "http://arxiv.org/abs/2310.06987v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs", "abstract": "Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.", "published": "2023-10-10 20:49:43", "link": "http://arxiv.org/abs/2310.07008v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?", "abstract": "Large Language Models (LLMs), through their contextualized representations,\nhave been empirically proven to encapsulate syntactic, semantic, word sense,\nand common-sense knowledge. However, there has been limited exploration of\ntheir physical reasoning abilities, specifically concerning the crucial\nattributes for comprehending everyday objects. To address this gap, we\nintroduce NEWTON, a repository and benchmark for evaluating the physics\nreasoning skills of LLMs. Further, to enable domain-specific adaptation of this\nbenchmark, we present a pipeline to enable researchers to generate a variant of\nthis benchmark that has been customized to the objects and attributes relevant\nfor their application. The NEWTON repository comprises a collection of 2800\nobject-attribute pairs, providing the foundation for generating infinite-scale\nassessment templates. The NEWTON benchmark consists of 160K QA questions,\ncurated using the NEWTON repository to investigate the physical reasoning\ncapabilities of several mainstream language models across foundational,\nexplicit, and implicit reasoning tasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs for physical reasoning. We find\nthat LLMs like GPT-4 demonstrate strong reasoning capabilities in\nscenario-based tasks but exhibit less consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing language models, paving the way for\ntheir integration into physically grounded settings, such as robotic\nmanipulation. Project site: https://newtonreasoning.github.io", "published": "2023-10-10 21:08:51", "link": "http://arxiv.org/abs/2310.07018v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Automatic Macro Mining from Interaction Traces at Scale", "abstract": "Macros are building block tasks of our everyday smartphone activity (e.g.,\n\"login\", or \"booking a flight\"). Effectively extracting macros is important for\nunderstanding mobile interaction and enabling task automation. These macros are\nhowever difficult to extract at scale as they can be comprised of multiple\nsteps yet hidden within programmatic components of mobile apps. In this paper,\nwe introduce a novel approach based on Large Language Models (LLMs) to\nautomatically extract semantically meaningful macros from both random and\nuser-curated mobile interaction traces. The macros produced by our approach are\nautomatically tagged with natural language descriptions and are fully\nexecutable. We conduct multiple studies to validate the quality of extracted\nmacros, including user evaluation, comparative analysis against human-curated\ntasks, and automatic execution of these macros. These experiments and analyses\nshow the effectiveness of our approach and the usefulness of extracted macros\nin various downstream applications.", "published": "2023-10-10 21:23:47", "link": "http://arxiv.org/abs/2310.07023v2", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis\n  Prediction", "abstract": "Multi-label text classification (MLTC) tasks in the medical domain often face\nthe long-tail label distribution problem. Prior works have explored\nhierarchical label structures to find relevant information for few-shot\nclasses, but mostly neglected to incorporate external knowledge from medical\nguidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification\nfor diagnosis prediction with two innovations: (1) automated construction of\nheterogeneous knowledge graphs from external sources to capture semantic\nrelations among diverse medical entities, (2) incorporating the heterogeneous\nknowledge graphs in few-shot classification using a label-wise attention\nmechanism. We construct DKEC using three online medical knowledge sources and\nevaluate it on a real-world Emergency Medical Services (EMS) dataset and a\npublic electronic health record (EHR) dataset. Results show that DKEC\noutperforms the state-of-the-art label-wise attention networks and transformer\nmodels of different sizes, particularly for the few-shot classes. More\nimportantly, it helps the smaller language models achieve comparable\nperformance to large language models.", "published": "2023-10-10 22:53:15", "link": "http://arxiv.org/abs/2310.07059v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework\n  for Speech Recognition", "abstract": "We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.", "published": "2023-10-10 09:04:33", "link": "http://arxiv.org/abs/2310.06434v2", "categories": ["cs.CL", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for\n  Unbiased Question-Answering", "abstract": "In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.", "published": "2023-10-10 01:22:41", "link": "http://arxiv.org/abs/2310.06238v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Discriminative Speech Recognition Rescoring with Pre-trained Language\n  Models", "abstract": "Second pass rescoring is a critical component of competitive automatic speech\nrecognition (ASR) systems. Large language models have demonstrated their\nability in using pre-trained information for better rescoring of ASR\nhypothesis. Discriminative training, directly optimizing the minimum\nword-error-rate (MWER) criterion typically improves rescoring. In this study,\nwe propose and explore several discriminative fine-tuning schemes for\npre-trained LMs. We propose two architectures based on different pooling\nstrategies of output embeddings and compare with probability based MWER. We\nconduct detailed comparisons between pre-trained causal and bidirectional LMs\nin discriminative settings. Experiments on LibriSpeech demonstrate that all\nMWER training schemes are beneficial, giving additional gains upto 8.5\\% WER.\nProposed pooling variants achieve lower latency while retaining most\nimprovements. Finally, our study concludes that bidirectionality is better\nutilized with discriminative training.", "published": "2023-10-10 01:52:30", "link": "http://arxiv.org/abs/2310.06248v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Privacy-oriented manipulation of speaker representations", "abstract": "Speaker embeddings are ubiquitous, with applications ranging from speaker\nrecognition and diarization to speech synthesis and voice anonymisation. The\namount of information held by these embeddings lends them versatility, but also\nraises privacy concerns. Speaker embeddings have been shown to contain\ninformation on age, sex, health and more, which speakers may want to keep\nprivate, especially when this information is not required for the target task.\nIn this work, we propose a method for removing and manipulating private\nattributes from speaker embeddings that leverages a Vector-Quantized\nVariational Autoencoder architecture, combined with an adversarial classifier\nand a novel mutual information loss. We validate our model on two attributes,\nsex and age, and perform experiments with ignorant and fully-informed\nattackers, and with in-domain and out-of-domain data.", "published": "2023-10-10 14:18:57", "link": "http://arxiv.org/abs/2310.06652v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Modeling of Speech-dependent Own Voice Transfer Characteristics for\n  Hearables with In-ear Microphones", "abstract": "Many hearables contain an in-ear microphone, which may be used to capture the\nown voice of its user. However, due to the hearable occluding the ear canal,\nthe in-ear microphone mostly records body-conducted speech, typically suffering\nfrom band-limitation effects and amplification at low frequencies. Since the\nocclusion effect is determined by the ratio between the air-conducted and\nbody-conducted components of own voice, the own voice transfer characteristics\nbetween the outer face of the hearable and the in-ear microphone depend on the\nspeech content and the individual talker. In this paper, we propose a\nspeech-dependent model of the own voice transfer characteristics based on\nphoneme recognition, assuming a linear time-invariant relative transfer\nfunction for each phoneme. We consider both individual models as well as models\naveraged over several talkers. Experimental results based on recordings with a\nprototype hearable show that the proposed speech-dependent model enables to\nsimulate in-ear signals more accurately than a speech-independent model in\nterms of technical measures, especially under utterance mismatch and talker\nmismatch. Additionally, simulation results show that talker-averaged models\ngeneralize better to different talkers than individual models.", "published": "2023-10-10 12:09:56", "link": "http://arxiv.org/abs/2310.06554v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-modal Cognitive Consensus guided Audio-Visual Segmentation", "abstract": "Audio-Visual Segmentation (AVS) aims to extract the sounding object from a\nvideo frame, which is represented by a pixel-wise segmentation mask for\napplication scenarios such as multi-modal video editing, augmented reality, and\nintelligent robot systems. The pioneering work conducts this task through dense\nfeature-level audio-visual interaction, which ignores the dimension gap between\ndifferent modalities. More specifically, the audio clip could only provide a\nGlobal semantic label in each sequence, but the video frame covers multiple\nsemantic objects across different Local regions, which leads to mislocalization\nof the representationally similar but semantically different object. In this\npaper, we propose a Cross-modal Cognitive Consensus guided Network (C3N) to\nalign the audio-visual semantics from the global dimension and progressively\ninject them into the local regions via an attention mechanism. Firstly, a\nCross-modal Cognitive Consensus Inference Module (C3IM) is developed to extract\na unified-modal label by integrating audio/visual classification confidence and\nsimilarities of modality-agnostic label embeddings. Then, we feed the\nunified-modal label back to the visual backbone as the explicit semantic-level\nguidance via a Cognitive Consensus guided Attention Module (CCAM), which\nhighlights the local features corresponding to the interested object. Extensive\nexperiments on the Single Sound Source Segmentation (S4) setting and Multiple\nSound Source Segmentation (MS3) setting of the AVSBench dataset demonstrate the\neffectiveness of the proposed method, which achieves state-of-the-art\nperformance. Code is available at https://github.com/ZhaofengSHI/AVS-C3N.", "published": "2023-10-10 02:04:38", "link": "http://arxiv.org/abs/2310.06259v5", "categories": ["eess.IV", "cs.SD", "eess.AS", "68U10", "I.4.6"], "primary_category": "eess.IV"}
{"title": "Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup\n  Anomalous Sound Detection", "abstract": "Unsupervised anomalous sound detection (ASD) aims to identify anomalous\nsounds by learning the features of normal operational sounds and sensing their\ndeviations. Recent approaches have focused on the self-supervised task\nutilizing the classification of normal data, and advanced models have shown\nthat securing representation space for anomalous data is important through\nrepresentation learning yielding compact intra-class and well-separated\nintra-class distributions. However, we show that conventional approaches often\nfail to ensure sufficient intra-class compactness and exhibit angular disparity\nbetween samples and their corresponding centers. In this paper, we propose a\ntraining technique aimed at ensuring intra-class compactness and increasing the\nangle gap between normal and abnormal samples. Furthermore, we present an\narchitecture that extracts features for important temporal regions, enabling\nthe model to learn which time frames should be emphasized or suppressed.\nExperimental results demonstrate that the proposed method achieves the best\nperformance giving 0.90%, 0.83%, and 2.16% improvement in terms of AUC, pAUC,\nand mAUC, respectively, compared to the state-of-the-art method on DCASE 2020\nChallenge Task2 dataset.", "published": "2023-10-10 07:04:36", "link": "http://arxiv.org/abs/2310.06364v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Proceedings of The first international workshop on eXplainable AI for\n  the Arts (XAIxArts)", "abstract": "This first international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\n  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C\n2023).", "published": "2023-10-10 08:53:54", "link": "http://arxiv.org/abs/2310.06428v1", "categories": ["cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Topological data analysis of human vowels: Persistent homologies across\n  representation spaces", "abstract": "Topological Data Analysis (TDA) has been successfully used for various tasks\nin signal/image processing, from visualization to supervised/unsupervised\nclassification. Often, topological characteristics are obtained from persistent\nhomology theory. The standard TDA pipeline starts from the raw signal data or a\nrepresentation of it. Then, it consists in building a multiscale topological\nstructure on the top of the data using a pre-specified filtration, and finally\nto compute the topological signature to be further exploited. The commonly used\ntopological signature is a persistent diagram (or transformations of it).\nCurrent research discusses the consequences of the many ways to exploit\ntopological signatures, much less often the choice of the filtration, but to\nthe best of our knowledge, the choice of the representation of a signal has not\nbeen the subject of any study yet. This paper attempts to provide some answers\non the latter problem. To this end, we collected real audio data and built a\ncomparative study to assess the quality of the discriminant information of the\ntopological signatures extracted from three different representation spaces.\nEach audio signal is represented as i) an embedding of observed data in a\nhigher dimensional space using Taken's representation, ii) a spectrogram viewed\nas a surface in a 3D ambient space, iii) the set of spectrogram's zeroes. From\nvowel audio recordings, we use topological signature for three prediction\nproblems: speaker gender, vowel type, and individual. We show that\ntopologically-augmented random forest improves the Out-of-Bag Error (OOB) over\nsolely based Mel-Frequency Cepstral Coefficients (MFCC) for the last two\nproblems. Our results also suggest that the topological information extracted\nfrom different signal representations is complementary, and that spectrogram's\nzeros offers the best improvement for gender prediction.", "published": "2023-10-10 10:37:54", "link": "http://arxiv.org/abs/2310.06508v1", "categories": ["cs.SD", "eess.AS", "stat.AP", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Prosody Analysis of Audiobooks", "abstract": "Recent advances in text-to-speech have made it possible to generate\nnatural-sounding audio from text. However, audiobook narrations involve\ndramatic vocalizations and intonations by the reader, with greater reliance on\nemotions, dialogues, and descriptions in the narrative. Using our dataset of 93\naligned book-audiobook pairs, we present improved models for prosody prediction\nproperties (pitch, volume, and rate of speech) from narrative text using\nlanguage modeling. Our predicted prosody attributes correlate much better with\nhuman audiobook readings than results from a state-of-the-art commercial TTS\nsystem: our predicted pitch shows a higher correlation with human reading for\n22 out of the 24 books, while our predicted volume attribute proves more\nsimilar to human reading for 23 out of the 24 books. Finally, we present a\nhuman evaluation study to quantify the extent that people prefer\nprosody-enhanced audiobook readings over commercial text-to-speech systems.", "published": "2023-10-10 18:33:47", "link": "http://arxiv.org/abs/2310.06930v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Model Fusion for End-to-end Speech Recognition", "abstract": "Recent advances in deep learning and automatic speech recognition (ASR) have\nenabled the end-to-end (E2E) ASR system and boosted the accuracy to a new\nlevel. The E2E systems implicitly model all conventional ASR components, such\nas the acoustic model (AM) and the language model (LM), in a single network\ntrained on audio-text pairs. Despite this simpler system architecture, fusing a\nseparate LM, trained exclusively on text corpora, into the E2E system has\nproven to be beneficial. However, the application of LM fusion presents certain\ndrawbacks, such as its inability to address the domain mismatch issue inherent\nto the internal AM. Drawing inspiration from the concept of LM fusion, we\npropose the integration of an external AM into the E2E system to better address\nthe domain mismatch. By implementing this novel approach, we have achieved a\nsignificant reduction in the word error rate, with an impressive drop of up to\n14.3% across varied test sets. We also discovered that this AM fusion approach\nis particularly beneficial in enhancing named entity recognition.", "published": "2023-10-10 23:00:17", "link": "http://arxiv.org/abs/2310.07062v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic\n  System Identification with Application to Audio Processing", "abstract": "Improving the interpretability of deep neural networks has recently gained\nincreased attention, especially when the power of deep learning is leveraged to\nsolve problems in physics. Interpretability helps us understand a model's\nability to generalize and reveal its limitations. In this paper, we introduce a\ncausal interpretable deep structure for modeling dynamic systems. Our proposed\nmodel makes use of the harmonic analysis by modeling the system in a\ntime-frequency domain while maintaining high temporal and spectral resolution.\nMoreover, the model is built in an order recursive manner which allows for\nfast, robust, and exact second order optimization without the need for an\nexplicit Hessian calculation. To circumvent the resulting high dimensionality\nof the building blocks of our system, a neural network is designed to identify\nthe frequency interdependencies. The proposed model is illustrated and\nvalidated on nonlinear system identification problems as required for audio\nsignal processing tasks. Crowd-sourced experimentation contrasting the\nperformance of the proposed approach to other state-of-the-art solutions on an\nacoustic echo cancellation scenario confirms the effectiveness of our method\nfor real-life applications.", "published": "2023-10-10 21:32:15", "link": "http://arxiv.org/abs/2310.07032v1", "categories": ["cs.SD", "cs.LG", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.SD"}
