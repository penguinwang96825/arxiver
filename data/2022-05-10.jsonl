{"title": "ParaCotta: Synthetic Multilingual Paraphrase Corpora from the Most\n  Diverse Translation Sample Pair", "abstract": "We release our synthetic parallel paraphrase corpus across 17 languages:\nArabic, Catalan, Czech, German, English, Spanish, Estonian, French, Hindi,\nIndonesian, Italian, Dutch, Romanian, Russian, Swedish, Vietnamese, and\nChinese. Our method relies only on monolingual data and a neural machine\ntranslation system to generate paraphrases, hence simple to apply. We generate\nmultiple translation samples using beam search and choose the most lexically\ndiverse pair according to their sentence BLEU. We compare our generated corpus\nwith the \\texttt{ParaBank2}. According to our evaluation, our synthetic\nparaphrase pairs are semantically similar and lexically diverse.", "published": "2022-05-10 03:40:14", "link": "http://arxiv.org/abs/2205.04651v1", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "AdMix: A Mixed Sample Data Augmentation Method for Neural Machine\n  Translation", "abstract": "In Neural Machine Translation (NMT), data augmentation methods such as\nback-translation have proven their effectiveness in improving translation\nperformance. In this paper, we propose a novel data augmentation approach for\nNMT, which is independent of any additional training data. Our approach, AdMix,\nconsists of two parts: 1) introduce faint discrete noise (word replacement,\nword dropping, word swapping) into the original sentence pairs to form\naugmented samples; 2) generate new synthetic training data by softly mixing the\naugmented samples with their original samples in training corpus. Experiments\non three translation datasets of different scales show that AdMix achieves\nsignifi cant improvements (1.0 to 2.7 BLEU points) over strong Transformer\nbaseline. When combined with other data augmentation techniques (e.g.,\nback-translation), our approach can obtain further improvements.", "published": "2022-05-10 05:43:27", "link": "http://arxiv.org/abs/2205.04686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Importance of Context in Very Low Resource Language Modeling", "abstract": "This paper investigates very low resource language model pretraining, when\nless than 100 thousand sentences are available. We find that, in very low\nresource scenarios, statistical n-gram language models outperform\nstate-of-the-art neural models. Our experiments show that this is mainly due to\nthe focus of the former on a local context. As such, we introduce three methods\nto improve a neural model's performance in the low-resource setting, finding\nthat limiting the model's self-attention is the most effective one, improving\non downstream tasks such as NLI and POS tagging by up to 5% for the languages\nwe test on: English, Hindi, and Turkish.", "published": "2022-05-10 11:19:56", "link": "http://arxiv.org/abs/2205.04810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the prosody GAP: Genetic Algorithm with People to efficiently\n  sample emotional prosody", "abstract": "The human voice effectively communicates a range of emotions with nuanced\nvariations in acoustics. Existing emotional speech corpora are limited in that\nthey are either (a) highly curated to induce specific emotions with predefined\ncategories that may not capture the full extent of emotional experiences, or\n(b) entangled in their semantic and prosodic cues, limiting the ability to\nstudy these cues separately. To overcome this challenge, we propose a new\napproach called 'Genetic Algorithm with People' (GAP), which integrates human\ndecision and production into a genetic algorithm. In our design, we allow\ncreators and raters to jointly optimize the emotional prosody over generations.\nWe demonstrate that GAP can efficiently sample from the emotional speech space\nand capture a broad range of emotions, and show comparable results to\nstate-of-the-art emotional speech corpora. GAP is language-independent and\nsupports large crowd-sourcing, thus can support future large-scale\ncross-cultural research.", "published": "2022-05-10 11:45:15", "link": "http://arxiv.org/abs/2205.04820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UL2: Unifying Language Learning Paradigms", "abstract": "Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "published": "2022-05-10 19:32:20", "link": "http://arxiv.org/abs/2205.05131v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ratatouille: A tool for Novel Recipe Generation", "abstract": "Due to availability of a large amount of cooking recipes online, there is a\ngrowing interest in using this as data to create novel recipes. Novel Recipe\nGeneration is a problem in the field of Natural Language Processing in which\nour main interest is to generate realistic, novel cooking recipes. To come up\nwith such novel recipes, we trained various Deep Learning models such as LSTMs\nand GPT-2 with a large amount of recipe data. We present Ratatouille\n(https://cosylab.iiitd.edu.in/ratatouille2/), a web based application to\ngenerate novel recipes.", "published": "2022-05-10 11:20:19", "link": "http://arxiv.org/abs/2206.08267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level Privacy for Document Embeddings", "abstract": "User language data can contain highly sensitive personal content. As such, it\nis imperative to offer users a strong and interpretable privacy guarantee when\nlearning from their data. In this work, we propose SentDP: pure local\ndifferential privacy at the sentence level for a single user document. We\npropose a novel technique, DeepCandidate, that combines concepts from robust\nstatistics and language modeling to produce high-dimensional, general-purpose\n$\\epsilon$-SentDP document embeddings. This guarantees that any single sentence\nin a document can be substituted with any other sentence while keeping the\nembedding $\\epsilon$-indistinguishable. Our experiments indicate that these\nprivate document embeddings are useful for downstream tasks like sentiment\nanalysis and topic classification and even outperform baseline methods with\nweaker guarantees like word-level Metric DP.", "published": "2022-05-10 00:19:35", "link": "http://arxiv.org/abs/2205.04605v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the\n  Federated Setting", "abstract": "We study the knowledge extrapolation problem to embed new components (i.e.,\nentities and relations) that come with emerging knowledge graphs (KGs) in the\nfederated setting. In this problem, a model trained on an existing KG needs to\nembed an emerging KG with unseen entities and relations. To solve this problem,\nwe introduce the meta-learning setting, where a set of tasks are sampled on the\nexisting KG to mimic the link prediction task on the emerging KG. Based on\nsampled tasks, we meta-train a graph neural network framework that can\nconstruct features for unseen components based on structural information and\noutput embeddings for them. Experimental results show that our proposed method\ncan effectively embed unseen components and outperforms models that consider\ninductive settings for KGs and baselines that directly use conventional KG\nembedding methods.", "published": "2022-05-10 06:27:32", "link": "http://arxiv.org/abs/2205.04692v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Distillation to Hard Negative Sampling: Making Sparse Neural IR\n  Models More Effective", "abstract": "Neural retrievers based on dense representations combined with Approximate\nNearest Neighbors search have recently received a lot of attention, owing their\nsuccess to distillation and/or better sampling of examples for training --\nwhile still relying on the same backbone architecture. In the meantime, sparse\nrepresentation learning fueled by traditional inverted indexing techniques has\nseen a growing interest, inheriting from desirable IR priors such as explicit\nlexical matching. While some architectural variants have been proposed, a\nlesser effort has been put in the training of such models. In this work, we\nbuild on SPLADE -- a sparse expansion-based retriever -- and show to which\nextent it is able to benefit from the same training improvements as dense\nmodels, by studying the effect of distillation, hard-negative mining as well as\nthe Pre-trained Language Model initialization. We furthermore study the link\nbetween effectiveness and efficiency, on in-domain and zero-shot settings,\nleading to state-of-the-art results in both scenarios for sufficiently\nexpressive models.", "published": "2022-05-10 08:08:43", "link": "http://arxiv.org/abs/2205.04733v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Controlling Extra-Textual Attributes about Dialogue Participants -- A\n  Case Study of English-to-Polish Neural Machine Translation", "abstract": "Unlike English, morphologically rich languages can reveal characteristics of\nspeakers or their conversational partners, such as gender and number, via\npronouns, morphological endings of words and syntax. When translating from\nEnglish to such languages, a machine translation model needs to opt for a\ncertain interpretation of textual context, which may lead to serious\ntranslation errors if extra-textual information is unavailable. We investigate\nthis challenge in the English-to-Polish language direction. We focus on the\nunderresearched problem of utilising external metadata in automatic translation\nof TV dialogue, proposing a case study where a wide range of approaches for\ncontrolling attributes in translation is employed in a multi-attribute\nscenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,\nwith other models achieving competitive performance. We additionally contribute\na novel attribute-annotated dataset of Polish TV dialogue and a morphological\nanalysis script used to evaluate attribute control in models.", "published": "2022-05-10 08:45:39", "link": "http://arxiv.org/abs/2205.04747v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "White-box Testing of NLP models with Mask Neuron Coverage", "abstract": "Recent literature has seen growing interest in using black-box strategies\nlike CheckList for testing the behavior of NLP models. Research on white-box\ntesting has developed a number of methods for evaluating how thoroughly the\ninternal behavior of deep models is tested, but they are not applicable to NLP\nmodels. We propose a set of white-box testing methods that are customized for\ntransformer-based NLP models. These include Mask Neuron Coverage (MNCOVER) that\nmeasures how thoroughly the attention layers in models are exercised during\ntesting. We show that MNCOVER can refine testing suites generated by CheckList\nby substantially reduce them in size, for more than 60\\% on average, while\nretaining failing tests -- thereby concentrating the fault detection power of\nthe test suite. Further we show how MNCOVER can be used to guide CheckList\ninput generation, evaluate alternative NLP testing methods, and drive data\naugmentation to improve accuracy.", "published": "2022-05-10 17:07:23", "link": "http://arxiv.org/abs/2205.05050v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Climate Awareness in NLP Research", "abstract": "The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.", "published": "2022-05-10 17:56:23", "link": "http://arxiv.org/abs/2205.05071v4", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Problems with Cosine as a Measure of Embedding Similarity for High\n  Frequency Words", "abstract": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,\nQA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in\nwhich word similarities estimated by cosine over BERT embeddings are\nunderstated and trace this effect to training data frequency. We find that\nrelative to human judgements, cosine similarity underestimates the similarity\nof frequent words with other instances of the same word or other words across\ncontexts, even after controlling for polysemy and other factors. We conjecture\nthat this underestimation of similarity for high frequency words is due to\ndifferences in the representational geometry of high and low frequency words\nand provide a formal argument for the two-dimensional case.", "published": "2022-05-10 18:00:06", "link": "http://arxiv.org/abs/2205.05092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Richer Countries and Richer Representations", "abstract": "We examine whether some countries are more richly represented in embedding\nspace than others. We find that countries whose names occur with low frequency\nin training corpora are more likely to be tokenized into subwords, are less\nsemantically distinct in embedding space, and are less likely to be correctly\npredicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted\nfor, \"The country producing the most cocoa is [MASK].\". Although these\nperformance discrepancies and representational harms are due to frequency, we\nfind that frequency is highly correlated with a country's GDP; thus\nperpetuating historic power and wealth inequalities. We analyze the\neffectiveness of mitigation strategies; recommend that researchers report\ntraining word frequencies; and recommend future work for the community to\ndefine and design representational guarantees.", "published": "2022-05-10 18:00:08", "link": "http://arxiv.org/abs/2205.05093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human Language Modeling", "abstract": "Natural language is generated by people, yet traditional language modeling\nviews words or documents as if generated independently. Here, we propose human\nlanguage modeling (HuLM), a hierarchical extension to the language modeling\nproblem whereby a human-level exists to connect sequences of documents (e.g.\nsocial media messages) and capture the notion that human language is moderated\nby changing human states. We introduce, HaRT, a large-scale transformer model\nfor the HuLM task, pre-trained on approximately 100,000 social media users, and\ndemonstrate its effectiveness in terms of both language modeling (perplexity)\nfor social media and fine-tuning for 4 downstream tasks spanning document- and\nuser-levels: stance detection, sentiment classification, age estimation, and\npersonality assessment. Results on all tasks meet or surpass the current\nstate-of-the-art.", "published": "2022-05-10 19:11:12", "link": "http://arxiv.org/abs/2205.05128v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sibylvariant Transformations for Robust Text Classification", "abstract": "The vast majority of text transformation techniques in NLP are inherently\nlimited in their ability to expand input space coverage due to an implicit\nconstraint to preserve the original class label. In this work, we propose the\nnotion of sibylvariance (SIB) to describe the broader set of transforms that\nrelax the label-preserving constraint, knowably vary the expected class, and\nlead to significantly more diverse input distributions. We offer a unified\nframework to organize all data transformations, including two types of SIB: (1)\nTransmutations convert one discrete kind into another, (2) Mixture Mutations\nblend two or more classes together. To explore the role of sibylvariance within\nNLP, we implemented 41 text transformations, including several novel techniques\nlike Concept2Sentence and SentMix. Sibylvariance also enables a unique form of\nadaptive training that generates new input mixtures for the most confused class\npairs, challenging the learner to differentiate with greater nuance. Our\nexperiments on six benchmark datasets strongly support the efficacy of\nsibylvariance for generalization performance, defect detection, and adversarial\nrobustness.", "published": "2022-05-10 19:38:54", "link": "http://arxiv.org/abs/2205.05137v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing Activation Recomputation in Large Transformer Models", "abstract": "Training large transformer models is one of the most important computational\nchallenges of modern AI. In this paper, we show how to significantly accelerate\ntraining of large transformer models by reducing activation recomputation.\nActivation recomputation is commonly used to work around memory capacity\nconstraints. Rather than storing activations for backpropagation, they are\ntraditionally recomputed, which saves memory but adds redundant compute. In\nthis work, we show most of this redundant compute is unnecessary because we can\nreduce memory consumption sufficiently without it. We present two novel yet\nvery simple techniques: sequence parallelism and selective activation\nrecomputation. In conjunction with tensor parallelism, these techniques almost\neliminate the need to recompute activations. We evaluate our approach on\nlanguage models up to one trillion parameters in scale and show that our method\nreduces activation memory by 5x, while reducing execution time overhead from\nactivation recomputation by over 90%. For example, when training a 530B\nparameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops\nUtilization of 54.2%, which is 29% faster than the 42.1% we achieve using\nrecomputation. Our implementation will be available in both Megatron-LM and\nNeMo-Megatron.", "published": "2022-05-10 22:40:17", "link": "http://arxiv.org/abs/2205.05198v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ANGLEr: A Next-Generation Natural Language Exploratory Framework", "abstract": "Natural language processing is used for solving a wide variety of problems.\nSome scholars and interest groups working with language resources are not well\nversed in programming, so there is a need for a good graphical framework that\nallows users to quickly design and test natural language processing pipelines\nwithout the need for programming. The existing frameworks do not satisfy all\nthe requirements for such a tool. We, therefore, propose a new framework that\nprovides a simple way for its users to build language processing pipelines. It\nalso allows a simple programming language agnostic way for adding new modules,\nwhich will help the adoption by natural language processing developers and\nresearchers. The main parts of the proposed framework consist of (a) a\npluggable Docker-based architecture, (b) a general data model, and (c) APIs\ndescription along with the graphical user interface. The proposed design is\nbeing used for implementation of a new natural language processing framework,\ncalled ANGLEr.", "published": "2022-05-10 13:32:13", "link": "http://arxiv.org/abs/2206.08266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SuMe: A Dataset Towards Summarizing Biomedical Mechanisms", "abstract": "Can language models read biomedical texts and explain the biomedical\nmechanisms discussed? In this work we introduce a biomedical mechanism\nsummarization task. Biomedical studies often investigate the mechanisms behind\nhow one entity (e.g., a protein or a chemical) affects another in a biological\ncontext. The abstracts of these publications often include a focused set of\nsentences that present relevant supporting statements regarding such\nrelationships, associated experimental evidence, and a concluding sentence that\nsummarizes the mechanism underlying the relationship. We leverage this\nstructure and create a summarization task, where the input is a collection of\nsentences and the main entities in an abstract, and the output includes the\nrelationship and a sentence that summarizes the mechanism. Using a small amount\nof manually labeled mechanism sentences, we train a mechanism sentence\nclassifier to filter a large biomedical abstract collection and create a\nsummarization dataset with 22k instances. We also introduce conclusion sentence\ngeneration as a pretraining task with 611k instances. We benchmark the\nperformance of large bio-domain language models. We find that while the\npretraining task help improves performance, the best model produces acceptable\nmechanism outputs in only 32% of the instances, which shows the task presents\nsignificant challenges in biomedical language understanding and summarization.", "published": "2022-05-10 03:42:30", "link": "http://arxiv.org/abs/2205.04652v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ALLSH: Active Learning Guided by Local Sensitivity and Hardness", "abstract": "Active learning, which effectively collects informative unlabeled data for\nannotation, reduces the demand for labeled data. In this work, we propose to\nretrieve unlabeled samples with a local sensitivity and hardness-aware\nacquisition function. The proposed method generates data copies through local\nperturbations and selects data points whose predictive likelihoods diverge the\nmost from their copies. We further empower our acquisition function by\ninjecting the select-worst case perturbation. Our method achieves consistent\ngains over the commonly used active learning strategies in various\nclassification tasks. Furthermore, we observe consistent improvements over the\nbaselines on the study of prompt selection in prompt-based few-shot learning.\nThese experiments demonstrate that our acquisition guided by local sensitivity\nand hardness can be effective and beneficial for many NLP tasks.", "published": "2022-05-10 15:39:11", "link": "http://arxiv.org/abs/2205.04980v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Answer Visual Questions from Web Videos", "abstract": "Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and the VideoQA feature\nprobe evaluation setting and show excellent results, in particular for rare\nanswers. Furthermore, our method achieves competitive results on MSRVTT-QA,\nActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA\ndataset generation approach generalizes to another source of web video and text\ndata. We use our method to generate the WebVidVQA3M dataset from the WebVid\ndataset, i.e., videos with alt-text annotations, and show its benefits for\ntraining VideoQA models. Finally, for a detailed evaluation we introduce iVQA,\na new VideoQA dataset with reduced language bias and high-quality manual\nannotations. Code, datasets and trained models are available at\nhttps://antoyang.github.io/just-ask.html", "published": "2022-05-10 16:34:26", "link": "http://arxiv.org/abs/2205.05019v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extracting Latent Steering Vectors from Pretrained Language Models", "abstract": "Prior work on controllable text generation has focused on learning how to\ncontrol language models through trainable decoding, smart-prompt design, or\nfine-tuning based on a desired objective. We hypothesize that the information\nneeded to steer the model to generate a target sentence is already encoded\nwithin the model. Accordingly, we explore a different approach altogether:\nextracting latent vectors directly from pretrained language model decoders\nwithout fine-tuning. Experiments show that there exist steering vectors, which,\nwhen added to the hidden states of the language model, generate a target\nsentence nearly perfectly (> 99 BLEU) for English sentences from a variety of\ndomains. We show that vector arithmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with performance comparable to models\ntailored to this task. We find that distances between steering vectors reflect\nsentence similarity when evaluated on a textual similarity benchmark (STS-B),\noutperforming pooled hidden states of models. Finally, we present an analysis\nof the intrinsic properties of the steering vectors. Taken together, our\nresults suggest that frozen LMs can be effectively controlled through their\nlatent steering space.", "published": "2022-05-10 19:04:37", "link": "http://arxiv.org/abs/2205.05124v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Separator-Transducer-Segmenter: Streaming Recognition and Segmentation\n  of Multi-party Speech", "abstract": "Streaming recognition and segmentation of multi-party conversations with\noverlapping speech is crucial for the next generation of voice assistant\napplications. In this work we address its challenges discovered in the previous\nwork on multi-turn recurrent neural network transducer (MT-RNN-T) with a novel\napproach, separator-transducer-segmenter (STS), that enables tighter\nintegration of speech separation, recognition and segmentation in a single\nmodel. First, we propose a new segmentation modeling strategy through\nstart-of-turn and end-of-turn tokens that improves segmentation without\nrecognition accuracy degradation. Second, we further improve both speech\nrecognition and segmentation accuracy through an emission regularization\nmethod, FastEmit, and multi-task training with speech activity information as\nan additional training signal. Third, we experiment with end-of-turn emission\nlatency penalty to improve end-point detection for each speaker turn. Finally,\nwe establish a novel framework for segmentation analysis of multi-party\nconversations through emission latency metrics. With our best model, we report\n4.6% abs. turn counting accuracy improvement and 17% rel. word error rate (WER)\nimprovement on LibriCSS dataset compared to the previously published work.", "published": "2022-05-10 22:40:39", "link": "http://arxiv.org/abs/2205.05199v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Preliminary assessment of a cost-effective headphone calibration\n  procedure for soundscape evaluations", "abstract": "The introduction of ISO 12913-2:2018 has provided a framework for\nstandardized data collection and reporting procedures for soundscape\npractitioners. A strong emphasis was placed on the use of calibrated head and\ntorso simulators (HATS) for binaural audio capture to obtain an accurate\nsubjective impression and acoustic measure of the soundscape under evaluation.\nTo auralise the binaural recordings as recorded or at set levels, the audio\nstimuli and the headphone setup are usually calibrated with a HATS. However,\ncalibrated HATS are too financially prohibitive for most research teams,\ninevitably diminishing the availability of the soundscape standard. With the\nincreasing availability of soundscape binaural recording datasets, and the\nimportance of cross-cultural validation of the soundscape ISO standards, e.g.\\\nvia the Soundscape Attributes Translation Project (SATP), it is imperative to\nassess the suitability of cost-effective headphone calibration methods to\nmaximise availability without severely compromising on accuracy. Hence, this\nstudy objectively examines an open-circuit voltage (OCV) calibration method in\ncomparison to a calibrated HATS on various soundcard and headphone\ncombinations. Preliminary experiments found that calibration with the OCV\nmethod differed significantly from the reference binaural recordings in sound\npressure levels, whereas negligible differences in levels were observed with\nthe HATS calibration.", "published": "2022-05-10 08:01:26", "link": "http://arxiv.org/abs/2205.04728v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gamified Speaker Comparison by Listening", "abstract": "We address speaker comparison by listening in a game-like environment,\nhypothesized to make the task more motivating for naive listeners. We present\nthe same 30 trials selected with the help of an x-vector speaker recognition\nsystem from VoxCeleb to a total of 150 crowdworkers recruited through Amazon's\nMechanical Turk. They are divided into cohorts of 50, each using one of three\nalternative interface designs: (i) a traditional (nongamified) design; (ii) a\ngamified design with feedback on decisions, along with points, game level\nindications, and possibility for interface customization; (iii) another\ngamified design with an additional constraint of maximum of 5 'lives' consumed\nby wrong answers. We analyze the impact of these interface designs to listener\nerror rates (both misses and false alarms), probability calibration, time of\nquitting, along with survey questionnaire. The results indicate improved\nperformance from (i) to (ii) and (iii), particularly in terms of balancing the\ntwo types of detection errors.", "published": "2022-05-10 14:25:06", "link": "http://arxiv.org/abs/2205.04923v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A 14uJ/Decision Keyword Spotting Accelerator with In-SRAM-Computing and\n  On Chip Learning for Customization", "abstract": "Keyword spotting has gained popularity as a natural way to interact with\nconsumer devices in recent years. However, because of its always-on nature and\nthe variety of speech, it necessitates a low-power design as well as user\ncustomization. This paper describes a low-power, energy-efficient keyword\nspotting accelerator with SRAM based in-memory computing (IMC) and on-chip\nlearning for user customization. However, IMC is constrained by macro size,\nlimited precision, and non-ideal effects. To address the issues mentioned\nabove, this paper proposes bias compensation and fine-tuning using an IMC-aware\nmodel design. Furthermore, because learning with low-precision edge devices\nresults in zero error and gradient values due to quantization, this paper\nproposes error scaling and small gradient accumulation to achieve the same\naccuracy as ideal model training. The simulation results show that with user\ncustomization, we can recover the accuracy loss from 51.08\\% to 89.76\\% with\ncompensation and fine-tuning and further improve to 96.71\\% with customization.\nThe chip implementation can successfully run the model with only 14$uJ$ per\ndecision. When compared to the state-of-the-art works, the presented design has\nhigher energy efficiency with additional on-chip model customization\ncapabilities for higher accuracy.", "published": "2022-05-10 04:42:20", "link": "http://arxiv.org/abs/2205.04665v1", "categories": ["cs.AR", "cs.LG", "eess.AS"], "primary_category": "cs.AR"}
{"title": "Learning Visual Styles from Audio-Visual Associations", "abstract": "From the patter of rain to the crunch of snow, the sounds we hear often\nconvey the visual textures that appear within a scene. In this paper, we\npresent a method for learning visual styles from unlabeled audio-visual data.\nOur model learns to manipulate the texture of a scene to match a sound, a\nproblem we term audio-driven image stylization. Given a dataset of paired\naudio-visual data, we learn to modify input images such that, after\nmanipulation, they are more likely to co-occur with a given input sound. In\nquantitative and qualitative evaluations, our sound-based model outperforms\nlabel-based approaches. We also show that audio can be an intuitive\nrepresentation for manipulating images, as adjusting a sound's volume or mixing\ntwo sounds together results in predictable changes to visual style. Project\nwebpage: https://tinglok.netlify.app/files/avstyle", "published": "2022-05-10 17:57:07", "link": "http://arxiv.org/abs/2205.05072v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Best of Both Worlds: Multi-task Audio-Visual Automatic Speech\n  Recognition and Active Speaker Detection", "abstract": "Under noisy conditions, automatic speech recognition (ASR) can greatly\nbenefit from the addition of visual signals coming from a video of the\nspeaker's face. However, when multiple candidate speakers are visible this\ntraditionally requires solving a separate problem, namely active speaker\ndetection (ASD), which entails selecting at each moment in time which of the\nvisible faces corresponds to the audio. Recent work has shown that we can solve\nboth problems simultaneously by employing an attention mechanism over the\ncompeting video tracks of the speakers' faces, at the cost of sacrificing some\naccuracy on active speaker detection. This work closes this gap in active\nspeaker detection accuracy by presenting a single model that can be jointly\ntrained with a multi-task loss. By combining the two tasks during training we\nreduce the ASD classification accuracy by approximately 25%, while\nsimultaneously improving the ASR performance when compared to the multi-person\nbaseline trained exclusively for ASR.", "published": "2022-05-10 23:03:19", "link": "http://arxiv.org/abs/2205.05206v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Symphony Generation with Permutation Invariant Language Model", "abstract": "In this work, we propose a permutation invariant language model, SymphonyNet,\nas a solution for symbolic symphony music generation. We propose a novel\nMulti-track Multi-instrument Repeatable (MMR) representation for symphonic\nmusic and model the music sequence using a Transformer-based auto-regressive\nlanguage model with specific 3-D positional embedding. To overcome length\noverflow when modeling extra-long symphony tokens, we also propose a modified\nByte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel\nlinear transformer decoder architecture as a backbone. Meanwhile, we train the\ndecoder to learn automatic orchestration as a joint task by masking instrument\ninformation from the input. We also introduce a large-scale symbolic symphony\ndataset for the advance of symphony generation research. Empirical results show\nthat the proposed approach can generate coherent, novel, complex and harmonious\nsymphony as a pioneer solution for multi-track multi-instrument symbolic music\ngeneration.", "published": "2022-05-10 13:08:49", "link": "http://arxiv.org/abs/2205.05448v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
