{"title": "Exploring the power of GPU's for training Polyglot language models", "abstract": "One of the major research trends currently is the evolution of heterogeneous\nparallel computing. GP-GPU computing is being widely used and several\napplications have been designed to exploit the massive parallelism that\nGP-GPU's have to offer. While GPU's have always been widely used in areas of\ncomputer vision for image processing, little has been done to investigate\nwhether the massive parallelism provided by GP-GPU's can be utilized\neffectively for Natural Language Processing(NLP) tasks. In this work, we\ninvestigate and explore the power of GP-GPU's in the task of learning language\nmodels. More specifically, we investigate the performance of training Polyglot\nlanguage models using deep belief neural networks. We evaluate the performance\nof training the model on the GPU and present optimizations that boost the\nperformance on the GPU.One of the key optimizations, we propose increases the\nperformance of a function involved in calculating and updating the gradient by\napproximately 50 times on the GPU for sufficiently large batch sizes. We show\nthat with the above optimizations, the GP-GPU's performance on the task\nincreases by factor of approximately 3-4. The optimizations we made are generic\nTheano optimizations and hence potentially boost the performance of other\nmodels which rely on these operations.We also show that these optimizations\nresult in the GPU's performance at this task being now comparable to that on\nthe CPU. We conclude by presenting a thorough evaluation of the applicability\nof GP-GPU's for this task and highlight the factors limiting the performance of\ntraining a Polyglot model on the GPU.", "published": "2014-04-05 21:25:54", "link": "http://arxiv.org/abs/1404.1521v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
