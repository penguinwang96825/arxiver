{"title": "Discourse Level Factors for Sentence Deletion in Text Simplification", "abstract": "This paper presents a data-driven study focusing on analyzing and predicting\nsentence deletion -- a prevalent but understudied phenomenon in document\nsimplification -- on a large English text simplification corpus. We inspect\nvarious document and discourse factors associated with sentence deletion, using\na new manually annotated sentence alignment corpus we collected. We reveal that\nprofessional editors utilize different strategies to meet readability standards\nof elementary and middle schools. To predict whether a sentence will be deleted\nduring simplification to a certain level, we harness automatically aligned data\nto train a classification model. Evaluated on our manually annotated data, our\nbest models reached F1 scores of 65.2 and 59.7 for this task at the levels of\nelementary and middle school, respectively. We find that discourse level\nfactors contribute to the challenging task of predicting sentence deletion for\nsimplification.", "published": "2019-11-23 16:23:21", "link": "http://arxiv.org/abs/1911.10384v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Parsing and Generation for Abstractive Summarization", "abstract": "Sentences produced by abstractive summarization systems can be ungrammatical\nand fail to preserve the original meanings, despite being locally fluent. In\nthis paper we propose to remedy this problem by jointly generating a sentence\nand its syntactic dependency parse while performing abstraction. If generating\na word can introduce an erroneous relation to the summary, the behavior must be\ndiscouraged. The proposed method thus holds promise for producing grammatical\nsentences and encouraging the summary to stay true-to-original. Our\ncontributions of this work are twofold. First, we present a novel neural\narchitecture for abstractive summarization that combines a sequential decoder\nwith a tree-based decoder in a synchronized manner to generate a summary\nsentence and its syntactic parse. Secondly, we describe a novel human\nevaluation protocol to assess if, and to what extent, a summary remains true to\nits original meanings. We evaluate our method on a number of summarization\ndatasets and demonstrate competitive results against strong baselines.", "published": "2019-11-23 17:28:54", "link": "http://arxiv.org/abs/1911.10389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling the Amount of Verbatim Copying in Abstractive Summarization", "abstract": "An abstract must not change the meaning of the original text. A single most\neffective way to achieve that is to increase the amount of copying while still\nallowing for text abstraction. Human editors can usually exercise control over\ncopying, resulting in summaries that are more extractive than abstractive, or\nvice versa. However, it remains poorly understood whether modern neural\nabstractive summarizers can provide the same flexibility, i.e., learning from\nsingle reference summaries to generate multiple summary hypotheses with varying\ndegrees of copying. In this paper, we present a neural summarization model\nthat, by learning from single human abstracts, can produce a broad spectrum of\nsummaries ranging from purely extractive to highly generative ones. We frame\nthe task of summarization as language modeling and exploit alternative\nmechanisms to generate summary hypotheses. Our method allows for control over\ncopying during both training and decoding stages of a neural summarization\nmodel. Through extensive experiments we illustrate the significance of our\nproposed method on controlling the amount of verbatim copying and achieve\ncompetitive results over strong baselines. Our analysis further reveals\ninteresting and unobvious facts.", "published": "2019-11-23 17:34:19", "link": "http://arxiv.org/abs/1911.10390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When is ACL's Deadline? A Scientific Conversational Agent", "abstract": "Our conversational agent UKP-ATHENA assists NLP researchers in finding and\nexploring scientific literature, identifying relevant authors, planning or\npost-processing conference visits, and preparing paper submissions using a\nunified interface based on natural language inputs and responses. UKP-ATHENA\nenables new access paths to our swiftly evolving research area with its massive\namounts of scientific information and high turnaround times. UKP-ATHENA's\nresponses connect information from multiple heterogeneous sources which\nresearchers currently have to explore manually one after another. Unlike a\nsearch engine, UKP-ATHENA maintains the context of a conversation to allow for\nefficient information access on papers, researchers, and conferences. Our\narchitecture consists of multiple components with reference implementations\nthat can be easily extended by new skills and domains. Our user-based\nevaluation shows that UKP-ATHENA already responds 45% of different formulations\nof defined intents with 37% information coverage rate.", "published": "2019-11-23 17:41:02", "link": "http://arxiv.org/abs/1911.10392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Transformer-based approach to Irony and Sarcasm detection", "abstract": "Figurative Language (FL) seems ubiquitous in all social-media discussion\nforums and chats, posing extra challenges to sentiment analysis endeavors.\nIdentification of FL schemas in short texts remains largely an unresolved issue\nin the broader field of Natural Language Processing (NLP), mainly due to their\ncontradictory and metaphorical meaning content. The main FL expression forms\nare sarcasm, irony and metaphor. In the present paper we employ advanced Deep\nLearning (DL) methodologies to tackle the problem of identifying the\naforementioned FL forms. Significantly extending our previous work [71], we\npropose a neural network methodology that builds on a recently proposed\npre-trained transformer-based network architecture which, is further enhanced\nwith the employment and devise of a recurrent convolutional neural network\n(RCNN). With this set-up, data preprocessing is kept in minimum. The\nperformance of the devised hybrid neural architecture is tested on four\nbenchmark datasets, and contrasted with other relevant state of the art\nmethodologies and systems. Results demonstrate that the proposed methodology\nachieves state of the art performance under all benchmark datasets,\noutperforming, even by a large margin, all other methodologies and published\nstudies.", "published": "2019-11-23 18:37:48", "link": "http://arxiv.org/abs/1911.10401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using the Web as an Implicit Training Set: Application to Noun Compound\n  Syntax and Semantics", "abstract": "An important characteristic of English written text is the abundance of noun\ncompounds - sequences of nouns acting as a single noun, e.g., colon cancer\ntumor suppressor protein. While eventually mastered by domain experts, their\ninterpretation poses a major challenge for automated analysis. Understanding\nnoun compounds' syntax and semantics is important for many natural language\napplications, including question answering, machine translation, information\nretrieval, and information extraction. I address the problem of noun compounds\nsyntax by means of novel, highly accurate unsupervised and lightly supervised\nalgorithms using the Web as a corpus and search engines as interfaces to that\ncorpus. Traditionally the Web has been viewed as a source of page hit counts,\nused as an estimate for n-gram word frequencies. I extend this approach by\nintroducing novel surface features and paraphrases, which yield\nstate-of-the-art results for the task of noun compound bracketing. I also show\nhow these kinds of features can be applied to other structural ambiguity\nproblems, like prepositional phrase attachment and noun phrase coordination. I\naddress noun compound semantics by automatically generating paraphrasing verbs\nand prepositions that make explicit the hidden semantic relations between the\nnouns in a noun compound. I also demonstrate how these paraphrasing verbs can\nbe used to solve various relational similarity problems, and how paraphrasing\nnoun compounds can improve machine translation.", "published": "2019-11-23 21:33:31", "link": "http://arxiv.org/abs/1912.01113v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Keyword Extraction for Full-sentence VQA", "abstract": "In the majority of the existing Visual Question Answering (VQA) research, the\nanswers consist of short, often single words, as per instructions given to the\nannotators during dataset construction. This study envisions a VQA task for\nnatural situations, where the answers are more likely to be sentences rather\nthan single words. To bridge the gap between this natural VQA and existing VQA\napproaches, a novel unsupervised keyword extraction method is proposed. The\nmethod is based on the principle that the full-sentence answers can be\ndecomposed into two parts: one that contains new information answering the\nquestion (i.e., keywords), and one that contains information already included\nin the question. Discriminative decoders were designed to achieve such\ndecomposition, and the method was experimentally implemented on VQA datasets\ncontaining full-sentence answers. The results show that the proposed model can\naccurately extract the keywords without being given explicit annotations\ndescribing them.", "published": "2019-11-23 12:18:03", "link": "http://arxiv.org/abs/1911.10354v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SemEval-2013 Task 4: Free Paraphrases of Noun Compounds", "abstract": "In this paper, we describe SemEval-2013 Task 4: the definition, the data, the\nevaluation and the results. The task is to capture some of the meaning of\nEnglish noun compounds via paraphrasing. Given a two-word noun compound, the\nparticipating system is asked to produce an explicitly ranked list of its\nfree-form paraphrases. The list is automatically compared and evaluated against\na similarly ranked list of paraphrases proposed by human annotators, recruited\nand managed through Amazon's Mechanical Turk. The comparison of raw paraphrases\nis sensitive to syntactic and morphological variation. The \"gold\" ranking is\nbased on the relative popularity of paraphrases among annotators. To make the\nranking more reliable, highly similar paraphrases are grouped, so as to\ndownplay superficial differences in syntax and morphology. Three systems\nparticipated in the task. They all beat a simple baseline on one of the two\nevaluation measures, but not on both measures. This shows that the task is\ndifficult.", "published": "2019-11-23 21:42:23", "link": "http://arxiv.org/abs/1911.10421v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations\n  Between Pairs of Nominals", "abstract": "In response to the continuing research interest in computational semantic\nanalysis, we have proposed a new task for SemEval-2010: multi-way\nclassification of mutually exclusive semantic relations between pairs of\nnominals. The task is designed to compare different approaches to the problem\nand to provide a standard testbed for future research. In this paper, we define\nthe task, describe the creation of the datasets, and discuss the results of the\nparticipating 28 systems submitted by 10 teams.", "published": "2019-11-23 21:49:10", "link": "http://arxiv.org/abs/1911.10422v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
