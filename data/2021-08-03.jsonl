{"title": "M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in\n  Conversations", "abstract": "Humor recognition in conversations is a challenging task that has recently\ngained popularity due to its importance in dialogue understanding, including in\nmultimodal settings (i.e., text, acoustics, and visual). The few existing\ndatasets for humor are mostly in English. However, due to the tremendous growth\nin multilingual content, there is a great demand to build models and systems\nthat support multilingual information access. To this end, we propose a dataset\nfor Multimodal Multiparty Hindi Humor (M2H2) recognition in conversations\ncontaining 6,191 utterances from 13 episodes of a very popular TV series\n\"Shrimaan Shrimati Phir Se\". Each utterance is annotated with humor/non-humor\nlabels and encompasses acoustic, visual, and textual modalities. We propose\nseveral strong multimodal baselines and show the importance of contextual and\nmultimodal information for humor recognition in conversations. The empirical\nresults on M2H2 dataset demonstrate that multimodal information complements\nunimodal information for humor recognition. The dataset and the baselines are\navailable at http://www.iitp.ac.in/~ai-nlp-ml/resources.html and\nhttps://github.com/declare-lab/M2H2-dataset.", "published": "2021-08-03 02:54:09", "link": "http://arxiv.org/abs/2108.01260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Summarization with Supporting Utterance Flow Modeling and Fact\n  Regularization", "abstract": "Dialogue summarization aims to generate a summary that indicates the key\npoints of a given dialogue. In this work, we propose an end-to-end neural model\nfor dialogue summarization with two novel modules, namely, the \\emph{supporting\nutterance flow modeling module} and the \\emph{fact regularization module}. The\nsupporting utterance flow modeling helps to generate a coherent summary by\nsmoothly shifting the focus from the former utterances to the later ones. The\nfact regularization encourages the generated summary to be factually consistent\nwith the ground-truth summary during model training, which helps to improve the\nfactual correctness of the generated summary in inference time. Furthermore, we\nalso introduce a new benchmark dataset for dialogue summarization. Extensive\nexperiments on both existing and newly-introduced datasets demonstrate the\neffectiveness of our model.", "published": "2021-08-03 03:09:25", "link": "http://arxiv.org/abs/2108.01268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Evaluate Your Dialogue Models: A Review of Approaches", "abstract": "Evaluating the quality of a dialogue system is an understudied problem. The\nrecent evolution of evaluation method motivated this survey, in which an\nexplicit and comprehensive analysis of the existing methods is sought. We are\nfirst to divide the evaluation methods into three classes, i.e., automatic\nevaluation, human-involved evaluation and user simulator based evaluation.\nThen, each class is covered with main features and the related evaluation\nmetrics. The existence of benchmarks, suitable for the evaluation of dialogue\ntechniques are also discussed in detail. Finally, some open issues are pointed\nout to bring the evaluation method into a new frontier.", "published": "2021-08-03 08:52:33", "link": "http://arxiv.org/abs/2108.01369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dynamic Head Importance Computation Mechanism for Neural Machine\n  Translation", "abstract": "Multiple parallel attention mechanisms that use multiple attention heads\nfacilitate greater performance of the Transformer model for various\napplications e.g., Neural Machine Translation (NMT), text classification. In\nmulti-head attention mechanism, different heads attend to different parts of\nthe input. However, the limitation is that multiple heads might attend to the\nsame part of the input, resulting in multiple heads being redundant. Thus, the\nmodel resources are under-utilized. One approach to avoid this is to prune\nleast important heads based on certain importance score. In this work, we focus\non designing a Dynamic Head Importance Computation Mechanism (DHICM) to\ndynamically calculate the importance of a head with respect to the input. Our\ninsight is to design an additional attention layer together with multi-head\nattention, and utilize the outputs of the multi-head attention along with the\ninput, to compute the importance for each head. Additionally, we add an extra\nloss function to prevent the model from assigning same score to all heads, to\nidentify more important heads and improvise performance. We analyzed\nperformance of DHICM for NMT with different languages. Experiments on different\ndatasets show that DHICM outperforms traditional Transformer-based approach by\nlarge margin, especially, when less training data is available.", "published": "2021-08-03 09:16:55", "link": "http://arxiv.org/abs/2108.01377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Missing Links Predictable? An Inferential Benchmark for Knowledge\n  Graph Completion", "abstract": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that\nimproves upon existing benchmarks in inferential ability, assumptions, and\npatterns. First, each testing sample is predictable with supportive data in the\ntraining set. To ensure it, we propose to utilize rule-guided train/test\ngeneration, instead of conventional random split. Second, InferWiki initiates\nthe evaluation following the open-world assumption and improves the inferential\ndifficulty of the closed-world assumption, by providing manually annotated\nnegative and unknown triples. Third, we include various inference patterns\n(e.g., reasoning path length and types) for comprehensive evaluation. In\nexperiments, we curate two settings of InferWiki varying in sizes and\nstructures, and apply the construction process on CoDEx as comparative\ndatasets. The results and empirical analyses demonstrate the necessity and\nhigh-quality of InferWiki. Nevertheless, the performance gap among various\ninferential assumptions and patterns presents the difficulty and inspires\nfuture research direction. Our datasets can be found in\nhttps://github.com/TaoMiner/inferwiki", "published": "2021-08-03 09:51:15", "link": "http://arxiv.org/abs/2108.01387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExBERT: An External Knowledge Enhanced BERT for Natural Language\n  Inference", "abstract": "Neural language representation models such as BERT, pre-trained on\nlarge-scale unstructured corpora lack explicit grounding to real-world\ncommonsense knowledge and are often unable to remember facts required for\nreasoning and inference. Natural Language Inference (NLI) is a challenging\nreasoning task that relies on common human understanding of language and\nreal-world commonsense knowledge. We introduce a new model for NLI called\nExternal Knowledge Enhanced BERT (ExBERT), to enrich the contextual\nrepresentation with real-world commonsense knowledge from external knowledge\nsources and enhance BERT's language understanding and reasoning capabilities.\nExBERT takes full advantage of contextual word representations obtained from\nBERT and employs them to retrieve relevant external knowledge from knowledge\ngraphs and to encode the retrieved external knowledge. Our model adaptively\nincorporates the external knowledge context required for reasoning over the\ninputs. Extensive experiments on the challenging SciTail and SNLI benchmarks\ndemonstrate the effectiveness of ExBERT: in comparison to the previous\nstate-of-the-art, we obtain an accuracy of 95.9% on SciTail and 91.5% on SNLI.", "published": "2021-08-03 15:56:49", "link": "http://arxiv.org/abs/2108.01589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your fairness may vary: Pretrained language model fairness in toxic text\n  classification", "abstract": "The popularity of pretrained language models in natural language processing\nsystems calls for a careful evaluation of such models in down-stream tasks,\nwhich have a higher potential for societal impact. The evaluation of such\nsystems usually focuses on accuracy measures. Our findings in this paper call\nfor attention to be paid to fairness measures as well. Through the analysis of\nmore than a dozen pretrained language models of varying sizes on two toxic text\nclassification tasks (English), we demonstrate that focusing on accuracy\nmeasures alone can lead to models with wide variation in fairness\ncharacteristics. Specifically, we observe that fairness can vary even more than\naccuracy with increasing training data size and different random\ninitializations. At the same time, we find that little of the fairness\nvariation is explained by model size, despite claims in the literature. To\nimprove model fairness without retraining, we show that two post-processing\nmethods developed for structured, tabular data can be successfully applied to a\nrange of pretrained language models. Warning: This paper contains samples of\noffensive text.", "published": "2021-08-03 02:16:12", "link": "http://arxiv.org/abs/2108.01250v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More but Correct: Generating Diversified and Entity-revised Medical\n  Response", "abstract": "Medical Dialogue Generation (MDG) is intended to build a medical dialogue\nsystem for intelligent consultation, which can communicate with patients in\nreal-time, thereby improving the efficiency of clinical diagnosis with broad\napplication prospects. This paper presents our proposed framework for the\nChinese MDG organized by the 2021 China conference on knowledge graph and\nsemantic computing (CCKS) competition, which requires generating\ncontext-consistent and medically meaningful responses conditioned on the\ndialogue history. In our framework, we propose a pipeline system composed of\nentity prediction and entity-aware dialogue generation, by adding predicted\nentities to the dialogue model with a fusion mechanism, thereby utilizing\ninformation from different sources. At the decoding stage, we propose a new\ndecoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve\nentity correctness and promote the length and quality of the final response.\nThe proposed method wins both the CCKS and the International Conference on\nLearning Representations (ICLR) 2021 Workshop Machine Learning for Preventing\nand Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which\ndemonstrate the practicality and effectiveness of our method.", "published": "2021-08-03 03:03:50", "link": "http://arxiv.org/abs/2108.01266v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study of Multilingual End-to-End Speech Recognition for Kazakh,\n  Russian, and English", "abstract": "We study training a single end-to-end (E2E) automatic speech recognition\n(ASR) model for three languages used in Kazakhstan: Kazakh, Russian, and\nEnglish. We first describe the development of multilingual E2E ASR based on\nTransformer networks and then perform an extensive assessment on the\naforementioned languages. We also compare two variants of output grapheme set\nconstruction: combined and independent. Furthermore, we evaluate the impact of\nLMs and data augmentation techniques on the recognition performance of the\nmultilingual E2E ASR. In addition, we present several datasets for training and\nevaluation purposes. Experiment results show that the multilingual models\nachieve comparable performances to the monolingual baselines with a similar\nnumber of parameters. Our best monolingual and multilingual models achieved\n20.9% and 20.5% average word error rates on the combined test set,\nrespectively. To ensure the reproducibility of our experiments and results, we\nshare our training recipes, datasets, and pre-trained models.", "published": "2021-08-03 04:04:01", "link": "http://arxiv.org/abs/2108.01280v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "sarcasm detection and quantification in arabic tweets", "abstract": "The role of predicting sarcasm in the text is known as automatic sarcasm\ndetection. Given the prevalence and challenges of sarcasm in sentiment-bearing\ntext, this is a critical phase in most sentiment analysis tasks. With the\nincreasing popularity and usage of different social media platforms among users\naround the world, people are using sarcasm more and more in their day-to-day\nconversations, social media posts and tweets, and it is considered as a way for\npeople to express their sentiment about some certain topics or issues. As a\nresult of the increasing popularity, researchers started to focus their\nresearch endeavors on detecting sarcasm from a text in different languages\nespecially the English language. However, the task of sarcasm detection is a\nchallenging task due to the nature of sarcastic texts; which can be relative\nand significantly differs from one person to another depending on the topic,\nregion, the user's mentality and other factors. In addition to these\nchallenges, sarcasm detection in the Arabic language has its own challenges due\nto the complexity of the Arabic language, such as being morphologically rich,\nwith many dialects that significantly vary between each other, while also being\nlowly resourced. In recent years, only few research attempts started tackling\nthe task of sarcasm detection in Arabic, including creating and collecting\ncorpora, organizing workshops and establishing baseline models. This paper\nintends to create a new humanly annotated Arabic corpus for sarcasm detection\ncollected from tweets, and implementing a new approach for sarcasm detection\nand quantification in Arabic tweets. The annotation technique followed in this\npaper is unique in sarcasm detection and the proposed approach tackles the\nproblem as a regression problem instead of classification; i.e., the model\nattempts to predict the level of sarcasm instead of binary classification.", "published": "2021-08-03 11:48:27", "link": "http://arxiv.org/abs/2108.01425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative\n  Pre-Training", "abstract": "Although pre-trained language models have remarkably enhanced the generation\nability of dialogue systems, open-domain Chinese dialogue systems are still\nlimited by the dialogue data and the model size compared with English ones. In\nthis paper, we propose EVA, a Chinese dialogue system that contains the largest\nChinese pre-trained dialogue model with 2.8B parameters. To build this model,\nwe collect the largest Chinese dialogue dataset named WDC-Dialogue from various\npublic social media. This dataset contains 1.4B context-response pairs and is\nused as the pre-training corpus of EVA. Extensive experiments on automatic and\nhuman evaluation show that EVA outperforms other Chinese pre-trained dialogue\nmodels especially in the multi-turn interaction of human-bot conversations.", "published": "2021-08-03 14:55:24", "link": "http://arxiv.org/abs/2108.01547v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting BERT For Multimodal Target Sentiment Classification Through\n  Input Space Translation", "abstract": "Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{\\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.", "published": "2021-08-03 18:02:38", "link": "http://arxiv.org/abs/2108.01682v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Improving Counterfactual Generation for Fair Hate Speech Detection", "abstract": "Bias mitigation approaches reduce models' dependence on sensitive features of\ndata, such as social group tokens (SGTs), resulting in equal predictions across\nthe sensitive features. In hate speech detection, however, equalizing model\npredictions may ignore important differences among targeted social groups, as\nhate speech can contain stereotypical language specific to each SGT. Here, to\ntake the specific language about each SGT into account, we rely on\ncounterfactual fairness and equalize predictions among counterfactuals,\ngenerated by changing the SGTs. Our method evaluates the similarity in sentence\nlikelihoods (via pre-trained language models) among counterfactuals, to treat\nSGTs equally only within interchangeable contexts. By applying logit pairing to\nequalize outcomes on the restricted set of counterfactuals for each instance,\nwe improve fairness metrics while preserving model performance on hate speech\ndetection.", "published": "2021-08-03 19:47:27", "link": "http://arxiv.org/abs/2108.01721v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain\n  Management", "abstract": "Recent advances in Natural Language Processing (NLP), and specifically\nautomated Question Answering (QA) systems, have demonstrated both impressive\nlinguistic fluency and a pernicious tendency to reflect social biases. In this\nstudy, we introduce Q-Pain, a dataset for assessing bias in medical QA in the\ncontext of pain management, one of the most challenging forms of clinical\ndecision-making. Along with the dataset, we propose a new, rigorous framework,\nincluding a sample experimental design, to measure the potential biases present\nwhen making treatment decisions. We demonstrate its use by assessing two\nreference Question-Answering systems, GPT-2 and GPT-3, and find statistically\nsignificant differences in treatment between intersectional race-gender\nsubgroups, thus reaffirming the risks posed by AI in medical settings, and the\nneed for datasets like ours to ensure safety before medical AI applications are\ndeployed.", "published": "2021-08-03 21:55:28", "link": "http://arxiv.org/abs/2108.01764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Performance Evaluation of Attention-Based Neural ASR under Mixed\n  Speech Input", "abstract": "In order to evaluate the performance of the attention based neural ASR under\nnoisy conditions, the current trend is to present hours of various noisy speech\ndata to the model and measure the overall word/phoneme error rate (W/PER). In\ngeneral, it is unclear how these models perform when exposed to a cocktail\nparty setup in which two or more speakers are active. In this paper, we present\nthe mixtures of speech signals to a popular attention-based neural ASR, known\nas Listen, Attend, and Spell (LAS), at different target-to-interference ratio\n(TIR) and measure the phoneme error rate. In particular, we investigate in\ndetails when two phonemes are mixed what will be the predicted phoneme; in this\nfashion we build a model in which the most probable predictions for a phoneme\nare given. We found a 65% relative increase in PER when LAS was presented with\nmixed speech signals at TIR = 0 dB and the performance approaches the unmixed\nscenario at TIR = 30 dB. Our results show the model, when presented with mixed\nphonemes signals, tend to predict those that have higher accuracies during\nevaluation of original phoneme signals.", "published": "2021-08-03 02:08:22", "link": "http://arxiv.org/abs/2108.01245v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large-Scale Differentially Private BERT", "abstract": "In this work, we study the large-scale pretraining of BERT-Large with\ndifferentially private SGD (DP-SGD). We show that combined with a careful\nimplementation, scaling up the batch size to millions (i.e., mega-batches)\nimproves the utility of the DP-SGD step for BERT; we also enhance its\nefficiency by using an increasing batch size schedule. Our implementation\nbuilds on the recent work of [SVK20], who demonstrated that the overhead of a\nDP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives\nin conjunction with the XLA compiler [XLA17]. Our implementation achieves a\nmasked language model accuracy of 60.5% at a batch size of 2M, for $\\epsilon =\n5.36$. To put this number in perspective, non-private BERT models achieve an\naccuracy of $\\sim$70%.", "published": "2021-08-03 16:51:36", "link": "http://arxiv.org/abs/2108.01624v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "DarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio\n  Synthesis with GANs", "abstract": "Generative Adversarial Networks (GANs) have achieved excellent audio\nsynthesis quality in the last years. However, making them operable with\nsemantically meaningful controls remains an open challenge. An obvious approach\nis to control the GAN by conditioning it on metadata contained in audio\ndatasets. Unfortunately, audio datasets often lack the desired annotations,\nespecially in the musical domain. A way to circumvent this lack of annotations\nis to generate them, for example, with an automatic audio-tagging system. The\noutput probabilities of such systems (so-called \"soft labels\") carry rich\ninformation about the characteristics of the respective audios and can be used\nto distill the knowledge from a teacher model into a student model. In this\nwork, we perform knowledge distillation from a large audio tagging system into\nan adversarial audio synthesizer that we call DarkGAN. Results show that\nDarkGAN can synthesize musical audio with acceptable quality and exhibits\nmoderate attribute control even with out-of-distribution input conditioning. We\nrelease the code and provide audio examples on the accompanying website.", "published": "2021-08-03 00:26:55", "link": "http://arxiv.org/abs/2108.01216v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An analysis of Iranian Music Intervals based on Pitch Histogram", "abstract": "Since the early twentieth century, intervals and tuning systems have been\nsubjects of discussion among Iranian musicians and scholars. The process of\nWesternization and then a cultural back to roots movement are among the reasons\nthat motivated debates about the appropriate tuning in this musical culture. In\nthis paper, we first review the historical context of the intervals in\nPerso-Arabic musical culture since Farabi in the tenth century. Then we focus\non the audio histogram of the vocal performance of each piece in the repertoire\n(radif) of Karimi, one of the masters of the art, and use Dynamic Time Warping\nfor alignment of pitch and MIDI notes. We collected the intervals used in the\nperformance of each piece (gushe) in the repertoire and then analyzed the\nresults. Unlike the traditional methods of measuring the frequency of each note\nplayed on the tar (an Iranian lute) practiced by contemporary music scholars,\nour computational method is independent of a given instrument and can be\nexecuted on any performance with minimum effort.", "published": "2021-08-03 04:09:40", "link": "http://arxiv.org/abs/2108.01283v1", "categories": ["cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Amortized Neural Networks for Low-Latency Speech Recognition", "abstract": "We introduce Amortized Neural Networks (AmNets), a compute cost- and\nlatency-aware network architecture particularly well-suited for sequence\nmodeling tasks. We apply AmNets to the Recurrent Neural Network Transducer\n(RNN-T) to reduce compute cost and latency for an automatic speech recognition\n(ASR) task. The AmNets RNN-T architecture enables the network to dynamically\nswitch between encoder branches on a frame-by-frame basis. Branches are\nconstructed with variable levels of compute cost and model capacity. Here, we\nachieve variable compute for two well-known candidate techniques: one using\nsparse pruning and the other using matrix factorization. Frame-by-frame\nswitching is determined by an arbitrator network that requires negligible\ncompute overhead. We present results using both architectures on LibriSpeech\ndata and show that our proposed architecture can reduce inference cost by up to\n45\\% and latency to nearly real-time without incurring a loss in accuracy.", "published": "2021-08-03 15:05:13", "link": "http://arxiv.org/abs/2108.01553v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning a Neural Diff for Speech Models", "abstract": "As more speech processing applications execute locally on edge devices, a set\nof resource constraints must be considered. In this work we address one of\nthese constraints, namely over-the-network data budgets for transferring models\nfrom server to device. We present neural update approaches for release of\nsubsequent speech model generations abiding by a data budget. We detail two\narchitecture-agnostic methods which learn compact representations for\ntransmission to devices. We experimentally validate our techniques with results\non two tasks (automatic speech recognition and spoken language understanding)\non open source data sets by demonstrating when applied in succession, our\nbudgeted updates outperform comparable model compression baselines by\nsignificant margins.", "published": "2021-08-03 15:14:00", "link": "http://arxiv.org/abs/2108.01561v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Benchmarking Initiative for Audio-Domain Music Generation Using the\n  Freesound Loop Dataset", "abstract": "This paper proposes a new benchmark task for generat-ing musical passages in\nthe audio domain by using thedrum loops from the FreeSound Loop Dataset, which\narepublicly re-distributable. Moreover, we use a larger col-lection of drum\nloops from Looperman to establish fourmodel-based objective metrics for\nevaluation, releasingthese metrics as a library for quantifying and\nfacilitatingthe progress of musical audio generation. Under this eval-uation\nframework, we benchmark the performance of threerecent deep generative\nadversarial network (GAN) mod-els we customize to generate loops, including\nStyleGAN,StyleGAN2, and UNAGAN. We also report a subjectiveevaluation of these\nmodels. Our evaluation shows that theone based on StyleGAN2 performs the best\nin both objec-tive and subjective metrics.", "published": "2021-08-03 15:37:11", "link": "http://arxiv.org/abs/2108.01576v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bifocal Neural ASR: Exploiting Keyword Spotting for Inference\n  Optimization", "abstract": "We present Bifocal RNN-T, a new variant of the Recurrent Neural Network\nTransducer (RNN-T) architecture designed for improved inference time latency on\nspeech recognition tasks. The architecture enables a dynamic pivot for its\nruntime compute pathway, namely taking advantage of keyword spotting to select\nwhich component of the network to execute for a given audio frame. To\naccomplish this, we leverage a recurrent cell we call the Bifocal LSTM\n(BFLSTM), which we detail in the paper. The architecture is compatible with\nother optimization strategies such as quantization, sparsification, and\napplying time-reduction layers, making it especially applicable for deployed,\nreal-time speech recognition settings. We present the architecture and report\ncomparative experimental results on voice-assistant speech recognition tasks.\nSpecifically, we show our proposed Bifocal RNN-T can improve inference cost by\n29.1% with matching word error rates and only a minor increase in memory size.", "published": "2021-08-03 18:58:39", "link": "http://arxiv.org/abs/2108.01704v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and\n  Emotion-based Music Generation", "abstract": "While there are many music datasets with emotion labels in the literature,\nthey cannot be used for research on symbolic-domain music analysis or\ngeneration, as there are usually audio files only. In this paper, we present\nthe EMOPIA (pronounced `yee-m\\`{o}-pi-uh') dataset, a shared multi-modal (audio\nand MIDI) database focusing on perceived emotion in pop piano music, to\nfacilitate research on various tasks related to music emotion. The dataset\ncontains 1,087 music clips from 387 songs and clip-level emotion labels\nannotated by four dedicated annotators. Since the clips are not restricted to\none clip per song, they can also be used for song-level analysis. We present\nthe methodology for building the dataset, covering the song list curation, clip\nselection, and emotion annotation processes. Moreover, we prototype use cases\non clip-level music emotion classification and emotion-based symbolic music\ngeneration by training and evaluating corresponding models using the dataset.\nThe result demonstrates the potential of EMOPIA for being used in future\nexploration on piano emotion-related MIR tasks.", "published": "2021-08-03 08:59:26", "link": "http://arxiv.org/abs/2108.01374v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Music Performance Assessment with Contrastive Learning", "abstract": "Several automatic approaches for objective music performance assessment (MPA)\nhave been proposed in the past, however, existing systems are not yet capable\nof reliably predicting ratings with the same accuracy as professional judges.\nThis study investigates contrastive learning as a potential method to improve\nexisting MPA systems. Contrastive learning is a widely used technique in\nrepresentation learning to learn a structured latent space capable of\nseparately clustering multiple classes. It has been shown to produce state of\nthe art results for image-based classification problems. We introduce a\nweighted contrastive loss suitable for regression tasks applied to a\nconvolutional neural network and show that contrastive loss results in\nperformance gains in regression tasks for MPA. Our results show that\ncontrastive-based methods are able to match and exceed SoTA performance for MPA\nregression tasks by creating better class clusters within the latent space of\nthe neural networks.", "published": "2021-08-03 19:24:25", "link": "http://arxiv.org/abs/2108.01711v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Empirical Evaluation of End-to-End Polyphonic Optical Music\n  Recognition", "abstract": "Previous work has shown that neural architectures are able to perform optical\nmusic recognition (OMR) on monophonic and homophonic music with high accuracy.\nHowever, piano and orchestral scores frequently exhibit polyphonic passages,\nwhich add a second dimension to the task. Monophonic and homophonic music can\nbe described as homorhythmic, or having a single musical rhythm. Polyphonic\nmusic, on the other hand, can be seen as having multiple rhythmic sequences, or\nvoices, concurrently. We first introduce a workflow for creating large-scale\npolyphonic datasets suitable for end-to-end recognition from sheet music\npublicly available on the MuseScore forum. We then propose two novel\nformulations for end-to-end polyphonic OMR -- one treating the problem as a\ntype of multi-task binary classification, and the other treating it as\nmulti-sequence detection. Building upon the encoder-decoder architecture and an\nimage encoder proposed in past work on end-to-end OMR, we propose two novel\ndecoder models -- FlagDecoder and RNNDecoder -- that correspond to the two\nformulations. Finally, we compare the empirical performance of these end-to-end\napproaches to polyphonic OMR and observe a new state-of-the-art performance\nwith our multi-sequence detection decoder, RNNDecoder.", "published": "2021-08-03 22:04:40", "link": "http://arxiv.org/abs/2108.01769v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
