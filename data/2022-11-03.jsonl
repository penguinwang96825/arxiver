{"title": "Continual Learning of Neural Machine Translation within Low Forgetting\n  Risk Regions", "abstract": "This paper considers continual learning of large-scale pretrained neural\nmachine translation model without accessing the previous training data or\nintroducing model separation. We argue that the widely used\nregularization-based methods, which perform multi-objective learning with an\nauxiliary loss, suffer from the misestimate problem and cannot always achieve a\ngood balance between the previous and new tasks. To solve the problem, we\npropose a two-stage training method based on the local features of the real\nloss. We first search low forgetting risk regions, where the model can retain\nthe performance on the previous task as the parameters are updated, to avoid\nthe catastrophic forgetting problem. Then we can continually train the model\nwithin this region only with the new training data to fit the new task.\nSpecifically, we propose two methods to search the low forgetting risk regions,\nwhich are based on the curvature of loss and the impacts of the parameters on\nthe model output, respectively. We conduct experiments on domain adaptation and\nmore challenging language adaptation tasks, and the experimental results show\nthat our method can achieve significant improvements compared with several\nstrong baselines.", "published": "2022-11-03 01:21:10", "link": "http://arxiv.org/abs/2211.01542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales", "abstract": "Neural language models (LMs) have achieved impressive results on various\nlanguage-based reasoning tasks by utilizing latent knowledge encoded in their\nown pretrained parameters. To make this reasoning process more explicit, recent\nworks retrieve a rationalizing LM's internal knowledge by training or prompting\nit to generate free-text rationales, which can be used to guide task\npredictions made by either the same LM or a separate reasoning LM. However,\nrationalizing LMs require expensive rationale annotation and/or computation,\nwithout any assurance that their generated rationales improve LM task\nperformance or faithfully reflect LM decision-making. In this paper, we propose\nPINTO, an LM pipeline that rationalizes via prompt-based learning, and learns\nto faithfully reason over rationales via counterfactual regularization. First,\nPINTO maps out a suitable reasoning process for the task input by prompting a\nfrozen rationalizing LM to generate a free-text rationale. Second, PINTO's\nreasoning LM is fine-tuned to solve the task using the generated rationale as\ncontext, while regularized to output less confident predictions when the\nrationale is perturbed. Across four datasets, we show that PINTO significantly\nimproves the generalization ability of the reasoning LM, yielding higher\nperformance on both in-distribution and out-of-distribution test sets. Also, we\nfind that PINTO's rationales are more faithful to its task predictions than\nthose generated by competitive baselines.", "published": "2022-11-03 02:55:54", "link": "http://arxiv.org/abs/2211.01562v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Vocabulary Argument Role Prediction for Event Extraction", "abstract": "The argument role in event extraction refers to the relation between an event\nand an argument participating in it. Despite the great progress in event\nextraction, existing studies still depend on roles pre-defined by domain\nexperts. These studies expose obvious weakness when extending to emerging event\ntypes or new domains without available roles. Therefore, more attention and\neffort needs to be devoted to automatically customizing argument roles. In this\npaper, we define this essential but under-explored task: open-vocabulary\nargument role prediction. The goal of this task is to infer a set of argument\nroles for a given event type. We propose a novel unsupervised framework,\nRolePred for this task. Specifically, we formulate the role prediction problem\nas an in-filling task and construct prompts for a pre-trained language model to\ngenerate candidate roles. By extracting and analyzing the candidate arguments,\nthe event-specific roles are further merged and selected. To standardize the\nresearch of this task, we collect a new event extraction dataset from\nWikiPpedia including 142 customized argument roles with rich semantics. On this\ndataset, RolePred outperforms the existing methods by a large margin. Source\ncode and dataset are available on our GitHub repository:\nhttps://github.com/yzjiao/RolePred", "published": "2022-11-03 04:13:37", "link": "http://arxiv.org/abs/2211.01577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised\n  Knowledge-Grounded Conversation", "abstract": "Recent advances in large-scale pre-training provide large models with the\npotential to learn knowledge from the raw text. It is thus natural to ask\nwhether it is possible to leverage these large models as knowledge bases for\ndownstream tasks. In this work, we answer the aforementioned question in\nunsupervised knowledge-grounded conversation. We explore various methods that\nbest elicit knowledge from large models. Our human study indicates that, though\nhallucinations exist, large models post the unique advantage of being able to\noutput common sense and summarize facts that cannot be directly retrieved from\nthe search engine. To better exploit such generated knowledge in dialogue\ngeneration, we treat the generated knowledge as a noisy knowledge source and\npropose the posterior-based reweighing as well as the noisy training strategy.\nEmpirical results on two benchmarks show advantages over the state-of-the-art\nmethods.", "published": "2022-11-03 04:48:38", "link": "http://arxiv.org/abs/2211.01587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-efficient End-to-end Information Extraction for Statistical Legal\n  Analysis", "abstract": "Legal practitioners often face a vast amount of documents. Lawyers, for\ninstance, search for appropriate precedents favorable to their clients, while\nthe number of legal precedents is ever-growing. Although legal search engines\ncan assist finding individual target documents and narrowing down the number of\ncandidates, retrieved information is often presented as unstructured text and\nusers have to examine each document thoroughly which could lead to information\noverloading. This also makes their statistical analysis challenging. Here, we\npresent an end-to-end information extraction (IE) system for legal documents.\nBy formulating IE as a generation task, our system can be easily applied to\nvarious tasks without domain-specific engineering effort. The experimental\nresults of four IE tasks on Korean precedents shows that our IE system can\nachieve competent scores (-2.3 on average) compared to the rule-based baseline\nwith as few as 50 training examples per task and higher score (+5.4 on average)\nwith 200 examples. Finally, our statistical analysis on two case\ncategories--drunk driving and fraud--with 35k precedents reveals the resulting\nstructured information from our IE system faithfully reflects the macroscopic\nfeatures of Korean legal system.", "published": "2022-11-03 10:27:37", "link": "http://arxiv.org/abs/2211.01692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A speech corpus for chronic kidney disease", "abstract": "In this study, we present a speech corpus of patients with chronic kidney\ndisease (CKD) that will be used for research on pathological voice analysis,\nautomatic illness identification, and severity prediction. This paper\nintroduces the steps involved in creating this corpus, including the choice of\nspeech-related parameters and speech lists as well as the recording technique.\nThe speakers in this corpus, 289 CKD patients with varying degrees of severity\nwho were categorized based on estimated glomerular filtration rate (eGFR),\ndelivered sustained vowels, sentence, and paragraph stimuli. This study\ncompared and analyzed the voice characteristics of CKD patients with those of\nthe control group; the results revealed differences in voice quality,\nphoneme-level pronunciation, prosody, glottal source, and aerodynamic\nparameters.", "published": "2022-11-03 10:57:48", "link": "http://arxiv.org/abs/2211.01705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-based Instance Discrimination Network for Relational Triple\n  Extraction", "abstract": "Joint entity and relation extraction has been a core task in the field of\ninformation extraction. Recent approaches usually consider the extraction of\nrelational triples from a stereoscopic perspective, either learning a\nrelation-specific tagger or separate classifiers for each relation type.\nHowever, they still suffer from error propagation, relation redundancy and lack\nof high-level connections between triples. To address these issues, we propose\na novel query-based approach to construct instance-level representations for\nrelational triples. By metric-based comparison between query embeddings and\ntoken embeddings, we can extract all types of triples in one step, thus\neliminating the error propagation problem. In addition, we learn the\ninstance-level representation of relational triples via contrastive learning.\nIn this way, relational triples can not only enclose rich class-level semantics\nbut also access to high-order global connections. Experimental results show\nthat our proposed method achieves the state of the art on five widely used\nbenchmarks.", "published": "2022-11-03 13:34:56", "link": "http://arxiv.org/abs/2211.01797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human in the loop approaches in multi-modal conversational task guidance\n  system development", "abstract": "Development of task guidance systems for aiding humans in a situated task\nremains a challenging problem. The role of search (information retrieval) and\nconversational systems for task guidance has immense potential to help the task\nperformers achieve various goals. However, there are several technical\nchallenges that need to be addressed to deliver such conversational systems,\nwhere common supervised approaches fail to deliver the expected results in\nterms of overall performance, user experience and adaptation to realistic\nconditions. In this preliminary work we first highlight some of the challenges\ninvolved during the development of such systems. We then provide an overview of\nexisting datasets available and highlight their limitations. We finally develop\na model-in-the-loop wizard-of-oz based data collection tool and perform a pilot\nexperiment.", "published": "2022-11-03 14:05:30", "link": "http://arxiv.org/abs/2211.01824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Prompt Tuning for Text Summarization", "abstract": "Prompts with different control signals (e.g., length, keywords, etc.) can be\nused to control text summarization. When control signals are available, they\ncan control the properties of generated summaries and potentially improve\nsummarization quality (since more information are given). Unfortunately,\ncontrol signals are not already available during inference time. In this paper,\nwe propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which\nis a single model that can be applied in both controlled and uncontrolled\n(without control signals) modes. During training, Lotus learns latent prompt\nrepresentations from prompts with gold control signals using a contrastive\nlearning objective. Experiments show Lotus in uncontrolled mode consistently\nimproves upon strong (uncontrollable) summarization models across four\ndifferent summarization datasets. We also demonstrate generated summaries can\nbe controlled using prompts with user specified control tokens.", "published": "2022-11-03 14:18:48", "link": "http://arxiv.org/abs/2211.01837v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Circling Back to Recurrent Models of Language", "abstract": "Just because some purely recurrent models suffer from being hard to optimize\nand inefficient on today's hardware, they are not necessarily bad models of\nlanguage. We demonstrate this by the extent to which these models can still be\nimproved by a combination of a slightly better recurrent cell, architecture,\nobjective, as well as optimization. In the process, we establish a new state of\nthe art for language modelling on small datasets and on Enwik8 with dynamic\nevaluation.", "published": "2022-11-03 14:36:25", "link": "http://arxiv.org/abs/2211.01848v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual information integration for stance detection via\n  cross-attention", "abstract": "Stance detection deals with identifying an author's stance towards a target.\nMost existing stance detection models are limited because they do not consider\nrelevant contextual information which allows for inferring the stance\ncorrectly. Complementary context can be found in knowledge bases but\nintegrating the context into pretrained language models is non-trivial due to\nthe graph structure of standard knowledge bases. To overcome this, we explore\nan approach to integrate contextual information as text which allows for\nintegrating contextual information from heterogeneous sources, such as\nstructured knowledge sources and by prompting large language models. Our\napproach can outperform competitive baselines on a large and diverse stance\ndetection benchmark in a cross-target setup, i.e. for targets unseen during\ntraining. We demonstrate that it is more robust to noisy context and can\nregularize for unwanted correlations between labels and target-specific\nvocabulary. Finally, it is independent of the pretrained language model in use.", "published": "2022-11-03 15:04:29", "link": "http://arxiv.org/abs/2211.01874v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and\n  its Intensity", "abstract": "Prerecorded laughter accompanying dialog in comedy TV shows encourages the\naudience to laugh by clearly marking humorous moments in the show. We present\nan approach for automatically detecting humor in the Friends TV show using\nmultimodal data. Our model is capable of recognizing whether an utterance is\nhumorous or not and assess the intensity of it. We use the prerecorded laughter\nin the show as annotation as it marks humor and the length of the audience's\nlaughter tells us how funny a given joke is. We evaluate the model on episodes\nthe model has not been exposed to during the training phase. Our results show\nthat the model is capable of correctly detecting whether an utterance is\nhumorous 78% of the time and how long the audience's laughter reaction should\nlast with a mean absolute error of 600 milliseconds.", "published": "2022-11-03 15:22:21", "link": "http://arxiv.org/abs/2211.01889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inverse scaling can become U-shaped", "abstract": "Scaling up language models has been empirically shown to improve performance\non a wide range of downstream tasks. However, if we were to observe worse\nperformance as a function of scale (\"inverse scaling\") on certain tasks, this\nwould indicate that scaling can also encourage behaviors that are misaligned\nwith human preferences. The Inverse Scaling Prize (McKenzie et al. 2022)\nidentified eleven such inverse scaling tasks, evaluated on models of up to 280B\nparameters and up to 500 zettaFLOPs of training compute. This paper takes a\ncloser look at these inverse scaling tasks. We evaluate models of up to 540B\nparameters, trained on five times more compute than those evaluated in the\nInverse Scaling Prize. With this increased range of model sizes and training\ncompute, only four out of the eleven tasks remain inverse scaling. Six out of\nthe eleven tasks exhibit \"U-shaped scaling\", where performance decreases up to\na certain size, and then increases again up to the largest model evaluated (the\none remaining task displays positive scaling). In addition, we find that 1-shot\nexamples and chain-of-thought can help mitigate undesirable scaling patterns\neven further. U-shaped scaling suggests that the inverse scaling trend observed\nin McKenzie et al. (2022) may not continue to hold for larger models, which we\nattribute to the presence of distractor tasks that only sufficiently large\nmodels can avoid.", "published": "2022-11-03 17:26:44", "link": "http://arxiv.org/abs/2211.02011v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logographic Information Aids Learning Better Representations for Natural\n  Language Inference", "abstract": "Statistical language models conventionally implement representation learning\nbased on the contextual distribution of words or other formal units, whereas\nany information related to the logographic features of written text are often\nignored, assuming they should be retrieved relying on the cooccurence\nstatistics. On the other hand, as language models become larger and require\nmore data to learn reliable representations, such assumptions may start to fall\nback, especially under conditions of data sparsity. Many languages, including\nChinese and Vietnamese, use logographic writing systems where surface forms are\nrepresented as a visual organization of smaller graphemic units, which often\ncontain many semantic cues. In this paper, we present a novel study which\nexplores the benefits of providing language models with logographic information\nin learning better semantic representations. We test our hypothesis in the\nnatural language inference (NLI) task by evaluating the benefit of computing\nmulti-modal representations that combine contextual information with glyph\ninformation. Our evaluation results in six languages with different typology\nand writing systems suggest significant benefits of using multi-modal\nembeddings in languages with logograhic systems, especially for words with less\noccurence statistics.", "published": "2022-11-03 20:40:14", "link": "http://arxiv.org/abs/2211.02136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time-aware Prompting for Text Generation", "abstract": "In this paper, we study the effects of incorporating timestamps, such as\ndocument creation dates, into generation systems. Two types of time-aware\nprompts are investigated: (1) textual prompts that encode document timestamps\nin natural language sentences; and (2) linear prompts that convert timestamps\ninto continuous vectors. To explore extrapolation to future data points, we\nfurther introduce a new data-to-text generation dataset, TempWikiBio,\ncontaining more than 4 millions of chronologically ordered revisions of\nbiographical articles from English Wikipedia, each paired with structured\npersonal profiles. Through data-to-text generation on TempWikiBio, text-to-text\ngeneration on the content transfer dataset, and summarization on XSum, we show\nthat linear prompts on encoder and textual prompts improve the generation\nquality on all datasets. Despite having less performance drop when testing on\ndata drawn from a later time, linear prompts focus more on non-temporal\ninformation and are less sensitive to the given timestamps, according to human\nevaluations and sensitivity analyses. Meanwhile, textual prompts establish the\nassociation between the given timestamps and the output dates, yielding more\nfactual temporal information in the output.", "published": "2022-11-03 22:10:25", "link": "http://arxiv.org/abs/2211.02162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models via Epistemic Neural Networks", "abstract": "Language models often pre-train on large unsupervised text corpora, then\nfine-tune on additional task-specific data. However, typical fine-tuning\nschemes do not prioritize the examples that they tune on. We show that, if you\ncan prioritize informative training data, you can achieve better performance\nwhile using fewer labels. To do this we augment a language model with an\nepinet: a small additional network that helps to estimate model uncertainty and\nforms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that\ncan know what they don't know. Using an epinet to prioritize uncertain data, we\ncan fine-tune BERT on GLUE tasks to the same performance while using 2x less\ndata than training without prioritization. We also investigate performance in\nsynthetic neural network generative models designed to build understanding. In\neach setting, using an epinet outperforms heuristic active learning schemes.", "published": "2022-11-03 03:24:46", "link": "http://arxiv.org/abs/2211.01568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Spelling to Grammar: A New Framework for Chinese Grammatical Error\n  Correction", "abstract": "Chinese Grammatical Error Correction (CGEC) aims to generate a correct\nsentence from an erroneous sequence, where different kinds of errors are mixed.\nThis paper divides the CGEC task into two steps, namely spelling error\ncorrection and grammatical error correction. Specifically, we propose a novel\nzero-shot approach for spelling error correction, which is simple but\neffective, obtaining a high precision to avoid error accumulation of the\npipeline structure. To handle grammatical error correction, we design\npart-of-speech (POS) features and semantic class features to enhance the neural\nnetwork model, and propose an auxiliary task to predict the POS sequence of the\ntarget sentence. Our proposed framework achieves a 42.11 F0.5 score on CGEC\ndataset without using any synthetic data or data augmentation methods, which\noutperforms the previous state-of-the-art by a wide margin of 1.30 points.\nMoreover, our model produces meaningful POS representations that capture\ndifferent POS words and convey reasonable POS transition rules.", "published": "2022-11-03 07:30:09", "link": "http://arxiv.org/abs/2211.01625v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Grammatical Error Correction Evaluation and Beyond", "abstract": "Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore\nand BARTScore) have been widely used in several sentence generation tasks\n(e.g., machine translation and text summarization) due to their better\ncorrelation with human judgments over traditional overlap-based methods.\nAlthough PT-based methods have become the de facto standard for training\ngrammatical error correction (GEC) systems, GEC evaluation still does not\nbenefit from pretrained knowledge. This paper takes the first step towards\nunderstanding and improving GEC evaluation with pretraining. We first find that\narbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory\ncorrelation results because of the excessive attention to inessential systems\noutputs (e.g., unchanged parts). To alleviate the limitation, we propose a\nnovel GEC evaluation metric to achieve the best of both worlds, namely PT-M2\nwhich only uses PT-based metrics to score those corrected parts. Experimental\nresults on the CoNLL14 evaluation task show that PT-M2 significantly\noutperforms existing methods, achieving a new state-of-the-art result of 0.949\nPearson correlation. Further analysis reveals that PT-M2 is robust to evaluate\ncompetitive GEC systems. Source code and scripts are freely available at\nhttps://github.com/pygongnlp/PT-M2.", "published": "2022-11-03 07:55:12", "link": "http://arxiv.org/abs/2211.01635v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Chinese Word Segmentation and Span-based Constituency Parsing", "abstract": "In constituency parsing, span-based decoding is an important direction.\nHowever, for Chinese sentences, because of their linguistic characteristics, it\nis necessary to utilize other models to perform word segmentation first, which\nintroduces a series of uncertainties and generally leads to errors in the\ncomputation of the constituency tree afterward. This work proposes a method for\njoint Chinese word segmentation and Span-based Constituency Parsing by adding\nextra labels to individual Chinese characters on the parse trees. Through\nexperiments, the proposed algorithm outperforms the recent models for joint\nsegmentation and constituency parsing on CTB 5.1.", "published": "2022-11-03 08:19:00", "link": "http://arxiv.org/abs/2211.01638v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing\n  Subnetworks Adaptively", "abstract": "Large-scale pre-trained language models have achieved impressive results on a\nwide range of downstream tasks recently. However, fine-tuning an extremely\nlarge-scale pre-trained language model on limited target datasets is often\nplagued by overfitting and representation degradation. In this paper, we\npropose a Dynamic Parameter Selection (DPS) algorithm for the large-scale\npre-trained models during fine-tuning, which adaptively selects a more\npromising subnetwork to perform staging updates based on gradients of\nback-propagation. Experiments on the GLUE benchmark show that DPS outperforms\nprevious fine-tuning methods in terms of overall performance and stability, and\nconsistently achieves better results with variable pre-trained language models.\nIn addition, DPS brings a large magnitude of improvement in out-of-domain\ntransferring experiments and low-resource scenarios, which shows that it can\nmaintain stable general contextual features and reduce the representation\ncollapse. We release our code at https://github.com/ZhangHaojie077/DPS", "published": "2022-11-03 08:32:12", "link": "http://arxiv.org/abs/2211.01642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Video Event Extraction via Tracking Visual States of Arguments", "abstract": "Video event extraction aims to detect salient events from a video and\nidentify the arguments for each event as well as their semantic roles. Existing\nmethods focus on capturing the overall visual scene of each frame, ignoring\nfine-grained argument-level information. Inspired by the definition of events\nas changes of states, we propose a novel framework to detect video events by\ntracking the changes in the visual states of all involved arguments, which are\nexpected to provide the most informative evidence for the extraction of video\nevents. In order to capture the visual state changes of arguments, we decompose\nthem into changes in pixels within objects, displacements of objects, and\ninteractions among multiple arguments. We further propose Object State\nEmbedding, Object Motion-aware Embedding and Argument Interaction Embedding to\nencode and track these changes respectively. Experiments on various video event\nextraction tasks demonstrate significant improvements compared to\nstate-of-the-art models. In particular, on verb classification, we achieve\n3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation\nRecognition.", "published": "2022-11-03 13:12:49", "link": "http://arxiv.org/abs/2211.01781v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Zero-shot Video Moment Retrieval With Off-the-Shelf Models", "abstract": "For the majority of the machine learning community, the expensive nature of\ncollecting high-quality human-annotated data and the inability to efficiently\nfinetune very large state-of-the-art pretrained models on limited compute are\nmajor bottlenecks for building models for new tasks. We propose a zero-shot\nsimple approach for one such task, Video Moment Retrieval (VMR), that does not\nperform any additional finetuning and simply repurposes off-the-shelf models\ntrained on other tasks. Our three-step approach consists of moment proposal,\nmoment-query matching and postprocessing, all using only off-the-shelf models.\nOn the QVHighlights benchmark for VMR, we vastly improve performance of\nprevious zero-shot approaches by at least 2.5x on all metrics and reduce the\ngap between zero-shot and state-of-the-art supervised by over 74%. Further, we\nalso show that our zero-shot approach beats non-pretrained supervised models on\nthe Recall metrics and comes very close on mAP metrics; and that it also\nperforms better than the best pretrained supervised model on shorter moments.\nFinally, we ablate and analyze our results and propose interesting future\ndirections.", "published": "2022-11-03 23:11:04", "link": "http://arxiv.org/abs/2211.02178v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Using Large Pre-Trained Language Model to Assist FDA in Premarket\n  Medical Device", "abstract": "This paper proposes a possible method using natural language processing that\nmight assist in the FDA medical device marketing process. Actual device\ndescriptions are taken and matched with the device description in FDA Title 21\nof CFR to determine their corresponding device type. Both pre-trained word\nembeddings such as FastText and large pre-trained sentence embedding models\nsuch as sentence transformers are evaluated on their accuracy in characterizing\na piece of device description. An experiment is also done to test whether these\nmodels can identify the devices wrongly classified in the FDA database. The\nresult shows that sentence transformer with T5 and MPNet and GPT-3 semantic\nsearch embedding show high accuracy in identifying the correct classification\nby narrowing down the correct label to be contained in the first 15 most likely\nresults, as compared to 2585 types of device descriptions that must be manually\nsearched through. On the other hand, all methods demonstrate high accuracy in\nidentifying completely incorrectly labeled devices, but all fail to identify\nfalse device classifications that are wrong but closely related to the true\nlabel.", "published": "2022-11-03 04:18:05", "link": "http://arxiv.org/abs/2212.01217v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spam Review Detection Using Deep Learning", "abstract": "A robust and reliable system of detecting spam reviews is a crying need in\ntodays world in order to purchase products without being cheated from online\nsites. In many online sites, there are options for posting reviews, and thus\ncreating scopes for fake paid reviews or untruthful reviews. These concocted\nreviews can mislead the general public and put them in a perplexity whether to\nbelieve the review or not. Prominent machine learning techniques have been\nintroduced to solve the problem of spam review detection. The majority of\ncurrent research has concentrated on supervised learning methods, which require\nlabeled data - an inadequacy when it comes to online review. Our focus in this\narticle is to detect any deceptive text reviews. In order to achieve that we\nhave worked with both labeled and unlabeled data and proposed deep learning\nmethods for spam review detection which includes Multi-Layer Perceptron (MLP),\nConvolutional Neural Network (CNN) and a variant of Recurrent Neural Network\n(RNN) that is Long Short-Term Memory (LSTM). We have also applied some\ntraditional machine learning classifiers such as Nave Bayes (NB), K Nearest\nNeighbor (KNN) and Support Vector Machine (SVM) to detect spam reviews and\nfinally, we have shown the performance comparison for both traditional and deep\nlearning classifiers.", "published": "2022-11-03 09:41:48", "link": "http://arxiv.org/abs/2211.01675v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "H_eval: A new hybrid evaluation metric for automatic speech recognition\n  tasks", "abstract": "Many studies have examined the shortcomings of word error rate (WER) as an\nevaluation metric for automatic speech recognition (ASR) systems. Since WER\nconsiders only literal word-level correctness, new evaluation metrics based on\nsemantic similarity such as semantic distance (SD) and BERTScore have been\ndeveloped. However, we found that these metrics have their own limitations,\nsuch as a tendency to overly prioritise keywords. We propose H_eval, a new\nhybrid evaluation metric for ASR systems that considers both semantic\ncorrectness and error rate and performs significantly well in scenarios where\nWER and SD perform poorly. Due to lighter computation compared to BERTScore, it\noffers 49 times reduction in metric computation time. Furthermore, we show that\nH_eval correlates strongly with downstream NLP tasks. Also, to reduce the\nmetric calculation time, we built multiple fast and lightweight models using\ndistillation techniques", "published": "2022-11-03 11:23:36", "link": "http://arxiv.org/abs/2211.01722v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transformers on Multilingual Clause-Level Morphology", "abstract": "This paper describes our winning systems in MRL: The 1st Shared Task on\nMultilingual Clause-level Morphology (EMNLP 2022 Workshop) designed by KUIS AI\nNLP team. We present our work for all three parts of the shared task:\ninflection, reinflection, and analysis. We mainly explore transformers with two\napproaches: (i) training models from scratch in combination with data\naugmentation, and (ii) transfer learning with prefix-tuning at multilingual\nmorphological tasks. Data augmentation significantly improves performance for\nmost languages in the inflection and reinflection tasks. On the other hand,\nPrefix-tuning on a pre-trained mGPT model helps us to adapt analysis tasks in\nlow-data and multilingual settings. While transformer architectures with data\naugmentation achieved the most promising results for inflection and\nreinflection tasks, prefix-tuning on mGPT received the highest results for the\nanalysis task. Our systems received 1st place in all three tasks in MRL 2022.", "published": "2022-11-03 11:53:39", "link": "http://arxiv.org/abs/2211.01736v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crosslingual Generalization through Multitask Finetuning", "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.", "published": "2022-11-03 13:19:32", "link": "http://arxiv.org/abs/2211.01786v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Human-Level Prompt Engineers", "abstract": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.", "published": "2022-11-03 15:43:03", "link": "http://arxiv.org/abs/2211.01910v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Probing Statistical Representations For End-To-End ASR", "abstract": "End-to-End automatic speech recognition (ASR) models aim to learn a\ngeneralised speech representation to perform recognition. In this domain there\nis little research to analyse internal representation dependencies and their\nrelationship to modelling approaches. This paper investigates cross-domain\nlanguage model dependencies within transformer architectures using SVCCA and\nuses these insights to exploit modelling approaches. It was found that specific\nneural representations within the transformer layers exhibit correlated\nbehaviour which impacts recognition performance.\n  Altogether, this work provides analysis of the modelling approaches affecting\ncontextual dependencies and ASR performance, and can be used to create or adapt\nbetter performing End-to-End ASR models and also for downstream tasks.", "published": "2022-11-03 17:08:14", "link": "http://arxiv.org/abs/2211.01993v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "lilGym: Natural Language Visual Reasoning with Reinforcement Learning", "abstract": "We present lilGym, a new benchmark for language-conditioned reinforcement\nlearning in visual environments. lilGym is based on 2,661 highly-compositional\nhuman-written natural language statements grounded in an interactive visual\nenvironment. We introduce a new approach for exact reward computation in every\npossible world state by annotating all statements with executable Python\nprograms. Each statement is paired with multiple start states and reward\nfunctions to form thousands of distinct Markov Decision Processes of varying\ndifficulty. We experiment with lilGym with different models and learning\nregimes. Our results and analysis show that while existing methods are able to\nachieve non-trivial performance, lilGym forms a challenging open problem.\nlilGym is available at https://lil.nlp.cornell.edu/lilgym/.", "published": "2022-11-03 17:08:26", "link": "http://arxiv.org/abs/2211.01994v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dynamic Kernels and Channel Attention for Low Resource Speaker\n  Verification", "abstract": "State-of-the-art speaker verification frameworks have typically focused on\ndeveloping models with increasingly deeper (more layers) and wider (number of\nchannels) models to improve their verification performance. Instead, this paper\nproposes an approach to increase the model resolution capability using\nattention-based dynamic kernels in a convolutional neural network to adapt the\nmodel parameters to be feature-conditioned. The attention weights on the\nkernels are further distilled by channel attention and multi-layer feature\naggregation to learn global features from speech. This approach provides an\nefficient solution to improving representation capacity with lower data\nresources. This is due to the self-adaptation to inputs of the structures of\nthe model parameters. The proposed dynamic convolutional model achieved 1.62\\%\nEER and 0.18 miniDCF on the VoxCeleb1 test set and has a 17\\% relative\nimprovement compared to the ECAPA-TDNN using the same training resources.", "published": "2022-11-03 17:13:28", "link": "http://arxiv.org/abs/2211.02000v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LMentry: A Language Model Benchmark of Elementary Language Tasks", "abstract": "As the performance of large language models rapidly improves, benchmarks are\ngetting larger and more complex as well. We present LMentry, a benchmark that\navoids this \"arms race\" by focusing on a compact set of tasks that are trivial\nto humans, e.g. writing a sentence containing a specific word, identifying\nwhich words in a list belong to a specific category, or choosing which of two\nwords is longer. LMentry is specifically designed to provide quick and\ninterpretable insights into the capabilities and robustness of large language\nmodels. Our experiments reveal a wide variety of failure cases that, while\nimmediately obvious to humans, pose a considerable challenge for large language\nmodels, including OpenAI's latest 175B-parameter instruction-tuned model,\nTextDavinci002. LMentry complements contemporary evaluation approaches of large\nlanguage models, providing a quick, automatic, and easy-to-run \"unit test\",\nwithout resorting to large benchmark suites of complex tasks.", "published": "2022-11-03 18:01:12", "link": "http://arxiv.org/abs/2211.02069v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overcoming Barriers to Skill Injection in Language Modeling: Case Study\n  in Arithmetic", "abstract": "Through their transfer learning abilities, highly-parameterized large\npre-trained language models have dominated the NLP landscape for a multitude of\ndownstream language tasks. Though linguistically proficient, the inability of\nthese models to incorporate the learning of non-linguistic entities (numerals\nand arithmetic reasoning) limits their usage for tasks that require numeric\ncomprehension or strict mathematical reasoning. However, as we illustrate in\nthis paper, building a general purpose language model that also happens to be\nproficient in mathematical reasoning is not as straight-forward as training it\non a numeric dataset. In this work, we develop a novel framework that enables\nlanguage models to be mathematically proficient while retaining their\nlinguistic prowess. Specifically, we offer information-theoretic interventions\nto overcome the catastrophic forgetting of linguistic skills that occurs while\ninjecting non-linguistic skills into language models.", "published": "2022-11-03 18:53:30", "link": "http://arxiv.org/abs/2211.02098v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DetAIL : A Tool to Automatically Detect and Analyze Drift In Language", "abstract": "Machine learning and deep learning-based decision making has become part of\ntoday's software. The goal of this work is to ensure that machine learning and\ndeep learning-based systems are as trusted as traditional software. Traditional\nsoftware is made dependable by following rigorous practice like static\nanalysis, testing, debugging, verifying, and repairing throughout the\ndevelopment and maintenance life-cycle. Similarly for machine learning systems,\nwe need to keep these models up to date so that their performance is not\ncompromised. For this, current systems rely on scheduled re-training of these\nmodels as new data kicks in. In this work, we propose to measure the data drift\nthat takes place when new data kicks in so that one can adaptively re-train the\nmodels whenever re-training is actually required irrespective of schedules. In\naddition to that, we generate various explanations at sentence level and\ndataset level to capture why a given payload text has drifted.", "published": "2022-11-03 19:50:12", "link": "http://arxiv.org/abs/2211.04250v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Phonetic-assisted Multi-Target Units Modeling for Improving\n  Conformer-Transducer ASR system", "abstract": "Exploiting effective target modeling units is very important and has always\nbeen a concern in end-to-end automatic speech recognition (ASR). In this work,\nwe propose a phonetic-assisted multi target units (PMU) modeling approach, to\nenhance the Conformer-Transducer ASR system in a progressive representation\nlearning manner. Specifically, PMU first uses the pronunciation-assisted\nsubword modeling (PASM) and byte pair encoding (BPE) to produce\nphonetic-induced and text-induced target units separately; Then, three new\nframeworks are investigated to enhance the acoustic encoder, including a basic\nPMU, a paraCTC and a pcaCTC, they integrate the PASM and BPE units at different\nlevels for CTC and transducer multi-task training. Experiments on both\nLibriSpeech and accented ASR tasks show that, the proposed PMU significantly\noutperforms the conventional BPE, it reduces the WER of LibriSpeech clean,\nother, and six accented ASR testsets by relative 12.7%, 6.0% and 7.7%,\nrespectively.", "published": "2022-11-03 03:36:51", "link": "http://arxiv.org/abs/2211.01571v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge\n  (ICSRC): Dataset, Tracks, Baseline and Results", "abstract": "This paper summarizes the outcomes from the ISCSLP 2022 Intelligent Cockpit\nSpeech Recognition Challenge (ICSRC). We first address the necessity of the\nchallenge and then introduce the associated dataset collected from a new-energy\nvehicle (NEV) covering a variety of cockpit acoustic conditions and linguistic\ncontents. We then describe the track arrangement and the baseline system.\nSpecifically, we set up two tracks in terms of allowed model/system size to\ninvestigate resource-constrained and -unconstrained setups, targeting to\nvehicle embedded as well as cloud ASR systems respectively. Finally we\nsummarize the challenge results and provide the major observations from the\nsubmitted systems.", "published": "2022-11-03 04:45:28", "link": "http://arxiv.org/abs/2211.01585v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolution channel separation and frequency sub-bands aggregation for\n  music genre classification", "abstract": "In music, short-term features such as pitch and tempo constitute long-term\nsemantic features such as melody and narrative. A music genre classification\n(MGC) system should be able to analyze these features. In this research, we\npropose a novel framework that can extract and aggregate both short- and\nlong-term features hierarchically. Our framework is based on ECAPA-TDNN, where\nall the layers that extract short-term features are affected by the layers that\nextract long-term features because of the back-propagation training. To prevent\nthe distortion of short-term features, we devised the convolution channel\nseparation technique that separates short-term features from long-term feature\nextraction paths. To extract more diverse features from our framework, we\nincorporated the frequency sub-bands aggregation method, which divides the\ninput spectrogram along frequency bandwidths and processes each segment. We\nevaluated our framework using the Melon Playlist dataset which is a large-scale\ndataset containing 600 times more data than GTZAN which is a widely used\ndataset in MGC studies. As the result, our framework achieved 70.4% accuracy,\nwhich was improved by 16.9% compared to a conventional framework.", "published": "2022-11-03 06:03:39", "link": "http://arxiv.org/abs/2211.01599v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Data Augmentation Using VAE-GAN for Disordered Speech\n  Recognition", "abstract": "Automatic recognition of disordered speech remains a highly challenging task\nto date. The underlying neuro-motor conditions, often compounded with\nco-occurring physical disabilities, lead to the difficulty in collecting large\nquantities of impaired speech required for ASR system development. This paper\npresents novel variational auto-encoder generative adversarial network\n(VAE-GAN) based personalized disordered speech augmentation approaches that\nsimultaneously learn to encode, generate and discriminate synthesized impaired\nspeech. Separate latent features are derived to learn dysarthric speech\ncharacteristics and phoneme context representations. Self-supervised\npre-trained Wav2vec 2.0 embedding features are also incorporated. Experiments\nconducted on the UASpeech corpus suggest the proposed adversarial data\naugmentation approach consistently outperformed the baseline speed perturbation\nand non-VAE GAN augmentation methods with trained hybrid TDNN and End-to-end\nConformer systems. After LHUC speaker adaptation, the best system using VAE-GAN\nbased augmentation produced an overall WER of 27.78% on the UASpeech test set\nof 16 dysarthric speakers, and the lowest published WER of 57.31% on the subset\nof speakers with \"Very Low\" intelligibility.", "published": "2022-11-03 08:38:57", "link": "http://arxiv.org/abs/2211.01646v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-based emotion recognition with self-supervised models using\n  attentive channel-wise correlations and label smoothing", "abstract": "When recognizing emotions from speech, we encounter two common problems: how\nto optimally capture emotion-relevant information from the speech signal and\nhow to best quantify or categorize the noisy subjective emotion labels.\nSelf-supervised pre-trained representations can robustly capture information\nfrom speech enabling state-of-the-art results in many downstream tasks\nincluding emotion recognition. However, better ways of aggregating the\ninformation across time need to be considered as the relevant emotion\ninformation is likely to appear piecewise and not uniformly across the signal.\nFor the labels, we need to take into account that there is a substantial degree\nof noise that comes from the subjective human annotations. In this paper, we\npropose a novel approach to attentive pooling based on correlations between the\nrepresentations' coefficients combined with label smoothing, a method aiming to\nreduce the confidence of the classifier on the training labels. We evaluate our\nproposed approach on the benchmark dataset IEMOCAP, and demonstrate high\nperformance surpassing that in the literature. The code to reproduce the\nresults is available at github.com/skakouros/s3prl_attentive_correlation.", "published": "2022-11-03 12:37:59", "link": "http://arxiv.org/abs/2211.01756v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fearless Steps Challenge Phase-1 Evaluation Plan", "abstract": "The Fearless Steps Challenge 2019 Phase-1 (FSC-P1) is the inaugural Challenge\nof the Fearless Steps Initiative hosted by the Center for Robust Speech Systems\n(CRSS) at the University of Texas at Dallas. The goal of this Challenge is to\nevaluate the performance of state-of-the-art speech and language systems for\nlarge task-oriented teams with naturalistic audio in challenging environments.\nResearchers may select to participate in any single or multiple of these\nchallenge tasks. Researchers may also choose to employ the FEARLESS STEPS\ncorpus for other related speech applications. All participants are encouraged\nto submit their solutions and results for consideration in the ISCA\nINTERSPEECH-2019 special session.", "published": "2022-11-03 14:17:43", "link": "http://arxiv.org/abs/2211.02051v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Domain Features for Detecting Adversarial Attacks Against\n  Deep Speech Recognition in Noise", "abstract": "In recent years, significant progress has been made in deep model-based\nautomatic speech recognition (ASR), leading to its widespread deployment in the\nreal world. At the same time, adversarial attacks against deep ASR systems are\nhighly successful. Various methods have been proposed to defend ASR systems\nfrom these attacks. However, existing classification based methods focus on the\ndesign of deep learning models while lacking exploration of domain specific\nfeatures. This work leverages filter bank-based features to better capture the\ncharacteristics of attacks for improved detection. Furthermore, the paper\nanalyses the potentials of using speech and non-speech parts separately in\ndetecting adversarial attacks. In the end, considering adverse environments\nwhere ASR systems may be deployed, we study the impact of acoustic noise of\nvarious types and signal-to-noise ratios. Extensive experiments show that the\ninverse filter bank features generally perform better in both clean and noisy\nenvironments, the detection is effective using either speech or non-speech\npart, and the acoustic noise can largely degrade the detection performance.", "published": "2022-11-03 07:25:45", "link": "http://arxiv.org/abs/2211.01621v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel-Aware Pretraining of Joint Encoder-Decoder Self-Supervised Model\n  for Telephonic-Speech ASR", "abstract": "This paper proposes a novel technique to obtain better downstream ASR\nperformance from a joint encoder-decoder self-supervised model when trained\nwith speech pooled from two different channels (narrow and wide band). The\njoint encoder-decoder self-supervised model extends the HuBERT model with a\nTransformer decoder. HuBERT performs clustering of features and predicts the\nclass of every input frame. In simple pooling, which is our baseline, there is\nno way to identify the channel information. To incorporate channel information,\nwe have proposed non-overlapping cluster IDs for speech from different\nchannels. Our method gives a relative improvement of ~4% over the joint\nencoder-decoder self-supervised model built with simple pooling of data, which\nserves as our baseline.", "published": "2022-11-03 09:27:16", "link": "http://arxiv.org/abs/2211.01669v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and\n  Envelope-based Features for Machinery Fault Detection", "abstract": "Acoustic-based fault detection has a high potential to monitor the health\ncondition of mechanical parts. However, the background noise of an industrial\nenvironment may negatively influence the performance of fault detection.\nLimited attention has been paid to improving the robustness of fault detection\nagainst industrial environmental noise. Therefore, we present the Lenze\nproduction background-noise (LPBN) real-world dataset and an automated and\nnoise-robust auditory inspection (ARAI) system for the end-of-line inspection\nof geared motors. An acoustic array is used to acquire data from motors with a\nminor fault, major fault, or which are healthy. A benchmark is provided to\ncompare the psychoacoustic features with different types of envelope features\nbased on expert knowledge of the gearbox. To the best of our knowledge, we are\nthe first to apply time-varying psychoacoustic features for fault detection. We\ntrain a state-of-the-art one-class-classifier, on samples from healthy motors\nand separate the faulty ones for fault detection using a threshold. The\nbest-performing approaches achieve an area under curve of 0.87 (logarithm\nenvelope), 0.86 (time-varying psychoacoustics), and 0.91 (combination of both).", "published": "2022-11-03 10:56:17", "link": "http://arxiv.org/abs/2211.01704v2", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Discussion of Features for Acoustic Anomaly Detection under Industrial\n  Disturbing Noise in an End-of-Line Test of Geared Motors", "abstract": "In the end-of-line test of geared motors, the evaluation of product qual-ity\nis important. Due to time constraints and the high diversity of variants,\nacous-tic measurements are more economical than vibration measurements.\nHowever, the acoustic data is affected by industrial disturbing noise.\nTherefore, the aim of this study is to investigate the robustness of features\nused for anomaly detection in geared motor end-of-line testing. A real-world\ndataset with typical faults and acoustic disturbances is recorded by an\nacoustic array. This includes industrial noise from the production and\nsystematically produced disturbances, used to compare the robustness. Overall,\nit is proposed to apply features extracted from a log-envelope spectrum\ntogether with psychoacoustic features. The anomaly de-tection is done by using\nthe isolation forest or the more universal bagging random miner. Most\ndisturbances can be circumvented, while the use of a hammer or air pressure\noften causes problems. In general, these results are important for condi-tion\nmonitoring tasks that are based on acoustic or vibration measurements.\nFur-thermore, a real-world problem description is presented to improve common\nsig-nal processing and machine learning tasks.", "published": "2022-11-03 11:08:41", "link": "http://arxiv.org/abs/2211.01716v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Iterative autoregression: a novel trick to improve your low-latency\n  speech enhancement model", "abstract": "Streaming models are an essential component of real-time speech enhancement\ntools. The streaming regime constrains speech enhancement models to use only a\ntiny context of future information. As a result, the low-latency streaming\nsetup is generally considered a challenging task and has a significant negative\nimpact on the model's quality. However, the sequential nature of streaming\ngeneration offers a natural possibility for autoregression, that is, utilizing\nprevious predictions while making current ones. The conventional method for\ntraining autoregressive models is teacher forcing, but its primary drawback\nlies in the training-inference mismatch that can lead to a substantial\ndegradation in quality. In this study, we propose a straightforward yet\neffective alternative technique for training autoregressive low-latency speech\nenhancement models. We demonstrate that the proposed approach leads to stable\nimprovement across diverse architectures and training scenarios.", "published": "2022-11-03 12:32:33", "link": "http://arxiv.org/abs/2211.01751v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Audio-Visual Speech Recognition with Alignment Regularization", "abstract": "In this work, we propose a streaming AV-ASR system based on a hybrid\nconnectionist temporal classification (CTC)/attention neural network\narchitecture. The audio and the visual encoder neural networks are both based\non the conformer architecture, which is made streamable using chunk-wise\nself-attention (CSA) and causal convolution. Streaming recognition with a\ndecoder neural network is realized by using the triggered attention technique,\nwhich performs time-synchronous decoding with joint CTC/attention scoring.\nAdditionally, we propose a novel alignment regularization technique that\npromotes synchronization of the audio and visual encoder, which in turn results\nin better word error rates (WERs) at all SNR levels for streaming and offline\nAV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the\nLip Reading Sentences 3 (LRS3) dataset in an offline and online setup,\nrespectively, which both present state-of-the-art results when no external\ntraining data are used.", "published": "2022-11-03 20:20:47", "link": "http://arxiv.org/abs/2211.02133v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HyperSound: Generating Implicit Neural Representations of Audio Signals\n  with Hypernetworks", "abstract": "Implicit neural representations (INRs) are a rapidly growing research field,\nwhich provides alternative ways to represent multimedia signals. Recent\napplications of INRs include image super-resolution, compression of\nhigh-dimensional signals, or 3D rendering. However, these solutions usually\nfocus on visual data, and adapting them to the audio domain is not trivial.\nMoreover, it requires a separately trained model for every data sample. To\naddress this limitation, we propose HyperSound, a meta-learning method\nleveraging hypernetworks to produce INRs for audio signals unseen at training\ntime. We show that our approach can reconstruct sound waves with quality\ncomparable to other state-of-the-art models.", "published": "2022-11-03 14:20:32", "link": "http://arxiv.org/abs/2211.01839v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MarginNCE: Robust Sound Localization with a Negative Margin", "abstract": "The goal of this work is to localize sound sources in visual scenes with a\nself-supervised approach. Contrastive learning in the context of sound source\nlocalization leverages the natural correspondence between audio and visual\nsignals where the audio-visual pairs from the same source are assumed as\npositive, while randomly selected pairs are negatives. However, this approach\nbrings in noisy correspondences; for example, positive audio and visual pair\nsignals that may be unrelated to each other, or negative pairs that may contain\nsemantically similar samples to the positive one. Our key contribution in this\nwork is to show that using a less strict decision boundary in contrastive\nlearning can alleviate the effect of noisy correspondences in sound source\nlocalization. We propose a simple yet effective approach by slightly modifying\nthe contrastive loss with a negative margin. Extensive experimental results\nshow that our approach gives on-par or better performance than the\nstate-of-the-art methods. Furthermore, we demonstrate that the introduction of\na negative margin to existing methods results in a consistent improvement in\nperformance.", "published": "2022-11-03 16:44:14", "link": "http://arxiv.org/abs/2211.01966v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
