{"title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "abstract": "As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).", "published": "2017-04-03 02:10:19", "link": "http://arxiv.org/abs/1704.00405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Lexical and Syntactic Features for Detecting Content-dense\n  Texts in News", "abstract": "Content-dense news report important factual information about an event in\ndirect, succinct manner. Information seeking applications such as information\nextraction, question answering and summarization normally assume all text they\ndeal with is content-dense. Here we empirically test this assumption on news\narticles from the business, U.S. international relations, sports and science\njournalism domains. Our findings clearly indicate that about half of the news\ntexts in our study are in fact not content-dense and motivate the development\nof a supervised content-density detector. We heuristically label a large\ntraining corpus for the task and train a two-layer classifying model based on\nlexical and unlexicalized syntactic features. On manually annotated data, we\ncompare the performance of domain-specific classifiers, trained on data only\nfrom a given news domain and a general classifier in which data from all four\ndomains is pooled together. Our annotation and prediction experiments\ndemonstrate that the concept of content density varies depending on the domain\nand that naive annotators provide judgement biased toward the stereotypical\ndomain label. Domain-specific classifiers are more accurate for domains in\nwhich content-dense texts are typically fewer. Domain independent classifiers\nreproduce better naive crowdsourced judgements. Classification prediction is\nhigh across all conditions, around 80%.", "published": "2017-04-03 06:22:04", "link": "http://arxiv.org/abs/1704.00440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Transition-Based Directed Acyclic Graph Parser for UCCA", "abstract": "We present the first parser for UCCA, a cross-linguistically applicable\nframework for semantic representation, which builds on extensive typological\nwork and supports rapid annotation. UCCA poses a challenge for existing parsing\ntechniques, as it exhibits reentrancy (resulting in DAG structures),\ndiscontinuous structures and non-terminal nodes corresponding to complex\nsemantic units. To our knowledge, the conjunction of these formal properties is\nnot supported by any existing parser. Our transition-based parser, which uses a\nnovel transition set and features based on bidirectional LSTMs, has value not\njust for UCCA parsing: its ability to handle more general graph structures can\ninform the development of parsers for other semantic DAG structures, and in\nlanguages that frequently use discontinuous structures.", "published": "2017-04-03 12:40:54", "link": "http://arxiv.org/abs/1704.00552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "abstract": "The input to a neural sequence-to-sequence model is often determined by an\nup-stream system, e.g. a word segmenter, part of speech tagger, or speech\nrecognizer. These up-stream models are potentially error-prone. Representing\ninputs through word lattices allows making this uncertainty explicit by\ncapturing alternative sequences and their posterior probabilities in a compact\nform. In this work, we extend the TreeLSTM (Tai et al., 2015) into a\nLatticeLSTM that is able to consume word lattices, and can be used as encoder\nin an attentional encoder-decoder model. We integrate lattice posterior scores\ninto this architecture by extending the TreeLSTM's child-sum and forget gates\nand introducing a bias term into the attention mechanism. We experiment with\nspeech translation lattices and report consistent improvements over baselines\nthat translate either the 1-best hypothesis or the lattice without posterior\nscores.", "published": "2017-04-03 13:03:40", "link": "http://arxiv.org/abs/1704.00559v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency\n  and Compositionality", "abstract": "Increasing the capacity of recurrent neural networks (RNN) usually involves\naugmenting the size of the hidden layer, with significant increase of\ncomputational cost. Recurrent neural tensor networks (RNTN) increase capacity\nusing distinct hidden layer weights for each word, but with greater costs in\nmemory usage. In this paper, we introduce restricted recurrent neural tensor\nnetworks (r-RNTN) which reserve distinct hidden layer weights for frequent\nvocabulary words while sharing a single set of weights for infrequent words.\nPerplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve\nlanguage model performance over RNNs using only a small fraction of the\nparameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated\nRecurrent Units and Long Short-Term Memory.", "published": "2017-04-03 19:17:58", "link": "http://arxiv.org/abs/1704.00774v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments", "abstract": "Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.", "published": "2017-04-03 19:45:27", "link": "http://arxiv.org/abs/1704.00784v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-Task Learning of Keyphrase Boundary Classification", "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases\nin scientific articles and labelling them with respect to predefined types.\nAlthough important in practice, this task is so far underexplored, partly due\nto the lack of labelled data. To overcome this, we explore several auxiliary\ntasks, including semantic super-sense tagging and identification of multi-word\nexpressions, and cast the task as a multi-task learning problem with deep\nrecurrent neural networks. Our multi-task models perform significantly better\nthan previous state of the art approaches on two scientific KBC datasets,\nparticularly for long keyphrases.", "published": "2017-04-03 10:25:22", "link": "http://arxiv.org/abs/1704.00514v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Detection and Resolution of Rumours in Social Media: A Survey", "abstract": "Despite the increasing use of social media platforms for information and news\ngathering, its unmoderated nature often leads to the emergence and spread of\nrumours, i.e. pieces of information that are unverified at the time of posting.\nAt the same time, the openness of social media platforms provides opportunities\nto study how users share and discuss rumours, and to explore how natural\nlanguage processing and data mining techniques may be used to find ways of\ndetermining their veracity. In this survey we introduce and discuss two types\nof rumours that circulate on social media; long-standing rumours that circulate\nfor long periods of time, and newly-emerging rumours spawned during fast-paced\nevents such as breaking news, where reports are released piecemeal and often\nwith an unverified status in their early stages. We provide an overview of\nresearch into social media rumours with the ultimate goal of developing a\nrumour classification system that consists of four components: rumour\ndetection, rumour tracking, rumour stance classification and rumour veracity\nclassification. We delve into the approaches presented in the scientific\nliterature for the development of each of these four components. We summarise\nthe efforts and achievements so far towards the development of rumour\nclassification systems and conclude with suggestions for avenues for future\nresearch in social media mining for detection and resolution of rumours.", "published": "2017-04-03 15:57:44", "link": "http://arxiv.org/abs/1704.00656v3", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "It Takes Two to Tango: Towards Theory of AI's Mind", "abstract": "Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one's own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds. In this work, we\nargue that for human-AI teams to be effective, humans must also develop a\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\nand quirks. We instantiate these ideas within the domain of Visual Question\nAnswering (VQA). We find that using just a few examples (50), lay people can be\ntrained to better predict responses and oncoming failures of a complex VQA\nmodel. We further evaluate the role existing explanation (or interpretability)\nmodalities play in helping humans build ToAIM. Explainable AI has received\nconsiderable scientific and popular attention in recent times. Surprisingly, we\nfind that having access to the model's internal states - its confidence in its\ntop-k predictions, explicit or implicit attention maps which highlight regions\nin the image (and words in the question) the model is looking at (and listening\nto) while answering a question about an image - do not help people better\npredict its behavior.", "published": "2017-04-03 17:58:07", "link": "http://arxiv.org/abs/1704.00717v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
