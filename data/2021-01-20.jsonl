{"title": "Towards Confident Machine Reading Comprehension", "abstract": "There has been considerable progress on academic benchmarks for the Reading\nComprehension (RC) task with State-of-the-Art models closing the gap with human\nperformance on extractive question answering. Datasets such as SQuAD 2.0 & NQ\nhave also introduced an auxiliary task requiring models to predict when a\nquestion has no answer in the text. However, in production settings, it is also\nnecessary to provide confidence estimates for the performance of the underlying\nRC model at both answer extraction and \"answerability\" detection. We propose a\nnovel post-prediction confidence estimation model, which we call Mr.C (short\nfor Mr. Confident), that can be trained to improve a system's ability to\nrefrain from making incorrect predictions with improvements of up to 4 points\nas measured by Area Under the Curve (AUC) scores. Mr.C can benefit from a novel\nwhite-box feature that leverages the underlying RC model's gradients.\nPerformance prediction is particularly important in cases of domain shift (as\nmeasured by training RC models on SQUAD 2.0 and evaluating on NQ), where Mr.C\nnot only improves AUC, but also traditional answerability prediction (as\nmeasured by a 5 point improvement in F1).", "published": "2021-01-20 03:02:12", "link": "http://arxiv.org/abs/2101.07942v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Conquer: An Ensemble Approach for Hostile Post Detection in\n  Hindi", "abstract": "Recently the NLP community has started showing interest towards the\nchallenging task of Hostile Post Detection. This paper present our system for\nShared Task at Constraint2021 on \"Hostile Post Detection in Hindi\". The data\nfor this shared task is provided in Hindi Devanagari script which was collected\nfrom Twitter and Facebook. It is a multi-label multi-class classification\nproblem where each data instance is annotated into one or more of the five\nclasses: fake, hate, offensive, defamation, and non-hostile. We propose a two\nlevel architecture which is made up of BERT based classifiers and statistical\nclassifiers to solve this problem. Our team 'Albatross', scored 0.9709 Coarse\ngrained hostility F1 score measure on Hostile Post Detection in Hindi subtask\nand secured 2nd rank out of 45 teams for the task. Our submission is ranked 2nd\nand 3rd out of a total of 156 submissions with Coarse grained hostility F1\nscore of 0.9709 and 0.9703 respectively. Our fine grained scores are also very\nencouraging and can be improved with further finetuning. The code is publicly\navailable.", "published": "2021-01-20 05:38:07", "link": "http://arxiv.org/abs/2101.07973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Challenges of Persian User-generated Textual Content: A Machine\n  Learning-Based Approach", "abstract": "Over recent years a lot of research papers and studies have been published on\nthe development of effective approaches that benefit from a large amount of\nuser-generated content and build intelligent predictive models on top of them.\nThis research applies machine learning-based approaches to tackle the hurdles\nthat come with Persian user-generated textual content. Unfortunately, there is\nstill inadequate research in exploiting machine learning approaches to\nclassify/cluster Persian text. Further, analyzing Persian text suffers from a\nlack of resources; specifically from datasets and text manipulation tools.\nSince the syntax and semantics of the Persian language is different from\nEnglish and other languages, the available resources from these languages are\nnot instantly usable for Persian. In addition, recognition of nouns and\npronouns, parts of speech tagging, finding words' boundary, stemming or\ncharacter manipulations for Persian language are still unsolved issues that\nrequire further studying. Therefore, efforts have been made in this research to\naddress some of the challenges. This presented approach uses a\nmachine-translated datasets to conduct sentiment analysis for the Persian\nlanguage. Finally, the dataset has been rehearsed with different classifiers\nand feature engineering approaches. The results of the experiments have shown\npromising state-of-the-art performance in contrast to the previous efforts; the\nbest classifier was Support Vector Machines which achieved a precision of\n91.22%, recall of 91.71%, and F1 score of 91.46%.", "published": "2021-01-20 11:57:59", "link": "http://arxiv.org/abs/2101.08087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A survey of joint intent detection and slot-filling models in natural\n  language understanding", "abstract": "Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks have been deemed to proceed\nindependently. However, more recently, joint models for intent classification\nand slot filling have achieved state-of-the-art performance, and have proved\nthat there exists a strong relationship between the two tasks. This article is\na compilation of past work in natural language understanding, especially joint\nintent classification and slot filling. We observe three milestones in this\nresearch so far: Intent detection to identify the speaker's intention, slot\nfilling to label each word token in the speech/text, and finally, joint intent\nclassification and slot filling tasks. In this article, we describe trends,\napproaches, issues, data sets, evaluation metrics in intent classification and\nslot filling. We also discuss representative performance values, describe\nshared tasks, and provide pointers to future work, as given in prior works. To\ninterpret the state-of-the-art trends, we provide multiple tables that describe\nand summarise past research along different dimensions, including the types of\nfeatures, base approaches, and dataset domain used.", "published": "2021-01-20 12:15:11", "link": "http://arxiv.org/abs/2101.08091v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation", "abstract": "Despite pre-trained language models such as BERT have achieved appealing\nperformance in a wide range of natural language processing tasks, they are\ncomputationally expensive to be deployed in real-time applications. A typical\nmethod is to adopt knowledge distillation to compress these large pre-trained\nmodels (teacher models) to small student models. However, for a target domain\nwith scarce training data, the teacher can hardly pass useful knowledge to the\nstudent, which yields performance degradation for the student models. To tackle\nthis problem, we propose a method to learn to augment for data-scarce domain\nBERT knowledge distillation, by learning a cross-domain manipulation scheme\nthat automatically augments the target with the help of resource-rich source\ndomains. Specifically, the proposed method generates samples acquired from a\nstationary distribution near the target data and adopts a reinforced selector\nto automatically refine the augmentation strategy according to the performance\nof the student. Extensive experiments demonstrate that the proposed method\nsignificantly outperforms state-of-the-art baselines on four different tasks,\nand for the data-scarce domains, the compressed student models even perform\nbetter than the original large teacher model, with much fewer parameters (only\n${\\sim}13.3\\%$) when only a few labeled examples available.", "published": "2021-01-20 13:07:39", "link": "http://arxiv.org/abs/2101.08106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Sequence Tagging with Deep Pre-trained Models and\n  Bayesian Uncertainty Estimates", "abstract": "Annotating training data for sequence tagging of texts is usually very\ntime-consuming. Recent advances in transfer learning for natural language\nprocessing in conjunction with active learning open the possibility to\nsignificantly reduce the necessary annotation budget. We are the first to\nthoroughly investigate this powerful combination for the sequence tagging task.\nWe conduct an extensive empirical study of various Bayesian uncertainty\nestimation methods and Monte Carlo dropout options for deep pre-trained models\nin the active learning framework and find the best combinations for different\ntypes of models. Besides, we also demonstrate that to acquire instances during\nactive learning, a full-size Transformer can be substituted with a distilled\nversion, which yields better computational performance and reduces obstacles\nfor applying deep active learning in practice.", "published": "2021-01-20 13:59:25", "link": "http://arxiv.org/abs/2101.08133v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Taxonomy Help? Improving Semantic Question Matching using Question\n  Taxonomy", "abstract": "In this paper, we propose a hybrid technique for semantic question matching.\nIt uses our proposed two-layered taxonomy for English questions by augmenting\nstate-of-the-art deep learning models with question classes obtained from a\ndeep learning based question classifier. Experiments performed on three\nopen-domain datasets demonstrate the effectiveness of our proposed approach. We\nachieve state-of-the-art results on partial ordering question ranking (POQR)\nbenchmark dataset. Our empirical analysis shows that coupling standard\ndistributional features (provided by the question encoder) with knowledge from\ntaxonomy is more effective than either deep learning (DL) or taxonomy-based\nknowledge alone.", "published": "2021-01-20 16:23:04", "link": "http://arxiv.org/abs/2101.08201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "abstract": "Word alignment over parallel corpora has a wide variety of applications,\nincluding learning translation lexicons, cross-lingual transfer of language\nprocessing tools, and automatic evaluation or analysis of translation outputs.\nThe great majority of past work on word alignment has worked by performing\nunsupervised learning on parallel texts. Recently, however, other work has\ndemonstrated that pre-trained contextualized word embeddings derived from\nmultilingually trained language models (LMs) prove an attractive alternative,\nachieving competitive results on the word alignment task even in the absence of\nexplicit training on parallel data. In this paper, we examine methods to marry\nthe two approaches: leveraging pre-trained LMs but fine-tuning them on parallel\ntext with objectives designed to improve alignment quality, and proposing\nmethods to effectively extract alignments from these fine-tuned models. We\nperform experiments on five language pairs and demonstrate that our model can\nconsistently outperform previous state-of-the-art models of all varieties. In\naddition, we demonstrate that we are able to train multilingual word aligners\nthat can obtain robust performance on different language pairs. Our aligner,\nAWESOME (Aligning Word Embedding Spaces of Multilingual Encoders), with\npre-trained models is available at https://github.com/neulab/awesome-align", "published": "2021-01-20 17:54:47", "link": "http://arxiv.org/abs/2101.08231v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Generalization in Dialog State Tracking through Generative\n  Question Answering", "abstract": "Dialog State Tracking (DST), an integral part of modern dialog systems, aims\nto track user preferences and constraints (slots) in task-oriented dialogs. In\nreal-world settings with constantly changing services, DST systems must\ngeneralize to new domains and unseen slot types. Existing methods for DST do\nnot generalize well to new slot names and many require known ontologies of slot\ntypes and values for inference. We introduce a novel ontology-free framework\nthat supports natural language queries for unseen constraints and slots in\nmulti-domain task-oriented dialogs. Our approach is based on generative\nquestion-answering using a conditional language model pre-trained on\nsubstantive English sentences. Our model improves joint goal accuracy in\nzero-shot domain adaptation settings by up to 9% (absolute) over the previous\nstate-of-the-art on the MultiWOZ 2.1 dataset.", "published": "2021-01-20 21:47:20", "link": "http://arxiv.org/abs/2101.08333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Full-text Content of Academic Articles to Build a Methodology\n  Taxonomy of Information Science in China", "abstract": "Research on the construction of traditional information science methodology\ntaxonomy is mostly conducted manually. From the limited corpus, researchers\nhave attempted to summarize some of the research methodology entities into\nseveral abstract levels (generally three levels); however, they have been\nunable to provide a more granular hierarchy. Moreover, updating the methodology\ntaxonomy is traditionally a slow process. In this study, we collected full-text\nacademic papers related to information science. First, we constructed a basic\nmethodology taxonomy with three levels by manual annotation. Then, the word\nvectors of the research methodology entities were trained using the full-text\ndata. Accordingly, the research methodology entities were clustered and the\nbasic methodology taxonomy was expanded using the clustering results to obtain\na methodology taxonomy with more levels. This study provides new concepts for\nconstructing a methodology taxonomy of information science. The proposed\nmethodology taxonomy is semi-automated; it is more detailed than conventional\nschemes and the speed of taxonomy renewal has been enhanced.", "published": "2021-01-20 01:56:43", "link": "http://arxiv.org/abs/2101.07924v1", "categories": ["cs.DL", "cs.CL", "I.2.7"], "primary_category": "cs.DL"}
{"title": "WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation\n  Track", "abstract": "We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara\net al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2\n(Interactive Dialogue). In sub-task 1, we employ a pre-trained language model\nto generate topic-related responses and propose a response ensemble method for\nresponse selection. In sub-task2, we propose a novel Dialogue Planning Model\n(DPM) to capture conversation flow in the interaction with humans. We also\ndesign an integrated open-domain dialogue system containing pre-process,\ndialogue model, scoring model, and post-process, which can generate fluent,\ncoherent, consistent, and humanlike responses. We tie 1st on human ratings and\nalso get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on\ninteractive human evaluation in sub-task 2.", "published": "2021-01-20 03:19:50", "link": "http://arxiv.org/abs/2101.07947v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HIVE-4-MAT: Advancing the Ontology Infrastructure for Materials Science", "abstract": "Introduces HIVE-4-MAT - Helping Interdisciplinary Vocabulary Engineering for\nMaterials Science, an automatic linked data ontology application. Covers\ncontextual background for materials science, shared ontology infrastructures,\nand reviews the knowledge extraction and indexing process. HIVE-4-MAT's\nvocabulary browsing, term search and selection, and knowledge extraction and\nindexing are reviewed, and plans to integrate named entity recognition.\nConclusion highlights next steps with relation extraction to support better\nontologies.", "published": "2021-01-20 04:40:09", "link": "http://arxiv.org/abs/2101.07960v1", "categories": ["cs.DL", "cs.CL", "I.7"], "primary_category": "cs.DL"}
{"title": "Data-to-text Generation by Splicing Together Nearest Neighbors", "abstract": "We propose to tackle data-to-text generation tasks by directly splicing\ntogether retrieved segments of text from \"neighbor\" source-target pairs. Unlike\nrecent work that conditions on retrieved neighbors but generates text\ntoken-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text, by inserting or replacing them in partially\nconstructed generations. Standard techniques for training such a policy require\nan oracle derivation for each generation, and we prove that finding the\nshortest such derivation can be reduced to parsing under a particular weighted\ncontext-free grammar. We find that policies learned in this way perform on par\nwith strong baselines in terms of automatic and human evaluation, but allow for\nmore interpretable and controllable generation.", "published": "2021-01-20 18:43:11", "link": "http://arxiv.org/abs/2101.08248v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploratory Arabic Offensive Language Dataset Analysis", "abstract": "This paper adding more insights towards resources and datasets used in Arabic\noffensive language research. The main goal of this paper is to guide\nresearchers in Arabic offensive language in selecting appropriate datasets\nbased on their content, and in creating new Arabic offensive language resources\nto support and complement the available ones.", "published": "2021-01-20 23:45:33", "link": "http://arxiv.org/abs/2101.11434v1", "categories": ["cs.CL", "cs.CY", "J.m"], "primary_category": "cs.CL"}
{"title": "Classifying Scientific Publications with BERT -- Is Self-Attention a\n  Feature Selection Method?", "abstract": "We investigate the self-attention mechanism of BERT in a fine-tuning scenario\nfor the classification of scientific articles over a taxonomy of research\ndisciplines. We observe how self-attention focuses on words that are highly\nrelated to the domain of the article. Particularly, a small subset of\nvocabulary words tends to receive most of the attention. We compare and\nevaluate the subset of the most attended words with feature selection methods\nnormally used for text classification in order to characterize self-attention\nas a possible feature selection approach. Using ConceptNet as ground truth, we\nalso find that attended words are more related to the research fields of the\narticles. However, conventional feature selection methods are still a better\noption to learn classifiers from scratch. This result suggests that, while\nself-attention identifies domain-relevant terms, the discriminatory information\nin BERT is encoded in the contextualized outputs and the classification layer.\nIt also raises the question whether injecting feature selection methods in the\nself-attention mechanism could further optimize single sequence classification\nusing transformers.", "published": "2021-01-20 13:22:26", "link": "http://arxiv.org/abs/2101.08114v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Open-Domain Conversational Search Assistant with Transformers", "abstract": "Open-domain conversational search assistants aim at answering user questions\nabout open topics in a conversational manner. In this paper we show how the\nTransformer architecture achieves state-of-the-art results in key IR tasks,\nleveraging the creation of conversational assistants that engage in open-domain\nconversational search with single, yet informative, answers. In particular, we\npropose an open-domain abstractive conversational search agent pipeline to\naddress two major challenges: first, conversation context-aware search and\nsecond, abstractive search-answers generation. To address the first challenge,\nthe conversation context is modeled with a query rewriting method that unfolds\nthe context of the conversation up to a specific moment to search for the\ncorrect answers. These answers are then passed to a Transformer-based re-ranker\nto further improve retrieval performance. The second challenge, is tackled with\nrecent Abstractive Transformer architectures to generate a digest of the top\nmost relevant passages. Experiments show that Transformers deliver a solid\nperformance across all tasks in conversational search, outperforming the best\nTREC CAsT 2019 baseline.", "published": "2021-01-20 16:02:15", "link": "http://arxiv.org/abs/2101.08197v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "VOTE400(Voide Of The Elderly 400 Hours): A Speech Dataset to Study Voice\n  Interface for Elderly-Care", "abstract": "This paper introduces a large-scale Korean speech dataset, called VOTE400,\nthat can be used for analyzing and recognizing voices of the elderly people.\nThe dataset includes about 300 hours of continuous dialog speech and 100 hours\nof read speech, both recorded by the elderly people aged 65 years or over. A\npreliminary experiment showed that speech recognition system trained with\nVOTE400 can outperform conventional systems in speech recognition of elderly\npeople's voice. This work is a multi-organizational effort led by ETRI and\nMINDs Lab Inc. for the purpose of advancing the speech recognition performance\nof the elderly-care robots.", "published": "2021-01-20 05:28:05", "link": "http://arxiv.org/abs/2101.11469v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Diagnosis of Asthma using Hilbert-Huang Transform and Deep Learning\n  on Lung Sounds", "abstract": "Lung auscultation is the most effective and indispensable method for\ndiagnosing various respiratory disorders by using the sounds from the airways\nduring inspirium and exhalation using a stethoscope. In this study, the\nstatistical features are calculated from intrinsic mode functions that are\nextracted by applying the HilbertHuang Transform to the lung sounds from 12\ndifferent auscultation regions on the chest and back. The classification of the\nlung sounds from asthma and healthy subjects is performed using Deep Belief\nNetworks (DBN). The DBN classifier model with two hidden layers has been tested\nusing 5-fold cross validation method. The proposed DBN separated lung sounds\nfrom asthmatic and healthy subjects with high classification performance rates\nof 84.61%, 85.83%, and 77.11% for overall accuracy, sensitivity, and\nselectivity, respectively using frequencytime analysis.", "published": "2021-01-20 19:04:33", "link": "http://arxiv.org/abs/2101.08288v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
