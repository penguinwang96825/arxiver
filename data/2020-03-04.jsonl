{"title": "Posterior-GAN: Towards Informative and Coherent Response Generation with\n  Posterior Generative Adversarial Network", "abstract": "Neural conversational models learn to generate responses by taking into\naccount the dialog history. These models are typically optimized over the\nquery-response pairs with a maximum likelihood estimation objective. However,\nthe query-response tuples are naturally loosely coupled, and there exist\nmultiple responses that can respond to a given query, which leads the\nconversational model learning burdensome. Besides, the general dull response\nproblem is even worsened when the model is confronted with meaningless response\ntraining instances. Intuitively, a high-quality response not only responds to\nthe given query but also links up to the future conversations, in this paper,\nwe leverage the query-response-future turn triples to induce the generated\nresponses that consider both the given context and the future conversations. To\nfacilitate the modeling of these triples, we further propose a novel\nencoder-decoder based generative adversarial learning framework, Posterior\nGenerative Adversarial Network (Posterior-GAN), which consists of a forward and\na backward generative discriminator to cooperatively encourage the generated\nresponse to be informative and coherent by two complementary assessment\nperspectives. Experimental results demonstrate that our method effectively\nboosts the informativeness and coherence of the generated response on both\nautomatic and human evaluation, which verifies the advantages of considering\ntwo assessment perspectives.", "published": "2020-03-04 11:57:53", "link": "http://arxiv.org/abs/2003.02020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Low-Resource Machine Translation between Chinese and\n  Vietnamese with Back-Translation", "abstract": "Back translation (BT) has been widely used and become one of standard\ntechniques for data augmentation in Neural Machine Translation (NMT), BT has\nproven to be helpful for improving the performance of translation effectively,\nespecially for low-resource scenarios. While most works related to BT mainly\nfocus on European languages, few of them study languages in other areas around\nthe world. In this paper, we investigate the impacts of BT on Asia language\ntranslations between the extremely low-resource Chinese and Vietnamese language\npair. We evaluate and compare the effects of different sizes of synthetic data\non both NMT and Statistical Machine Translation (SMT) models for Chinese to\nVietnamese and Vietnamese to Chinese, with character-based and word-based\nsettings. Some conclusions from previous works are partially confirmed and we\nalso draw some other interesting findings and conclusions, which are beneficial\nto understand BT further.", "published": "2020-03-04 17:10:10", "link": "http://arxiv.org/abs/2003.02197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Adversarial Domain Adaptation for Implicit Discourse\n  Relation Classification", "abstract": "Implicit discourse relations are not only more challenging to classify, but\nalso to annotate, than their explicit counterparts. We tackle situations where\ntraining data for implicit relations are lacking, and exploit domain adaptation\nfrom explicit relations (Ji et al., 2015). We present an unsupervised\nadversarial domain adaptive network equipped with a reconstruction component.\nOur system outperforms prior works and other adversarial benchmarks for\nunsupervised domain adaptation. Additionally, we extend our system to take\nadvantage of labeled data if some are available.", "published": "2020-03-04 18:31:32", "link": "http://arxiv.org/abs/2003.02244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "jiant: A Software Toolkit for Research on General-Purpose Text\n  Understanding Models", "abstract": "We introduce jiant, an open source toolkit for conducting multitask and\ntransfer learning experiments on English NLU tasks. jiant enables modular and\nconfiguration-driven experimentation with state-of-the-art models and\nimplements a broad set of tasks for probing, transfer learning, and multitask\ntraining experiments. jiant implements over 50 NLU tasks, including all GLUE\nand SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published\nperformance on a variety of tasks and models, including BERT and RoBERTa. jiant\nis available at https://jiant.info.", "published": "2020-03-04 18:41:10", "link": "http://arxiv.org/abs/2003.02249v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on Efficiency, Accuracy and Document Structure for Answer\n  Sentence Selection", "abstract": "An essential task of most Question Answering (QA) systems is to re-rank the\nset of answer candidates, i.e., Answer Sentence Selection (A2S). These\ncandidates are typically sentences either extracted from one or more documents\npreserving their natural order or retrieved by a search engine. Most\nstate-of-the-art approaches to the task use huge neural models, such as BERT,\nor complex attentive architectures. In this paper, we argue that by exploiting\nthe intrinsic structure of the original rank together with an effective\nword-relatedness encoder, we can achieve competitive results with respect to\nthe state of the art while retaining high efficiency. Our model takes 9.5\nseconds to train on the WikiQA dataset, i.e., very fast in comparison with the\n$\\sim 18$ minutes required by a standard BERT-base fine-tuning.", "published": "2020-03-04 22:12:18", "link": "http://arxiv.org/abs/2003.02349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kleister: A novel task for Information Extraction involving Long\n  Documents with Complex Layout", "abstract": "State-of-the-art solutions for Natural Language Processing (NLP) are able to\ncapture a broad range of contexts, like the sentence-level context or\ndocument-level context for short documents. But these solutions are still\nstruggling when it comes to longer, real-world documents with the information\nencoded in the spatial structure of the document, such as page elements like\ntables, forms, headers, openings or footers; complex page layout or presence of\nmultiple pages.\n  To encourage progress on deeper and more complex Information Extraction (IE)\nwe introduce a new task (named Kleister) with two new datasets. Utilizing both\ntextual and structural layout features, an NLP system must find the most\nimportant information, about various types of entities, in long formal\ndocuments. We propose Pipeline method as a text-only baseline with different\nNamed Entity Recognition architectures (Flair, BERT, RoBERTa). Moreover, we\nchecked the most popular PDF processing tools for text extraction (pdf2djvu,\nTesseract and Textract) in order to analyze behavior of IE system in presence\nof errors introduced by these tools.", "published": "2020-03-04 22:45:22", "link": "http://arxiv.org/abs/2003.02356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text\n  Classification", "abstract": "Text categorization is the task of assigning labels to documents written in a\nnatural language, and it has numerous real-world applications including\nsentiment analysis as well as traditional topic assignment tasks. In this\npaper, we propose 5 different configurations for the semantic matrix-based\nmemory neural network with end-to-end learning manner and evaluate our proposed\nmethod on two corpora of news articles (AG news, Sogou news). The best\nperformance of our proposed method outperforms the baseline VDCNN models on the\ntext classification task and gives a faster speed for learning semantics.\nMoreover, we also evaluate our model on small scale datasets. The results show\nthat our proposed method can still achieve better results in comparison to\nVDCNN on the small scale dataset. This paper is to appear in the Proceedings of\nthe 2020 IEEE 14th International Conference on Semantic Computing (ICSC 2020),\nSan Diego, California, 2020.", "published": "2020-03-04 02:00:57", "link": "http://arxiv.org/abs/2003.01857v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Augmentation using Pre-trained Transformer Models", "abstract": "Language model based pre-trained models such as BERT have provided\nsignificant gains across different NLP tasks. In this paper, we study different\ntypes of transformer based pre-trained models such as auto-regressive models\n(GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional\ndata augmentation. We show that prepending the class labels to text sequences\nprovides a simple yet effective way to condition the pre-trained models for\ndata augmentation. Additionally, on three classification benchmarks,\npre-trained Seq2Seq model outperforms other data augmentation methods in a\nlow-resource setting. Further, we explore how different pre-trained model based\ndata augmentation differs in-terms of data diversity, and how well such methods\npreserve the class-label information.", "published": "2020-03-04 18:35:19", "link": "http://arxiv.org/abs/2003.02245v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phase transitions in a decentralized graph-based approach to human\n  language", "abstract": "Zipf's law establishes a scaling behavior for word-frequencies in large text\ncorpora. The appearance of Zipfian properties in human language has been\npreviously explained as an optimization problem for the interests of speakers\nand hearers. On the other hand, human-like vocabularies can be viewed as\nbipartite graphs. The aim here is double: within a bipartite-graph approach to\nhuman vocabularies, to propose a decentralized language game model for the\nformation of Zipfian properties. To do this, we define a language game, in\nwhich a population of artificial agents is involved in idealized linguistic\ninteractions. Numerical simulations show the appearance of a phase transition\nfrom an initially disordered state to three possible phases for language\nformation. Our results suggest that Zipfian properties in language seem to\narise partly from decentralized linguistic interactions between agents endowed\nwith bipartite word-meaning mappings.", "published": "2020-03-04 15:07:03", "link": "http://arxiv.org/abs/2003.02639v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "On Emergent Communication in Competitive Multi-Agent Teams", "abstract": "Several recent works have found the emergence of grounded compositional\nlanguage in the communication protocols developed by mostly cooperative\nmulti-agent systems when learned end-to-end to maximize performance on a\ndownstream task. However, human populations learn to solve complex tasks\ninvolving communicative behaviors not only in fully cooperative settings but\nalso in scenarios where competition acts as an additional external pressure for\nimprovement. In this work, we investigate whether competition for performance\nfrom an external, similar agent team could act as a social influence that\nencourages multi-agent populations to develop better communication protocols\nfor improved performance, compositionality, and convergence speed. We start\nfrom Task & Talk, a previously proposed referential game between two\ncooperative agents as our testbed and extend it into Task, Talk & Compete, a\ngame involving two competitive teams each consisting of two aforementioned\ncooperative agents. Using this new setting, we provide an empirical study\ndemonstrating the impact of competitive influence on multi-agent teams. Our\nresults show that an external competitive influence leads to improved accuracy\nand generalization, as well as faster emergence of communicative languages that\nare more informative and compositional.", "published": "2020-03-04 01:14:27", "link": "http://arxiv.org/abs/2003.01848v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Restoration of Fragmentary Babylonian Texts Using Recurrent Neural\n  Networks", "abstract": "The main source of information regarding ancient Mesopotamian history and\nculture are clay cuneiform tablets. Despite being an invaluable resource, many\ntablets are fragmented leading to missing information. Currently these missing\nparts are manually completed by experts. In this work we investigate the\npossibility of assisting scholars and even automatically completing the breaks\nin ancient Akkadian texts from Achaemenid period Babylonia by modelling the\nlanguage using recurrent neural networks.", "published": "2020-03-04 06:36:50", "link": "http://arxiv.org/abs/2003.01912v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "GraphTTS: graph-to-sequence modelling in neural text-to-speech", "abstract": "This paper leverages the graph-to-sequence method in neural text-to-speech\n(GraphTTS), which maps the graph embedding of the input sequence to\nspectrograms. The graphical inputs consist of node and edge representations\nconstructed from input texts. The encoding of these graphical inputs\nincorporates syntax information by a GNN encoder module. Besides, applying the\nencoder of GraphTTS as a graph auxiliary encoder (GAE) can analyse prosody\ninformation from the semantic structure of texts. This can remove the manual\nselection of reference audios process and makes prosody modelling an end-to-end\nprocedure. Experimental analysis shows that GraphTTS outperforms the\nstate-of-the-art sequence-to-sequence models by 0.24 in Mean Opinion Score\n(MOS). GAE can adjust the pause, ventilation and tones of synthesised audios\nautomatically. This experimental conclusion may give some inspiration to\nresearchers working on improving speech synthesis prosody.", "published": "2020-03-04 07:44:55", "link": "http://arxiv.org/abs/2003.01924v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit\n  Alignment", "abstract": "Targeting at both high efficiency and performance, we propose AlignTTS to\npredict the mel-spectrum in parallel. AlignTTS is based on a Feed-Forward\nTransformer which generates mel-spectrum from a sequence of characters, and the\nduration of each character is determined by a duration predictor.Instead of\nadopting the attention mechanism in Transformer TTS to align text to\nmel-spectrum, the alignment loss is presented to consider all possible\nalignments in training by use of dynamic programming. Experiments on the\nLJSpeech dataset show that our model achieves not only state-of-the-art\nperformance which outperforms Transformer TTS by 0.03 in mean option score\n(MOS), but also a high efficiency which is more than 50 times faster than\nreal-time.", "published": "2020-03-04 08:44:32", "link": "http://arxiv.org/abs/2003.01950v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-time, Universal, and Robust Adversarial Attacks Against Speaker\n  Recognition Systems", "abstract": "As the popularity of voice user interface (VUI) exploded in recent years,\nspeaker recognition system has emerged as an important medium of identifying a\nspeaker in many security-required applications and services. In this paper, we\npropose the first real-time, universal, and robust adversarial attack against\nthe state-of-the-art deep neural network (DNN) based speaker recognition\nsystem. Through adding an audio-agnostic universal perturbation on arbitrary\nenrolled speaker's voice input, the DNN-based speaker recognition system would\nidentify the speaker as any target (i.e., adversary-desired) speaker label. In\naddition, we improve the robustness of our attack by modeling the sound\ndistortions caused by the physical over-the-air propagation through estimating\nroom impulse response (RIR). Experiment using a public dataset of 109 English\nspeakers demonstrates the effectiveness and robustness of our proposed attack\nwith a high attack success rate of over 90%. The attack launching time also\nachieves a 100X speedup over contemporary non-universal attacks.", "published": "2020-03-04 19:30:15", "link": "http://arxiv.org/abs/2003.02301v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised and Interpretable Domain Adaptation to Rapidly Filter\n  Tweets for Emergency Services", "abstract": "During the onset of a disaster event, filtering relevant information from the\nsocial web data is challenging due to its sparse availability and practical\nlimitations in labeling datasets of an ongoing crisis. In this paper, we\nhypothesize that unsupervised domain adaptation through multi-task learning can\nbe a useful framework to leverage data from past crisis events for training\nefficient information filtering models during the sudden onset of a new crisis.\nWe present a novel method to classify relevant tweets during an ongoing crisis\nwithout seeing any new examples, using the publicly available dataset of TREC\nincident streams. Specifically, we construct a customized multi-task\narchitecture with a multi-domain discriminator for crisis analytics: multi-task\ndomain adversarial attention network. This model consists of dedicated\nattention layers for each task to provide model interpretability; critical for\nreal-word applications. As deep networks struggle with sparse datasets, we show\nthat this can be improved by sharing a base layer for multi-task learning and\ndomain adversarial training. Evaluation of domain adaptation for crisis events\nis performed by choosing a target event as the test set and training on the\nrest. Our results show that the multi-task model outperformed its single task\ncounterpart. For the qualitative evaluation of interpretability, we show that\nthe attention layer can be used as a guide to explain the model predictions and\nempower emergency services for exploring accountability of the model, by\nshowcasing the words in a tweet that are deemed important in the classification\nprocess. Finally, we show a practical implication of our work by providing a\nuse-case for the COVID-19 pandemic.", "published": "2020-03-04 06:40:14", "link": "http://arxiv.org/abs/2003.04991v2", "categories": ["cs.CL", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-Microphone Complex Spectral Mapping for Speech Dereverberation", "abstract": "This study proposes a multi-microphone complex spectral mapping approach for\nspeech dereverberation on a fixed array geometry. In the proposed approach, a\ndeep neural network (DNN) is trained to predict the real and imaginary (RI)\ncomponents of direct sound from the stacked reverberant (and noisy) RI\ncomponents of multiple microphones. We also investigate the integration of\nmulti-microphone complex spectral mapping with beamforming and post-filtering.\nExperimental results on multi-channel speech dereverberation demonstrate the\neffectiveness of the proposed approach.", "published": "2020-03-04 02:21:37", "link": "http://arxiv.org/abs/2003.01861v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Fast Adaptation on Cross-Accented Speech Recognition", "abstract": "Local dialects influence people to pronounce words of the same language\ndifferently from each other. The great variability and complex characteristics\nof accents creates a major challenge for training a robust and accent-agnostic\nautomatic speech recognition (ASR) system. In this paper, we introduce a\ncross-accented English speech recognition task as a benchmark for measuring the\nability of the model to adapt to unseen accents using the existing CommonVoice\ncorpus. We also propose an accent-agnostic approach that extends the\nmodel-agnostic meta-learning (MAML) algorithm for fast adaptation to unseen\naccents. Our approach significantly outperforms joint training in both\nzero-shot, few-shot, and all-shot in the mixed-region and cross-region settings\nin terms of word error rate.", "published": "2020-03-04 05:37:25", "link": "http://arxiv.org/abs/2003.01901v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Robust Speaker Clustering Method Based on Discrete Tied Variational\n  Autoencoder", "abstract": "Recently, the speaker clustering model based on aggregation hierarchy cluster\n(AHC) is a common method to solve two main problems: no preset category number\nclustering and fix category number clustering. In general, model takes features\nlike i-vectors as input of probability and linear discriminant analysis model\n(PLDA) aims to form the distance matric in long voice application scenario, and\nthen clustering results are obtained through the clustering model. However,\ntraditional speaker clustering method based on AHC has the shortcomings of\nlong-time running and remains sensitive to environment noise. In this paper, we\npropose a novel speaker clustering method based on Mutual Information (MI) and\na non-linear model with discrete variable, which under the enlightenment of\nTied Variational Autoencoder (TVAE), to enhance the robustness against noise.\nThe proposed method named Discrete Tied Variational Autoencoder (DTVAE) which\nshortens the elapsed time substantially. With experience results, it\noutperforms the general model and yields a relative Accuracy (ACC) improvement\nand significant time reduction.", "published": "2020-03-04 08:54:38", "link": "http://arxiv.org/abs/2003.01955v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ASMD: an automatic framework for compiling multimodal datasets with\n  audio and scores", "abstract": "This paper describes an open-source Python framework for handling datasets\nfor music processing tasks, built with the aim of improving the reproducibility\nof research projects in music computing and assessing the generalization\nabilities of machine learning models. The framework enables the automatic\ndownload and installation of several commonly used datasets for multimodal\nmusic processing. Specifically, we provide a Python API to access the datasets\nthrough Boolean set operations based on particular attributes, such as\nintersections and unions of composers, instruments, and so on. The framework is\ndesigned to ease the inclusion of new datasets and the respective ground-truth\nannotations so that one can build, convert, and extend one's own collection as\nwell as distribute it by means of a compliant format to take advantage of the\nAPI. All code and ground-truth are released under suitable open licenses.", "published": "2020-03-04 08:57:59", "link": "http://arxiv.org/abs/2003.01958v2", "categories": ["cs.MM", "cs.DL", "cs.SD", "eess.AS", "H.5.5; I.2.m; H.3.7; J.5"], "primary_category": "cs.MM"}
