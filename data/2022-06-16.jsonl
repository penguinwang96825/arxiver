{"title": "Text normalization for low-resource languages: the case of Ligurian", "abstract": "Text normalization is a crucial technology for low-resource languages which\nlack rigid spelling conventions or that have undergone multiple spelling\nreforms. Low-resource text normalization has so far relied upon hand-crafted\nrules, which are perceived to be more data efficient than neural methods. In\nthis paper we examine the case of text normalization for Ligurian, an\nendangered Romance language. We collect 4,394 Ligurian sentences paired with\ntheir normalized versions, as well as the first open source monolingual corpus\nfor Ligurian. We show that, in spite of the small amounts of data available, a\ncompact transformer-based model can be trained to achieve very low error rates\nby the use of backtranslation and appropriate tokenization.", "published": "2022-06-16 00:37:55", "link": "http://arxiv.org/abs/2206.07861v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PInKS: Preconditioned Commonsense Inference with Minimal Supervision", "abstract": "Reasoning with preconditions such as \"glass can be used for drinking water\nunless the glass is shattered\" remains an open problem for language models. The\nmain challenge lies in the scarcity of preconditions data and the model's lack\nof support for such reasoning. We present PInKS, Preconditioned Commonsense\nInference with WeaK Supervision, an improved model for reasoning with\npreconditions through minimum supervision. We show, both empirically and\ntheoretically, that PInKS improves the results on benchmarks focused on\nreasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1\nscores). We further investigate PInKS through PAC-Bayesian informativeness\nanalysis, precision measures, and ablation study.", "published": "2022-06-16 04:50:00", "link": "http://arxiv.org/abs/2206.07920v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Understanding with Uniformity and Explicit Regularization\n  of Embeddings in Embedding-based Neural Topic Models", "abstract": "Embedding-based neural topic models could explicitly represent words and\ntopics by embedding them to a homogeneous feature space, which shows higher\ninterpretability. However, there are no explicit constraints for the training\nof embeddings, leading to a larger optimization space. Also, a clear\ndescription of the changes in embeddings and the impact on model performance is\nstill lacking. In this paper, we propose an embedding regularized neural topic\nmodel, which applies the specially designed training constraints on word\nembedding and topic embedding to reduce the optimization space of parameters.\nTo reveal the changes and roles of embeddings, we introduce \\textbf{uniformity}\ninto the embedding-based neural topic model as the evaluation metric of\nembedding space. On this basis, we describe how embeddings tend to change\nduring training via the changes in the uniformity of embeddings. Furthermore,\nwe demonstrate the impact of changes in embeddings in embedding-based neural\ntopic models through ablation studies. The results of experiments on two\nmainstream datasets indicate that our model significantly outperforms baseline\nmodels in terms of the harmony between topic quality and document modeling.\nThis work is the first attempt to exploit uniformity to explore changes in\nembeddings of embedding-based neural topic models and their impact on model\nperformance to the best of our knowledge.", "published": "2022-06-16 07:02:55", "link": "http://arxiv.org/abs/2206.07960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIALOG-22 RuATD Generated Text Detection", "abstract": "Text Generation Models (TGMs) succeed in creating text that matches human\nlanguage style reasonably well. Detectors that can distinguish between\nTGM-generated text and human-written ones play an important role in preventing\nabuse of TGM.\n  In this paper, we describe our pipeline for the two DIALOG-22 RuATD tasks:\ndetecting generated text (binary task) and classification of which model was\nused to generate text (multiclass task). We achieved 1st place on the binary\nclassification task with an accuracy score of 0.82995 on the private test set\nand 4th place on the multiclass classification task with an accuracy score of\n0.62856 on the private test set. We proposed an ensemble method of different\npre-trained models based on the attention mechanism.", "published": "2022-06-16 09:33:26", "link": "http://arxiv.org/abs/2206.08029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Open-Domain QA System for e-Governance", "abstract": "The paper presents an open-domain Question Answering system for Romanian,\nanswering COVID-19 related questions. The QA system pipeline involves automatic\nquestion processing, automatic query generation, web searching for the top 10\nmost relevant documents and answer extraction using a fine-tuned BERT model for\nExtractive QA, trained on a COVID-19 data set that we have manually created.\nThe paper will present the QA system and its integration with the Romanian\nlanguage technologies portal RELATE, the COVID-19 data set and different\nevaluations of the QA performance.", "published": "2022-06-16 10:02:31", "link": "http://arxiv.org/abs/2206.08046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language\n  Models as a Demonstration Generator", "abstract": "Large-scale pre-trained language models (PLMs) are well-known for being\ncapable of solving a task simply by conditioning a few input-label pairs dubbed\ndemonstrations on a prompt without being explicitly tuned for the desired\ndownstream task. Such a process (i.e., in-context learning), however, naturally\nleads to high reliance on the demonstrations which are usually selected from\nexternal datasets. In this paper, we propose self-generated in-context learning\n(SG-ICL), which generates demonstrations for in-context learning from PLM\nitself to minimize the reliance on the external demonstration. We conduct\nexperiments on four different text classification tasks and show SG-ICL\nsignificantly outperforms zero-shot learning and is generally worth\napproximately 0.6 gold training samples. Moreover, our generated demonstrations\nshow more consistent performance with low variance compared to randomly\nselected demonstrations from the training dataset.", "published": "2022-06-16 10:52:13", "link": "http://arxiv.org/abs/2206.08082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Cost and Quality: An Exploration of Human-in-the-loop\n  Frameworks for Automated Short Answer Scoring", "abstract": "Short answer scoring (SAS) is the task of grading short text written by a\nlearner. In recent years, deep-learning-based approaches have substantially\nimproved the performance of SAS models, but how to guarantee high-quality\npredictions still remains a critical issue when applying such models to the\neducation field. Towards guaranteeing high-quality predictions, we present the\nfirst study of exploring the use of human-in-the-loop framework for minimizing\nthe grading cost while guaranteeing the grading quality by allowing a SAS model\nto share the grading task with a human grader. Specifically, by introducing a\nconfidence estimation method for indicating the reliability of the model\npredictions, one can guarantee the scoring quality by utilizing only\npredictions with high reliability for the scoring results and casting\npredictions with low reliability to human graders. In our experiments, we\ninvestigate the feasibility of the proposed framework using multiple confidence\nestimation methods and multiple SAS datasets. We find that our\nhuman-in-the-loop framework allows automatic scoring models and human graders\nto achieve the target scoring quality.", "published": "2022-06-16 16:43:18", "link": "http://arxiv.org/abs/2206.08288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Multi-Task Models for Misogyny Identification and Categorization on\n  Arabic Social Media", "abstract": "The prevalence of toxic content on social media platforms, such as hate\nspeech, offensive language, and misogyny, presents serious challenges to our\ninterconnected society. These challenging issues have attracted widespread\nattention in Natural Language Processing (NLP) community. In this paper, we\npresent the submitted systems to the first Arabic Misogyny Identification\nshared task. We investigate three multi-task learning models as well as their\nsingle-task counterparts. In order to encode the input text, our models rely on\nthe pre-trained MARBERT language model. The overall obtained results show that\nall our submitted models have achieved the best performances (top three ranked\nsubmissions) in both misogyny identification and categorization tasks.", "published": "2022-06-16 18:54:37", "link": "http://arxiv.org/abs/2206.08407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended\n  Sarcasm Detection in English and Arabic", "abstract": "Sarcasm is a form of figurative language where the intended meaning of a\nsentence differs from its literal meaning. This poses a serious challenge to\nseveral Natural Language Processing (NLP) applications such as Sentiment\nAnalysis, Opinion Mining, and Author Profiling. In this paper, we present our\nparticipating system to the intended sarcasm detection task in English and\nArabic languages. Our system\\footnote{The source code of our system is\navailable at \\url{https://github.com/AbdelkaderMH/iSarcasmEval}} consists of\nthree deep learning-based models leveraging two existing pre-trained language\nmodels for Arabic and English. We have participated in all sub-tasks. Our\nofficial submissions achieve the best performance on sub-task A for Arabic\nlanguage and rank second in sub-task B. For sub-task C, our system is ranked\n7th and 11th on Arabic and English datasets, respectively.", "published": "2022-06-16 19:14:54", "link": "http://arxiv.org/abs/2206.08415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogueScript: Using Dialogue Agents to Produce a Script", "abstract": "We present a novel approach to generating scripts by using agents with\ndifferent personality types. To manage character interaction in the script, we\nemploy simulated dramatic networks. Automatic and human evaluation on multiple\ncriteria shows that our approach outperforms a vanilla-GPT2-based baseline. We\nfurther introduce a new metric to evaluate dialogue consistency based on\nnatural language inference and demonstrate its validity.", "published": "2022-06-16 19:57:01", "link": "http://arxiv.org/abs/2206.08425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GAAMA 2.0: An Integrated System that Answers Boolean and Extractive\n  Questions", "abstract": "Recent machine reading comprehension datasets include extractive and boolean\nquestions but current approaches do not offer integrated support for answering\nboth question types. We present a multilingual machine reading comprehension\nsystem and front-end demo that handles boolean questions by providing both a\nYES/NO answer and highlighting supporting evidence, and handles extractive\nquestions by highlighting the answer in the passage. Our system, GAAMA 2.0, is\nranked first on the Tydi QA leaderboard at the time of this writing. We\ncontrast two different implementations of our approach. The first includes\nseveral independent stacks of transformers allowing easy deployment of each\ncomponent. The second is a single stack of transformers utilizing adapters to\nreduce GPU memory footprint in a resource-constrained environment.", "published": "2022-06-16 20:46:04", "link": "http://arxiv.org/abs/2206.08441v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Abusive Language Detection with Community Context", "abstract": "Uses of pejorative expressions can be benign or actively empowering. When\nmodels for abuse detection misclassify these expressions as derogatory, they\ninadvertently censor productive conversations held by marginalized groups. One\nway to engage with non-dominant perspectives is to add context around\nconversations. Previous research has leveraged user- and thread-level features,\nbut it often neglects the spaces within which productive conversations take\nplace. Our paper highlights how community context can improve classification\noutcomes in abusive language detection. We make two main contributions to this\nend. First, we demonstrate that online communities cluster by the nature of\ntheir support towards victims of abuse. Second, we establish how community\ncontext improves accuracy and reduces the false positive rates of\nstate-of-the-art abusive language classifiers. These findings suggest a\npromising direction for context-aware models in abusive language research.", "published": "2022-06-16 20:54:02", "link": "http://arxiv.org/abs/2206.08445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable AMR-Based Question Decomposition for Multi-hop Question\n  Answering", "abstract": "Effective multi-hop question answering (QA) requires reasoning over multiple\nscattered paragraphs and providing explanations for answers. Most existing\napproaches cannot provide an interpretable reasoning process to illustrate how\nthese models arrive at an answer. In this paper, we propose a Question\nDecomposition method based on Abstract Meaning Representation (QDAMR) for\nmulti-hop QA, which achieves interpretable reasoning by decomposing a multi-hop\nquestion into simpler sub-questions and answering them in order. Since\nannotating the decomposition is expensive, we first delegate the complexity of\nunderstanding the multi-hop question to an AMR parser. We then achieve the\ndecomposition of a multi-hop question via segmentation of the corresponding AMR\ngraph based on the required reasoning type. Finally, we generate sub-questions\nusing an AMR-to-Text generation model and answer them with an off-the-shelf QA\nmodel. Experimental results on HotpotQA demonstrate that our approach is\ncompetitive for interpretable reasoning and that the sub-questions generated by\nQDAMR are well-formed, outperforming existing question-decomposition-based\nmulti-hop QA approaches.", "published": "2022-06-16 23:46:33", "link": "http://arxiv.org/abs/2206.08486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Global Semantic Similarities in Knowledge Graphs by\n  Relational Prototype Entities", "abstract": "Knowledge graph (KG) embedding aims at learning the latent representations\nfor entities and relations of a KG in continuous vector spaces. An empirical\nobservation is that the head (tail) entities connected by the same relation\noften share similar semantic attributes -- specifically, they often belong to\nthe same category -- no matter how far away they are from each other in the KG;\nthat is, they share global semantic similarities. However, many existing\nmethods derive KG embeddings based on the local information, which fail to\neffectively capture such global semantic similarities among entities. To\naddress this challenge, we propose a novel approach, which introduces a set of\nvirtual nodes called \\textit{\\textbf{relational prototype entities}} to\nrepresent the prototypes of the head and tail entities connected by the same\nrelations. By enforcing the entities' embeddings close to their associated\nprototypes' embeddings, our approach can effectively encourage the global\nsemantic similarities of entities -- that can be far away in the KG --\nconnected by the same relation. Experiments on the entity alignment and KG\ncompletion tasks demonstrate that our approach significantly outperforms recent\nstate-of-the-arts.", "published": "2022-06-16 09:25:33", "link": "http://arxiv.org/abs/2206.08021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JU_NLP at HinglishEval: Quality Evaluation of the Low-Resource\n  Code-Mixed Hinglish Text", "abstract": "In this paper we describe a system submitted to the INLG 2022 Generation\nChallenge (GenChal) on Quality Evaluation of the Low-Resource Synthetically\nGenerated Code-Mixed Hinglish Text. We implement a Bi-LSTM-based neural network\nmodel to predict the Average rating score and Disagreement score of the\nsynthetic Hinglish dataset. In our models, we used word embeddings for English\nand Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score\nof 0.11, and mean squared error of 6.0 in the average rating score prediction\ntask. In the task of Disagreement score prediction, we achieve a F1 score of\n0.18, and mean squared error of 5.0.", "published": "2022-06-16 10:12:44", "link": "http://arxiv.org/abs/2206.08053v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards Robust Ranker for Text Retrieval", "abstract": "A ranker plays an indispensable role in the de facto 'retrieval & rerank'\npipeline, but its training still lags behind -- learning from moderate\nnegatives or/and serving as an auxiliary module for a retriever. In this work,\nwe first identify two major barriers to a robust ranker, i.e., inherent label\nnoises caused by a well-trained retriever and non-ideal negatives sampled for a\nhigh-capable ranker. Thereby, we propose multiple retrievers as negative\ngenerators improve the ranker's robustness, where i) involving extensive\nout-of-distribution label noises renders the ranker against each noise\ndistribution, and ii) diverse hard negatives from a joint distribution are\nrelatively close to the ranker's negative distribution, leading to more\nchallenging thus effective training. To evaluate our robust ranker (dubbed\nR$^2$anker), we conduct experiments in various settings on the popular passage\nretrieval benchmark, including BM25-reranking, full-ranking, retriever\ndistillation, etc. The empirical results verify the new state-of-the-art\neffectiveness of our model.", "published": "2022-06-16 10:27:46", "link": "http://arxiv.org/abs/2206.08063v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TransDrift: Modeling Word-Embedding Drift using Transformer", "abstract": "In modern NLP applications, word embeddings are a crucial backbone that can\nbe readily shared across a number of tasks. However as the text distributions\nchange and word semantics evolve over time, the downstream applications using\nthe embeddings can suffer if the word representations do not conform to the\ndata drift. Thus, maintaining word embeddings to be consistent with the\nunderlying data distribution is a key problem. In this work, we tackle this\nproblem and propose TransDrift, a transformer-based prediction model for word\nembeddings. Leveraging the flexibility of transformer, our model accurately\nlearns the dynamics of the embedding drift and predicts the future embedding.\nIn experiments, we compare with existing methods and show that our model makes\nsignificantly more accurate predictions of the word embedding than the\nbaselines. Crucially, by applying the predicted embeddings as a backbone for\ndownstream classification tasks, we show that our embeddings lead to superior\nperformance compared to the previous methods.", "published": "2022-06-16 10:48:26", "link": "http://arxiv.org/abs/2206.08081v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning Architecture for Automatic Essay Scoring", "abstract": "Automatic evaluation of essay (AES) and also called automatic essay scoring\nhas become a severe problem due to the rise of online learning and evaluation\nplatforms such as Coursera, Udemy, Khan academy, and so on. Researchers have\nrecently proposed many techniques for automatic evaluation. However, many of\nthese techniques use hand-crafted features and thus are limited from the\nfeature representation point of view. Deep learning has emerged as a new\nparadigm in machine learning which can exploit the vast data and identify the\nfeatures useful for essay evaluation. To this end, we propose a novel\narchitecture based on recurrent networks (RNN) and convolution neural network\n(CNN). In the proposed architecture, the multichannel convolutional layer\nlearns and captures the contextual features of the word n-gram from the word\nembedding vectors and the essential semantic concepts to form the feature\nvector at essay level using max-pooling operation. A variant of RNN called\nBi-gated recurrent unit (BGRU) is used to access both previous and subsequent\ncontextual representations. The experiment was carried out on eight data sets\navailable on Kaggle for the task of AES. The experimental results show that our\nproposed system achieves significantly higher grading accuracy than other deep\nlearning-based AES systems and also other state-of-the-art AES systems.", "published": "2022-06-16 14:56:24", "link": "http://arxiv.org/abs/2206.08232v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "'John ate 5 apples' != 'John ate some apples': Self-Supervised\n  Paraphrase Quality Detection for Algebraic Word Problems", "abstract": "This paper introduces the novel task of scoring paraphrases for Algebraic\nWord Problems (AWP) and presents a self-supervised method for doing so. In the\ncurrent online pedagogical setting, paraphrasing these problems is helpful for\nacademicians to generate multiple syntactically diverse questions for\nassessments. It also helps induce variation to ensure that the student has\nunderstood the problem instead of just memorizing it or using unfair means to\nsolve it. The current state-of-the-art paraphrase generation models often\ncannot effectively paraphrase word problems, losing a critical piece of\ninformation (such as numbers or units) which renders the question unsolvable.\nThere is a need for paraphrase scoring methods in the context of AWP to enable\nthe training of good paraphrasers. Thus, we propose ParaQD, a self-supervised\nparaphrase quality detection method using novel data augmentations that can\nlearn latent representations to separate a high-quality paraphrase of an\nalgebraic question from a poor one by a wide margin. Through extensive\nexperimentation, we demonstrate that our method outperforms existing\nstate-of-the-art self-supervised methods by up to 32% while also demonstrating\nimpressive zero-shot performance.", "published": "2022-06-16 16:01:59", "link": "http://arxiv.org/abs/2206.08263v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Methods for Estimating and Improving Robustness of Language Models", "abstract": "Despite their outstanding performance, large language models (LLMs) suffer\nnotorious flaws related to their preference for simple, surface-level textual\nrelations over full semantic complexity of the problem. This proposal\ninvestigates a common denominator of this problem in their weak ability to\ngeneralise outside of the training domain. We survey diverse research\ndirections providing estimations of model generalisation ability and find that\nincorporating some of these measures in the training objectives leads to\nenhanced distributional robustness of neural models. Based on these findings,\nwe present future research directions towards enhancing the robustness of LLMs.", "published": "2022-06-16 21:02:53", "link": "http://arxiv.org/abs/2206.08446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accelerating Inference and Language Model Fusion of Recurrent Neural\n  Network Transducers via End-to-End 4-bit Quantization", "abstract": "We report on aggressive quantization strategies that greatly accelerate\ninference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit\ninteger representation for both weights and activations and apply Quantization\nAware Training (QAT) to retrain the full model (acoustic encoder and language\nmodel) and achieve near-iso-accuracy. We show that customized quantization\nschemes that are tailored to the local properties of the network are essential\nto achieve good performance while limiting the computational overhead of QAT.\n  Density ratio Language Model fusion has shown remarkable accuracy gains on\nRNN-T workloads but it severely increases the computational cost of inference.\nWe show that our quantization strategies enable using large beam widths for\nhypothesis search while achieving streaming-compatible runtimes and a full\nmodel compression ratio of 7.6$\\times$ compared to the full precision model.\n  Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to\nINT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a\nReal Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03\ntest sets, we retain most of the gains associated with LM fusion, improving the\naverage WER by $>$1.5%.", "published": "2022-06-16 02:17:49", "link": "http://arxiv.org/abs/2206.07882v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Multimodal Dialogue State Tracking", "abstract": "Designed for tracking user goals in dialogues, a dialogue state tracker is an\nessential component in a dialogue system. However, the research of dialogue\nstate tracking has largely been limited to unimodality, in which slots and slot\nvalues are limited by knowledge domains (e.g. restaurant domain with slots of\nrestaurant name and price range) and are defined by specific database schema.\nIn this paper, we propose to extend the definition of dialogue state tracking\nto multimodality. Specifically, we introduce a novel dialogue state tracking\ntask to track the information of visual objects that are mentioned in\nvideo-grounded dialogues. Each new dialogue utterance may introduce a new video\nsegment, new visual objects, or new object attributes, and a state tracker is\nrequired to update these information slots accordingly. We created a new\nsynthetic benchmark and designed a novel baseline, Video-Dialogue Transformer\nNetwork (VDTN), for this task. VDTN combines both object-level features and\nsegment-level features and learns contextual dependencies between videos and\ndialogues to generate multimodal dialogue states. We optimized VDTN for a state\ngeneration task as well as a self-supervised video understanding task which\nrecovers video segment or object representations. Finally, we trained VDTN to\nuse the decoded states in a response prediction task. Together with\ncomprehensive ablation and qualitative analysis, we discovered interesting\ninsights towards building more capable multimodal dialogue systems.", "published": "2022-06-16 03:18:42", "link": "http://arxiv.org/abs/2206.07898v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Automatic Prosody Annotation with Pre-Trained Text-Speech Model", "abstract": "Prosodic boundary plays an important role in text-to-speech synthesis (TTS)\nin terms of naturalness and readability. However, the acquisition of prosodic\nboundary labels relies on manual annotation, which is costly and\ntime-consuming. In this paper, we propose to automatically extract prosodic\nboundary labels from text-audio data via a neural text-speech model with\npre-trained audio encoders. This model is pre-trained on text and speech data\nseparately and jointly fine-tuned on TTS data in a triplet format: {speech,\ntext, prosody}. The experimental results on both automatic evaluation and human\nevaluation demonstrate that: 1) the proposed text-speech prosody annotation\nframework significantly outperforms text-only baselines; 2) the quality of\nautomatic prosodic boundary annotations is comparable to human annotations; 3)\nTTS systems trained with model-annotated boundaries are slightly better than\nsystems that use manual ones.", "published": "2022-06-16 06:54:16", "link": "http://arxiv.org/abs/2206.07956v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis\n  Using Linguistic and Prosodic Contexts of Dialogue History", "abstract": "We propose an end-to-end empathetic dialogue speech synthesis (DSS) model\nthat considers both the linguistic and prosodic contexts of dialogue history.\nEmpathy is the active attempt by humans to get inside the interlocutor in\ndialogue, and empathetic DSS is a technology to implement this act in spoken\ndialogue systems. Our model is conditioned by the history of linguistic and\nprosody features for predicting appropriate dialogue context. As such, it can\nbe regarded as an extension of the conventional linguistic-feature-based\ndialogue history modeling. To train the empathetic DSS model effectively, we\ninvestigate 1) a self-supervised learning model pretrained with large speech\ncorpora, 2) a style-guided training using a prosody embedding of the current\nutterance to be predicted by the dialogue context embedding, 3) a cross-modal\nattention to combine text and speech modalities, and 4) a sentence-wise\nembedding to achieve fine-grained prosody modeling rather than utterance-wise\nmodeling. The evaluation results demonstrate that 1) simply considering\nprosodic contexts of the dialogue history does not improve the quality of\nspeech in empathetic DSS and 2) introducing style-guided training and\nsentence-wise embedding modeling achieves higher speech quality than that by\nthe conventional method.", "published": "2022-06-16 09:47:25", "link": "http://arxiv.org/abs/2206.08039v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonwords Pronunciation Classification in Language Development Tests for\n  Preschool Children", "abstract": "This work aims to automatically evaluate whether the language development of\nchildren is age-appropriate. Validated speech and language tests are used for\nthis purpose to test the auditory memory. In this work, the task is to\ndetermine whether spoken nonwords have been uttered correctly. We compare\ndifferent approaches that are motivated to model specific language structures:\nLow-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated\nembeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR\nacoustic model). Each of the approaches provides input for VGG-like 5-layer CNN\nclassifiers. We also examine the adaptation per nonword. The evaluation of the\nproposed systems was performed using recordings from different kindergartens of\nspoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model\nphonetic information; wav2vec2.0 is trained on grapheme labels, our ASR\nacoustic model features contain (sub-)phonetic information. We found that the\nmore granular the phonetic modeling is, the higher are the achieved recognition\nrates. The best system trained on ASR acoustic model features with VTLN\nachieved an accuracy of 89.4% and an area under the ROC (Receiver Operating\nCharacteristic) curve (AUC) of 0.923. This corresponds to an improvement in\naccuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.", "published": "2022-06-16 10:19:47", "link": "http://arxiv.org/abs/2206.08058v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language\n  Models", "abstract": "Video question answering (VideoQA) is a complex task that requires diverse\nmulti-modal data for training. Manual annotation of question and answers for\nvideos, however, is tedious and prohibits scalability. To tackle this problem,\nrecent methods consider zero-shot settings with no manual annotation of visual\nquestion-answer. In particular, a promising approach adapts frozen\nautoregressive language models pretrained on Web-scale text-only data to\nmulti-modal inputs. In contrast, we here build on frozen bidirectional language\nmodels (BiLM) and show that such an approach provides a stronger and cheaper\nalternative for zero-shot VideoQA. In particular, (i) we combine visual inputs\nwith the frozen BiLM using light trainable modules, (ii) we train such modules\nusing Web-scraped multi-modal data, and finally (iii) we perform zero-shot\nVideoQA inference through masked language modeling, where the masked text is\nthe answer to a given question. Our proposed approach, FrozenBiLM, outperforms\nthe state of the art in zero-shot VideoQA by a significant margin on a variety\nof datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,\nTGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in\nthe few-shot and fully-supervised setting. Our code and models are publicly\navailable at https://github.com/antoyang/FrozenBiLM.", "published": "2022-06-16 13:18:20", "link": "http://arxiv.org/abs/2206.08155v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "All the World's a (Hyper)Graph: A Data Drama", "abstract": "We introduce Hyperbard, a dataset of diverse relational data representations\nderived from Shakespeare's plays. Our representations range from simple graphs\ncapturing character co-occurrence in single scenes to hypergraphs encoding\ncomplex communication settings and character contributions as hyperedges with\nedge-specific node weights. By making multiple intuitive representations\nreadily available for experimentation, we facilitate rigorous representation\nrobustness checks in graph learning, graph mining, and network analysis,\nhighlighting the advantages and drawbacks of specific representations.\nLeveraging the data released in Hyperbard, we demonstrate that many solutions\nto popular graph mining problems are highly dependent on the representation\nchoice, thus calling current graph curation practices into question. As an\nhomage to our data source, and asserting that science can also be art, we\npresent all our points in the form of a play.", "published": "2022-06-16 14:51:28", "link": "http://arxiv.org/abs/2206.08225v3", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Paraformer: Fast and Accurate Parallel Transformer for\n  Non-autoregressive End-to-End Speech Recognition", "abstract": "Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.", "published": "2022-06-16 17:24:14", "link": "http://arxiv.org/abs/2206.08317v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Characteristics of Harmful Text: Towards Rigorous Benchmarking of\n  Language Models", "abstract": "Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.", "published": "2022-06-16 17:28:01", "link": "http://arxiv.org/abs/2206.08325v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Know your audience: specializing grounded language models with listener\n  subtraction", "abstract": "Effective communication requires adapting to the idiosyncrasies of each\ncommunicative context--such as the common ground shared with each partner.\nHumans demonstrate this ability to specialize to their audience in many\ncontexts, such as the popular game Dixit. We take inspiration from Dixit to\nformulate a multi-agent image reference game where a (trained) speaker model is\nrewarded for describing a target image such that one (pretrained) listener\nmodel can correctly identify it among distractors, but another listener cannot.\nTo adapt, the speaker must exploit differences in the knowledge it shares with\nthe different listeners. We show that finetuning an attention-based adapter\nbetween a CLIP vision encoder and a large language model in this contrastive,\nmulti-agent setting gives rise to context-dependent natural language\nspecialization from rewards only, without direct supervision. Through\ncontrolled experiments, we show that training a speaker with two listeners that\nperceive differently, using our method, allows the speaker to adapt to the\nidiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the\nspecialization to real-world data. Our experiments demonstrate a method for\nspecializing grounded language models without direct supervision and highlight\nthe interesting research challenges posed by complex multi-agent communication.", "published": "2022-06-16 17:52:08", "link": "http://arxiv.org/abs/2206.08349v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Predicting Hate Intensity of Twitter Conversation Threads", "abstract": "Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11% margin on the\nPerson correlation coefficient and a decrease of 25% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.", "published": "2022-06-16 18:51:36", "link": "http://arxiv.org/abs/2206.08406v4", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A CTC Triggered Siamese Network with Spatial-Temporal Dropout for Speech\n  Recognition", "abstract": "Siamese networks have shown effective results in unsupervised visual\nrepresentation learning. These models are designed to learn an invariant\nrepresentation of two augmentations for one input by maximizing their\nsimilarity. In this paper, we propose an effective Siamese network to improve\nthe robustness of End-to-End automatic speech recognition (ASR). We introduce\nspatial-temporal dropout to support a more violent disturbance for Siamese-ASR\nframework. Besides, we also relax the similarity regularization to maximize the\nsimilarities of distributions on the frames that connectionist temporal\nclassification (CTC) spikes occur rather than on all of them. The efficiency of\nthe proposed architecture is evaluated on two benchmarks, AISHELL-1 and\nLibrispeech, resulting in 7.13% and 6.59% relative character error rate (CER)\nand word error rate (WER) reductions respectively. Analysis shows that our\nproposed approach brings a better uniformity for the trained model and enlarges\nthe CTC spikes obviously.", "published": "2022-06-16 09:36:30", "link": "http://arxiv.org/abs/2206.08031v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Scattering Transform Network with Generalized Morse Wavelets and Its\n  Application to Music Genre Classification", "abstract": "We propose to use the Generalized Morse Wavelets (GMWs) instead of\ncommonly-used Morlet (or Gabor) wavelets in the Scattering Transform Network\n(STN), which we call the GMW-STN, for signal classification problems. The GMWs\nform a parameterized family of truly analytic wavelets while the Morlet\nwavelets are only approximately analytic. The analyticity of underlying wavelet\nfilters in the STN is particularly important for nonstationary oscillatory\nsignals such as music signals because it improves interpretability of the STN\nrepresentations by providing multiscale amplitude and phase (and consequently\nfrequency) information of input signals. We demonstrate the superiority of the\nGMW-STN over the conventional STN in music genre classification using the\nso-called GTZAN database. Moreover, we show the performance improvement of the\nGMW-STN by increasing its number of layers to three over the typical two-layer\nSTN.}", "published": "2022-06-16 00:30:09", "link": "http://arxiv.org/abs/2206.07857v1", "categories": ["eess.AS", "cs.LG", "68T10, 94A12, 65T60", "I.5.4"], "primary_category": "eess.AS"}
{"title": "To Dereverb Or Not to Dereverb? Perceptual Studies On Real-Time\n  Dereverberation Targets", "abstract": "In real life, room effect, also known as room reverberation, and the present\nbackground noise degrade the quality of speech. Recently, deep learning-based\nspeech enhancement approaches have shown a lot of promise and surpassed\ntraditional denoising and dereverberation methods. It is also well established\nthat these state-of-the-art denoising algorithms significantly improve the\nquality of speech as perceived by human listeners. But the role of\ndereverberation on subjective (perceived) speech quality, and whether the\nadditional artifacts introduced by dereverberation cause more harm than good\nare still unclear. In this paper, we attempt to answer these questions by\nevaluating a state of the art speech enhancement system in a comprehensive\nsubjective evaluation study for different choices of dereverberation targets.", "published": "2022-06-16 04:43:09", "link": "http://arxiv.org/abs/2206.07917v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised\n  Learning and Its Application to Children's ASR", "abstract": "Self-supervised learning (SSL) in the pretraining stage using un-annotated\nspeech data has been successful in low-resource automatic speech recognition\n(ASR) tasks. However, models trained through SSL are biased to the pretraining\ndata which is usually different from the data used in finetuning tasks, causing\na domain shifting problem, and thus resulting in limited knowledge transfer. We\npropose a novel framework, domain responsible adaptation and finetuning\n(DRAFT), to reduce domain shifting in pretrained speech models through an\nadditional adaptation stage. In DRAFT, residual adapters (RAs) are inserted in\nthe pretrained model to learn domain-related information with the same SSL loss\nas the pretraining stage. Only RA parameters are updated during the adaptation\nstage. DRAFT is agnostic to the type of SSL method used and is evaluated with\nthree widely used approaches: APC, Wav2vec2.0, and HuBERT. On two child ASR\ntasks (OGI and MyST databases), using SSL models trained with un-annotated\nadult speech data (Librispeech), relative WER improvements of up to 19.7% are\nobserved when compared to the pretrained models without adaptation. Additional\nexperiments examined the potential of cross knowledge transfer between the two\ndatasets and the results are promising, showing a broader usage of the proposed\nDRAFT framework.", "published": "2022-06-16 05:28:31", "link": "http://arxiv.org/abs/2206.07931v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based\n  on Self-supervised Pre-training", "abstract": "Recent studies have shown that the benefits provided by self-supervised\npre-training and self-training (pseudo-labeling) are complementary.\nSemi-supervised fine-tuning strategies under the pre-training framework,\nhowever, remain insufficiently studied. Besides, modern semi-supervised speech\nrecognition algorithms either treat unlabeled data indiscriminately or filter\nout noisy samples with a confidence threshold. The dissimilarities among\ndifferent unlabeled data are often ignored. In this paper, we propose Censer, a\nsemi-supervised speech recognition algorithm based on self-supervised\npre-training to maximize the utilization of unlabeled data. The pre-training\nstage of Censer adopts wav2vec2.0 and the fine-tuning stage employs an improved\nsemi-supervised learning algorithm from slimIPL, which leverages unlabeled data\nprogressively according to their pseudo labels' qualities. We also incorporate\na temporal pseudo label pool and an exponential moving average to control the\npseudo labels' update frequency and to avoid model divergence. Experimental\nresults on Libri-Light and LibriSpeech datasets manifest our proposed method\nachieves better performance compared to existing approaches while being more\nunified.", "published": "2022-06-16 14:02:20", "link": "http://arxiv.org/abs/2206.08189v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Event-related data conditioning for acoustic event classification", "abstract": "Models based on diverse attention mechanisms have recently shined in tasks\nrelated to acoustic event classification (AEC). Among them, self-attention is\noften used in audio-only tasks to help the model recognize different acoustic\nevents. Self-attention relies on the similarity between time frames, and uses\nglobal information from the whole segment to highlight specific features within\na frame. In real life, information related to acoustic events will attenuate\nover time, which means the information within some frames around the event\ndeserves more attention than distant time global information that may be\nunrelated to the event. This paper shows that self-attention may over-enhance\ncertain segments of audio representations, and smooth out the boundaries\nbetween events representations and background noises. Hence, this paper\nproposes an event-related data conditioning (EDC) for AEC. EDC directly works\non spectrograms. The idea of EDC is to adaptively select the frame-related\nattention range based on acoustic features, and gather the event-related local\ninformation to represent the frame. Experiments show that: 1) compared with\nspectrogram-based data augmentation methods and trainable feature weighting and\nself-attention, EDC outperforms them in both the original-size mode and the\naugmented mode; 2) EDC effectively gathers event-related local information and\nenhances boundaries between events and backgrounds, improving the performance\nof AEC.", "published": "2022-06-16 14:57:44", "link": "http://arxiv.org/abs/2206.08233v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EPG2S: Speech Generation and Speech Enhancement based on\n  Electropalatography and Audio Signals using Multimodal Learning", "abstract": "Speech generation and enhancement based on articulatory movements facilitate\ncommunication when the scope of verbal communication is absent, e.g., in\npatients who have lost the ability to speak. Although various techniques have\nbeen proposed to this end, electropalatography (EPG), which is a monitoring\ntechnique that records contact between the tongue and hard palate during\nspeech, has not been adequately explored. Herein, we propose a novel multimodal\nEPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech\ngeneration and enhancement. Different fusion strategies based on multiple\ncombinations of EPG and noisy speech signals are examined, and the viability of\nthe proposed method is investigated. Experimental results indicate that EPG2S\nachieves desirable speech generation outcomes based solely on EPG signals.\nFurther, the addition of noisy speech signals is observed to improve quality\nand intelligibility. Additionally, EPG2S is observed to achieve high-quality\nspeech enhancement based solely on audio signals, with the addition of EPG\nsignals further improving the performance. The late fusion strategy is deemed\nto be the most effective approach for simultaneous speech generation and\nenhancement.", "published": "2022-06-16 00:33:20", "link": "http://arxiv.org/abs/2206.07860v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCASE 2022: Comparative Analysis Of CNNs For Acoustic Scene\n  Classification Under Low-Complexity Considerations", "abstract": "Acoustic scene classification is an automatic listening problem that aims to\nassign an audio recording to a pre-defined scene based on its audio data. Over\nthe years (and in past editions of the DCASE) this problem has often been\nsolved with techniques known as ensembles (use of several machine learning\nmodels to combine their predictions in the inference phase). While these\nsolutions can show performance in terms of accuracy, they can be very expensive\nin terms of computational capacity, making it impossible to deploy them in IoT\ndevices. Due to the drift in this field of study, this task has two limitations\nin terms of model complexity. It should be noted that there is also the added\ncomplexity of mismatching devices (the audios provided are recorded by\ndifferent sources of information). This technical report makes a comparative\nstudy of two different network architectures: conventional CNN and Conv-mixer.\nAlthough both networks exceed the baseline required by the competition, the\nconventional CNN shows a higher performance, exceeding the baseline by 8\npercentage points. Solutions based on Conv-mixer architectures show worse\nperformance although they are much lighter solutions.", "published": "2022-06-16 09:03:56", "link": "http://arxiv.org/abs/2206.08007v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Privacy Protection on Speech Enhancement", "abstract": "Speech is easily leaked imperceptibly, such as being recorded by mobile\nphones in different situations. Private content in speech may be maliciously\nextracted through speech enhancement technology. Speech enhancement technology\nhas developed rapidly along with deep neural networks (DNNs), but adversarial\nexamples can cause DNNs to fail. In this work, we propose an adversarial method\nto degrade speech enhancement systems. Experimental results show that generated\nadversarial examples can erase most content information in original examples or\nreplace it with target speech content through speech enhancement. The word\nerror rate (WER) between an enhanced original example and enhanced adversarial\nexample recognition result can reach 89.0%. WER of target attack between\nenhanced adversarial example and target example is low to 33.75% . Adversarial\nperturbation can bring the rate of change to the original example to more than\n1.4430. This work can prevent the malicious extraction of speech.", "published": "2022-06-16 13:38:59", "link": "http://arxiv.org/abs/2206.08170v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Strategies to Improve Robustness of Target Speech Extraction to\n  Enrollment Variations", "abstract": "Target speech extraction is a technique to extract the target speaker's voice\nfrom mixture signals using a pre-recorded enrollment utterance that\ncharacterize the voice characteristics of the target speaker. One major\ndifficulty of target speech extraction lies in handling variability in\n``intra-speaker'' characteristics, i.e., characteristics mismatch between\ntarget speech and an enrollment utterance. While most conventional approaches\nfocus on improving {\\it average performance} given a set of enrollment\nutterances, here we propose to guarantee the {\\it worst performance}, which we\nbelieve is of great practical importance. In this work, we propose an\nevaluation metric called worst-enrollment source-to-distortion ratio (SDR) to\nquantitatively measure the robustness towards enrollment variations. We also\nintroduce a novel training scheme that aims at directly optimizing the\nworst-case performance by focusing on training with difficult enrollment cases\nwhere extraction does not perform well. In addition, we investigate the\neffectiveness of auxiliary speaker identification loss (SI-loss) as another way\nto improve robustness over enrollments. Experimental validation reveals the\neffectiveness of both worst-enrollment target training and SI-loss training to\nimprove robustness against enrollment variations, by increasing speaker\ndiscriminability.", "published": "2022-06-16 13:41:30", "link": "http://arxiv.org/abs/2206.08174v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Language Model With Million Context Length For Raw Audio", "abstract": "Modeling long-term dependencies for audio signals is a particularly\nchallenging problem, as even small-time scales yield on the order of a hundred\nthousand samples. With the recent advent of Transformers, neural architectures\nbecame good at modeling dependencies over longer time scales, but they suffered\nfrom quadratic constraints to scale them. We propose a generative\nauto-regressive architecture that can model audio waveforms over quite a large\ncontext, greater than 500,000 samples. Our work is adapted to learn time\ndependencies by learning a latent representation by a CNN front-end, and then\nlearning dependencies over these representations using Transformer encoders,\nfully trained end-to-end: thereby allowing to learn representations as it deems\nfit for the next sample. Unlike previous works that compared different time\nscales to show improvement, we use a standard dataset, with the same number of\nparameters/context to show improvements. We achieve a state-of-the-art\nperformance as compared to other approaches such as Wavenet, SaSHMI, and\nSample-RNN on a standard dataset for modeling long-term structure. This work\ngives very exciting direction for the field, given improvements in context\nmodeling that can be scaled with more data, as well as potentially better\nresults by using billions/trillions of parameters.", "published": "2022-06-16 16:57:43", "link": "http://arxiv.org/abs/2206.08297v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning", "abstract": "We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio\nrendering for 3D environments. Given a 3D mesh of a real-world environment,\nSoundSpaces can generate highly realistic acoustics for arbitrary sounds\ncaptured from arbitrary microphone locations. Together with existing 3D visual\nassets, it supports an array of audio-visual research tasks, such as\naudio-visual navigation, mapping, source localization and separation, and\nacoustic matching. Compared to existing resources, SoundSpaces 2.0 has the\nadvantages of allowing continuous spatial sampling, generalization to novel\nenvironments, and configurable microphone and material properties. To our\nknowledge, this is the first geometry-based acoustic simulation that offers\nhigh fidelity and realism while also being fast enough to use for embodied\nlearning. We showcase the simulator's properties and benchmark its performance\nagainst real-world audio measurements. In addition, we demonstrate two\ndownstream tasks -- embodied navigation and far-field automatic speech\nrecognition -- and highlight sim2real performance for the latter. SoundSpaces\n2.0 is publicly available to facilitate wider research for perceptual systems\nthat can both see and hear.", "published": "2022-06-16 17:17:44", "link": "http://arxiv.org/abs/2206.08312v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
