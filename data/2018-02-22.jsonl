{"title": "MPST: A Corpus of Movie Plot Synopses with Tags", "abstract": "Social tagging of movies reveals a wide range of heterogeneous information\nabout movies, like the genre, plot structure, soundtracks, metadata, visual and\nemotional experiences. Such information can be valuable in building automatic\nsystems to create tags for movies. Automatic tagging systems can help\nrecommendation engines to improve the retrieval of similar movies as well as\nhelp viewers to know what to expect from a movie in advance. In this paper, we\nset out to the task of collecting a corpus of movie plot synopses and tags. We\ndescribe a methodology that enabled us to build a fine-grained set of around 70\ntags exposing heterogeneous characteristics of movie plots and the multi-label\nassociations of these tags with some 14K movie plot synopses. We investigate\nhow these tags correlate with movies and the flow of emotions throughout\ndifferent types of movies. Finally, we use this corpus to explore the\nfeasibility of inferring tags from plot synopses. We expect the corpus will be\nuseful in other tasks where analysis of narratives is relevant.", "published": "2018-02-22 00:27:54", "link": "http://arxiv.org/abs/1802.07858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Named Entity Recognition for Short Social Media Posts", "abstract": "We introduce a new task called Multimodal Named Entity Recognition (MNER) for\nnoisy user-generated data such as tweets or Snapchat captions, which comprise\nshort text with accompanying images. These social media posts often come in\ninconsistent or incomplete syntax and lexical notations with very limited\nsurrounding textual contexts, bringing significant challenges for NER. To this\nend, we create a new dataset for MNER called SnapCaptions (Snapchat\nimage-caption pairs submitted to public and crowd-sourced stories with fully\nannotated named entities). We then build upon the state-of-the-art Bi-LSTM\nword/character based NER models with 1) a deep image network which incorporates\nrelevant visual context to augment textual information, and 2) a generic\nmodality-attention module which learns to attenuate irrelevant modalities while\namplifying the most informative ones to extract contexts from, adaptive to each\nsample and token. The proposed MNER model with modality attention significantly\noutperforms the state-of-the-art text-only NER models by successfully\nleveraging provided visual contexts, opening up potential applications of MNER\non myriads of social media platforms.", "published": "2018-02-22 00:54:47", "link": "http://arxiv.org/abs/1802.07862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIDIOMS: A Multilingual Linked Idioms Data Set", "abstract": "In this paper, we describe the LIDIOMS data set, a multilingual RDF\nrepresentation of idioms currently containing five languages: English, German,\nItalian, Portuguese, and Russian. The data set is intended to support natural\nlanguage processing applications by providing links between idioms across\nlanguages. The underlying data was crawled and integrated from various sources.\nTo ensure the quality of the crawled data, all idioms were evaluated by at\nleast two native speakers. Herein, we present the model devised for structuring\nthe data. We also provide the details of linking LIDIOMS to well-known\nmultilingual data sets such as BabelNet. The resulting data set complies with\nbest practices according to Linguistic Linked Open Data Community.", "published": "2018-02-22 16:38:40", "link": "http://arxiv.org/abs/1802.08148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RDF2PT: Generating Brazilian Portuguese Texts from RDF Data", "abstract": "The generation of natural language from Resource Description Framework (RDF)\ndata has recently gained significant attention due to the continuous growth of\nLinked Data. A number of these approaches generate natural language in\nlanguages other than English, however, no work has been proposed to generate\nBrazilian Portuguese texts out of RDF. We address this research gap by\npresenting RDF2PT, an approach that verbalizes RDF data to Brazilian Portuguese\nlanguage. We evaluated RDF2PT in an open questionnaire with 44 native speakers\ndivided into experts and non-experts. Our results suggest that RDF2PT is able\nto generate text which is similar to that generated by humans and can hence be\neasily understood.", "published": "2018-02-22 16:41:56", "link": "http://arxiv.org/abs/1802.08150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Multimodal Learning for Emotion Recognition in Spoken Language", "abstract": "In this paper, we present a novel deep multimodal framework to predict human\nemotions based on sentence-level spoken language. Our architecture has two\ndistinctive characteristics. First, it extracts the high-level features from\nboth text and audio via a hybrid deep multimodal structure, which considers the\nspatial information from text, temporal information from audio, and high-level\nassociations from low-level handcrafted features. Second, we fuse all features\nby using a three-layer deep neural network to learn the correlations across\nmodalities and train the feature extraction and fusion modules together,\nallowing optimal global fine-tuning of the entire structure. We evaluated the\nproposed framework on the IEMOCAP dataset. Our result shows promising\nperformance, achieving 60.4% in weighted accuracy for five emotion categories.", "published": "2018-02-22 22:34:24", "link": "http://arxiv.org/abs/1802.08332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Spatiotemporal Factors Associated With Sentiment on Twitter:\n  Synthesis and Suggestions for Improving the Identification of Localized\n  Deviations", "abstract": "Background: Studies examining how sentiment on social media varies depending\non timing and location appear to produce inconsistent results, making it hard\nto design systems that use sentiment to detect localized events for public\nhealth applications.\n  Objective: The aim of this study was to measure how common timing and\nlocation confounders explain variation in sentiment on Twitter.\n  Methods: Using a dataset of 16.54 million English-language tweets from 100\ncities posted between July 13 and November 30, 2017, we estimated the positive\nand negative sentiment for each of the cities using a dictionary-based\nsentiment analysis and constructed models to explain the differences in\nsentiment using time of day, day of week, weather, city, and interaction type\n(conversations or broadcasting) as factors and found that all factors were\nindependently associated with sentiment.\n  Results: In the full multivariable model of positive (Pearson r in test data\n0.236; 95\\% CI 0.231-0.241) and negative (Pearson r in test data 0.306; 95\\% CI\n0.301-0.310) sentiment, the city and time of day explained more of the variance\nthan weather and day of week. Models that account for these confounders produce\na different distribution and ranking of important events compared with models\nthat do not account for these confounders.\n  Conclusions: In public health applications that aim to detect localized\nevents by aggregating sentiment across populations of Twitter users, it is\nworthwhile accounting for baseline differences before looking for unexpected\nchanges.", "published": "2018-02-22 00:37:32", "link": "http://arxiv.org/abs/1802.07859v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Neural Predictive Coding using Convolutional Neural Networks towards\n  Unsupervised Learning of Speaker Characteristics", "abstract": "Learning speaker-specific features is vital in many applications like speaker\nrecognition, diarization and speech recognition. This paper provides a novel\napproach, we term Neural Predictive Coding (NPC), to learn speaker-specific\ncharacteristics in a completely unsupervised manner from large amounts of\nunlabeled training data that even contain many non-speech events and\nmulti-speaker audio streams. The NPC framework exploits the proposed short-term\nactive-speaker stationarity hypothesis which assumes two temporally-close short\nspeech segments belong to the same speaker, and thus a common representation\nthat can encode the commonalities of both the segments, should capture the\nvocal characteristics of that speaker. We train a convolutional deep siamese\nnetwork to produce \"speaker embeddings\" by learning to separate `same' vs\n`different' speaker pairs which are generated from an unlabeled data of audio\nstreams. Two sets of experiments are done in different scenarios to evaluate\nthe strength of NPC embeddings and compare with state-of-the-art in-domain\nsupervised methods. First, two speaker identification experiments with\ndifferent context lengths are performed in a scenario with comparatively\nlimited within-speaker channel variability. NPC embeddings are found to perform\nthe best at short duration experiment, and they provide complementary\ninformation to i-vectors for full utterance experiments. Second, a large scale\nspeaker verification task having a wide range of within-speaker channel\nvariability is adopted as an upper-bound experiment where comparisons are drawn\nwith in-domain supervised methods.", "published": "2018-02-22 00:37:49", "link": "http://arxiv.org/abs/1802.07860v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating High-Quality Query Suggestion Candidates for Task-Based\n  Search", "abstract": "We address the task of generating query suggestions for task-based search.\nThe current state of the art relies heavily on suggestions provided by a major\nsearch engine. In this paper, we solve the task without reliance on search\nengines. Specifically, we focus on the first step of a two-stage pipeline\napproach, which is dedicated to the generation of query suggestion candidates.\nWe present three methods for generating candidate suggestions and apply them on\nmultiple information sources. Using a purpose-built test collection, we find\nthat these methods are able to generate high-quality suggestion candidates.", "published": "2018-02-22 11:55:28", "link": "http://arxiv.org/abs/1802.07997v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Towards an Understanding of Entity-Oriented Search Intents", "abstract": "Entity-oriented search deals with a wide variety of information needs, from\ndisplaying direct answers to interacting with services. In this work, we aim to\nunderstand what are prominent entity-oriented search intents and how they can\nbe fulfilled. We develop a scheme of entity intent categories, and use them to\nannotate a sample of queries. Specifically, we annotate unique query refiners\non the level of entity types. We observe that, on average, over half of those\nrefiners seek to interact with a service, while over a quarter of the refiners\nsearch for information that may be looked up in a knowledge base.", "published": "2018-02-22 12:30:13", "link": "http://arxiv.org/abs/1802.08010v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People", "abstract": "The study of algorithms to automatically answer visual questions currently is\nmotivated by visual question answering (VQA) datasets constructed in artificial\nVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising\nfrom a natural VQA setting. VizWiz consists of over 31,000 visual questions\noriginating from blind people who each took a picture using a mobile phone and\nrecorded a spoken question about it, together with 10 crowdsourced answers per\nvisual question. VizWiz differs from the many existing VQA datasets because (1)\nimages are captured by blind photographers and so are often poor quality, (2)\nquestions are spoken and so are more conversational, and (3) often visual\nquestions cannot be answered. Evaluation of modern algorithms for answering\nvisual questions and deciding if a visual question is answerable reveals that\nVizWiz is a challenging dataset. We introduce this dataset to encourage a\nlarger community to develop more generalized algorithms that can assist blind\npeople.", "published": "2018-02-22 18:16:53", "link": "http://arxiv.org/abs/1802.08218v4", "categories": ["cs.CV", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Content-Based Citation Recommendation", "abstract": "We present a content-based method for recommending citations in an academic\npaper draft. We embed a given query document into a vector space, then use its\nnearest neighbors as candidates, and rerank the candidates using a\ndiscriminative model trained to distinguish between observed and unobserved\ncitations. Unlike previous work, our method does not require metadata such as\nauthor names which can be missing, e.g., during the peer review process.\nWithout using metadata, our method outperforms the best reported results on\nPubMed and DBLP datasets with relative improvements of over 18% in F1@20 and\nover 22% in MRR. We show empirically that, although adding metadata improves\nthe performance on standard metrics, it favors self-citations which are less\nuseful in a citation recommendation setup. We release an online portal\n(http://labs.semanticscholar.org/citeomatic/) for citation recommendation based\non our method, and a new dataset OpenCorpus of 7 million research articles to\nfacilitate future research on this task.", "published": "2018-02-22 21:13:47", "link": "http://arxiv.org/abs/1802.08301v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "High Order Recurrent Neural Networks for Acoustic Modelling", "abstract": "Vanishing long-term gradients are a major issue in training standard\nrecurrent neural networks (RNNs), which can be alleviated by long short-term\nmemory (LSTM) models with memory cells. However, the extra parameters\nassociated with the memory cells mean an LSTM layer has four times as many\nparameters as an RNN with the same hidden vector size. This paper addresses the\nvanishing gradient problem using a high order RNN (HORNN) which has additional\nconnections from multiple previous time steps. Speech recognition experiments\nusing British English multi-genre broadcast (MGB3) data showed that the\nproposed HORNN architectures for rectified linear unit and sigmoid activation\nfunctions reduced word error rates (WER) by 4.2% and 6.3% over the\ncorresponding RNNs, and gave similar WERs to a (projected) LSTM while using\nonly 20%--50% of the recurrent layer parameters and computation.", "published": "2018-02-22 22:01:05", "link": "http://arxiv.org/abs/1802.08314v1", "categories": ["cs.CL", "cs.AI", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sounderfeit: Cloning a Physical Model with Conditional Adversarial\n  Autoencoders", "abstract": "An adversarial autoencoder conditioned on known parameters of a physical\nmodeling bowed string synthesizer is evaluated for use in parameter estimation\nand resynthesis tasks. Latent dimensions are provided to capture variance not\nexplained by the conditional parameters. Results are compared with and without\nthe adversarial training, and a system capable of \"copying\" a given\nparameter-signal bidirectional relationship is examined. A real-time synthesis\nsystem built on a generative, conditioned and regularized neural network is\npresented, allowing to construct engaging sound synthesizers based purely on\nrecorded data.", "published": "2018-02-22 12:24:24", "link": "http://arxiv.org/abs/1802.08008v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
