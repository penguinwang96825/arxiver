{"title": "LLM-Collaboration on Automatic Science Journalism for the General\n  Audience", "abstract": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. However, this\ntask can be challenging as the audience often lacks specific knowledge about\nthe presented research. To address this challenge, we propose a framework that\nintegrates three LLMs mimicking the real-world\nwriting-reading-feedback-revision workflow, with one LLM acting as the\njournalist, a smaller LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including advanced models such as GPT-4.", "published": "2024-07-13 03:31:35", "link": "http://arxiv.org/abs/2407.09756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaskMoE: Boosting Token-Level Learning via Routing Mask in\n  Mixture-of-Experts", "abstract": "Scaling the size of a model enhances its capabilities but significantly\nincreases computation complexity. Mixture-of-Experts models (MoE) address the\nissue by allowing model size to scale up without substantially increasing\ntraining or inference costs. In MoE, there is an important module called the\nrouter, which is used to distribute each token to the experts. Currently, the\nmainstream routing methods include dynamic routing and fixed routing. Despite\ntheir promising results, MoE models encounter several challenges. Primarily,\nfor dynamic routing methods, the dispersion of training tokens across multiple\nexperts can lead to underfitting, particularly for infrequent tokens.\nAdditionally, though fixed routing methods can mitigate that issue, they\ncompromise on the diversity of representations. In this paper, we propose\n\\textbf{MaskMoE}, a method designed to enhance token-level learning by\nemploying a routing \\textbf{mask}ing technique within the\n\\textbf{M}ixture-\\textbf{o}f-\\textbf{E}xperts model. MaskMoE is capable of\nmaintaining representation diversity while achieving more comprehensive\ntraining. Experimental results demonstrate that our method outperforms previous\ndominant Mixture-of-Experts models in terms of both perplexity (PPL) and\ndownstream task performance.", "published": "2024-07-13 09:22:33", "link": "http://arxiv.org/abs/2407.09816v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraFinNLP 2024: The First Arabic Financial NLP Shared Task", "abstract": "The expanding financial markets of the Arab world require sophisticated\nArabic NLP tools. To address this need within the banking domain, the Arabic\nFinancial NLP (AraFinNLP) shared task proposes two subtasks: (i) Multi-dialect\nIntent Detection and (ii) Cross-dialect Translation and Intent Preservation.\nThis shared task uses the updated ArBanking77 dataset, which includes about 39k\nparallel queries in MSA and four dialects. Each query is labeled with one or\nmore of a common 77 intents in the banking domain. These resources aim to\nfoster the development of robust financial Arabic NLP, particularly in the\nareas of machine translation and banking chat-bots. A total of 45 unique teams\nregistered for this shared task, with 11 of them actively participated in the\ntest phase. Specifically, 11 teams participated in Subtask 1, while only 1 team\nparticipated in Subtask 2. The winning team of Subtask 1 achieved F1 score of\n0.8773, and the only team submitted in Subtask 2 achieved a 1.667 BLEU score.", "published": "2024-07-13 09:28:44", "link": "http://arxiv.org/abs/2407.09818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Low-Rank Training in Transformer Language Models:\n  Efficiency and Scaling Analysis", "abstract": "State-of-the-art LLMs often rely on scale with high computational costs,\nwhich has sparked a research agenda to reduce parameter counts and costs\nwithout significantly impacting performance. Our study focuses on\nTransformer-based LLMs, specifically applying low-rank parametrization to the\ncomputationally intensive feedforward networks (FFNs), which are less studied\nthan attention blocks. In contrast to previous works, (i) we explore low-rank\nparametrization at scale, up to 1.3B parameters; (ii) within Transformer\nlanguage models rather than convolutional architectures; and (iii) starting\nfrom training from scratch. Experiments on the large RefinedWeb dataset show\nthat low-rank parametrization is both efficient (e.g., 2.6$\\times$ FFN speed-up\nwith 32\\% parameters) and effective during training. Interestingly, these\nstructured FFNs exhibit steeper scaling curves than the original models.\nMotivated by this finding, we develop the wide and structured networks\nsurpassing the current medium-sized and large-sized Transformer in perplexity\nand throughput performance. Our code is available at\nhttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.", "published": "2024-07-13 10:08:55", "link": "http://arxiv.org/abs/2407.09835v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting", "abstract": "Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.", "published": "2024-07-13 13:03:45", "link": "http://arxiv.org/abs/2407.09879v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks", "abstract": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.", "published": "2024-07-13 13:58:24", "link": "http://arxiv.org/abs/2407.09893v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated\n  Dialogues", "abstract": "This paper investigates the quality of multi-agent dialogues in simulations\npowered by Large Language Models (LLMs). Analyzing dialogues and memory over\nmultiple sessions revealed significant issues such as repetition,\ninconsistency, and hallucination, exacerbated by the propagation of erroneous\ninformation. To combat these challenges, we propose a novel Screening,\nDiagnosis, and Regeneration (SDR) framework that detects and corrects utterance\nerrors through a comprehensive process involving immediate issue\nidentification, evidence gathering from past dialogues, and LLM analysis for\nutterance revision. By incorporating our SDR framework to Generative Agents\n(Park et al., 2023), we enhance the diversity, consistency, and factualness of\nthe generated dialogues. This work presents a pioneering approach to enhancing\ndialogue quality in multi-agent simulations, establishing a new standard for\nfuture research in the field.", "published": "2024-07-13 14:24:45", "link": "http://arxiv.org/abs/2407.09897v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimizing PLM-Based Few-Shot Intent Detectors", "abstract": "Recent research has demonstrated the feasibility of training efficient intent\ndetectors based on pre-trained language model~(PLM) with limited labeled data.\nHowever, deploying these detectors in resource-constrained environments such as\nmobile devices poses challenges due to their large sizes. In this work, we aim\nto address this issue by exploring techniques to minimize the size of PLM-based\nintent detectors trained with few-shot data. Specifically, we utilize large\nlanguage models (LLMs) for data augmentation, employ a cutting-edge model\ncompression method for knowledge distillation, and devise a vocabulary pruning\nmechanism called V-Prune. Through these approaches, we successfully achieve a\ncompression ratio of 21 in model memory usage, including both Transformer and\nthe vocabulary, while maintaining almost identical performance levels on four\nreal-world benchmarks.", "published": "2024-07-13 16:47:20", "link": "http://arxiv.org/abs/2407.09943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs", "abstract": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped, there is a notable lack of region-specific datasets generated by\nnative users in their own languages. This gap hinders the effective\nbenchmarking of LLMs for regional and cultural specificities. Furthermore, it\nalso limits the development of fine-tuned models. In this study, we propose a\nscalable, language-independent framework, NativQA, to seamlessly construct\nculturally and regionally aligned QA datasets in native languages, for LLM\nevaluation and tuning. We demonstrate the efficacy of the proposed framework by\ndesigning a multilingual natural QA dataset, \\mnqa, consisting of ~64k manually\nannotated QA pairs in seven languages, ranging from high to extremely low\nresource, based on queries from native speakers from 9 regions covering 18\ntopics. We benchmark open- and closed-source LLMs with the MultiNativQA\ndataset. We also showcase the framework efficacy in constructing fine-tuning\ndata especially for low-resource and dialectally-rich languages. We made both\nthe framework NativQA and MultiNativQA dataset publicly available for the\ncommunity (https://nativqa.gitlab.io).", "published": "2024-07-13 09:34:00", "link": "http://arxiv.org/abs/2407.09823v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Text-Based Detection of On-Hold Scripts in Contact Center Calls", "abstract": "Average hold time is a concern for call centers because it affects customer\nsatisfaction. Contact centers should instruct their agents to use special\non-hold scripts to maintain positive interactions with clients. This study\npresents a natural language processing model that detects on-hold phrases in\ncustomer service calls transcribed by automatic speech recognition technology.\nThe task of finding hold scripts in dialogue was formulated as a multiclass\ntext classification problem with three mutually exclusive classes: scripts for\nputting a client on hold, scripts for returning to a client, and phrases\nirrelevant to on-hold scripts. We collected an in-house dataset of calls and\nlabeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by\nexploring various hyperparameter sets and achieved high model performance. The\ndeveloped model can help agent monitoring by providing a way to check whether\nan agent follows predefined on-hold scripts.", "published": "2024-07-13 11:11:41", "link": "http://arxiv.org/abs/2407.09849v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building pre-train LLM Dataset for the INDIC Languages: a case study on\n  Hindi", "abstract": "Large language models (LLMs) demonstrated transformative capabilities in many\napplications that require automatically generating responses based on human\ninstruction. However, the major challenge for building LLMs, particularly in\nIndic languages, is the availability of high-quality data for building\nfoundation LLMs. In this paper, we are proposing a large pre-train dataset in\nHindi useful for the Indic language Hindi. We have collected the data span\nacross several domains including major dialects in Hindi. The dataset contains\n1.28 billion Hindi tokens. We have explained our pipeline including data\ncollection, pre-processing, and availability for LLM pre-training. The proposed\napproach can be easily extended to other Indic and low-resource languages and\nwill be available freely for LLM pre-training and LLM research purposes.", "published": "2024-07-13 11:29:20", "link": "http://arxiv.org/abs/2407.09855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP", "abstract": "Natural Language Processing (NLP) research has traditionally been\npredominantly focused on English, driven by the availability of resources, the\nsize of the research community, and market demands. Recently, there has been a\nnoticeable shift towards multilingualism in NLP, recognizing the need for\ninclusivity and effectiveness across diverse languages and cultures.\nMonolingual surveys have the potential to complement the broader trend towards\nmultilingualism in NLP by providing foundational insights and resources,\nnecessary for effectively addressing the linguistic diversity of global\ncommunication. However, monolingual NLP surveys are extremely rare in the\nliterature. This study introduces a generalizable methodology for creating\nsystematic and comprehensive monolingual NLP surveys, aimed at optimizing the\nprocess of constructing such surveys and thoroughly addressing a language's NLP\nsupport. Our approach integrates a structured search protocol to avoid\nselection bias and ensure reproducibility, an NLP task taxonomy to organize the\nsurveyed material coherently, and language resources (LRs) taxonomies to\nidentify potential benchmarks and highlight opportunities for improving\nresource availability (e.g., through better maintenance or licensing). We apply\nthis methodology to Greek NLP (2012-2023), providing a comprehensive overview\nof its current state and challenges. We discuss the progress of Greek NLP and\noutline the Greek LRs found, classified by availability and usability,\nassessing language support per NLP task. The presented systematic literature\nreview of Greek NLP serves as an application of our method that showcases the\nbenefits of monolingual NLP surveys more broadly. Similar applications could be\nconsidered for the myriads of languages whose progress in NLP lags behind that\nof well-supported languages.", "published": "2024-07-13 12:01:52", "link": "http://arxiv.org/abs/2407.09861v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FarFetched: Entity-centric Reasoning and Claim Validation for the Greek\n  Language based on Textually Represented Environments", "abstract": "Our collective attention span is shortened by the flood of online\ninformation. With \\textit{FarFetched}, we address the need for automated claim\nvalidation based on the aggregated evidence derived from multiple online news\nsources. We introduce an entity-centric reasoning framework in which latent\nconnections between events, actions, or statements are revealed via entity\nmentions and represented in a graph database. Using entity linking and semantic\nsimilarity, we offer a way for collecting and combining information from\ndiverse sources in order to generate evidence relevant to the user's claim.\nThen, we leverage textual entailment recognition to quantitatively determine\nwhether this assertion is credible, based on the created evidence. Our approach\ntries to fill the gap in automated claim validation for less-resourced\nlanguages and is showcased on the Greek language, complemented by the training\nof relevant semantic textual similarity (STS) and natural language inference\n(NLI) models that are evaluated on translated versions of common benchmarks.", "published": "2024-07-13 13:30:20", "link": "http://arxiv.org/abs/2407.09888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WojoodNER 2024: The Second Arabic Named Entity Recognition Shared Task", "abstract": "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.", "published": "2024-07-13 16:17:08", "link": "http://arxiv.org/abs/2407.09936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Document-level Clinical Entity and Relation Extraction via Knowledge\n  Base-Guided Generation", "abstract": "Generative pre-trained transformer (GPT) models have shown promise in\nclinical entity and relation extraction tasks because of their precise\nextraction and contextual understanding capability. In this work, we further\nleverage the Unified Medical Language System (UMLS) knowledge base to\naccurately identify medical concepts and improve clinical entity and relation\nextraction at the document level. Our framework selects UMLS concepts relevant\nto the text and combines them with prompts to guide language models in\nextracting entities. Our experiments demonstrate that this initial concept\nmapping and the inclusion of these mapped concepts in the prompts improves\nextraction results compared to few-shot extraction tasks on generic language\nmodels that do not leverage UMLS. Further, our results show that this approach\nis more effective than the standard Retrieval Augmented Generation (RAG)\ntechnique, where retrieved data is compared with prompt embeddings to generate\nresults. Overall, we find that integrating UMLS concepts with GPT models\nsignificantly improves entity and relation identification, outperforming the\nbaseline and RAG models. By combining the precise concept mapping capability of\nknowledge-based approaches like UMLS with the contextual understanding\ncapability of GPT, our method highlights the potential of these approaches in\nspecialized domains like healthcare.", "published": "2024-07-13 22:45:46", "link": "http://arxiv.org/abs/2407.10021v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bilingual Adaptation of Monolingual Foundation Models", "abstract": "We present an efficient method for adapting a monolingual Large Language\nModel (LLM) to another language, addressing challenges of catastrophic\nforgetting and tokenizer limitations. We focus this study on adapting Llama 2\nto Arabic. Our two-stage approach begins with expanding the vocabulary and\ntraining only the embeddings matrix, followed by full model continual\npre-training on a bilingual corpus. By continually pre-training on a mix of\nArabic and English corpora, the model retains its proficiency in English while\nacquiring capabilities in Arabic. Our approach results in significant\nimprovements in Arabic and slight enhancements in English, demonstrating\ncost-effective cross-lingual transfer. We perform ablations on embedding\ninitialization techniques, data mix ratios, and learning rates and release a\ndetailed training recipe. To demonstrate generalizability of this approach we\nalso adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.", "published": "2024-07-13 21:09:38", "link": "http://arxiv.org/abs/2407.12869v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Mitigating Code LLM Hallucinations with API Documentation", "abstract": "In this study, we address the issue of API hallucinations in various software\nengineering contexts. We introduce CloudAPIBench, a new benchmark designed to\nmeasure API hallucination occurrences. CloudAPIBench also provides annotations\nfor frequencies of API occurrences in the public domain, allowing us to study\nAPI hallucinations at various frequency levels. Our findings reveal that Code\nLLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58%\nvalid low frequency API invocations. We demonstrate that Documentation\nAugmented Generation (DAG) significantly improves performance for low frequency\nAPIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs\nwhen using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this,\nwe propose to intelligently trigger DAG where we check against an API index or\nleverage Code LLMs' confidence scores to retrieve only when needed. We\ndemonstrate that our proposed methods enhance the balance between low and high\nfrequency API performance, resulting in more reliable API invocations (8.20%\nabsolute improvement on CloudAPIBench for GPT-4o).", "published": "2024-07-13 00:16:26", "link": "http://arxiv.org/abs/2407.09726v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech\n  Recognition System", "abstract": "Multi-talker speech recognition and target-talker speech recognition, both\ninvolve transcription in multi-talker contexts, remain significant challenges.\nHowever, existing methods rarely attempt to simultaneously address both tasks.\nIn this study, we propose a pioneering approach to empower Whisper, which is a\nspeech foundation model, to tackle joint multi-talker and target-talker speech\nrecognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar\nseparator into its encoder to separate mixed embedding for multiple talkers;\n(ii) a Target Talker Identifier is introduced to identify the embedding flow of\nthe target talker on the fly, requiring only three-second enrollment speech as\na cue; (iii) soft prompt tuning for decoder is explored for better task\nadaptation. Our method outperforms previous methods on two- and three-talker\nLibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable\nzero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.", "published": "2024-07-13 09:28:24", "link": "http://arxiv.org/abs/2407.09817v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-Copilot: Leveraging Large Language Models for Speech Processing\n  via Task Decomposition, Modularization, and Program Generation", "abstract": "In this work, we introduce Speech-Copilot, a modular framework for\ninstruction-oriented speech-processing tasks that minimizes human effort in\ntoolset construction. Unlike end-to-end methods using large audio-language\nmodels, Speech-Copilot builds speech processing-specific toolsets by analyzing\npre-collected task instructions and breaking tasks into manageable sub-tasks.\nIt features a flexible agent based on large language models that performs tasks\nthrough program generation. Our approach achieves state-of-the-art performance\non the Dynamic-SUPERB benchmark, demonstrating its effectiveness across diverse\nspeech-processing tasks. Key contributions include: 1) developing an innovative\nframework for speech processing-specific toolset construction, 2) establishing\na high-performing agent based on large language models, and 3) offering a new\nperspective on addressing challenging instruction-oriented speech-processing\ntasks. Without additional training processes required by end-to-end approaches,\nour method provides a flexible and extendable solution for a wide range of\nspeech-processing applications.", "published": "2024-07-13 13:26:43", "link": "http://arxiv.org/abs/2407.09886v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transferring Structure Knowledge: A New Task to Fake news Detection\n  Towards Cold-Start Propagation", "abstract": "Many fake news detection studies have achieved promising performance by\nextracting effective semantic and structure features from both content and\npropagation trees. However, it is challenging to apply them to practical\nsituations, especially when using the trained propagation-based models to\ndetect news with no propagation data. Towards this scenario, we study a new\ntask named cold-start fake news detection, which aims to detect content-only\nsamples with missing propagation. To achieve the task, we design a simple but\neffective Structure Adversarial Net (SAN) framework to learn transferable\nfeatures from available propagation to boost the detection of content-only\nsamples. SAN introduces a structure discriminator to estimate dissimilarities\namong learned features with and without propagation, and further learns\nstructure-invariant features to enhance the generalization of existing\npropagation-based methods for content-only samples. We conduct qualitative and\nquantitative experiments on three datasets. Results show the challenge of the\nnew task and the effectiveness of our SAN framework.", "published": "2024-07-13 14:04:55", "link": "http://arxiv.org/abs/2407.09894v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Fine-grained Analysis of In-context Linear Estimation: Data,\n  Architecture, and Beyond", "abstract": "Recent research has shown that Transformers with linear attention are capable\nof in-context learning (ICL) by implementing a linear estimator through\ngradient descent steps. However, the existing results on the optimization\nlandscape apply under stylized settings where task and feature vectors are\nassumed to be IID and the attention weights are fully parameterized. In this\nwork, we develop a stronger characterization of the optimization and\ngeneralization landscape of ICL through contributions on architectures,\nlow-rank parameterization, and correlated designs: (1) We study the landscape\nof 1-layer linear attention and 1-layer H3, a state-space model. Under a\nsuitable correlated design assumption, we prove that both implement 1-step\npreconditioned gradient descent. We show that thanks to its native convolution\nfilters, H3 also has the advantage of implementing sample weighting and\noutperforming linear attention in suitable settings. (2) By studying correlated\ndesigns, we provide new risk bounds for retrieval augmented generation (RAG)\nand task-feature alignment which reveal how ICL sample complexity benefits from\ndistributional alignment. (3) We derive the optimal risk for low-rank\nparameterized attention weights in terms of covariance spectrum. Through this,\nwe also shed light on how LoRA can adapt to a new distribution by capturing the\nshift between task covariances. Experimental results corroborate our\ntheoretical findings. Overall, this work explores the optimization and risk\nlandscape of ICL in practically meaningful settings and contributes to a more\nthorough understanding of its mechanics.", "published": "2024-07-13 21:13:55", "link": "http://arxiv.org/abs/2407.10005v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "Causality extraction from medical text using Large Language Models\n  (LLMs)", "abstract": "This study explores the potential of natural language models, including large\nlanguage models, to extract causal relations from medical texts, specifically\nfrom Clinical Practice Guidelines (CPGs). The outcomes causality extraction\nfrom Clinical Practice Guidelines for gestational diabetes are presented,\nmarking a first in the field. We report on a set of experiments using variants\nof BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),\nnamely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better\nthan other models, including the Large Language Models, with an average\nF1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less\nconsistency. We also release the code and an annotated a corpus of causal\nstatements within the Clinical Practice Guidelines for gestational diabetes.", "published": "2024-07-13 22:33:29", "link": "http://arxiv.org/abs/2407.10020v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Beyond KV Caching: Shared Attention for Efficient LLMs", "abstract": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.", "published": "2024-07-13 07:23:07", "link": "http://arxiv.org/abs/2407.12866v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Autonomous GIS Agent Framework for Geospatial Data Retrieval", "abstract": "Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.", "published": "2024-07-13 14:23:57", "link": "http://arxiv.org/abs/2407.21024v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.IR"}
{"title": "IoT-LM: Large Multisensory Language Models for the Internet of Things", "abstract": "The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.", "published": "2024-07-13 08:20:37", "link": "http://arxiv.org/abs/2407.09801v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "CUSIDE-array: A Streaming Multi-Channel End-to-End Speech Recognition\n  System with Realistic Evaluations", "abstract": "Recently multi-channel end-to-end (ME2E) ASR systems have emerged. While\nstreaming single-channel end-to-end ASR has been extensively studied, streaming\nME2E ASR is limited in exploration. Additionally, recent studies call attention\nto the gap between in-distribution (ID) and out-of-distribution (OOD) tests and\ndoing realistic evaluations. This paper focuses on two research problems:\nrealizing streaming ME2E ASR and improving OOD generalization. We propose the\nCUSIDE-array method, which integrates the recent CUSIDE methodology (Chunking,\nSimulating Future Context and Decoding) into the neural beamformer approach of\nME2E ASR. It enables streaming processing of both front-end and back-end with a\ntotal latency of 402ms. The CUSIDE-array ME2E models are shown to achieve\nsuperior streaming results in both ID and OOD tests. Realistic evaluations\nconfirm the advantage of CUSIDE-array in its capability to consume\nsingle-channel data to improve OOD generalization via back-end pre-training and\nME2E fine-tuning.", "published": "2024-07-13 08:53:54", "link": "http://arxiv.org/abs/2407.09807v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for\n  Speech Separation, Recognition, and Synthesis", "abstract": "It is too early to conclude that Mamba is a better alternative to\ntransformers for speech before comparing Mamba with transformers in terms of\nboth performance and efficiency in multiple speech-related tasks. To reach this\nconclusion, we propose and evaluate three models for three tasks: Mamba-TasNet\nfor speech separation, ConMamba for speech recognition, and VALL-M for speech\nsynthesis. We compare them with transformers of similar sizes in performance,\nmemory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable\nor higher performance than their transformer counterparts: Sepformer,\nConformer, and VALL-E. They are more efficient than transformers in memory and\nspeed for speech longer than a threshold duration, inversely related to the\nresolution of a speech token. Mamba for separation is the most efficient, and\nMamba for recognition is the least. Further, we show that Mamba is not more\nefficient than transformer for speech shorter than the threshold duration and\nperforms worse in models that require joint modeling of text and speech, such\nas cross or masked attention of two inputs. Therefore, we argue that the\nsuperiority of Mamba or transformer depends on particular problems and models.\nCode available at https://github.com/xi-j/Mamba-TasNet and\nhttps://github.com/xi-j/Mamba-ASR.", "published": "2024-07-13 00:35:21", "link": "http://arxiv.org/abs/2407.09732v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
