{"title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation\n  Tasks", "abstract": "Neural sequence to sequence learning recently became a very promising\nparadigm in machine translation, achieving competitive results with statistical\nphrase-based systems. In this system description paper, we attempt to utilize\nseveral recently published methods used for neural sequential learning in order\nto build systems for WMT 2016 shared tasks of Automatic Post-Editing and\nMultimodal Machine Translation.", "published": "2016-06-23 21:23:29", "link": "http://arxiv.org/abs/1606.07481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward a Deep Neural Approach for Knowledge-Based IR", "abstract": "This paper tackles the problem of the semantic gap between a document and a\nquery within an ad-hoc information retrieval task. In this context, knowledge\nbases (KBs) have already been acknowledged as valuable means since they allow\nthe representation of explicit relations between entities. However, they do not\nnecessarily represent implicit relations that could be hidden in a corpora.\nThis latter issue is tackled by recent works dealing with deep representation\nlearn ing of texts. With this in mind, we argue that embedding KBs within deep\nneural architectures supporting documentquery matching would give rise to\nfine-grained latent representations of both words and their semantic relations.\nIn this paper, we review the main approaches of neural-based document ranking\nas well as those approaches for latent representation of entities and relations\nvia KBs. We then propose some avenues to incorporate KBs in deep neural\napproaches for document ranking. More particularly, this paper advocates that\nKBs can be used either to support enhanced latent representations of queries\nand documents based on both distributional and relational semantics or to serve\nas a semantic translator between their latent distributional representations.", "published": "2016-06-23 07:21:28", "link": "http://arxiv.org/abs/1606.07211v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "NN-grams: Unifying neural network and n-gram language models for Speech\n  Recognition", "abstract": "We present NN-grams, a novel, hybrid language model integrating n-grams and\nneural networks (NN) for speech recognition. The model takes as input both word\nhistories as well as n-gram counts. Thus, it combines the memorization capacity\nand scalability of an n-gram model with the generalization ability of neural\nnetworks. We report experiments where the model is trained on 26B words.\nNN-grams are efficient at run-time since they do not include an output soft-max\nlayer. The model is trained using noise contrastive estimation (NCE), an\napproach that transforms the estimation problem of neural networks into one of\nbinary classification between data samples and noise samples. We present\nresults with noise samples derived from either an n-gram distribution or from\nspeech recognition lattices. NN-grams outperforms an n-gram model on an Italian\nspeech recognition dictation task.", "published": "2016-06-23 20:37:06", "link": "http://arxiv.org/abs/1606.07470v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Gender and Interest Targeting for Sponsored Post Advertising at Tumblr", "abstract": "As one of the leading platforms for creative content, Tumblr offers\nadvertisers a unique way of creating brand identity. Advertisers can tell their\nstory through images, animation, text, music, video, and more, and promote that\ncontent by sponsoring it to appear as an advertisement in the streams of Tumblr\nusers. In this paper we present a framework that enabled one of the key\ntargeted advertising components for Tumblr, specifically gender and interest\ntargeting. We describe the main challenges involved in development of the\nframework, which include creating the ground truth for training gender\nprediction models, as well as mapping Tumblr content to an interest taxonomy.\nFor purposes of inferring user interests we propose a novel semi-supervised\nneural language model for categorization of Tumblr content (i.e., post tags and\npost keywords). The model was trained on a large-scale data set consisting of\n6.8 billion user posts, with very limited amount of categorized keywords, and\nwas shown to have superior performance over the bag-of-words model. We\nsuccessfully deployed gender and interest targeting capability in Yahoo\nproduction systems, delivering inference for users that cover more than 90% of\ndaily activities at Tumblr. Online performance results indicate advantages of\nthe proposed approach, where we observed 20% lift in user engagement with\nsponsored posts as compared to untargeted campaigns.", "published": "2016-06-23 05:03:03", "link": "http://arxiv.org/abs/1606.07189v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "H.2.8"], "primary_category": "cs.CL"}
{"title": "Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions", "abstract": "In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset.", "published": "2016-06-23 12:25:09", "link": "http://arxiv.org/abs/1606.07287v1", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.IR"}
{"title": "Analyzing the Behavior of Visual Question Answering Models", "abstract": "Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n-- with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n  Our behavior analysis reveals that despite recent progress, today's VQA\nmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump\nto conclusions\" (converge on a predicted answer after 'listening' to just half\nthe question), and are \"stubborn\" (do not change their answers across images).", "published": "2016-06-23 16:05:16", "link": "http://arxiv.org/abs/1606.07356v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in\n  Recurrent Neural Networks", "abstract": "Recurrent neural networks, and in particular long short-term memory (LSTM)\nnetworks, are a remarkably effective tool for sequence modeling that learn a\ndense black-box hidden representation of their sequential input. Researchers\ninterested in better understanding these models have studied the changes in\nhidden state representations over time and noticed some interpretable patterns\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\ntool for recurrent neural networks with a focus on understanding these hidden\nstate dynamics. The tool allows users to select a hypothesis input range to\nfocus on local state changes, to match these states changes to similar patterns\nin a large data set, and to align these results with structural annotations\nfrom their domain. We show several use cases of the tool for analyzing specific\nhidden state properties on dataset containing nesting, phrase structure, and\nchord progressions, and demonstrate how the tool can be used to isolate\npatterns for further statistical analysis. We characterize the domain, the\ndifferent stakeholders, and their goals and tasks.", "published": "2016-06-23 20:20:39", "link": "http://arxiv.org/abs/1606.07461v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "abstract": "Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense.", "published": "2016-06-23 21:54:44", "link": "http://arxiv.org/abs/1606.07493v5", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Predictions of Non-Linear Classifiers in NLP", "abstract": "Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.", "published": "2016-06-23 12:53:31", "link": "http://arxiv.org/abs/1606.07298v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "abstract": "While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.", "published": "2016-06-23 22:04:08", "link": "http://arxiv.org/abs/1606.07496v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "cs.NE", "H.2.8; H.3.3; I.2.6"], "primary_category": "cs.CV"}
