{"title": "Comparing Intrinsic Gender Bias Evaluation Measures without using Human\n  Annotated Examples", "abstract": "Numerous types of social biases have been identified in pre-trained language\nmodels (PLMs), and various intrinsic bias evaluation measures have been\nproposed for quantifying those social biases. Prior works have relied on human\nannotated examples to compare existing intrinsic bias evaluation measures.\nHowever, this approach is not easily adaptable to different languages nor\namenable to large scale evaluations due to the costs and difficulties when\nrecruiting human annotators. To overcome this limitation, we propose a method\nto compare intrinsic gender bias evaluation measures without relying on\nhuman-annotated examples. Specifically, we create multiple bias-controlled\nversions of PLMs using varying amounts of male vs. female gendered sentences,\nmined automatically from an unannotated corpus using gender-related word lists.\nNext, each bias-controlled PLM is evaluated using an intrinsic bias evaluation\nmeasure, and the rank correlation between the computed bias scores and the\ngender proportions used to fine-tune the PLMs is computed. Experiments on\nmultiple corpora and PLMs repeatedly show that the correlations reported by our\nproposed method that does not require human annotated examples are comparable\nto those computed using human annotated examples in prior work.", "published": "2023-01-28 03:11:50", "link": "http://arxiv.org/abs/2301.12074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark\n  Datasets", "abstract": "We investigate five English NLP benchmark datasets (on the superGLUE\nleaderboard) and two Swedish datasets for bias, along multiple axes. The\ndatasets are the following: Boolean Question (Boolq), CommitmentBank (CB),\nWinograd Schema Challenge (WSC), Wino-gender diagnostic (AXg), Recognising\nTextual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is\nknown to be common in data, which ML models learn from. In order to mitigate\nbias in data, it is crucial to be able to estimate it objectively. We use\nbipol, a novel multi-axes bias metric with explainability, to estimate and\nexplain how much bias exists in these datasets. Multilingual, multi-axes bias\nevaluation is not very common. Hence, we also contribute a new, large Swedish\nbias-labelled dataset (of 2 million samples), translated from the English\nversion and train the SotA mT5 model on it. In addition, we contribute new\nmulti-axes lexica for bias detection in Swedish. We make the codes, model, and\nnew dataset publicly available.", "published": "2023-01-28 09:28:19", "link": "http://arxiv.org/abs/2301.12139v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Sentence Transformer as A Multilingual Word Aligner", "abstract": "Multilingual pretrained language models (mPLMs) have shown their\neffectiveness in multilingual word alignment induction. However, these methods\nusually start from mBERT or XLM-R. In this paper, we investigate whether\nmultilingual sentence Transformer LaBSE is a strong multilingual word aligner.\nThis idea is non-trivial as LaBSE is trained to learn language-agnostic\nsentence-level embeddings, while the alignment extraction task requires the\nmore fine-grained word-level embeddings to be language-agnostic. We demonstrate\nthat the vanilla LaBSE outperforms other mPLMs currently used in the alignment\ntask, and then propose to finetune LaBSE on parallel corpus for further\nimprovement. Experiment results on seven language pairs show that our best\naligner outperforms previous state-of-the-art models of all varieties. In\naddition, our aligner supports different language pairs in a single model, and\neven achieves new state-of-the-art on zero-shot language pairs that does not\nappear in the finetuning process.", "published": "2023-01-28 09:28:55", "link": "http://arxiv.org/abs/2301.12140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Tagging with LSTM-CRF", "abstract": "In the present paper, two models are presented namely LSTM-CRF and\nBERT-LSTM-CRF for semantic tagging of universal semantic tag dataset. The\nexperiments show that the first model is much easier to converge while the\nsecond model that leverages BERT embedding, takes a long time to converge and\nneeds a big dataset for semtagging to be effective.", "published": "2023-01-28 14:06:17", "link": "http://arxiv.org/abs/2301.12206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing for Conversational Question Answering over Knowledge\n  Graphs", "abstract": "In this paper, we are interested in developing semantic parsers which\nunderstand natural language questions embedded in a conversation with a user\nand ground them to formal queries over definitions in a general purpose\nknowledge graph (KG) with very large vocabularies (covering thousands of\nconcept names and relations, and millions of entities). To this end, we develop\na dataset where user questions are annotated with Sparql parses and system\nanswers correspond to execution results thereof. We present two different\nsemantic parsing approaches and highlight the challenges of the task: dealing\nwith large vocabularies, modelling conversation context, predicting queries\nwith multiple entities, and generalising to new questions at test time. We hope\nour dataset will serve as useful testbed for the development of conversational\nsemantic parsers. Our dataset and models are released at\nhttps://github.com/EdinburghNLP/SPICE.", "published": "2023-01-28 14:45:11", "link": "http://arxiv.org/abs/2301.12217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Presence of informal language, such as emoticons, hashtags, and slang,\n  impact the performance of sentiment analysis models on social media text?", "abstract": "This study aimed to investigate the influence of the presence of informal\nlanguage, such as emoticons and slang, on the performance of sentiment analysis\nmodels applied to social media text. A convolutional neural network (CNN) model\nwas developed and trained on three datasets: a sarcasm dataset, a sentiment\ndataset, and an emoticon dataset. The model architecture was held constant for\nall experiments and the model was trained on 80% of the data and tested on 20%.\nThe results revealed that the model achieved an accuracy of 96.47% on the\nsarcasm dataset, with the lowest accuracy for class 1. On the sentiment\ndataset, the model achieved an accuracy of 95.28%. The amalgamation of sarcasm\nand sentiment datasets improved the accuracy of the model to 95.1%, and the\naddition of emoticon dataset has a slight positive impact on the accuracy of\nthe model to 95.37%. The study suggests that the presence of informal language\nhas a restricted impact on the performance of sentiment analysis models applied\nto social media text. However, the inclusion of emoticon data to the model can\nenhance the accuracy slightly.", "published": "2023-01-28 22:21:51", "link": "http://arxiv.org/abs/2301.12303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MQAG: Multiple-choice Question Answering and Generation for Assessing\n  Information Consistency in Summarization", "abstract": "State-of-the-art summarization systems can generate highly fluent summaries.\nThese summaries, however, may contain factual inconsistencies and/or\ninformation not present in the source. Hence, an important component of\nassessing the quality of summaries is to determine whether there is information\nconsistency between the source and the summary. Existing approaches are\ntypically based on lexical matching or representation-based methods. In this\nwork, we introduce an alternative scheme based on standard\ninformation-theoretic measures in which the information present in the source\nand summary is directly compared. We propose a Multiple-choice Question\nAnswering and Generation framework, MQAG, which approximates the information\nconsistency by computing the expected statistical distance between summary and\nsource answer distributions over automatically generated multiple-choice\nquestions. This approach exploits multiple-choice answer probabilities, as\npredicted answer distributions can be compared. We conduct experiments on four\nsummary evaluation datasets: QAG-CNNDM/XSum, XSum-Hallucination, Podcast\nAssessment, and SummEval. Experiments show that MQAG, using models trained on\nSQuAD or RACE, outperforms existing evaluation methods on the majority of\ntasks.", "published": "2023-01-28 23:08:25", "link": "http://arxiv.org/abs/2301.12307v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making\n  using Language Guided World Modelling", "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior\nknowledge of the world. However, if initialized with knowledge of high-level\nsubgoals and transitions between subgoals, RL agents could utilize this\nAbstract World Model (AWM) for planning and exploration. We propose using\nfew-shot large language models (LLMs) to hypothesize an AWM, that will be\nverified through world experience, to improve sample efficiency of RL agents.\nOur DECKARD agent applies LLM-guided exploration to item crafting in Minecraft\nin two phases: (1) the Dream phase where the agent uses an LLM to decompose a\ntask into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase\nwhere the agent learns a modular policy for each subgoal and verifies or\ncorrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and\nthen verifying the AWM based on agent experience not only increases sample\nefficiency over contemporary methods by an order of magnitude but is also\nrobust to and corrects errors in the LLM, successfully blending noisy\ninternet-scale information from LLMs with knowledge grounded in environment\ndynamics.", "published": "2023-01-28 02:04:07", "link": "http://arxiv.org/abs/2301.12050v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On Pre-trained Language Models for Antibody", "abstract": "Antibodies are vital proteins offering robust protection for the human body\nfrom pathogens. The development of general protein and antibody-specific\npre-trained language models both facilitate antibody prediction tasks. However,\nthere have been limited studies that comprehensively explore the representation\ncapability of distinct pre-trained language models on different antibody tasks.\nTo investigate the problem, we aim to answer several key questions in this\npaper, such as how pre-trained language models perform in antibody tasks with\ndifferent specificity and how introducing specific biological mechanisms to the\npre-training process can benefit the model. Additionally, we evaluate if the\nlearned antibody pre-trained representations can be applied to real-world\nantibody problems, like drug discovery and immune process understanding.\nPreviously, no benchmark available largely hindered the study to answer these\nquestions. To aid in our investigation, we provide an AnTibody Understanding\nEvaluation (ATUE) benchmark. We comprehensively evaluate the performance of\nprotein pre-trained language models by empirical study along with conclusions\nand new insights. Our ATUE and code are released at\nhttps://github.com/dqwang122/EATLM.", "published": "2023-01-28 07:05:15", "link": "http://arxiv.org/abs/2301.12112v2", "categories": ["cs.CL", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "Underwater Robotics Semantic Parser Assistant", "abstract": "Semantic parsing is a means of taking natural language and putting it in a\nform that a computer can understand. There has been a multitude of approaches\nthat take natural language utterances and form them into lambda calculus\nexpressions -- mathematical functions to describe logic. Here, we experiment\nwith a sequence to sequence model to take natural language utterances, convert\nthose to lambda calculus expressions, when can then be parsed, and place them\nin an XML format that can be used by a finite state machine. Experimental\nresults show that we can have a high accuracy model such that we can bridge the\ngap between technical and nontechnical individuals in the robotics field.", "published": "2023-01-28 09:04:43", "link": "http://arxiv.org/abs/2301.12134v1", "categories": ["cs.CL", "cs.AI", "I.2.7; C.3"], "primary_category": "cs.CL"}
{"title": "How learners produce data from text in classifying clickbait", "abstract": "Text provides a compelling example of unstructured data that can be used to\nmotivate and explore classification problems. Challenges arise regarding the\nrepresentation of features of text and student linkage between text\nrepresentations as character strings and identification of features that embed\nconnections with underlying phenomena. In order to observe how students reason\nwith text data in scenarios designed to elicit certain aspects of the domain,\nwe employed a task-based interview method using a structured protocol with six\npairs of undergraduate students. Our goal was to shed light on students'\nunderstanding of text as data using a motivating task to classify headlines as\n\"clickbait\" or \"news\". Three types of features (function, content, and form)\nsurfaced, the majority from the first scenario. Our analysis of the interviews\nindicates that this sequence of activities engaged the participants in thinking\nat both the human-perception level and the computer-extraction level and\nconceptualizing connections between them.", "published": "2023-01-28 20:23:39", "link": "http://arxiv.org/abs/2302.01292v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "AutoPEFT: Automatic Configuration Search for Parameter-Efficient\n  Fine-Tuning", "abstract": "Large pretrained language models are widely used in downstream NLP tasks via\ntask-specific fine-tuning, but such procedures can be costly. Recently,\nParameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task\nperformance while updating much fewer parameters than full model fine-tuning\n(FFT). However, it is non-trivial to make informed design choices on the PEFT\nconfigurations, such as their architecture, the number of tunable parameters,\nand even the layers in which the PEFT modules are inserted. Consequently, it is\nhighly likely that the current, manually designed configurations are suboptimal\nin terms of their performance-efficiency trade-off. Inspired by advances in\nneural architecture search, we propose AutoPEFT for automatic PEFT\nconfiguration selection: we first design an expressive configuration search\nspace with multiple representative PEFT modules as building blocks. Using\nmulti-objective Bayesian optimisation in a low-cost setup, we then discover a\nPareto-optimal set of configurations with strong performance-cost trade-offs\nacross different numbers of parameters that are also highly transferable across\ndifferent tasks. Empirically, on GLUE and SuperGLUE tasks, we show that\nAutoPEFT-discovered configurations significantly outperform existing PEFT\nmethods and are on par or better than FFT without incurring substantial\ntraining efficiency costs.", "published": "2023-01-28 08:51:23", "link": "http://arxiv.org/abs/2301.12132v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Orcas Have Semantic Language? Machine Learning to Predict Orca\n  Behaviors Using Partially Labeled Vocalization Data", "abstract": "Orcinus orca (killer whales) exhibit complex calls. They last about a second.\nIn a call, an orca typically uses multiple frequencies simultaneously, varies\nthe frequencies, and varies their volumes. Behavior data is hard to obtain\nbecause orcas live under water and travel quickly. Sound data is relatively\neasy to capture. As a science goal, we would like to know whether orca\nvocalizations constitute a semantic language. We do this by studying whether\nmachine learning can predict behavior from vocalizations. Such prediction would\nalso help scientific research and safety applications because one would like to\npredict behavior while only having to capture sound. A significant challenge in\nthis process is lack of labeled data. We work with recent recordings of McMurdo\nSound orcas [Wellard et al. 2020] where each recording is labeled with the\nbehaviors observed during the recording. This yields a dataset where sound\nsegments - continuous vocalizations that can be thought of as call sequences or\nmore general structures - within the recordings are labeled with superfluous\nbehaviors. Despite that, with a careful combination of recent machine learning\ntechniques, we achieve 96.4% classification accuracy. This suggests that orcas\ndo use a semantic language. It is also promising for research and applications.", "published": "2023-01-28 06:04:22", "link": "http://arxiv.org/abs/2302.10983v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "who is snoring? snore based user recognition", "abstract": "Snoring is one of the most prominent symptoms of Obstructive Sleep\nApnea-Hypopnea Syndrome (OSAH), a highly prevalent disease that causes\nrepetitive collapse and cessation of the upper airway. Thus, accurate snore\nsound monitoring and analysis is crucial. However, the traditional monitoring\nmethod polysomnography (PSG) requires the patients to stay at a sleep clinic\nfor the whole night and be connected to many pieces of equipment. An\nalternative and less invasive way is passive monitoring using a smartphone at\nhome or in the clinical settings. But, there is a challenge: the environment\nmay be shared by people such that the raw audio may contain the snore\nactivities of the bed partner or other person. False capturing of the snoring\nactivity could lead to critical false alarms and misdiagnosis of the patients.\nTo address this limitation, we propose a hypothesis that snore sound contains\nunique identity information which can be used for user recognition. We analyzed\nvarious machine learning models: Gaussian Mixture Model (GMM), GMM-UBM\n(Universial Background Model), and a Deep Neural Network (DNN) on MPSSC - an\nopen source snoring dataset to evaluate the validity of our hypothesis. Our\nresults are promising as we achieved around 90% accuracy in identification and\nverification tasks. This work marks the first step towards understanding the\npracticality of snore based user monitoring to enable multiple healthcare\napplicaitons.", "published": "2023-01-28 14:28:57", "link": "http://arxiv.org/abs/2301.12209v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-domain Neural Pitch and Periodicity Estimation", "abstract": "Pitch is a foundational aspect of our perception of audio signals. Pitch\ncontours are commonly used to analyze speech and music signals and as input\nfeatures for many audio tasks, including music transcription, singing voice\nsynthesis, and prosody editing. In this paper, we describe a set of techniques\nfor improving the accuracy of widely-used neural pitch and periodicity\nestimators to achieve state-of-the-art performance on both speech and music. We\nalso introduce a novel entropy-based method for extracting periodicity and\nper-frame voiced-unvoiced classifications from statistical inference-based\npitch estimators (e.g., neural networks), and show how to train a neural pitch\nestimator to simultaneously handle both speech and music data (i.e.,\ncross-domain estimation) without performance degradation. Our estimator\nimplementations run 11.2x faster than real-time on a Intel i9-9820X 10-core\n3.30 GHz CPU$\\unicode{x2014}$approaching the speed of state-of-the-art\nDSP-based pitch estimators$\\unicode{x2014}$or 408x faster than real-time on a\nNVIDIA GeForce RTX 3090 GPU. We release all of our code and models as\nPitch-Estimating Neural Networks (penn), an open-source, pip-installable Python\nmodule for training, evaluating, and performing inference with pitch- and\nperiodicity-estimating neural networks. The code for penn is available at\nhttps://github.com/interactiveaudiolab/penn.", "published": "2023-01-28 17:30:47", "link": "http://arxiv.org/abs/2301.12258v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automated Arrangements of Multi-Part Music for Sets of Monophonic\n  Instruments", "abstract": "Arranging music for a different set of instruments that it was originally\nwritten for is traditionally a tedious and time-consuming process, performed by\nexperts with intricate knowledge of the specific instruments and involving\nsignificant experimentation. In this paper we study the problem of automating\nmusic arrangements for music pieces written for monophonic instruments or\nvoices. We designed and implemented an algorithm that can always produce a\nmusic arrangement when feasible by transposing the music piece to a different\nscale, permuting the assigned parts to instruments/voices, and transposing\nindividual parts by one or more octaves. We also published open source software\nwritten in Python that processes MusicXML files and allows musicians to\nexperiment with music arrangements. It is our hope that our software can serve\nas a platform for future extensions that will include music reductions and\ninclusion of polyphonic instruments.", "published": "2023-01-28 04:13:45", "link": "http://arxiv.org/abs/2301.12084v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
