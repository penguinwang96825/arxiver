{"title": "Linguistic Reflexes of Well-Being and Happiness in Echo", "abstract": "Different theories posit different sources for feelings of well-being and\nhappiness. Appraisal theory grounds our emotional responses in our goals and\ndesires and their fulfillment, or lack of fulfillment. Self Determination\ntheory posits that the basis for well-being rests on our assessment of our\ncompetence, autonomy, and social connection. And surveys that measure happiness\nempirically note that people require their basic needs to be met for food and\nshelter, but beyond that tend to be happiest when socializing, eating or having\nsex. We analyze a corpus of private microblogs from a well-being application\ncalled ECHO, where users label each written post about daily events with a\nhappiness score between 1 and 9. Our goal is to ground the linguistic\ndescriptions of events that users experience in theories of well-being and\nhappiness, and then examine the extent to which different theoretical accounts\ncan explain the variance in the happiness scores. We show that recurrent event\ntypes, such as OBLIGATION and INCOMPETENCE, which affect people's feelings of\nwell-being are not captured in current lexical or semantic resources.", "published": "2017-08-31 22:03:53", "link": "http://arxiv.org/abs/1709.00094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for\n  Fine-grained Domain Adaptation", "abstract": "One weakness of machine-learned NLP models is that they typically perform\npoorly on out-of-domain data. In this work, we study the task of identifying\nproducts being bought and sold in online cybercrime forums, which exhibits\nparticularly challenging cross-domain effects. We formulate a task that\nrepresents a hybrid of slot-filling information extraction and named entity\nrecognition and annotate data from four different forums. Each of these forums\nconstitutes its own \"fine-grained domain\" in that the forums cover different\nmarket sectors with different properties, even though all forums are in the\nbroad domain of cybercrime. We characterize these domain differences in the\ncontext of a learning-based system: supervised models see decreased accuracy\nwhen applied to new forums, and standard techniques for semi-supervised\nlearning and domain adaptation have limited effectiveness on this data, which\nsuggests the need to improve these techniques. We release a dataset of 1,938\nannotated posts from across the four forums.", "published": "2017-08-31 08:18:12", "link": "http://arxiv.org/abs/1708.09609v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Human and Machine Judgements for Russian Semantic Relatedness", "abstract": "Semantic relatedness of terms represents similarity of meaning by a numerical\nscore. On the one hand, humans easily make judgments about semantic\nrelatedness. On the other hand, this kind of information is useful in language\nprocessing systems. While semantic relatedness has been extensively studied for\nEnglish using numerous language resources, such as associative norms, human\njudgments, and datasets generated from lexical databases, no evaluation\nresources of this kind have been available for Russian to date. Our\ncontribution addresses this problem. We present five language resources of\ndifferent scale and purpose for Russian semantic relatedness, each being a list\nof triples (word_i, word_j, relatedness_ij). Four of them are designed for\nevaluation of systems for computing semantic relatedness, complementing each\nother in terms of the semantic relation type they represent. These benchmarks\nwere used to organize a shared task on Russian semantic relatedness, which\nattracted 19 teams. We use one of the best approaches identified in this\ncompetition to generate the fifth high-coverage resource, the first open\ndistributional thesaurus of Russian. Multiple evaluations of this thesaurus,\nincluding a large-scale crowdsourcing study involving native speakers, indicate\nits high accuracy.", "published": "2017-08-31 13:33:04", "link": "http://arxiv.org/abs/1708.09702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Lexico-Functional Patterns for First-Person Affect", "abstract": "Informal first-person narratives are a unique resource for computational\nmodels of everyday events and people's affective reactions to them. People\nblogging about their day tend not to explicitly say I am happy. Instead they\ndescribe situations from which other humans can readily infer their affective\nreactions. However current sentiment dictionaries are missing much of the\ninformation needed to make similar inferences. We build on recent work that\nmodels affect in terms of lexical predicate functions and affect on the\npredicate's arguments. We present a method to learn proxies for these functions\nfrom first-person narratives. We construct a novel fine-grained test set, and\nshow that the patterns we learn improve our ability to predict first-person\naffective reactions to everyday events, from a Stanford sentiment baseline of\n.67F to .75F.", "published": "2017-08-31 16:04:26", "link": "http://arxiv.org/abs/1708.09789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning across Low-Resource, Related Languages for Neural\n  Machine Translation", "abstract": "We present a simple method to improve neural translation of a low-resource\nlanguage pair using parallel data from a related, also low-resource, language\npair. The method is based on the transfer method of Zoph et al., but whereas\ntheir method ignores any source vocabulary overlap, ours exploits it. First, we\nsplit words using Byte Pair Encoding (BPE) to increase vocabulary overlap.\nThen, we train a model on the first language pair and transfer its parameters,\nincluding its source word embeddings, to another model and continue training on\nthe second language pair. Our experiments show that transfer learning helps\nword-based translation only slightly, but when used on top of a much stronger\nBPE baseline, it yields larger improvements of up to 4.3 BLEU.", "published": "2017-08-31 16:34:38", "link": "http://arxiv.org/abs/1708.09803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering", "abstract": "In recent years researchers have achieved considerable success applying\nneural network methods to question answering (QA). These approaches have\nachieved state of the art results in simplified closed-domain settings such as\nthe SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected\npassage, from which the answer to a given question may be extracted. More\nrecently, researchers have begun to tackle open-domain QA, in which the model\nis given a question and access to a large corpus (e.g., wikipedia) instead of a\npre-selected passage (Chen et al., 2017a). This setting is more complex as it\nrequires large-scale search for relevant passages by an information retrieval\ncomponent, combined with a reading comprehension model that \"reads\" the\npassages to generate an answer to the question. Performance in this setting\nlags considerably behind closed-domain performance. In this paper, we present a\nnovel open-domain QA system called Reinforced Ranker-Reader $(R^3)$, based on\ntwo algorithmic innovations. First, we propose a new pipeline for open-domain\nQA with a Ranker component, which learns to rank retrieved passages in terms of\nlikelihood of generating the ground-truth answer to a given question. Second,\nwe propose a novel method that jointly trains the Ranker along with an\nanswer-generation Reader model, based on reinforcement learning. We report\nextensive experimental results showing that our method significantly improves\non the state of the art for multiple open-domain QA datasets.", "published": "2017-08-31 18:08:35", "link": "http://arxiv.org/abs/1709.00023v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Glyph-aware Embedding of Chinese Characters", "abstract": "Given the advantage and recent success of English character-level and\nsubword-unit models in several NLP tasks, we consider the equivalent modeling\nproblem for Chinese. Chinese script is logographic and many Chinese logograms\nare composed of common substructures that provide semantic, phonetic and\nsyntactic hints. In this work, we propose to explicitly incorporate the visual\nappearance of a character's glyph in its representation, resulting in a novel\nglyph-aware embedding of Chinese characters. Being inspired by the success of\nconvolutional neural networks in computer vision, we use them to incorporate\nthe spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In\nthe context of two basic Chinese NLP tasks of language modeling and word\nsegmentation, the model learns to represent each character's task-relevant\nsemantic and syntactic information in the character-level embedding.", "published": "2017-08-31 18:19:08", "link": "http://arxiv.org/abs/1709.00028v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weather impacts expressed sentiment", "abstract": "We conduct the largest ever investigation into the relationship between\nmeteorological conditions and the sentiment of human expressions. To do this,\nwe employ over three and a half billion social media posts from tens of\nmillions of individuals from both Facebook and Twitter between 2009 and 2016.\nWe find that cold temperatures, hot temperatures, precipitation, narrower daily\ntemperature ranges, humidity, and cloud cover are all associated with worsened\nexpressions of sentiment, even when excluding weather-related posts. We compare\nthe magnitude of our estimates with the effect sizes associated with notable\nhistorical events occurring within our data.", "published": "2017-08-31 20:36:07", "link": "http://arxiv.org/abs/1709.00071v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Seq2SQL: Generating Structured Queries from Natural Language using\n  Reinforcement Learning", "abstract": "A significant amount of the world's knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.", "published": "2017-08-31 23:12:15", "link": "http://arxiv.org/abs/1709.00103v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Video Descriptions with Topic Guidance", "abstract": "Generating video descriptions in natural language (a.k.a. video captioning)\nis a more challenging task than image captioning as the videos are\nintrinsically more complicated than images in two aspects. First, videos cover\na broader range of topics, such as news, music, sports and so on. Second,\nmultiple topics could coexist in the same video. In this paper, we propose a\nnovel caption model, topic-guided model (TGM), to generate topic-oriented\ndescriptions for videos in the wild via exploiting topic information. In\naddition to predefined topics, i.e., category tags crawled from the web, we\nalso mine topics in a data-driven way based on training captions by an\nunsupervised topic mining model. We show that data-driven topics reflect a\nbetter topic schema than the predefined topics. As for testing video topic\nprediction, we treat the topic mining model as teacher to train the student,\nthe topic prediction model, by utilizing the full multi-modalities in the video\nespecially the speech modality. We propose a series of caption models to\nexploit topic guidance, including implicitly using the topics as input features\nto generate words related to the topic and explicitly modifying the weights in\nthe decoder with topics to function as an ensemble of topic-aware language\ndecoders. Our comprehensive experimental results on the current largest video\ncaption dataset MSR-VTT prove the effectiveness of our topic-guided model,\nwhich significantly surpasses the winning performance in the 2016 MSR video to\nlanguage challenge.", "published": "2017-08-31 11:17:53", "link": "http://arxiv.org/abs/1708.09666v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Video Captioning with Guidance of Multimodal Latent Topics", "abstract": "The topic diversity of open-domain videos leads to various vocabularies and\nlinguistic expressions in describing video contents, and therefore, makes the\nvideo captioning task even more challenging. In this paper, we propose an\nunified caption framework, M&M TGM, which mines multimodal topics in\nunsupervised fashion from data and guides the caption decoder with these\ntopics. Compared to pre-defined topics, the mined multimodal topics are more\nsemantically and visually coherent and can reflect the topic distribution of\nvideos better. We formulate the topic-aware caption generation as a multi-task\nlearning problem, in which we add a parallel task, topic prediction, in\naddition to the caption task. For the topic prediction task, we use the mined\ntopics as the teacher to train a student topic prediction model, which learns\nto predict the latent topics from multimodal contents of videos. The topic\nprediction provides intermediate supervision to the learning process. As for\nthe caption task, we propose a novel topic-aware decoder to generate more\naccurate and detailed video descriptions with the guidance from latent topics.\nThe entire learning procedure is end-to-end and it optimizes both tasks\nsimultaneously. The results from extensive experiments conducted on the MSR-VTT\nand Youtube2Text datasets demonstrate the effectiveness of our proposed model.\nM&M TGM not only outperforms prior state-of-the-art methods on multiple\nevaluation metrics and on both benchmark datasets, but also achieves better\ngeneralization ability.", "published": "2017-08-31 11:18:28", "link": "http://arxiv.org/abs/1708.09667v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Deep Neural Network Activation Entropy to cope with Unseen\n  Data in Speech Recognition", "abstract": "Unseen data conditions can inflict serious performance degradation on systems\nrelying on supervised machine learning algorithms. Because data can often be\nunseen, and because traditional machine learning algorithms are trained in a\nsupervised manner, unsupervised adaptation techniques must be used to adapt the\nmodel to the unseen data conditions. However, unsupervised adaptation is often\nchallenging, as one must generate some hypothesis given a model and then use\nthat hypothesis to bootstrap the model to the unseen data conditions.\nUnfortunately, reliability of such hypotheses is often poor, given the mismatch\nbetween the training and testing datasets. In such cases, a model hypothesis\nconfidence measure enables performing data selection for the model adaptation.\nUnderlying this approach is the fact that for unseen data conditions, data\nvariability is introduced to the model, which the model propagates to its\noutput decision, impacting decision reliability. In a fully connected network,\nthis data variability is propagated as distortions from one layer to the next.\nThis work aims to estimate the propagation of such distortion in the form of\nnetwork activation entropy, which is measured over a short- time running window\non the activation from each neuron of a given hidden layer, and these\nmeasurements are then used to compute summary entropy. This work demonstrates\nthat such an entropy measure can help to select data for unsupervised model\nadaptation, resulting in performance gains in speech recognition tasks. Results\nfrom standard benchmark speech recognition tasks show that the proposed\napproach can alleviate the performance degradation experienced under unseen\ndata conditions by iteratively adapting the model to the unseen datas acoustic\ncondition.", "published": "2017-08-31 01:00:19", "link": "http://arxiv.org/abs/1708.09516v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Joint Separation and Denoising of Noisy Multi-talker Speech using\n  Recurrent Neural Networks and Permutation Invariant Training", "abstract": "In this paper we propose to use utterance-level Permutation Invariant\nTraining (uPIT) for speaker independent multi-talker speech separation and\ndenoising, simultaneously. Specifically, we train deep bi-directional Long\nShort-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for\nsingle-channel speaker independent multi-talker speech separation in multiple\nnoisy conditions, including both synthetic and real-life noise signals. We\nfocus our experiments on generalizability and noise robustness of models that\nrely on various types of a priori knowledge e.g. in terms of noise type and\nnumber of simultaneous speakers. We show that deep bi-directional LSTM RNNs\ntrained using uPIT in noisy environments can improve the Signal-to-Distortion\nRatio (SDR) as well as the Extended Short-Time Objective Intelligibility\n(ESTOI) measure, on the speaker independent multi-talker speech separation and\ndenoising task, for various noise types and Signal-to-Noise Ratios (SNRs).\nSpecifically, we first show that LSTM RNNs can achieve large SDR and ESTOI\nimprovements, when evaluated using known noise types, and that a single model\nis capable of handling multiple noise types with only a slight decrease in\nperformance. Furthermore, we show that a single LSTM RNN can handle both\ntwo-speaker and three-speaker noisy mixtures, without a priori knowledge about\nthe exact number of speakers. Finally, we show that LSTM RNNs trained using\nuPIT generalize well to noise types not seen during training.", "published": "2017-08-31 07:01:21", "link": "http://arxiv.org/abs/1708.09588v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
