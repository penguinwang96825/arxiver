{"title": "A Space Mapping approach for the calibration of financial models with the application to the Heston model", "abstract": "We present a novel approach for parameter calibration of the Heston model for\npricing an Asian put option, namely space mapping. Since few parameters of the\nHeston model can be directly extracted from real market data, calibration to\nreal market data is implicit and therefore a challenging task. In addition,\nsome of the parameters in the model are non-linear, which makes it difficult to\nfind the global minimum of the optimization problem within the calibration. Our\napproach is based on the idea of space mapping, exploiting the residuum of a\ncoarse surrogate model that allows optimization and a fine model that needs to\nbe calibrated. In our case, the pricing of an Asian option using the Heston\nmodel SDE is the fine model, and the surrogate is chosen to be the Heston model\nPDE pricing a European option. We formally derive a gradient descent algorithm\nfor the PDE constrained calibration model using well-known techniques from\noptimization with PDEs. Our main goal is to provide evidence that the space\nmapping approach can be useful in financial calibration tasks. Numerical\nresults underline the feasibility of our approach.", "published": "2025-01-24 14:24:10", "link": "http://arxiv.org/abs/2501.14521v1", "categories": ["math.NA", "cs.NA", "q-fin.CP", "65M06, 65K10, 91-08"], "primary_category": "math.NA"}
{"title": "Optimal Investment under Mutual Strategy Influence among Agents", "abstract": "In financial markets, agents often mutually influence each other's investment\nstrategies and adjust their strategies to align with others. However, there is\nlimited quantitative study of agents' investment strategies in such scenarios.\nIn this work, we formulate the optimal investment differential game problem to\nstudy the mutual influence among agents. We derive the analytical solutions for\nagents' optimal strategies and propose a fast algorithm to find approximate\nsolutions with low computational complexity. We theoretically analyze the\nimpact of mutual influence on agents' optimal strategies and terminal wealth.\nWhen the mutual influence is strong and approaches infinity, we show that\nagents' optimal strategies converge to the asymptotic strategy. Furthermore, in\ngeneral cases, we prove that agents' optimal strategies are linear combinations\nof the asymptotic strategy and their rational strategies without others'\ninfluence. We validate the performance of the fast algorithm and verify the\ncorrectness of our analysis using numerical experiments. This work is crucial\nto comprehend mutual influence among agents and design effective mechanisms to\nguide their strategies in financial markets.", "published": "2025-01-24 05:58:13", "link": "http://arxiv.org/abs/2501.14259v1", "categories": ["eess.SY", "cs.SY", "math.OC", "q-fin.MF", "q-fin.PM"], "primary_category": "eess.SY"}
{"title": "Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet\n  Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with\nimpressive outcomes being achieved on high-resource languages. However, the\napplication of cross-lingual transfer to the ASTE task has been relatively\nunexplored, and current code-switching methods still suffer from term boundary\ndetection issues and out-of-dictionary problems. In this study, we introduce a\nnovel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap\nbetween the bilingual training phase and the monolingual test-time prediction.\nDuring training, a generative model is developed based on bilingual\ncode-switched training data and can produce bilingual ASTE triplets for\nbilingual inputs. In the testing stage, we employ an alignment-based\ncode-switching technique for test-time augmentation. Extensive experiments on\ncross-lingual ASTE datasets validate the effectiveness of our proposed method.\nWe achieve an average improvement of 3.7% in terms of weighted-averaged F1 in\nfour datasets with different languages. Additionally, we set a benchmark using\nChatGPT and GPT-4, and demonstrate that even smaller generative models\nfine-tuned with our proposed TT-CSW framework surpass ChatGPT and GPT-4 by\n14.2% and 5.0% respectively.", "published": "2025-01-24 00:00:51", "link": "http://arxiv.org/abs/2501.14144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of\n  Token Perplexity", "abstract": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies.", "published": "2025-01-24 08:18:56", "link": "http://arxiv.org/abs/2501.14315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable\n  Neuron Editing", "abstract": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs.", "published": "2025-01-24 12:41:30", "link": "http://arxiv.org/abs/2501.14457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer:\n  Tasks and Experimental Setups Matter", "abstract": "Cross-lingual transfer is a popular approach to increase the amount of\ntraining data for NLP tasks in a low-resource context. However, the best\nstrategy to decide which cross-lingual data to include is unclear. Prior\nresearch often focuses on a small set of languages from a few language families\nand/or a single task. It is still an open question how these findings extend to\na wider variety of languages and tasks. In this work, we analyze cross-lingual\ntransfer for 266 languages from a wide variety of language families. Moreover,\nwe include three popular NLP tasks: POS tagging, dependency parsing, and topic\nclassification. Our findings indicate that the effect of linguistic similarity\non transfer performance depends on a range of factors: the NLP task, the (mono-\nor multilingual) input representations, and the definition of linguistic\nsimilarity.", "published": "2025-01-24 13:48:10", "link": "http://arxiv.org/abs/2501.14491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models", "abstract": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.", "published": "2025-01-24 13:53:54", "link": "http://arxiv.org/abs/2501.14497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WanJuanSiLu: A High-Quality Open-Source Webtext Dataset for Low-Resource\n  Languages", "abstract": "This paper introduces the open-source dataset WanJuanSiLu, designed to\nprovide high-quality training corpora for low-resource languages, thereby\nadvancing the research and development of multilingual models. To achieve this,\nwe have developed a systematic data processing framework tailored for\nlow-resource languages. This framework encompasses key stages such as data\nextraction, corpus cleaning, content deduplication, security filtering, quality\nevaluation, and theme classification. Through the implementation of this\nframework, we have significantly improved both the quality and security of the\ndataset, while maintaining its linguistic diversity. As of now, data for all\nfive languages have been fully open-sourced. The dataset can be accessed at\nhttps://opendatalab.com/applyMultilingualCorpus, and GitHub repository is\navailable at https://github.com/opendatalab/WanJuan3.0", "published": "2025-01-24 14:06:29", "link": "http://arxiv.org/abs/2501.14506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiom Detection in Sorani Kurdish Texts", "abstract": "Idiom detection using Natural Language Processing (NLP) is the computerized\nprocess of recognizing figurative expressions within a text that convey\nmeanings beyond the literal interpretation of the words. While idiom detection\nhas seen significant progress across various languages, the Kurdish language\nfaces a considerable research gap in this area despite the importance of idioms\nin tasks like machine translation and sentiment analysis. This study addresses\nidiom detection in Sorani Kurdish by approaching it as a text classification\ntask using deep learning techniques. To tackle this, we developed a dataset\ncontaining 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse\ncontexts. Using this dataset, we developed and evaluated three deep learning\nmodels: KuBERT-based transformer sequence classification, a Recurrent\nConvolutional Neural Network (RCNN), and a BiLSTM model with an attention\nmechanism. The evaluations revealed that the transformer model, the fine-tuned\nBERT, consistently outperformed the others, achieving nearly 99% accuracy while\nthe RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the\neffectiveness of Transformer-based architectures in low-resource languages like\nKurdish. This research provides a dataset, three optimized models, and insights\ninto idiom detection, laying a foundation for advancing Kurdish NLP.", "published": "2025-01-24 14:31:30", "link": "http://arxiv.org/abs/2501.14528v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives", "abstract": "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).", "published": "2025-01-24 16:36:07", "link": "http://arxiv.org/abs/2501.14617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion", "abstract": "To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs.", "published": "2025-01-24 17:15:09", "link": "http://arxiv.org/abs/2501.14649v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Understanding Table Instruction Tuning: Decoupling the\n  Effects from Data versus Models", "abstract": "Recent advances in natural language processing have leveraged instruction\ntuning to enhance Large Language Models (LLMs) for table-related tasks.\nHowever, previous works train different base models with different training\ndata, lacking an apples-to-apples comparison across the result table LLMs. To\naddress this, we fine-tune base models from the Mistral, OLMo, and Phi families\non existing public training datasets. Our replication achieves performance on\npar with or surpassing existing table LLMs, establishing new state-of-the-art\nperformance on Hitab, a table question-answering dataset. More importantly,\nthrough systematic out-of-domain evaluation, we decouple the contributions of\ntraining data and the base model, providing insight into their individual\nimpacts. In addition, we assess the effects of table-specific instruction\ntuning on general-purpose benchmarks, revealing trade-offs between\nspecialization and generalization.", "published": "2025-01-24 18:50:26", "link": "http://arxiv.org/abs/2501.14717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparable Corpora: Opportunities for New Research Directions", "abstract": "Most conference papers present new results, but this paper will focus more on\nopportunities for the audience to make their own contributions. This paper is\nintended to challenge the community to think more broadly about what we can do\nwith comparable corpora. We will start with a review of the history, and then\nsuggest new directions for future research. This was a keynote at BUCC-2025, a\nworkshop associated with Coling-2025.", "published": "2025-01-24 18:54:11", "link": "http://arxiv.org/abs/2501.14721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs", "abstract": "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.", "published": "2025-01-24 23:32:44", "link": "http://arxiv.org/abs/2501.14981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Framework for Semantic Similarity Analysis of Human and\n  AI-Generated Text Using Transformer Architectures and Ensemble Techniques", "abstract": "The rapid advancement of large language models (LLMs) has made detecting\nAI-generated text an increasingly critical challenge. Traditional methods often\nfail to capture the nuanced semantic differences between human and\nmachine-generated content. We therefore propose a novel approach based on\nsemantic similarity analysis, leveraging a multi-layered architecture that\ncombines a pre-trained DeBERTa-v3-large model, Bi-directional LSTMs, and linear\nattention pooling to capture both local and global semantic patterns. To\nenhance performance, we employ advanced input and output augmentation\ntechniques such as sector-level context integration and wide output\nconfigurations. These techniques enable the model to learn more discriminative\nfeatures and generalize across diverse domains. Experimental results show that\nthis approach works better than traditional methods, proving its usefulness for\nAI-generated text detection and other text comparison tasks.", "published": "2025-01-24 07:07:37", "link": "http://arxiv.org/abs/2501.14288v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes", "abstract": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, e.g., when LLMs fail to operate as intended. This study\nexamines the alignment of LLMs with human values for the domain of politics.\nPrior research has shown that LLM-generated outputs can include political\nleanings and mimic the stances of political parties on various issues. However,\nthe extent and conditions under which LLMs deviate from empirical positions are\ninsufficiently examined. To address this gap, we analyze the factors that\ncontribute to LLMs' deviations from empirical positions on political issues,\naiming to quantify these deviations and identify the conditions that cause\nthem.\n  Drawing on findings from cognitive science about representativeness\nheuristics, i.e., situations where humans lean on representative attributes of\na target group in a way that leads to exaggerated beliefs, we scrutinize LLM\nresponses through this heuristics' lens. We conduct experiments to determine\nhow LLMs inflate predictions about political parties, which results in\nstereotyping. We find that while LLMs can mimic certain political parties'\npositions, they often exaggerate these positions more than human survey\nrespondents do. Also, LLMs tend to overemphasize representativeness more than\nhumans. This study highlights the susceptibility of LLMs to representativeness\nheuristics, suggesting a potential vulnerability of LLMs that facilitates\npolitical stereotyping. We also test prompt-based mitigation strategies,\nfinding that strategies that can mitigate representative heuristics in humans\nare also effective in reducing the influence of representativeness on\nLLM-generated responses.", "published": "2025-01-24 07:24:23", "link": "http://arxiv.org/abs/2501.14294v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain-of-Retrieval Augmented Generation", "abstract": "This paper introduces an approach for training o1-like RAG models that\nretrieve and reason over relevant information step by step before generating\nthe final answer. Conventional RAG methods usually perform a single retrieval\nstep before the generation process, which limits their effectiveness in\naddressing complex queries due to imperfect retrieval results. In contrast, our\nproposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the\nmodel to dynamically reformulate the query based on the evolving state. To\ntrain CoRAG effectively, we utilize rejection sampling to automatically\ngenerate intermediate retrieval chains, thereby augmenting existing RAG\ndatasets that only provide the correct final answer. At test time, we propose\nvarious decoding strategies to scale the model's test-time compute by\ncontrolling the length and number of sampled retrieval chains. Experimental\nresults across multiple benchmarks validate the efficacy of CoRAG, particularly\nin multi-hop question answering tasks, where we observe more than 10 points\nimprovement in EM score compared to strong baselines. On the KILT benchmark,\nCoRAG establishes a new state-of-the-art performance across a diverse range of\nknowledge-intensive tasks. Furthermore, we offer comprehensive analyses to\nunderstand the scaling behavior of CoRAG, laying the groundwork for future\nresearch aimed at developing factual and grounded foundation models.", "published": "2025-01-24 09:12:52", "link": "http://arxiv.org/abs/2501.14342v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes\n  Domains", "abstract": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://anonymous.4open.science/r/Domaino1s-006F/.", "published": "2025-01-24 11:57:39", "link": "http://arxiv.org/abs/2501.14431v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "State Space Models for Extractive Summarization in Low Resource\n  Scenarios", "abstract": "Extractive summarization involves selecting the most relevant sentences from\na text. Recently, researchers have focused on advancing methods to improve\nstate-of-the-art results in low-resource settings. Motivated by these\nadvancements, we propose the MPoincareSum method. This method applies the Mamba\nstate space model to generate the semantics of reviews and sentences, which are\nthen concatenated. A Poincare compression is used to select the most meaningful\nfeatures, followed by the application of a linear layer to predict sentence\nrelevance based on the corresponding review. Finally, we paraphrase the\nrelevant sentences to create the final summary. To evaluate the effectiveness\nof MPoincareSum, we conducted extensive experiments using the Amazon review\ndataset. The performance of the method was assessed using ROUGE scores. The\nexperimental results demonstrate that MPoincareSum outperforms several existing\napproaches in the literature", "published": "2025-01-24 17:49:34", "link": "http://arxiv.org/abs/2501.14673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Table Instruction Tuning", "abstract": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection.", "published": "2025-01-24 18:06:07", "link": "http://arxiv.org/abs/2501.14693v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLP-based assessment of prescription appropriateness from Italian\n  referrals", "abstract": "Objective: This study proposes a Natural Language Processing pipeline to\nevaluate prescription appropriateness in Italian referrals, where reasons for\nprescriptions are recorded only as free text, complicating automated\ncomparisons with guidelines. The pipeline aims to derive, for the first time, a\ncomprehensive summary of the reasons behind these referrals and a\nquantification of their appropriateness. While demonstrated in a specific case\nstudy, the approach is designed to generalize to other types of examinations.\n  Methods: Leveraging embeddings from a transformer-based model, the proposed\napproach clusters referral texts, maps clusters to labels, and aligns these\nlabels with existing guidelines. We present a case study on a dataset of\n496,971 referrals, consisting of all referrals for venous echocolordopplers of\nthe lower limbs between 2019 and 2021 in the Lombardy Region. A sample of 1,000\nreferrals was manually annotated to validate the results.\n  Results: The pipeline exhibited high performance for referrals' reasons\n(Prec=92.43%, Rec=83.28%) and excellent results for referrals' appropriateness\n(Prec=93.58%, Rec=91.52%) on the annotated subset. Analysis of the entire\ndataset identified clusters matching guideline-defined reasons - both\nappropriate and inappropriate - as well as clusters not addressed in the\nguidelines. Overall, 34.32% of referrals were marked as appropriate, 34.07%\ninappropriate, 14.37% likely inappropriate, and 17.24% could not be mapped to\nguidelines.\n  Conclusions: The proposed pipeline effectively assessed prescription\nappropriateness across a large dataset, serving as a valuable tool for health\nauthorities. Findings have informed the Lombardy Region's efforts to strengthen\nrecommendations and reduce the burden of inappropriate referrals.", "published": "2025-01-24 18:24:16", "link": "http://arxiv.org/abs/2501.14701v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.1; J.3"], "primary_category": "cs.CL"}
{"title": "The Karp Dataset", "abstract": "Understanding the mathematical reasoning capabilities of Large Language\nModels (LLMs) is a central topic in the study of artificial intelligence. This\nnew domain necessitates the creation of datasets of reasoning tasks for both\ntraining and benchmarking the performance of LLMs. To this end, we introduce\nthe Karp dataset: The first dataset composed of detailed proofs of\nNP-completeness reductions. The reductions vary in difficulty, ranging from\nsimple exercises of undergraduate courses to more challenging reductions from\nacademic papers. We compare the performance of state-of-the-art models on this\ntask and demonstrate the effect of fine-tuning with the Karp dataset on\nreasoning capacity.", "published": "2025-01-24 18:30:19", "link": "http://arxiv.org/abs/2501.14705v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing", "abstract": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.", "published": "2025-01-24 18:46:37", "link": "http://arxiv.org/abs/2501.14713v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the locality bias and results in the Long Range Arena", "abstract": "The Long Range Arena (LRA) benchmark was designed to evaluate the performance\nof Transformer improvements and alternatives in long-range dependency modeling\ntasks. The Transformer and its main variants performed poorly on this\nbenchmark, and a new series of architectures such as State Space Models (SSMs)\ngained some traction, greatly outperforming Transformers in the LRA. Recent\nwork has shown that with a denoising pre-training phase, Transformers can\nachieve competitive results in the LRA with these new architectures. In this\nwork, we discuss and explain the superiority of architectures such as MEGA and\nSSMs in the Long Range Arena, as well as the recent improvement in the results\nof Transformers, pointing to the positional and local nature of the tasks. We\nshow that while the LRA is a benchmark for long-range dependency modeling, in\nreality most of the performance comes from short-range dependencies. Using\ntraining techniques to mitigate data inefficiency, Transformers are able to\nreach state-of-the-art performance with proper positional encoding. In\naddition, with the same techniques, we were able to remove all restrictions\nfrom SSM convolutional kernels and learn fully parameterized convolutions\nwithout decreasing performance, suggesting that the design choices behind SSMs\nsimply added inductive biases and learning efficiency for these particular\ntasks. Our insights indicate that LRA results should be interpreted with\ncaution and call for a redesign of the benchmark.", "published": "2025-01-24 15:34:50", "link": "http://arxiv.org/abs/2501.14850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Adaptation of LoRA Fine-Tuning for Efficient and Task-Specific\n  Optimization of Large Language Models", "abstract": "This paper presents a novel methodology of fine-tuning for large language\nmodels-dynamic LoRA. Building from the standard Low-Rank Adaptation framework,\nthis methodology further adds dynamic adaptation mechanisms to improve\nefficiency and performance. The key contribution of dynamic LoRA lies within\nits adaptive weight allocation mechanism coupled with an input feature-based\nadaptive strategy. These enhancements allow for a more precise fine-tuning\nprocess that is more tailored to specific tasks. Traditional LoRA methods use\nstatic adapter settings, not considering the different importance of model\nlayers. In contrast, dynamic LoRA introduces a mechanism that dynamically\nevaluates the layer's importance during fine-tuning. This evaluation enables\nthe reallocation of adapter parameters to fit the unique demands of each\nindividual task, which leads to better optimization results. Another gain in\nflexibility arises from the consideration of the input feature distribution,\nwhich helps the model generalize better when faced with complicated and diverse\ndatasets. The joint approach boosts not only the performance over each single\ntask but also the generalization ability of the model. The efficiency of the\ndynamic LoRA was validated in experiments on benchmark datasets, such as GLUE,\nwith surprising results. More specifically, this method achieved 88.1% accuracy\nwith an F1-score of 87.3%. Noticeably, these improvements were made at a slight\nincrease in computational costs: only 0.1% more resources than standard LoRA.\nThis balance between performance and efficiency positions dynamic LoRA as a\npractical, scalable solution for fine-tuning LLMs, especially in\nresource-constrained scenarios. To take it a step further, its adaptability\nmakes it a promising foundation for much more advanced applications, including\nmultimodal tasks.", "published": "2025-01-24 18:54:14", "link": "http://arxiv.org/abs/2501.14859v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DrawEduMath: Evaluating Vision Language Models with Expert-Annotated\n  Students' Hand-Drawn Math Images", "abstract": "In real-world settings, vision language models (VLMs) should robustly handle\nnaturalistic, noisy visual content as well as domain-specific language and\nconcepts. For example, K-12 educators using digital learning platforms may need\nto examine and provide feedback across many images of students' math work. To\nassess the potential of VLMs to support educators in settings like this one, we\nintroduce DrawEduMath, an English-language dataset of 2,030 images of students'\nhandwritten responses to K-12 math problems. Teachers provided detailed\nannotations, including free-form descriptions of each image and 11,661\nquestion-answer (QA) pairs. These annotations capture a wealth of pedagogical\ninsights, ranging from students' problem-solving strategies to the composition\nof their drawings, diagrams, and writing. We evaluate VLMs on teachers' QA\npairs, as well as 44,362 synthetic QA pairs derived from teachers' descriptions\nusing language models (LMs). We show that even state-of-the-art VLMs leave much\nroom for improvement on DrawEduMath questions. We also find that synthetic QAs,\nthough imperfect, can yield similar model rankings as teacher-written QAs. We\nrelease DrawEduMath to support the evaluation of VLMs' abilities to reason\nmathematically over images gathered with educational contexts in mind.", "published": "2025-01-24 19:03:42", "link": "http://arxiv.org/abs/2501.14877v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Verify with Caution: The Pitfalls of Relying on Imperfect Factuality\n  Metrics", "abstract": "Improvements in large language models have led to increasing optimism that\nthey can serve as reliable evaluators of natural language generation outputs.\nIn this paper, we challenge this optimism by thoroughly re-evaluating five\nstate-of-the-art factuality metrics on a collection of 11 datasets for\nsummarization, retrieval-augmented generation, and question answering. We find\nthat these evaluators are inconsistent with each other and often misestimate\nsystem-level performance, both of which can lead to a variety of pitfalls. We\nfurther show that these metrics exhibit biases against highly paraphrased\noutputs and outputs that draw upon faraway parts of the source documents. We\nurge users of these factuality metrics to proceed with caution and manually\nvalidate the reliability of these metrics in their domain of interest before\nproceeding.", "published": "2025-01-24 19:17:06", "link": "http://arxiv.org/abs/2501.14883v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs", "abstract": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.", "published": "2025-01-24 19:31:06", "link": "http://arxiv.org/abs/2501.14892v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Context-Aware Neural Gradient Mapping for Fine-Grained Instruction\n  Processing", "abstract": "The integration of contextual embeddings into the optimization processes of\nlarge language models is an advancement in natural language processing. The\nContext-Aware Neural Gradient Mapping framework introduces a dynamic gradient\nadjustment mechanism, incorporating contextual embeddings directly into the\noptimization process. This approach facilitates real-time parameter\nadjustments, enhancing task-specific generalization even in the presence of\nsparse or noisy data inputs. The mathematical foundation of this framework\nrelies on gradient descent modifications, where contextual embeddings are\nderived from a supplementary neural network trained to map input features to\noptimal adaptation gradients. By employing differential geometry principles,\nhigh-dimensional input dependencies are encoded into low-dimensional gradient\nmanifolds, enabling efficient adaptation without necessitating the retraining\nof the entire model. Empirical evaluations demonstrate that the proposed\nframework consistently outperforms baseline models across various metrics,\nincluding accuracy, robustness to noise, and computational efficiency. The\nintegration of context-specific embeddings allows for a more complex\nunderstanding of language, thereby improving the model's ability to handle\ndiverse linguistic phenomena. Furthermore, the computational efficiency\nachieved through this method demonstrates its scalability for large-scale\nlanguage models operating under diverse constraints.", "published": "2025-01-24 21:49:24", "link": "http://arxiv.org/abs/2501.14936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models", "abstract": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts.", "published": "2025-01-24 21:55:14", "link": "http://arxiv.org/abs/2501.14940v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A review of annotation classification tools in the educational domain", "abstract": "An annotation consists of a portion of information that is associated with a\npiece of content in order to explain something about the content or to add more\ninformation. The use of annotations as a tool in the educational field has\npositive effects on the learning process. The usual way to use this instrument\nis to provide students with contents, usually textual, with which they must\nassociate annotations. In most cases this task is performed in groups of\nstudents who work collaboratively. This process encourages analysis and\nunderstanding of the contents since they have to understand them in order to\nannotate them, and also encourages teamwork. To facilitate its use, computer\napplications have been devel-oped in recent decades that implement the\nannotation process and offer a set of additional functionalities. One of these\nfunctionalities is the classification of the annotations made. This\nfunctionality can be exploited in various ways in the learning process, such as\nguiding the students in the annotation process, providing information to the\nstudent about how the annotation process is done and to the teacher about how\nthe students write and how they understand the content, as well as implementing\nother innovative educational processes. In this sense, the classification of\nannotations plays a critical role in the application of the annotation in the\neducational field. There are many studies of annotations, but most of them\nconsider the classification aspect marginally only. This paper presents an\ninitial study of the classification mech-anisms used in the annotation tools,\nidentifying four types of cases: absence of classification mechanisms,\nclassification based on pre-established vocabularies, classification based on\nextensible vocabularies, and classification based on struc-tured vocabularies.", "published": "2025-01-24 23:18:21", "link": "http://arxiv.org/abs/2501.14976v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language\n  Model in Language Game", "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can\nnot only make stratigic decisions but also engage in flexible and meaningful\ncommunication. Inspired by Wittgenstein's language game theory in Philosophical\nInvestigations, we propose that language agents can learn through in-context\ninteraction rather than traditional multi-stage frameworks that separate\ndecision-making from language expression. Using Werewolf, a social deduction\ngame that tests language understanding, strategic interaction, and\nadaptability, we develop the Multi-agent Kahneman & Tversky's Optimization\n(MaKTO). MaKTO engages diverse models in extensive gameplay to generate\nunpaired desirable and unacceptable responses, then employs KTO to refine the\nmodel's decision-making process. In 9-player Werewolf games, MaKTO achieves a\n61% average win rate across various models, outperforming GPT-4o and two-stage\nRL agents by relative improvements of 23.0% and 10.9%, respectively. Notably,\nMaKTO also demonstrates human-like performance, winning 60% against expert\nplayers and showing only 49% detectability in Turing-style blind tests.", "published": "2025-01-24 04:09:03", "link": "http://arxiv.org/abs/2501.14225v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Humanity's Last Exam", "abstract": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.", "published": "2025-01-24 05:27:46", "link": "http://arxiv.org/abs/2501.14249v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating\n  Real-World Human Jailbreak Behaviors", "abstract": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text.", "published": "2025-01-24 05:31:27", "link": "http://arxiv.org/abs/2501.14250v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and\n  Contamination-Resistant Evaluation", "abstract": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops", "published": "2025-01-24 06:39:38", "link": "http://arxiv.org/abs/2501.14275v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large\n  Language Model on Knowledge Graph", "abstract": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works.", "published": "2025-01-24 07:47:40", "link": "http://arxiv.org/abs/2501.14300v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.AI"}
{"title": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style\n  Subspace Editing", "abstract": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM.", "published": "2025-01-24 10:04:53", "link": "http://arxiv.org/abs/2501.14371v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model\n  Critiques", "abstract": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}.", "published": "2025-01-24 13:48:10", "link": "http://arxiv.org/abs/2501.14492v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do LLMs Provide Consistent Answers to Health-Related Questions across\n  Languages?", "abstract": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information.", "published": "2025-01-24 18:51:26", "link": "http://arxiv.org/abs/2501.14719v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mitigating GenAI-powered Evidence Pollution for Out-of-Context\n  Multimodal Misinformation Detection", "abstract": "While large generative artificial intelligence (GenAI) models have achieved\nsignificant success, they also raise growing concerns about online information\nsecurity due to their potential misuse for generating deceptive content.\nOut-of-context (OOC) multimodal misinformation detection, which often retrieves\nWeb evidence to identify the repurposing of images in false contexts, faces the\nissue of reasoning over GenAI-polluted evidence to derive accurate predictions.\nExisting works simulate GenAI-powered pollution at the claim level with\nstylistic rewriting to conceal linguistic cues, and ignore evidence-level\npollution for such information-seeking applications. In this work, we\ninvestigate how polluted evidence affects the performance of existing OOC\ndetectors, revealing a performance degradation of more than 9 percentage\npoints. We propose two strategies, cross-modal evidence reranking and\ncross-modal claim-evidence reasoning, to address the challenges posed by\npolluted evidence. Extensive experiments on two benchmark datasets show that\nthese strategies can effectively enhance the robustness of existing\nout-of-context detectors amidst polluted evidence.", "published": "2025-01-24 18:59:31", "link": "http://arxiv.org/abs/2501.14728v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.MM"}
{"title": "Unmasking Conversational Bias in AI Multiagent Systems", "abstract": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725.", "published": "2025-01-24 09:10:02", "link": "http://arxiv.org/abs/2501.14844v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval", "abstract": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future.", "published": "2025-01-24 10:49:45", "link": "http://arxiv.org/abs/2501.14846v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning\n  in Large Language Models", "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that most\nstate-of-the-art (SOTA) LLMs perform significantly worse than the human\naverage, demonstrating substantial room for model improvement. All code and\ndata are available at https://github.com/michaelchen-lab/JustLogic", "published": "2025-01-24 15:49:10", "link": "http://arxiv.org/abs/2501.14851v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "abstract": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.", "published": "2025-01-24 20:54:29", "link": "http://arxiv.org/abs/2501.14917v3", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "E-Gen: Leveraging E-Graphs to Improve Continuous Representations of\n  Symbolic Expressions", "abstract": "Vector representations have been pivotal in advancing natural language\nprocessing (NLP), with prior research focusing on embedding techniques for\nmathematical expressions using mathematically equivalent formulations. While\neffective, these approaches are constrained by the size and diversity of\ntraining data. In this work, we address these limitations by introducing E-Gen,\na novel e-graph-based dataset generation scheme that synthesizes large and\ndiverse mathematical expression datasets, surpassing prior methods in size and\noperator variety. Leveraging this dataset, we train embedding models using two\nstrategies: (1) generating mathematically equivalent expressions, and (2)\ncontrastive learning to explicitly group equivalent expressions. We evaluate\nthese embeddings on both in-distribution and out-of-distribution mathematical\nlanguage processing tasks, comparing them against prior methods. Finally, we\ndemonstrate that our embedding-based approach outperforms state-of-the-art\nlarge language models (LLMs) on several tasks, underscoring the necessity of\noptimizing embedding methods for the mathematical data modality. The source\ncode and datasets are available at https://github.com/MLPgroup/E-Gen.", "published": "2025-01-24 22:39:08", "link": "http://arxiv.org/abs/2501.14951v2", "categories": ["cs.LG", "cs.CL", "cs.SC"], "primary_category": "cs.LG"}
{"title": "MISCON: A Mission-Driven Conversational Consultant for Pre-Venture\n  Entrepreneurs in Food Deserts", "abstract": "This work-in-progress report describes MISCON, a conversational consultant\nbeing developed for a public mission project called NOURISH. With MISCON,\naspiring small business owners in a food-insecure region and their advisors in\nCommunity-based organizations would be able to get information, recommendation\nand analysis regarding setting up food businesses. MISCON conversations are\nmodeled as state machine that uses a heterogeneous knowledge graph as well as\nseveral analytical tools and services including a variety of LLMs. In this\nshort report, we present the functional architecture and some design\nconsiderations behind MISCON.", "published": "2025-01-24 22:39:49", "link": "http://arxiv.org/abs/2501.14954v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "K.4.2; I.2.7; H.5.2"], "primary_category": "cs.AI"}
{"title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form\n  Text Generation", "abstract": "Evaluating personalized text generated by large language models (LLMs) is\nchallenging, as only the LLM user, i.e., prompt author, can reliably assess the\noutput, but re-engaging the same individuals across studies is infeasible. This\npaper addresses the challenge of evaluating personalized text generation by\nintroducing ExPerT, an explainable reference-based evaluation framework. ExPerT\nleverages an LLM to extract atomic aspects and their evidence from the\ngenerated and reference texts, match the aspects, and evaluate their alignment\nbased on content and writing style -- two key attributes in personalized text\ngeneration. Additionally, ExPerT generates detailed, fine-grained explanations\nfor every step of the evaluation process, enhancing transparency and\ninterpretability. Our experiments demonstrate that ExPerT achieves a 7.2%\nrelative improvement in alignment with human judgments compared to the\nstate-of-the-art text generation evaluation methods. Furthermore, human\nevaluators rated the usability of ExPerT's explanations at 4.7 out of 5,\nhighlighting its effectiveness in making evaluation decisions more\ninterpretable.", "published": "2025-01-24 22:44:22", "link": "http://arxiv.org/abs/2501.14956v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM4DistReconfig: A Fine-tuned Large Language Model for Power\n  Distribution Network Reconfiguration", "abstract": "Power distribution networks are evolving due to the integration of DERs and\nincreased customer participation. To maintain optimal operation, minimize\nlosses, and meet varying load demands, frequent network reconfiguration is\nnecessary. Traditionally, the reconfiguration task relies on optimization\nsoftware and expert operators, but as systems grow more complex, faster and\nmore adaptive solutions are required without expert intervention. Data-driven\nreconfiguration is gaining traction for its accuracy, speed, and robustness\nagainst incomplete network data. LLMs, with their ability to capture complex\npatterns, offer a promising approach for efficient and responsive network\nreconfiguration in evolving complex power networks.\n  In this work, we introduce LLM4DistReconfig, a deep learning-based approach\nutilizing a fine-tuned LLM to solve the distribution network reconfiguration\nproblem. By carefully crafting prompts and designing a custom loss function, we\ntrain the LLM with inputs representing network parameters such as buses,\navailable lines, open lines, node voltages, and system loss. The model then\npredicts optimal reconfigurations by outputting updated network configurations\nthat minimize system loss while meeting operational constraints. Our approach\nsignificantly reduces inference time compared to classical algorithms, allowing\nfor near real-time optimal reconfiguration after training. Experimental results\nshow that our method generates optimal configurations minimizing system loss\nfor five individual and a combined test dataset. It also produces minimal\ninvalid edges, no cycles, or subgraphs across all datasets, fulfilling\ndomain-specific needs. Additionally, the generated responses contain less than\n5% improper outputs on seen networks and satisfactory results on unseen\nnetworks, demonstrating its effectiveness and reliability for the\nreconfiguration task.", "published": "2025-01-24 22:46:14", "link": "http://arxiv.org/abs/2501.14960v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Internal Activation Revision: Safeguarding Vision Language Models\n  Without Parameter Update", "abstract": "Vision-language models (VLMs) demonstrate strong multimodal capabilities but\nhave been found to be more susceptible to generating harmful content compared\nto their backbone large language models (LLMs). Our investigation reveals that\nthe integration of images significantly shifts the model's internal activations\nduring the forward pass, diverging from those triggered by textual input.\nMoreover, the safety alignments of LLMs embedded within VLMs are not\nsufficiently robust to handle the activations discrepancies, making the models\nvulnerable to even the simplest jailbreaking attacks. To address this issue, we\npropose an \\textbf{internal activation revision} approach that efficiently\nrevises activations during generation, steering the model toward safer outputs.\nOur framework incorporates revisions at both the layer and head levels,\noffering control over the model's generation at varying levels of granularity.\nIn addition, we explore three strategies for constructing positive and negative\nsamples and two approaches for extracting revision vectors, resulting in\ndifferent variants of our method. Comprehensive experiments demonstrate that\nthe internal activation revision method significantly improves the safety of\nwidely used VLMs, reducing attack success rates by an average of 48.94\\%,\n34.34\\%, 43.92\\%, and 52.98\\% on SafeBench, Safe-Unsafe, Unsafe, and\nMM-SafetyBench, respectively, while minimally impacting model helpfulness.", "published": "2025-01-24 06:17:22", "link": "http://arxiv.org/abs/2501.16378v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant", "abstract": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective.", "published": "2025-01-24 08:15:05", "link": "http://arxiv.org/abs/2501.17176v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost", "abstract": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.", "published": "2025-01-24 17:01:14", "link": "http://arxiv.org/abs/2501.17178v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizable Audio Deepfake Detection via Latent Space Refinement and\n  Augmentation", "abstract": "Advances in speech synthesis technologies, like text-to-speech (TTS) and\nvoice conversion (VC), have made detecting deepfake speech increasingly\nchallenging. Spoofing countermeasures often struggle to generalize effectively,\nparticularly when faced with unseen attacks. To address this, we propose a\nnovel strategy that integrates Latent Space Refinement (LSR) and Latent Space\nAugmentation (LSA) to improve the generalization of deepfake detection systems.\nLSR introduces multiple learnable prototypes for the spoof class, refining the\nlatent space to better capture the intricate variations within spoofed data.\nLSA further diversifies spoofed data representations by applying augmentation\ntechniques directly in the latent space, enabling the model to learn a broader\nrange of spoofing patterns. We evaluated our approach on four representative\ndatasets, i.e. ASVspoof 2019 LA, ASVspoof 2021 LA and DF, and In-The-Wild. The\nresults show that LSR and LSA perform well individually, and their integration\nachieves competitive results, matching or surpassing current state-of-the-art\nmethods.", "published": "2025-01-24 04:54:08", "link": "http://arxiv.org/abs/2501.14240v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Characteristic-Specific Partial Fine-Tuning for Efficient Emotion and\n  Speaker Adaptation in Codec Language Text-to-Speech Models", "abstract": "Recently, emotional speech generation and speaker cloning have garnered\nsignificant interest in text-to-speech (TTS). With the open-sourcing of codec\nlanguage TTS models trained on massive datasets with large-scale parameters,\nadapting these general pre-trained TTS models to generate speech with specific\nemotional expressions and target speaker characteristics has become a topic of\ngreat attention. Common approaches, such as full and adapter-based fine-tuning,\noften overlook the specific contributions of model parameters to emotion and\nspeaker control. Treating all parameters uniformly during fine-tuning,\nespecially when the target data has limited content diversity compared to the\npre-training corpus, results in slow training speed and an increased risk of\ncatastrophic forgetting. To address these challenges, we propose a\ncharacteristic-specific partial fine-tuning strategy, short as CSP-FT. First,\nwe use a weighted-sum approach to analyze the contributions of different\nTransformer layers in a pre-trained codec language TTS model for emotion and\nspeaker control in the generated speech. We then selectively fine-tune the\nlayers with the highest and lowest characteristic-specific contributions to\ngenerate speech with target emotional expression and speaker identity.\nExperimental results demonstrate that our method achieves performance\ncomparable to, or even surpassing, full fine-tuning in generating speech with\nspecific emotional expressions and speaker identities. Additionally, CSP-FT\ndelivers approximately 2x faster training speeds, fine-tunes only around 8% of\nparameters, and significantly reduces catastrophic forgetting. Furthermore, we\nshow that codec language TTS models perform competitively with self-supervised\nmodels in speaker identification and emotion classification tasks, offering\nvaluable insights for developing universal speech processing models.", "published": "2025-01-24 06:35:20", "link": "http://arxiv.org/abs/2501.14273v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition\n  Models from Encoder-Decoder to LLM Integration", "abstract": "We present FireRedASR, a family of large-scale automatic speech recognition\n(ASR) models for Mandarin, designed to meet diverse requirements in superior\nperformance and optimal efficiency across various applications. FireRedASR\ncomprises two variants:\n  FireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and\nto enable seamless end-to-end speech interaction. It adopts an\nEncoder-Adapter-LLM framework leveraging large language model (LLM)\ncapabilities. On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters)\nachieves an average Character Error Rate (CER) of 3.05%, surpassing the latest\nSOTA of 3.33% with an 8.4% relative CER reduction (CERR). It demonstrates\nsuperior generalization capability over industrial-grade baselines, achieving\n24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and\nintelligent assistant.\n  FireRedASR-AED: Designed to balance high performance and computational\nefficiency and to serve as an effective speech representation module in\nLLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED)\narchitecture. On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters)\nachieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still\noutperforming the latest SOTA model with over 12B parameters. It offers a more\ncompact size, making it suitable for resource-constrained applications.\n  Moreover, both models exhibit competitive results on Chinese dialects and\nEnglish speech benchmarks and excel in singing lyrics recognition. To advance\nresearch in speech processing, we release our models and inference code at\nhttps://github.com/FireRedTeam/FireRedASR.", "published": "2025-01-24 09:21:41", "link": "http://arxiv.org/abs/2501.14350v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Intelligibility for Generative Target Speech Extraction via\n  Joint Optimization with Target Speaker ASR", "abstract": "Target speech extraction (TSE) isolates the speech of a specific speaker from\na multi-talker overlapped speech mixture. Most existing TSE models rely on\ndiscriminative methods, typically predicting a time-frequency spectrogram mask\nfor the target speech. However, imperfections in these masks often result in\nover-/under-suppression of target/non-target speech, degrading perceptual\nquality. Generative methods, by contrast, re-synthesize target speech based on\nthe mixture and target speaker cues, achieving superior perceptual quality.\nNevertheless, these methods often overlook speech intelligibility, leading to\nalterations or loss of semantic content in the re-synthesized speech. Inspired\nby the Whisper model's success in target speaker ASR, we propose a generative\nTSE framework based on the pre-trained Whisper model to address the above\nissues. This framework integrates semantic modeling with flow-based acoustic\nmodeling to achieve both high intelligibility and perceptual quality. Results\nfrom multiple benchmarks demonstrate that the proposed method outperforms\nexisting generative and discriminative baselines. We present speech samples on\nour demo page.", "published": "2025-01-24 13:19:56", "link": "http://arxiv.org/abs/2501.14477v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diffusion based Text-to-Music Generation with Global and Local Text\n  based Conditioning", "abstract": "Diffusion based Text-To-Music (TTM) models generate music corresponding to\ntext descriptions. Typically UNet based diffusion models condition on text\nembeddings generated from a pre-trained large language model or from a\ncross-modality audio-language representation model. This work proposes a\ndiffusion based TTM, in which the UNet is conditioned on both (i) a uni-modal\nlanguage model (e.g., T5) via cross-attention and (ii) a cross-modal\naudio-language representation model (e.g., CLAP) via Feature-wise Linear\nModulation (FiLM). The diffusion model is trained to exploit both a local text\nrepresentation from the T5 and a global representation from the CLAP.\nFurthermore, we propose modifications that extract both global and local\nrepresentations from the T5 through pooling mechanisms that we call mean\npooling and self-attention pooling. This approach mitigates the need for an\nadditional encoder (e.g., CLAP) to extract a global representation, thereby\nreducing the number of model parameters. Our results show that incorporating\nthe CLAP global embeddings to the T5 local embeddings enhances text adherence\n(KL=1.47) compared to a baseline model solely relying on the T5 local\nembeddings (KL=1.54). Alternatively, extracting global text embeddings directly\nfrom the T5 local embeddings through the proposed mean pooling approach yields\nsuperior generation quality (FAD=1.89) while exhibiting marginally inferior\ntext adherence (KL=1.51) against the model conditioned on both CLAP and T5 text\nembeddings (FAD=1.94 and KL=1.47). Our proposed solution is not only efficient\nbut also compact in terms of the number of parameters required.", "published": "2025-01-24 17:57:47", "link": "http://arxiv.org/abs/2501.14680v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Spatial Cues from Cochlear Implant Microphones to Efficiently\n  Enhance Speech Separation in Real-World Listening Scenes", "abstract": "Speech separation approaches for single-channel, dry speech mixtures have\nsignificantly improved. However, real-world spatial and reverberant acoustic\nenvironments remain challenging, limiting the effectiveness of these approaches\nfor assistive hearing devices like cochlear implants (CIs). To address this, we\nquantify the impact of real-world acoustic scenes on speech separation and\nexplore how spatial cues can enhance separation quality efficiently. We analyze\nperformance based on implicit spatial cues (inherent in the acoustic input and\nlearned by the model) and explicit spatial cues (manually calculated spatial\nfeatures added as auxiliary inputs). Our findings show that spatial cues (both\nimplicit and explicit) improve separation for mixtures with spatially separated\nand nearby talkers. Furthermore, spatial cues enhance separation when spectral\ncues are ambiguous, such as when voices are similar. Explicit spatial cues are\nparticularly beneficial when implicit spatial cues are weak. For instance,\nsingle CI microphone recordings provide weaker implicit spatial cues than\nbilateral CIs, but even single CIs benefit from explicit cues. These results\nemphasize the importance of training models on real-world data to improve\ngeneralizability in everyday listening scenarios. Additionally, our statistical\nanalyses offer insights into how data properties influence model performance,\nsupporting the development of efficient speech separation approaches for CIs\nand other assistive devices in real-world settings.", "published": "2025-01-24 16:30:58", "link": "http://arxiv.org/abs/2501.14610v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
