{"title": "CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs", "abstract": "This paper is the second in a series of studies on developing efficient\nartificial intelligence-based approaches to pathfinding on extremely large\ngraphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical\napplications. The open-source CayleyPy project is a central component of our\nresearch. The present paper proposes a novel combination of a reinforcement\nlearning approach with a more direct diffusion distance approach from the first\npaper. Our analysis includes benchmarking various choices for the key building\nblocks of the approach: architectures of the neural network, generators for the\nrandom walks and beam search pathfinding. We compared these methods against the\nclassical computer algebra system GAP, demonstrating that they \"overcome the\nGAP\" for the considered examples. As a particular mathematical application we\nexamine the Cayley graph of the symmetric group with cyclic shift and\ntransposition generators. We provide strong support for the OEIS-A186783\nconjecture that the diameter is equal to n(n-1)/2 by machine learning and\nmathematical methods. We identify the conjectured longest element and generate\nits decomposition of the desired length. We prove a diameter lower bound of\nn(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm\nwith given complexity. We also present several conjectures motivated by\nnumerical experiments, including observations on the central limit phenomenon\n(with growth approximated by a Gumbel distribution), the uniform distribution\nfor the spectrum of the graph, and a numerical study of sorting networks. To\nstimulate crowdsourcing activity, we create challenges on the Kaggle platform\nand invite contributions to improve and benchmark approaches on Cayley graph\npathfinding and other tasks.", "published": "2025-02-25 21:53:41", "link": "http://arxiv.org/abs/2502.18663v1", "categories": ["cs.LG", "cs.DM", "cs.SI", "math.CO", "math.GR"], "primary_category": "cs.LG"}
{"title": "Unbent Collections of Orthogonal Drawings", "abstract": "Recently, there has been interest in representing single graphs by multiple\ndrawings; for example, using graph stories, storyplans, or uncrossed\ncollections. In this paper, we apply this idea to orthogonal graph drawing. Due\nto the orthogonal drawing style, we focus on plane 4-graphs, that is, planar\ngraphs of maximum degree 4 whose embedding is fixed. Our goal is to represent\nany plane 4-graph $G$ by an unbent collection, that is, a collection of\northogonal drawings of $G$ that adhere to the embedding of $G$ and ensure that\neach edge of $G$ is drawn without bends in at least one of the drawings. We\ninvestigate two objectives. First, we consider minimizing the number of\ndrawings in an unbent collection. We prove that every plane 4-graph can be\nrepresented by a collection with at most three drawings, which is tight. We\nalso give sufficient conditions for a graph to admit an unbent collection of\nsize 2. Second, we consider minimizing the total number of bends over all\ndrawings in an unbent collection. We show that this problem is NP-hard and give\na 3-approximation algorithm. For the special case of plane triconnected cubic\ngraphs, we show how to compute minimum-bend collections in linear time.", "published": "2025-02-25 17:35:01", "link": "http://arxiv.org/abs/2502.18390v1", "categories": ["cs.CG", "cs.DM", "cs.DS", "math.CO", "68R10"], "primary_category": "cs.CG"}
{"title": "Property Testing in Bounded Degree Hypergraphs", "abstract": "We extend the bounded degree graph model for property testing introduced by\nGoldreich and Ron (Algorithmica, 2002) to hypergraphs. In this framework, we\nanalyse the query complexity of three fundamental hypergraph properties:\ncolorability, $k$-partiteness, and independence number. We present a randomized\nalgorithm for testing $k$-partiteness within families of $k$-uniform $n$-vertex\nhypergraphs of bounded treewidth whose query complexity does not depend on $n$.\nIn addition, we prove optimal lower bounds of $\\Omega(n)$ on the query\ncomplexity of testing algorithms for $k$-colorability, $k$-partiteness, and\nindependence number in $k$-uniform $n$-vertex hypergraphs of bounded degree.\nFor each of these properties, we consider the problem of explicitly\nconstructing $k$-uniform hypergraphs of bounded degree that differ in\n$\\Theta(n)$ hyperedges from any hypergraph satisfying the property, but where\nviolations of the latter cannot be detected in any neighborhood of $o(n)$\nvertices.", "published": "2025-02-25 17:25:16", "link": "http://arxiv.org/abs/2502.18382v2", "categories": ["cs.CC", "cs.DM"], "primary_category": "cs.CC"}
{"title": "Graph Inference with Effective Resistance Queries", "abstract": "The goal of graph inference is to design algorithms for learning properties\nof a hidden graph using queries to an oracle that returns information about the\ngraph. Graph reconstruction, verification, and property testing are all types\nof graph inference.\n  In this work, we study graph inference using an oracle that returns the\neffective resistance (ER) between a pair of vertices. Effective resistance is a\ndistance originating from the study of electrical circuits with many\napplications. However, ER has received little attention from a graph inference\nperspective. Indeed, although it is known that an $n$-vertex graph can be\nuniquely reconstructed from all $\\binom{n}{2}$ possible ER queries, little else\nis known. We address this gap with several new results, including:\n  1. $O(n)$-query algorithms for testing whether a graph is a tree; deciding\nwhether two graphs are equal assuming one is a subgraph of the other; and\ntesting whether a given vertex (or edge) is a cut vertex (or cut edge).\n  2. Property testing algorithms, including for testing whether a graph is\nvertex- or edge-biconnected. We also give a reduction to adapt property testing\nresults from the bounded-degree model to our ER query model. This yields\nER-query-based algorithms for testing $k$-connectivity, bipartiteness,\nplanarity, and containment of a fixed subgraph.\n  3. Graph reconstruction algorithms, including an algorithm for reconstructing\na graph from a low-width tree decomposition; a $\\Theta(k^2)$-query,\npolynomial-time algorithm for recovering the adjacency matrix $A$ of a hidden\ngraph, given $A$ with $k$ of its entries deleted; and a $k$-query,\nexponential-time algorithm for the same task.\n  We also compare the power of ER queries and shortest path queries, which are\nclosely related but better studied. Interestingly, we show that the two query\nmodels are incomparable in power.", "published": "2025-02-25 16:37:25", "link": "http://arxiv.org/abs/2502.18350v1", "categories": ["cs.DS", "cs.DM", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Local iterative algorithms for approximate symmetry guided by network centralities", "abstract": "Recently, the influence of potentially present symmetries has begun to be\nstudied in complex networks. A typical way of studying symmetries is via the\nautomorphism group of the corresponding graph. Since complex networks are often\nsubject to uncertainty and automorphisms are very sensitive to small changes,\nthis characterization needs to be modified to an approximate version for\nsuccessful application. This paper considers a recently introduced approximate\nsymmetry of complex networks computed as an automorphism with acceptance of\nsmall edge preservation error, see Liu 2020. This problem is generally very\nhard with respect to the large space of candidate permutations, and hence the\ncorresponding computation methods typically lead to the utilization of local\nalgorithms such as the simulated annealing used in the original work. This\npaper proposes a new heuristic algorithm extending such iterative search\nalgorithm method by using network centralities as heuristics. Centralities are\nshown to be a good tool to navigate the local search towards more appropriate\npermutations and lead to better search results.", "published": "2025-02-25 12:38:40", "link": "http://arxiv.org/abs/2502.18155v1", "categories": ["cs.SI", "cs.DM", "cs.NA", "math.NA", "05C60, 05C82, 68R10, 68W25", "G.2.2; F.2.2"], "primary_category": "cs.SI"}
{"title": "Merge-width and First-Order Model Checking", "abstract": "We introduce merge-width, a family of graph parameters that unifies several\nstructural graph measures, including treewidth, degeneracy, twin-width,\nclique-width, and generalized coloring numbers. Our parameters are based on new\ndecompositions called construction sequences. These are sequences of ever\ncoarser partitions of the vertex set, where each pair of parts has a specified\ndefault connection, and all vertex pairs of the graph that differ from the\ndefault are marked as resolved. The radius-$r$ merge-width is the maximum\nnumber of parts reached from a vertex by following a path of at most $r$\nresolved edges. Graph classes of bounded merge-width -- for which the\nradius-$r$ merge-width parameter can be bounded by a constant, for each fixed\n$r=1,2,3,\\ldots$ -- include all classes of bounded expansion or of bounded\ntwin-width, thus unifying two central notions from the Sparsity and Twin-width\nframeworks. Furthermore, they are preserved under first-order transductions,\nwhich attests to their robustness. We conjecture that classes of bounded\nmerge-width are equivalent to the previously introduced classes of bounded\nflip-width.\n  As our main result, we show that the model checking problem for first-order\nlogic is fixed-parameter tractable on graph classes of bounded merge-width,\nassuming the input includes a witnessing construction sequence. This unites and\nextends two previous model checking results: the result of Dvo\\v{r}\\'{a}k,\nKr\\'{a}l, and Thomas for classes of bounded expansion, and the result of\nBonnet, Kim, Thomass\\'e, and Watrigant for classes of bounded twin-width.\n  Finally, we suggest future research directions that could impact the study of\nstructural and algorithmic graph theory, in particular of monadically dependent\ngraph classes, which we conjecture to coincide with classes of almost bounded\nmerge-width.", "published": "2025-02-25 10:31:58", "link": "http://arxiv.org/abs/2502.18065v1", "categories": ["math.CO", "cs.DM", "cs.DS", "cs.LO"], "primary_category": "math.CO"}
{"title": "An unconditional lower bound for the active-set method on the hypercube", "abstract": "The existence of a polynomial-time pivot rule for the simplex method is a\nfundamental open question in optimization. While many super-polynomial lower\nbounds exist for individual or very restricted classes of pivot rules, there\ncurrently is little hope for an unconditional lower bound that addresses all\npivot rules. We approach this question by considering the active-set method as\na natural generalization of the simplex method to non-linear objectives. This\ngeneralization allows us to prove the first unconditional lower bound for all\npivot rules. More precisely, we construct a multivariate polynomial of degree\nlinear in the number of dimensions such that the active-set method started in\nthe origin visits all vertices of the hypercube. We hope that our framework\nserves as a starting point for a new angle of approach to understanding the\ncomplexity of the simplex method.", "published": "2025-02-25 09:28:43", "link": "http://arxiv.org/abs/2502.18019v1", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "primary_category": "cs.DM"}
{"title": "Adaptive Nesterov Accelerated Distributional Deep Hedging for Efficient Volatility Risk Management", "abstract": "In the field of financial derivatives trading, managing volatility risk is\ncrucial for protecting investment portfolios from market changes. Traditional\nVega hedging strategies, which often rely on basic and rule-based models, are\nhard to adapt well to rapidly changing market conditions. We introduce a new\nframework for dynamic Vega hedging, the Adaptive Nesterov Accelerated\nDistributional Deep Hedging (ANADDH), which combines distributional\nreinforcement learning with a tailored design based on adaptive Nesterov\nacceleration. This approach improves the learning process in complex financial\nenvironments by modeling the hedging efficiency distribution, providing a more\naccurate and responsive hedging strategy. The design of adaptive Nesterov\nacceleration refines gradient momentum adjustments, significantly enhancing the\nstability and speed of convergence of the model. Through empirical analysis and\ncomparisons, our method demonstrates substantial performance gains over\nexisting hedging techniques. Our results confirm that this innovative\ncombination of distributional reinforcement learning with the proposed\noptimization techniques improves financial risk management and highlights the\npractical benefits of implementing advanced neural network architectures in the\nfinance sector.", "published": "2025-02-25 02:12:16", "link": "http://arxiv.org/abs/2502.17777v1", "categories": ["cs.LG", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Exactly solvable model of the square-root price impact dynamics under the long-range market-order correlation", "abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.", "published": "2025-02-25 07:12:03", "link": "http://arxiv.org/abs/2502.17906v2", "categories": ["q-fin.TR", "cond-mat.stat-mech", "econ.GN", "q-fin.EC", "q-fin.MF", "q-fin.PR"], "primary_category": "q-fin.TR"}
{"title": "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks", "abstract": "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.", "published": "2025-02-25 13:11:24", "link": "http://arxiv.org/abs/2502.18177v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena", "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved performance in natural language processing tasks. However, their\nability to generalize to dynamic, unseen tasks, particularly in numerical\nreasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on\nproblems with predefined optimal solutions, which may not align with real-world\nscenarios where clear answers are absent. To bridge this gap, we design the\nAgent Trading Arena, a virtual numerical game simulating complex economic\nsystems through zero-sum games, where agents invest in stock portfolios. Our\nexperiments reveal that LLMs, including GPT-4o, struggle with algebraic\nreasoning when dealing with plain-text stock data, often focusing on local\ndetails rather than global trends. In contrast, LLMs perform significantly\nbetter with geometric reasoning when presented with visual data, such as\nscatter plots or K-line charts, suggesting that visual representations enhance\nnumerical reasoning. This capability is further improved by incorporating the\nreflection module, which aids in the analysis and interpretation of complex\ndata. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate\nstronger reasoning with visual data compared to text. Our code and data are\npublicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.", "published": "2025-02-25 08:41:01", "link": "http://arxiv.org/abs/2502.17967v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "To Make, or to Take, That Is the Question: Impact of LOB Mechanics on Natural Trading Strategies", "abstract": "Working at a very granular level, using data from a live trading experiment\non the Binance linear Bitcoin perpetual-the most liquid crypto market\nworldwide-we examine the effects of (i) basic order book mechanics and (ii) the\nstrong persistence of price changes from the immediate to the short timescale,\nrevealing the interplay between returns, queue sizes, and orders' queue\npositions. For maker orders, we find a negative correlation between fill\nlikelihood and subsequent short-term returns, posing a significant challenge\nfor maker order-based strategies, while the main hurdle with taker orders is\novercoming the taker fee. These dynamics render natural (and commonly-cited)\ntrading strategies highly unprofitable. Finally, we use the understanding\ngained to identify situations (Reversals) in which a successful trading\nstrategy can operate; we construct a signal for Reversals and demonstrate its\nefficacy.", "published": "2025-02-25 20:22:59", "link": "http://arxiv.org/abs/2502.18625v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks", "abstract": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.", "published": "2025-02-25 02:10:30", "link": "http://arxiv.org/abs/2502.17775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Human Evaluation in Machine Translation with Comparative\n  Judgment", "abstract": "Human evaluation is crucial for assessing rapidly evolving language models\nbut is influenced by annotator proficiency and task design. This study explores\nthe integration of comparative judgment into human annotation for machine\ntranslation (MT) and evaluates three annotation setups-point-wise\nMultidimensional Quality Metrics (MQM), side-by-side (SxS) MQM, and its\nsimplified version SxS relative ranking (RR). In MQM, annotators mark error\nspans with categories and severity levels. SxS MQM extends MQM to pairwise\nerror annotation for two translations of the same input, while SxS RR focuses\non selecting the better output without labeling errors.\n  Key findings are: (1) the SxS settings achieve higher inter-annotator\nagreement than MQM; (2) SxS MQM enhances inter-translation error marking\nconsistency compared to MQM by, on average, 38.5% for explicitly compared MT\nsystems and 19.5% for others; (3) all annotation settings return stable system\nrankings, with SxS RR offering a more efficient alternative to (SxS) MQM; (4)\nthe SxS settings highlight subtle errors overlooked in MQM without altering\nabsolute system evaluations.\n  To spur further research, we will release the triply annotated datasets\ncomprising 377 ZhEn and 104 EnDe annotation examples.", "published": "2025-02-25 03:02:24", "link": "http://arxiv.org/abs/2502.17797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your Language Model May Think Too Rigidly: Achieving Reasoning\n  Consistency with Symmetry-Enhanced Training", "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nacross various tasks. However, even minor variations in query phrasing, despite\npreserving the underlying semantic meaning, can significantly affect their\nperformance. To address this, we focus on enhancing LLMs' awareness of symmetry\nin query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a\ndata-centric approach that improves the model's ability to extract useful\ninformation from context. Unlike existing methods that emphasize reasoning\nchain augmentation, our approach improves model robustness at the knowledge\nextraction stage through query augmentations, enabling more data-efficient\ntraining and stronger generalization to Out-of-Distribution (OOD) settings.\nExtensive experiments on both logical and arithmetic reasoning tasks show that\nMEND enhances reasoning performance across diverse query variations, providing\nnew insight into improving LLM robustness through structured dataset curation.", "published": "2025-02-25 03:03:35", "link": "http://arxiv.org/abs/2502.17800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Through Generation: Why Generation Is Better for Prediction", "abstract": "This paper argues that generating output tokens is more effective than using\npooled representations for prediction tasks because token-level generation\nretains more mutual information. Since LLMs are trained on massive text corpora\nusing next-token prediction, generation aligns naturally with their learned\nbehavior. Using the Data Processing Inequality (DPI), we provide both\ntheoretical and empirical evidence supporting this claim. However,\nautoregressive models face two key challenges when used for prediction: (1)\nexposure bias, where the model sees ground truth tokens during training but\nrelies on its own predictions during inference, leading to errors, and (2)\nformat mismatch, where discrete tokens do not always align with the tasks\nrequired output structure. To address these challenges, we introduce\nPredGen(Predicting Through Generating), an end to end framework that (i) uses\nscheduled sampling to reduce exposure bias, and (ii) introduces a task adapter\nto convert the generated tokens into structured outputs. Additionally, we\nintroduce Writer-Director Alignment Loss (WDAL), which ensures consistency\nbetween token generation and final task predictions, improving both text\ncoherence and numerical accuracy. We evaluate PredGen on multiple\nclassification and regression benchmarks. Our results show that PredGen\nconsistently outperforms standard baselines, demonstrating its effectiveness in\nstructured prediction tasks.", "published": "2025-02-25 03:48:19", "link": "http://arxiv.org/abs/2502.17817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems", "abstract": "Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems\n(CSPs) where reflective reasoning is crucial for deriving solutions that meet\nall given constraints. Each type of task focuses on distinct constraint\npatterns, such as knowledge-based, logical, and spatial constraints, providing\na comprehensive evaluation of diverse problem-solving scenarios. We conduct\nextensive evaluation on both conventional models and o1-like models. Our\nexperimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,\nrespectively. These findings underscore the significant room for improvement in\nthe reflective reasoning capabilities of current LLMs. The leaderboard of our\nbenchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench", "published": "2025-02-25 04:51:17", "link": "http://arxiv.org/abs/2502.17848v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any\n  Crowdsourcing", "abstract": "Previous research has shown that humans are more receptive towards language\nmodels that that exhibit empathetic behavior. While empathy is essential for\ndeveloping helpful dialogue agents, very few large corpora containing\nempathetic dialogues are available for fine-tune LLMs. The few existing corpora\nhave largely relied on crowdsourcing to simulate empathetic conversations, a\nprocess that is expensive, time-consuming, and not scalable to larger datasets.\nWe propose a data generation framework for developing SYNTHEMPATHY, a large\ncorpus containing 105k empathetic responses to real-life situations compiled\nthrough LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY\ncorpus exhibits an increase in the average empathy score.", "published": "2025-02-25 05:07:27", "link": "http://arxiv.org/abs/2502.17857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama", "abstract": "LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the\nuser (i.e. the player) plays the role of a character in the story, has\nconversations with characters played by LLM agents, and experiences an\nunfolding story. This paper begins with understanding interactive drama from\ntwo aspects: Immersion, the player's feeling of being present in the story, and\nAgency, the player's ability to influence the story world. Both are crucial to\ncreating an enjoyable interactive experience, while they have been\nunderexplored in previous work. To enhance these two aspects, we first propose\nPlaywriting-guided Generation, a novel method that helps LLMs craft dramatic\nstories with substantially improved structures and narrative quality.\nAdditionally, we introduce Plot-based Reflection for LLM agents to refine their\nreactions to align with the player's intentions. Our evaluation relies on human\njudgment to assess the gains of our methods in terms of immersion and agency.", "published": "2025-02-25 06:06:16", "link": "http://arxiv.org/abs/2502.17878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through\n  Ranking Chain-of-Thoughts", "abstract": "Retrieval-Augmented Generation (RAG) enhances the performance of Large\nLanguage Models (LLMs) by incorporating external knowledge. However, LLMs still\nencounter challenges in effectively utilizing the knowledge from retrieved\ndocuments, often being misled by irrelevant or noisy information. To address\nthis issue, we introduce RankCoT, a knowledge refinement method that\nincorporates reranking signals in generating CoT-based summarization for\nknowledge refinement based on given query and all retrieval documents. During\ntraining, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates\nbased on the query and individual documents. It then fine-tunes the LLM to\ndirectly reproduce the best CoT from these candidate outputs based on all\nretrieved documents, which requires LLM to filter out irrelevant documents\nduring generating CoT-style summarization. Additionally, RankCoT incorporates a\nself-reflection mechanism that further refines the CoT outputs, resulting in\nhigher-quality training data. Our experiments demonstrate the effectiveness of\nRankCoT, showing its superior performance over other knowledge refinement\nmodels. Further analysis reveals that RankCoT can provide shorter but effective\nrefinement results, enabling the generator to produce more accurate answers.\nAll code and data are available at https://github.com/NEUIR/RankCoT.", "published": "2025-02-25 06:18:05", "link": "http://arxiv.org/abs/2502.17888v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Identify Implicit Suicidal Ideation? An\n  Empirical Evaluation", "abstract": "We present a comprehensive evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in suicide prevention, focusing on two critical\naspects: the Identification of Implicit Suicidal ideation (IIS) and the\nProvision of Appropriate Supportive responses (PAS). We introduce \\ourdata, a\nnovel dataset of 1,308 test cases built upon psychological frameworks including\nD/S-IAT and Negative Automatic Thinking, alongside real-world scenarios.\nThrough extensive experiments with 8 widely used LLMs under different\ncontextual settings, we find that current models struggle significantly with\ndetecting implicit suicidal ideation and providing appropriate support,\nhighlighting crucial limitations in applying LLMs to mental health contexts.\nOur findings underscore the need for more sophisticated approaches in\ndeveloping and evaluating LLMs for sensitive psychological applications.", "published": "2025-02-25 06:53:00", "link": "http://arxiv.org/abs/2502.17899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking\n  Evaluation of Large Language Models", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking\nstudies. However, existing automated fact-checking evaluation methods rely on\nstatic datasets and classification metrics, which fail to automatically\nevaluate the justification production and uncover the nuanced limitations of\nLLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven\nframework that adaptively and dynamically assesses LLMs' fact-checking\ncapabilities. Leveraging importance sampling principles and multi-agent\ncollaboration, FACT-AUDIT generates adaptive and scalable datasets, performs\niterative model-centric evaluations, and updates assessments based on\nmodel-specific responses. By incorporating justification production alongside\nverdict prediction, this framework provides a comprehensive and evolving audit\nof LLMs' factual reasoning capabilities, to investigate their trustworthiness.\nExtensive experiments demonstrate that FACT-AUDIT effectively differentiates\namong state-of-the-art LLMs, providing valuable insights into model strengths\nand limitations in model-centric fact-checking analysis.", "published": "2025-02-25 07:44:22", "link": "http://arxiv.org/abs/2502.17924v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advantage-Guided Distillation for Preference Alignment in Small Language\n  Models", "abstract": "Alignment techniques enable Large Language Models (LLMs) to generate outputs\nthat align with human preferences and play a crucial role in their\neffectiveness. However, their impact often diminishes when applied to Small\nLanguage Models (SLMs), likely due to the limited capacity of these models.\nInstead of directly applying existing alignment techniques to SLMs, we propose\nto utilize a well-aligned teacher LLM to guide the alignment process for these\nmodels, thereby facilitating the transfer of the teacher's knowledge of human\npreferences to the student model. To achieve this, we first explore a\nstraightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that\nemploys knowledge distillation with two KL-divergence constraints from the\naligned teacher to the unaligned student. To further enhance the student's\nability to distinguish between preferred and dispreferred responses, we then\npropose Advantage-Guided Distillation for Preference Alignment (ADPA), which\nleverages an advantage function from the aligned teacher to deliver more\nnuanced, distribution-level reward signals for the student's alignment. Our\nexperimental results show that these two approaches appreciably improve the\nalignment of SLMs and narrow the performance gap with larger counterparts.\nAmong them, ADPA demonstrates superior performance and achieves even greater\neffectiveness when integrated with DCKD. Our code is available at\nhttps://github.com/SLIT-AI/ADPA.", "published": "2025-02-25 07:47:22", "link": "http://arxiv.org/abs/2502.17927v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation", "abstract": "Legal case documents play a critical role in judicial proceedings. As the\nnumber of cases continues to rise, the reliance on manual drafting of legal\ncase documents is facing increasing pressure and challenges. The development of\nlarge language models (LLMs) offers a promising solution for automating\ndocument generation. However, existing benchmarks fail to fully capture the\ncomplexities involved in drafting legal case documents in real-world scenarios.\nTo address this gap, we introduce CaseGen, the benchmark for multi-stage legal\ncase documents generation in the Chinese legal domain. CaseGen is based on 500\nreal case samples annotated by legal experts and covers seven essential case\nsections. It supports four key tasks: drafting defense statements, writing\ntrial facts, composing legal reasoning, and generating judgment results. To the\nbest of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs\nin the context of legal case document generation. To ensure an accurate and\ncomprehensive evaluation, we design the LLM-as-a-judge evaluation framework and\nvalidate its effectiveness through human annotations. We evaluate several\nwidely used general-domain LLMs and legal-specific LLMs, highlighting their\nlimitations in case document generation and pinpointing areas for potential\nimprovement. This work marks a step toward a more effective framework for\nautomating legal case documents drafting, paving the way for the reliable\napplication of AI in the legal field. The dataset and code are publicly\navailable at https://github.com/CSHaitao/CaseGen.", "published": "2025-02-25 08:03:32", "link": "http://arxiv.org/abs/2502.17943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Large Language Models in Agentic Multilingual National Bias", "abstract": "Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education.", "published": "2025-02-25 08:07:42", "link": "http://arxiv.org/abs/2502.17945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments", "abstract": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic.", "published": "2025-02-25 08:27:28", "link": "http://arxiv.org/abs/2502.17956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning", "abstract": "Large Language Models (LLMs) excel in reasoning tasks through\nChain-of-Thought (CoT) prompting. However, CoT prompting greatly increases\ncomputational demands, which has prompted growing interest in distilling CoT\ncapabilities into Small Language Models (SLMs). This study systematically\nexamines the factors influencing CoT distillation, including the choice of\ngranularity, format and teacher model. Through experiments involving four\nteacher models and seven student models across seven mathematical and\ncommonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,\nSLMs exhibit a non-monotonic relationship with granularity, with stronger\nmodels benefiting from finer-grained reasoning and weaker models performing\nbetter with simpler CoT supervision; (2) CoT format significantly impacts LLMs\nbut has minimal effect on SLMs, likely due to their reliance on supervised\nfine-tuning rather than pretraining preferences; (3) Stronger teacher models do\nNOT always produce better student models, as diversity and complexity in CoT\nsupervision can outweigh accuracy alone. These findings emphasize the need to\ntailor CoT strategies to specific student model, offering actionable insights\nfor optimizing CoT distillation in SLMs. The code and datasets are available at\nhttps://github.com/EIT-NLP/Distilling-CoT-Reasoning.", "published": "2025-02-25 09:08:45", "link": "http://arxiv.org/abs/2502.18001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verdict: A Library for Scaling Judge-Time Compute", "abstract": "The use of LLMs as automated judges (\"LLM-as-a-judge\") is now widespread, yet\nstandard judges suffer from a multitude of reliability issues. To address these\nchallenges, we introduce Verdict, an open-source library for scaling judge-time\ncompute to enhance the accuracy, reliability, and interpretability of automated\nevaluators. Verdict leverages the composition of modular reasoning units --\nsuch as verification, debate, and aggregation -- and increased inference-time\ncompute to improve LLM judge quality. Across a variety of challenging tasks\nsuch as content moderation, fact-checking, and hallucination detection, Verdict\njudges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing\norders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning\nmodels. Ultimately, we hope Verdict serves as a useful framework for\nresearchers and practitioners building scalable, interpretable, and reliable\nLLM-based evaluators.", "published": "2025-02-25 09:26:44", "link": "http://arxiv.org/abs/2502.18018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference", "abstract": "Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary", "published": "2025-02-25 09:32:08", "link": "http://arxiv.org/abs/2502.18023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble", "abstract": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.", "published": "2025-02-25 09:48:53", "link": "http://arxiv.org/abs/2502.18036v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty-aware abstention in medical diagnosis based on medical texts", "abstract": "This study addresses the critical issue of reliability for AI-assisted\nmedical diagnosis. We focus on the selection prediction approach that allows\nthe diagnosis system to abstain from providing the decision if it is not\nconfident in the diagnosis. Such selective prediction (or abstention)\napproaches are usually based on the modeling predictive uncertainty of machine\nlearning models involved.\n  This study explores uncertainty quantification in machine learning models for\nmedical text analysis, addressing diverse tasks across multiple datasets. We\nfocus on binary mortality prediction from textual data in MIMIC-III,\nmulti-label medical code prediction using ICD-10 codes from MIMIC-IV, and\nmulti-class classification with a private outpatient visits dataset.\nAdditionally, we analyze mental health datasets targeting depression and\nanxiety detection, utilizing various text-based sources, such as essays, social\nmedia posts, and clinical descriptions.\n  In addition to comparing uncertainty methods, we introduce HUQ-2, a new\nstate-of-the-art method for enhancing reliability in selective prediction\ntasks. Our results provide a detailed comparison of uncertainty quantification\nmethods. They demonstrate the effectiveness of HUQ-2 in capturing and\nevaluating uncertainty, paving the way for more reliable and interpretable\napplications in medical text analysis.", "published": "2025-02-25 10:15:21", "link": "http://arxiv.org/abs/2502.18050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification in Retrieval Augmented Question Answering", "abstract": "Retrieval augmented Question Answering (QA) helps QA models overcome\nknowledge gaps by incorporating retrieved evidence, typically a set of\npassages, alongside the question at test time. Previous studies show that this\napproach improves QA performance and reduces hallucinations, without, however,\nassessing whether the retrieved passages are indeed useful at answering\ncorrectly. In this work, we propose to quantify the uncertainty of a QA model\nvia estimating the utility of the passages it is provided with. We train a\nlightweight neural model to predict passage utility for a target QA model and\nshow that while simple information theoretic metrics can predict answer\ncorrectness up to a certain extent, our approach efficiently approximates or\noutperforms more expensive sampling-based methods. Code and data are available\nat https://github.com/lauhaide/ragu.", "published": "2025-02-25 11:24:52", "link": "http://arxiv.org/abs/2502.18108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts", "abstract": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "published": "2025-02-25 12:23:52", "link": "http://arxiv.org/abs/2502.18148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting Voices: LoReSpeech as a Low-Resource Speech Parallel Corpus", "abstract": "Aligned audio corpora are fundamental to NLP technologies such as ASR and\nspeech translation, yet they remain scarce for underrepresented languages,\nhindering their technological integration. This paper introduces a methodology\nfor constructing LoReSpeech, a low-resource speech-to-speech translation\ncorpus. Our approach begins with LoReASR, a sub-corpus of short audios aligned\nwith their transcriptions, created through a collaborative platform. Building\non LoReASR, long-form audio recordings, such as biblical texts, are aligned\nusing tools like the MFA. LoReSpeech delivers both intra- and inter-language\nalignments, enabling advancements in multilingual ASR systems, direct\nspeech-to-speech translation models, and linguistic preservation efforts, while\nfostering digital inclusivity. This work is conducted within Tutlayt AI project\n(https://tutlayt.fr).", "published": "2025-02-25 14:00:15", "link": "http://arxiv.org/abs/2502.18215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent", "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain.", "published": "2025-02-25 14:13:03", "link": "http://arxiv.org/abs/2502.18228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond In-Distribution Success: Scaling Curves of CoT Granularity for\n  Language Model Generalization", "abstract": "Generalization to novel compound tasks under distribution shift is important\nfor deploying transformer-based language models (LMs). This work investigates\nChain-of-Thought (CoT) reasoning as a means to enhance OOD generalization.\nThrough controlled experiments across several compound tasks, we reveal three\nkey insights: (1) While QA-trained models achieve near-perfect in-distribution\naccuracy, their OOD performance degrades catastrophically, even with 10000k+\ntraining examples; (2) the granularity of CoT data strongly correlates with\ngeneralization performance; finer-grained CoT data leads to better\ngeneralization; (3) CoT exhibits remarkable sample efficiency, matching QA\nperformance with much less (even 80%) data.\n  Theoretically, we demonstrate that compound tasks inherently permit shortcuts\nin Q-A data that misalign with true reasoning principles, while CoT forces\ninternalization of valid dependency structures, and thus can achieve better\ngeneralization. Further, we show that transformer positional embeddings can\namplify generalization by emphasizing subtask condition recurrence in long CoT\nsequences. Our combined theoretical and empirical analysis provides compelling\nevidence for CoT reasoning as a crucial training paradigm for enabling LM\ngeneralization under real-world distributional shifts for compound tasks.", "published": "2025-02-25 15:04:17", "link": "http://arxiv.org/abs/2502.18273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Adjust Softmax", "abstract": "The softmax function is crucial in Transformer attention, which normalizes\neach row of the attention scores with summation to one, achieving superior\nperformances over other alternative functions. However, the softmax function\ncan face a gradient vanishing issue when some elements of the attention scores\napproach extreme values, such as probabilities close to one or zero. In this\npaper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by\nmodifying $softmax(x)$ to $x \\cdot softmax(x)$ and its normalized variant\n$\\frac{(x - min(x_{\\min},0))}{max(0,x_{max})-min(x_{min},0)} \\cdot softmax(x)$.\nWe theoretically show that SA-Softmax provides enhanced gradient properties\ncompared to the vanilla softmax function. Moreover, SA-Softmax Attention can be\nseamlessly integrated into existing Transformer models to their attention\nmechanisms with minor adjustments. We conducted experiments to evaluate the\nempirical performance of Transformer models using SA-Softmax compared to the\nvanilla softmax function. These experiments, involving models with up to 2.7\nbillion parameters, are conducted across diverse datasets, language tasks, and\npositional encoding methods.", "published": "2025-02-25 15:07:40", "link": "http://arxiv.org/abs/2502.18277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases", "abstract": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.", "published": "2025-02-25 15:16:17", "link": "http://arxiv.org/abs/2502.18282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Modeling in Multimodal Speech Analysis Across the Psychosis\n  Spectrum", "abstract": "Capturing subtle speech disruptions across the psychosis spectrum is\nchallenging because of the inherent variability in speech patterns. This\nvariability reflects individual differences and the fluctuating nature of\nsymptoms in both clinical and non-clinical populations. Accounting for\nuncertainty in speech data is essential for predicting symptom severity and\nimproving diagnostic precision. Speech disruptions characteristic of psychosis\nappear across the spectrum, including in non-clinical individuals. We develop\nan uncertainty-aware model integrating acoustic and linguistic features to\npredict symptom severity and psychosis-related traits. Quantifying uncertainty\nin specific modalities allows the model to address speech variability,\nimproving prediction accuracy. We analyzed speech data from 114 participants,\nincluding 32 individuals with early psychosis and 82 with low or high\nschizotypy, collected through structured interviews, semi-structured\nautobiographical tasks, and narrative-driven interactions in German. The model\nimproved prediction accuracy, reducing RMSE and achieving an F1-score of 83%\nwith ECE = 4.5e-2, showing robust performance across different interaction\ncontexts. Uncertainty estimation improved model interpretability by identifying\nreliability differences in speech markers such as pitch variability, fluency\ndisruptions, and spectral instability. The model dynamically adjusted to task\nstructures, weighting acoustic features more in structured settings and\nlinguistic features in unstructured contexts. This approach strengthens early\ndetection, personalized assessment, and clinical decision-making in\npsychosis-spectrum research.", "published": "2025-02-25 15:19:21", "link": "http://arxiv.org/abs/2502.18285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction", "abstract": "In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0", "published": "2025-02-25 15:51:25", "link": "http://arxiv.org/abs/2502.18308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Looking forward: Linguistic theory and methods", "abstract": "This chapter examines current developments in linguistic theory and methods,\nfocusing on the increasing integration of computational, cognitive, and\nevolutionary perspectives. We highlight four major themes shaping contemporary\nlinguistics: (1) the explicit testing of hypotheses about symbolic\nrepresentation, such as efficiency, locality, and conceptual semantic\ngrounding; (2) the impact of artificial neural networks on theoretical debates\nand linguistic analysis; (3) the importance of intersubjectivity in linguistic\ntheory; and (4) the growth of evolutionary linguistics. By connecting\nlinguistics with computer science, psychology, neuroscience, and biology, we\nprovide a forward-looking perspective on the changing landscape of linguistic\nresearch.", "published": "2025-02-25 16:03:15", "link": "http://arxiv.org/abs/2502.18313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging", "abstract": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks.", "published": "2025-02-25 16:09:38", "link": "http://arxiv.org/abs/2502.18316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods", "abstract": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.", "published": "2025-02-25 17:33:20", "link": "http://arxiv.org/abs/2502.18389v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KiRAG: Knowledge-Driven Iterative Retriever for Enhancing\n  Retrieval-Augmented Generation", "abstract": "Iterative retrieval-augmented generation (iRAG) models offer an effective\napproach for multi-hop question answering (QA). However, their retrieval\nprocess faces two key challenges: (1) it can be disrupted by irrelevant\ndocuments or factually inaccurate chain-of-thoughts; (2) their retrievers are\nnot designed to dynamically adapt to the evolving information needs in\nmulti-step reasoning, making it difficult to identify and retrieve the missing\ninformation required at each iterative step. Therefore, we propose KiRAG, which\nuses a knowledge-driven iterative retriever model to enhance the retrieval\nprocess of iRAG. Specifically, KiRAG decomposes documents into knowledge\ntriples and performs iterative retrieval with these triples to enable a\nfactually reliable retrieval process. Moreover, KiRAG integrates reasoning into\nthe retrieval process to dynamically identify and retrieve knowledge that\nbridges information gaps, effectively adapting to the evolving information\nneeds. Empirical results show that KiRAG significantly outperforms existing\niRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on\nmulti-hop QA.", "published": "2025-02-25 17:47:53", "link": "http://arxiv.org/abs/2502.18397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Language Models for Specialized Domains", "abstract": "Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs.", "published": "2025-02-25 18:20:00", "link": "http://arxiv.org/abs/2502.18424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language\n  Models", "abstract": "PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. We present olmOCR, an open-source Python\ntoolkit for processing PDFs into clean, linearized plain text in natural\nreading order while preserving structured content like sections, tables, lists,\nequations, and more. Our toolkit runs a fine-tuned 7B vision language model\n(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and convert a million PDF pages for only\n$190 USD. We release all components of olmOCR including VLM weights, data and\ntraining code, as well as inference code built on serving frameworks including\nvLLM and SGLang.", "published": "2025-02-25 18:38:38", "link": "http://arxiv.org/abs/2502.18443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What are Foundation Models Cooking in the Post-Soviet World?", "abstract": "The culture of the Post-Soviet states is complex, shaped by a turbulent\nhistory that continues to influence current events. In this study, we\ninvestigate the Post-Soviet cultural food knowledge of foundation models by\nconstructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in\nthe Russian and Ukrainian languages, centered around the Post-Soviet region. We\ndemonstrate that leading models struggle to correctly identify the origins of\ndishes from Post-Soviet nations in both text-only and multimodal Question\nAnswering (QA), instead over-predicting countries linked to the language the\nquestion is asked in. Through analysis of pretraining data, we show that these\nresults can be explained by misleading dish-origin co-occurrences, along with\nlinguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move\nbeyond QA-based assessments, we test models' abilities to produce accurate\nvisual descriptions of dishes. The weak correlation between this task and QA\nsuggests that QA alone may be insufficient as an evaluation of cultural\nunderstanding. To foster further research, we will make BORSch publicly\navailable at https://github.com/alavrouk/BORSch.", "published": "2025-02-25 19:09:56", "link": "http://arxiv.org/abs/2502.18583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neurobiber: Fast and Interpretable Stylistic Feature Extraction", "abstract": "Linguistic style is pivotal for understanding how texts convey meaning and\nfulfill communicative purposes, yet extracting detailed stylistic features at\nscale remains challenging. We present Neurobiber, a transformer-based system\nfor fast, interpretable style profiling built on Biber's Multidimensional\nAnalysis (MDA). Neurobiber predicts 96 Biber-style features from our\nopen-source BiberPlus library (a Python toolkit that computes stylistic\nfeatures and provides integrated analytics, e.g., PCA and factor analysis).\nDespite being up to 56 times faster than existing open source systems,\nNeurobiber replicates classic MDA insights on the CORE corpus and achieves\ncompetitive performance on the PAN 2020 authorship verification task without\nextensive retraining. Its efficient and interpretable representations readily\nintegrate into downstream NLP pipelines, facilitating large-scale stylometric\nresearch, forensic analysis, and real-time text monitoring. All components are\nmade publicly available.", "published": "2025-02-25 19:11:56", "link": "http://arxiv.org/abs/2502.18590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain of Draft: Thinking Faster by Writing Less", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks. Our code and data are available at\nhttps://github.com/sileix/chain-of-draft.", "published": "2025-02-25 19:36:06", "link": "http://arxiv.org/abs/2502.18600v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contextual effects of sentiment deployment in human and machine\n  translation", "abstract": "This paper illustrates how the overall sentiment of a text may be shifted in\ntranslation and the implications for automated sentiment analyses, particularly\nthose that utilize machine translation and assess findings via semantic\nsimilarity metrics. While human and machine translation will produce more\nlemmas that fit the expected frequency of sentiment in the target language,\nonly machine translation will also reduce the overall semantic field of the\ntext, particularly in regard to words with epistemic content.", "published": "2025-02-25 21:03:35", "link": "http://arxiv.org/abs/2502.18642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steered Generation via Gradient Descent on Sparse Features", "abstract": "Large language models (LLMs) encode a diverse range of linguistic features\nwithin their latent representations, which can be harnessed to steer their\noutput toward specific target characteristics. In this paper, we modify the\ninternal structure of LLMs by training sparse autoencoders to learn a sparse\nrepresentation of the query embedding, allowing precise control over the\nmodel's attention distribution. We demonstrate that manipulating this sparse\nrepresentation effectively transforms the output toward different stylistic and\ncognitive targets. Specifically, in an educational setting, we show that the\ncognitive complexity of LLM-generated feedback can be systematically adjusted\nby modifying the encoded query representation at a specific layer. To achieve\nthis, we guide the learned sparse embedding toward the representation of\nsamples from the desired cognitive complexity level, using gradient-based\noptimization in the latent space.", "published": "2025-02-25 21:06:14", "link": "http://arxiv.org/abs/2502.18644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews\n  in Human Resources", "abstract": "Optimizing language models for use in conversational agents requires large\nquantities of example dialogues. Increasingly, these dialogues are\nsynthetically generated by using powerful large language models (LLMs),\nespecially in domains with challenges to obtain authentic human data. One such\ndomain is human resources (HR). In this context, we compare two LLM-based\ndialogue generation methods for the use case of generating HR job interviews,\nand assess whether one method generates higher-quality dialogues that are more\nchallenging to distinguish from genuine human discourse. The first method uses\na single prompt to generate the complete interview dialog. The second method\nuses two agents that converse with each other. To evaluate dialogue quality\nunder each method, we ask a judge LLM to determine whether AI was used for\ninterview generation, using pairwise interview comparisons. We demonstrate that\ndespite a sixfold increase in token cost, interviews generated with the\ndual-prompt method achieve a win rate up to ten times higher than those\ngenerated with the single-prompt method. This difference remains consistent\nregardless of whether GPT-4o or Llama 3.3 70B is used for either interview\ngeneration or judging quality.", "published": "2025-02-25 21:19:27", "link": "http://arxiv.org/abs/2502.18650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminative Finetuning of Generative Large Language Models without\n  Reward Models and Preference Data", "abstract": "Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted\nby SFT$\\rightarrow$PO has become the standard for improving pretrained large\nlanguage models (LLMs), with PO demonstrating significant performance gains.\nHowever, PO methods rely on either human-labeled preference data or a strong\nreward model to generate preference data. Can we fine-tune LLMs without\npreference data or reward models while achieving competitive performance to\nSFT$\\rightarrow$PO? We address this question by introducing Discriminative\nFine-Tuning (DFT), a novel approach that eliminates the need for preference\ndata. Unlike SFT, which employs a generative approach and overlooks negative\ndata, DFT adopts a discriminative paradigm that that increases the probability\nof positive answers while suppressing potentially negative ones, shifting from\ntoken prediction to data prediction. Our contributions include: (i) a\ndiscriminative probabilistic framework for fine-tuning LLMs by explicitly\nmodeling the discriminative likelihood of an answer among all possible outputs\ngiven an input; (ii) efficient algorithms to optimize this discriminative\nlikelihood; and (iii) extensive experiments demonstrating DFT's effectiveness,\nachieving performance better than SFT and comparable to if not better than\nSFT$\\rightarrow$PO. The code can be found at https://github.com/PenGuln/DFT.", "published": "2025-02-25 22:38:55", "link": "http://arxiv.org/abs/2502.18679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large\n  Language Models", "abstract": "The diversity of human language, shaped by social, cultural, and regional\ninfluences, presents significant challenges for natural language processing\n(NLP) systems. Existing benchmarks often overlook intra-language variations,\nleaving speakers of non-standard dialects underserved. To address this gap, we\nintroduce EnDive (English Diversity), a benchmark that evaluates five\nwidely-used large language models (LLMs) across tasks in language\nunderstanding, algorithmic reasoning, mathematics, and logic. Our framework\ntranslates Standard American English datasets into five underrepresented\ndialects using few-shot prompting with verified examples from native speakers,\nand compare these translations against rule-based methods via fluency\nassessments, preference tests, and semantic similarity metrics. Human\nevaluations confirm high translation quality, with average scores of at least\n6.02/7 for faithfulness, fluency, and formality. By filtering out\nnear-identical translations, we create a challenging dataset that reveals\nsignificant performance disparities - models consistently underperform on\ndialectal inputs compared to Standard American English. EnDive thus advances\ndialect-aware NLP by uncovering model biases and promoting more equitable\nlanguage technologies.", "published": "2025-02-25 23:48:33", "link": "http://arxiv.org/abs/2504.07100v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MuCoS: Efficient Drug-Target Prediction through Multi-Context-Aware\n  Sampling", "abstract": "Drug-target interactions are critical for understanding biological processes\nand advancing drug discovery. However, traditional methods such as ComplEx-SE,\nTransE, and DistMult struggle with unseen relationships and negative triplets,\nwhich limits their effectiveness in drug-target prediction. To address these\nchallenges, we propose Multi-Context-Aware Sampling (MuCoS), an efficient and\npositively accurate method for drug-target prediction. MuCoS reduces\ncomputational complexity by prioritizing neighbors of higher density to capture\ninformative structural patterns. These optimized neighborhood representations\nare integrated with BERT, enabling contextualized embeddings for accurate\nprediction of missing relationships or tail entities. MuCoS avoids the need for\nnegative triplet sampling, reducing computation while improving performance\nover unseen entities and relations. Experiments on the KEGG50k biomedical\ndataset show that MuCoS improved over existing models by 13\\% on MRR, 7\\% on\nHits@1, 4\\% on Hits@3, and 18\\% on Hits@10 for the general relationship, and by\n6\\% on MRR, 1\\% on Hits@1, 3\\% on Hits@3, and 12\\% on Hits@10 for prediction of\ndrug-target relationship.", "published": "2025-02-25 02:27:32", "link": "http://arxiv.org/abs/2502.17784v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring the Potential of Large Language Models for Estimating the\n  Reading Comprehension Question Difficulty", "abstract": "Reading comprehension is a key for individual success, yet the assessment of\nquestion difficulty remains challenging due to the extensive human annotation\nand large-scale testing required by traditional methods such as linguistic\nanalysis and Item Response Theory (IRT). While these robust approaches provide\nvaluable insights, their scalability is limited. There is potential for Large\nLanguage Models (LLMs) to automate question difficulty estimation; however,\nthis area remains underexplored. Our study investigates the effectiveness of\nLLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of\nreading comprehension questions using the Study Aid and Reading Assessment\n(SARA) dataset. We evaluated both the accuracy of the models in answering\ncomprehension questions and their ability to classify difficulty levels as\ndefined by IRT. The results indicate that, while the models yield difficulty\nestimates that align meaningfully with derived IRT parameters, there are\nnotable differences in their sensitivity to extreme item characteristics. These\nfindings suggest that LLMs can serve as the scalable method for automated\ndifficulty assessment, particularly in dynamic interactions between learners\nand Adaptive Instructional Systems (AIS), bridging the gap between traditional\npsychometric techniques and modern AIS for reading comprehension and paving the\nway for more adaptive and personalized educational assessments.", "published": "2025-02-25 02:28:48", "link": "http://arxiv.org/abs/2502.17785v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement", "abstract": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.", "published": "2025-02-25 02:39:57", "link": "http://arxiv.org/abs/2502.17787v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue\n  Models", "abstract": "In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.", "published": "2025-02-25 03:31:48", "link": "http://arxiv.org/abs/2502.17810v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can Multimodal LLMs Perform Time Series Anomaly Detection?", "abstract": "Large language models (LLMs) have been increasingly used in time series\nanalysis. However, the potential of multimodal LLMs (MLLMs), particularly\nvision-language models, for time series remains largely under-explored. One\nnatural way for humans to detect time series anomalies is through visualization\nand textual description. Motivated by this, we raise a critical and practical\nresearch question: Can multimodal LLMs perform time series anomaly detection?\nTo answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in\ntime series anomaly detection (TSAD). Our approach transforms time series\nnumerical data into the image format and feed these images into various MLLMs,\nincluding proprietary models (GPT-4o and Gemini-1.5) and open-source models\n(LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In\ntotal, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios\nand 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with\nthe univariate case (point- and range-wise anomalies), we extend our evaluation\nto more practical scenarios, including multivariate and irregular time series\nscenarios, and variate-wise anomalies. Our study reveals several key insights:\n  1) MLLMs detect range- and variate-wise anomalies more effectively than\npoint-wise anomalies.\n  2) MLLMs are highly robust to irregular time series, even with 25% of the\ndata missing.\n  3) Open-source MLLMs perform comparably to proprietary models in TSAD. While\nopen-source MLLMs excel on univariate time series, proprietary MLLMs\ndemonstrate superior effectiveness on multivariate time series.\n  To the best of our knowledge, this is the first work to comprehensively\ninvestigate MLLMs for TSAD, particularly for multivariate and irregular time\nseries scenarios. We release our dataset and code at\nhttps://github.com/mllm-ts/VisualTimeAnomaly to support future research.", "published": "2025-02-25 03:37:43", "link": "http://arxiv.org/abs/2502.17812v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A General Framework to Enhance Fine-tuning-based LLM Unlearning", "abstract": "Unlearning has been proposed to remove copyrighted and privacy-sensitive data\nfrom Large Language Models (LLMs). Existing approaches primarily rely on\nfine-tuning-based methods, which can be categorized into gradient ascent-based\n(GA-based) and suppression-based methods. However, they often degrade model\nutility (the ability to respond to normal prompts). In this work, we aim to\ndevelop a general framework that enhances the utility of fine-tuning-based\nunlearning methods. To achieve this goal, we first investigate the common\nproperty between GA-based and suppression-based methods. We unveil that\nGA-based methods unlearn by distinguishing the target data (i.e., the data to\nbe removed) and suppressing related generations, which is essentially the same\nstrategy employed by suppression-based methods. Inspired by this finding, we\nintroduce Gated Representation UNlearning (GRUN) which has two components: a\nsoft gate function for distinguishing target data and a suppression module\nusing Representation Fine-tuning (ReFT) to adjust representations rather than\nmodel parameters. Experiments show that GRUN significantly improves the\nunlearning and utility. Meanwhile, it is general for fine-tuning-based methods,\nefficient and promising for sequential unlearning.", "published": "2025-02-25 04:03:04", "link": "http://arxiv.org/abs/2502.17823v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Science Across Languages: Assessing LLM Multilingual Translation of\n  Scientific Papers", "abstract": "Scientific research is inherently global. However, the vast majority of\nacademic journals are published exclusively in English, creating barriers for\nnon-native-English-speaking researchers. In this study, we leverage large\nlanguage models (LLMs) to translate published scientific articles while\npreserving their native JATS XML formatting, thereby developing a practical,\nautomated approach for implementation by academic journals. Using our approach,\nwe translate articles across multiple scientific disciplines into 28 languages.\nTo evaluate translation accuracy, we introduce a novel question-and-answer (QA)\nbenchmarking method, in which an LLM generates comprehension-based questions\nfrom the original text and then answers them based on the translated text. Our\nbenchmark results show an average performance of 95.9%, showing that the key\nscientific details are accurately conveyed. In a user study, we translate the\nscientific papers of 15 researchers into their native languages, finding that\nthe authors consistently found the translations to accurately capture the\noriginal information in their articles. Interestingly, a third of the authors\nfound many technical terms \"overtranslated,\" expressing a preference to keep\nterminology more familiar in English untranslated. Finally, we demonstrate how\nin-context learning techniques can be used to align translations with\ndomain-specific preferences such as mitigating overtranslation, highlighting\nthe adaptability and utility of LLM-driven scientific translation. The code and\ntranslated articles are available at https://hankleid.github.io/ProjectMundo.", "published": "2025-02-25 06:08:48", "link": "http://arxiv.org/abs/2502.17882v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Scaling LLM Pre-training with Vocabulary Curriculum", "abstract": "Modern language models rely on static vocabularies, fixed before pretraining,\nin contrast to the adaptive vocabulary acquisition observed in human language\nlearning. To bridge this gap, we introduce vocabulary curriculum learning, an\napproach that improves pretraining efficiency with log-linear scaling gains\nrelative to vocabulary size. Our method alternates between entropy-guided\nvocabulary expansion and model optimization, enabling models to learn\ntransferable representations across diverse tokenization granularities. This\napproach naturally gives rise to an optimal computation allocation pattern:\nlonger tokens capture predictable content, while shorter tokens focus on more\ncomplex, harder-to-predict contexts. Experiments on small-scale GPT models\ndemonstrate improved scaling efficiency, reinforcing the effectiveness of\ndynamic tokenization. We release our code to support further research and plan\nto extend our experiments to larger models and diverse domains.", "published": "2025-02-25 07:18:29", "link": "http://arxiv.org/abs/2502.17910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models' Factuality Depends on the Language of Inquiry", "abstract": "Multilingual language models (LMs) are expected to recall factual knowledge\nconsistently across languages, yet they often fail to transfer knowledge\nbetween languages even when they possess the correct information in one of the\nlanguages. For example, we find that an LM may correctly identify Rashed Al\nShashai as being from Saudi Arabia when asked in Arabic, but consistently fails\nto do so when asked in English or Swahili. To systematically investigate this\nlimitation, we introduce a benchmark of 10,000 country-related facts across 13\nlanguages and propose three novel metrics: Factual Recall Score, Knowledge\nTransferability Score, and Cross-Lingual Factual Knowledge Transferability\nScore-to quantify factual recall and knowledge transferability in LMs across\ndifferent languages. Our results reveal fundamental weaknesses in today's\nstate-of-the-art LMs, particularly in cross-lingual generalization where models\nfail to transfer knowledge effectively across different languages, leading to\ninconsistent performance sensitive to the language used. Our findings emphasize\nthe need for LMs to recognize language-specific factual reliability and\nleverage the most trustworthy information across languages. We release our\nbenchmark and evaluation framework to drive future research in multilingual\nknowledge transfer.", "published": "2025-02-25 08:27:18", "link": "http://arxiv.org/abs/2502.17955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Synthetic Data Strategies for Domain-Specific Generative Retrieval", "abstract": "This paper investigates synthetic data generation strategies in developing\ngenerative retrieval models for domain-specific corpora, thereby addressing the\nscalability challenges inherent in manually annotating in-domain queries. We\nstudy the data strategies for a two-stage training framework: in the first\nstage, which focuses on learning to decode document identifiers from queries,\nwe investigate LLM-generated queries across multiple granularity (e.g. chunks,\nsentences) and domain-relevant search constraints that can better capture\nnuanced relevancy signals. In the second stage, which aims to refine document\nranking through preference learning, we explore the strategies for mining hard\nnegatives based on the initial model's predictions. Experiments on public\ndatasets over diverse domains demonstrate the effectiveness of our synthetic\ndata generation and hard negative sampling approach.", "published": "2025-02-25 08:27:54", "link": "http://arxiv.org/abs/2502.17957v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "abstract": "Recent studies have shown that making a model spend more time thinking\nthrough longer Chain of Thoughts (CoTs) enables it to gain significant\nimprovements in complex reasoning tasks. While current researches continue to\nexplore the benefits of increasing test-time compute by extending the CoT\nlengths of Large Language Models (LLMs), we are concerned about a potential\nissue hidden behind the current pursuit of test-time scaling: Would excessively\nscaling the CoT length actually bring adverse effects to a model's reasoning\nperformance? Our explorations on mathematical reasoning tasks reveal an\nunexpected finding that scaling with longer CoTs can indeed impair the\nreasoning performance of LLMs in certain domains. Moreover, we discover that\nthere exists an optimal scaled length distribution that differs across\ndifferent domains. Based on these insights, we propose a Thinking-Optimal\nScaling strategy. Our method first uses a small set of seed data with varying\nresponse length distributions to teach the model to adopt different reasoning\nefforts for deep thinking. Then, the model selects its shortest correct\nresponse under different reasoning efforts on additional problems for\nself-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct\noutperform other distillation-based 32B o1-like models across various math\nbenchmarks, and achieve performance on par with QwQ-32B-Preview.", "published": "2025-02-25 10:48:05", "link": "http://arxiv.org/abs/2502.18080v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Offensive Memes with Social Biases in Singapore Context Using\n  Multimodal Large Language Models", "abstract": "Traditional online content moderation systems struggle to classify modern\nmultimodal means of communication, such as memes, a highly nuanced and\ninformation-dense medium. This task is especially hard in a culturally diverse\nsociety like Singapore, where low-resource languages are used and extensive\nknowledge on local context is needed to interpret online content. We curate a\nlarge collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to\nclassify offensive memes in Singapore context. We show the effectiveness of\nfine-tuned VLMs on our dataset, and propose a pipeline containing OCR,\ntranslation and a 7-billion parameter-class VLM. Our solutions reach 80.62%\naccuracy and 0.8192 AUROC on a held-out test set, and can greatly aid human in\nmoderating online contents. The dataset, code, and model weights have been\nopen-sourced at https://github.com/aliencaocao/vlm-for-memes-aisg.", "published": "2025-02-25 11:15:49", "link": "http://arxiv.org/abs/2502.18101v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge", "abstract": "Given that substantial amounts of domain-specific knowledge are stored in\nstructured formats, such as web data organized through HTML, Large Language\nModels (LLMs) are expected to fully comprehend this structured information to\nbroaden their applications in various real-world downstream tasks. Current\napproaches for applying LLMs to structured data fall into two main categories:\nserialization-based and operation-based methods. Both approaches, whether\nrelying on serialization or using SQL-like operations as an intermediary,\nencounter difficulties in fully capturing structural relationships and\neffectively handling sparse data. To address these unique characteristics of\nstructured data, we propose HyperG, a hypergraph-based generation framework\naimed at enhancing LLMs' ability to process structured knowledge. Specifically,\nHyperG first augment sparse data with contextual information, leveraging the\ngenerative power of LLMs, and incorporate a prompt-attentive hypergraph\nlearning (PHL) network to encode both the augmented information and the\nintricate structural relationships within the data. To validate the\neffectiveness and generalization of HyperG, we conduct extensive experiments\nacross two different downstream tasks requiring structured knowledge.", "published": "2025-02-25 11:47:32", "link": "http://arxiv.org/abs/2502.18125v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic\n  Planning over Rewriting Augmented Searchers", "abstract": "Retrieval-Augmented Generation (RAG) is a crucial method for mitigating\nhallucinations in Large Language Models (LLMs) and integrating external\nknowledge into their responses. Existing RAG methods typically employ query\nrewriting to clarify the user intent and manage multi-hop logic, while using\nhybrid retrieval to expand search scope. However, the tight coupling of query\nrewriting to the dense retriever limits its compatibility with hybrid\nretrieval, impeding further RAG performance improvements. To address this\nchallenge, we introduce a high-level searcher that decomposes complex queries\ninto atomic queries, independent of any retriever-specific optimizations.\nAdditionally, to harness the strengths of sparse retrievers for precise keyword\nretrieval, we have developed a new sparse searcher that employs Lucene syntax\nto enhance retrieval accuracy.Alongside web and dense searchers, these\ncomponents seamlessly collaborate within our proposed method,\n\\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the\nretrieval logic, while the low-level searchers (sparse, web, and dense) refine\nthe queries for optimal retrieval. This approach enhances both the completeness\nand accuracy of the retrieval process, overcoming challenges associated with\ncurrent query rewriting techniques in hybrid retrieval scenarios. Empirical\nexperiments conducted on five datasets, encompassing both single-hop and\nmulti-hop question answering tasks, demonstrate the superior performance of\nLevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the\nstate-of-the-art proprietary model, GPT4o, underscoring its effectiveness and\npotential impact on the RAG field.", "published": "2025-02-25 12:09:16", "link": "http://arxiv.org/abs/2502.18139v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can LLMs Explain Themselves Counterfactually?", "abstract": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.", "published": "2025-02-25 12:40:41", "link": "http://arxiv.org/abs/2502.18156v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models", "abstract": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models is becoming increasingly infeasible due to high computational\ndemands. Moreover, FT also increases the risk of catastrophic forgetting. As an\nalternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only\na small subset of parameters, LoRA achieves performance similar to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting still remains. To address these\nlimitations, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a\nnovel PEFT variant designed to mitigate catastrophic forgetting while improving\nfine-tuning performance. Our method introduces a novel normalization technique,\nSigmoid-based Magnitude Norm (S-MagNorm), which enhances parameter retention\nand fine-tuning efficiency. SECURA has been evaluated on a diverse range of\ntasks, including mathematical problem-solving (GSM8K), complex\nquestion-answering (CNNDM), translation (NewsDE), and complex multiple-choice\nreasoning (LogiQA). Experimental results demonstrate that it achieves an\naverage fine-tuning improvement of 3.59% across four MCQ tasks and 2.51% across\nfive QA tasks on Gemma2 2B, Qwen2 1.5B, Qwen2 7B, Llama3 8B, and Llama3.1 8B,\noutperforming DoRA. Additionally, SECURA demonstrates superior knowledge\nretention capabilities, achieving state-of-the-art performance in 16 continual\nlearning tests and maintaining more than 70% accuracy on LLMs' basic knowledge\ncompared to Experience Replay (ER), sequential learning (SEQ), EWC, I-LoRA, and\nCUR-LoRA.", "published": "2025-02-25 13:00:05", "link": "http://arxiv.org/abs/2502.18168v4", "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs", "abstract": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.", "published": "2025-02-25 13:11:53", "link": "http://arxiv.org/abs/2502.18179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grandes modelos de lenguaje: de la predicci\u00f3n de palabras a la\n  comprensi\u00f3n?", "abstract": "Large language models, such as the well-known ChatGPT, have brought about an\nunexpected revolution in the field of artificial intelligence. On the one hand,\nthey have numerous practical applications and enormous potential still to be\nexplored. On the other hand, they are also the subject of debate from\nscientific, philosophical, and social perspectives: there are doubts about the\nexact mechanisms of their functioning and their actual capacity for language\ncomprehension, and their applications raise ethical dilemmas. In this chapter,\nwe describe how this technology has been developed and the fundamentals of its\noperation, allowing us to better understand its capabilities and limitations\nand to introduce some of the main debates surrounding its development and use.\n  --\n  Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una\ninesperada revoluci\\'on en el \\'ambito de la inteligencia artificial. Por un\nlado, cuentan con multitud de aplicaciones pr\\'acticas y un enorme potencial\ntodav\\'ia por explorar. Por otro lado, son tambi\\'en objeto de debate, tanto\ndesde el punto de vista cient\\'ifico y filos\\'ofico como social: hay dudas\nsobre los mecanismos exactos de su funcionamiento y su capacidad real de\ncomprensi\\'on del lenguaje, y sus aplicaciones plantean dilemas \\'eticos. En\neste cap\\'itulo describimos c\\'omo se ha llegado a esta tecnolog\\'ia y los\nfundamentos de su funcionamiento, permiti\\'endonos as\\'i comprender mejor sus\ncapacidades y limitaciones e introducir algunos de los principales debates que\nrodean su desarrollo y uso.", "published": "2025-02-25 13:44:49", "link": "http://arxiv.org/abs/2502.18205v1", "categories": ["cs.CL", "cs.CY", "68T50", "I.2.7; K.4.0"], "primary_category": "cs.CL"}
{"title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding", "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards.", "published": "2025-02-25 13:54:03", "link": "http://arxiv.org/abs/2502.18209v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support", "abstract": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions. To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.", "published": "2025-02-25 15:05:12", "link": "http://arxiv.org/abs/2502.18274v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching", "abstract": "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.", "published": "2025-02-25 15:29:07", "link": "http://arxiv.org/abs/2502.18292v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology", "abstract": "Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains.", "published": "2025-02-25 16:11:40", "link": "http://arxiv.org/abs/2502.18318v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "BRIDO: Bringing Democratic Order to Abstractive Summarization", "abstract": "Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.", "published": "2025-02-25 16:33:50", "link": "http://arxiv.org/abs/2502.18342v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DBR: Divergence-Based Regularization for Debiasing Natural Language\n  Understanding Models", "abstract": "Pre-trained language models (PLMs) have achieved impressive results on\nvarious natural language processing tasks. However, recent research has\nrevealed that these models often rely on superficial features and shortcuts\ninstead of developing a genuine understanding of language, especially for\nnatural language understanding (NLU) tasks. Consequently, the models struggle\nto generalize to out-of-domain data. In this work, we propose Divergence Based\nRegularization (DBR) to mitigate this shortcut learning behavior. Our method\nmeasures the divergence between the output distributions for original examples\nand examples where shortcut tokens have been masked. This process prevents the\nmodel's predictions from being overly influenced by shortcut features or\nbiases. We evaluate our model on three NLU tasks and find that it improves\nout-of-domain performance with little loss of in-domain accuracy. Our results\ndemonstrate that reducing the reliance on shortcuts and superficial features\ncan enhance the generalization ability of large pre-trained language models.", "published": "2025-02-25 16:44:10", "link": "http://arxiv.org/abs/2502.18353v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback", "abstract": "Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean.", "published": "2025-02-25 18:11:37", "link": "http://arxiv.org/abs/2502.18414v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning", "abstract": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.", "published": "2025-02-25 18:26:48", "link": "http://arxiv.org/abs/2502.18431v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing", "abstract": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.", "published": "2025-02-25 18:42:26", "link": "http://arxiv.org/abs/2502.18448v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response", "abstract": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.", "published": "2025-02-25 18:51:06", "link": "http://arxiv.org/abs/2502.18452v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers", "abstract": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.", "published": "2025-02-25 18:59:07", "link": "http://arxiv.org/abs/2502.18460v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality\n  Assessment for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated vast capabilities on\ngenerative tasks in recent years, yet they struggle with guaranteeing the\nfactual correctness of the generated content. This makes these models\nunreliable in realistic situations where factually accurate responses are\nexpected. In this paper, we propose FactReasoner, a new factuality assessor\nthat relies on probabilistic reasoning to assess the factuality of a long-form\ngenerated response. Specifically, FactReasoner decomposes the response into\natomic units, retrieves relevant contexts for them from an external knowledge\nsource, and constructs a joint probability distribution over the atoms and\ncontexts using probabilistic encodings of the logical relationships\n(entailment, contradiction) between the textual utterances corresponding to the\natoms and contexts. FactReasoner then computes the posterior probability of\nwhether atomic units in the response are supported by the retrieved contexts.\nOur experiments on labeled and unlabeled benchmark datasets demonstrate clearly\nthat FactReasoner improves considerably over state-of-the-art prompt-based\napproaches in terms of both factual precision and recall.", "published": "2025-02-25 19:01:48", "link": "http://arxiv.org/abs/2502.18573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration\n  Framework Leveraging BERT", "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance\nthe accuracy and robustness of text classification models. Leveraging BERT as\nthe primary classifier, our framework dynamically escalates low-confidence\npredictions to a specialized multi-agent system comprising Lexical, Contextual,\nLogic, Consensus, and Explainability agents. This collaborative approach allows\nfor comprehensive analysis and consensus-driven decision-making, significantly\nimproving classification performance across diverse text classification tasks.\nEmpirical evaluations on benchmark datasets demonstrate that our framework\nachieves a 5.5% increase in accuracy compared to standard BERT-based\nclassifiers, underscoring its effectiveness and academic novelty in advancing\nmulti-agent systems within natural language processing.", "published": "2025-02-25 21:30:16", "link": "http://arxiv.org/abs/2502.18653v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity\n  Recognition", "abstract": "Zero-shot named entity recognition (NER) aims to develop entity recognition\nsystems from unannotated text corpora. This task presents substantial\nchallenges due to minimal human intervention. Recent work has adapted large\nlanguage models (LLMs) for zero-shot NER by crafting specialized prompt\ntemplates. It advances model self-learning abilities by incorporating\nself-annotated demonstrations. However, two important challenges persist: (i)\nCorrelations between contexts surrounding entities are overlooked, leading to\nwrong type predictions or entity omissions. (ii) The indiscriminate use of task\ndemonstrations, retrieved through shallow similarity-based strategies, severely\nmisleads LLMs during inference.\n  In this paper, we introduce the cooperative multi-agent system (CMAS), a\nnovel framework for zero-shot NER that uses the collective intelligence of\nmultiple agents to address the challenges outlined above. CMAS has four main\nagents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor,\n(iii) a demonstration discriminator, and (iv) an overall predictor. To\nexplicitly capture correlations between contexts surrounding entities, CMAS\nreformulates NER into two subtasks: recognizing named entities and identifying\nentity type-related features within the target sentence. To enable controllable\nutilization of demonstrations, a demonstration discriminator is established to\nincorporate the self-reflection mechanism, automatically evaluating helpfulness\nscores for the target sentence. Experimental results show that CMAS\nsignificantly improves zero-shot NER performance across six benchmarks,\nincluding both domain-specific and general-domain scenarios. Furthermore, CMAS\ndemonstrates its effectiveness in few-shot settings and with various LLM\nbackbones.", "published": "2025-02-25 23:30:43", "link": "http://arxiv.org/abs/2502.18702v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis", "abstract": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.", "published": "2025-02-25 00:59:27", "link": "http://arxiv.org/abs/2503.00032v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:\n  Features, Techniques, Performance, Future Prospects", "abstract": "Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and\nexciting Large Language Model (LLM) technologies for reasoning, multimodal\ncapabilities, and general linguistic performance worldwide. DeepSeek employs a\nMixture-of-Experts (MoE) approach, activating only the parameters most relevant\nto the task at hand, which makes it especially effective for domain-specific\nwork. On the other hand, ChatGPT relies on a dense transformer model enhanced\nthrough reinforcement learning from human feedback (RLHF), and then Google\nGemini actually uses a multimodal transformer architecture that integrates\ntext, code, and images into a single framework. However, by using those\ntechnologies, people can be able to mine their desired text, code, images, etc,\nin a cost-effective and domain-specific inference. People may choose those\ntechniques based on the best performance. In this regard, we offer a\ncomparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this\nresearch. Initially, we focus on their methods and materials, appropriately\nincluding the data selection criteria. Then, we present state-of-the-art\nfeatures of DeepSeek, ChatGPT, and Gemini based on their applications. Most\nimportantly, we show the technological comparison among them and also cover the\ndataset analysis for various applications. Finally, we address extensive\nresearch areas and future potential guidance regarding LLM-based AI research\nfor the community.", "published": "2025-02-25 19:55:35", "link": "http://arxiv.org/abs/2503.04783v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Tip of the Tongue Query Elicitation for Simulated Evaluation", "abstract": "Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a\nspecific identifier, such as a document title. While common, existing search\nsystems often fail to effectively support TOT scenarios. Research on TOT\nretrieval is further constrained by the challenge of collecting queries, as\ncurrent approaches rely heavily on community question-answering (CQA) websites,\nleading to labor-intensive evaluation and domain bias. To overcome these\nlimitations, we introduce two methods for eliciting TOT queries - leveraging\nlarge language models (LLMs) and human participants - to facilitate simulated\nevaluations of TOT retrieval systems. Our LLM-based TOT user simulator\ngenerates synthetic TOT queries at scale, achieving high correlations with how\nCQA-based TOT queries rank TOT retrieval systems when tested in the Movie\ndomain. Additionally, these synthetic queries exhibit high linguistic\nsimilarity to CQA-derived queries. For human-elicited queries, we developed an\ninterface that uses visual stimuli to place participants in a TOT state,\nenabling the collection of natural queries. In the Movie domain, system rank\ncorrelation and linguistic similarity analyses confirm that human-elicited\nqueries are both effective and closely resemble CQA-based queries. These\napproaches reduce reliance on CQA-based data collection while expanding\ncoverage to underrepresented domains, such as Landmark and Person. LLM-elicited\nqueries for the Movie, Landmark, and Person domains have been released as test\nqueries in the TREC 2024 TOT track, with human-elicited queries scheduled for\ninclusion in the TREC 2025 TOT track. Additionally, we provide source code for\nsynthetic query generation and the human query collection interface, along with\ncurated visual stimuli used for eliciting TOT queries.", "published": "2025-02-25 02:11:42", "link": "http://arxiv.org/abs/2502.17776v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "An Overview of Large Language Models for Statisticians", "abstract": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.", "published": "2025-02-25 03:40:36", "link": "http://arxiv.org/abs/2502.17814v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation", "abstract": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem.", "published": "2025-02-25 04:38:38", "link": "http://arxiv.org/abs/2502.17839v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in\n  Bilingual Complex Ophthalmology Reasoning", "abstract": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and\nthree other recently released large language models (LLMs) in bilingual complex\nophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs)\nrelated to diagnosis (n = 39) and management (n = 91) were collected from the\nChinese ophthalmology senior professional title examination and categorized\ninto six topics. These MCQs were translated into English using DeepSeek-R1. The\nresponses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated\nunder default configurations between February 15 and February 20, 2025.\nAccuracy was calculated as the proportion of correctly answered questions, with\nomissions and extra answers considered incorrect. Reasoning ability was\nevaluated through analyzing reasoning logic and the causes of reasoning error.\nResults: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862\nin Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and\nOpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs\n(all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027),\nand 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the\nhighest accuracy across five topics in both Chinese and English MCQs. It also\nexcelled in management questions conducted in Chinese (all P<0.05). Reasoning\nability analysis showed that the four LLMs shared similar reasoning logic.\nIgnoring key positive history, ignoring key positive signs, misinterpretation\nmedical data, and too aggressive were the most common causes of reasoning\nerrors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual\ncomplex ophthalmology reasoning tasks than three other state-of-the-art LLMs.\nWhile its clinical applicability remains challenging, it shows promise for\nsupporting diagnosis and clinical decision-making.", "published": "2025-02-25 08:08:53", "link": "http://arxiv.org/abs/2502.17947v1", "categories": ["cs.CL", "cs.AI", "cs.PF"], "primary_category": "cs.CL"}
{"title": "MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment\n  Classification", "abstract": "Due to the lack of quality data for low-resource Bantu languages, significant\nchallenges are presented in text classification and other practical\nimplementations. In this paper, we introduce an advanced model combining\nLanguage-Independent Data Augmentation (LiDA) with Multi-Head Attention based\nweighted embeddings to selectively enhance critical data points and improve\ntext classification performance. This integration allows us to create robust\ndata augmentation strategies that are effective across various linguistic\ncontexts, ensuring that our model can handle the unique syntactic and semantic\nfeatures of Bantu languages. This approach not only addresses the data scarcity\nissue but also sets a foundation for future research in low-resource language\nprocessing and classification tasks.", "published": "2025-02-25 08:53:27", "link": "http://arxiv.org/abs/2502.17987v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents", "abstract": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.", "published": "2025-02-25 09:26:12", "link": "http://arxiv.org/abs/2502.18017v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention\n  Matching for Low-Resource languages", "abstract": "Language model compression through knowledge distillation has emerged as a\npromising approach for deploying large language models in resource-constrained\nenvironments. However, existing methods often struggle to maintain performance\nwhen distilling multilingual models, especially for low-resource languages. In\nthis paper, we present a novel hybrid distillation approach that combines\ntraditional knowledge distillation with a simplified attention matching\nmechanism, specifically designed for multilingual contexts. Our method\nintroduces an extremely compact student model architecture, significantly\nsmaller than conventional multilingual models. We evaluate our approach on five\nAfrican languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled\nstudent model; AfroXLMR-Comet successfully captures both the output\ndistribution and internal attention patterns of a larger teacher model\n(AfroXLMR-Large) while reducing the model size by over 85%. Experimental\nresults demonstrate that our hybrid approach achieves competitive performance\ncompared to the teacher model, maintaining an accuracy within 85% of the\noriginal model's performance while requiring substantially fewer computational\nresources. Our work provides a practical framework for deploying efficient\nmultilingual models in resource-constrained environments, particularly\nbenefiting applications involving African languages.", "published": "2025-02-25 09:28:47", "link": "http://arxiv.org/abs/2502.18020v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defining bias in AI-systems: Biased models are fair models", "abstract": "The debate around bias in AI systems is central to discussions on algorithmic\nfairness. However, the term bias often lacks a clear definition, despite\nfrequently being contrasted with fairness, implying that an unbiased model is\ninherently fair. In this paper, we challenge this assumption and argue that a\nprecise conceptualization of bias is necessary to effectively address fairness\nconcerns. Rather than viewing bias as inherently negative or unfair, we\nhighlight the importance of distinguishing between bias and discrimination. We\nfurther explore how this shift in focus can foster a more constructive\ndiscourse within academic debates on fairness in AI systems.", "published": "2025-02-25 10:28:16", "link": "http://arxiv.org/abs/2502.18060v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Bayesian Optimization for Controlled Image Editing via LLMs", "abstract": "In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.", "published": "2025-02-25 11:41:33", "link": "http://arxiv.org/abs/2502.18116v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations", "abstract": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.", "published": "2025-02-25 12:21:45", "link": "http://arxiv.org/abs/2502.18147v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Steering Language Model to Stable Speech Emotion Recognition via\n  Contextual Perception and Chain of Thought", "abstract": "Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of\ncomprehending diverse audio signal, performing audio analysis and generating\ntextual responses. However, in speech emotion recognition (SER), ALMs often\nsuffer from hallucinations, resulting in misclassifications or irrelevant\noutputs. To address these challenges, we propose C$^2$SER, a novel ALM designed\nto enhance the stability and accuracy of SER through Contextual perception and\nChain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic\nperception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S\nextends Emotion2Vec with semi-supervised learning to enhance emotional\ndiscrimination. Additionally, C$^2$SER employs a CoT approach, processing SER\nin a step-by-step manner while leveraging speech content and speaking styles to\nimprove recognition. To further enhance stability, C$^2$SER introduces\nself-distillation from explicit CoT to implicit CoT, mitigating error\naccumulation and boosting recognition accuracy. Extensive experiments show that\nC$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap,\ndelivering more stable and precise emotion recognition. We release the training\ncode, checkpoints, and test sets to facilitate further research.", "published": "2025-02-25 13:26:25", "link": "http://arxiv.org/abs/2502.18186v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Iterative Counterfactual Data Augmentation", "abstract": "Counterfactual data augmentation (CDA) is a method for controlling\ninformation or biases in training datasets by generating a complementary\ndataset with typically opposing biases. Prior work often either relies on\nhand-crafted rules or algorithmic CDA methods which can leave unwanted\ninformation in the augmented dataset. In this work, we show iterative CDA\n(ICDA) with initial, high-noise interventions can converge to a state with\nsignificantly lower noise. Our ICDA procedure produces a dataset where one\ntarget signal in the training dataset maintains high mutual information with a\ncorresponding label and the information of spurious signals are reduced. We\nshow training on the augmented datasets produces rationales on documents that\nbetter align with human annotation. Our experiments include six human produced\ndatasets and two large-language model generated datasets.", "published": "2025-02-25 14:33:50", "link": "http://arxiv.org/abs/2502.18249v1", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "AMPO: Active Multi-Preference Optimization", "abstract": "Multi-preference optimization enriches language-model alignment beyond\npairwise preferences by contrasting entire sets of helpful and undesired\nresponses, thereby enabling richer training signals for large language models.\nDuring self-play alignment, these models often produce numerous candidate\nanswers per query, rendering it computationally infeasible to include all\nresponses in the training objective. In this work, we propose $\\textit{Active\nMulti-Preference Optimization}$ (AMPO), a novel approach that combines\non-policy generation, a multi-preference group-contrastive loss, and active\nsubset selection. Specifically, we score and embed large candidate pools of\nresponses and then select a small, yet informative, subset that covers reward\nextremes and distinct semantic clusters for preference optimization. Our\ncontrastive training scheme is capable of identifying not only the best and\nworst answers but also subtle, underexplored modes that are crucial for robust\nalignment. Theoretically, we provide guarantees for expected reward\nmaximization using our active selection method, and empirically, AMPO achieves\nstate-of-the-art results on $\\textit{AlpacaEval}$ using Llama 8B.", "published": "2025-02-25 15:29:51", "link": "http://arxiv.org/abs/2502.18293v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AgentRM: Enhancing Agent Generalization with Reward Modeling", "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.", "published": "2025-02-25 17:58:02", "link": "http://arxiv.org/abs/2502.18407v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rank1: Test-Time Compute for Reranking in Information Retrieval", "abstract": "We introduce Rank1, the first reranking model trained to take advantage of\ntest-time compute. Rank1 demonstrates the applicability within retrieval of\nusing a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for\ndistillation in order to rapidly improve the performance of a smaller model. We\ngather and open-source a dataset of more than 600,000 examples of R1 reasoning\ntraces from queries and passages in MS MARCO. Models trained on this dataset\nshow: (1) state-of-the-art performance on advanced reasoning and instruction\nfollowing datasets; (2) work remarkably well out of distribution due to the\nability to respond to user-input prompts; and (3) have explainable reasoning\nchains that can be given to users or RAG-based systems. Further, we demonstrate\nthat quantized versions of these models retain strong performance while using\nless compute/memory. Overall, Rank1 shows that test-time compute allows for a\nfundamentally new type of explainable and performant reranker model for search.", "published": "2025-02-25 18:14:06", "link": "http://arxiv.org/abs/2502.18418v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Exploring Gender Disparities in Automatic Speech Recognition Technology", "abstract": "This study investigates factors influencing Automatic Speech Recognition\n(ASR) systems' fairness and performance across genders, beyond the conventional\nexamination of demographics. Using the LibriSpeech dataset and the Whisper\nsmall model, we analyze how performance varies across different gender\nrepresentations in training data. Our findings suggest a complex interplay\nbetween the gender ratio in training data and ASR performance. Optimal fairness\noccurs at specific gender distributions rather than a simple 50-50 split.\nFurthermore, our findings suggest that factors like pitch variability can\nsignificantly affect ASR accuracy. This research contributes to a deeper\nunderstanding of biases in ASR systems, highlighting the importance of\ncarefully curated training data in mitigating gender bias.", "published": "2025-02-25 18:29:38", "link": "http://arxiv.org/abs/2502.18434v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions", "abstract": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous.", "published": "2025-02-25 18:30:25", "link": "http://arxiv.org/abs/2502.18435v2", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution", "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.", "published": "2025-02-25 18:45:04", "link": "http://arxiv.org/abs/2502.18449v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large\n  Language Model-Powered Pre-Screening Pipeline", "abstract": "Background: Recruitment for cohorts involving complex liver diseases, such as\nhepatocellular carcinoma and liver cirrhosis, often requires interpreting\nsemantically complex criteria. Traditional manual screening methods are\ntime-consuming and prone to errors. While AI-powered pre-screening offers\npotential solutions, challenges remain regarding accuracy, efficiency, and data\nprivacy. Methods: We developed a novel patient pre-screening pipeline that\nleverages clinical expertise to guide the precise, safe, and efficient\napplication of large language models. The pipeline breaks down complex criteria\ninto a series of composite questions and then employs two strategies to perform\nsemantic question-answering through electronic health records - (1) Pathway A,\nAnthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset\nStances within an Agent Collaboration strategy, particularly in managing\ncomplex clinical reasoning scenarios. The pipeline is evaluated on three key\nmetrics-precision, time consumption, and counterfactual inference - at both the\nquestion and criterion levels. Results: Our pipeline achieved high precision\n(0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled\nin complex reasoning, while Pathway A was effective in precise data extraction\nwith faster processing times. Both pathways achieved comparable precision. The\npipeline showed promising results in hepatocellular carcinoma (0.878) and\ncirrhosis trials (0.843). Conclusions: This data-secure and time-efficient\npipeline shows high precision in hepatopathy trials, providing promising\nsolutions for streamlining clinical trial workflows. Its efficiency and\nadaptability make it suitable for improving patient recruitment. And its\ncapability to function in resource-constrained environments further enhances\nits utility in clinical settings.", "published": "2025-02-25 02:06:39", "link": "http://arxiv.org/abs/2502.18531v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate\n  Hallucinations in VQA", "abstract": "Visual Question Answering requires models to generate accurate answers by\nintegrating visual and textual understanding. However, VQA models still\nstruggle with hallucinations, producing convincing but incorrect answers,\nparticularly in knowledge-driven and Out-of-Distribution scenarios. We\nintroduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA\nwith Retrieval-Augmented Generation to ground answers in external knowledge\nsources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the\nOK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and\nimproving robustness in both in-domain and Out-of-Distribution settings. These\nfindings highlight the potential of FilterRAG to improve Visual Question\nAnswering systems for real-world deployment.", "published": "2025-02-25 06:06:47", "link": "http://arxiv.org/abs/2502.18536v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PII-Bench: Evaluating Query-Aware Privacy Protection Systems", "abstract": "The widespread adoption of Large Language Models (LLMs) has raised\nsignificant privacy concerns regarding the exposure of personally identifiable\ninformation (PII) in user prompts. To address this challenge, we propose a\nquery-unrelated PII masking strategy and introduce PII-Bench, the first\ncomprehensive evaluation framework for assessing privacy protection systems.\nPII-Bench comprises 2,842 test samples across 55 fine-grained PII categories,\nfeaturing diverse scenarios from single-subject descriptions to complex\nmulti-party interactions. Each sample is carefully crafted with a user query,\ncontext description, and standard answer indicating query-relevant PII. Our\nempirical evaluation reveals that while current models perform adequately in\nbasic PII detection, they show significant limitations in determining PII query\nrelevance. Even state-of-the-art LLMs struggle with this task, particularly in\nhandling complex multi-subject scenarios, indicating substantial room for\nimprovement in achieving intelligent PII masking.", "published": "2025-02-25 14:49:08", "link": "http://arxiv.org/abs/2502.18545v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CR"}
{"title": "Scalable Best-of-N Selection for Large Language Models via\n  Self-Certainty", "abstract": "Best-of-N selection is a key technique for improving the reasoning\nperformance of Large Language Models (LLMs) through increased test-time\ncomputation. Current state-of-the-art methods often employ computationally\nintensive reward models for response evaluation and selection. Reward-free\nalternatives, like self-consistency and universal self-consistency, are limited\nin their ability to handle open-ended generation tasks or scale effectively. To\naddress these limitations, we propose self-certainty, a novel and efficient\nmetric that leverages the inherent probability distribution of LLM outputs to\nestimate response quality without requiring external reward models. We\nhypothesize that higher distributional self-certainty, aggregated across\nmultiple samples, correlates with improved response accuracy, as it reflects\ngreater confidence in the generated output. Through extensive experiments on\nvarious reasoning tasks, we demonstrate that self-certainty (1) scales\neffectively with increasing sample size $N$, akin to reward models but without\nthe computational overhead; (2) complements chain-of-thought, improving\nreasoning performance beyond greedy decoding; and (3) generalizes to open-ended\ntasks where traditional self-consistency methods fall short. Our findings\nestablish self-certainty as a practical and efficient way for improving LLM\nreasoning capabilities. The code is available at\nhttps://github.com/backprop07/Self-Certainty", "published": "2025-02-25 19:08:07", "link": "http://arxiv.org/abs/2502.18581v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems", "abstract": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives.", "published": "2025-02-25 20:52:06", "link": "http://arxiv.org/abs/2502.18635v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T20, 68Q32, 90C29, 62P30", "I.2.6; I.2.7; G.1.6; G.3"], "primary_category": "cs.LG"}
{"title": "Scaffolding Empathy: Training Counselors with Simulated Patients and\n  Utterance-level Performance Visualizations", "abstract": "Learning therapeutic counseling involves significant role-play experience\nwith mock patients, with current manual training methods providing only\nintermittent granular feedback. We seek to accelerate and optimize counselor\ntraining by providing frequent, detailed feedback to trainees as they interact\nwith a simulated patient. Our first application domain involves training\nmotivational interviewing skills for counselors. Motivational interviewing is a\ncollaborative counseling style in which patients are guided to talk about\nchanging their behavior, with empathetic counseling an essential ingredient. We\ndeveloped and evaluated an LLM-powered training system that features a\nsimulated patient and visualizations of turn-by-turn performance feedback\ntailored to the needs of counselors learning motivational interviewing. We\nconducted an evaluation study with professional and student counselors,\ndemonstrating high usability and satisfaction with the system. We present\ndesign implications for the development of automated systems that train users\nin counseling skills and their generalizability to other types of social skills\ntraining.", "published": "2025-02-25 22:12:24", "link": "http://arxiv.org/abs/2502.18673v1", "categories": ["cs.HC", "cs.CL", "cs.MA"], "primary_category": "cs.HC"}
{"title": "Speaking the Right Language: The Impact of Expertise Alignment in\n  User-AI Interactions", "abstract": "Using a sample of 25,000 Bing Copilot conversations, we study how the agent\nresponds to users of varying levels of domain expertise and the resulting\nimpact on user experience along multiple dimensions. Our findings show that\nacross a variety of topical domains, the agent largely responds at proficient\nor expert levels of expertise (77% of conversations) which correlates with\npositive user experience regardless of the user's level of expertise.\nMisalignment, such that the agent responds at a level of expertise below that\nof the user, has a negative impact on overall user experience, with the impact\nmore profound for more complex tasks. We also show that users engage more, as\nmeasured by the number of words in the conversation, when the agent responds at\na level of expertise commensurate with that of the user. Our findings\nunderscore the importance of alignment between user and AI when designing\nhuman-centered AI systems, to ensure satisfactory and productive interactions.", "published": "2025-02-25 22:46:51", "link": "http://arxiv.org/abs/2502.18685v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse\n  Preference Alignment", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.", "published": "2025-02-25 23:22:12", "link": "http://arxiv.org/abs/2502.18699v1", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Efficient Test-Time Scaling via Self-Calibration", "abstract": "Increasing test-time computation is a straightforward approach to enhancing\nthe quality of responses in Large Language Models (LLMs). While Best-of-N\nsampling and Self-Consistency with majority voting are simple and effective,\nthey require a fixed number of sampling responses for each query, regardless of\nits complexity. This could result in wasted computation for simpler questions\nand insufficient exploration for more challenging ones. In this work, we argue\nthat model confidence of responses can be used for improving the efficiency of\ntest-time scaling. Unfortunately, LLMs are known to be overconfident and\nprovide unreliable confidence estimation. To address this limitation, we\nintroduce Self-Calibration by distilling Self-Consistency-derived confidence\ninto the model itself. This enables reliable confidence estimation at test time\nwith one forward pass. We then design confidence-based efficient test-time\nscaling methods to handle queries of various difficulty, such as Early-Stopping\nfor Best-of-N and Self-Consistency with calibrated confidence. Experiments on\nthree LLMs across six datasets demonstrate the effectiveness of our approach.\nSpecifically, applying confidence-based Early Stopping to Best-of-N improves\nMathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses,\nindicating the efficacy of confidence-based sampling strategy at inference\ntime.", "published": "2025-02-25 00:21:14", "link": "http://arxiv.org/abs/2503.00031v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Constraining Sequential Model Editing with Editing Anchor Compression", "abstract": "Large language models (LLMs) struggle with hallucinations due to false or\noutdated knowledge. Given the high resource demands of retraining these models,\nthere is an increasing focus on developing model editing. However, the general\nabilities of LLMs across downstream tasks are prone to significant degradation\nduring sequential editing. This paper statistically observes that the parameter\nmatrix after editing exhibits a significant deviation compared to its previous\nstate as the number of edits increases. This serious deviation affects the\noriginal knowledge associations within LLMs and leads to the degradation of\ntheir general abilities. To this end, a framework termed Editing Anchor\nCompression (EAC) is proposed to constrain the deviation of the parameter\nmatrix during sequential editing. It compresses the editing information by\nselecting editing anchors that are important in encoding new relations without\ndeviating too much from the original matrix, thereby preserving the general\nabilities. Experiments of applying EAC to two popular editing methods on three\nLLMs across four tasks are conducted. Evaluation results show that EAC\neffectively minimizes unreasonable deviations caused by model editing,\npreserving over 70% of the general abilities while better retaining the editing\nknowledge compared to the original counterpart methods.", "published": "2025-02-25 03:56:49", "link": "http://arxiv.org/abs/2503.00035v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment\n  in LVLMs", "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in\nmultimodal comprehension, thanks to extensive pre-training and fine-tuning on\nlarge-scale visual datasets. However, despite their robust textual safety\nmechanisms, they remain vulnerable to harmful visual inputs. Existing\nsafeguards-typically relying on pre-filtering or fine-tuning-incur high costs\nand diminish overall utility. To address this critical vulnerability, we\nintroduce SafeCLIP, a lightweight method that leverages LVLMs inherent\nmultimodal alignment for zero-shot toxic image detection. By projecting CLIPs\ndiscarded CLS token into its text space and matching it with toxic descriptors,\nSafeCLIP detects harmful content without any architectural changes-adding\nminimal latency and enabling dynamic safety corrections during inference and\nfine-tuning.Experiments show that SafeCLIP achieves a 66.9% defense success\nrate with only 3.2% false positive rate and 7.2% overhead. In contrast,\nstate-of-the-art methods achieve 52.9% success but have a 10.7% false positive\nrate and 210% overhead. Our work demonstrates that leveraging inherent\nmultimodal alignment can yield efficient, low-cost LVLM safety. Code is\navailable at anonymous.4open.science/r/safeclip-2C01.", "published": "2025-02-25 06:51:16", "link": "http://arxiv.org/abs/2503.00037v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "from Benign import Toxic: Jailbreaking the Language Model via\n  Adversarial Metaphors", "abstract": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.", "published": "2025-02-25 08:41:25", "link": "http://arxiv.org/abs/2503.00038v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical\n  Reasoning", "abstract": "Multimodal Large Language Models (MLLMs) have become a powerful tool for\nintegrating visual and textual information. Despite their exceptional\nperformance on visual understanding benchmarks, measuring their ability to\nreason abstractly across multiple images remains a significant challenge. To\naddress this, we introduce VOILA, a large-scale, open-ended, dynamic benchmark\ndesigned to evaluate MLLMs' perceptual understanding and abstract relational\nreasoning. VOILA employs an analogical mapping approach in the visual domain,\nrequiring models to generate an image that completes an analogy between two\ngiven image pairs, reference and application, without relying on predefined\nchoices. Our experiments demonstrate that the analogical reasoning tasks in\nVOILA present a challenge to MLLMs. Through multi-step analysis, we reveal that\ncurrent MLLMs struggle to comprehend inter-image relationships and exhibit\nlimited capabilities in high-level relational reasoning. Notably, we observe\nthat performance improves when following a multi-step strategy of least-to-most\nprompting. Comprehensive evaluations on open-source models and GPT-4o show that\non text-based answers, the best accuracy for challenging scenarios is 13%\n(LLaMa 3.2) and even for simpler tasks is only 29% (GPT-4o), while human\nperformance is significantly higher at 70% across both difficulty levels.", "published": "2025-02-25 23:36:19", "link": "http://arxiv.org/abs/2503.00043v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?", "abstract": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, with the quality of preference datasets playing a crucial\nrole in this process. While existing metrics primarily assess data quality\nbased on either explicit or implicit reward margins, they often provide\ncontradictory evaluations for the same data. To address this issue, we\nintroduce the alignment potential metric, which quantifies the gap from the\nmodel's current implicit reward margin to the target explicit reward margin,\nthereby estimating the model's potential to align with the preference data.\nEmpirical results demonstrate that training on data selected by this metric\nconsistently enhances alignment performance, surpassing existing metrics across\ndifferent base models and optimization objectives. Furthermore, our method\nextends to self-play data generation frameworks, where the metric is used to\nidentify high-quality data within the self-generated content by LLMs. Under\nthis data generation scenario, our method surpasses current state-of-the-art\n(SOTA) results across various training settings and demonstrates continuous\nimprovements in alignment performance as dataset size and training iterations\nincrease.", "published": "2025-02-25 06:43:24", "link": "http://arxiv.org/abs/2503.01864v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Systems and Algorithms for Convolutional Multi-Hybrid Language Models at\n  Scale", "abstract": "We introduce convolutional multi-hybrid architectures, with a design grounded\non two simple observations. First, operators in hybrid models can be tailored\nto token manipulation tasks such as in-context recall, multi-token recall, and\ncompression, with input-dependent convolutions and attention offering\ncomplementary performance. Second, co-designing convolution operators and\nhardware-aware algorithms enables efficiency gains in regimes where previous\nalternative architectures struggle to surpass Transformers. At the 40 billion\nparameter scale, we train end-to-end 1.2 to 2.9 times faster than optimized\nTransformers, and 1.1 to 1.4 times faster than previous generation hybrids. On\nH100 GPUs and model width 4096, individual operators in the proposed\nmulti-hybrid StripedHyena 2 architecture achieve two-fold throughput\nimprovement over linear attention and state-space models. Multi-hybrids excel\nat sequence modeling over byte-tokenized data, as demonstrated by the Evo 2\nline of models. We discuss the foundations that enable these results, including\narchitecture design, overlap-add blocked kernels for tensor cores, and\ndedicated all-to-all and point-to-point context parallelism strategies.", "published": "2025-02-25 19:47:20", "link": "http://arxiv.org/abs/2503.01868v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "From Small to Large Language Models: Revisiting the Federalist Papers", "abstract": "For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm", "published": "2025-02-25 21:50:46", "link": "http://arxiv.org/abs/2503.01869v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Effect of Gender Fair Job Description on Generative AI Images", "abstract": "STEM fields are traditionally male-dominated, with gender biases shaping\nperceptions of job accessibility. This study analyzed gender representation in\nSTEM occupation images generated by OpenAI DALL-E 3 \\& Black Forest FLUX.1\nusing 150 prompts in three linguistic forms: German generic masculine, German\npair form, and English. As control, 20 pictures of social occupations were\ngenerated as well. Results revealed significant male bias across all forms,\nwith the German pair form showing reduced bias but still overrepresenting men\nfor the STEM-Group and mixed results for the Group of Social Occupations. These\nfindings highlight generative AI's role in reinforcing societal biases,\nemphasizing the need for further discussion on diversity (in AI). Further\naspects analyzed are age-distribution and ethnic diversity.", "published": "2025-02-25 10:21:29", "link": "http://arxiv.org/abs/2503.05769v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Synthetic Categorical Restructuring large Or How AIs Gradually Extract\n  Efficient Regularities from Their Experience of the World", "abstract": "How do language models segment their internal experience of the world of\nwords to progressively learn to interact with it more efficiently? This study\nin the neuropsychology of artificial intelligence investigates the phenomenon\nof synthetic categorical restructuring, a process through which each successive\nperceptron neural layer abstracts and combines relevant categorical\nsub-dimensions from the thought categories of its previous layer. This process\nshapes new, even more efficient categories for analyzing and processing the\nsynthetic system's own experience of the linguistic external world to which it\nis exposed. Our genetic neuron viewer, associated with this study, allows\nvisualization of the synthetic categorical restructuring phenomenon occurring\nduring the transition from perceptron layer 0 to 1 in GPT2-XL.", "published": "2025-02-25 16:49:19", "link": "http://arxiv.org/abs/2503.10643v1", "categories": ["cs.CL", "cs.NE", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Automated Knowledge Component Generation and Knowledge Tracing for\n  Coding Problems", "abstract": "Knowledge components (KCs) mapped to problems help model student learning,\ntracking their mastery levels on fine-grained skills thereby facilitating\npersonalized learning and feedback in online learning platforms. However,\ncrafting and tagging KCs to problems, traditionally performed by human domain\nexperts, is highly labor-intensive. We present a fully automated, LLM-based\npipeline for KC generation and tagging for open-ended programming problems. We\nalso develop an LLM-based knowledge tracing (KT) framework to leverage these\nLLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive\nquantitative and qualitative evaluations validating the effectiveness of\nKCGen-KT. On a real-world dataset of student code submissions to open-ended\nprogramming problems, KCGen-KT outperforms existing KT methods. We investigate\nthe learning curves of generated KCs and show that LLM-generated KCs have a\ncomparable level-of-fit to human-written KCs under the performance factor\nanalysis (PFA) model. We also conduct a human evaluation to show that the KC\ntagging accuracy of our pipeline is reasonably accurate when compared to that\nby human domain experts.", "published": "2025-02-25 20:40:51", "link": "http://arxiv.org/abs/2502.18632v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Can Large Language Models Extract Customer Needs as well as Professional\n  Analysts?", "abstract": "Identifying customer needs (CNs) is important for product management, product\ndevelopment, and marketing. Applications rely on professional analysts\ninterpreting textual data (e.g., interview transcripts, online reviews) to\nunderstand the nuances of customer experience and concisely formulate \"jobs to\nbe done.\" The task is cognitively complex and time-consuming. Current practice\nfacilitates the process with keyword search and machine learning but relies on\nhuman judgment to formulate CNs. We examine whether Large Language Models\n(LLMs) can automatically extract CNs. Because evaluating CNs requires\nprofessional judgment, we partnered with a marketing consulting firm to conduct\na blind study of CNs extracted by: (1) a foundational LLM with prompt\nengineering only (Base LLM), (2) an LLM fine-tuned with professionally\nidentified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs\nas well as or better than professional analysts when extracting CNs. The\nextracted CNs are well-formulated, sufficiently specific to identify\nopportunities, and justified by source content (no hallucinations). The SFT LLM\nis efficient and provides more complete coverage of CNs. The Base LLM was not\nsufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce\nmanual effort, enhance the precision of CN articulation, and provide improved\ninsight for innovation and marketing strategy.", "published": "2025-02-25 21:55:35", "link": "http://arxiv.org/abs/2503.01870v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Determined Blind Source Separation with Sinkhorn Divergence-based\n  Optimal Allocation of the Source Power", "abstract": "Blind source separation (BSS) refers to the process of recovering multiple\nsource signals from observations recorded by an array of sensors. Common\napproaches to BSS, including independent vector analysis (IVA), and independent\nlow-rank matrix analysis (ILRMA), typically rely on second-order models to\ncapture the statistical independence of source signals for separation. However,\nthese methods generally do not account for the implicit structural information\nacross frequency bands, which may lead to model mismatches between the assumed\nsource distributions and the distributions of the separated source signals\nestimated from the observed mixtures. To tackle these limitations, this paper\nshows that conventional approaches such as IVA and ILRMA can easily be\nleveraged by the Sinkhorn divergence, incorporating an optimal transport (OT)\nframework to adaptively correct source variance estimates. This allows for the\nrecovery of the source distribution while modeling the inter-band signal\ndependence and reallocating source power across bands. As a result, enhanced\nversions of these algorithms are developed, integrating a Sinkhorn iterative\nscheme into their standard implementations. Extensive simulations demonstrate\nthat the proposed methods consistently enhance BSS performance.", "published": "2025-02-25 13:20:34", "link": "http://arxiv.org/abs/2502.18182v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Silent Speech Sentence Recognition with Six-Axis Accelerometers using\n  Conformer and CTC Algorithm", "abstract": "Silent speech interfaces (SSI) are being actively developed to assist\nindividuals with communication impairments who have long suffered from daily\nhardships and a reduced quality of life. However, silent sentences are\ndifficult to segment and recognize due to elision and linking. A novel silent\nspeech sentence recognition method is proposed to convert the facial motion\nsignals collected by six-axis accelerometers into transcribed words and\nsentences. A Conformer-based neural network with the\nConnectionist-Temporal-Classification algorithm is used to gain contextual\nunderstanding and translate the non-acoustic signals into words sequences,\nsolely requesting the constituent words in the database. Test results show that\nthe proposed method achieves a 97.17% accuracy in sentence recognition,\nsurpassing the existing silent speech recognition methods with a typical\naccuracy of 85%-95%, and demonstrating the potential of accelerometers as an\navailable SSI modality for high-accuracy silent speech sentence recognition.", "published": "2025-02-25 04:10:26", "link": "http://arxiv.org/abs/2502.17829v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Enhancing Speech Quality through the Integration of BGRU and Transformer\n  Architectures", "abstract": "Speech enhancement plays an essential role in improving the quality of speech\nsignals in noisy environments. This paper investigates the efficacy of\nintegrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models\nfor speech enhancement tasks. Through a comprehensive experimental evaluation,\nour study demonstrates the superiority of this hybrid architecture over\ntraditional methods and standalone models. The combined BGRU-Transformer\nframework excels in capturing temporal dependencies and learning complex signal\npatterns, leading to enhanced noise reduction and improved speech quality.\nResults show significant performance gains compared to existing approaches,\nhighlighting the potential of this integrated model in real-world applications.\nThe seamless integration of BGRU and Transformer architectures not only\nenhances system robustness but also opens the road for advanced speech\nprocessing techniques. This research contributes to the ongoing efforts in\nspeech enhancement technology and sets a solid foundation for future\ninvestigations into optimizing model architectures, exploring many application\nscenarios, and advancing the field of speech processing in noisy environments.", "published": "2025-02-25 07:18:35", "link": "http://arxiv.org/abs/2502.17911v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms", "abstract": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.", "published": "2025-02-25 09:12:07", "link": "http://arxiv.org/abs/2502.18008v5", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music", "abstract": "Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation.", "published": "2025-02-25 15:53:18", "link": "http://arxiv.org/abs/2502.18309v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "From Vision to Sound: Advancing Audio Anomaly Detection with\n  Vision-Based Algorithms", "abstract": "Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.", "published": "2025-02-25 16:22:42", "link": "http://arxiv.org/abs/2502.18328v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
