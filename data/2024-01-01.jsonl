{"title": "Predicting Anti-microbial Resistance using Large Language Models", "abstract": "During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.", "published": "2024-01-01 03:04:14", "link": "http://arxiv.org/abs/2401.00642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models on Controllable Generation under\n  Diversified Instructions", "abstract": "While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.", "published": "2024-01-01 07:35:31", "link": "http://arxiv.org/abs/2401.00690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents", "abstract": "The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.", "published": "2024-01-01 16:51:20", "link": "http://arxiv.org/abs/2401.00812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large language model for Bible sentiment analysis: Sermon on the Mount", "abstract": "The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.", "published": "2024-01-01 07:35:29", "link": "http://arxiv.org/abs/2401.00689v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\n  Large Language Models in Real-world Scenarios", "abstract": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the complex capabilities required for LLMs\nto effectively use tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. The code and data are available at\nhttps://github.com/Junjie-Ye/ToolEyes.", "published": "2024-01-01 12:49:36", "link": "http://arxiv.org/abs/2401.00741v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Translation Testing via Syntactic Tree Pruning", "abstract": "Machine translation systems have been widely adopted in our daily life,\nmaking life easier and more convenient. Unfortunately, erroneous translations\nmay result in severe consequences, such as financial losses. This requires to\nimprove the accuracy and the reliability of machine translation systems.\nHowever, it is challenging to test machine translation systems because of the\ncomplexity and intractability of the underlying neural models. To tackle these\nchallenges, we propose a novel metamorphic testing approach by syntactic tree\npruning (STP) to validate machine translation systems. Our key insight is that\na pruned sentence should have similar crucial semantics compared with the\noriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\npruning strategy by basic sentence structure and dependency relations on the\nlevel of syntactic tree representation; (2) generates source sentence pairs\nbased on the metamorphic relation; (3) reports suspicious issues whose\ntranslations break the consistency property by a bag-of-words model. We further\nevaluate STP on two state-of-the-art machine translation systems (i.e., Google\nTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\nThe results show that STP can accurately find 5,073 unique erroneous\ntranslations in Google Translate and 5,100 unique erroneous translations in\nBing Microsoft Translator (400% more than state-of-the-art techniques), with\n64.5% and 65.4% precision, respectively. The reported erroneous translations\nvary in types and more than 90% of them cannot be found by state-of-the-art\ntechniques. There are 9,393 erroneous translations unique to STP, which is\n711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\nto detect translation errors for the original sentences with a recall reaching\n74.0%, improving state-of-the-art techniques by 55.1% on average.", "published": "2024-01-01 13:28:46", "link": "http://arxiv.org/abs/2401.00751v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Temporal Validity Change Prediction", "abstract": "Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.", "published": "2024-01-01 14:58:53", "link": "http://arxiv.org/abs/2401.00779v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PerSHOP -- A Persian dataset for shopping dialogue systems modeling", "abstract": "Nowadays, dialogue systems are used in many fields of industry and research.\nThere are successful instances of these systems, such as Apple Siri, Google\nAssistant, and IBM Watson. Task-oriented dialogue system is a category of\nthese, that are used in specific tasks. They can perform tasks such as booking\nplane tickets or making restaurant reservations. Shopping is one of the most\npopular areas on these systems. The bot replaces the human salesperson and\ninteracts with the customers by speaking. To train the models behind the scenes\nof these systems, annotated data is needed. In this paper, we developed a\ndataset of dialogues in the Persian language through crowd-sourcing. We\nannotated these dialogues to train a model. This dataset contains nearly 22k\nutterances in 15 different domains and 1061 dialogues. This is the largest\nPersian dataset in this field, which is provided freely so that future\nresearchers can use it. Also, we proposed some baseline models for natural\nlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\nintent classification and entity extraction. The F-1 score metric obtained for\nintent classification is around 91% and for entity extraction is around 93%,\nwhich can be a baseline for future research.", "published": "2024-01-01 16:42:56", "link": "http://arxiv.org/abs/2401.00811v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Computational Framework for Behavioral Assessment of LLM Therapists", "abstract": "The emergence of large language models (LLMs) like ChatGPT has increased\ninterest in their use as therapists to address mental health challenges and the\nwidespread lack of access to care. However, experts have emphasized the\ncritical need for systematic evaluation of LLM-based mental health\ninterventions to accurately assess their capabilities and limitations. Here, we\npropose BOLT, a proof-of-concept computational framework to systematically\nassess the conversational behavior of LLM therapists. We quantitatively measure\nLLM behavior across 13 psychotherapeutic approaches with in-context learning\nmethods. Then, we compare the behavior of LLMs against high- and low-quality\nhuman therapy. Our analysis based on Motivational Interviewing therapy reveals\nthat LLMs often resemble behaviors more commonly exhibited in low-quality\ntherapy rather than high-quality therapy, such as offering a higher degree of\nproblem-solving advice when clients share emotions. However, unlike low-quality\ntherapy, LLMs reflect significantly more upon clients' needs and strengths. Our\nfindings caution that LLM therapists still require further research for\nconsistent, high-quality care.", "published": "2024-01-01 17:32:28", "link": "http://arxiv.org/abs/2401.00820v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Graph-Convolutional Autoencoder Ensembles for the Humanities,\n  Illustrated with a Study of the American Slave Trade", "abstract": "We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.", "published": "2024-01-01 17:48:25", "link": "http://arxiv.org/abs/2401.00824v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fast and Effective Weight Update for Pruned Large Language Models", "abstract": "Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and effective weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nWe further extend it with a simple gradual pruning mask selection and achieve\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.", "published": "2024-01-01 23:10:23", "link": "http://arxiv.org/abs/2401.02938v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-tuning and Utilization Methods of Domain-specific LLMs", "abstract": "Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.", "published": "2024-01-01 06:22:04", "link": "http://arxiv.org/abs/2401.02981v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FinDABench: Benchmarking Financial Data Analysis Ability of Large\n  Language Models", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of tasks. However, their proficiency and reliability in the\nspecialized domain of financial data analysis, particularly focusing on\ndata-driven thinking, remain uncertain. To bridge this gap, we introduce\n\\texttt{FinDABench}, a comprehensive benchmark designed to evaluate the\nfinancial data analysis capabilities of LLMs within this context.\n\\texttt{FinDABench} assesses LLMs across three dimensions: 1)\n\\textbf{Foundational Ability}, evaluating the models' ability to perform\nfinancial numerical calculation and corporate sentiment risk assessment; 2)\n\\textbf{Reasoning Ability}, determining the models' ability to quickly\ncomprehend textual information and analyze abnormal financial reports; and 3)\n\\textbf{Technical Skill}, examining the models' use of technical knowledge to\naddress real-world data analysis challenges involving analysis generation and\ncharts visualization from multiple perspectives. We will release\n\\texttt{FinDABench}, and the evaluation scripts at\n\\url{https://github.com/cubenlp/BIBench}. \\texttt{FinDABench} aims to provide a\nmeasure for in-depth analysis of LLM abilities and foster the advancement of\nLLMs in the field of financial data analysis.", "published": "2024-01-01 15:26:23", "link": "http://arxiv.org/abs/2401.02982v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Mental Health Care: a Scoping Review", "abstract": "The integration of large language models (LLMs) in mental health care is an\nemerging field. There is a need to systematically review the application\noutcomes and delineate the advantages and limitations in clinical settings.\nThis review aims to provide a comprehensive overview of the use of LLMs in\nmental health care, assessing their efficacy, challenges, and potential for\nfuture applications. A systematic search was conducted across multiple\ndatabases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and\nPsyArXiv in November 2023. All forms of original research, peer-reviewed or\nnot, published or disseminated between October 1, 2019, and December 2, 2023,\nare included without language restrictions if they used LLMs developed after T5\nand directly addressed research questions in mental health care settings. From\nan initial pool of 313 articles, 34 met the inclusion criteria based on their\nrelevance to LLM application in mental health care and the robustness of\nreported outcomes. Diverse applications of LLMs in mental health care are\nidentified, including diagnosis, therapy, patient engagement enhancement, etc.\nKey challenges include data availability and reliability, nuanced handling of\nmental states, and effective evaluation methods. Despite successes in accuracy\nand accessibility improvement, gaps in clinical applicability and ethical\nconsiderations were evident, pointing to the need for robust data, standardized\nevaluations, and interdisciplinary collaboration. LLMs hold substantial promise\nfor enhancing mental health care. For their full potential to be realized,\nemphasis must be placed on developing robust datasets, development and\nevaluation frameworks, ethical guidelines, and interdisciplinary collaborations\nto address current limitations.", "published": "2024-01-01 17:35:52", "link": "http://arxiv.org/abs/2401.02984v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Digger: Detecting Copyright Content Mis-usage in Large Language Model\n  Training", "abstract": "Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n  In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.", "published": "2024-01-01 06:04:52", "link": "http://arxiv.org/abs/2401.00676v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Large Language Models aren't all that you need", "abstract": "This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.", "published": "2024-01-01 08:32:50", "link": "http://arxiv.org/abs/2401.00698v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of\n  Large Language Models", "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.", "published": "2024-01-01 13:53:53", "link": "http://arxiv.org/abs/2401.00757v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.SE"}
{"title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models", "abstract": "Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.", "published": "2024-01-01 14:02:27", "link": "http://arxiv.org/abs/2401.00761v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language\n  Models", "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.", "published": "2024-01-01 15:30:19", "link": "http://arxiv.org/abs/2401.00788v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SecFormer: Fast and Accurate Privacy-Preserving Inference for\n  Transformer Models via SMPC", "abstract": "With the growing use of Transformer models hosted on cloud platforms to offer\ninference services, privacy concerns are escalating, especially concerning\nsensitive data like investment plans and bank account details. Secure\nMulti-Party Computing (SMPC) emerges as a promising solution to protect the\nprivacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for Transformer models often leads\nto considerable slowdowns or declines in performance. This is largely due to\nthe multitude of nonlinear operations in the Transformer architecture, which\nare not well-suited to SMPC and difficult to circumvent or optimize\neffectively. To address this concern, we introduce a comprehensive PPI\nframework called SecFormer to achieve fast and accurate PPI for Transformer\nmodels. We successfully eliminate the high-cost exponential and maximum\noperations in PPI without sacrificing model performance and develop a suite of\nefficient SMPC protocols by employing suitable numerical computation methods to\nboost other complex nonlinear functions in PPI, including GeLU, LayerNorm, and\na redesigned Softmax. Our extensive experiments reveal that SecFormer\noutperforms MPCFormer in performance, showing improvements of $3.4\\%$ and\n$24.7\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In\nterms of efficiency, SecFormer is 3.57 and 3.58 times faster than PUMA for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness\nand speed.", "published": "2024-01-01 15:40:35", "link": "http://arxiv.org/abs/2401.00793v4", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models", "abstract": "Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.", "published": "2024-01-01 14:06:55", "link": "http://arxiv.org/abs/2401.00763v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.SE"}
{"title": "Enhancing Pre-trained ASR System Fine-tuning for Dysarthric Speech\n  Recognition using Adversarial Data Augmentation", "abstract": "Automatic recognition of dysarthric speech remains a highly challenging task\nto date. Neuro-motor conditions and co-occurring physical disabilities create\ndifficulty in large-scale data collection for ASR system development. Adapting\nSSL pre-trained ASR models to limited dysarthric speech via data-intensive\nparameter fine-tuning leads to poor generalization. To this end, this paper\npresents an extensive comparative study of various data augmentation approaches\nto improve the robustness of pre-trained ASR model fine-tuning to dysarthric\nspeech. These include: a) conventional speaker-independent perturbation of\nimpaired speech; b) speaker-dependent speed perturbation, or GAN-based\nadversarial perturbation of normal, control speech based on their time\nalignment against parallel dysarthric speech; c) novel Spectral basis GAN-based\nadversarial data augmentation operating on non-parallel data. Experiments\nconducted on the UASpeech corpus suggest GAN-based data augmentation\nconsistently outperforms fine-tuned Wav2vec2.0 and HuBERT models using no data\naugmentation and speed perturbation across different data expansion operating\npoints by statistically significant word error rate (WER) reductions up to\n2.01% and 0.96% absolute (9.03% and 4.63% relative) respectively on the\nUASpeech test set of 16 dysarthric speakers. After cross-system outputs\nrescoring, the best system produced the lowest published WER of 16.53% (46.47%\non very low intelligibility) on UASpeech.", "published": "2024-01-01 04:21:19", "link": "http://arxiv.org/abs/2401.00662v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ultraspherical/Gegenbauer polynomials to unify 2D/3D Ambisonic\n  directivity designs", "abstract": "This report on axisymmetric ultraspherical/Gegenbauer polynomials and their\nuse in Ambisonic directivity design in 2D and 3D presents an alternative\nmathematical formalism to what can be read in, e.g., my and Matthias Frank's\nbook on Ambisonics or J\\'er\\^ome Daniel's thesis, Gary Elko's differential\narray book chapters, or Boaz Rafaely's spherical microphone array book.\n  Ultraspherical/Gegenbauer polynomials are highly valuable when designing\naxisymmetric beams and understanding spherical t designs, and this report will\nshed some light on what circular, spherical, and ultraspherical axisymmetric\npolynomials are. While mathematically interesting by themselves already, they\ncan be useful in spherical beamforming as described in the literature on\nspherical and differential microphone arrays.\n  In this report, these ultraspherical/Gegenbauer polynomials will be used to\nuniformly derive for arbitrary dimensions D the various directivity designs or\nAmbisonic order weightings known from literature: max-DI/basic, max-rE ,\nsupercardioid, cardioid/inphase. Is there a way to relate higher-order\ncardioids and supercardioids? How could one define directivity patterns with an\non-axis flatness constraint?", "published": "2024-01-01 17:00:54", "link": "http://arxiv.org/abs/2401.00813v5", "categories": ["eess.AS", "cs.SD", "34", "J.2"], "primary_category": "eess.AS"}
{"title": "The role of direct sound spherical harmonics representation in\n  externalization using binaural reproduction", "abstract": "The importance of the information in the direct sound to human perception of\nspatial sound sources is an ongoing research topic. The classification between\ndirect sound and diffuse or reverberant sound forms the basis of numerous\nstudies in the field of spatial audio. In particular, parametric spatial audio\nrepresentation methods use this classification and employ signal processing in\norder to enhance the audio quality at reproduction. However, current literature\ndoes not provide information concerning the impact of ideal direct sound\nrepresentation on externalization, in the context of Ambisonics. This paper\naims to assess the importance of the spatial information in the direct sound in\nthe externalization of a sound field when using binaural reproduction. This is\ndone in the spherical harmonics (SH) domain, where an ideal direct sound\nrepresentation within an otherwise Ambisonics signal is simulated, and its\nperceived externalization is evaluated in a formal listening test. This\ninvestigation leads to the conclusion that externalization of a first order\nAmbisonics signal may be significantly improved by enhancing the direct sound\ncomponent, up to a level similar to a third order Ambisonics signal.", "published": "2024-01-01 19:01:10", "link": "http://arxiv.org/abs/2401.00936v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Multi-Modal Control in Music-Driven Dance Generation", "abstract": "Existing music-driven 3D dance generation methods mainly concentrate on\nhigh-quality dance generation, but lack sufficient control during the\ngeneration process. To address these issues, we propose a unified framework\ncapable of generating high-quality dance movements and supporting multi-modal\ncontrol, including genre control, semantic control, and spatial control. First,\nwe decouple the dance generation network from the dance control network,\nthereby avoiding the degradation in dance quality when adding additional\ncontrol information. Second, we design specific control strategies for\ndifferent control information and integrate them into a unified framework.\nExperimental results show that the proposed dance generation framework\noutperforms state-of-the-art methods in terms of motion quality and\ncontrollability.", "published": "2024-01-01 09:25:20", "link": "http://arxiv.org/abs/2401.01382v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
