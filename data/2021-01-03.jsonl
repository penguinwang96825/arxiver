{"title": "An Efficient Transformer Decoder with Compressed Sub-layers", "abstract": "The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.", "published": "2021-01-03 02:05:01", "link": "http://arxiv.org/abs/2101.00542v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentive Tree-structured Network for Monotonicity Reasoning", "abstract": "Many state-of-art neural models designed for monotonicity reasoning perform\npoorly on downward inference. To address this shortcoming, we developed an\nattentive tree-structured neural network. It consists of a tree-based\nlong-short-term-memory network (Tree-LSTM) with soft attention. It is designed\nto model the syntactic parse tree information from the sentence pair of a\nreasoning task. A self-attentive aggregator is used for aligning the\nrepresentations of the premise and the hypothesis. We present our model and\nevaluate it using the Monotonicity Entailment Dataset (MED). We show and\nattempt to explain that our model outperforms existing models on MED.", "published": "2021-01-03 01:29:48", "link": "http://arxiv.org/abs/2101.00540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recoding latent sentence representations -- Dynamic gradient-based\n  activation modification in RNNs", "abstract": "In Recurrent Neural Networks (RNNs), encoding information in a suboptimal or\nerroneous way can impact the quality of representations based on later elements\nin the sequence and subsequently lead to wrong predictions and a worse model\nperformance. In humans, challenging cases like garden path sentences (an\ninstance of this being the infamous \"The horse raced past the barn fell\") can\nlead their language understanding astray. However, they are still able to\ncorrect their representation accordingly and recover when new information is\nencountered. Inspired by this, I propose an augmentation to standard RNNs in\nform of a gradient-based correction mechanism: This way I hope to enable such\nmodels to dynamically adapt their inner representation of a sentence, adding a\nway to correct deviations as soon as they occur. This could therefore lead to\nmore robust models using more flexible representations, even during inference\ntime.\n  I conduct different experiments in the context of language modeling, where\nthe impact of using such a mechanism is examined in detail. To this end, I look\nat modifications based on different kinds of time-dependent error signals and\nhow they influence the model performance. Furthermore, this work contains a\nstudy of the model's confidence in its predictions during training and for\nchallenging test samples and the effect of the manipulation thereof. Lastly, I\nalso study the difference in behavior of these novel models compared to a\nstandard LSTM baseline and investigate error cases in detail to identify points\nof future research. I show that while the proposed approach comes with\npromising theoretical guarantees and an appealing intuition, it is only able to\nproduce minor improvements over the baseline due to challenges in its practical\napplication and the efficacy of the tested model variants.", "published": "2021-01-03 17:54:17", "link": "http://arxiv.org/abs/2101.00674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Neural Networks on SVD Boosted Latent Spaces for Semantic\n  Classification", "abstract": "The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.", "published": "2021-01-03 05:30:37", "link": "http://arxiv.org/abs/2101.00563v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adversarial Unsupervised Domain Adaptation for Harmonic-Percussive\n  Source Separation", "abstract": "This paper addresses the problem of domain adaptation for the task of music\nsource separation. Using datasets from two different domains, we compare the\nperformance of a deep learning-based harmonic-percussive source separation\nmodel under different training scenarios, including supervised joint training\nusing data from both domains and pre-training in one domain with fine-tuning in\nanother. We propose an adversarial unsupervised domain adaptation approach\nsuitable for the case where no labelled data (ground-truth source signals) from\na target domain is available. By leveraging unlabelled data (only mixtures)\nfrom this domain, experiments show that our framework can improve separation\nperformance on the new domain without losing any considerable performance on\nthe original domain. The paper also introduces the Tap & Fiddle dataset, a\ndataset containing recordings of Scandinavian fiddle tunes along with isolated\ntracks for 'foot-tapping' and 'violin'.", "published": "2021-01-03 20:46:42", "link": "http://arxiv.org/abs/2101.00701v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
