{"title": "Large-Scale Machine Translation between Arabic and Hebrew: Available\n  Corpora and Initial Results", "abstract": "Machine translation between Arabic and Hebrew has so far been limited by a\nlack of parallel corpora, despite the political and cultural importance of this\nlanguage pair. Previous work relied on manually-crafted grammars or pivoting\nvia English, both of which are unsatisfactory for building a scalable and\naccurate MT system. In this work, we compare standard phrase-based and neural\nsystems on Arabic-Hebrew translation. We experiment with tokenization by\nexternal tools and sub-word modeling by character-level neural models, and show\nthat both methods lead to improved translation performance, with a small\nadvantage to the neural models.", "published": "2016-09-25 05:07:55", "link": "http://arxiv.org/abs/1609.07701v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) heavily relies on word-level modelling to\nlearn semantic representations of input sentences. However, for languages\nwithout natural word delimiters (e.g., Chinese) where input sentences have to\nbe tokenized first, conventional NMT is confronted with two issues: 1) it is\ndifficult to find an optimal tokenization granularity for source sentence\nmodelling, and 2) errors in 1-best tokenizations may propagate to the encoder\nof NMT. To handle these issues, we propose word-lattice based Recurrent Neural\nNetwork (RNN) encoders for NMT, which generalize the standard RNN to word\nlattice topology. The proposed encoders take as input a word lattice that\ncompactly encodes multiple tokenizations, and learn to generate new hidden\nstates from arbitrarily many inputs and hidden states in preceding time steps.\nAs such, the word-lattice based encoders not only alleviate the negative impact\nof tokenization errors but also are more expressive and flexible to embed input\nsentences. Experiment results on Chinese-English translation demonstrate the\nsuperiorities of the proposed encoders over the conventional encoder.", "published": "2016-09-25 10:59:01", "link": "http://arxiv.org/abs/1609.07730v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Factorized Model for Transitive Verbs in Compositional Distributional\n  Semantics", "abstract": "We present a factorized compositional distributional semantics model for the\nrepresentation of transitive verb constructions. Our model first produces\n(subject, verb) and (verb, object) vector representations based on the\nsimilarity of the nouns in the construction to each of the nouns in the\nvocabulary and the tendency of these nouns to take the subject and object roles\nof the verb. These vectors are then combined into a final (subject,verb,object)\nrepresentation through simple vector operations. On two established tasks for\nthe transitive verb construction our model outperforms recent previous work.", "published": "2016-09-25 15:10:16", "link": "http://arxiv.org/abs/1609.07756v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
