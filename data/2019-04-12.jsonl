{"title": "Modeling Interpersonal Linguistic Coordination in Conversations using\n  Word Mover's Distance", "abstract": "Linguistic coordination is a well-established phenomenon in spoken\nconversations and often associated with positive social behaviors and outcomes.\nWhile there have been many attempts to measure lexical coordination or\nentrainment in literature, only a few have explored coordination in syntactic\nor semantic space. In this work, we attempt to combine these different aspects\nof coordination into a single measure by leveraging distances in a neural word\nrepresentation space. In particular, we adopt the recently proposed Word\nMover's Distance with word2vec embeddings and extend it to measure the\ndissimilarity in language used in multiple consecutive speaker turns. To\nvalidate our approach, we apply this measure for two case studies in the\nclinical psychology domain. We find that our proposed measure is correlated\nwith the therapist's empathy towards their patient in Motivational Interviewing\nand with affective behaviors in Couples Therapy. In both case studies, our\nproposed metric exhibits higher correlation than previously proposed measures.\nWhen applied to the couples with relationship improvement, we also notice a\nsignificant decrease in the proposed measure over the course of therapy,\nindicating higher linguistic coordination.", "published": "2019-04-12 01:54:01", "link": "http://arxiv.org/abs/1904.06002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Crowdsourced Frame Disambiguation Corpus with Ambiguity", "abstract": "We present a resource for the task of FrameNet semantic frame disambiguation\nof over 5,000 word-sentence pairs from the Wikipedia corpus. The annotations\nwere collected using a novel crowdsourcing approach with multiple workers per\nsentence to capture inter-annotator disagreement. In contrast to the typical\napproach of attributing the best single frame to each word, we provide a list\nof frames with disagreement-based scores that express the confidence with which\neach frame applies to the word. This is based on the idea that inter-annotator\ndisagreement is at least partly caused by ambiguity that is inherent to the\ntext and frames. We have found many examples where the semantics of individual\nframes overlap sufficiently to make them acceptable alternatives for\ninterpreting a sentence. We have argued that ignoring this ambiguity creates an\noverly arbitrary target for training and evaluating natural language processing\nsystems - if humans cannot agree, why would we expect the correct answer from a\nmachine to be any different? To process this data we also utilized an expanded\nlemma-set provided by the Framester system, which merges FN with WordNet to\nenhance coverage. Our dataset includes annotations of 1,000 sentence-word pairs\nwhose lemmas are not part of FN. Finally we present metrics for evaluating\nframe disambiguation systems that account for ambiguity.", "published": "2019-04-12 08:48:03", "link": "http://arxiv.org/abs/1904.06101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Political Text Scaling Meets Computational Semantics", "abstract": "During the last fifteen years, automatic text scaling has become one of the\nkey tools of the Text as Data community in political science. Prominent text\nscaling algorithms, however, rely on the assumption that latent positions can\nbe captured just by leveraging the information about word frequencies in\ndocuments under study. We challenge this traditional view and present a new,\nsemantically aware text scaling algorithm, SemScale, which combines recent\ndevelopments in the area of computational linguistics with unsupervised\ngraph-based clustering. We conduct an extensive quantitative analysis over a\ncollection of speeches from the European Parliament in five different languages\nand from two different legislative terms, and show that a scaling approach\nrelying on semantic document representations is often better at capturing known\nunderlying political dimensions than the established frequency-based (i.e.,\nsymbolic) scaling method. We further validate our findings through a series of\nexperiments focused on text preprocessing and feature selection, document\nrepresentation, scaling of party manifestos, and a supervised extension of our\nalgorithm. To catalyze further research on this new branch of text scaling\nmethods, we release a Python implementation of SemScale with all included data\nsets and evaluation procedures.", "published": "2019-04-12 13:05:06", "link": "http://arxiv.org/abs/1904.06217v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIT (BHU) Varanasi at MSR-SRST 2018: A Language Model Based Approach for\n  Natural Language Generation", "abstract": "This paper describes our submission system for the Shallow Track of Surface\nRealization Shared Task 2018 (SRST'18). The task was to convert genuine UD\nstructures, from which word order information had been removed and the tokens\nhad been lemmatized, into their correct sentential form. We divide the problem\nstatement into two parts, word reinflection and correct word order prediction.\nFor the first sub-problem, we use a Long Short Term Memory based\nEncoder-Decoder approach. For the second sub-problem, we present a Language\nModel (LM) based approach. We apply two different sub-approaches in the LM\nBased approach and the combined result of these two approaches is considered as\nthe final output of the system.", "published": "2019-04-12 13:52:29", "link": "http://arxiv.org/abs/1904.06234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CITE: A Corpus of Image-Text Discourse Relations", "abstract": "This paper presents a novel crowd-sourced resource for multimodal discourse:\nour resource characterizes inferences in image-text contexts in the domain of\ncooking recipes in the form of coherence relations. Like previous corpora\nannotating discourse structure between text arguments, such as the Penn\nDiscourse Treebank, our new corpus aids in establishing a better understanding\nof natural communication and common-sense reasoning, while our findings have\nimplications for a wide range of applications, such as understanding and\ngeneration of multimodal documents.", "published": "2019-04-12 15:46:31", "link": "http://arxiv.org/abs/1904.06286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Representational Hub of Language and Vision Models", "abstract": "The multimodal models used in the emerging field at the intersection of\ncomputational linguistics and computer vision implement the bottom-up\nprocessing of the `Hub and Spoke' architecture proposed in cognitive science to\nrepresent how the brain processes and combines multi-sensory inputs. In\nparticular, the Hub is implemented as a neural network encoder. We investigate\nthe effect on this encoder of various vision-and-language tasks proposed in the\nliterature: visual question answering, visual reference resolution, and\nvisually grounded dialogue. To measure the quality of the representations\nlearned by the encoder, we use two kinds of analyses. First, we evaluate the\nencoder pre-trained on the different vision-and-language tasks on an existing\ndiagnostic task designed to assess multimodal semantic understanding. Second,\nwe carry out a battery of analyses aimed at studying how the encoder merges and\nexploits the two modalities.", "published": "2019-04-12 05:18:35", "link": "http://arxiv.org/abs/1904.06038v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multimodal Speech Emotion Recognition and Ambiguity Resolution", "abstract": "Identifying emotion from speech is a non-trivial task pertaining to the\nambiguous definition of emotion itself. In this work, we adopt a\nfeature-engineering based approach to tackle the task of speech emotion\nrecognition. Formalizing our problem as a multi-class classification problem,\nwe compare the performance of two categories of models. For both, we extract\neight hand-crafted features from the audio signal. In the first approach, the\nextracted features are used to train six traditional machine learning\nclassifiers, whereas the second approach is based on deep learning wherein a\nbaseline feed-forward neural network and an LSTM-based classifier are trained\nover the same features. In order to resolve ambiguity in communication, we also\ninclude features from the text domain. We report accuracy, f-score, precision,\nand recall for the different experiment settings we evaluated our models in.\nOverall, we show that lighter machine learning based models trained over a few\nhand-crafted features are able to achieve performance comparable to the current\ndeep learning based state-of-the-art method for emotion recognition.", "published": "2019-04-12 03:22:13", "link": "http://arxiv.org/abs/1904.06022v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Direct speech-to-speech translation with a sequence-to-sequence model", "abstract": "We present an attention-based sequence-to-sequence neural network which can\ndirectly translate speech from one language into speech in another language,\nwithout relying on an intermediate text representation. The network is trained\nend-to-end, learning to map speech spectrograms into target spectrograms in\nanother language, corresponding to the translated content (in a different\ncanonical voice). We further demonstrate the ability to synthesize translated\nspeech using the voice of the source speaker. We conduct experiments on two\nSpanish-to-English speech translation datasets, and find that the proposed\nmodel slightly underperforms a baseline cascade of a direct speech-to-text\ntranslation model and a text-to-speech synthesis model, demonstrating the\nfeasibility of the approach on this very challenging task.", "published": "2019-04-12 05:15:31", "link": "http://arxiv.org/abs/1904.06037v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Building a mixed-lingual neural TTS system with only monolingual data", "abstract": "When deploying a Chinese neural text-to-speech (TTS) synthesis system, one of\nthe challenges is to synthesize Chinese utterances with English phrases or\nwords embedded. This paper looks into the problem in the encoder-decoder\nframework when only monolingual data from a target speaker is available.\nSpecifically, we view the problem from two aspects: speaker consistency within\nan utterance and naturalness. We start the investigation with an Average Voice\nModel which is built from multi-speaker monolingual data, i.e. Mandarin and\nEnglish data. On the basis of that, we look into speaker embedding for speaker\nconsistency within an utterance and phoneme embedding for naturalness and\nintelligibility and study the choice of data for model training. We report the\nfindings and discuss the challenges to build a mixed-lingual TTS system with\nonly monolingual data.", "published": "2019-04-12 06:54:34", "link": "http://arxiv.org/abs/1904.06063v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Speech Domain Adaptation Based on Disentangled\n  Representation Learning for Robust Speech Recognition", "abstract": "In general, the performance of automatic speech recognition (ASR) systems is\nsignificantly degraded due to the mismatch between training and test\nenvironments. Recently, a deep-learning-based image-to-image translation\ntechnique to translate an image from a source domain to a desired domain was\npresented, and cycle-consistent adversarial network (CycleGAN) was applied to\nlearn a mapping for speech-to-speech conversion from a speaker to a target\nspeaker. However, this method might not be adequate to remove corrupting noise\ncomponents for robust ASR because it was designed to convert speech itself. In\nthis paper, we propose a domain adaptation method based on generative\nadversarial nets (GANs) with disentangled representation learning to achieve\nrobustness in ASR systems. In particular, two separated encoders, context and\ndomain encoders, are introduced to learn distinct latent variables. The latent\nvariables allow us to convert the domain of speech according to its context and\ndomain representation. We improved word accuracies by 6.55~15.70\\% for the\nCHiME4 challenge corpus by applying a noisy-to-clean environment adaptation for\nrobust ASR. In addition, similar to the method based on the CycleGAN, this\nmethod can be used for gender adaptation in gender-mismatched recognition.", "published": "2019-04-12 07:55:53", "link": "http://arxiv.org/abs/1904.06086v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STC Speaker Recognition Systems for the VOiCES From a Distance Challenge", "abstract": "This paper presents the Speech Technology Center (STC) speaker recognition\n(SR) systems submitted to the VOiCES From a Distance challenge 2019. The\nchallenge's SR task is focused on the problem of speaker recognition in single\nchannel distant/far-field audio under noisy conditions. In this work we\ninvestigate different deep neural networks architectures for speaker embedding\nextraction to solve the task. We show that deep networks with residual frame\nlevel connections outperform more shallow architectures. Simple energy based\nspeech activity detector (SAD) and automatic speech recognition (ASR) based SAD\nare investigated in this work. We also address the problem of data preparation\nfor robust embedding extractors training. The reverberation for the data\naugmentation was performed using automatic room impulse response generator. In\nour systems we used discriminatively trained cosine similarity metric learning\nmodel as embedding backend. Scores normalization procedure was applied for each\nindividual subsystem we used. Our final submitted systems were based on the\nfusion of different subsystems. The results obtained on the VOiCES development\nand evaluation sets demonstrate effectiveness and robustness of the proposed\nsystems when dealing with distant/far-field audio under noisy conditions.", "published": "2019-04-12 08:23:26", "link": "http://arxiv.org/abs/1904.06093v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adapting Sequence to Sequence models for Text Normalization in Social\n  Media", "abstract": "Social media offer an abundant source of valuable raw data, however informal\nwriting can quickly become a bottleneck for many natural language processing\n(NLP) tasks. Off-the-shelf tools are usually trained on formal text and cannot\nexplicitly handle noise found in short online posts. Moreover, the variety of\nfrequently occurring linguistic variations presents several challenges, even\nfor humans who might not be able to comprehend the meaning of such posts,\nespecially when they contain slang and abbreviations. Text Normalization aims\nto transform online user-generated text to a canonical form. Current text\nnormalization systems rely on string or phonetic similarity and classification\nmodels that work on a local fashion. We argue that processing contextual\ninformation is crucial for this task and introduce a social media text\nnormalization hybrid word-character attention-based encoder-decoder model that\ncan serve as a pre-processing step for NLP applications to adapt to noisy text\nin social media. Our character-based component is trained on synthetic\nadversarial examples that are designed to capture errors commonly found in\nonline user-generated text. Experiments show that our model surpasses neural\narchitectures designed for text normalization and achieves comparable\nperformance with state-of-the-art related work.", "published": "2019-04-12 08:45:43", "link": "http://arxiv.org/abs/1904.06100v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RNN-based speech synthesis using a continuous sinusoidal model", "abstract": "Recently in statistical parametric speech synthesis, we proposed a continuous\nsinusoidal model (CSM) using continuous F0 (contF0) in combination with Maximum\nVoiced Frequency (MVF), which was successfully giving state-of-the-art vocoders\nperformance (e.g. similar to STRAIGHT) in synthesized speech. In this paper, we\naddress the use of sequence-to-sequence modeling with recurrent neural networks\n(RNNs). Bidirectional long short-term memory (Bi-LSTM) is investigated and\napplied using our CSM to model contF0, MVF, and Mel-Generalized Cepstrum (MGC)\nfor more natural sounding synthesized speech. For refining the output of the\ncontF0 estimation, post-processing based on time-warping approach is applied to\nreduce the unwanted voiced component of the unvoiced speech sounds, resulting\nin an enhanced contF0 track. The overall conclusion is covered by objective\nevaluation and subjective listening test, showing that the proposed framework\nprovides satisfactory results in terms of naturalness and intelligibility, and\nis comparable to the high-quality WORLD model based RNNs.", "published": "2019-04-12 07:23:54", "link": "http://arxiv.org/abs/1904.06075v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DNN-based Acoustic-to-Articulatory Inversion using Ultrasound Tongue\n  Imaging", "abstract": "Speech sounds are produced as the coordinated movement of the speaking\norgans. There are several available methods to model the relation of\narticulatory movements and the resulting speech signal. The reverse problem is\noften called as acoustic-to-articulatory inversion (AAI). In this paper we have\nimplemented several different Deep Neural Networks (DNNs) to estimate the\narticulatory information from the acoustic signal. There are several previous\nworks related to performing this task, but most of them are using\nElectroMagnetic Articulography (EMA) for tracking the articulatory movement.\nCompared to EMA, Ultrasound Tongue Imaging (UTI) is a technique of higher\ncost-benefit if we take into account equipment cost, portability, safety and\nvisualized structures. Seeing that, our goal is to train a DNN to obtain UT\nimages, when using speech as input. We also test two approaches to represent\nthe articulatory information: 1) the EigenTongue space and 2) the raw\nultrasound image. As an objective quality measure for the reconstructed UT\nimages, we use MSE, Structural Similarity Index (SSIM) and Complex-Wavelet SSIM\n(CW-SSIM). Our experimental results show that CW-SSIM is the most useful error\nmeasure in the UTI context. We tested three different system configurations: a)\nsimple DNN composed of 2 hidden layers with 64x64 pixels of an UTI file as\ntarget; b) the same simple DNN but with ultrasound images projected to the\nEigenTongue space as the target; c) and a more complex DNN composed of 5 hidden\nlayers with UTI files projected to the EigenTongue space. In a subjective\nexperiment the subjects found that the neural networks with two hidden layers\nwere more suitable for this inversion task.", "published": "2019-04-12 07:42:20", "link": "http://arxiv.org/abs/1904.06083v1", "categories": ["cs.SD", "eess.AS", "q-bio.TO"], "primary_category": "cs.SD"}
{"title": "Examining the Mapping Functions of Denoising Autoencoders in Singing\n  Voice Separation", "abstract": "The goal of this work is to investigate what singing voice separation\napproaches based on neural networks learn from the data. We examine the mapping\nfunctions of neural networks based on the denoising autoencoder (DAE) model\nthat are conditioned on the mixture magnitude spectra. To approximate the\nmapping functions, we propose an algorithm inspired by the knowledge\ndistillation, denoted the neural couplings algorithm (NCA). The NCA yields a\nmatrix that expresses the mapping of the mixture to the target source magnitude\ninformation. Using the NCA, we examine the mapping functions of three\nfundamental DAE-based models in music source separation; one with single-layer\nencoder and decoder, one with multi-layer encoder and single-layer decoder, and\none using skip-filtering connections (SF) with a single-layer encoding and\ndecoding. We first train these models with realistic data to estimate the\nsinging voice magnitude spectra from the corresponding mixture. We then use the\noptimized models and test spectral data as input to the NCA. Our experimental\nfindings show that approaches based on the DAE model learn scalar filtering\noperators, exhibiting a predominant diagonal structure in their corresponding\nmapping functions, limiting the exploitation of inter-frequency structure of\nmusic data. In contrast, skip-filtering connections are shown to assist the DAE\nmodel in learning filtering operators that exploit richer inter-frequency\nstructures.", "published": "2019-04-12 11:22:43", "link": "http://arxiv.org/abs/1904.06157v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Assisted Sound Sample Generation with Musical Conditioning in\n  Adversarial Auto-Encoders", "abstract": "Generative models have thrived in computer vision, enabling unprecedented\nimage processes. Yet the results in audio remain less advanced. Our project\ntargets real-time sound synthesis from a reduced set of high-level parameters,\nincluding semantic controls that can be adapted to different sound libraries\nand specific tags. These generative variables should allow expressive\nmodulations of target musical qualities and continuously mix into new styles.\nTo this extent we train AEs on an orchestral database of individual note\nsamples, along with their intrinsic attributes: note class, timbre domain and\nextended playing techniques. We condition the decoder for control over the\nrendered note attributes and use latent adversarial training for learning\nexpressive style parameters that can ultimately be mixed. We evaluate both\ngenerative performances and latent representation. Our ablation study\ndemonstrates the effectiveness of the musical conditioning mechanisms. The\nproposed model generates notes as magnitude spectrograms from any probabilistic\nlatent code samples, with expressive control of orchestral timbres and playing\nstyles. Its training data subsets can directly be visualized in the 3D latent\nrepresentation. Waveform rendering can be done offline with GLA. In order to\nallow real-time interactions, we fine-tune the decoder with a pretrained MCNN\nand embed the full waveform generation pipeline in a plugin. Moreover the\nencoder could be used to process new input samples, after manipulating their\nlatent attribute representation, the decoder can generate sample variations as\nan audio effect would. Our solution remains rather fast to train, it can\ndirectly be applied to other sound domains, including an user's libraries with\ncustom sound tags that could be mapped to specific generative controls. As a\nresult, it fosters creativity and intuitive audio style experimentations.", "published": "2019-04-12 13:02:17", "link": "http://arxiv.org/abs/1904.06215v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
