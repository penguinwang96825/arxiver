{"title": "Leveraging Advantages of Interactive and Non-Interactive Models for\n  Vector-Based Cross-Lingual Information Retrieval", "abstract": "Interactive and non-interactive model are the two de-facto standard\nframeworks in vector-based cross-lingual information retrieval (V-CLIR), which\nembed queries and documents in synchronous and asynchronous fashions,\nrespectively. From the retrieval accuracy and computational efficiency\nperspectives, each model has its own superiority and shortcoming. In this\npaper, we propose a novel framework to leverage the advantages of these two\nparadigms. Concretely, we introduce semi-interactive mechanism, which builds\nour model upon non-interactive architecture but encodes each document together\nwith its associated multilingual queries. Accordingly, cross-lingual features\ncan be better learned like an interactive model. Besides, we further transfer\nknowledge from a well-trained interactive model to ours by reusing its word\nembeddings and adopting knowledge distillation. Our model is initialized from a\nmultilingual pre-trained language model M-BERT, and evaluated on two\nopen-resource CLIR datasets derived from Wikipedia and an in-house dataset\ncollected from a real-world search engine. Extensive analyses reveal that our\nmethods significantly boost the retrieval accuracy while maintaining the\ncomputational efficiency.", "published": "2021-11-03 03:03:19", "link": "http://arxiv.org/abs/2111.01992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared\n  Task", "abstract": "This report describes Microsoft's machine translation systems for the WMT21\nshared task on large-scale multilingual machine translation. We participated in\nall three evaluation tracks including Large Track and two Small Tracks where\nthe former one is unconstrained and the latter two are fully constrained. Our\nmodel submissions to the shared task were initialized with\nDeltaLM\\footnote{\\url{https://aka.ms/deltalm}}, a generic pre-trained\nmultilingual encoder-decoder model, and fine-tuned correspondingly with the\nvast collected parallel data and allowed data sources according to track\nsettings, together with applying progressive learning and iterative\nback-translation approaches to further improve the performance. Our final\nsubmissions ranked first on three tracks in terms of the automatic evaluation\nmetric.", "published": "2021-11-03 09:16:17", "link": "http://arxiv.org/abs/2111.02086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence\n  Matching", "abstract": "This paper presents a deep neural architecture, for Natural Language Sentence\nMatching (NLSM) by adding a deep recursive encoder to BERT so called BERT with\nDeep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that\nBERT still does not capture the full complexity of text, so a deep recursive\nencoder is applied on top of BERT. Three Bi-LSTM layers with residual\nconnection are used to design a recursive encoder and an attention module is\nused on top of this encoder. To obtain the final vector, a pooling layer\nconsisting of average and maximum pooling is used. We experiment our model on\nfour benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian\nreligious questions dataset. This paper focuses on improving the BERT results\nin the NLSM task. In this regard, comparisons between BERT-DRE and BERT are\nconducted, and it is shown that in all cases, BERT-DRE outperforms BERT. The\nBERT algorithm on the religious dataset achieved an accuracy of 89.70%, and\nBERT-DRE architectures improved to 90.29% using the same dataset.", "published": "2021-11-03 12:56:13", "link": "http://arxiv.org/abs/2111.02188v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SERC: Syntactic and Semantic Sequence based Event Relation\n  Classification", "abstract": "Temporal and causal relations play an important role in determining the\ndependencies between events. Classifying the temporal and causal relations\nbetween events has many applications, such as generating event timelines, event\nsummarization, textual entailment and question answering. Temporal and causal\nrelations are closely related and influence each other. So we propose a joint\nmodel that incorporates both temporal and causal features to perform causal\nrelation classification. We use the syntactic structure of the text for\nidentifying temporal and causal relations between two events from the text. We\nextract parts-of-speech tag sequence, dependency tag sequence and word sequence\nfrom the text. We propose an LSTM based model for temporal and causal relation\nclassification that captures the interrelations between the three encoded\nfeatures. Evaluation of our model on four popular datasets yields promising\nresults for temporal and causal relation classification.", "published": "2021-11-03 14:58:52", "link": "http://arxiv.org/abs/2111.02265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HmBlogs: A big general Persian corpus", "abstract": "This paper introduces the hmBlogs corpus for Persian, as a low resource\nlanguage. This corpus has been prepared based on a collection of nearly 20\nmillion blog posts over a period of about 15 years from a space of Persian\nblogs and includes more than 6.8 billion tokens. It can be claimed that this\ncorpus is currently the largest Persian corpus that has been prepared\nindependently for the Persian language. This corpus is presented in both raw\nand preprocessed forms, and based on the preprocessed corpus some word\nembedding models are produced. By the provided models, the hmBlogs is compared\nwith some of the most important corpora available in Persian, and the results\nshow the superiority of the hmBlogs corpus over the others. These evaluations\nalso present the importance and effects of corpora, evaluation datasets, model\nproduction methods, different hyperparameters and even the evaluation methods.\nIn addition to evaluating the corpus and its produced language models, this\nresearch also presents a semantic analogy dataset.", "published": "2021-11-03 17:26:52", "link": "http://arxiv.org/abs/2111.02362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Athena 2.0: Contextualized Dialogue Management for an Alexa Prize\n  SocialBot", "abstract": "Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last\ntwo Alexa Prize Grand Challenges. One reason for Athena's success is its novel\ndialogue management strategy, which allows it to dynamically construct\ndialogues and responses from component modules, leading to novel conversations\nwith every interaction. Here we describe Athena's system design and performance\nin the Alexa Prize during the 20/21 competition. A live demo of Athena as well\nas video recordings will provoke discussion on the state of the art in\nconversational AI.", "published": "2021-11-03 20:54:20", "link": "http://arxiv.org/abs/2111.02519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.", "published": "2021-11-03 09:12:33", "link": "http://arxiv.org/abs/2111.02080v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation and Moderation of Open-domain Dialogue Systems", "abstract": "The development of Open-Domain Dialogue Systems (ODS)is a trending topic due\nto the large number of research challenges, large societal and business impact,\nand advances in the underlying technology. However, the development of these\nkinds of systems requires two important characteristics:1) automatic evaluation\nmechanisms that show high correlations with human judgements across multiple\ndialogue evaluation aspects (with explainable features for providing\nconstructive and explicit feedback on the quality of generative models'\nresponses for quick development and deployment)and 2) mechanisms that can help\nto control chatbot responses,while avoiding toxicity and employing intelligent\nways to handle toxic user comments and keeping interaction flow and engagement.\nThis track at the 10th Dialogue System Technology Challenge (DSTC10) is part of\nthe ongoing effort to promote scalable and toxic-free ODS. This paper describes\nthe datasets and baselines provided to participants, as well as submission\nevaluation results for each of the two proposed subtasks.", "published": "2021-11-03 10:08:05", "link": "http://arxiv.org/abs/2111.02110v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Lingua Custodia's participation at the WMT 2021 Machine Translation\n  using Terminologies shared task", "abstract": "This paper describes Lingua Custodia's submission to the WMT21 shared task on\nmachine translation using terminologies. We consider three directions, namely\nEnglish to French, Russian, and Chinese. We rely on a Transformer-based\narchitecture as a building block, and we explore a method which introduces two\nmain changes to the standard procedure to handle terminologies. The first one\nconsists in augmenting the training data in such a way as to encourage the\nmodel to learn a copy behavior when it encounters terminology constraint terms.\nThe second change is constraint token masking, whose purpose is to ease copy\nbehavior learning and to improve model generalization. Empirical results show\nthat our method satisfies most terminology constraints while maintaining high\ntranslation quality.", "published": "2021-11-03 10:36:32", "link": "http://arxiv.org/abs/2111.02120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Implicit Sentiment in Aspect-based Sentiment Analysis with\n  Supervised Contrastive Pre-Training", "abstract": "Aspect-based sentiment analysis aims to identify the sentiment polarity of a\nspecific aspect in product reviews. We notice that about 30% of reviews do not\ncontain obvious opinion words, but still convey clear human-aware sentiment\norientation, which is known as implicit sentiment. However, recent neural\nnetwork-based approaches paid little attention to implicit sentiment entailed\nin the reviews. To overcome this issue, we adopt Supervised Contrastive\nPre-training on large-scale sentiment-annotated corpora retrieved from\nin-domain language resources. By aligning the representation of implicit\nsentiment expressions to those with the same sentiment label, the pre-training\nprocess leads to better capture of both implicit and explicit sentiment\norientation towards aspects in reviews. Experimental results show that our\nmethod achieves state-of-the-art performance on SemEval2014 benchmarks, and\ncomprehensive analysis validates its effectiveness on learning implicit\nsentiment.", "published": "2021-11-03 13:03:17", "link": "http://arxiv.org/abs/2111.02194v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion\n  Mining", "abstract": "User-generated content from social media is produced in many languages,\nmaking it technically challenging to compare the discussed themes from one\ndomain across different cultures and regions. It is relevant for domains in a\nglobalized world, such as market research, where people from two nations and\nmarkets might have different requirements for a product. We propose a simple,\nmodern, and effective method for building a single topic model with sentiment\nanalysis capable of covering multiple languages simultanteously, based on a\npre-trained state-of-the-art deep neural network for natural language\nunderstanding. To demonstrate its feasibility, we apply the model to newspaper\narticles and user comments of a specific domain, i.e., organic food products\nand related consumption behavior. The themes match across languages.\nAdditionally, we obtain an high proportion of stable and domain-relevant\ntopics, a meaningful relation between topics and their respective textual\ncontents, and an interpretable representation for social media documents.\nMarketing can potentially benefit from our method, since it provides an\neasy-to-use means of addressing specific customer interests from different\nmarket regions around the globe. For reproducibility, we provide the code,\ndata, and results of our study.", "published": "2021-11-03 14:49:50", "link": "http://arxiv.org/abs/2111.02259v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A PubMedBERT-based Classifier with Data Augmentation Strategy for\n  Detecting Medication Mentions in Tweets", "abstract": "As a major social media platform, Twitter publishes a large number of\nuser-generated text (tweets) on a daily basis. Mining such data can be used to\naddress important social, public health, and emergency management issues that\nare infeasible through other means. An essential step in many text mining\npipelines is named entity recognition (NER), which presents some special\nchallenges for tweet data. Among them are nonstandard expressions, extreme\nimbalanced classes, and lack of context information, etc. The track 3 of\nBioCreative challenge VII (BC7) was organized to evaluate methods for detecting\nmedication mentions in tweets. In this paper, we report our work on BC7 track\n3, where we explored a PubMedBERT-based classifier trained with a combination\nof multiple data augmentation approaches. Our method achieved an F1 score of\n0.762, which is substantially higher than the mean of all submissions (0.696).", "published": "2021-11-03 14:29:24", "link": "http://arxiv.org/abs/2112.02998v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenPrompt: An Open-source Framework for Prompt-learning", "abstract": "Prompt-learning has become a new paradigm in modern natural language\nprocessing, which directly adapts pre-trained language models (PLMs) to\n$cloze$-style prediction, autoregressive modeling, or sequence to sequence\ngeneration, resulting in promising performances on various tasks. However, no\nstandard implementation framework of prompt-learning is proposed yet, and most\nexisting prompt-learning codebases, often unregulated, only provide limited\nimplementations for specific scenarios. Since there are many details such as\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\nto be considered in prompt-learning, practitioners face impediments to quickly\nadapting the desired prompt learning methods to their applications. In this\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\nequipped with efficiency, modularity, and extendibility, and its combinability\nallows the freedom to combine different PLMs, task formats, and prompting\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\nframeworks and evaluate the generalization of them on different NLP tasks\nwithout constraints. OpenPrompt is publicly released at {\\url{\nhttps://github.com/thunlp/OpenPrompt}}.", "published": "2021-11-03 03:31:14", "link": "http://arxiv.org/abs/2111.01998v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Speaker Role Identification in Air Traffic\n  Communication Using Deep Learning Approaches", "abstract": "Automatic spoken instruction understanding (SIU) of the controller-pilot\nconversations in the air traffic control (ATC) requires not only recognizing\nthe words and semantics of the speech but also determining the role of the\nspeaker. However, few of the published works on the automatic understanding\nsystems in air traffic communication focus on speaker role identification\n(SRI). In this paper, we formulate the SRI task of controller-pilot\ncommunication as a binary classification problem. Furthermore, the text-based,\nspeech-based, and speech and text based multi-modal methods are proposed to\nachieve a comprehensive comparison of the SRI task. To ablate the impacts of\nthe comparative approaches, various advanced neural network architectures are\napplied to optimize the implementation of text-based and speech-based methods.\nMost importantly, a multi-modal speaker role identification network (MMSRINet)\nis designed to achieve the SRI task by considering both the speech and textual\nmodality features. To aggregate modality features, the modal fusion module is\nproposed to fuse and squeeze acoustic and textual representations by modal\nattention mechanism and self-attention pooling layer, respectively. Finally,\nthe comparative approaches are validated on the ATCSpeech corpus collected from\na real-world ATC environment. The experimental results demonstrate that all the\ncomparative approaches are worked for the SRI task, and the proposed MMSRINet\nshows the competitive performance and robustness than the other methods on both\nseen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively.", "published": "2021-11-03 07:00:20", "link": "http://arxiv.org/abs/2111.02041v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs", "abstract": "Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.", "published": "2021-11-03 10:16:39", "link": "http://arxiv.org/abs/2111.02114v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A cross-modal fusion network based on self-attention and residual\n  structure for multimodal emotion recognition", "abstract": "The audio-video based multimodal emotion recognition has attracted a lot of\nattention due to its robust performance. Most of the existing methods focus on\nproposing different cross-modal fusion strategies. However, these strategies\nintroduce redundancy in the features of different modalities without fully\nconsidering the complementary properties between modal information, and these\napproaches do not guarantee the non-loss of original semantic information\nduring intra- and inter-modal interactions. In this paper, we propose a novel\ncross-modal fusion network based on self-attention and residual structure\n(CFN-SR) for multimodal emotion recognition. Firstly, we perform representation\nlearning for audio and video modalities to obtain the semantic features of the\ntwo modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed\nthe features of the two modalities into the cross-modal blocks separately to\nensure efficient complementarity and completeness of information through the\nself-attention mechanism and residual structure. Finally, we obtain the output\nof emotions by splicing the obtained fused representation with the original\nrepresentation. To verify the effectiveness of the proposed method, we conduct\nexperiments on the RAVDESS dataset. The experimental results show that the\nproposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with\n26.30M parameters. Our code is available at\nhttps://github.com/skeletonNN/CFN-SR.", "published": "2021-11-03 12:24:03", "link": "http://arxiv.org/abs/2111.02172v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "End-to-End Annotator Bias Approximation on Crowdsourced Single-Label\n  Sentiment Analysis", "abstract": "Sentiment analysis is often a crowdsourcing task prone to subjective labels\ngiven by many annotators. It is not yet fully understood how the annotation\nbias of each annotator can be modeled correctly with state-of-the-art methods.\nHowever, resolving annotator bias precisely and reliably is the key to\nunderstand annotators' labeling behavior and to successfully resolve\ncorresponding individual misconceptions and wrongdoings regarding the\nannotation task. Our contribution is an explanation and improvement for precise\nneural end-to-end bias modeling and ground truth estimation, which reduces an\nundesired mismatch in that regard of the existing state-of-the-art.\nClassification experiments show that it has potential to improve accuracy in\ncases where each sample is annotated only by one single annotator. We provide\nthe whole source code publicly and release an own domain-specific sentiment\ndataset containing 10,000 sentences discussing organic food products. These are\ncrawled from social media and are singly labeled by 10 non-expert annotators.", "published": "2021-11-03 16:20:16", "link": "http://arxiv.org/abs/2111.02326v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VLMo: Unified Vision-Language Pre-Training with\n  Mixture-of-Modality-Experts", "abstract": "We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA, NLVR2 and image-text retrieval.\nThe code and pretrained models are available at https://aka.ms/vlmo.", "published": "2021-11-03 17:20:36", "link": "http://arxiv.org/abs/2111.02358v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers", "abstract": "Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.", "published": "2021-11-03 17:55:36", "link": "http://arxiv.org/abs/2111.02387v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Klarna Product Page Dataset: Web Element Nomination with Graph\n  Neural Networks and Large Language Models", "abstract": "Web automation holds the potential to revolutionize how users interact with\nthe digital world, offering unparalleled assistance and simplifying tasks via\nsophisticated computational methods. Central to this evolution is the web\nelement nomination task, which entails identifying unique elements on webpages.\nUnfortunately, the development of algorithmic designs for web automation is\nhampered by the scarcity of comprehensive and realistic datasets that reflect\nthe complexity faced by real-world applications on the Web. To address this, we\nintroduce the Klarna Product Page Dataset, a comprehensive and diverse\ncollection of webpages that surpasses existing datasets in richness and\nvariety. The dataset features 51,701 manually labeled product pages from 8,175\ne-commerce websites across eight geographic regions, accompanied by a dataset\nof rendered page screenshots. To initiate research on the Klarna Product Page\nDataset, we empirically benchmark a range of Graph Neural Networks (GNNs) on\nthe web element nomination task. We make three important contributions. First,\nwe found that a simple Convolutional GNN (GCN) outperforms complex\nstate-of-the-art nomination methods. Second, we introduce a training refinement\nprocedure that involves identifying a small number of relevant elements from\neach page using the aforementioned GCN. These elements are then passed to a\nlarge language model for the final nomination. This procedure significantly\nimproves the nomination accuracy by 16.8 percentage points on our challenging\ndataset, without any need for fine-tuning. Finally, in response to another\nprevalent challenge in this field - the abundance of training methodologies\nsuitable for element nomination - we introduce the Challenge Nomination\nTraining Procedure, a novel training approach that further boosts nomination\naccuracy.", "published": "2021-11-03 12:13:52", "link": "http://arxiv.org/abs/2111.02168v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.HC", "cs.IR", "68T07"], "primary_category": "cs.LG"}
{"title": "Automatic Embedding of Stories Into Collections of Independent Media", "abstract": "We look at how machine learning techniques that derive properties of items in\na collection of independent media can be used to automatically embed stories\ninto such collections. To do so, we use models that extract the tempo of songs\nto make a music playlist follow a narrative arc. Our work specifies an\nopen-source tool that uses pre-trained neural network models to extract the\nglobal tempo of a set of raw audio files and applies these measures to create a\nnarrative-following playlist. This tool is available at\nhttps://github.com/dylanashley/playlist-story-builder/releases/tag/v1.0.0", "published": "2021-11-03 13:36:47", "link": "http://arxiv.org/abs/2111.02216v1", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "H.5.5; I.2.6; J.5"], "primary_category": "cs.CL"}
{"title": "A Strongly-Labelled Polyphonic Dataset of Urban Sounds with\n  Spatiotemporal Context", "abstract": "This paper introduces SINGA:PURA, a strongly labelled polyphonic urban sound\ndataset with spatiotemporal context. The data were collected via several\nrecording units deployed across Singapore as a part of a wireless acoustic\nsensor network. These recordings were made as part of a project to identify and\nmitigate noise sources in Singapore, but also possess a wider applicability to\nsound event detection, classification, and localization. This paper introduces\nan accompanying hierarchical label taxonomy, which has been designed to be\ncompatible with other existing datasets for urban sound tagging while also able\nto capture sound events unique to the Singaporean context. This paper details\nthe data collection, annotation, and processing methodologies for the creation\nof the dataset. We further perform exploratory data analysis and include the\nperformance of a baseline model on the dataset as a benchmark.", "published": "2021-11-03 03:52:34", "link": "http://arxiv.org/abs/2111.02006v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparison of Discrete and Soft Speech Units for Improved Voice\n  Conversion", "abstract": "The goal of voice conversion is to transform source speech into a target\nvoice, keeping the content unchanged. In this paper, we focus on\nself-supervised representation learning for voice conversion. Specifically, we\ncompare discrete and soft speech units as input features. We find that discrete\nrepresentations effectively remove speaker information but discard some\nlinguistic content - leading to mispronunciations. As a solution, we propose\nsoft speech units. To learn soft units, we predict a distribution over discrete\nspeech units. By modeling uncertainty, soft units capture more content\ninformation, improving the intelligibility and naturalness of converted speech.\nSamples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code\navailable at https://github.com/bshall/soft-vc/.", "published": "2021-11-03 17:58:03", "link": "http://arxiv.org/abs/2111.02392v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STC speaker recognition systems for the NIST SRE 2021", "abstract": "This paper presents a description of STC Ltd. systems submitted to the NIST\n2021 Speaker Recognition Evaluation for both fixed and open training\nconditions. These systems consists of a number of diverse subsystems based on\nusing deep neural networks as feature extractors. During the NIST 2021 SRE\nchallenge we focused on the training of the state-of-the-art deep speaker\nembeddings extractors like ResNets and ECAPA networks by using additive angular\nmargin based loss functions. Additionally, inspired by the recent success of\nthe wav2vec 2.0 features in automatic speech recognition we explored the\neffectiveness of this approach for the speaker verification filed. According to\nour observation the fine-tuning of the pretrained large wav2vec 2.0 model\nprovides our best performing systems for open track condition. Our experiments\nwith wav2vec 2.0 based extractors for the fixed condition showed that\nunsupervised autoregressive pretraining with Contrastive Predictive Coding loss\nopens the door to training powerful transformer-based extractors from raw\nspeech signals. For video modality we developed our best solution with\nRetinaFace face detector and deep ResNet face embeddings extractor trained on\nlarge face image datasets. The final results for primary systems were obtained\nby different configurations of subsystems fusion on the score level followed by\nscore calibration.", "published": "2021-11-03 15:31:01", "link": "http://arxiv.org/abs/2111.02298v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Weight, Block or Unit? Exploring Sparsity Tradeoffs for Speech\n  Enhancement on Tiny Neural Accelerators", "abstract": "We explore network sparsification strategies with the aim of compressing\nneural speech enhancement (SE) down to an optimal configuration for a new\ngeneration of low power microcontroller based neural accelerators (microNPU's).\nWe examine three unique sparsity structures: weight pruning, block pruning and\nunit pruning; and discuss their benefits and drawbacks when applied to SE. We\nfocus on the interplay between computational throughput, memory footprint and\nmodel quality. Our method supports all three structures above and jointly\nlearns integer quantized weights along with sparsity. Additionally, we\ndemonstrate offline magnitude based pruning of integer quantized models as a\nperformance baseline. Although efficient speech enhancement is an active area\nof research, our work is the first to apply block pruning to SE and the first\nto address SE model compression in the context of microNPU's. Using weight\npruning, we show that we are able to compress an already compact model's memory\nfootprint by a factor of 42x from 3.7MB to 87kB while only losing 0.1 dB SDR in\nperformance. We also show a computational speedup of 6.7x with a corresponding\nSDR drop of only 0.59 dB SDR using block pruning.", "published": "2021-11-03 17:06:36", "link": "http://arxiv.org/abs/2111.02351v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning-based Non-Intrusive Multi-Objective Speech Assessment\n  Model with Cross-Domain Features", "abstract": "In this study, we propose a cross-domain multi-objective speech assessment\nmodel called MOSA-Net, which can estimate multiple speech assessment metrics\nsimultaneously. Experimental results show that MOSA-Net can improve the linear\ncorrelation coefficient (LCC) by 0.026 (0.990 vs 0.964 in seen noise\nenvironments) and 0.012 (0.969 vs 0.957 in unseen noise environments) in\nperceptual evaluation of speech quality (PESQ) prediction, compared to\nQuality-Net, an existing single-task model for PESQ prediction, and improve LCC\nby 0.021 (0.985 vs 0.964 in seen noise environments) and 0.047 (0.836 vs 0.789\nin unseen noise environments) in short-time objective intelligibility (STOI)\nprediction, compared to STOI-Net (based on CRNN), an existing single-task model\nfor STOI prediction. Moreover, MOSA-Net, originally trained to assess objective\nscores, can be used as a pre-trained model to be effectively adapted to an\nassessment model for predicting subjective quality and intelligibility scores\nwith a limited amount of training data. Experimental results show that MOSA-Net\ncan improve LCC by 0.018 (0.805 vs 0.787) in mean opinion score (MOS)\nprediction, compared to MOS-SSL, a strong single-task model for MOS prediction.\nIn light of the confirmed prediction capability, we further adopt the latent\nrepresentations of MOSA-Net to guide the speech enhancement (SE) process and\nderive a quality-intelligibility (QI)-aware SE (QIA-SE) approach accordingly.\nExperimental results show that QIA-SE provides superior enhancement performance\ncompared with the baseline SE system in terms of objective evaluation metrics\nand qualitative evaluation test. For example, QIA-SE can improve PESQ by 0.301\n(2.953 vs 2.652 in seen noise environments) and 0.18 (2.658 vs 2.478 in unseen\nnoise environments) over a CNN-based baseline SE model.", "published": "2021-11-03 17:30:43", "link": "http://arxiv.org/abs/2111.02363v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
