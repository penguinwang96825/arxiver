{"title": "Phrase Grounding by Soft-Label Chain Conditional Random Field", "abstract": "The phrase grounding task aims to ground each entity mention in a given\ncaption of an image to a corresponding region in that image. Although there are\nclear dependencies between how different mentions of the same caption should be\ngrounded, previous structured prediction methods that aim to capture such\ndependencies need to resort to approximate inference or non-differentiable\nlosses. In this paper, we formulate phrase grounding as a sequence labeling\ntask where we treat candidate regions as potential labels, and use neural chain\nConditional Random Fields (CRFs) to model dependencies among regions for\nadjacent mentions. In contrast to standard sequence labeling tasks, the phrase\ngrounding task is defined such that there may be multiple correct candidate\nregions. To address this multiplicity of gold labels, we define so-called\nSoft-Label Chain CRFs, and present an algorithm that enables convenient\nend-to-end training. Our method establishes a new state-of-the-art on phrase\ngrounding on the Flickr30k Entities dataset. Analysis shows that our model\nbenefits both from the entity dependencies captured by the CRF and from the\nsoft-label training regime. Our code is available at\n\\url{github.com/liujch1998/SoftLabelCCRF}", "published": "2019-09-01 01:57:45", "link": "http://arxiv.org/abs/1909.00301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Higher-order Comparisons of Sentence Encoder Representations", "abstract": "Representational Similarity Analysis (RSA) is a technique developed by\nneuroscientists for comparing activity patterns of different measurement\nmodalities (e.g., fMRI, electrophysiology, behavior). As a framework, RSA has\nseveral advantages over existing approaches to interpretation of language\nencoders based on probing or diagnostic classification: namely, it does not\nrequire large training samples, is not prone to overfitting, and it enables a\nmore transparent comparison between the representational geometries of\ndifferent models and modalities. We demonstrate the utility of RSA by\nestablishing a previously unknown correspondence between widely-employed\npretrained language encoders and human processing difficulty via eye-tracking\ndata, showcasing its potential in the interpretability toolbox for neural\nmodels", "published": "2019-09-01 02:13:12", "link": "http://arxiv.org/abs/1909.00303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-aware Multilingual Semantic Role Labeling", "abstract": "Recently, semantic role labeling (SRL) has earned a series of success with\neven higher performance improvements, which can be mainly attributed to\nsyntactic integration and enhanced word representation. However, most of these\nefforts focus on English, while SRL on multiple languages more than English has\nreceived relatively little attention so that is kept underdevelopment. Thus\nthis paper intends to fill the gap on multilingual SRL with special focus on\nthe impact of syntax and contextualized word representation. Unlike existing\nwork, we propose a novel method guided by syntactic rule to prune arguments,\nwhich enables us to integrate syntax into multilingual SRL model simply and\neffectively. We present a unified SRL model designed for multiple languages\ntogether with the proposed uniform syntax enhancement. Our model achieves new\nstate-of-the-art results on the CoNLL-2009 benchmarks of all seven languages.\nBesides, we pose a discussion on the syntactic role among different languages\nand verify the effectiveness of deep enhanced representation for multilingual\nSRL.", "published": "2019-09-01 02:48:43", "link": "http://arxiv.org/abs/1909.00310v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment\n  Analysis", "abstract": "Aspect based sentiment analysis (ABSA) aims to identify the sentiment\npolarity towards the given aspect in a sentence, while previous models\ntypically exploit an aspect-independent (weakly associative) encoder for\nsentence representation generation. In this paper, we propose a novel\nAspect-Guided Deep Transition model, named AGDT, which utilizes the given\naspect to guide the sentence encoding from scratch with the specially-designed\ndeep transition architecture. Furthermore, an aspect-oriented objective is\ndesigned to enforce AGDT to reconstruct the given aspect with the generated\nsentence representation. In doing so, our AGDT can accurately generate\naspect-specific sentence representation, and thus conduct more accurate\nsentiment predictions. Experimental results on multiple SemEval datasets\ndemonstrate the effectiveness of our proposed approach, which significantly\noutperforms the best reported results with the same setting.", "published": "2019-09-01 05:22:30", "link": "http://arxiv.org/abs/1909.00324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing AMR-to-Text Generation with Dual Graph Representations", "abstract": "Generating text from graph-based data, such as Abstract Meaning\nRepresentation (AMR), is a challenging task due to the inherent difficulty in\nhow to properly encode the structure of a graph with labeled edges. To address\nthis difficulty, we propose a novel graph-to-sequence model that encodes\ndifferent but complementary perspectives of the structural information\ncontained in the AMR graph. The model learns parallel top-down and bottom-up\nrepresentations of nodes capturing contrasting views of the graph. We also\ninvestigate the use of different node message passing strategies, employing\ndifferent state-of-the-art graph encoders to compute node representations based\non incoming and outgoing perspectives. In our experiments, we demonstrate that\nthe dual graph representation leads to improvements in AMR-to-text generation,\nachieving state-of-the-art results on two AMR datasets.", "published": "2019-09-01 08:22:38", "link": "http://arxiv.org/abs/1909.00352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Model to Learn Both: Zero Pronoun Prediction and Translation", "abstract": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should\nbe recalled in non-pro-drop languages. This discourse phenomenon poses a\nsignificant challenge for machine translation (MT) when translating texts from\npro-drop to non-pro-drop languages. In this paper, we propose a unified and\ndiscourse-aware ZP translation approach for neural MT models. Specifically, we\njointly learn to predict and translate ZPs in an end-to-end manner, allowing\nboth components to interact with each other. In addition, we employ\nhierarchical neural networks to exploit discourse-level context, which is\nbeneficial for ZP prediction and thus translation. Experimental results on both\nChinese-English and Japanese-English data show that our approach significantly\nand accumulatively improves both translation performance and ZP prediction\naccuracy over not only baseline but also previous works using external ZP\nprediction models. Extensive analyses confirm that the performance improvement\ncomes from the alleviation of different kinds of errors especially caused by\nsubjective ZPs.", "published": "2019-09-01 10:07:20", "link": "http://arxiv.org/abs/1909.00369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attention with Structural Position Representations", "abstract": "Although self-attention networks (SANs) have advanced the state-of-the-art on\nvarious NLP tasks, one criticism of SANs is their ability of encoding positions\nof input words (Shaw et al., 2018). In this work, we propose to augment SANs\nwith structural position representations to model the latent structure of the\ninput sentence, which is complementary to the standard sequential positional\nrepresentations. Specifically, we use dependency tree to represent the\ngrammatical structure of a sentence, and propose two strategies to encode the\npositional relationships among words in the dependency tree. Experimental\nresults on NIST Chinese-to-English and WMT14 English-to-German translation\ntasks show that the proposed approach consistently boosts performance over both\nthe absolute and relative sequential position representations.", "published": "2019-09-01 11:34:32", "link": "http://arxiv.org/abs/1909.00383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You Shall Know a User by the Company It Keeps: Dynamic Representations\n  for Social Media Users in NLP", "abstract": "Information about individuals can help to better understand what they say,\nparticularly in social media where texts are short. Current approaches to\nmodelling social media users pay attention to their social connections, but\nexploit this information in a static way, treating all connections uniformly.\nThis ignores the fact, well known in sociolinguistics, that an individual may\nbe part of several communities which are not equally relevant in all\ncommunicative situations. We present a model based on Graph Attention Networks\nthat captures this observation. It dynamically explores the social graph of a\nuser, computes a user representation given the most relevant connections for a\ntarget task, and combines it with linguistic information to make a prediction.\nWe apply our model to three different tasks, evaluate it against alternative\nmodels, and analyse the results extensively, showing that it significantly\noutperforms other current methods.", "published": "2019-09-01 14:48:04", "link": "http://arxiv.org/abs/1909.00412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What You See is What You Get: Visual Pronoun Coreference Resolution in\n  Dialogues", "abstract": "Grounding a pronoun to a visual object it refers to requires complex\nreasoning from various information sources, especially in conversational\nscenarios. For example, when people in a conversation talk about something all\nspeakers can see, they often directly use pronouns (e.g., it) to refer to it\nwithout previous introduction. This fact brings a huge challenge for modern\nnatural language understanding systems, particularly conventional context-based\npronoun coreference models. To tackle this challenge, in this paper, we\nformally define the task of visual-aware pronoun coreference resolution (PCR)\nand introduce VisPro, a large-scale dialogue PCR dataset, to investigate\nwhether and how the visual information can help resolve pronouns in dialogues.\nWe then propose a novel visual-aware PCR model, VisCoref, for this task and\nconduct comprehensive experiments and case studies on our dataset. Results\ndemonstrate the importance of the visual information in this PCR case and show\nthe effectiveness of the proposed model.", "published": "2019-09-01 15:47:03", "link": "http://arxiv.org/abs/1909.00421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Improved Neural Baseline for Temporal Relation Extraction", "abstract": "Determining temporal relations (e.g., before or after) between events has\nbeen a challenging natural language understanding task, partly due to the\ndifficulty to generate large amounts of high-quality training data.\nConsequently, neural approaches have not been widely used on it, or showed only\nmoderate improvements. This paper proposes a new neural system that achieves\nabout 10% absolute improvement in accuracy over the previous best system (25%\nerror reduction) on two benchmark datasets. The proposed system is trained on\nthe state-of-the-art MATRES dataset and applies contextualized word embeddings,\na Siamese encoder of a temporal common sense knowledge base, and global\ninference via integer linear programming (ILP). We suggest that the new\napproach could serve as a strong baseline for future research in this area.", "published": "2019-09-01 16:47:57", "link": "http://arxiv.org/abs/1909.00429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Cross-Lingual Effectiveness of Massively Multilingual\n  Neural Machine Translation", "abstract": "The recently proposed massively multilingual neural machine translation (NMT)\nsystem has been shown to be capable of translating over 100 languages to and\nfrom English within a single model. Its improved translation performance on low\nresource languages hints at potential cross-lingual transfer capability for\ndownstream tasks. In this paper, we evaluate the cross-lingual effectiveness of\nrepresentations from the encoder of a massively multilingual NMT model on 5\ndownstream classification and sequence labeling tasks covering a diverse set of\nover 50 languages. We compare against a strong baseline, multilingual BERT\n(mBERT), in different cross-lingual transfer learning scenarios and show gains\nin zero-shot transfer in 4 out of these 5 tasks.", "published": "2019-09-01 17:32:21", "link": "http://arxiv.org/abs/1909.00437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Discriminative Neural Model for Cross-Lingual Word Alignment", "abstract": "We introduce a novel discriminative word alignment model, which we integrate\ninto a Transformer-based machine translation model. In experiments based on a\nsmall number of labeled examples (~1.7K-5K sentences) we evaluate its\nperformance intrinsically on both English-Chinese and English-Arabic alignment,\nwhere we achieve major improvements over unsupervised baselines (11-27 F1). We\nevaluate the model extrinsically on data projection for Chinese NER, showing\nthat our alignments lead to higher performance when used to project NER tags\nfrom English to Chinese. Finally, we perform an ablation analysis and an\nannotation experiment that jointly support the utility and feasibility of\nfuture manual alignment elicitation.", "published": "2019-09-01 18:37:32", "link": "http://arxiv.org/abs/1909.00444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Repurposing Decoder-Transformer Language Models for Abstractive\n  Summarization", "abstract": "Neural network models have shown excellent fluency and performance when\napplied to abstractive summarization. Many approaches to neural abstractive\nsummarization involve the introduction of significant inductive bias,\nexemplified through the use of components such as pointer-generator\narchitectures, coverage, and partially extractive procedures, designed to mimic\nthe process by which humans summarize documents. We show that it is possible to\nattain competitive performance by instead directly viewing summarization as a\nlanguage modeling problem and effectively leveraging transfer learning. We\nintroduce a simple procedure built upon decoder-transformers to obtain highly\ncompetitive ROUGE scores for summarization performance using a language\nmodeling loss alone, with no beam-search or other decoding-time optimization,\nand instead relying on efficient nucleus sampling and greedy decoding.", "published": "2019-09-01 05:26:30", "link": "http://arxiv.org/abs/1909.00325v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Neural Machine Translation with Word Importance", "abstract": "Although neural machine translation (NMT) has advanced the state-of-the-art\non various language pairs, the interpretability of NMT remains unsatisfactory.\nIn this work, we propose to address this gap by focusing on understanding the\ninput-output behavior of NMT models. Specifically, we measure the word\nimportance by attributing the NMT output to every input word through a\ngradient-based method. We validate the approach on a couple of perturbation\noperations, language pairs, and model architectures, demonstrating its\nsuperiority on identifying input words with higher influence on translation\nperformance. Encouragingly, the calculated importance can serve as indicators\nof input words that are under-translated by NMT models. Furthermore, our\nanalysis reveals that words of certain syntactic categories have higher\nimportance while the categories vary across language pairs, which can inspire\nbetter design principles of NMT architectures for multi-lingual translation.", "published": "2019-09-01 06:04:48", "link": "http://arxiv.org/abs/1909.00326v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Monitoring stance towards vaccination in Twitter messages", "abstract": "We developed a system to automatically classify stance towards vaccination in\nTwitter messages, with a focus on messages with a negative stance. Such a\nsystem makes it possible to monitor the ongoing stream of messages on social\nmedia, offering actionable insights into public hesitance with respect to\nvaccination. For Dutch Twitter messages that mention vaccination-related key\nterms, we annotated their stance and feeling in relation to vaccination\n(provided that they referred to this topic). Subsequently, we used these coded\ndata to train and test different machine learning set-ups. With the aim to best\nidentify messages with a negative stance towards vaccination, we compared\nset-ups at an increasing dataset size and decreasing reliability, at an\nincreasing number of categories to distinguish, and with different\nclassification algorithms. We found that Support Vector Machines trained on a\ncombination of strictly and laxly labeled data with a more fine-grained\nlabeling yielded the best result, at an F1-score of 0.36 and an Area under the\nROC curve of 0.66, outperforming a rule-based sentiment analysis baseline that\nyielded an F1-score of 0.25 and an Area under the ROC curve of 0.57. The\noutcomes of our study indicate that stance prediction by a computerized system\nonly is a challenging task. Our analysis of the data and behavior of our system\nsuggests that an approach is needed in which the use of a larger training\ndataset is combined with a setting in which a human-in-the-loop provides the\nsystem with feedback on its predictions.", "published": "2019-09-01 07:00:26", "link": "http://arxiv.org/abs/1909.00338v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Global Entity Disambiguation with BERT", "abstract": "We propose a global entity disambiguation (ED) model based on BERT. To\ncapture global contextual information for ED, our model treats not only words\nbut also entities as input tokens, and solves the task by sequentially\nresolving mentions to their referent entities and using resolved entities as\ninputs at each step. We train the model using a large entity-annotated corpus\nobtained from Wikipedia. We achieve new state-of-the-art results on five\nstandard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The\nsource code and model checkpoint are available at\nhttps://github.com/studio-ousia/luke.", "published": "2019-09-01 16:29:53", "link": "http://arxiv.org/abs/1909.00426v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QuASE: Question-Answer Driven Sentence Encoding", "abstract": "Question-answering (QA) data often encodes essential information in many\nfacets. This paper studies a natural question: Can we get supervision from QA\ndata for other tasks (typically, non-QA ones)? For example, {\\em can we use\nQAMR (Michael et al., 2017) to improve named entity recognition?} We suggest\nthat simply further pre-training BERT is often not the best option, and propose\nthe {\\em question-answer driven sentence encoding (QuASE)} framework. QuASE\nlearns representations from QA data, using BERT or other state-of-the-art\ncontextual language models. In particular, we observe the need to distinguish\nbetween two types of sentence encodings, depending on whether the target task\nis a single- or multi-sentence input; in both cases, the resulting encoding is\nshown to be an easy-to-use plugin for many downstream tasks. This work may\npoint out an alternative way to supervise NLP tasks.", "published": "2019-09-01 06:30:57", "link": "http://arxiv.org/abs/1909.00333v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Unified Neural Coherence Model", "abstract": "Recently, neural approaches to coherence modeling have achieved\nstate-of-the-art results in several evaluation tasks. However, we show that\nmost of these models often fail on harder tasks with more realistic application\nscenarios. In particular, the existing models underperform on tasks that\nrequire the model to be sensitive to local contexts such as candidate ranking\nin conversational dialogue and in machine translation. In this paper, we\npropose a unified coherence model that incorporates sentence grammar,\ninter-sentence coherence relations, and global coherence patterns into a common\nneural framework. With extensive experiments on local and global discrimination\ntasks, we demonstrate that our proposed model outperforms existing models by a\ngood margin, and establish a new state-of-the-art.", "published": "2019-09-01 08:16:53", "link": "http://arxiv.org/abs/1909.00349v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Machine Reading Comprehension", "abstract": "Though the community has made great progress on Machine Reading Comprehension\n(MRC) task, most of the previous works are solving English-based MRC problems,\nand there are few efforts on other languages mainly due to the lack of\nlarge-scale training data. In this paper, we propose Cross-Lingual Machine\nReading Comprehension (CLMRC) task for the languages other than English.\nFirstly, we present several back-translation approaches for CLMRC task, which\nis straightforward to adopt. However, to accurately align the answer into\nanother language is difficult and could introduce additional noise. In this\ncontext, we propose a novel model called Dual BERT, which takes advantage of\nthe large-scale training data provided by rich-resource language (such as\nEnglish) and learn the semantic relations between the passage and question in a\nbilingual context, and then utilize the learned knowledge to improve reading\ncomprehension performance of low-resource language. We conduct experiments on\ntwo Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The\nresults show consistent and significant improvements over various\nstate-of-the-art systems by a large margin, which demonstrate the potentials in\nCLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC", "published": "2019-09-01 09:14:52", "link": "http://arxiv.org/abs/1909.00361v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Dataset of General-Purpose Rebuttal", "abstract": "In Natural Language Understanding, the task of response generation is usually\nfocused on responses to short texts, such as tweets or a turn in a dialog. Here\nwe present a novel task of producing a critical response to a long\nargumentative text, and suggest a method based on general rebuttal arguments to\naddress it. We do this in the context of the recently-suggested task of\nlistening comprehension over argumentative content: given a speech on some\nspecified topic, and a list of relevant arguments, the goal is to determine\nwhich of the arguments appear in the speech. The general rebuttals we describe\nhere (written in English) overcome the need for topic-specific arguments to be\nprovided, by proving to be applicable for a large set of topics. This allows\ncreating responses beyond the scope of topics for which specific arguments are\navailable. All data collected during this work is freely available for\nresearch.", "published": "2019-09-01 13:24:35", "link": "http://arxiv.org/abs/1909.00393v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through\n  Weakly Supervised Co-Training", "abstract": "User-generated reviews can be decomposed into fine-grained segments (e.g.,\nsentences, clauses), each evaluating a different aspect of the principal entity\n(e.g., price, quality, appearance). Automatically detecting these aspects can\nbe useful for both users and downstream opinion mining applications. Current\nsupervised approaches for learning aspect classifiers require many fine-grained\naspect labels, which are labor-intensive to obtain. And, unfortunately,\nunsupervised topic models often fail to capture the aspects of interest. In\nthis work, we consider weakly supervised approaches for training aspect\nclassifiers that only require the user to provide a small set of seed words\n(i.e., weakly positive indicators) for the aspects of interest. First, we show\nthat current weakly supervised approaches do not effectively leverage the\npredictive power of seed words for aspect detection. Next, we propose a\nstudent-teacher approach that effectively leverages seed words in a\nbag-of-words classifier (teacher); in turn, we use the teacher to train a\nsecond model (student) that is potentially more powerful (e.g., a neural\nnetwork that uses pre-trained word embeddings). Finally, we show that iterative\nco-training can be used to cope with noisy seed words, leading to both improved\nteacher and student models. Our proposed approach consistently outperforms\nprevious weakly supervised approaches (by 14.1 absolute F1 points on average)\nin six different domains of product reviews and six multilingual datasets of\nrestaurant reviews.", "published": "2019-09-01 15:12:23", "link": "http://arxiv.org/abs/1909.00415v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transfer Learning Between Related Tasks Using Expected Label Proportions", "abstract": "Deep learning systems thrive on abundance of labeled training data but such\ndata is not always available, calling for alternative methods of supervision.\nOne such method is expectation regularization (XR) (Mann and McCallum, 2007),\nwhere models are trained based on expected label proportions. We propose a\nnovel application of the XR framework for transfer learning between related\ntasks, where knowing the labels of task A provides an estimation of the label\nproportion of task B. We then use a model trained for A to label a large\ncorpus, and use this corpus with an XR loss to train a model for task B. To\nmake the XR framework applicable to large-scale deep-learning setups, we\npropose a stochastic batched approximation procedure. We demonstrate the\napproach on the task of Aspect-based Sentiment classification, where we\neffectively use a sentence-level sentiment predictor to train accurate\naspect-based predictor. The method improves upon fully supervised neural system\ntrained on aspect-level data, and is also cumulative with LM-based pretraining,\nas we demonstrate by improving a BERT-based Aspect-based Sentiment model.", "published": "2019-09-01 17:11:35", "link": "http://arxiv.org/abs/1909.00430v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Topics to Avoid: Demoting Latent Confounds in Text Classification", "abstract": "Despite impressive performance on many text classification tasks, deep neural\nnetworks tend to learn frequent superficial patterns that are specific to the\ntraining data and do not always generalize well. In this work, we observe this\nlimitation with respect to the task of native language identification. We find\nthat standard text classifiers which perform well on the test set end up\nlearning topical features which are confounds of the prediction task (e.g., if\nthe input text mentions Sweden, the classifier predicts that the author's\nnative language is Swedish). We propose a method that represents the latent\ntopical confounds and a model which \"unlearns\" confounding features by\npredicting both the label of the input text and the confound; but we train the\ntwo predictors adversarially in an alternating fashion to learn a text\nrepresentation that predicts the correct label but is less prone to using\ninformation about the confound. We show that this model generalizes better and\nlearns features that are indicative of the writing style rather than the\ncontent.", "published": "2019-09-01 19:18:44", "link": "http://arxiv.org/abs/1909.00453v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "abstract": "A significant barrier to progress in data-driven approaches to building\ndialog systems is the lack of high quality, goal-oriented conversational data.\nTo help satisfy this elementary requirement, we introduce the initial release\nof the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising\nsix domains. Two procedures were used to create this collection, each with\nunique advantages. The first involves a two-person, spoken \"Wizard of Oz\" (WOz)\napproach in which trained agents and crowdsourced workers interact to complete\nthe task while the second is \"self-dialog\" in which crowdsourced workers write\nthe entire dialog themselves. We do not restrict the workers to detailed\nscripts or to a small knowledge base and hence we observe that our dataset\ncontains more realistic and diverse conversations in comparison to existing\ndatasets. We offer several baseline models including state of the art neural\nseq2seq architectures with benchmark performance as well as qualitative human\nevaluations. Dialogs are labeled with API calls and arguments, a simple and\ncost effective approach which avoids the requirement of complex annotation\nschema. The layer of abstraction between the dialog model and the service\nprovider API allows for a given model to interact with multiple services that\nprovide similar functionally. Finally, the dataset will evoke interest in\nwritten vs. spoken language, discourse patterns, error handling and other\nlinguistic phenomena related to dialog system research, development and design.", "published": "2019-09-01 22:18:39", "link": "http://arxiv.org/abs/1909.05358v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
