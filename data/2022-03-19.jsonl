{"title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and\n  Logical Reasoning", "abstract": "Charts are very popular for analyzing data. When exploring charts, people\noften ask a variety of complex reasoning questions that involve several logical\nand arithmetic operations. They also commonly refer to visual features of a\nchart in their questions. However, most existing datasets do not focus on such\ncomplex reasoning questions as their questions are template-based and answers\ncome from a fixed-vocabulary. In this work, we present a large-scale benchmark\ncovering 9.6K human-written questions as well as 23.1K questions generated from\nhuman-written chart summaries. To address the unique challenges in our\nbenchmark involving visual and logical reasoning over charts, we present two\ntransformer-based models that combine visual features and the data table of the\nchart in a unified way to answer questions. While our models achieve the\nstate-of-the-art results on the previous datasets as well as on our benchmark,\nthe evaluation also reveals several challenges in answering complex reasoning\nquestions.", "published": "2022-03-19 05:00:30", "link": "http://arxiv.org/abs/2203.10244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue\n  Comprehension", "abstract": "Comprehending a dialogue requires a model to capture diverse kinds of key\ninformation in the utterances, which are either scattered around or implicitly\nimplied in different turns of conversations. Therefore, dialogue comprehension\nrequires diverse capabilities such as paraphrasing, summarizing, and\ncommonsense reasoning. Towards the objective of pre-training a zero-shot\ndialogue comprehension model, we develop a novel narrative-guided pre-training\nstrategy that learns by narrating the key information from a dialogue input.\nHowever, the dialogue-narrative parallel corpus for such a pre-training\nstrategy is currently unavailable. For this reason, we first construct a\ndialogue-narrative parallel corpus by automatically aligning movie subtitles\nand their synopses. We then pre-train a BART model on the data and evaluate its\nperformance on four dialogue-based tasks that require comprehension.\nExperimental results show that our model not only achieves superior zero-shot\nperformance but also exhibits stronger fine-grained dialogue comprehension\ncapabilities. The data and code are available at\nhttps://github.com/zhaochaocs/Diana", "published": "2022-03-19 05:20:25", "link": "http://arxiv.org/abs/2203.10249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering\n  for Zero-Shot Cross-Lingual Transfer and Generation", "abstract": "Recently, the NLP community has witnessed a rapid advancement in multilingual\nand cross-lingual transfer research where the supervision is transferred from\nhigh-resource languages (HRLs) to low-resource languages (LRLs). However, the\ncross-lingual transfer is not uniform across languages, particularly in the\nzero-shot setting. Towards this goal, one promising research direction is to\nlearn shareable structures across multiple tasks with limited annotated data.\nThe downstream multilingual applications may benefit from such a learning setup\nas most of the languages across the globe are low-resource and share some\nstructures with other languages. In this paper, we propose a novel\nmeta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures\nfrom typologically diverse languages based on meta-learning and language\nclustering. This is a step towards uniform cross-lingual transfer for unseen\nlanguages. We first cluster the languages based on language representations and\nidentify the centroid language of each cluster. Then, a meta-learning algorithm\nis trained with all centroid languages and evaluated on the other languages in\nthe zero-shot setting. We demonstrate the effectiveness of this modeling on two\nNLG tasks (Abstractive Text Summarization and Question Generation), 5 popular\ndatasets and 30 typologically diverse languages. Consistent improvements over\nstrong baselines demonstrate the efficacy of the proposed framework. The\ncareful design of the model makes this end-to-end NLG setup less vulnerable to\nthe accidental translation problem, which is a prominent concern in zero-shot\ncross-lingual NLG tasks.", "published": "2022-03-19 05:22:07", "link": "http://arxiv.org/abs/2203.10250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Read Top News First: A Document Reordering Approach for Multi-Document\n  News Summarization", "abstract": "A common method for extractive multi-document news summarization is to\nre-formulate it as a single-document summarization problem by concatenating all\ndocuments as a single meta-document. However, this method neglects the relative\nimportance of documents. We propose a simple approach to reorder the documents\naccording to their relative importance before concatenating and summarizing\nthem. The reordering makes the salient content easier to learn by the\nsummarization model. Experiments show that our approach outperforms previous\nstate-of-the-art methods with more complex architectures.", "published": "2022-03-19 06:01:11", "link": "http://arxiv.org/abs/2203.10254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency-based Mixture Language Models", "abstract": "Various models have been proposed to incorporate knowledge of syntactic\nstructures into neural language models. However, previous works have relied\nheavily on elaborate components for a specific language model, usually\nrecurrent neural network (RNN), which makes themselves unwieldy in practice to\nfit into other neural language models, such as Transformer and GPT-2. In this\npaper, we introduce the Dependency-based Mixture Language Models. In detail, we\nfirst train neural language models with a novel dependency modeling objective\nto learn the probability distribution of future dependent tokens given context.\nWe then formulate the next-token probability by mixing the previous dependency\nmodeling probability distributions with self-attention. Extensive experiments\nand human evaluations show that our method can be easily and effectively\napplied to different neural language models while improving neural text\ngeneration on various tasks.", "published": "2022-03-19 06:28:30", "link": "http://arxiv.org/abs/2203.10256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clickbait Spoiling via Question Answering and Passage Retrieval", "abstract": "We introduce and study the task of clickbait spoiling: generating a short\ntext that satisfies the curiosity induced by a clickbait post. Clickbait links\nto a web page and advertises its contents by arousing curiosity instead of\nproviding an informative summary. Our contributions are approaches to classify\nthe type of spoiler needed (i.e., a phrase or a passage), and to generate\nappropriate spoilers. A large-scale evaluation and error analysis on a new\ncorpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait\nSpoiling Corpus 2022 -- shows that our spoiler type classifier achieves an\naccuracy of 80%, while the question answering model DeBERTa-large outperforms\nall others in generating spoilers for both types.", "published": "2022-03-19 09:40:33", "link": "http://arxiv.org/abs/2203.10282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From meaning to perception -- exploring the space between word and odor\n  perception embeddings", "abstract": "In this paper we propose the use of the Word2vec algorithm in order to obtain\nodor perception embeddings (or smell embeddings), only using publicly available\nperfume descriptions. Besides showing meaningful similarity relationships among\neach other, these embeddings also demonstrate to possess some shared\ninformation with their respective word embeddings. The meaningfulness of these\nembeddings suggests that aesthetics might provide enough constraints for using\nalgorithms motivated by distributional semantics on non-randomly combined data.\nFurthermore, they provide possibilities for new ways of classifying odors and\nanalyzing perfumes. We have also employed the embeddings in an attempt to\nunderstand the aesthetic nature of perfumes, based on the difference between\nreal and randomly generated perfumes. In an additional tentative experiment we\nexplore the possibility of a mapping between the word embedding space and the\nodor perception embedding space by fitting a regressor on the shared vocabulary\nand then predict the odor perception embeddings of words without an a priori\nassociated smell, such as night or sky.", "published": "2022-03-19 10:44:20", "link": "http://arxiv.org/abs/2203.10294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Pre-trained Language Models and Hand-crafted Features for\n  Unsupervised POS Tagging", "abstract": "In recent years, large-scale pre-trained language models (PLMs) have made\nextraordinary progress in most NLP tasks. But, in the unsupervised POS tagging\ntask, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA)\nperformance. The recent SOTA performance is yielded by a Guassian HMM variant\nproposed by He et al. (2018). However, as a generative model, HMM makes very\nstrong independence assumptions, making it very challenging to incorporate\ncontexualized word representations from PLMs. In this work, we for the first\ntime propose a neural conditional random field autoencoder (CRF-AE) model for\nunsupervised POS tagging. The discriminative encoder of CRF-AE can\nstraightforwardly incorporate ELMo word representations. Moreover, inspired by\nfeature-rich HMM, we reintroduce hand-crafted features into the decoder of\nCRF-AE. Finally, experiments clearly show that our model outperforms previous\nstate-of-the-art models by a large margin on Penn Treebank and multilingual\nUniversal Dependencies treebank v2.0.", "published": "2022-03-19 12:33:38", "link": "http://arxiv.org/abs/2203.10315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretraining with Artificial Language: Studying Transferable Knowledge in\n  Language Models", "abstract": "We investigate what kind of structural knowledge learned in neural network\nencoders is transferable to processing natural language. We design artificial\nlanguages with structural properties that mimic natural language, pretrain\nencoders on the data, and see how much performance the encoder exhibits on\ndownstream tasks in natural language. Our experimental results show that\npretraining with an artificial language with a nesting dependency structure\nprovides some knowledge transferable to natural language. A follow-up probing\nanalysis indicates that its success in the transfer is related to the amount of\nencoded contextual information and what is transferred is the knowledge of\nposition-aware context dependence of language. Our results provide insights\ninto how neural network encoders process human languages and the source of\ncross-lingual transferability of recent multilingual language models.", "published": "2022-03-19 13:29:48", "link": "http://arxiv.org/abs/2203.10326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuReader_retrieval: A Large-scale Chinese Benchmark for Passage\n  Retrieval from Web Search Engine", "abstract": "In this paper, we present DuReader_retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader_retrieval contains more than 90K queries and\nover 8M unique passages from a commercial search engine. To alleviate the\nshortcomings of other datasets and ensure the quality of our benchmark, we (1)\nreduce the false negatives in development and test sets by manually annotating\nresults pooled from multiple retrievers, and (2) remove the training queries\nthat are semantically similar to the development and testing queries.\nAdditionally, we provide two out-of-domain testing sets for cross-domain\nevaluation, as well as a set of human translated queries for for cross-lingual\nretrieval evaluation. The experiments demonstrate that DuReader_retrieval is\nchallenging and a number of problems remain unsolved, such as the salient\nphrase mismatch and the syntactic mismatch between queries and paragraphs.\nThese experiments also show that dense retrievers do not generalize well across\ndomains, and cross-lingual retrieval is essentially challenging.\nDuReader_retrieval is publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.", "published": "2022-03-19 03:24:53", "link": "http://arxiv.org/abs/2203.10232v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Similarity and Content-based Phonetic Self Attention for Speech\n  Recognition", "abstract": "Transformer-based speech recognition models have achieved great success due\nto the self-attention (SA) mechanism that utilizes every frame in the feature\nextraction process. Especially, SA heads in lower layers capture various\nphonetic characteristics by the query-key dot product, which is designed to\ncompute the pairwise relationship between frames. In this paper, we propose a\nvariant of SA to extract more representative phonetic features. The proposed\nphonetic self-attention (phSA) is composed of two different types of phonetic\nattention; one is similarity-based and the other is content-based. In short,\nsimilarity-based attention captures the correlation between frames while\ncontent-based attention only considers each frame without being affected by\nother frames. We identify which parts of the original dot product equation are\nrelated to two different attention patterns and improve each part with simple\nmodifications. Our experiments on phoneme classification and speech recognition\nshow that replacing SA with phSA for lower layers improves the recognition\nperformance without increasing the latency and the parameter size.", "published": "2022-03-19 05:35:26", "link": "http://arxiv.org/abs/2203.10252v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Phrase-Level Universal Visual\n  Representations", "abstract": "Multimodal machine translation (MMT) aims to improve neural machine\ntranslation (NMT) with additional visual information, but most existing MMT\nmethods require paired input of source sentence and image, which makes them\nsuffer from shortage of sentence-image pairs. In this paper, we propose a\nphrase-level retrieval-based method for MMT to get visual information for the\nsource input from existing sentence-image data sets so that MMT can break the\nlimitation of paired sentence-image input. Our method performs retrieval at the\nphrase level and hence learns visual information from pairs of source phrase\nand grounded region, which can mitigate data sparsity. Furthermore, our method\nemploys the conditional variational auto-encoder to learn visual\nrepresentations which can filter redundant visual information and only retain\nvisual information related to the phrase. Experiments show that the proposed\nmethod significantly outperforms strong baselines on multiple MMT datasets,\nespecially when the textual context is limited.", "published": "2022-03-19 11:21:13", "link": "http://arxiv.org/abs/2203.10299v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Reason Deductively: Math Word Problem Solving as Complex\n  Relation Extraction", "abstract": "Solving math word problems requires deductive reasoning over the quantities\nin the text. Various recent research efforts mostly relied on\nsequence-to-sequence or sequence-to-tree models to generate mathematical\nexpressions without explicitly performing relational reasoning between\nquantities in the given context. While empirically effective, such approaches\ntypically do not provide explanations for the generated expressions. In this\nwork, we view the task as a complex relation extraction problem, proposing a\nnovel approach that presents explainable deductive reasoning steps to\niteratively construct target expressions, where each step involves a primitive\noperation over two quantities defining their relation. Through extensive\nexperiments on four benchmark datasets, we show that the proposed model\nsignificantly outperforms existing strong baselines. We further demonstrate\nthat the deductive procedure not only presents more explainable steps but also\nenables us to make more accurate predictions on questions that require more\ncomplex reasoning.", "published": "2022-03-19 12:37:16", "link": "http://arxiv.org/abs/2203.10316v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Knowledge Graph Completion and Question Answering", "abstract": "Knowledge graph embedding (KGE) models represent each entity and relation of\na knowledge graph (KG) with low-dimensional embedding vectors. These methods\nhave recently been applied to KG link prediction and question answering over\nincomplete KGs (KGQA). KGEs typically create an embedding for each entity in\nthe graph, which results in large model sizes on real-world graphs with\nmillions of entities. For downstream tasks these atomic entity representations\noften need to be integrated into a multi stage pipeline, limiting their\nutility. We show that an off-the-shelf encoder-decoder Transformer model can\nserve as a scalable and versatile KGE model obtaining state-of-the-art results\nfor KG link prediction and incomplete KG question answering. We achieve this by\nposing KG link prediction as a sequence-to-sequence task and exchange the\ntriple scoring approach taken by prior KGE methods with autoregressive\ndecoding. Such a simple but powerful method reduces the model size up to 98%\ncompared to conventional KGE models while keeping inference time tractable.\nAfter finetuning this model on the task of KGQA over incomplete KGs, our\napproach outperforms baselines on multiple large-scale datasets without\nextensive hyperparameter tuning.", "published": "2022-03-19 13:01:49", "link": "http://arxiv.org/abs/2203.10321v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding COVID-19 News Coverage using Medical NLP", "abstract": "Being a global pandemic, the COVID-19 outbreak received global media\nattention. In this study, we analyze news publications from CNN and The\nGuardian - two of the world's most influential media organizations. The dataset\nincludes more than 36,000 articles, analyzed using the clinical and biomedical\nNatural Language Processing (NLP) models from the Spark NLP for Healthcare\nlibrary, which enables a deeper analysis of medical concepts than previously\nachieved. The analysis covers key entities and phrases, observed biases, and\nchange over time in news coverage by correlating mined medical symptoms,\nprocedures, drugs, and guidance with commonly mentioned demographic and\noccupational groups. Another analysis is of extracted Adverse Drug Events about\ndrug and vaccine manufacturers, which when reported by major news outlets has\nan impact on vaccine hesitancy.", "published": "2022-03-19 15:07:46", "link": "http://arxiv.org/abs/2203.10338v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge", "abstract": "In this work, we focus on the problem of distinguishing a human written news\narticle from a news article that is created by manipulating entities in a human\nwritten news article (e.g., replacing entities with factually incorrect\nentities). Such manipulated articles can mislead the reader by posing as a\nhuman written news article. We propose a neural network based detector that\ndetects manipulated news articles by reasoning about the facts mentioned in the\narticle. Our proposed detector exploits factual knowledge via graph\nconvolutional neural network along with the textual information in the news\narticle. We also create challenging datasets for this task by considering\nvarious strategies to generate the new replacement entity (e.g., entity\ngeneration from GPT-2). In all the settings, our proposed model either matches\nor outperforms the state-of-the-art detector in terms of accuracy. Our code and\ndata are available at https://github.com/UBC-NLP/manipulated_entity_detection.", "published": "2022-03-19 15:35:59", "link": "http://arxiv.org/abs/2203.10343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Representative Keywords Selection: A Probabilistic Approach", "abstract": "We propose a probabilistic approach to select a subset of a \\textit{target\ndomain representative keywords} from a candidate set, contrasting with a\ncontext domain. Such a task is crucial for many downstream tasks in natural\nlanguage processing. To contrast the target domain and the context domain, we\nadapt the \\textit{two-component mixture model} concept to generate a\ndistribution of candidate keywords. It provides more importance to the\n\\textit{distinctive} keywords of the target domain than common keywords\ncontrasting with the context domain. To support the \\textit{representativeness}\nof the selected keywords towards the target domain, we introduce an\n\\textit{optimization algorithm} for selecting the subset from the generated\ncandidate distribution. We have shown that the optimization algorithm can be\nefficiently implemented with a near-optimal approximation guarantee. Finally,\nextensive experiments on multiple domains demonstrate the superiority of our\napproach over other baselines for the tasks of keyword summary generation and\ntrending keywords selection.", "published": "2022-03-19 18:04:12", "link": "http://arxiv.org/abs/2203.10365v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Online Behaviour of the Algerian Abusers in Social Media Networks", "abstract": "Connecting to social media networks becomes a daily task for the majority of\npeople around the world, and the amount of shared information is growing\nexponentially. Thus, controlling the way in which people communicate is\nnecessary, in order to protect them from disorientation, conflicts,\naggressions, etc. In this paper, we conduct a statistical study on the\ncyber-bullying and the abusive content in social media (i.e. Facebook), where\nwe try to spot the online behaviour of the abusers in the Algerian community.\nMore specifically, we have involved 200 Facebook users from different regions\namong 600 to carry out this study. The aim of this investigation is to aid\nautomatic systems of abuse detection to take decision by incorporating the\nonline activity. Abuse detection systems require a large amount of data to\nperform better on such kind of texts (i.e. unstructured and informal texts),\nand this is due to the lack of standard orthography, where there are various\nAlgerian dialects and languages spoken.", "published": "2022-03-19 18:22:06", "link": "http://arxiv.org/abs/2203.10369v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language", "abstract": "Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in natural language.\nRecent works show that such models can also produce the reasoning steps (i.e.,\nthe proof graph) that emulate the model's logical reasoning process. Currently,\nthese black-box models generate both the proof graph and intermediate\ninferences within the same model and thus may be unfaithful. In this work, we\nframe the deductive logical reasoning task by defining three modular\ncomponents: rule selection, fact selection, and knowledge composition. The rule\nand fact selection steps select the candidate rule and facts to be used and\nthen the knowledge composition combines them to generate new inferences. This\nensures model faithfulness by assured causal relation from the proof step to\nthe inference reasoning. To test our framework, we propose FaiRR (Faithful and\nRobust Reasoner) where the above three components are independently modeled by\ntransformers. We observe that FaiRR is robust to novel language perturbations,\nand is faster at inference than previous works on existing reasoning datasets.\nAdditionally, in contrast to black-box generative models, the errors made by\nFaiRR are more interpretable due to the modular approach.", "published": "2022-03-19 07:18:13", "link": "http://arxiv.org/abs/2203.10261v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-channel CNN to classify nepali covid-19 related tweets using\n  hybrid features", "abstract": "Because of the current COVID-19 pandemic with its increasing fears among\npeople, it has triggered several health complications such as depression and\nanxiety. Such complications have not only affected the developed countries but\nalso developing countries such as Nepal. These complications can be understood\nfrom peoples' tweets/comments posted online after their proper analysis and\nsentiment classification. Nevertheless, owing to the limited number of\ntokens/words in each tweet, it is always crucial to capture multiple\ninformation associated with them for their better understanding. In this study,\nwe, first, represent each tweet by combining both syntactic and semantic\ninformation, called hybrid features. The syntactic information is generated\nfrom the bag of words method, whereas the semantic information is generated\nfrom the combination of the fastText-based (ft) and domain-specific (ds)\nmethods. Second, we design a novel multi-channel convolutional neural network\n(MCNN), which ensembles the multiple CNNs, to capture multi-scale information\nfor better classification. Last, we evaluate the efficacy of both the proposed\nfeature extraction method and the MCNN model classifying tweets into three\nsentiment classes (positive, neutral and negative) on NepCOV19Tweets dataset,\nwhich is the only public COVID-19 tweets dataset in Nepali language. The\nevaluation results show that the proposed hybrid features outperform individual\nfeature extraction methods with the highest classification accuracy of 69.7%\nand the MCNN model outperforms the existing methods with the highest\nclassification accuracy of 71.3% during classification.", "published": "2022-03-19 09:55:05", "link": "http://arxiv.org/abs/2203.10286v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Perturbations in the Wild: Leveraging Human-Written Text Perturbations\n  for Realistic Adversarial Attack and Defense", "abstract": "We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K\nhuman-written text perturbations in the wild and leverages them for realistic\nadversarial attack. Unlike existing character-based attacks which often\ndeductively hypothesize a set of manipulation strategies, our work is grounded\non actual observations from real-world texts. We find that adversarial texts\ngenerated by ANTHRO achieve the best trade-off between (1) attack success rate,\n(2) semantic preservation of the original text, and (3) stealthiness--i.e.\nindistinguishable from human writings hence harder to be flagged as suspicious.\nSpecifically, our attacks accomplished around 83% and 91% attack success rates\non BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger\nbaseline with an increase of 50% and 40% in terms of semantic preservation and\nstealthiness when evaluated by both layperson and professional human workers.\nANTHRO can further enhance a BERT classifier's performance in understanding\ndifferent variations of human-written toxic texts via adversarial training when\ncompared to the Perspective API.", "published": "2022-03-19 16:00:01", "link": "http://arxiv.org/abs/2203.10346v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "On Robust Prefix-Tuning for Text Classification", "abstract": "Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.", "published": "2022-03-19 18:52:47", "link": "http://arxiv.org/abs/2203.10378v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Distinguishing Non-natural from Natural Adversarial Samples for More\n  Robust Pre-trained Language Model", "abstract": "Recently, the problem of robustness of pre-trained language models (PrLMs)\nhas received increasing research interest. Latest studies on adversarial\nattacks achieve high attack success rates against PrLMs, claiming that PrLMs\nare not robust. However, we find that the adversarial samples that PrLMs fail\nare mostly non-natural and do not appear in reality. We question the validity\nof current evaluation of robustness of PrLMs based on these non-natural\nadversarial samples and propose an anomaly detector to evaluate the robustness\nof PrLMs with more natural adversarial samples. We also investigate two\napplications of the anomaly detector: (1) In data augmentation, we employ the\nanomaly detector to force generating augmented data that are distinguished as\nnon-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply\nthe anomaly detector to a defense framework to enhance the robustness of PrLMs.\nIt can be used to defend all types of attacks and achieves higher accuracy on\nboth adversarial samples and compliant samples than other defense frameworks.", "published": "2022-03-19 14:06:46", "link": "http://arxiv.org/abs/2203.11199v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Radiology Text Analysis System (RadText): Architecture and Evaluation", "abstract": "Analyzing radiology reports is a time-consuming and error-prone task, which\nraises the need for an efficient automated radiology report analysis system to\nalleviate the workloads of radiologists and encourage precise diagnosis. In\nthis work, we present RadText, an open-source radiology text analysis system\ndeveloped by Python. RadText offers an easy-to-use text analysis pipeline,\nincluding de-identification, section segmentation, sentence split and word\ntokenization, named entity recognition, parsing, and negation detection.\nRadText features a flexible modular design, provides a hybrid text processing\nschema, and supports raw text processing and local processing, which enables\nbetter usability and improved data privacy. RadText adopts BioC as the unified\ninterface, and also standardizes the input / output into a structured\nrepresentation compatible with Observational Medical Outcomes Partnership\n(OMOP) Common Data Model (CDM). This allows for a more systematic approach to\nobservational research across multiple, disparate data sources. We evaluated\nRadText on the MIMIC-CXR dataset, with five new disease labels we annotated for\nthis work. RadText demonstrates highly accurate classification performances,\nwith an average precision of, a recall of 0.94, and an F-1 score of 0.92. We\nhave made our code, documentation, examples, and the test set available at\nhttps://github.com/bionlplab/radtext .", "published": "2022-03-19 17:16:12", "link": "http://arxiv.org/abs/2204.09599v1", "categories": ["cs.CL", "cs.AI", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Analyzing speaker verification embedding extractors and back-ends under\n  language and channel mismatch", "abstract": "In this paper, we analyze the behavior and performance of speaker embeddings\nand the back-end scoring model under domain and language mismatch. We present\nour findings regarding ResNet-based speaker embedding architectures and show\nthat reduced temporal stride yields improved performance. We then consider a\nPLDA back-end and show how a combination of small speaker subspace,\nlanguage-dependent PLDA mixture, and nuisance-attribute projection can have a\ndrastic impact on the performance of the system. Besides, we present an\nefficient way of scoring and fusing class posterior logit vectors recently\nshown to perform well for speaker verification task. The experiments are\nperformed using the NIST SRE 2021 setup.", "published": "2022-03-19 11:21:46", "link": "http://arxiv.org/abs/2203.10300v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Track-Wise Ensemble Event Independent Network for Polyphonic Sound\n  Event Localization and Detection", "abstract": "Polyphonic sound event localization and detection (SELD) aims at detecting\ntypes of sound events with corresponding temporal activities and spatial\nlocations. In this paper, a track-wise ensemble event independent network with\na novel data augmentation method is proposed. The proposed model is based on\nour previous proposed Event-Independent Network V2 and is extended by conformer\nblocks and dense blocks. The track-wise ensemble model with track-wise output\nformat is proposed to solve an ensemble model problem for track-wise output\nformat that track permutation may occur among different models. The data\naugmentation approach contains several data augmentation chains, which are\ncomposed of random combinations of several data augmentation operations. The\nmethod also utilizes log-mel spectrograms, intensity vectors, and Spatial\nCues-Augmented Log-Spectrogram (SALSA) for different models. We evaluate our\nproposed method in the Task of the L3DAS22 challenge and obtain the top ranking\nsolution with a location-dependent F-score to be 0.699. Source code is\nreleased.", "published": "2022-03-19 03:00:41", "link": "http://arxiv.org/abs/2203.10228v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Cross Domain Acoustic-to-articulatory Inverted Features For\n  Disordered Speech Recognition", "abstract": "Articulatory features are inherently invariant to acoustic signal distortion\nand have been successfully incorporated into automatic speech recognition (ASR)\nsystems for normal speech. Their practical application to disordered speech\nrecognition is often limited by the difficulty in collecting such specialist\ndata from impaired speakers. This paper presents a cross-domain\nacoustic-to-articulatory (A2A) inversion approach that utilizes the parallel\nacoustic-articulatory data of the 15-hour TORGO corpus in model training before\nbeing cross-domain adapted to the 102.7-hour UASpeech corpus and to produce\narticulatory features. Mixture density networks based neural A2A inversion\nmodels were used. A cross-domain feature adaptation network was also used to\nreduce the acoustic mismatch between the TORGO and UASpeech data. On both\ntasks, incorporating the A2A generated articulatory features consistently\noutperformed the baseline hybrid DNN/TDNN, CTC and Conformer based end-to-end\nsystems constructed using acoustic features only. The best multi-modal system\nincorporating video modality and the cross-domain articulatory features as well\nas data augmentation and learning hidden unit contributions (LHUC) speaker\nadaptation produced the lowest published word error rate (WER) of 24.82% on the\n16 dysarthric speakers of the benchmark UASpeech task.", "published": "2022-03-19 08:47:18", "link": "http://arxiv.org/abs/2203.10274v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
