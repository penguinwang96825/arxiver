{"title": "Continuous Decomposition of Granularity for Neural Paraphrase Generation", "abstract": "While Transformers have had significant success in paragraph generation, they\ntreat sentences as linear sequences of tokens and often neglect their\nhierarchical information. Prior work has shown that decomposing the levels of\ngranularity~(e.g., word, phrase, or sentence) for input tokens has produced\nsubstantial improvements, suggesting the possibility of enhancing Transformers\nvia more fine-grained modeling of granularity. In this work, we propose a\ncontinuous decomposition of granularity for neural paraphrase generation\n(C-DNPG). In order to efficiently incorporate granularity into sentence\nencoding, C-DNPG introduces a granularity-aware attention (GA-Attention)\nmechanism which extends the multi-head self-attention with: 1) a granularity\nhead that automatically infers the hierarchical structure of a sentence by\nneurally estimating the granularity level of each input token; and 2) two novel\nattention masks, namely, granularity resonance and granularity scope, to\nefficiently encode granularity into attention. Experiments on two benchmarks,\nincluding Quora question pairs and Twitter URLs have shown that C-DNPG\noutperforms baseline models by a remarkable margin and achieves\nstate-of-the-art results in terms of many metrics. Qualitative analysis reveals\nthat C-DNPG indeed captures fine-grained levels of granularity with\neffectiveness.", "published": "2022-09-05 05:02:42", "link": "http://arxiv.org/abs/2209.01765v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-focused Extractive Summarisation for Biomedical and COVID-19\n  Complex Question Answering", "abstract": "This paper presents Macquarie University's participation to the two most\nrecent BioASQ Synergy Tasks (as per June 2022), and to the BioASQ10 Task~B\n(BioASQ10b), Phase~B. In these tasks, participating systems are expected to\ngenerate complex answers to biomedical questions, where the answers may contain\nmore than one sentence. We apply query-focused extractive summarisation\ntechniques. In particular, we follow a sentence classification-based approach\nthat scores each candidate sentence associated to a question, and the $n$\nhighest-scoring sentences are returned as the answer. The Synergy Task\ncorresponds to an end-to-end system that requires document selection, snippet\nselection, and finding the final answer, but it has very limited training data.\nFor the Synergy task, we selected the candidate sentences following two phases:\ndocument retrieval and snippet retrieval, and the final answer was found by\nusing a DistilBERT/ALBERT classifier that had been trained on the training data\nof BioASQ9b. Document retrieval was achieved as a standard search over the\nCORD-19 data using the search API provided by the BioASQ organisers, and\nsnippet retrieval was achieved by re-ranking the sentences of the top retrieved\ndocuments, using the cosine similarity of the question and candidate sentence.\nWe observed that vectors represented via sBERT have an edge over tf.idf.\nBioASQ10b Phase B focuses on finding the specific answers to biomedical\nquestions. For this task, we followed a data-centric approach. We hypothesised\nthat the training data of the first BioASQ years might be biased and we\nexperimented with different subsets of the training data. We observed an\nimprovement of results when the system was trained on the second half of the\nBioASQ10b training data.", "published": "2022-09-05 07:56:44", "link": "http://arxiv.org/abs/2209.01815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine\n  Reading Comprehension", "abstract": "The issue of shortcut learning is widely known in NLP and has been an\nimportant research focus in recent years. Unintended correlations in the data\nenable models to easily solve tasks that were meant to exhibit advanced\nlanguage understanding and reasoning capabilities. In this survey paper, we\nfocus on the field of machine reading comprehension (MRC), an important task\nfor showcasing high-level language understanding that also suffers from a range\nof shortcuts. We summarize the available techniques for measuring and\nmitigating shortcuts and conclude with suggestions for further progress in\nshortcut research. Importantly, we highlight two concerns for shortcut\nmitigation in MRC: (1) the lack of public challenge sets, a necessary component\nfor effective and reusable evaluation, and (2) the lack of certain mitigation\ntechniques that are prominent in other areas.", "published": "2022-09-05 08:19:26", "link": "http://arxiv.org/abs/2209.01824v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Figurative Language Generation", "abstract": "Figurative language generation is the task of reformulating a given text in\nthe desired figure of speech while still being faithful to the original\ncontext. We take the first step towards multi-figurative language modelling by\nproviding a benchmark for the automatic generation of five common figurative\nforms in English. We train mFLAG employing a scheme for multi-figurative\nlanguage pre-training on top of BART, and a mechanism for injecting the target\nfigurative information into the encoder; this enables the generation of text\nwith the target figurative form from another figurative form without parallel\nfigurative-figurative sentence pairs. Our approach outperforms all strong\nbaselines. We also offer some qualitative analysis and reflections on the\nrelationship between the different figures of speech.", "published": "2022-09-05 08:48:09", "link": "http://arxiv.org/abs/2209.01835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Annotation Makes Language Models Better Few-Shot Learners", "abstract": "Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%\nrelative gain under an annotation budget of 18/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https://github.com/HKUNLP/icl-selective-annotation.", "published": "2022-09-05 14:01:15", "link": "http://arxiv.org/abs/2209.01975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Incremental Event Detection", "abstract": "Event detection tasks can enable the quick detection of events from texts and\nprovide powerful support for downstream natural language processing tasks. Most\nsuch methods can only detect a fixed set of predefined event classes. To extend\nthem to detect a new class without losing the ability to detect old classes\nrequires costly retraining of the model from scratch. Incremental learning can\neffectively solve this problem, but it requires abundant data of new classes.\nIn practice, however, the lack of high-quality labeled data of new event\nclasses makes it difficult to obtain enough data for model training. To address\nthe above mentioned issues, we define a new task, few-shot incremental event\ndetection, which focuses on learning to detect a new event class with limited\ndata, while retaining the ability to detect old classes to the extent possible.\nWe created a benchmark dataset IFSED for the few-shot incremental event\ndetection task based on FewEvent and propose two benchmarks, IFSED-K and\nIFSED-KP. Experimental results show that our approach has a higher F1-score\nthan baseline methods and is more stable.", "published": "2022-09-05 14:21:26", "link": "http://arxiv.org/abs/2209.01979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Dummy Grandpa, do you know anything?\": Identifying and Characterizing\n  Ad hominem Fallacy Usage in the Wild", "abstract": "Today, participating in discussions on online forums is extremely commonplace\nand these discussions have started rendering a strong influence on the overall\nopinion of online users. Naturally, twisting the flow of the argument can have\na strong impact on the minds of naive users, which in the long run might have\nsocio-political ramifications, for example, winning an election or spreading\ntargeted misinformation. Thus, these platforms are potentially highly\nvulnerable to malicious players who might act individually or as a cohort to\nbreed fallacious arguments with a motive to sway public opinion. Ad hominem\narguments are one of the most effective forms of such fallacies. Although a\nsimple fallacy, it is effective enough to sway public debates in offline world\nand can be used as a precursor to shutting down the voice of opposition by\nslander.\n  In this work, we take a first step in shedding light on the usage of ad\nhominem fallacies in the wild. First, we build a powerful ad hominem detector\nwith high accuracy (F1 more than 83%, showing a significant improvement over\nprior work), even for datasets for which annotated instances constitute a very\nsmall fraction. We then used our detector on 265k arguments collected from the\nonline debate forum - CreateDebate. Our crowdsourced surveys validate our\nin-the-wild predictions on CreateDebate data (94% match with manual\nannotation). Our analysis revealed that a surprising 31.23% of CreateDebate\ncontent contains ad hominem fallacy, and a cohort of highly active users post\nsignificantly more ad hominem to suppress opposing views. Then, our temporal\nanalysis revealed that ad hominem argument usage increased significantly since\nthe 2016 US Presidential election, not only for topics like Politics, but also\nfor Science and Law. We conclude by discussing important implications of our\nwork to detect and defend against ad hominem fallacies.", "published": "2022-09-05 17:16:44", "link": "http://arxiv.org/abs/2209.02062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual\n  Retrieval", "abstract": "Fact-checking has gained increasing attention due to the widespread of\nfalsified information. Most fact-checking approaches focus on claims made in\nEnglish only due to the data scarcity issue in other languages. The lack of\nfact-checking datasets in low-resource languages calls for an effective\ncross-lingual transfer technique for fact-checking. Additionally, trustworthy\ninformation in different languages can be complementary and helpful in\nverifying facts. To this end, we present the first fact-checking framework\naugmented with cross-lingual retrieval that aggregates evidence retrieved from\nmultiple languages through a cross-lingual retriever. Given the absence of\ncross-lingual information retrieval datasets with claim-like queries, we train\nthe retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a\nself-supervised algorithm that creates training instances by translating the\ntitle of a passage. The goal for X-ICT is to learn cross-lingual retrieval in\nwhich the model learns to identify the passage corresponding to a given\ntranslated title. On the X-Fact dataset, our approach achieves 2.23% absolute\nF1 improvement in the zero-shot cross-lingual setup over prior systems. The\nsource code and data are publicly available at\nhttps://github.com/khuangaf/CONCRETE.", "published": "2022-09-05 17:36:14", "link": "http://arxiv.org/abs/2209.02071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Susceptibility of Pre-Trained Language Models via\n  Handcrafted Adversarial Examples", "abstract": "Recent advances in the development of large language models have resulted in\npublic access to state-of-the-art pre-trained language models (PLMs), including\nGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder\nRepresentations from Transformers (BERT). However, evaluations of PLMs, in\npractice, have shown their susceptibility to adversarial attacks during the\ntraining and fine-tuning stages of development. Such attacks can result in\nerroneous outputs, model-generated hate speech, and the exposure of users'\nsensitive information. While existing research has focused on adversarial\nattacks during either the training or the fine-tuning of PLMs, there is a\ndeficit of information on attacks made between these two development phases. In\nthis work, we highlight a major security vulnerability in the public release of\nGPT-3 and further investigate this vulnerability in other state-of-the-art\nPLMs. We restrict our work to pre-trained models that have not undergone\nfine-tuning. Further, we underscore token distance-minimized perturbations as\nan effective adversarial approach, bypassing both supervised and unsupervised\nquality measures. Following this approach, we observe a significant decrease in\ntext classification quality when evaluating for semantic similarity.", "published": "2022-09-05 20:29:17", "link": "http://arxiv.org/abs/2209.02128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual and Cross-Domain Crisis Classification for Low-Resource\n  Scenarios", "abstract": "Social media data has emerged as a useful source of timely information about\nreal-world crisis events. One of the main tasks related to the use of social\nmedia for disaster management is the automatic identification of crisis-related\nmessages. Most of the studies on this topic have focused on the analysis of\ndata for a particular type of event in a specific language. This limits the\npossibility of generalizing existing approaches because models cannot be\ndirectly applied to new types of events or other languages. In this work, we\nstudy the task of automatically classifying messages that are related to crisis\nevents by leveraging cross-language and cross-domain labeled data. Our goal is\nto make use of labeled data from high-resource languages to classify messages\nfrom other (low-resource) languages and/or of new (previously unseen) types of\ncrisis situations. For our study we consolidated from the literature a large\nunified dataset containing multiple crisis events and languages. Our empirical\nfindings show that it is indeed possible to leverage data from crisis events in\nEnglish to classify the same type of event in other languages, such as Spanish\nand Italian (80.0% F1-score). Furthermore, we achieve good performance for the\ncross-domain task (80.0% F1-score) in a cross-lingual setting. Overall, our\nwork contributes to improving the data scarcity problem that is so important\nfor multilingual crisis classification. In particular, mitigating cold-start\nsituations in emergency events, when time is of essence.", "published": "2022-09-05 20:57:23", "link": "http://arxiv.org/abs/2209.02139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which structure of academic articles do referees pay more attention to?:\n  perspective of peer review and full-text of academic articles", "abstract": "Purpose\n  The purpose of this paper is to explore which structures of academic articles\nreferees would pay more attention to, what specific content referees focus on,\nand whether the distribution of PRC is related to the citations.\nDesign/methodology/approach\n  Firstly, utilizing the feature words of section title and hierarchical\nattention network model (HAN) to identify the academic article structures.\nSecondly, analyzing the distribution of PRC in different structures according\nto the position information extracted by rules in PRC. Thirdly, analyzing the\ndistribution of feature words of PRC extracted by the Chi-square test and\nTF-IDF in different structures. Finally, four correlation analysis methods are\nused to analyze whether the distribution of PRC in different structures is\ncorrelated to the citations. Findings\n  The count of PRC distributed in Materials and Methods and Results section is\nsignificantly more than that in the structure of Introduction and Discussion,\nindicating that referees pay more attention to the Material and Methods and\nResults. The distribution of feature words of PRC in different structures is\nobviously different, which can reflect the content of referees' concern. There\nis no correlation between the distribution of PRC in different structures and\nthe citations. Research limitations/implications\n  Due to the differences in the way referees write peer review reports, the\nrules used to extract position information cannot cover all PRC.\nOriginality/value\n  The paper finds a pattern in the distribution of PRC in different academic\narticle structures proving the long-term empirical understanding. It also\nprovides insight into academic article writing: researchers should ensure the\nscientificity of methods and the reliability of results when writing academic\narticle to obtain a high degree of recognition from referees.", "published": "2022-09-05 08:56:43", "link": "http://arxiv.org/abs/2209.01841v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "How Much User Context Do We Need? Privacy by Design in Mental Health NLP\n  Application", "abstract": "Clinical NLP tasks such as mental health assessment from text, must take\nsocial constraints into account - the performance maximization must be\nconstrained by the utmost importance of guaranteeing privacy of user data.\nConsumer protection regulations, such as GDPR, generally handle privacy by\nrestricting data availability, such as requiring to limit user data to 'what is\nnecessary' for a given purpose. In this work, we reason that providing stricter\nformal privacy guarantees, while increasing the volume of user data in the\nmodel, in most cases increases benefit for all parties involved, especially for\nthe user. We demonstrate our arguments on two existing suicide risk assessment\ndatasets of Twitter and Reddit posts. We present the first analysis juxtaposing\nuser history length and differential privacy budgets and elaborate how modeling\nadditional user context enables utility preservation while maintaining\nacceptable user privacy guarantees.", "published": "2022-09-05 15:41:45", "link": "http://arxiv.org/abs/2209.02022v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "PromptAttack: Prompt-based Attack for Language Models via Gradient\n  Search", "abstract": "As the pre-trained language models (PLMs) continue to grow, so do the\nhardware and data requirements for fine-tuning PLMs. Therefore, the researchers\nhave come up with a lighter method called \\textit{Prompt Learning}. However,\nduring the investigations, we observe that the prompt learning methods are\nvulnerable and can easily be attacked by some illegally constructed prompts,\nresulting in classification errors, and serious security problems for PLMs.\nMost of the current research ignores the security issue of prompt-based\nmethods. Therefore, in this paper, we propose a malicious prompt template\nconstruction method (\\textbf{PromptAttack}) to probe the security performance\nof PLMs. Several unfriendly template construction approaches are investigated\nto guide the model to misclassify the task. Extensive experiments on three\ndatasets and three PLMs prove the effectiveness of our proposed approach\nPromptAttack. We also conduct experiments to verify that our method is\napplicable in few-shot scenarios.", "published": "2022-09-05 10:28:20", "link": "http://arxiv.org/abs/2209.01882v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Bridging Music and Text with Crowdsourced Music Comments: A\n  Sequence-to-Sequence Framework for Thematic Music Comments Generation", "abstract": "We consider a novel task of automatically generating text descriptions of\nmusic. Compared with other well-established text generation tasks such as image\ncaption, the scarcity of well-paired music and text datasets makes it a much\nmore challenging task. In this paper, we exploit the crowd-sourced music\ncomments to construct a new dataset and propose a sequence-to-sequence model to\ngenerate text descriptions of music. More concretely, we use the dilated\nconvolutional layer as the basic component of the encoder and a memory based\nrecurrent neural network as the decoder. To enhance the authenticity and\nthematicity of generated texts, we further propose to fine-tune the model with\na discriminator as well as a novel topic evaluator. To measure the quality of\ngenerated texts, we also propose two new evaluation metrics, which are more\naligned with human evaluation than traditional metrics such as BLEU.\nExperimental results verify that our model is capable of generating fluent and\nmeaningful comments while containing thematic and content information of the\noriginal music.", "published": "2022-09-05 14:51:51", "link": "http://arxiv.org/abs/2209.01996v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distilling the Knowledge of BERT for CTC-based ASR", "abstract": "Connectionist temporal classification (CTC) -based models are attractive\nbecause of their fast inference in automatic speech recognition (ASR). Language\nmodel (LM) integration approaches such as shallow fusion and rescoring can\nimprove the recognition accuracy of CTC-based ASR by taking advantage of the\nknowledge in text corpora. However, they significantly slow down the inference\nof CTC. In this study, we propose to distill the knowledge of BERT for\nCTC-based ASR, extending our previous study for attention-based ASR. CTC-based\nASR learns the knowledge of BERT during training and does not use BERT during\ntesting, which maintains the fast inference of CTC. Different from\nattention-based models, CTC-based models make frame-level predictions, so they\nneed to be aligned with token-level predictions of BERT for distillation. We\npropose to obtain alignments by calculating the most plausible CTC paths.\nExperimental evaluations on the Corpus of Spontaneous Japanese (CSJ) and\nTED-LIUM2 show that our method improves the performance of CTC-based ASR\nwithout the cost of inference speed.", "published": "2022-09-05 16:08:35", "link": "http://arxiv.org/abs/2209.02030v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rare but Severe Neural Machine Translation Errors Induced by Minimal\n  Deletion: An Empirical Study on Chinese and English", "abstract": "We examine the inducement of rare but severe errors in English-Chinese and\nChinese-English in-domain neural machine translation by minimal deletion of the\nsource text with character-based models. By deleting a single character, we can\ninduce severe translation errors. We categorize these errors and compare the\nresults of deleting single characters and single words. We also examine the\neffect of training data size on the number and types of pathological cases\ninduced by these minimal perturbations, finding significant variation. We find\nthat deleting a word hurts overall translation score more than deleting a\ncharacter, but certain errors are more likely to occur when deleting\ncharacters, with language direction also influencing the effect.", "published": "2022-09-05 21:14:21", "link": "http://arxiv.org/abs/2209.02145v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Movement Detection of Tongue and Related Body Parts Using IR-UWB Radar", "abstract": "Because an impulse radio ultra-wideband (IR-UWB) radar can detect targets\nwith high accuracy, work through occluding materials, and operate without\ncontact, it is an attractive hardware solution for building silent speech\ninterfaces, which are non-audio-based speech communication devices. As tongue\nmovement is strongly engaged in pronunciation, detecting its movement is\ncrucial for developing silent speech interfaces. In this study, we attempted to\nclassify the motionless and moving states of an invisible tongue and its\nrelated body parts using an IR-UWB radar whose antennas were pointed toward the\nparticipant's chin. Using the proposed feature extraction algorithm and a\nGaussian mixture model - hidden Markov model, we classified two states of the\ninvisible tongue of four individual participants with a minimum accuracy of\n90%.", "published": "2022-09-05 04:51:20", "link": "http://arxiv.org/abs/2209.01762v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion", "abstract": "Disentangling speaker and content attributes of a speech signal into separate\nlatent representations followed by decoding the content with an exchanged\nspeaker representation is a popular approach for voice conversion, which can be\ntrained with non-parallel and unlabeled speech data. However, previous\napproaches perform disentanglement only implicitly via some sort of information\nbottleneck or normalization, where it is usually hard to find a good trade-off\nbetween voice conversion and content reconstruction. Further, previous works\nusually do not consider an adaptation of the speaking rate to the target\nspeaker or they put some major restrictions to the data or use case. Therefore,\nthe contribution of this work is two-fold. First, we employ an explicit and\nfully unsupervised disentanglement approach, which has previously only been\nused for representation learning, and show that it allows to obtain both\nsuperior voice conversion and content reconstruction. Second, we investigate\nsimple and generic approaches to linearly adapt the length of a speech signal,\nand hence the speaking rate, to a target speaker and show that the proposed\nadaptation allows to increase the speaking rate similarity with respect to the\ntarget speaker.", "published": "2022-09-05 14:20:42", "link": "http://arxiv.org/abs/2209.01978v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploiting Pre-trained Feature Networks for Generative Adversarial\n  Networks in Audio-domain Loop Generation", "abstract": "While generative adversarial networks (GANs) have been widely used in\nresearch on audio generation, the training of a GAN model is known to be\nunstable, time consuming, and data inefficient. Among the attempts to\nameliorate the training process of GANs, the idea of Projected GAN emerges as\nan effective solution for GAN-based image generation, establishing the\nstate-of-the-art in different image applications. The core idea is to use a\npre-trained classifier to constrain the feature space of the discriminator to\nstabilize and improve GAN training. This paper investigates whether Projected\nGAN can similarly improve audio generation, by evaluating the performance of a\nStyleGAN2-based audio-domain loop generation model with and without using a\npre-trained feature space in the discriminator. Moreover, we compare the\nperformance of using a general versus domain-specific classifier as the\npre-trained audio classifier. With experiments on both drum loop and synth loop\ngeneration, we show that a general audio classifier works better, and that with\nProjected GAN our loop generation models can converge around 5 times faster\nwithout performance degradation.", "published": "2022-09-05 04:06:05", "link": "http://arxiv.org/abs/2209.01751v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Event Localization and Detection for Real Spatial Sound Scenes:\n  Event-Independent Network and Data Augmentation Chains", "abstract": "Sound event localization and detection (SELD) is a joint task of sound event\ndetection and direction-of-arrival estimation. In DCASE 2022 Task 3, types of\ndata transform from computationally generated spatial recordings to recordings\nof real-sound scenes. Our system submitted to the DCASE 2022 Task 3 is based on\nour previous proposed Event-Independent Network V2 (EINV2) with a novel data\naugmentation method. Our method employs EINV2 with a track-wise output format,\npermutation-invariant training, and a soft parameter-sharing strategy, to\ndetect different sound events of the same class but in different locations. The\nConformer structure is used for extending EINV2 to learn local and global\nfeatures. A data augmentation method, which contains several data augmentation\nchains composed of stochastic combinations of several different data\naugmentation operations, is utilized to generalize the model. To mitigate the\nlack of real-scene recordings in the development dataset and the presence of\nsound events being unbalanced, we exploit FSD50K, AudioSet, and TAU Spatial\nRoom Impulse Response Database (TAU-SRIR DB) to generate simulated datasets for\ntraining. We present results on the validation set of Sony-TAu Realistic\nSpatial Soundscapes 2022 (STARSS22) in detail. Experimental results indicate\nthat the ability to generalize to different environments and unbalanced\nperformance among different classes are two main challenges. We evaluate our\nproposed method in Task 3 of the DCASE 2022 challenge and obtain the second\nrank in the teams ranking. Source code is released.", "published": "2022-09-05 07:30:06", "link": "http://arxiv.org/abs/2209.01802v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predict-and-Update Network: Audio-Visual Speech Recognition Inspired by\n  Human Speech Perception", "abstract": "Audio and visual signals complement each other in human speech perception, so\ndo they in speech recognition. The visual hint is less evident than the\nacoustic hint, but more robust in a complex acoustic environment, as far as\nspeech perception is concerned. It remains a challenge how we effectively\nexploit the interaction between audio and visual signals for automatic speech\nrecognition. There have been studies to exploit visual signals as redundant or\ncomplementary information to audio input in a synchronous manner. Human studies\nsuggest that visual signal primes the listener in advance as to when and on\nwhich frequency to attend to. We propose a Predict-and-Update Network (P&U\nnet), to simulate such a visual cueing mechanism for Audio-Visual Speech\nRecognition (AVSR). In particular, we first predict the character posteriors of\nthe spoken words, i.e. the visual embedding, based on the visual signals. The\naudio signal is then conditioned on the visual embedding via a novel\ncross-modal Conformer, that updates the character posteriors. We validate the\neffectiveness of the visual cueing mechanism through extensive experiments. The\nproposed P&U net outperforms the state-of-the-art AVSR methods on both LRS2-BBC\nand LRS3-BBC datasets, with the relative reduced Word Error Rate (WER)s\nexceeding 10% and 40% under clean and noisy conditions, respectively.", "published": "2022-09-05 05:19:09", "link": "http://arxiv.org/abs/2209.01768v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Instrument Separation of Symbolic Music by Explicitly Guided Diffusion\n  Model", "abstract": "Similar to colorization in computer vision, instrument separation is to\nassign instrument labels (e.g. piano, guitar...) to notes from unlabeled\nmixtures which contain only performance information. To address the problem, we\nadopt diffusion models and explicitly guide them to preserve consistency\nbetween mixtures and music. The quantitative results show that our proposed\nmodel can generate high-fidelity samples for multitrack symbolic music with\ncreativity.", "published": "2022-09-05 04:50:38", "link": "http://arxiv.org/abs/2209.02696v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
