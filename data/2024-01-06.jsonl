{"title": "Examining Forgetting in Continual Pre-training of Aligned Large Language\n  Models", "abstract": "Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.", "published": "2024-01-06 05:34:09", "link": "http://arxiv.org/abs/2401.03129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Joint-Reasoning based Disease Q&A System", "abstract": "Medical question answer (QA) assistants respond to lay users' health-related\nqueries by synthesizing information from multiple sources using natural\nlanguage processing and related techniques. They can serve as vital tools to\nalleviate issues of misinformation, information overload, and complexity of\nmedical language, thus addressing lay users' information needs while reducing\nthe burden on healthcare professionals. QA systems, the engines of such\nassistants, have typically used either language models (LMs) or knowledge\ngraphs (KG), though the approaches could be complementary. LM-based QA systems\nexcel at understanding complex questions and providing well-formed answers, but\nare prone to factual mistakes. KG-based QA systems, which represent facts well,\nare mostly limited to answering short-answer questions with pre-created\ntemplates. While a few studies have jointly used LM and KG approaches for\ntext-based QA, this was done to answer multiple-choice questions. Extant QA\nsystems also have limitations in terms of automation and performance. We\naddress these challenges by designing a novel, automated disease QA system\nwhich effectively utilizes both LM and KG techniques through a joint-reasoning\napproach to answer disease-related questions appropriate for lay users. Our\nevaluation of the system using a range of quality metrics demonstrates its\nefficacy over benchmark systems, including the popular ChatGPT.", "published": "2024-01-06 09:55:22", "link": "http://arxiv.org/abs/2401.03181v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Defeasibility in Causal Reasoning", "abstract": "Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.", "published": "2024-01-06 10:08:33", "link": "http://arxiv.org/abs/2401.03183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models", "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.", "published": "2024-01-06 12:40:45", "link": "http://arxiv.org/abs/2401.03205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIXAR: Auto-Regressive Language Modeling in Pixel Space", "abstract": "Recent work showed the possibility of building open-vocabulary large language\nmodels (LLMs) that directly operate on pixel representations. These models are\nimplemented as autoencoders that reconstruct masked patches of rendered text.\nHowever, these pixel-based LLMs are limited to discriminative tasks (e.g.,\nclassification) and, similar to BERT, cannot be used to generate text.\nTherefore, they cannot be used for generative tasks such as free-form question\nanswering. In this work, we introduce PIXAR, the first pixel-based\nautoregressive LLM that performs text generation. Consisting of only a decoder,\nPIXAR can perform free-form generative tasks while keeping the number of\nparameters on par with previous encoder-decoder models. Furthermore, we\nhighlight the challenges of generating text as non-noisy images and show this\nis due to using a maximum likelihood objective. To overcome this problem, we\npropose an adversarial pretraining stage that improves the readability and\naccuracy of PIXAR by 8.1 on LAMBADA and 8.5 on bAbI -- making it comparable to\nGPT-2 on text generation tasks. This paves the way to build open-vocabulary\nLLMs that operate on perceptual input only and calls into question the\nnecessity of the usual symbolic input representation, i.e., text as\n(sub)tokens.", "published": "2024-01-06 22:49:38", "link": "http://arxiv.org/abs/2401.03321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoT-Driven Framework for Short Text Classification: Enhancing and\n  Transferring Capabilities from Large to Smaller Model", "abstract": "Short Text Classification (STC) is crucial for processing and understanding\nthe brief but substantial content prevalent on contemporary digital platforms.\nThe STC encounters difficulties in grasping the semantic and syntactic\nintricacies, an issue that is apparent in traditional pre-trained language\nmodels. Although Graph Convolutional Networks enhance performance by\nintegrating external knowledge bases, these methods are limited by the quality\nand extent of the knowledge applied. Recently, the emergence of Large Language\nModels (LLMs) and Chain-of-Thought (CoT) has significantly improved the\nperformance of complex reasoning tasks. However, some studies have highlighted\nthe limitations of their application in fundamental NLP tasks. Consequently,\nthis study first employs CoT to investigate and enhance the capabilities of\nLLMs in STC tasks. We propose the Syntactic and Semantic Enrichment CoT\n(SSE-CoT) method, effectively decomposing the STC tasks into four distinct\nsteps: (i) essential concept identification, (ii) common-sense knowledge\nretrieval, (iii) text rewriting, and (iv) classification. Furthermore,\nrecognizing resource constraints in sectors like finance and healthcare, we\nthen introduce the CoT-Driven Multi-Task Learning (CDMT) framework to extend\nthese capabilities to smaller models. This framework begins by extracting\nrationales from LLMs and subsequently fine-tunes smaller models to optimize\ntheir performance. Extensive experimentation across six short-text benchmarks\nvalidated the efficacy of the proposed methods. In particular, SSE-CoT achieved\nstate-of-the-art performance with substantial improvements on all datasets,\nparticularly on the Ohsumed and TagMyNews datasets.", "published": "2024-01-06 08:28:20", "link": "http://arxiv.org/abs/2401.03158v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks", "abstract": "Text-video retrieval is a challenging task that aims to identify relevant\nvideos given textual queries. Compared to conventional textual retrieval, the\nmain obstacle for text-video retrieval is the semantic gap between the textual\nnature of queries and the visual richness of video content. Previous works\nprimarily focus on aligning the query and the video by finely aggregating\nword-frame matching signals. Inspired by the human cognitive process of\nmodularly judging the relevance between text and video, the judgment needs\nhigh-order matching signal due to the consecutive and complex nature of video\ncontents. In this paper, we propose chunk-level text-video matching, where the\nquery chunks are extracted to describe a specific retrieval unit, and the video\nchunks are segmented into distinct clips from videos. We formulate the\nchunk-level matching as n-ary correlations modeling between words of the query\nand frames of the video and introduce a multi-modal hypergraph for n-ary\ncorrelation modeling. By representing textual units and video frames as nodes\nand using hyperedges to depict their relationships, a multi-modal hypergraph is\nconstructed. In this way, the query and the video can be aligned in a\nhigh-order semantic space. In addition, to enhance the model's generalization\nability, the extracted features are fed into a variational inference component\nfor computation, obtaining the variational representation under the Gaussian\ndistribution. The incorporation of hypergraphs and variational inference allows\nour model to capture complex, n-ary interactions among textual and visual\ncontents. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance on the text-video retrieval task.", "published": "2024-01-06 09:38:55", "link": "http://arxiv.org/abs/2401.03177v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reflections on Inductive Thematic Saturation as a potential metric for\n  measuring the validity of an inductive Thematic Analysis with LLMs", "abstract": "This paper presents a set of reflections on saturation and the use of Large\nLanguage Models (LLMs) for performing Thematic Analysis (TA). The paper\nsuggests that initial thematic saturation (ITS) could be used as a metric to\nassess part of the transactional validity of TA with LLM, focusing on the\ninitial coding. The paper presents the initial coding of two datasets of\ndifferent sizes, and it reflects on how the LLM reaches some form of analytical\nsaturation during the coding. The procedure proposed in this work leads to the\ncreation of two codebooks, one comprising the total cumulative initial codes\nand the other the total unique codes. The paper proposes a metric to\nsynthetically measure ITS using a simple mathematical calculation employing the\nratio between slopes of cumulative codes and unique codes. The paper\ncontributes to the initial body of work exploring how to perform qualitative\nanalysis with LLMs.", "published": "2024-01-06 15:34:38", "link": "http://arxiv.org/abs/2401.03239v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Enhancing Essay Scoring with Adversarial Weights Perturbation and\n  Metric-specific AttentionPooling", "abstract": "The objective of this study is to improve automated feedback tools designed\nfor English Language Learners (ELLs) through the utilization of data science\ntechniques encompassing machine learning, natural language processing, and\neducational data analytics. Automated essay scoring (AES) research has made\nstrides in evaluating written essays, but it often overlooks the specific needs\nof English Language Learners (ELLs) in language development. This study\nexplores the application of BERT-related techniques to enhance the assessment\nof ELLs' writing proficiency within AES.\n  To address the specific needs of ELLs, we propose the use of DeBERTa, a\nstate-of-the-art neural language model, for improving automated feedback tools.\nDeBERTa, pretrained on large text corpora using self-supervised learning,\nlearns universal language representations adaptable to various natural language\nunderstanding tasks. The model incorporates several innovative techniques,\nincluding adversarial training through Adversarial Weights Perturbation (AWP)\nand Metric-specific AttentionPooling (6 kinds of AP) for each label in the\ncompetition.\n  The primary focus of this research is to investigate the impact of\nhyperparameters, particularly the adversarial learning rate, on the performance\nof the model. By fine-tuning the hyperparameter tuning process, including the\ninfluence of 6AP and AWP, the resulting models can provide more accurate\nevaluations of language proficiency and support tailored learning tasks for\nELLs. This work has the potential to significantly benefit ELLs by improving\ntheir English language proficiency and facilitating their educational journey.", "published": "2024-01-06 06:05:12", "link": "http://arxiv.org/abs/2401.05433v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity Matching for Patent Documents Using Ensemble\n  BERT-related Model and Novel Text Processing Method", "abstract": "In the realm of patent document analysis, assessing semantic similarity\nbetween phrases presents a significant challenge, notably amplifying the\ninherent complexities of Cooperative Patent Classification (CPC) research.\nFirstly, this study addresses these challenges, recognizing early CPC work\nwhile acknowledging past struggles with language barriers and document\nintricacy. Secondly, it underscores the persisting difficulties of CPC\nresearch.\n  To overcome these challenges and bolster the CPC system, This paper presents\ntwo key innovations. Firstly, it introduces an ensemble approach that\nincorporates four BERT-related models, enhancing semantic similarity accuracy\nthrough weighted averaging. Secondly, a novel text preprocessing method\ntailored for patent documents is introduced, featuring a distinctive input\nstructure with token scoring that aids in capturing semantic relationships\nduring CPC context training, utilizing BCELoss. Our experimental findings\nconclusively establish the effectiveness of both our Ensemble Model and novel\ntext processing strategies when deployed on the U.S. Patent Phrase to Phrase\nMatching dataset.", "published": "2024-01-06 02:35:49", "link": "http://arxiv.org/abs/2401.06782v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples", "abstract": "Aligning large language models (LLMs) with human values is a vital task for\nLLM practitioners. Current alignment techniques have several limitations: (1)\nrequiring a large amount of annotated data; (2) demanding heavy human\ninvolvement; (3) lacking a systematic mechanism to continuously improve. In\nthis work, we study aligning LLMs to a new domain with limited samples (e.g. <\n100). We propose an algorithm that can self-align LLMs iteratively without\nactive human involvement. Unlike existing works, our algorithm relies on\nneither human-crafted instructions nor labeled rewards, significantly reducing\nhuman involvement. In addition, our algorithm can self-improve the alignment\ncontinuously. The key idea is to first retrieve high-quality samples related to\nthe target domain and use them as In-context Learning examples to generate more\nsamples. Then we use the self-generated samples to finetune the LLM\niteratively. We show that our method can unlock the LLMs' self-generalization\nability to perform alignment with near-zero human supervision. We test our\nalgorithm on three benchmarks in safety, truthfulness, and\ninstruction-following, and show good performance in alignment, domain\nadaptability, and scalability.", "published": "2024-01-06 14:00:12", "link": "http://arxiv.org/abs/2401.06785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Part-of-Speech Tagger for Bodo Language using Deep Learning approach", "abstract": "Language Processing systems such as Part-of-speech tagging, Named entity\nrecognition, Machine translation, Speech recognition, and Language modeling\n(LM) are well-studied in high-resource languages. Nevertheless, research on\nthese systems for several low-resource languages, including Bodo, Mizo,\nNagamese, and others, is either yet to commence or is in its nascent stages.\nLanguage model plays a vital role in the downstream tasks of modern NLP.\nExtensive studies are carried out on LMs for high-resource languages.\nNevertheless, languages such as Bodo, Rabha, and Mising continue to lack\ncoverage. In this study, we first present BodoBERT, a language model for the\nBodo language. To the best of our knowledge, this work is the first such effort\nto develop a language model for Bodo. Secondly, we present an ensemble DL-based\nPOS tagging model for Bodo. The POS tagging model is based on combinations of\nBiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We\ncover several language models in the experiment to see how well they work in\nPOS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A\ncomparative experiment was also conducted on Assamese POS taggers, considering\nthat the language is spoken in the same region as Bodo.", "published": "2024-01-06 09:37:56", "link": "http://arxiv.org/abs/2401.03175v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model\n  Editing", "abstract": "Large language models are known for encoding a vast amount of factual\nknowledge, but they often becomes outdated due to the ever-changing nature of\nexternal information. A promising solution to this challenge is the utilization\nof model editing methods to update the knowledge in an efficient manner.\nHowever, the majority of existing model editing techniques are limited to\nmonolingual frameworks, thus failing to address the crucial issue of\ncross-lingual knowledge synchronization for multilingual models. To tackle this\nproblem, we propose a simple yet effective method that trains multilingual\npatch neuron to store cross-lingual knowledge. It can be easily adapted to\nexisting approaches to enhance their cross-lingual editing capabilities. To\nevaluate our method, we conduct experiments using both the XNLI dataset and a\nself-constructed XFEVER dataset. Experimental results demonstrate that our\nproposed method achieves improved performance in cross-lingual editing tasks\nwithout requiring excessive modifications to the original methodology, thereby\nshowcasing its user-friendly characteristics. Codes will be released soon.", "published": "2024-01-06 10:40:24", "link": "http://arxiv.org/abs/2401.03190v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "VLLaVO: Mitigating Visual Gap through LLMs", "abstract": "Recent advances achieved by deep learning models rely on the independent and\nidentically distributed assumption, hindering their applications in real-world\nscenarios with domain shifts. To tackle this issue, cross-domain learning aims\nat extracting domain-invariant knowledge to reduce the domain shift between\ntraining and testing data. However, in visual cross-domain learning,\ntraditional methods concentrate solely on the image modality, disregarding the\npotential benefits of incorporating the text modality. In this work, we propose\nVLLaVO, combining Vision language models and Large Language models as Visual\ncross-dOmain learners. VLLaVO uses vision-language models to convert images\ninto detailed textual descriptions. A large language model is then finetuned on\ntextual descriptions of the source/target domain generated by a designed\ninstruction template. Extensive experimental results under domain\ngeneralization and unsupervised domain adaptation settings demonstrate the\neffectiveness of the proposed method.", "published": "2024-01-06 16:33:39", "link": "http://arxiv.org/abs/2401.03253v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Context Through Contrast", "abstract": "Neural machine translation benefits from semantically rich representations.\nConsiderable progress in learning such representations has been achieved by\nlanguage modelling and mutual information maximization objectives using\ncontrastive learning. The language-dependent nature of language modelling\nintroduces a trade-off between the universality of the learned representations\nand the model's performance on the language modelling tasks. Although\ncontrastive learning improves performance, its success cannot be attributed to\nmutual information alone. We propose a novel Context Enhancement step to\nimprove performance on neural machine translation by maximizing mutual\ninformation using the Barlow Twins loss. Unlike other approaches, we do not\nexplicitly augment the data but view languages as implicit augmentations,\neradicating the risk of disrupting semantic information. Further, our method\ndoes not learn embeddings from scratch and can be generalised to any set of\npre-trained embeddings. Finally, we evaluate the language-agnosticism of our\nembeddings through language classification and use them for neural machine\ntranslation to compare with state-of-the-art approaches.", "published": "2024-01-06 22:13:51", "link": "http://arxiv.org/abs/2401.03314v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiSiam: A Multiple Input Siamese Network For Social Media Text\n  Classification And Duplicate Text Detection", "abstract": "Social media accounts post increasingly similar content, creating a chaotic\nexperience across platforms, which makes accessing desired information\ndifficult. These posts can be organized by categorizing and grouping duplicates\nacross social handles and accounts. There can be more than one duplicate of a\npost, however, a conventional Siamese neural network only considers a pair of\ninputs for duplicate text detection. In this paper, we first propose a\nmultiple-input Siamese network, MultiSiam. This condensed network is then used\nto propose another model, SMCD (Social Media Classification and Duplication\nModel) to perform both duplicate text grouping and categorization. The\nMultiSiam network, just like the Siamese, can be used in multiple applications\nby changing the sub-network appropriately.", "published": "2024-01-06 09:13:34", "link": "http://arxiv.org/abs/2401.06783v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language\n  Models", "abstract": "Cognitive dynamics are pivotal to advance human understanding of the world.\nRecent advancements in large language models (LLMs) reveal their potential for\ncognitive simulation. However, these LLM-based cognitive studies primarily\nfocus on static modeling, overlooking the dynamic nature of cognition. To\nbridge this gap, we propose the concept of the cognitive dynamics of LLMs and\npresent a corresponding task with the inspiration of longitudinal studies.\nTowards the task, we develop CogBench, a novel benchmark to assess the\ncognitive dynamics of LLMs and validate it through participant surveys. We also\ndesign two evaluation metrics for CogBench, including Authenticity and\nRationality. Recognizing the inherent static nature of LLMs, we introduce\nCogGPT for the task, which features an innovative iterative cognitive mechanism\naimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate\nthe superiority of CogGPT over existing methods, particularly in its ability to\nfacilitate role-specific cognitive dynamics under continuous information flows.", "published": "2024-01-06 03:59:59", "link": "http://arxiv.org/abs/2401.08438v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Design framework for spherical microphone and loudspeaker arrays in a\n  multiple-input multiple-output system", "abstract": "Spherical microphone arrays (SMAs) and spherical loudspeaker arrays (SLAs)\nfacilitate the study of room acoustics due to the three-dimensional analysis\nthey provide. More recently, systems that combine both arrays, referred to as\nmultiple-input multiple-output (MIMO) systems, have been proposed due to the\nadded spatial diversity they facilitate. The literature provides frameworks for\ndesigning SMAs and SLAs separately, including error analysis from which the\noperating frequency range (OFR) of an array is defined. However, such a\nframework does not exist for the joint design of a SMA and a SLA that comprise\na MIMO system. This paper develops a design framework for MIMO systems based on\na model that addresses errors and highlights the importance of a matched\ndesign. Expanding on a free-field assumption, errors are incorporated\nseparately for each array and error bounds are defined, facilitating error\nanalysis for the system. The dependency of the error bounds on the SLA and SMA\nparameters is studied and it is recommended that parameters should be chosen to\nassure matched OFRs of the arrays in MIMO system design. A design example is\nprovided, demonstrating the superiority of a matched system over an unmatched\nsystem in the synthesis of directional room impulse responses.", "published": "2024-01-06 20:08:56", "link": "http://arxiv.org/abs/2401.03291v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in\n  End-to-End ASR", "abstract": "Confidence estimation of predictions from an End-to-End (E2E) Automatic\nSpeech Recognition (ASR) model benefits ASR's downstream and upstream tasks.\nClass-probability-based confidence scores do not accurately represent the\nquality of overconfident ASR predictions. An ancillary Confidence Estimation\nModel (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use\nbinary target scores for CEM training. However, the binary labels do not reveal\nthe granular information of predicted words, such as temporal alignment between\nreference and hypothesis and whether the predicted word is entirely incorrect\nor contains spelling errors. Addressing this issue, we propose a novel\nTemporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address\nthe data imbalance of target scores while training CEM, we use shrinkage loss\nto focus on hard-to-learn data points and minimise the impact of easily learned\ndata points. We conduct experiments with ASR models trained in three languages,\nnamely Hindi, Tamil, and Kannada, with varying training data sizes. Experiments\nshow that TeLeS generalises well across domains. To demonstrate the\napplicability of the proposed method, we formulate a TeLeS-based Acquisition\n(TeLeS-A) function for sampling uncertainty in active learning. We observe a\nsignificant reduction in the Word Error Rate (WER) as compared to SOTA methods.", "published": "2024-01-06 16:29:13", "link": "http://arxiv.org/abs/2401.03251v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Theoretical Framework for the Optimization of Microphone Array\n  Configuration for Humanoid Robot Audition", "abstract": "An important aspect of a humanoid robot is audition. Previous work has\npresented robot systems capable of sound localization and source segregation\nbased on microphone arrays with various configurations. However, no theoretical\nframework for the design of these arrays has been presented. In the current\npaper, a design framework is proposed based on a novel array quality measure.\nThe measure is based on the effective rank of a matrix composed of the\ngeneralized head related transfer functions (GHRTFs) that account for\nmicrophone positions other than the ears. The measure is shown to be\ntheoretically related to standard array performance measures such as\nbeamforming robustness and DOA estimation accuracy. Then, the measure is\napplied to produce sample designs of microphone arrays. Their performance is\ninvestigated numerically, verifying the advantages of array design based on the\nproposed theoretical framework.", "published": "2024-01-06 19:21:44", "link": "http://arxiv.org/abs/2401.03286v1", "categories": ["eess.AS", "cs.RO", "cs.SD"], "primary_category": "eess.AS"}
