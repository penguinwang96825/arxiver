{"title": "Legal Judgment Prediction via Multi-Perspective Bi-Feedback Network", "abstract": "The Legal Judgment Prediction (LJP) is to determine judgment results based on\nthe fact descriptions of the cases. LJP usually consists of multiple subtasks,\nsuch as applicable law articles prediction, charges prediction, and the term of\nthe penalty prediction. These multiple subtasks have topological dependencies,\nthe results of which affect and verify each other. However, existing methods\nuse dependencies of results among multiple subtasks inefficiently. Moreover,\nfor cases with similar descriptions but different penalties, current methods\ncannot predict accurately because the word collocation information is ignored.\nIn this paper, we propose a Multi-Perspective Bi-Feedback Network with the Word\nCollocation Attention mechanism based on the topology structure among subtasks.\nSpecifically, we design a multi-perspective forward prediction and backward\nverification framework to utilize result dependencies among multiple subtasks\neffectively. To distinguish cases with similar descriptions but different\npenalties, we integrate word collocations features of fact descriptions into\nthe network via an attention mechanism. The experimental results show our model\nachieves significant improvements over baselines on all prediction tasks.", "published": "2019-05-10 07:04:12", "link": "http://arxiv.org/abs/1905.03969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Restoring Arabic vowels through omission-tolerant dictionary lookup", "abstract": "Vowels in Arabic are optional orthographic symbols written as diacritics\nabove or below letters. In Arabic texts, typically more than 97 percent of\nwritten words do not explicitly show any of the vowels they contain; that is to\nsay, depending on the author, genre and field, less than 3 percent of words\ninclude any explicit vowel. Although numerous studies have been published on\nthe issue of restoring the omitted vowels in speech technologies, little\nattention has been given to this problem in papers dedicated to written Arabic\ntechnologies.f In this research, we present Arabic-Unitex, an Arabic Language\nResource, with emphasis on vowel representation and encoding. Specifically, we\npresent two dozens of rules formalizing a detailed description of vowel\nomission in written text. They are typographical rules integrated into\nlarge-coverage resources for morphological annotation. For restoring vowels,\nour resources are capable of identifying words in which the vowels are not\nshown, as well as words in which the vowels are partially or fully included. By\ntaking into account these rules, our resources are able to compute and restore\nfor each word form a list of compatible fully vowelized candidates through\nomission-tolerant dictionary lookup. Our program performs the analysis of 5000\nwords/second for running text (20 pages/second). Based on these comprehensive\nlinguistic resources, we created a spell checker that detects any\ninvalid/misplaced vowel in a fully or partially vowelized form. Finally, our\nresources provide a lexical coverage of more than 99 percent of the words used\nin popular newspapers, and restore vowels in words (out of context) simply and\nefficiently.", "published": "2019-05-10 10:14:41", "link": "http://arxiv.org/abs/1905.04051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MobiVSR: A Visual Speech Recognition Solution for Mobile Devices", "abstract": "Visual speech recognition (VSR) is the task of recognizing spoken language\nfrom video input only, without any audio. VSR has many applications as an\nassistive technology, especially if it could be deployed in mobile devices and\nembedded systems. The need of intensive computational resources and large\nmemory footprint are two of the major obstacles in developing neural network\nmodels for VSR in a resource constrained environment. We propose a novel\nend-to-end deep neural network architecture for word level VSR called MobiVSR\nwith a design parameter that aids in balancing the model's accuracy and\nparameter count. We use depthwise-separable 3D convolution for the first time\nin the domain of VSR and show how it makes our model efficient. MobiVSR\nachieves an accuracy of 73\\% on a challenging Lip Reading in the Wild dataset\nwith 6 times fewer parameters and 20 times lesser memory footprint than the\ncurrent state of the art. MobiVSR can also be compressed to 6 MB by applying\npost training quantization.", "published": "2019-05-10 06:58:35", "link": "http://arxiv.org/abs/1905.03968v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Language Modeling with Deep Transformers", "abstract": "We explore deep autoregressive Transformer models in language modeling for\nspeech recognition. We focus on two aspects. First, we revisit Transformer\nmodel configurations specifically for language modeling. We show that well\nconfigured Transformer models outperform our baseline models based on the\nshallow stack of LSTM recurrent neural network layers. We carry out experiments\non the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level\nand 10K byte-pair encoding subword-level language modeling. We apply our\nword-level models to conventional hybrid speech recognition by lattice\nrescoring, and the subword-level models to attention based encoder-decoder\nmodels by shallow fusion. Second, we show that deep Transformer language models\ndo not require positional encoding. The positional encoding is an essential\naugmentation for the self-attention mechanism which is invariant to sequence\nordering. However, in autoregressive setup, as is the case for language\nmodeling, the amount of information increases along the position dimension,\nwhich is a positional signal by its own. The analysis of attention weights\nshows that deep autoregressive self-attention models can automatically make use\nof such positional information. We find that removing the positional encoding\neven slightly improves the performance of these models.", "published": "2019-05-10 15:50:00", "link": "http://arxiv.org/abs/1905.04226v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Densifying Assumed-sparse Tensors: Improving Memory Efficiency and MPI\n  Collective Performance during Tensor Accumulation for Parallelized Training\n  of Neural Machine Translation Models", "abstract": "Neural machine translation - using neural networks to translate human\nlanguage - is an area of active research exploring new neuron types and network\ntopologies with the goal of dramatically improving machine translation\nperformance. Current state-of-the-art approaches, such as the multi-head\nattention-based transformer, require very large translation corpuses and many\nepochs to produce models of reasonable quality. Recent attempts to parallelize\nthe official TensorFlow \"Transformer\" model across multiple nodes have hit\nroadblocks due to excessive memory use and resulting out of memory errors when\nperforming MPI collectives. This paper describes modifications made to the\nHorovod MPI-based distributed training framework to reduce memory usage for\ntransformer models by converting assumed-sparse tensors to dense tensors, and\nsubsequently replacing sparse gradient gather with dense gradient reduction.\nThe result is a dramatic increase in scale-out capability, with CPU-only\nscaling tests achieving 91% weak scaling efficiency up to 1200 MPI processes\n(300 nodes), and up to 65% strong scaling efficiency up to 400 MPI processes\n(200 nodes) using the Stampede2 supercomputer.", "published": "2019-05-10 09:44:35", "link": "http://arxiv.org/abs/1905.04035v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Survey on Evaluation Methods for Dialogue Systems", "abstract": "In this paper we survey the methods and concepts developed for the evaluation\nof dialogue systems. Evaluation is a crucial part during the development\nprocess. Often, dialogue systems are evaluated by means of human evaluations\nand questionnaires. However, this tends to be very cost and time intensive.\nThus, much work has been put into finding methods, which allow to reduce the\ninvolvement of human labour. In this survey, we present the main concepts and\nmethods. For this, we differentiate between the various classes of dialogue\nsystems (task-oriented dialogue systems, conversational dialogue systems, and\nquestion-answering dialogue systems). We cover each class by introducing the\nmain technologies developed for the dialogue systems and then by presenting the\nevaluation methods regarding this class.", "published": "2019-05-10 11:14:12", "link": "http://arxiv.org/abs/1905.04071v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Check-It: A Plugin for Detecting and Reducing the Spread of Fake News\n  and Misinformation on the Web", "abstract": "Over the past few years, we have been witnessing the rise of misinformation\non the Web. People fall victims of fake news during their daily lives and\nassist their further propagation knowingly and inadvertently. There have been\nmany initiatives that are trying to mitigate the damage caused by fake news,\nfocusing on signals from either domain flag-lists, online social networks or\nartificial intelligence. In this work, we present Check-It, a system that\ncombines, in an intelligent way, a variety of signals into a pipeline for fake\nnews identification. Check-It is developed as a web browser plugin with the\nobjective of efficient and timely fake news detection, respecting the user's\nprivacy. Experimental results show that Check-It is able to outperform the\nstate-of-the-art methods. On a dataset, consisting of 9 millions of articles\nlabeled as fake and real, Check-It obtains classification accuracies that\nexceed 99%.", "published": "2019-05-10 17:00:40", "link": "http://arxiv.org/abs/1905.04260v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "A logical-based corpus for cross-lingual evaluation", "abstract": "At present, different deep learning models are presenting high accuracy on\npopular inference datasets such as SNLI, MNLI, and SciTail. However, there are\ndifferent indicators that those datasets can be exploited by using some simple\nlinguistic patterns. This fact poses difficulties to our understanding of the\nactual capacity of machine learning models to solve the complex task of textual\ninference. We propose a new set of syntactic tasks focused on contradiction\ndetection that require specific capacities over linguistic logical forms such\nas: Boolean coordination, quantifiers, definite description, and counting\noperators. We evaluate two kinds of deep learning models that implicitly\nexploit language structure: recurrent models and the Transformer network BERT.\nWe show that although BERT is clearly more efficient to generalize over most\nlogical forms, there is space for improvement when dealing with counting\noperators. Since the syntactic tasks can be implemented in different languages,\nwe show a successful case of cross-lingual transfer learning between English\nand Portuguese.", "published": "2019-05-10 19:39:55", "link": "http://arxiv.org/abs/1905.05704v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A New Anchor Word Selection Method for the Separable Topic Discovery", "abstract": "Separable Non-negative Matrix Factorization (SNMF) is an important method for\ntopic modeling, where \"separable\" assumes every topic contains at least one\nanchor word, defined as a word that has non-zero probability only on that\ntopic. SNMF focuses on the word co-occurrence patterns to reveal topics by two\nsteps: anchor word selection and topic recovery. The quality of the anchor\nwords strongly influences the quality of the extracted topics. Existing anchor\nword selection algorithm is to greedily find an approximate convex hull in a\nhigh-dimensional word co-occurrence space. In this work, we propose a new\nmethod for the anchor word selection by associating the word co-occurrence\nprobability with the words similarity and assuming that the most different\nwords on semantic are potential candidates for the anchor words. Therefore, if\nthe similarity of a word-pair is very low, then the two words are very likely\nto be the anchor words. According to the statistical information of text\ncorpora, we can get the similarity of all word-pairs. We build the word\nsimilarity graph where the nodes correspond to words and weights on edges stand\nfor the word-pair similarity. Following this way, we design a greedy method to\nfind a minimum edge-weight anchor clique of a given size in the graph for the\nanchor word selection. Extensive experiments on real-world corpus demonstrate\nthe effectiveness of the proposed anchor word selection method that outperforms\nthe common convex hull-based methods on the revealed topic quality. Meanwhile,\nour method is much faster than typical SNMF based method.", "published": "2019-05-10 12:16:10", "link": "http://arxiv.org/abs/1905.06109v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Binaural LCMV Beamforming with Partial Noise Estimation", "abstract": "Besides reducing undesired sources (interfering sources and background\nnoise), another important objective of a binaural beamforming algorithm is to\npreserve the spatial impression of the acoustic scene, which can be achieved by\npreserving the binaural cues of all sound sources. While the binaural minimum\nvariance distortionless response (BMVDR) beamformer provides a good noise\nreduction performance and preserves the binaural cues of the desired source, it\ndoes not allow to control the reduction of the interfering sources and distorts\nthe binaural cues of the interfering sources and the background noise. Hence,\nseveral extensions have been proposed. First, the binaural linearly constrained\nminimum variance (BLCMV) beamformer uses additional constraints, enabling to\ncontrol the reduction of the interfering sources while preserving their\nbinaural cues. Second, the BMVDR with partial noise estimation (BMVDR-N) mixes\nthe output signals of the BMVDR with the noisy reference microphone signals,\nenabling to control the binaural cues of the background noise. Merging the\nadvantages of both extensions, in this paper we propose the BLCMV with partial\nnoise estimation (BLCMV-N). We show that the output signals of the BLCMV-N can\nbe interpreted as a mixture of the noisy reference microphone signals and the\noutput signals of a BLCMV using an adjusted interference scaling parameter. We\nprovide a theoretical comparison between the BMVDR, the BLCMV, the BMVDR-N and\nthe proposed BLCMV-N in terms of noise and interference reduction performance\nand binaural cue preservation. Experimental results using recorded signals as\nwell as the results of a perceptual listening test show that the BLCMV-N is\nable to preserve the binaural cues of an interfering source (like the BLCMV),\nwhile enabling to trade off between noise reduction performance and binaural\ncue preservation of the background noise (like the BMVDR-N).", "published": "2019-05-10 10:12:29", "link": "http://arxiv.org/abs/1905.04050v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Semi-supervised and Population Based Training for Voice Commands\n  Recognition", "abstract": "We present a rapid design methodology that combines automated hyper-parameter\ntuning with semi-supervised training to build highly accurate and robust models\nfor voice commands classification. Proposed approach allows quick evaluation of\nnetwork architectures to fit performance and power constraints of available\nhardware, while ensuring good hyper-parameter choices for each network in\nreal-world scenarios. Leveraging the vast amount of unlabeled data with a\nstudent/teacher based semi-supervised method, classification accuracy is\nimproved from 84% to 94% in the validation set. For model optimization, we\nexplore the hyper-parameter space through population based training and obtain\nan optimized model in the same time frame as it takes to train a single model.", "published": "2019-05-10 15:58:38", "link": "http://arxiv.org/abs/1905.04230v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Multiclass Language Identification using Deep Learning on Spectral\n  Images of Audio Signals", "abstract": "The first step in any voice recognition software is to determine what\nlanguage a speaker is using, and ideally this process would be automated. The\ntechnique described in this paper, language identification for audio\nspectrograms (LIFAS), uses spectrograms generated from audio signals as inputs\nto a convolutional neural network (CNN) to be used for language identification.\nLIFAS requires minimal pre-processing on the audio signals as the spectrograms\nare generated during each batch as they are input to the network during\ntraining.\n  LIFAS utilizes deep learning tools that are shown to be successful on image\nprocessing tasks and applies it to audio signal classification. LIFAS performs\nbinary language classification with an accuracy of 97\\%, and multi-class\nclassification with six languages at an accuracy of 89\\% on 3.75 second audio\nclips.", "published": "2019-05-10 19:15:59", "link": "http://arxiv.org/abs/1905.04348v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Role of non-linear data processing on speech recognition task in the\n  framework of reservoir computing", "abstract": "The reservoir computing neural network architecture is widely used to test\nhardware systems for neuromorphic computing. One of the preferred tasks for\nbench-marking such devices is automatic speech recognition. However, this task\nrequires acoustic transformations from sound waveforms with varying amplitudes\nto frequency domain maps that can be seen as feature extraction techniques.\nDepending on the conversion method, these may obscure the contribution of the\nneuromorphic hardware to the overall speech recognition performance. Here, we\nquantify and separate the contributions of the acoustic transformations and the\nneuromorphic hardware to the speech recognition success rate. We show that the\nnon-linearity in the acoustic transformation plays a critical role in feature\nextraction. We compute the gain in word success rate provided by a reservoir\ncomputing device compared to the acoustic transformation only, and show that it\nis an appropriate benchmark for comparing different hardware. Finally, we\nexperimentally and numerically quantify the impact of the different acoustic\ntransformations for neuromorphic hardware based on magnetic nano-oscillators.", "published": "2019-05-10 10:52:08", "link": "http://arxiv.org/abs/1906.02812v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
