{"title": "A Transformer-based Response Evaluator for Open-Domain Spoken\n  Conversation", "abstract": "Many open-domain dialogue systems rely on multiple response generators, any\nof which can contribute a response to the dialogue in a particular context.\nThus the ability to compare potential responses and then select the best plays\nan important role in ensuring a dialogue system is coherent and engaging.\nDialogue coherence goes beyond simply remaining on topic -- some trivia may be\non topic and engaging when mentioned out of the blue, but may not be coherent\nand grounded in the context of the conversation. We carry out experiments on\nresponse selection in the Athena system, an Alexa Prize SocialBot that has\ndedicated content and multiple topic-specific response generators for a large\nnumber of topics. First, we collect a corpus of Athena conversations with live\nhuman traffic, where potential responses from all enabled response generators\nare logged and subsequently annotated for response quality. We compare several\noff-the-shelf response ranking methods for open-domain dialogue to\nAthena-Heuristic, a heuristic response ranker that was field-tested in Athena\nduring the third Alexa Prize competition. We also compare these to a\ntransformer-based response ranker we call Athena-RR, that we train on our\nAthena conversations. Athena-RR uses both the conversational context and the\ndialogue state to rank the potential responses. We find that Athena-RR with a\nRecall@1 of 70.79\\% outperforms Athena-Heuristic and all of the off-the-shelf\nrankers by a large margin. We then conduct a live A/B study comparing\nAthena-Heuristic to Athena-RR in a 6,358 conversations with Alexa users. We\nshow that Athena-RR leads to significantly longer conversations that receive\nsignificantly higher user ratings than the heuristic rule-based ranker.", "published": "2023-02-09 03:38:07", "link": "http://arxiv.org/abs/2302.04424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Constraints with Prompting for Zero-Shot Event Argument\n  Classification", "abstract": "Determining the role of event arguments is a crucial subtask of event\nextraction. Most previous supervised models leverage costly annotations, which\nis not practical for open-domain applications. In this work, we propose to use\nglobal constraints with prompting to effectively tackles event argument\nclassification without any annotation and task-specific training. Specifically,\ngiven an event and its associated passage, the model first creates several new\npassages by prefix prompts and cloze prompts, where prefix prompts indicate\nevent type and trigger span, and cloze prompts connect each candidate role with\nthe target argument span. Then, a pre-trained language model scores the new\npassages, making the initial prediction. Our novel prompt templates can easily\nadapt to all events and argument types without manual effort. Next, the model\nregularizes the prediction by global constraints exploiting cross-task,\ncross-argument, and cross-event relations. Extensive experiments demonstrate\nour model's effectiveness: it outperforms the best zero-shot baselines by 12.5%\nand 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1,\nrespectively, without given argument spans. We have made our code publicly\navailable.", "published": "2023-02-09 06:39:29", "link": "http://arxiv.org/abs/2302.04459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Robust Character Detection in Fantasy Novels", "abstract": "Named Entity Recognition (NER) is a low-level task often used as a foundation\nfor solving higher level NLP problems. In the context of character detection in\nnovels, NER false negatives can be an issue as they possibly imply missing\ncertain characters or relationships completely. In this article, we demonstrate\nthat applying a straightforward data augmentation technique allows training a\nmodel achieving higher recall, at the cost of a certain amount of precision\nregarding ambiguous entities. We show that this decrease in precision can be\nmitigated by giving the model more local context, which resolves some of the\nambiguities.", "published": "2023-02-09 10:47:03", "link": "http://arxiv.org/abs/2302.04555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Question Answering against Distribution Shifts with Test-Time\n  Adaptation: An Empirical Study", "abstract": "A deployed question answering (QA) model can easily fail when the test data\nhas a distribution shift compared to the training data. Robustness tuning (RT)\nmethods have been widely studied to enhance model robustness against\ndistribution shifts before model deployment. However, can we improve a model\nafter deployment? To answer this question, we evaluate test-time adaptation\n(TTA) to improve a model after deployment. We first introduce COLDQA, a unified\nevaluation benchmark for robust QA against text corruption and changes in\nlanguage and domain. We then evaluate previous TTA methods on COLDQA and\ncompare them to RT methods. We also propose a novel TTA method called online\nimitation learning (OIL). Through extensive experiments, we find that TTA is\ncomparable to RT methods, and applying TTA after RT can significantly boost the\nperformance on COLDQA. Our proposed OIL improves TTA to be more robust to\nvariation in hyper-parameters and test distributions over time.", "published": "2023-02-09 13:10:53", "link": "http://arxiv.org/abs/2302.04618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Approach for Auto-Formulation of Optimization Problems", "abstract": "In the Natural Language for Optimization (NL4Opt) NeurIPS 2022 competition,\ncompetitors focus on improving the accessibility and usability of optimization\nsolvers, with the aim of subtask 1: recognizing the semantic entities that\ncorrespond to the components of the optimization problem; subtask 2: generating\nformulations for the optimization problem. In this paper, we present the\nsolution of our team. First, we treat subtask 1 as a named entity recognition\n(NER) problem with the solution pipeline including pre-processing methods,\nadversarial training, post-processing methods and ensemble learning. Besides,\nwe treat subtask 2 as a generation problem with the solution pipeline including\nspecially designed prompts, adversarial training, post-processing methods and\nensemble learning. Our proposed methods have achieved the F1-score of 0.931 in\nsubtask 1 and the accuracy of 0.867 in subtask 2, which won the fourth and\nthird places respectively in this competition. Our code is available at\nhttps://github.com/bigdata-ustc/nl4opt.", "published": "2023-02-09 13:57:06", "link": "http://arxiv.org/abs/2302.04643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Generalizability of Collaborative Dialogue Analysis with\n  Multi-Feature Embeddings", "abstract": "Conflict prediction in communication is integral to the design of virtual\nagents that support successful teamwork by providing timely assistance. The aim\nof our research is to analyze discourse to predict collaboration success.\nUnfortunately, resource scarcity is a problem that teamwork researchers\ncommonly face since it is hard to gather a large number of training examples.\nTo alleviate this problem, this paper introduces a multi-feature embedding\n(MFeEmb) that improves the generalizability of conflict prediction models\ntrained on dialogue sequences. MFeEmb leverages textual, structural, and\nsemantic information from the dialogues by incorporating lexical, dialogue\nacts, and sentiment features. The use of dialogue acts and sentiment features\nreduces performance loss from natural distribution shifts caused mainly by\nchanges in vocabulary.\n  This paper demonstrates the performance of MFeEmb on domain adaptation\nproblems in which the model is trained on discourse from one task domain and\napplied to predict team performance in a different domain. The generalizability\nof MFeEmb is quantified using the similarity measure proposed by Bontonou et\nal. (2021). Our results show that MFeEmb serves as an excellent domain-agnostic\nrepresentation for meta-pretraining a few-shot model on collaborative\nmultiparty dialogues.", "published": "2023-02-09 15:58:32", "link": "http://arxiv.org/abs/2302.04716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.", "published": "2023-02-09 16:49:57", "link": "http://arxiv.org/abs/2302.04761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Multilingual Study of Visual Constraints on Linguistic\n  Selection of Descriptions", "abstract": "We present a large, multilingual study into how vision constrains linguistic\nchoice, covering four languages and five linguistic properties, such as verb\ntransitivity or use of numerals. We propose a novel method that leverages\nexisting corpora of images with captions written by native speakers, and apply\nit to nine corpora, comprising 600k images and 3M captions. We study the\nrelation between visual input and linguistic choices by training classifiers to\npredict the probability of expressing a property from raw images, and find\nevidence supporting the claim that linguistic properties are constrained by\nvisual context across languages. We complement this investigation with a corpus\nstudy, taking the test case of numerals. Specifically, we use existing\nannotations (number or type of objects) to investigate the effect of different\nvisual conditions on the use of numeral expressions in captions, and show that\nsimilar patterns emerge across languages. Our methods and findings both confirm\nand extend existing research in the cognitive literature. We additionally\ndiscuss possible applications for language generation.", "published": "2023-02-09 17:57:58", "link": "http://arxiv.org/abs/2302.04811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation Selection Using Unlabeled Data for Chain-of-Thought\n  Prompting", "abstract": "Recent work has shown how to prompt large language models with explanations\nto obtain strong performance on textual reasoning tasks, i.e., the\nchain-of-thought paradigm. However, subtly different explanations can yield\nwidely varying downstream task accuracy. Explanations that have not been\n\"tuned\" for a task, such as off-the-shelf explanations written by nonexperts,\nmay lead to mediocre performance. This paper tackles the problem of how to\noptimize explanation-infused prompts in a blackbox fashion. We first generate\nsets of candidate explanations for each example in the prompt using a\nleave-one-out scheme, then find an effective combination of these explanations\nwith a two-stage framework. We first evaluate explanations for each in-context\nexample in isolation according to two proxy metrics, log likelihood and\naccuracy on new examples. Then, we search over combinations of explanations to\nfind one that yields high performance against a silver-labeled development set.\nAcross four textual reasoning tasks spanning question answering, mathematical\nreasoning, and natural language inference, results show that our proxy metrics\ncorrelate with ground truth accuracy and our overall method can effectively\nimprove prompts over crowdworker annotations and naive search strategies", "published": "2023-02-09 18:02:34", "link": "http://arxiv.org/abs/2302.04813v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning", "abstract": "In this paper, we propose FrameBERT, a RoBERTa-based model that can\nexplicitly learn and incorporate FrameNet Embeddings for concept-level metaphor\ndetection. FrameBERT not only achieves better or comparable performance to the\nstate-of-the-art, but also is more explainable and interpretable compared to\nexisting models, attributing to its ability of accounting for external\nknowledge of FrameNet.", "published": "2023-02-09 18:41:04", "link": "http://arxiv.org/abs/2302.04834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging supplementary text data to kick-start automatic speech\n  recognition system development with limited transcriptions", "abstract": "Recent research using pre-trained transformer models suggests that just 10\nminutes of transcribed speech may be enough to fine-tune such a model for\nautomatic speech recognition (ASR) -- at least if we can also leverage vast\namounts of text data (803 million tokens). But is that much text data\nnecessary? We study the use of different amounts of text data, both for\ncreating a lexicon that constrains ASR decoding to possible words (e.g. *dogz\nvs. dogs), and for training larger language models that bias the system toward\nprobable word sequences (e.g. too dogs vs. two dogs). We perform experiments\nusing 10 minutes of transcribed speech from English (for replicating prior\nwork) and two additional pairs of languages differing in the availability of\nsupplemental text data: Gronings and Frisian (~7.5M token corpora available),\nand Besemah and Nasal (only small lexica available). For all languages, we\nfound that using only a lexicon did not appreciably improve ASR performance.\nFor Gronings and Frisian, we found that lexica and language models derived from\n'novel-length' 80k token subcorpora reduced the word error rate (WER) to 39% on\naverage. Our findings suggest that where a text corpus in the upper tens of\nthousands of tokens or more is available, fine-tuning a transformer model with\njust tens of minutes of transcribed speech holds some promise towards obtaining\nhuman-correctable transcriptions near the 30% WER rule-of-thumb.", "published": "2023-02-09 23:30:49", "link": "http://arxiv.org/abs/2302.04975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Re-Label Method For Data-Centric Machine Learning", "abstract": "In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.", "published": "2023-02-09 01:09:57", "link": "http://arxiv.org/abs/2302.04391v10", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge\n  Memorization", "abstract": "Pre-trained language models (PLM) have achieved remarkable advancement in\ntable-to-text generation tasks. However, the lack of labeled domain-specific\nknowledge and the topology gap between tabular data and text make it difficult\nfor PLMs to yield faithful text. Low-resource generation likewise faces unique\nchallenges in this domain. Inspired by how humans descript tabular data with\nprior knowledge, we suggest a new framework: PromptMize, which targets\ntable-to-text generation under few-shot settings. The design of our framework\nconsists of two aspects: a prompt planner and a knowledge adapter. The prompt\nplanner aims to generate a prompt signal that provides instance guidance for\nPLMs to bridge the topology gap between tabular data and text. Moreover, the\nknowledge adapter memorizes domain-specific knowledge from the unlabelled\ncorpus to supply essential information during generation. Extensive experiments\nand analyses are investigated on three open domain few-shot NLG datasets:\nhuman, song, and book. Compared with previous state-of-the-art approaches, our\nmodel achieves remarkable performance in generating quality as judged by human\nand automatic evaluations.", "published": "2023-02-09 03:04:11", "link": "http://arxiv.org/abs/2302.04415v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Contextomized Quotes in News Headlines by Contrastive Learning", "abstract": "Quotes are critical for establishing credibility in news articles. A direct\nquote enclosed in quotation marks has a strong visual appeal and is a sign of a\nreliable citation. Unfortunately, this journalistic practice is not strictly\nfollowed, and a quote in the headline is often \"contextomized.\" Such a quote\nuses words out of context in a way that alters the speaker's intention so that\nthere is no semantically matching quote in the body text. We present QuoteCSE,\na contrastive learning framework that represents the embedding of news quotes\nbased on domain-driven positive and negative samples to identify such an\neditorial strategy. The dataset and code are available at\nhttps://github.com/ssu-humane/contextomized-quote-contrastive.", "published": "2023-02-09 07:04:11", "link": "http://arxiv.org/abs/2302.04465v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination", "abstract": "The Covid-19 pandemic had an enormous effect on our lives, especially on\npeople's interactions. By introducing Covid-19 vaccines, both positive and\nnegative opinions were raised over the subject of taking vaccines or not. In\nthis paper, using data gathered from Twitter, including tweets and user\nprofiles, we offer a comprehensive analysis of public opinion in Iran about the\nCoronavirus vaccines. For this purpose, we applied a search query technique\ncombined with a topic modeling approach to extract vaccine-related tweets. We\nutilized transformer-based models to classify the content of the tweets and\nextract themes revolving around vaccination. We also conducted an emotion\nanalysis to evaluate the public happiness and anger around this topic. Our\nresults demonstrate that Covid-19 vaccination has attracted considerable\nattention from different angles, such as governmental issues, safety or\nhesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like\npublic vaccination and the rate of infection deeply impacted public emotional\nstatus and users' interactions.", "published": "2023-02-09 09:08:19", "link": "http://arxiv.org/abs/2302.04511v3", "categories": ["cs.CL", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "NLP-based Decision Support System for Examination of Eligibility\n  Criteria from Securities Prospectuses at the German Central Bank", "abstract": "As part of its digitization initiative, the German Central Bank (Deutsche\nBundesbank) wants to examine the extent to which natural Language Processing\n(NLP) can be used to make independent decisions upon the eligibility criteria\nof securities prospectuses. Every month, the Directorate General Markets at the\nGerman Central Bank receives hundreds of scanned prospectuses in PDF format,\nwhich must be manually processed to decide upon their eligibility. We found\nthat this tedious and time-consuming process can be (semi-)automated by\nemploying modern NLP model architectures, which learn the linguistic feature\nrepresentation in text to identify the present eligible and ineligible\ncriteria. The proposed Decision Support System provides decisions of\ndocument-level eligibility criteria accompanied by human-understandable\nexplanations of the decisions. The aim of this project is to model the\ndescribed use case and to evaluate the extent to which current research results\nfrom the field of NLP can be applied to this problem. After creating a\nheterogeneous domain-specific dataset containing annotations of eligible and\nnon-eligible mentions of relevant criteria, we were able to successfully build,\ntrain and deploy a semi-automatic decider model. This model is based on\ntransformer-based language models and decision trees, which integrate the\nestablished rule-based parts of the decision processes. Results suggest that it\nis possible to efficiently model the problem and automate decision making to\nmore than 90% for many of the considered eligibility criteria.", "published": "2023-02-09 11:00:58", "link": "http://arxiv.org/abs/2302.04562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating a Structured Summary of Numerous Academic Papers: Dataset and\n  Method", "abstract": "Writing a survey paper on one research topic usually needs to cover the\nsalient content from numerous related papers, which can be modeled as a\nmulti-document summarization (MDS) task. Existing MDS datasets usually focus on\nproducing the structureless summary covering a few input documents. Meanwhile,\nprevious structured summary generation works focus on summarizing a single\ndocument into a multi-section summary. These existing datasets and methods\ncannot meet the requirements of summarizing numerous academic papers into a\nstructured summary. To deal with the scarcity of available data, we propose\nBigSurvey, the first large-scale dataset for generating comprehensive summaries\nof numerous academic papers on each topic. We collect target summaries from\nmore than seven thousand survey papers and utilize their 430 thousand reference\npapers' abstracts as input documents. To organize the diverse content from\ndozens of input documents and ensure the efficiency of processing long text\nsequences, we propose a summarization method named category-based alignment and\nsparse transformer (CAST). The experimental results show that our CAST method\noutperforms various advanced summarization methods.", "published": "2023-02-09 11:42:07", "link": "http://arxiv.org/abs/2302.04580v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.7"], "primary_category": "cs.CL"}
{"title": "Augmenting NLP data to counter Annotation Artifacts for NLI Tasks", "abstract": "In this paper, we explore Annotation Artifacts - the phenomena wherein large\npre-trained NLP models achieve high performance on benchmark datasets but do\nnot actually \"solve\" the underlying task and instead rely on some dataset\nartifacts (same across train, validation, and test sets) to figure out the\nright answer. We explore this phenomenon on the well-known Natural Language\nInference task by first using contrast and adversarial examples to understand\nlimitations to the model's performance and show one of the biases arising from\nannotation artifacts (the way training data was constructed by the annotators).\nWe then propose a data augmentation technique to fix this bias and measure its\neffectiveness.", "published": "2023-02-09 15:34:53", "link": "http://arxiv.org/abs/2302.04700v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Binarized Neural Machine Translation", "abstract": "The rapid scaling of language models is motivating research using\nlow-bitwidth quantization. In this work, we propose a novel binarization\ntechnique for Transformers applied to machine translation (BMT), the first of\nits kind. We identify and address the problem of inflated dot-product variance\nwhen using one-bit weights and activations. Specifically, BMT leverages\nadditional LayerNorms and residual connections to improve binarization quality.\nExperiments on the WMT dataset show that a one-bit weight-only Transformer can\nachieve the same quality as a float one, while being 16x smaller in size.\nOne-bit activations incur varying degrees of quality drop, but mitigated by the\nproposed architectural changes. We further conduct a scaling law study using\nproduction-scale translation datasets, which shows that one-bit weight\nTransformers scale and generalize well in both in-domain and out-of-domain\nsettings. Implementation in JAX/Flax will be open sourced.", "published": "2023-02-09 19:27:34", "link": "http://arxiv.org/abs/2302.04907v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Learning with Many Demonstration Examples", "abstract": "Large pre-training language models (PLMs) have shown promising in-context\nlearning abilities. However, due to the backbone transformer architecture,\nexisting PLMs are bottlenecked by the memory and computational cost when\nscaling up to a large context size, leaving instruction tuning and in-context\nlearning of many demonstration examples, as well as long-range language\nmodeling under-explored. In this study, we propose a long-range language model\nEVALM based on an efficient transformer mechanism. EVALM is trained with 8k\ntokens per batch line and can test up to 256k-lengthed contexts with\nextrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on\nEVALM, we scale up the size of examples efficiently in both instruction tuning\nand in-context learning to explore the boundary of the benefits from more\nannotated data. Experimental results on a diverse set of tasks show that EVALM\nachieves 4.1% higher accuracy on average, and the average length of achieving\nthe best accuracy score over tasks is around 12k. We find that in-context\nlearning can achieve higher performance with more demonstrations under\nmany-shot instruction tuning (8k), and further extending the length of\ninstructions (16k) can further improve the upper bound of scaling in-context\nlearning.", "published": "2023-02-09 20:53:12", "link": "http://arxiv.org/abs/2302.04931v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correcting Real-Word Spelling Errors: A New Hybrid Approach", "abstract": "Spelling correction is one of the main tasks in the field of Natural Language\nProcessing. Contrary to common spelling errors, real-word errors cannot be\ndetected by conventional spelling correction methods. The real-word correction\nmodel proposed by Mays, Damerau and Mercer showed a great performance in\ndifferent evaluations. In this research, however, a new hybrid approach is\nproposed which relies on statistical and syntactic knowledge to detect and\ncorrect real-word errors. In this model, Constraint Grammar (CG) is used to\ndiscriminate among sets of correction candidates in the search space. Mays,\nDamerau and Mercer's trigram approach is manipulated to estimate the\nprobability of syntactically well-formed correction candidates. The approach\nproposed here is tested on the Wall Street Journal corpus. The model can prove\nto be more practical than some other models, such as WordNet-based method of\nHirst and Budanitsky and fixed windows size method of Wilcox-O'Hearn and Hirst.", "published": "2023-02-09 06:03:11", "link": "http://arxiv.org/abs/2302.06407v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real-Time Visual Feedback to Guide Benchmark Creation: A\n  Human-and-Metric-in-the-Loop Workflow", "abstract": "Recent research has shown that language models exploit `artifacts' in\nbenchmarks to solve tasks, rather than truly learning them, leading to inflated\nmodel performance. In pursuit of creating better benchmarks, we propose VAIDA,\na novel benchmark creation paradigm for NLP, that focuses on guiding\ncrowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.\nVAIDA facilitates sample correction by providing realtime visual feedback and\nrecommendations to improve sample quality. Our approach is domain, model, task,\nand metric agnostic, and constitutes a paradigm shift for robust, validated,\nand dynamic benchmark creation via human-and-metric-in-the-loop workflows. We\nevaluate via expert review and a user study with NASA TLX. We find that VAIDA\ndecreases effort, frustration, mental, and temporal demands of crowdworkers and\nanalysts, simultaneously increasing the performance of both user groups with a\n45.8% decrease in the level of artifacts in created samples. As a by product of\nour user study, we observe that created samples are adversarial across models,\nleading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)\nin performance.", "published": "2023-02-09 04:43:10", "link": "http://arxiv.org/abs/2302.04434v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing E-Commerce Recommendation using Pre-Trained Language Model and\n  Fine-Tuning", "abstract": "Pretrained Language Models (PLM) have been greatly successful on a board\nrange of natural language processing (NLP) tasks. However, it has just started\nbeing applied to the domain of recommendation systems. Traditional\nrecommendation algorithms failed to incorporate the rich textual information in\ne-commerce datasets, which hinderss the performance of those models. We present\na thorough investigation on the effect of various strategy of incorporating\nPLMs into traditional recommender algorithms on one of the e-commerce datasets,\nand we compare the results with vanilla recommender baseline models. We show\nthat the application of PLMs and domain specific fine-tuning lead to an\nincrease on the predictive capability of combined models. These results\naccentuate the importance of utilizing textual information in the context of\ne-commerce, and provides insight on how to better apply PLMs alongside\ntraditional recommender system algorithms. The code used in this paper is\navailable on Github: https://github.com/NuofanXu/bert_retail_recommender.", "published": "2023-02-09 05:20:52", "link": "http://arxiv.org/abs/2302.04443v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Read and Reap the Rewards: Learning to Play Atari with the Help of\n  Instruction Manuals", "abstract": "High sample complexity has long been a challenge for RL. On the other hand,\nhumans learn to perform tasks not only from interaction or demonstrations, but\nalso by reading unstructured text documents, e.g., instruction manuals.\nInstruction manuals and wiki pages are among the most abundant data that could\ninform agents of valuable features and policies or task-specific environmental\ndynamics and reward structures. Therefore, we hypothesize that the ability to\nutilize human-written instruction manuals to assist learning policies for\nspecific tasks should lead to a more efficient and better-performing agent. We\npropose the Read and Reward framework. Read and Reward speeds up RL algorithms\non Atari games by reading manuals released by the Atari game developers. Our\nframework consists of a QA Extraction module that extracts and summarizes\nrelevant information from the manual and a Reasoning module that evaluates\nobject-agent interactions based on information from the manual. An auxiliary\nreward is then provided to a standard A2C RL agent, when interaction is\ndetected. Experimentally, various RL algorithms obtain significant improvement\nin performance and training speed when assisted by our design.", "published": "2023-02-09 05:47:03", "link": "http://arxiv.org/abs/2302.04449v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bag of Tricks for Training Data Extraction from Language Models", "abstract": "With the advance of language models, privacy protection is receiving more\nattention. Training data extraction is therefore of great importance, as it can\nserve as a potential tool to assess privacy leakage. However, due to the\ndifficulty of this task, most of the existing methods are proof-of-concept and\nstill not effective enough. In this paper, we investigate and benchmark tricks\nfor improving training data extraction using a publicly available dataset.\nBecause most existing extraction methods use a pipeline of\ngenerating-then-ranking, i.e., generating text candidates as potential training\ndata and then ranking them based on specific criteria, our research focuses on\nthe tricks for both text generation (e.g., sampling strategy) and text ranking\n(e.g., token-level criteria). The experimental results show that several\npreviously overlooked tricks can be crucial to the success of training data\nextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks\noutperform the baseline by a large margin in most cases, providing a much\nstronger baseline for future research. The code is available at\nhttps://github.com/weichen-yu/LM-Extraction.", "published": "2023-02-09 06:46:42", "link": "http://arxiv.org/abs/2302.04460v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Attention via Control Variates", "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax\nattention with linear runtime and space complexity. However, the approximation\ngap between RFA and conventional softmax attention is not well studied. Built\nupon previous progress of RFA, we characterize this gap through the lens of\ncontrol variates and show that RFA can be decomposed into a sum of multiple\ncontrol variate estimators for each element in the sequence. This new framework\nreveals that exact softmax attention can be recovered from RFA by manipulating\neach control variate. Besides, it allows us to develop a more flexible form of\ncontrol variates, resulting in a novel attention mechanism that significantly\nreduces the approximation gap while maintaining linear complexity. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art efficient\nattention mechanisms on both vision and language tasks.", "published": "2023-02-09 10:16:20", "link": "http://arxiv.org/abs/2302.04542v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Lightweight Transformers for Clinical Natural Language Processing", "abstract": "Specialised pre-trained language models are becoming more frequent in NLP\nsince they can potentially outperform models trained on generic texts. BioBERT\nand BioClinicalBERT are two examples of such models that have shown promise in\nmedical NLP tasks. Many of these models are overparametrised and\nresource-intensive, but thanks to techniques like Knowledge Distillation (KD),\nit is possible to create smaller versions that perform almost as well as their\nlarger counterparts. In this work, we specifically focus on development of\ncompact language models for processing clinical texts (i.e. progress notes,\ndischarge summaries etc). We developed a number of efficient lightweight\nclinical transformers using knowledge distillation and continual learning, with\nthe number of parameters ranging from 15 million to 65 million. These models\nperformed comparably to larger models such as BioBERT and ClinicalBioBERT and\nsignificantly outperformed other compact models trained on general or\nbiomedical data. Our extensive evaluation was done across several standard\ndatasets and covered a wide range of clinical text-mining tasks, including\nNatural Language Inference, Relation Extraction, Named Entity Recognition, and\nSequence Classification. To our knowledge, this is the first comprehensive\nstudy specifically focused on creating efficient and compact transformers for\nclinical NLP tasks. The models and code used in this study can be found on our\nHuggingface profile at https://huggingface.co/nlpie and Github page at\nhttps://github.com/nlpie-research/Lightweight-Clinical-Transformers,\nrespectively, promoting reproducibility of our results.", "published": "2023-02-09 16:07:31", "link": "http://arxiv.org/abs/2302.04725v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Language Models for Cross Lingual Fact Extraction\n  from Low Resource Indian Languages", "abstract": "Massive knowledge graphs like Wikidata attempt to capture world knowledge\nabout multiple entities. Recent approaches concentrate on automatically\nenriching these KGs from text. However a lot of information present in the form\nof natural text in low resource languages is often missed out. Cross Lingual\nInformation Extraction aims at extracting factual information in the form of\nEnglish triples from low resource Indian Language text. Despite its massive\npotential, progress made on this task is lagging when compared to Monolingual\nInformation Extraction. In this paper, we propose the task of Cross Lingual\nFact Extraction(CLFE) from text and devise an end-to-end generative approach\nfor the same which achieves an overall F1 score of 77.46.", "published": "2023-02-09 17:29:56", "link": "http://arxiv.org/abs/2302.04790v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models", "abstract": "Research on neural networks has focused on understanding a single model\ntrained on a single dataset. However, relatively little is known about the\nrelationships between different models, particularly those trained or tested on\ndifferent datasets. We address this by studying how the weight space and the\nunderlying loss landscape of different models are interconnected.\n  Specifically, we demonstrate that finetuned models that were optimized for\nhigh performance, reside in well-defined regions in weight space, and vice\nversa -- that any model that resides anywhere in those regions also exhibits\nhigh performance. Notably, we show that language models that have been\nfinetuned on the same dataset form a tight cluster in the weight space, while\nmodels finetuned on different datasets from the same underlying task form a\nlooser cluster. Moreover, traversing around the region between the models leads\nto new models that perform comparably or even better than models obtained via\nfinetuning, even on tasks that the original models were not finetuned on.\n  Our findings provide insight into the relationships between models,\ndemonstrating that a model positioned between two similar models can acquire\nthe knowledge of both. We leverage this and design a method for selecting a\nbetter model for efficient finetuning. Specifically, we show that starting from\nthe center of the region is as effective, if not more, than using the\npretrained model in 11 out of 12 datasets, resulting in an average accuracy\nimprovement of 3.06.", "published": "2023-02-09 18:59:18", "link": "http://arxiv.org/abs/2302.04863v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ELBA: Learning by Asking for Embodied Visual Navigation and Task\n  Completion", "abstract": "The research community has shown increasing interest in designing intelligent\nembodied agents that can assist humans in accomplishing tasks. Although there\nhave been significant advancements in related vision-language benchmarks, most\nprior work has focused on building agents that follow instructions rather than\nendowing agents the ability to ask questions to actively resolve ambiguities\narising naturally in embodied environments. To address this gap, we propose an\nEmbodied Learning-By-Asking (ELBA) model that learns when and what questions to\nask to dynamically acquire additional information for completing the task. We\nevaluate ELBA on the TEACh vision-dialog navigation and task completion\ndataset. Experimental results show that the proposed method achieves improved\ntask performance compared to baseline models without question-answering\ncapabilities.", "published": "2023-02-09 18:59:41", "link": "http://arxiv.org/abs/2302.04865v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Offsite-Tuning: Transfer Learning without Full Model", "abstract": "Transfer learning is important for foundation models to adapt to downstream\ntasks. However, many foundation models are proprietary, so users must share\ntheir data with model owners to fine-tune the models, which is costly and raise\nprivacy concerns. Moreover, fine-tuning large foundation models is\ncomputation-intensive and impractical for most downstream users. In this paper,\nwe propose Offsite-Tuning, a privacy-preserving and efficient transfer learning\nframework that can adapt billion-parameter foundation models to downstream data\nwithout access to the full model. In offsite-tuning, the model owner sends a\nlight-weight adapter and a lossy compressed emulator to the data owner, who\nthen fine-tunes the adapter on the downstream data with the emulator's\nassistance. The fine-tuned adapter is then returned to the model owner, who\nplugs it into the full model to create an adapted foundation model.\nOffsite-tuning preserves both parties' privacy and is computationally more\nefficient than the existing fine-tuning methods that require access to the full\nmodel weights. We demonstrate the effectiveness of offsite-tuning on various\nlarge language and vision foundation models. Offsite-tuning can achieve\ncomparable accuracy as full model fine-tuning while being privacy-preserving\nand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is\navailable at https://github.com/mit-han-lab/offsite-tuning.", "published": "2023-02-09 18:59:55", "link": "http://arxiv.org/abs/2302.04870v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text\n  Using General Purpose Language Models", "abstract": "Accurate and comprehensive material databases extracted from research papers\nare crucial for materials science and engineering, but their development\nrequires significant human effort. With large language models (LLMs)\ntransforming the way humans interact with text, LLMs provide an opportunity to\nrevolutionize data extraction. In this study, we demonstrate a simple and\nefficient method for extracting materials data from full-text research papers\nleveraging the capabilities of LLMs combined with human supervision. This\napproach is particularly suitable for mid-sized databases and requires minimal\nto no coding or prior knowledge about the extracted property. It offers high\nrecall and nearly perfect precision in the resulting database. The method is\neasily adaptable to new and superior language models, ensuring continued\nutility. We show this by evaluating and comparing its performance on GPT-3 and\nGPT-3.5/4 (which underlie ChatGPT), as well as free alternatives such as BART\nand DeBERTaV3. We provide a detailed analysis of the method's performance in\nextracting sentences containing bulk modulus data, achieving up to 90%\nprecision at 96% recall, depending on the amount of human effort involved. We\nfurther demonstrate the method's broader effectiveness by developing a database\nof critical cooling rates for metallic glasses over twice the size of previous\nhuman curated databases.", "published": "2023-02-09 19:56:37", "link": "http://arxiv.org/abs/2302.04914v3", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "AutoNMT: A Framework to Streamline the Research of Seq2Seq Models", "abstract": "We present AutoNMT, a framework to streamline the research of seq-to-seq\nmodels by automating the data pipeline (i.e., file management, data\npreprocessing, and exploratory analysis), automating experimentation in a\ntoolkit-agnostic manner, which allows users to use either their own models or\nexisting seq-to-seq toolkits such as Fairseq or OpenNMT, and finally,\nautomating the report generation (plots and summaries). Furthermore, this\nlibrary comes with its own seq-to-seq toolkit so that users can easily\ncustomize it for non-standard tasks.", "published": "2023-02-09 23:42:30", "link": "http://arxiv.org/abs/2302.04981v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised clustering of file dialects according to monotonic\n  decompositions of mixtures", "abstract": "This paper proposes an unsupervised classification method that partitions a\nset of files into non-overlapping dialects based upon their behaviors,\ndetermined by messages produced by a collection of programs that consume them.\nThe pattern of messages can be used as the signature of a particular kind of\nbehavior, with the understanding that some messages are likely to co-occur,\nwhile others are not. Patterns of messages can be used to classify files into\ndialects. A dialect is defined by a subset of messages, called the required\nmessages. Once files are conditioned upon dialect and its required messages,\nthe remaining messages are statistically independent.\n  With this definition of dialect in hand, we present a greedy algorithm that\ndeduces candidate dialects from a dataset consisting of a matrix of\nfile-message data, demonstrate its performance on several file formats, and\nprove conditions under which it is optimal. We show that an analyst needs to\nconsider fewer dialects than distinct message patterns, which reduces their\ncognitive load when studying a complex format.", "published": "2023-02-09 21:15:36", "link": "http://arxiv.org/abs/2304.09082v1", "categories": ["cs.PL", "cs.CL", "cs.IR", "62P30 (Primary), 06A07 (Secondary)", "D.3.4"], "primary_category": "cs.PL"}
{"title": "ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models", "abstract": "In recent years, the burgeoning interest in diffusion models has led to\nsignificant advances in image and speech generation. Nevertheless, the direct\nsynthesis of music waveforms from unrestricted textual prompts remains a\nrelatively underexplored domain. In response to this lacuna, this paper\nintroduces a pioneering contribution in the form of a text-to-waveform music\ngeneration model, underpinned by the utilization of diffusion models. Our\nmethodology hinges on the innovative incorporation of free-form textual prompts\nas conditional factors to guide the waveform generation process within the\ndiffusion model framework. Addressing the challenge of limited text-music\nparallel data, we undertake the creation of a dataset by harnessing web\nresources, a task facilitated by weak supervision techniques. Furthermore, a\nrigorous empirical inquiry is undertaken to contrast the efficacy of two\ndistinct prompt formats for text conditioning, namely, music tags and\nunconstrained textual descriptions. The outcomes of this comparative analysis\naffirm the superior performance of our proposed model in terms of enhancing\ntext-music relevance. Finally, our work culminates in a demonstrative\nexhibition of the excellent capabilities of our model in text-to-music\ngeneration. We further demonstrate that our generated music in the waveform\ndomain outperforms previous works by a large margin in terms of diversity,\nquality, and text-music relevance.", "published": "2023-02-09 06:27:09", "link": "http://arxiv.org/abs/2302.04456v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot\n  Image Captioning", "abstract": "Augmenting pretrained language models (LMs) with a vision encoder (e.g.,\nFlamingo) has obtained the state-of-the-art results in image-to-text\ngeneration. However, these models store all the knowledge within their\nparameters, thus often requiring enormous model parameters to model the\nabundant visual concepts and very rich textual descriptions. Additionally, they\nare inefficient in incorporating new data, requiring a computational-expensive\nfine-tuning process. In this work, we introduce a Retrieval-augmented Visual\nLanguage Model, Re-ViLM, built upon the Flamingo, that supports retrieving the\nrelevant knowledge from the external database for zero and in-context few-shot\nimage-to-text generations. By storing certain knowledge explicitly in the\nexternal database, our approach reduces the number of model parameters and can\neasily accommodate new data during evaluation by simply updating the database.\nWe also construct an interleaved image and text data that facilitates\nin-context few-shot learning capabilities. We demonstrate that Re-ViLM\nsignificantly boosts performance for image-to-text generation tasks, especially\nfor zero-shot and few-shot generation in out-of-domain settings with 4 times\nless parameters compared with baseline methods.", "published": "2023-02-09 18:57:56", "link": "http://arxiv.org/abs/2302.04858v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Joint Acoustic Echo Cancellation and Speech Dereverberation Using Kalman\n  filters", "abstract": "This paper proposes a joint acoustic echo cancellation (AEC) and speech\ndereverberation (DR) algorithm in the short-time Fourier transform domain. The\nreverberant microphone signals are described using an auto-regressive (AR)\nmodel. The AR coefficients and the loudspeaker-to-microphone acoustic transfer\nfunctions (ATFs) are considered time-varying and are modeled simultaneously\nusing a first-order Markov process. This leads to a solution where these\nparameters can be optimally estimated using Kalman filters. It is shown that\nthe proposed algorithm outperforms vanilla solutions that solve AEC and DR\nsequentially and one state-of-the-art joint DRAEC algorithm based on semi-blind\nsource separation, in terms of both speech quality and echo reduction\nperformance.", "published": "2023-02-09 07:18:10", "link": "http://arxiv.org/abs/2302.04469v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Incorporating Total Variation Regularization in the design of an\n  intelligent Query by Humming system", "abstract": "A Query-By-Humming (QBH) system constitutes a particular case of music\ninformation retrieval where the input is a user-hummed melody and the output is\nthe original song which contains that melody. A typical QBH system consists of\nmelody extraction and candidate melody retrieval.\n  For melody extraction, accurate note transcription is the key enabling\ntechnology. However, current transcription methods are unable to definitively\ncapture the melody and address inaccuracies in user-hummed queries. In this\npaper, we incorporate Total Variation Regularization (TVR) to denoise queries.\nThis approach accounts for user error in humming without loss of meaningful\ndata and reliably captures the underlying melody.\n  For candidate melody retrieval, we employ a deep learning approach to time\nseries classification using a Fully Convolutional Neural Network. The trained\nnetwork classifies the incoming query as belonging to one of the target songs.\n  For our experiments, we use Roger Jang's MIR-QBSH dataset which is the\nstandard MIREX dataset. We demonstrate that inclusion of TVR denoised queries\nin the training set enhances the overall accuracy of the system to 93% which is\nhigher than other state-of-the-art QBH systems.", "published": "2023-02-09 11:34:23", "link": "http://arxiv.org/abs/2302.04577v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Composite T60 Regression and Classification Approach for Speech\n  Dereverberation", "abstract": "Dereverberation is often performed directly on the reverberant audio signal,\nwithout knowledge of the acoustic environment. Reverberation time, T60,\nhowever, is an essential acoustic factor that reflects how reverberation may\nimpact a signal. In this work, we propose to perform dereverberation while\nleveraging key acoustic information from the environment. More specifically, we\ndevelop a joint learning approach that uses a composite T60 module and a\nseparate dereverberation module to simultaneously perform reverberation time\nestimation and dereverberation. The reverberation time module provides key\nfeatures to the dereverberation module during fine tuning. We evaluate our\napproach in simulated and real environments, and compare against several\napproaches. The results show that this composite framework improves performance\nin environments.", "published": "2023-02-09 20:56:09", "link": "http://arxiv.org/abs/2302.04932v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hypernetworks build Implicit Neural Representations of Sounds", "abstract": "Implicit Neural Representations (INRs) are nowadays used to represent\nmultimedia signals across various real-life applications, including image\nsuper-resolution, image compression, or 3D rendering. Existing methods that\nleverage INRs are predominantly focused on visual data, as their application to\nother modalities, such as audio, is nontrivial due to the inductive biases\npresent in architectural attributes of image-based INR models. To address this\nlimitation, we introduce HyperSound, the first meta-learning approach to\nproduce INRs for audio samples that leverages hypernetworks to generalize\nbeyond samples observed in training. Our approach reconstructs audio samples\nwith quality comparable to other state-of-the-art models and provides a viable\nalternative to contemporary sound representations used in deep neural networks\nfor audio processing, such as spectrograms.", "published": "2023-02-09 22:24:26", "link": "http://arxiv.org/abs/2302.04959v3", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Help the Blind See: Assistance for the Visually Impaired through\n  Augmented Acoustic Simulation", "abstract": "An estimated 253 million people have visual impairments. These visual\nimpairments affect everyday lives, and limit their understanding of the outside\nworld. This can pose a risk to health from falling or collisions. We propose a\nsolution to this through quick and detailed communication of environmental\nspatial geometry through sound, providing the blind and visually impaired the\nability to understand their spatial environment through sound technology. The\nmodel consists of fast object detection and 3D environmental mapping, which is\ncommunicated through a series of quick sound notes. These sound notes are at\ndifferent frequencies, pitches, and arrangements in order to precisely\ncommunicate the depth and location of points within the environment. Sounds are\ncommunicated in the form of musical notes in order to be easily recognizable\nand distinguishable. A unique algorithm is used to segment objects, providing\nminimal accuracy loss and improvement from the normal O(n2 ) to O(n) (which is\nsignificant, as N in point clouds can often be in the range of 105 ). In\ntesting, we achieved an R-value of 0.866 on detailed objects and an accuracy of\n87.5% on an outdoor scene at night with large amounts of noise. We also provide\na supplementary video demo of our system.", "published": "2023-02-09 02:32:33", "link": "http://arxiv.org/abs/2303.13536v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
