{"title": "Controllable Text Generation with Focused Variation", "abstract": "This work introduces Focused-Variation Network (FVN), a novel model to\ncontrol language generation. The main problems in previous controlled language\ngeneration models range from the difficulty of generating text according to the\ngiven attributes, to the lack of diversity of the generated texts. FVN\naddresses these issues by learning disjoint discrete latent spaces for each\nattribute inside codebooks, which allows for both controllability and\ndiversity, while at the same time generating fluent text. We evaluate FVN on\ntwo text generation datasets with annotated content and style, and show\nstate-of-the-art performance as assessed by automatic and human evaluations.", "published": "2020-09-25 06:31:06", "link": "http://arxiv.org/abs/2009.12046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Focus-Constrained Attention Mechanism for CVAE-based Response Generation", "abstract": "To model diverse responses for a given post, one promising way is to\nintroduce a latent variable into Seq2Seq models. The latent variable is\nsupposed to capture the discourse-level information and encourage the\ninformativeness of target responses. However, such discourse-level information\nis often too coarse for the decoder to be utilized. To tackle it, our idea is\nto transform the coarse-grained discourse-level information into fine-grained\nword-level information. Specifically, we firstly measure the semantic\nconcentration of corresponding target response on the post words by introducing\na fine-grained focus signal. Then, we propose a focus-constrained attention\nmechanism to take full advantage of focus in well aligning the input to the\ntarget response. The experimental results demonstrate that by exploiting the\nfine-grained signal, our model can generate more diverse and informative\nresponses compared with several state-of-the-art models.", "published": "2020-09-25 09:38:59", "link": "http://arxiv.org/abs/2009.12102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerKey: A Persian News Corpus for Keyphrase Extraction and Generation", "abstract": "Keyphrases provide an extremely dense summary of a text. Such information can\nbe used in many Natural Language Processing tasks, such as information\nretrieval and text summarization. Since previous studies on Persian keyword or\nkeyphrase extraction have not published their data, the field suffers from the\nlack of a human extracted keyphrase dataset. In this paper, we introduce\nPerKey, a corpus of 553k news articles from six Persian news websites and\nagencies with relatively high quality author extracted keyphrases, which is\nthen filtered and cleaned to achieve higher quality keyphrases. The resulted\ndata was put into human assessment to ensure the quality of the keyphrases. We\nalso measured the performance of different supervised and unsupervised\ntechniques, e.g. TFIDF, MultipartiteRank, KEA, etc. on the dataset using\nprecision, recall, and F1-score.", "published": "2020-09-25 14:36:41", "link": "http://arxiv.org/abs/2009.12269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Keyphrase Generation Using Sequence-to-Sequence Models", "abstract": "Keyphrases are a very short summary of an input text and provide the main\nsubjects discussed in the text. Keyphrase extraction is a useful upstream task\nand can be used in various natural language processing problems, for example,\ntext summarization and information retrieval, to name a few. However, not all\nthe keyphrases are explicitly mentioned in the body of the text. In real-world\nexamples there are always some topics that are discussed implicitly. Extracting\nsuch keyphrases requires a generative approach, which is adopted here. In this\npaper, we try to tackle the problem of keyphrase generation and extraction from\nnews articles using deep sequence-to-sequence models. These models\nsignificantly outperform the conventional methods such as Topic Rank, KPMiner,\nand KEA in the task of keyphrase extraction.", "published": "2020-09-25 14:40:14", "link": "http://arxiv.org/abs/2009.12271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Text Augmentation on Social Media Text in Vietnamese", "abstract": "In the text classification problem, the imbalance of labels in datasets\naffect the performance of the text-classification models. Practically, the data\nabout user comments on social networking sites not altogether appeared - the\nadministrators often only allow positive comments and hide negative comments.\nThus, when collecting the data about user comments on the social network, the\ndata is usually skewed about one label, which leads the dataset to become\nimbalanced and deteriorate the model's ability. The data augmentation\ntechniques are applied to solve the imbalance problem between classes of the\ndataset, increasing the prediction model's accuracy. In this paper, we\nperformed augmentation techniques on the VLSP2019 Hate Speech Detection on\nVietnamese social texts and the UIT - VSFC: Vietnamese Students' Feedback\nCorpus for Sentiment Analysis. The result of augmentation increases by about\n1.5% in the F1-macro score on both corpora.", "published": "2020-09-25 16:18:52", "link": "http://arxiv.org/abs/2009.12319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing FB Chatbot Based on Deep Learning Using RASA Framework for\n  University Enquiries", "abstract": "Smart systems for Universities powered by Artificial Intelligence have been\nmassively developed to help humans in various tasks. The chatbot concept is not\nsomething new in today society which is developing with recent technology.\nCollege students or candidates of college students often need actual\ninformation like asking for something to customer service, especially during\nthis pandemic, when it is difficult to have an immediate face-to-face meeting.\nChatbots are functionally helping in several things such as curriculum\ninformation, admission for new students, schedule info for any lecture courses,\nstudents grade information, and some adding features for Muslim worships\nschedule, also weather forecast information. This Chatbot is developed by Deep\nLearning models, which was adopted by an artificial intelligence model that\nreplicates human intelligence with some specific training schemes. This kind of\nDeep Learning is based on RNN which has some specific memory savings scheme for\nthe Deep Learning Model, specifically this chatbot using LSTM which already\nintegrates by RASA framework. LSTM is also known as Long Short Term Memory\nwhich efficiently saves some required memory but will remove some memory that\nis not needed. This Chatbot uses the FB platform because of the FB users have\nalready reached up to 60.8% of its entire population in Indonesia. Here's the\nchatbot only focuses on case studies at campus of the Magister Informatics FTI\nUniversity of Islamic Indonesia. This research is a first stage development\nwithin fairly sufficient simulate data.", "published": "2020-09-25 17:01:19", "link": "http://arxiv.org/abs/2009.12341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A little goes a long way: Improving toxic language classification\n  despite data scarcity", "abstract": "Detection of some types of toxic language is hampered by extreme scarcity of\nlabeled training data. Data augmentation - generating new synthetic data from a\nlabeled seed dataset - can help. The efficacy of data augmentation on toxic\nlanguage classification has not been fully explored. We present the first\nsystematic study on how data augmentation techniques impact performance across\ntoxic language classifiers, ranging from shallow logistic regression\narchitectures to BERT - a state-of-the-art pre-trained Transformer network. We\ncompare the performance of eight techniques on very scarce seed datasets. We\nshow that while BERT performed the best, shallow classifiers performed\ncomparably when trained on data augmented with a combination of three\ntechniques, including GPT-2-generated sentences. We discuss the interplay of\nperformance and computational overhead, which can inform the choice of\ntechniques under different constraints.", "published": "2020-09-25 17:04:17", "link": "http://arxiv.org/abs/2009.12344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XTE: Explainable Text Entailment", "abstract": "Text entailment, the task of determining whether a piece of text logically\nfollows from another piece of text, is a key component in NLP, providing input\nfor many semantic applications such as question answering, text summarization,\ninformation extraction, and machine translation, among others. Entailment\nscenarios can range from a simple syntactic variation to more complex semantic\nrelationships between pieces of text, but most approaches try a\none-size-fits-all solution that usually favors some scenario to the detriment\nof another. Furthermore, for entailments requiring world knowledge, most\nsystems still work as a \"black box\", providing a yes/no answer that does not\nexplain the underlying reasoning process. In this work, we introduce XTE -\nExplainable Text Entailment - a novel composite approach for recognizing text\nentailment which analyzes the entailment pair to decide whether it must be\nresolved syntactically or semantically. Also, if a semantic matching is\ninvolved, we make the answer interpretable, using external knowledge bases\ncomposed of structured lexical definitions to generate natural language\njustifications that explain the semantic relationship holding between the\npieces of text. Besides outperforming well-established entailment algorithms,\nour composite approach gives an important step towards Explainable AI, allowing\nthe inference model interpretation, making the semantic reasoning process\nexplicit and understandable.", "published": "2020-09-25 20:49:07", "link": "http://arxiv.org/abs/2009.12431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BET: A Backtranslation Approach for Easy Data Augmentation in\n  Transformer-based Paraphrase Identification Context", "abstract": "Newly-introduced deep learning architectures, namely BERT, XLNet, RoBERTa and\nALBERT, have been proved to be robust on several NLP tasks. However, the\ndatasets trained on these architectures are fixed in terms of size and\ngeneralizability. To relieve this issue, we apply one of the most inexpensive\nsolutions to update these datasets. We call this approach BET by which we\nanalyze the backtranslation data augmentation on the transformer-based\narchitectures. Using the Google Translate API with ten intermediary languages\nfrom ten different language families, we externally evaluate the results in the\ncontext of automatic paraphrase identification in a transformer-based\nframework. Our findings suggest that BET improves the paraphrase identification\nperformance on the Microsoft Research Paraphrase Corpus (MRPC) to more than 3%\non both accuracy and F1 score. We also analyze the augmentation in the low-data\nregime with downsampled versions of MRPC, Twitter Paraphrase Corpus (TPC) and\nQuora Question Pairs. In many low-data cases, we observe a switch from a\nfailing model on the test set to reasonable performances. The results\ndemonstrate that BET is a highly promising data augmentation technique: to push\nthe current state-of-the-art of existing datasets and to bootstrap the\nutilization of deep learning architectures in the low-data regime of a hundred\nsamples.", "published": "2020-09-25 22:06:06", "link": "http://arxiv.org/abs/2009.12452v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems", "abstract": "In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify\nthe system design process of task-oriented dialogue systems and alleviate the\nover-dependency on annotated data. MinTL is a simple yet effective transfer\nlearning framework, which allows us to plug-and-play pre-trained seq2seq\nmodels, and jointly learn dialogue state tracking and dialogue response\ngeneration. Unlike previous approaches, which use a copy mechanism to\n\"carryover\" the old dialogue states to the new one, we introduce Levenshtein\nbelief spans (Lev), that allows efficient dialogue state tracking with a\nminimal generation length. We instantiate our learning framework with two\npre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive\nexperiments demonstrate that: 1) our systems establish new state-of-the-art\nresults on end-to-end response generation, 2) MinTL-based systems are more\nrobust than baseline methods in the low resource setting, and they achieve\ncompetitive results with only 20\\% training data, and 3) Lev greatly improves\nthe inference efficiency.", "published": "2020-09-25 02:19:13", "link": "http://arxiv.org/abs/2009.12005v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revealing the Myth of Higher-Order Inference in Coreference Resolution", "abstract": "This paper analyzes the impact of higher-order inference (HOI) on the task of\ncoreference resolution. HOI has been adapted by almost all recent coreference\nresolution models without taking much investigation on its true effectiveness\nover representation learning. To make a comprehensive analysis, we implement an\nend-to-end coreference system as well as four HOI approaches, attended\nantecedent, entity equalization, span clustering, and cluster merging, where\nthe latter two are our original methods. We find that given a high-performing\nencoder such as SpanBERT, the impact of HOI is negative to marginal, providing\na new perspective of HOI to this task. Our best model using cluster merging\nshows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.", "published": "2020-09-25 03:28:07", "link": "http://arxiv.org/abs/2009.12013v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoETER: Automated Entity Type Representation for Knowledge Graph\n  Embedding", "abstract": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing\nentities and relations in continuous vector spaces. Some traditional KGE models\nleveraging additional type information can improve the representation of\nentities which however totally rely on the explicit types or neglect the\ndiverse type representations specific to various relations. Besides, none of\nthe existing methods is capable of inferring all the relation patterns of\nsymmetry, inversion and composition as well as the complex properties of 1-N,\nN-1 and N-N relations, simultaneously. To explore the type information for any\nKG, we develop a novel KGE framework with Automated Entity TypE Representation\n(AutoETER), which learns the latent type embedding of each entity by regarding\neach relation as a translation operation between the types of two entities with\na relation-aware projection mechanism. Particularly, our designed automated\ntype representation learning mechanism is a pluggable module which can be\neasily incorporated with any KGE model. Besides, our approach could model and\ninfer all the relation patterns and complex relations. Experiments on four\ndatasets demonstrate the superior performance of our model compared to\nstate-of-the-art baselines on link prediction tasks, and the visualization of\ntype clustering provides clearly the explanation of type embeddings and\nverifies the effectiveness of our model.", "published": "2020-09-25 04:27:35", "link": "http://arxiv.org/abs/2009.12030v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "No Answer is Better Than Wrong Answer: A Reflection Model for Document\n  Level Machine Reading Comprehension", "abstract": "The Natural Questions (NQ) benchmark set brings new challenges to Machine\nReading Comprehension: the answers are not only at different levels of\ngranularity (long and short), but also of richer types (including no-answer,\nyes/no, single-span and multi-span). In this paper, we target at this challenge\nand handle all answer types systematically. In particular, we propose a novel\napproach called Reflection Net which leverages a two-step training procedure to\nidentify the no-answer and wrong-answer cases. Extensive experiments are\nconducted to verify the effectiveness of our approach. At the time of paper\nwriting (May.~20,~2020), our approach achieved the top 1 on both long and short\nanswer leaderboard, with F1 scores of 77.2 and 64.1, respectively.", "published": "2020-09-25 06:57:52", "link": "http://arxiv.org/abs/2009.12056v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Sentence Embedding Method by Mutual Information\n  Maximization", "abstract": "BERT is inefficient for sentence-pair tasks such as clustering or semantic\nsearch as it needs to evaluate combinatorially many sentence pairs which is\nvery time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by\nlearning semantically meaningful representations of single sentences, such that\nsimilarity comparison can be easily accessed. However, SBERT is trained on\ncorpus with high-quality labeled sentence pairs, which limits its application\nto tasks where labeled data is extremely scarce. In this paper, we propose a\nlightweight extension on top of BERT and a novel self-supervised learning\nobjective based on mutual information maximization strategies to derive\nmeaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our\nmethod is not restricted by the availability of labeled data, such that it can\nbe applied on different domain-specific corpus. Experimental results show that\nthe proposed method significantly outperforms other unsupervised sentence\nembedding baselines on common semantic textual similarity (STS) tasks and\ndownstream supervised tasks. It also outperforms SBERT in a setting where\nin-domain labeled data is not available, and achieves performance competitive\nwith supervised methods on various tasks.", "published": "2020-09-25 07:16:51", "link": "http://arxiv.org/abs/2009.12061v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The algebra of non-deterministic programs: demonic operators, orders and\n  axioms", "abstract": "Demonic composition, demonic refinement and demonic union are alternatives to\nthe usual \"angelic\" composition, angelic refinement (inclusion) and angelic\n(usual) union defined on binary relations. We first motivate both the angelic\nand demonic via an analysis of the behaviour of non-deterministic programs,\nwith the angelic associated with partial correctness and demonic with total\ncorrectness, both cases emerging from a richer algebraic model of\nnon-deterministic programs incorporating both aspects. Zareckii has shown that\nthe isomorphism class of algebras of binary relations under angelic composition\nand inclusion is finitely axiomatised as the class of ordered semigroups. The\nproof can be used to establish that the same axiomatisation applies to binary\nrelations under demonic composition and refinement, and a further modification\nof the proof can be used to incorporate a zero element representing the empty\nrelation in the angelic case and the full relation in the demonic case. For the\nsignature of angelic composition and union, it is known that no finite\naxiomatisation exists, and we show the analogous result for demonic composition\nand demonic union by showing that the same axiomatisation holds for both. We\nshow that the isomorphism class of algebras of binary relations with the\n\"mixed\" signature of demonic composition and angelic inclusion has no finite\naxiomatisation. As a contrast, we show that the isomorphism class of partial\nalgebras of binary relations with the partial operation of constellation\nproduct and inclusion (also a \"mixed\" signature) is finitely axiomatisable.", "published": "2020-09-25 08:13:07", "link": "http://arxiv.org/abs/2009.12081v2", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Weird AI Yankovic: Generating Parody Lyrics", "abstract": "Lyrics parody swaps one set of words that accompany a melody with a new set\nof words, preserving the number of syllables per line and the rhyme scheme.\nLyrics parody generation is a challenge for controllable text generation. We\nshow how a specialized sampling procedure, combined with backward text\ngeneration with XLNet can produce parody lyrics that reliably meet the syllable\nand rhyme scheme constraints.We introduce the Weird AI Yankovic system and\nprovide a case study evaluation. We conclude with societal implications of\nneural lyric parody generation.", "published": "2020-09-25 13:56:20", "link": "http://arxiv.org/abs/2009.12240v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are scene graphs good enough to improve Image Captioning?", "abstract": "Many top-performing image captioning models rely solely on object features\ncomputed with an object detection model to generate image descriptions.\nHowever, recent studies propose to directly use scene graphs to introduce\ninformation about object relations into captioning, hoping to better describe\ninteractions between objects. In this work, we thoroughly investigate the use\nof scene graphs in image captioning. We empirically study whether using\nadditional scene graph encoders can lead to better image descriptions and\npropose a conditional graph attention network (C-GAT), where the image\ncaptioning decoder state is used to condition the graph updates. Finally, we\ndetermine to what extent noise in the predicted scene graphs influence caption\nquality. Overall, we find no significant difference between models that use\nscene graph features and models that only use object detection features across\ndifferent captioning metrics, which suggests that existing scene graph\ngeneration models are still too noisy to be useful in image captioning.\nMoreover, although the quality of predicted scene graphs is very low in\ngeneral, when using high quality scene graphs we obtain gains of up to 3.3\nCIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to\nreproduce all our experiments in\nhttps://github.com/iacercalixto/butd-image-captioning.", "published": "2020-09-25 16:09:08", "link": "http://arxiv.org/abs/2009.12313v2", "categories": ["cs.CV", "cs.CL", "68T50, 68T45", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Visually Grounded Compound PCFGs", "abstract": "Exploiting visual groundings for language understanding has recently been\ndrawing much attention. In this work, we study visually grounded grammar\ninduction and learn a constituency parser from both unlabeled text and its\nvisual groundings. Existing work on this task (Shi et al., 2019) optimizes a\nparser via Reinforce and derives the learning signal only from the alignment of\nimages and sentences. While their model is relatively accurate overall, its\nerror distribution is very uneven, with low performance on certain constituents\ntypes (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6%\nrecall on noun phrases, NPs). This is not surprising as the learning signal is\nlikely insufficient for deriving all aspects of phrase-structure syntax and\ngradient estimates are noisy. We show that using an extension of probabilistic\ncontext-free grammar model we can do fully-differentiable end-to-end visually\ngrounded learning. Additionally, this enables us to complement the image-text\nalignment loss with a language modeling objective. On the MSCOCO test captions,\nour model establishes a new state of the art, outperforming its non-grounded\nversion and, thus, confirming the effectiveness of visual groundings in\nconstituency grammar induction. It also substantially outperforms the previous\ngrounded model, with largest improvements on more `abstract' categories (e.g.,\n+55.1% recall on VPs).", "published": "2020-09-25 19:07:00", "link": "http://arxiv.org/abs/2009.12404v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Sparse Sentence Encoding without Supervision: An Exploration of\n  Sparsity in Variational Autoencoders", "abstract": "It has been long known that sparsity is an effective inductive bias for\nlearning efficient representation of data in vectors with fixed dimensionality,\nand it has been explored in many areas of representation learning. Of\nparticular interest to this work is the investigation of the sparsity within\nthe VAE framework which has been explored a lot in the image domain, but has\nbeen lacking even a basic level of exploration in NLP. Additionally, NLP is\nalso lagging behind in terms of learning sparse representations of large units\nof text e.g., sentences. We use the VAEs that induce sparse latent\nrepresentations of large units of text to address the aforementioned\nshortcomings. First, we move in this direction by measuring the success of\nunsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification\nbaselines for text and propose a hierarchical sparse VAE model to address the\nstability issue of SOTA. Then, we look at the implications of sparsity on text\nclassification across 3 datasets, and highlight a link between performance of\nsparse latent representations on downstream tasks and its ability to encode\ntask-related information.", "published": "2020-09-25 20:08:32", "link": "http://arxiv.org/abs/2009.12421v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Diagnostic Study of Explainability Techniques for Text Classification", "abstract": "Recent developments in machine learning have introduced models that approach\nhuman performance at the cost of increased architectural complexity. Efforts to\nmake the rationales behind the models' predictions transparent have inspired an\nabundance of new explainability techniques. Provided with an already trained\nmodel, they compute saliency scores for the words of an input instance.\nHowever, there exists no definitive guide on (i) how to choose such a technique\ngiven a particular application task and model architecture, and (ii) the\nbenefits and drawbacks of using each such technique. In this paper, we develop\na comprehensive list of diagnostic properties for evaluating existing\nexplainability techniques. We then employ the proposed list to compare a set of\ndiverse explainability techniques on downstream text classification tasks and\nneural network architectures. We also compare the saliency scores assigned by\nthe explainability techniques with human annotations of salient input regions\nto find relations between a model's performance and the agreement of its\nrationales with human ones. Overall, we find that the gradient-based\nexplanations perform best across tasks and model architectures, and we present\nfurther insights into the properties of the reviewed explainability techniques.", "published": "2020-09-25 12:01:53", "link": "http://arxiv.org/abs/2009.13295v1", "categories": ["cs.CL", "cs.LG", "cs.CL, cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Attention Meets Perturbations: Robust and Interpretable Attention with\n  Adversarial Training", "abstract": "Although attention mechanisms have been applied to a variety of deep learning\nmodels and have been shown to improve the prediction performance, it has been\nreported to be vulnerable to perturbations to the mechanism. To overcome the\nvulnerability to perturbations in the mechanism, we are inspired by adversarial\ntraining (AT), which is a powerful regularization technique for enhancing the\nrobustness of the models. In this paper, we propose a general training\ntechnique for natural language processing tasks, including AT for attention\n(Attention AT) and more interpretable AT for attention (Attention iAT). The\nproposed techniques improved the prediction performance and the model\ninterpretability by exploiting the mechanisms with AT. In particular, Attention\niAT boosts those advantages by introducing adversarial perturbation, which\nenhances the difference in the attention of the sentences. Evaluation\nexperiments with ten open datasets revealed that AT for attention mechanisms,\nespecially Attention iAT, demonstrated (1) the best performance in nine out of\nten tasks and (2) more interpretable attention (i.e., the resulting attention\ncorrelated more strongly with gradient-based word importance) for all tasks.\nAdditionally, the proposed techniques are (3) much less dependent on\nperturbation size in AT. Our code is available at\nhttps://github.com/shunk031/attention-meets-perturbation", "published": "2020-09-25 07:26:45", "link": "http://arxiv.org/abs/2009.12064v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Debiasing NLU Models from Unknown Biases", "abstract": "NLU models often exploit biases to achieve high dataset-specific performance\nwithout properly learning the intended task. Recently proposed debiasing\nmethods are shown to be effective in mitigating this tendency. However, these\nmethods rely on a major assumption that the types of bias should be known\na-priori, which limits their application to many NLU tasks and datasets. In\nthis work, we present the first step to bridge this gap by introducing a\nself-debiasing framework that prevents models from mainly utilizing biases\nwithout knowing them in advance. The proposed framework is general and\ncomplementary to the existing debiasing methods. We show that it allows these\nexisting methods to retain the improvement on the challenge datasets (i.e.,\nsets of examples designed to expose models' reliance on biases) without\nspecifically targeting certain biases. Furthermore, the evaluation suggests\nthat applying the framework results in improved overall robustness.", "published": "2020-09-25 15:49:39", "link": "http://arxiv.org/abs/2009.12303v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RecoBERT: A Catalog Language Model for Text-Based Recommendations", "abstract": "Language models that utilize extensive self-supervised pre-training from\nunlabeled text, have recently shown to significantly advance the\nstate-of-the-art performance in a variety of language understanding tasks.\nHowever, it is yet unclear if and how these recent models can be harnessed for\nconducting text-based recommendations. In this work, we introduce RecoBERT, a\nBERT-based approach for learning catalog-specialized language models for\ntext-based item recommendations. We suggest novel training and inference\nprocedures for scoring similarities between pairs of items, that don't require\nitem similarity labels. Both the training and the inference techniques were\ndesigned to utilize the unlabeled structure of textual catalogs, and minimize\nthe discrepancy between them. By incorporating four scores during inference,\nRecoBERT can infer text-based item-to-item similarities more accurately than\nother techniques. In addition, we introduce a new language understanding task\nfor wine recommendations using similarities based on professional wine reviews.\nAs an additional contribution, we publish annotated recommendations dataset\ncrafted by human wine experts. Finally, we evaluate RecoBERT and compare it to\nvarious state-of-the-art NLP models on wine and fashion recommendations tasks.", "published": "2020-09-25 14:23:38", "link": "http://arxiv.org/abs/2009.13292v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Learning to Match Jobs with Resumes from Sparse Interaction Data using\n  Multi-View Co-Teaching Network", "abstract": "With the ever-increasing growth of online recruitment data, job-resume\nmatching has become an important task to automatically match jobs with suitable\nresumes. This task is typically casted as a supervised text matching problem.\nSupervised learning is powerful when the labeled data is sufficient. However,\non online recruitment platforms, job-resume interaction data is sparse and\nnoisy, which affects the performance of job-resume match algorithms. To\nalleviate these problems, in this paper, we propose a novel multi-view\nco-teaching network from sparse interaction data for job-resume matching. Our\nnetwork consists of two major components, namely text-based matching model and\nrelation-based matching model. The two parts capture semantic compatibility in\ntwo different views, and complement each other. In order to address the\nchallenges from sparse and noisy data, we design two specific strategies to\ncombine the two components. First, two components share the learned parameters\nor representations, so that the original representations of each component can\nbe enhanced. More importantly, we adopt a co-teaching mechanism to reduce the\ninfluence of noise in training data. The core idea is to let the two components\nhelp each other by selecting more reliable training instances. The two\nstrategies focus on representation enhancement and data enhancement,\nrespectively. Compared with pure text-based matching models, the proposed\napproach is able to learn better data representations from limited or even\nsparse interaction data, which is more resistible to noise in training data.\nExperiment results have demonstrated that our model is able to outperform\nstate-of-the-art methods for job-resume matching.", "published": "2020-09-25 03:09:54", "link": "http://arxiv.org/abs/2009.13299v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A consolidated view of loss functions for supervised deep learning-based\n  speech enhancement", "abstract": "Deep learning-based speech enhancement for real-time applications recently\nmade large advancements. Due to the lack of a tractable perceptual optimization\ntarget, many myths around training losses emerged, whereas the contribution to\nsuccess of the loss functions in many cases has not been investigated isolated\nfrom other factors such as network architecture, features, or training\nprocedures. In this work, we investigate a wide variety of loss spectral\nfunctions for a recurrent neural network architecture suitable to operate in\nonline frame-by-frame processing. We relate magnitude-only with phase-aware\nlosses, ratios, correlation metrics, and compressed metrics. Our results reveal\nthat combining magnitude-only with phase-aware objectives always leads to\nimprovements, even when the phase is not enhanced. Furthermore, using\ncompressed spectral values also yields a significant improvement. On the other\nhand, phase-sensitive improvement is best achieved by linear domain losses such\nas mean absolute error.", "published": "2020-09-25 15:17:26", "link": "http://arxiv.org/abs/2009.12286v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Autoencoding GMM-based Unsupervised Anomaly Detection in Acoustic\n  Signals and its Hyper-parameter Optimization", "abstract": "Failures or breakdowns in factory machinery can be costly to companies, so\nthere is an increasing demand for automatic machine inspection. Existing\napproaches to acoustic signal-based unsupervised anomaly detection, such as\nthose using a deep autoencoder (DA) or Gaussian mixture model (GMM), have poor\nanomaly-detection performance. In this work, we propose a new method based on a\ndeep autoencoding Gaussian mixture model with hyper-parameter optimization\n(DAGMM-HO). In our method, the DAGMM-HO applies the conventional DAGMM to the\naudio domain for the first time, with the idea that its total optimization on\nreduction of dimensions and statistical modelling will improve the\nanomaly-detection performance. In addition, the DAGMM-HO solves the\nhyper-parameter sensitivity problem of the conventional DAGMM by performing\nhyper-parameter optimization based on the gap statistic and the cumulative\neigenvalues. Our evaluation of the proposed method with experimental data of\nthe industrial fans showed that it significantly outperforms previous\napproaches and achieves up to a 20% improvement based on the standard AUC\nscore.", "published": "2020-09-25 06:14:59", "link": "http://arxiv.org/abs/2009.12042v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
