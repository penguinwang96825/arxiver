{"title": "Explaining Predictive Uncertainty by Looking Back at Model Explanations", "abstract": "Predictive uncertainty estimation of pre-trained language models is an\nimportant measure of how likely people can trust their predictions. However,\nlittle is known about what makes a model prediction uncertain. Explaining\npredictive uncertainty is an important complement to explaining prediction\nlabels in helping users understand model decision making and gaining their\ntrust on model predictions, while has been largely ignored in prior works. In\nthis work, we propose to explain the predictive uncertainty of pre-trained\nlanguage models by extracting uncertain words from existing model explanations.\nWe find the uncertain words are those identified as making negative\ncontributions to prediction labels, while actually explaining the predictive\nuncertainty. Experiments show that uncertainty explanations are indispensable\nto explaining models and helping humans understand model prediction behavior.", "published": "2022-01-11 02:04:50", "link": "http://arxiv.org/abs/2201.03742v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Robustness to Adversarial Word Substitutions", "abstract": "Deep-learning-based NLP models are found to be vulnerable to word\nsubstitution perturbations. Before they are widely adopted, the fundamental\nissues of robustness need to be addressed. Along this line, we propose a formal\nframework to evaluate word-level robustness. First, to study safe regions for a\nmodel, we introduce robustness radius which is the boundary where the model can\nresist any perturbation. As calculating the maximum robustness radius is\ncomputationally hard, we estimate its upper and lower bound. We repurpose\nattack methods as ways of seeking upper bound and design a pseudo-dynamic\nprogramming algorithm for a tighter upper bound. Then verification method is\nutilized for a lower bound. Further, for evaluating the robustness of regions\noutside a safe radius, we reexamine robustness from another view:\nquantification. A robustness metric with a rigorous statistical guarantee is\nintroduced to measure the quantification of adversarial examples, which\nindicates the model's susceptibility to perturbations outside the safe radius.\nThe metric helps us figure out why state-of-the-art models like BERT can be\neasily fooled by a few word substitutions, but generalize well in the presence\nof real-world noises.", "published": "2022-01-11 08:18:39", "link": "http://arxiv.org/abs/2201.03829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The GINCO Training Dataset for Web Genre Identification of Documents Out\n  in the Wild", "abstract": "This paper presents a new training dataset for automatic genre identification\nGINCO, which is based on 1,125 crawled Slovenian web documents that consist of\n650 thousand words. Each document was manually annotated for genre with a new\nannotation schema that builds upon existing schemata, having primarily clarity\nof labels and inter-annotator agreement in mind. The dataset consists of\nvarious challenges related to web-based data, such as machine translated\ncontent, encoding errors, multiple contents presented in one document etc.,\nenabling evaluation of classifiers in realistic conditions. The initial machine\nlearning experiments on the dataset show that (1) pre-Transformer models are\ndrastically less able to model the phenomena, with macro F1 metrics ranging\naround 0.22, while Transformer-based models achieve scores of around 0.58, and\n(2) multilingual Transformer models work as well on the task as the monolingual\nmodels that were previously proven to be superior to multilingual models on\nstandard NLP tasks.", "published": "2022-01-11 09:39:15", "link": "http://arxiv.org/abs/2201.03857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prior Knowledge Enhances Radiology Report Generation", "abstract": "Radiology report generation aims to produce computer-aided diagnoses to\nalleviate the workload of radiologists and has drawn increasing attention\nrecently. However, previous deep learning methods tend to neglect the mutual\ninfluences between medical findings, which can be the bottleneck that limits\nthe quality of generated reports. In this work, we propose to mine and\nrepresent the associations among medical findings in an informative knowledge\ngraph and incorporate this prior knowledge with radiology report generation to\nhelp improve the quality of generated reports. Experiment results demonstrate\nthe superior performance of our proposed method on the IU X-ray dataset with a\nROUGE-L of 0.384$\\pm$0.007 and CIDEr of 0.340$\\pm$0.011. Compared with previous\nworks, our model achieves an average of 1.6% improvement (2.0% and 1.5%\nimprovements in CIDEr and ROUGE-L, respectively). The experiments suggest that\nprior knowledge can bring performance gains to accurate radiology report\ngeneration. We will make the code publicly available at\nhttps://github.com/bionlplab/report_generation_amia2022.", "published": "2022-01-11 03:32:29", "link": "http://arxiv.org/abs/2201.03761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command\n  Recognition", "abstract": "With the rise of deep learning and intelligent vehicle, the smart assistant\nhas become an essential in-car component to facilitate driving and provide\nextra functionalities. In-car smart assistants should be able to process\ngeneral as well as car-related commands and perform corresponding actions,\nwhich eases driving and improves safety. However, there is a data scarcity\nissue for low resource languages, hindering the development of research and\napplications. In this paper, we introduce a new dataset, Cantonese In-car\nAudio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in\nthe Cantonese language with both video and audio data. It consists of 4,984\nsamples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese\nspeakers. Furthermore, we augment our dataset using common in-car background\nnoises to simulate real environments, producing a dataset 10 times larger than\nthe collected one. We provide detailed statistics of both the clean and the\naugmented versions of our dataset. Moreover, we implement two multimodal\nbaselines to demonstrate the validity of CI-AVSR. Experiment results show that\nleveraging the visual signal improves the overall performance of the model.\nAlthough our best model can achieve a considerable quality on the clean test\nset, the speech recognition quality on the noisy data is still inferior and\nremains as an extremely challenging task for real in-car speech recognition\nsystems. The dataset and code will be released at\nhttps://github.com/HLTCHKUST/CI-AVSR.", "published": "2022-01-11 06:32:12", "link": "http://arxiv.org/abs/2201.03804v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Turkish Sentiment Analysis Using Machine Learning Methods: Application\n  on Online Food Order Site Reviews", "abstract": "Satisfaction measurement, which emerges in every sector today, is a very\nimportant factor for many companies. In this study, it is aimed to reach the\nhighest accuracy rate with various machine learning algorithms by using the\ndata on Yemek Sepeti and variations of this data. The accuracy values of each\nalgorithm were calculated together with the various natural language processing\nmethods used. While calculating these accuracy values, the parameters of the\nalgorithms used were tried to be optimized. The models trained in this study on\nlabeled data can be used on unlabeled data and can give companies an idea in\nmeasuring customer satisfaction. It was observed that 3 different natural\nlanguage processing methods applied resulted in approximately 5% accuracy\nincrease in most of the developed models.", "published": "2022-01-11 09:13:56", "link": "http://arxiv.org/abs/2201.03848v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis with Deep Learning Models: A Comparative Study on a\n  Decade of Sinhala Language Facebook Data", "abstract": "The relationship between Facebook posts and the corresponding reaction\nfeature is an interesting subject to explore and understand. To achieve this\nend, we test state-of-the-art Sinhala sentiment analysis models against a data\nset containing a decade worth of Sinhala posts with millions of reactions. For\nthe purpose of establishing benchmarks and with the goal of identifying the\nbest model for Sinhala sentiment analysis, we also test, on the same data set\nconfiguration, other deep learning models catered for sentiment analysis. In\nthis study we report that the 3 layer Bidirectional LSTM model achieves an F1\nscore of 84.58% for Sinhala sentiment analysis, surpassing the current\nstate-of-the-art model; Capsule B, which only manages to get an F1 score of\n82.04%. Further, since all the deep learning models show F1 scores above 75% we\nconclude that it is safe to claim that Facebook reactions are suitable to\npredict the sentiment of a text.", "published": "2022-01-11 13:31:15", "link": "http://arxiv.org/abs/2201.03941v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Scale Analysis of Open MOOC Reviews to Support Learners' Course\n  Selection", "abstract": "The recent pandemic has changed the way we see education. It is not\nsurprising that children and college students are not the only ones using\nonline education. Millions of adults have signed up for online classes and\ncourses during last years, and MOOC providers, such as Coursera or edX, are\nreporting millions of new users signing up in their platforms. However,\nstudents do face some challenges when choosing courses. Though online review\nsystems are standard among many verticals, no standardized or fully\ndecentralized review systems exist in the MOOC ecosystem. In this vein, we\nbelieve that there is an opportunity to leverage available open MOOC reviews in\norder to build simpler and more transparent reviewing systems, allowing users\nto really identify the best courses out there. Specifically, in our research we\nanalyze 2.4 million reviews (which is the largest MOOC reviews dataset used\nuntil now) from five different platforms in order to determine the following:\n(1) if the numeric ratings provide discriminant information to learners, (2) if\nNLP-driven sentiment analysis on textual reviews could provide valuable\ninformation to learners, (3) if we can leverage NLP-driven topic finding\ntechniques to infer themes that could be important for learners, and (4) if we\ncan use these models to effectively characterize MOOCs based on the open\nreviews. Results show that numeric ratings are clearly biased (63\\% of them are\n5-star ratings), and the topic modeling reveals some interesting topics related\nwith course advertisements, the real applicability, or the difficulty of the\ndifferent courses. We expect our study to shed some light on the area and\npromote a more transparent approach in online education reviews, which are\nbecoming more and more popular as we enter the post-pandemic era.", "published": "2022-01-11 10:24:49", "link": "http://arxiv.org/abs/2201.06967v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "CVSS Corpus and Massively Multilingual Speech-to-Speech Translation", "abstract": "We introduce CVSS, a massively multilingual-to-English speech-to-speech\ntranslation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21\nlanguages into English. CVSS is derived from the Common Voice speech corpus and\nthe CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the\ntranslation text from CoVoST 2 into speech using state-of-the-art TTS systems.\nTwo versions of translation speeches are provided: 1) CVSS-C: All the\ntranslation speeches are in a single high-quality canonical voice; 2) CVSS-T:\nThe translation speeches are in voices transferred from the corresponding\nsource speeches. In addition, CVSS provides normalized translation text which\nmatches the pronunciation in the translation speech. On each version of CVSS,\nwe built baseline multilingual direct S2ST models and cascade S2ST models,\nverifying the effectiveness of the corpus. To build strong cascade S2ST\nbaselines, we trained an ST model on CoVoST 2, which outperforms the previous\nstate-of-the-art trained on the corpus without extra data by 5.8 BLEU.\nNevertheless, the performance of the direct S2ST models approaches the strong\ncascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU\ndifference on ASR transcribed translation when initialized from matching ST\nmodels.", "published": "2022-01-11 00:27:08", "link": "http://arxiv.org/abs/2201.03713v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular\n  Vision-Language Pre-training", "abstract": "Vision-language pre-training has been an emerging and fast-developing\nresearch topic, which transfers multi-modal knowledge from rich-resource\npre-training task to limited-resource downstream tasks. Unlike existing works\nthat predominantly learn a single generic encoder, we present a pre-trainable\nUniversal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language\nperception (e.g., visual question answering) and generation (e.g., image\ncaptioning). Uni-EDEN is a two-stream Transformer based structure, consisting\nof three modules: object and sentence encoders that separately learns the\nrepresentations of each modality, and sentence decoder that enables both\nmulti-modal reasoning and sentence generation via inter-modal interaction.\nConsidering that the linguistic representations of each image can span\ndifferent granularities in this hierarchy including, from simple to\ncomprehensive, individual label, a phrase, and a natural sentence, we pre-train\nUni-EDEN through multi-granular vision-language proxy tasks: Masked Object\nClassification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence\nMatching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is\nendowed with the power of both multi-modal representation extraction and\nlanguage modeling. Extensive experiments demonstrate the compelling\ngeneralizability of Uni-EDEN by fine-tuning it to four vision-language\nperception and generation downstream tasks.", "published": "2022-01-11 16:15:07", "link": "http://arxiv.org/abs/2201.04026v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Feature Extraction based Model for Hate Speech Identification", "abstract": "The detection of hate speech online has become an important task, as\noffensive language such as hurtful, obscene and insulting content can harm\nmarginalized people or groups. This paper presents TU Berlin team experiments\nand results on the task 1A and 1B of the shared task on hate speech and\noffensive content identification in Indo-European languages 2021. The success\nof different Natural Language Processing models is evaluated for the respective\nsubtasks throughout the competition. We tested different models based on\nrecurrent neural networks in word and character levels and transfer learning\napproaches based on Bert on the provided dataset by the competition. Among the\ntested models that have been used for the experiments, the transfer\nlearning-based models achieved the best results in both subtasks.", "published": "2022-01-11 22:53:28", "link": "http://arxiv.org/abs/2201.04227v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MR-SVS: Singing Voice Synthesis with Multi-Reference Encoder", "abstract": "Multi-speaker singing voice synthesis is to generate the singing voice sung\nby different speakers. To generalize to new speakers, previous zero-shot\nsinging adaptation methods obtain the timbre of the target speaker with a\nfixed-size embedding from single reference audio. However, they face several\nchallenges: 1) the fixed-size speaker embedding is not powerful enough to\ncapture full details of the target timbre; 2) single reference audio does not\ncontain sufficient timbre information of the target speaker; 3) the pitch\ninconsistency between different speakers also leads to a degradation in the\ngenerated voice. In this paper, we propose a new model called MR-SVS to tackle\nthese problems. Specifically, we employ both a multi-reference encoder and a\nfixed-size encoder to encode the timbre of the target speaker from multiple\nreference audios. The Multi-reference encoder can capture more details and\nvariations of the target timbre. Besides, we propose a well-designed pitch\nshift method to address the pitch inconsistency problem. Experiments indicate\nthat our method outperforms the baseline method both in naturalness and\nsimilarity.", "published": "2022-01-11 09:58:39", "link": "http://arxiv.org/abs/2201.03864v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Enhance or Not: Neural Network-Based Switching of Enhanced\n  and Observed Signals for Overlapping Speech Recognition", "abstract": "The combination of a deep neural network (DNN) -based speech enhancement (SE)\nfront-end and an automatic speech recognition (ASR) back-end is a widely used\napproach to implement overlapping speech recognition. However, the SE front-end\ngenerates processing artifacts that can degrade the ASR performance. We\npreviously found that such performance degradation can occur even under fully\noverlapping conditions, depending on the signal-to-interference ratio (SIR) and\nsignal-to-noise ratio (SNR). To mitigate the degradation, we introduced a\nrule-based method to switch the ASR input between the enhanced and observed\nsignals, which showed promising results. However, the rule's optimality was\nunclear because it was heuristically designed and based only on SIR and SNR\nvalues. In this work, we propose a DNN-based switching method that directly\nestimates whether ASR will perform better on the enhanced or observed signals.\nWe also introduce soft-switching that computes a weighted sum of the enhanced\nand observed signals for ASR input, with weights given by the switching model's\noutput posteriors. The proposed learning-based switching showed performance\ncomparable to that of rule-based oracle switching. The soft-switching further\nimproved the ASR performance and achieved a relative character error rate\nreduction of up to 23 % as compared with the conventional method.", "published": "2022-01-11 10:58:09", "link": "http://arxiv.org/abs/2201.03881v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music2Video: Automatic Generation of Music Video with fusion of audio\n  and text", "abstract": "Creation of images using generative adversarial networks has been widely\nadapted into multi-modal regime with the advent of multi-modal representation\nmodels pre-trained on large corpus. Various modalities sharing a common\nrepresentation space could be utilized to guide the generative models to create\nimages from text or even from audio source. Departing from the previous methods\nthat solely rely on either text or audio, we exploit the expressiveness of both\nmodality. Based on the fusion of text and audio, we create video whose content\nis consistent with the distinct modalities that are provided. A simple approach\nto automatically segment the video into variable length intervals and maintain\ntime consistency in generated video is part of our method. Our proposed\nframework for generating music video shows promising results in application\nlevel where users can interactively feed in music source and text source to\ncreate artistic music videos. Our code is available at\nhttps://github.com/joeljang/music2video.", "published": "2022-01-11 06:59:21", "link": "http://arxiv.org/abs/2201.03809v2", "categories": ["cs.SD", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Hybrid Models of Tensor-Train Networks for Spoken Command\n  Recognition", "abstract": "This work aims to design a low complexity spoken command recognition (SCR)\nsystem by considering different trade-offs between the number of model\nparameters and classification accuracy. More specifically, we exploit a deep\nhybrid architecture of a tensor-train (TT) network to build an end-to-end SRC\npipeline. Our command recognition system, namely CNN+(TT-DNN), is composed of\nconvolutional layers at the bottom for spectral feature extraction and TT\nlayers at the top for command classification. Compared with a traditional\nend-to-end CNN baseline for SCR, our proposed CNN+(TT-DNN) model replaces fully\nconnected (FC) layers with TT ones and it can substantially reduce the number\nof model parameters while maintaining the baseline performance of the CNN\nmodel. We initialize the CNN+(TT-DNN) model in a randomized manner or based on\na well-trained CNN+DNN, and assess the CNN+(TT-DNN) models on the Google Speech\nCommand Dataset. Our experimental results show that the proposed CNN+(TT-DNN)\nmodel attains a competitive accuracy of 96.31% with 4 times fewer model\nparameters than the CNN model. Furthermore, the CNN+(TT-DNN) model can obtain a\n97.2% accuracy when the number of parameters is increased.", "published": "2022-01-11 05:57:38", "link": "http://arxiv.org/abs/2201.10609v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
