{"title": "No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for\n  Training Large Transformer Models", "abstract": "Recent research has shown the existence of significant redundancy in large\nTransformer models. One can prune the redundant parameters without\nsignificantly sacrificing the generalization performance. However, we question\nwhether the redundant parameters could have contributed more if they were\nproperly trained. To answer this question, we propose a novel training strategy\nthat encourages all parameters to be trained sufficiently. Specifically, we\nadaptively adjust the learning rate for each parameter according to its\nsensitivity, a robust gradient-based measure reflecting this parameter's\ncontribution to the model performance. A parameter with low sensitivity is\nredundant, and we improve its fitting by increasing its learning rate. In\ncontrast, a parameter with high sensitivity is well-trained, and we regularize\nit by decreasing its learning rate to prevent further overfitting. We conduct\nextensive experiments on natural language understanding, neural machine\ntranslation, and image classification to demonstrate the effectiveness of the\nproposed schedule. Analysis shows that the proposed schedule indeed reduces the\nredundancy and improves generalization performance.", "published": "2022-02-06 00:22:28", "link": "http://arxiv.org/abs/2202.02664v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Effective is Incongruity? Implications for Code-mix Sarcasm\n  Detection", "abstract": "The presence of sarcasm in conversational systems and social media like\nchatbots, Facebook, Twitter, etc. poses several challenges for downstream NLP\ntasks. This is attributed to the fact that the intended meaning of a sarcastic\ntext is contrary to what is expressed. Further, the use of code-mix language to\nexpress sarcasm is increasing day by day. Current NLP techniques for code-mix\ndata have limited success due to the use of different lexicon, syntax, and\nscarcity of labeled corpora. To solve the joint problem of code-mixing and\nsarcasm detection, we propose the idea of capturing incongruity through\nsub-word level embeddings learned via fastText. Empirical results shows that\nour proposed model achieves F1-score on code-mix Hinglish dataset comparable to\npretrained multilingual models while training 10x faster and using a lower\nmemory footprint", "published": "2022-02-06 04:05:09", "link": "http://arxiv.org/abs/2202.02702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating natural language processing models with generalization\n  metrics that do not need access to any training or testing data", "abstract": "Selecting suitable architecture parameters and training hyperparameters is\nessential for enhancing machine learning (ML) model performance. Several recent\nempirical studies conduct large-scale correlational analysis on neural networks\n(NNs) to search for effective \\emph{generalization metrics} that can guide this\ntype of model selection. Effective metrics are typically expected to correlate\nstrongly with test performance. In this paper, we expand on prior analyses by\nexamining generalization-metric-based model selection with the following\nobjectives: (i) focusing on natural language processing (NLP) tasks, as prior\nwork primarily concentrates on computer vision (CV) tasks; (ii) considering\nmetrics that directly predict \\emph{test error} instead of the\n\\emph{generalization gap}; (iii) exploring metrics that do not need access to\ndata to compute. From these objectives, we are able to provide the first model\nselection results on large pretrained Transformers from Huggingface using\ngeneralization metrics. Our analyses consider (I) hundreds of Transformers\ntrained in different settings, in which we systematically vary the amount of\ndata, the model size and the optimization hyperparameters, (II) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding GPT2, BERT, etc., and (III) a total of 28 existing and novel\ngeneralization metrics. Despite their niche status, we find that metrics\nderived from the heavy-tail (HT) perspective are particularly useful in NLP\ntasks, exhibiting stronger correlations than other, more popular metrics. To\nfurther examine these metrics, we extend prior formulations relying on power\nlaw (PL) spectral distributions to exponential (EXP) and\nexponentially-truncated power law (E-TPL) families.", "published": "2022-02-06 20:07:35", "link": "http://arxiv.org/abs/2202.02842v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Channel Attention-Based Target Speaker Voice Activity Detection:\n  Experimental Results for M2MeT Challenge", "abstract": "In this paper, we present the speaker diarization system for the\nMulti-channel Multi-party Meeting Transcription Challenge (M2MeT) from team\nDKU_DukeECE. As the highly overlapped speech exists in the dataset, we employ\nan x-vector-based target-speaker voice activity detection (TS-VAD) to find the\noverlap between speakers. For the single-channel scenario, we separately train\na model for each of the 8 channels and fuse the results. We also employ the\ncross-channel self-attention to further improve the performance, where the\nnon-linear spatial correlations between different channels are learned and\nfused. Experimental results on the evaluation set show that the single-channel\nTS-VAD reduces the DER by over 75% from 12.68\\% to 3.14%. The multi-channel\nTS-VAD further reduces the DER by 28% and achieves a DER of 2.26%. Our final\nsubmitted system achieves a DER of 2.98% on the AliMeeting test set, which\nranks 1st in the M2MET challenge.", "published": "2022-02-06 02:44:07", "link": "http://arxiv.org/abs/2202.02687v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tubes Among Us: Analog Attack on Automatic Speaker Identification", "abstract": "Recent years have seen a surge in the popularity of acoustics-enabled\npersonal devices powered by machine learning. Yet, machine learning has proven\nto be vulnerable to adversarial examples. A large number of modern systems\nprotect themselves against such attacks by targeting artificiality, i.e., they\ndeploy mechanisms to detect the lack of human involvement in generating the\nadversarial examples. However, these defenses implicitly assume that humans are\nincapable of producing meaningful and targeted adversarial examples. In this\npaper, we show that this base assumption is wrong. In particular, we\ndemonstrate that for tasks like speaker identification, a human is capable of\nproducing analog adversarial examples directly with little cost and\nsupervision: by simply speaking through a tube, an adversary reliably\nimpersonates other speakers in eyes of ML models for speaker identification.\nOur findings extend to a range of other acoustic-biometric tasks such as\nliveness detection, bringing into question their use in security-critical\nsettings in real life, such as phone banking.", "published": "2022-02-06 10:33:13", "link": "http://arxiv.org/abs/2202.02751v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Exploring Self-Attention Mechanisms for Speech Separation", "abstract": "Transformers have enabled impressive improvements in deep learning. They\noften outperform recurrent and convolutional models in many tasks while taking\nadvantage of parallel processing. Recently, we proposed the SepFormer, which\nobtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix\ndatasets. This paper studies in-depth Transformers for speech separation. In\nparticular, we extend our previous findings on the SepFormer by providing\nresults on more challenging noisy and noisy-reverberant datasets, such as\nLibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech\nenhancement and provide experimental evidence on denoising and dereverberation\ntasks. Finally, we investigate, for the first time in speech separation, the\nuse of efficient self-attention mechanisms such as Linformers, Lonformers, and\nReFormers. We found that they reduce memory requirements significantly. For\nexample, we show that the Reformer-based attention outperforms the popular\nConv-TasNet model on the WSJ0-2Mix dataset while being faster at inference and\ncomparable in terms of memory consumption.", "published": "2022-02-06 23:13:27", "link": "http://arxiv.org/abs/2202.02884v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
