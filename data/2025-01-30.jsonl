{"title": "On non-uniqueness in the option valuation problem", "abstract": "It is known that the value of a call option in the case of constant\nelasticity processes (CEV) with the indicator $\\alpha$ exceeding the critical\n$\\alpha=1$ is determined in a non-unique way.\n  We show how, based on an already existing mathematical theory concerning the\ncorrectness of boundary conditions for degenerate parabolic equations on the\nsemi-axis $[0,\\infty)$, this phenomenon can be explained. Namely, for\n$1<\\alpha\\le \\frac32$ the non-uniqueness is due to the fact that the initial\ndata of the call option are outside the T\\\"acklind class, and for $\\alpha>\n\\frac32$ it is due to the absence boundary condition for $x=\\infty$.", "published": "2025-01-30 19:50:11", "link": "http://arxiv.org/abs/2501.18721v1", "categories": ["math.AP", "q-fin.MF", "35K65 35G16 35A02"], "primary_category": "math.AP"}
{"title": "Diverse Preference Optimization", "abstract": "Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. In this work we introduce Diverse Preference Optimization (DivPO),\nan optimization method which learns to generate much more diverse responses\nthan standard pipelines, while maintaining the quality of the generations. In\nDivPO, preference pairs are selected by first considering a pool of responses,\nand a measure of diversity among them, and selecting chosen examples as being\nmore rare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\nan 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines.", "published": "2025-01-30 02:47:41", "link": "http://arxiv.org/abs/2501.18101v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models", "abstract": "Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions.", "published": "2025-01-30 05:39:01", "link": "http://arxiv.org/abs/2501.18154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextually Structured Token Dependency Encoding for Large Language\n  Models", "abstract": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation.", "published": "2025-01-30 08:51:48", "link": "http://arxiv.org/abs/2501.18205v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Select Datapoints for Efficient Human Evaluation of NLG Models?", "abstract": "Human evaluation is the gold-standard for evaluating text generation models.\nIt is also expensive, and to fit budgetary constraints, a random subset of the\ntest data is often chosen in practice. The randomly selected data may not\naccurately represent test performance, making this approach economically\ninefficient for model comparison. Thus, in this work, we develop a suite of\nselectors to get the most informative datapoints for human evaluation while\ntaking the evaluation costs into account. We show that selectors based on\nvariance in automated metric scores, diversity in model outputs, or Item\nResponse Theory outperform random selection. We further develop an approach to\ndistill these selectors to the scenario where the model outputs are not yet\navailable. In particular, we introduce source-based estimators, which predict\nitem usefulness for human evaluation just based on the source texts. We\ndemonstrate the efficacy of our selectors in two common NLG tasks, machine\ntranslation and summarization, and show that up to only ~50% of the test data\nis needed to produce the same evaluation result as the entire data. Our\nimplementations are published in the subset2evaluate package.", "published": "2025-01-30 10:33:26", "link": "http://arxiv.org/abs/2501.18251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GENIE: Generative Note Information Extraction model for structuring EHR\n  data", "abstract": "Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization.", "published": "2025-01-30 15:42:24", "link": "http://arxiv.org/abs/2501.18435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering", "abstract": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and\nretrieval-augmented settings. We also found that increasing the number of\nlanguages involved in CALM training leads to higher accuracy and consistency.\nWe offer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability.", "published": "2025-01-30 16:15:38", "link": "http://arxiv.org/abs/2501.18457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch", "abstract": "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.", "published": "2025-01-30 17:23:50", "link": "http://arxiv.org/abs/2501.18512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs", "abstract": "Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.", "published": "2025-01-30 18:58:18", "link": "http://arxiv.org/abs/2501.18585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Large Language Models for Long Clinical Text Summarization\n  with Temporal Reasoning", "abstract": "Recent advancements in large language models (LLMs) have shown potential for\ntransforming data processing in healthcare, particularly in understanding\ncomplex clinical narratives. This study evaluates the efficacy of zero-shot\nLLMs in summarizing long clinical texts that require temporal reasoning, a\ncritical aspect for comprehensively capturing patient histories and treatment\ntrajectories. We applied a series of advanced zero-shot LLMs to extensive\nclinical documents, assessing their ability to integrate and accurately reflect\ntemporal dynamics without prior task-specific training. While the models\nefficiently identified key temporal events, they struggled with chronological\ncoherence over prolonged narratives. The evaluation, combining quantitative and\nqualitative methods, highlights the strengths and limitations of zero-shot LLMs\nin clinical text summarization. The results suggest that while promising,\nzero-shot LLMs require further refinement to effectively support clinical\ndecision-making processes, underscoring the need for enhanced model training\napproaches that better capture the nuances of temporal information in long\ncontext medical documents.", "published": "2025-01-30 19:58:45", "link": "http://arxiv.org/abs/2501.18724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Robustness of Large Language Models across Language\n  Complexity", "abstract": "With the advancement of large language models (LLMs), an increasing number of\nstudent models have leveraged LLMs to analyze textual artifacts generated by\nstudents to understand and evaluate their learning. These student models\ntypically employ pre-trained LLMs to vectorize text inputs into embeddings and\nthen use the embeddings to train models to detect the presence or absence of a\nconstruct of interest. However, how reliable and robust are these models at\nprocessing language with different levels of complexity? In the context of\nlearning where students may have different language backgrounds with various\nlevels of writing skills, it is critical to examine the robustness of such\nmodels to ensure that these models work equally well for text with varying\nlevels of language complexity. Coincidentally, a few (but limited) research\nstudies show that the use of language can indeed impact the performance of\nLLMs. As such, in the current study, we examined the robustness of several\nLLM-based student models that detect student self-regulated learning (SRL) in\nmath problem-solving. Specifically, we compared how the performance of these\nmodels vary using texts with high and low lexical, syntactic, and semantic\ncomplexity measured by three linguistic measures.", "published": "2025-01-30 20:33:59", "link": "http://arxiv.org/abs/2501.18738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy", "abstract": "Long-context large language models (LLMs) have achieved remarkable\nadvancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et\nal., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et\nal., 2023). By adjusting RoPE parameters and incorporating training data with\nextended contexts, we can train performant models with considerably longer\ninput sequences. However, existing RoPE-based methods exhibit performance\nlimitations when applied to extended context lengths. This paper presents a\ncomprehensive analysis of various attention mechanisms, including RoPE, No\nPositional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying\ntheir strengths and shortcomings in long-context modeling. Our investigation\nidentifies distinctive attention patterns in these methods and highlights their\nimpact on long-context performance, providing valuable insights for\narchitectural design. Building on these findings, we propose a novel\narchitectural based on a hybrid attention mechanism that not only surpasses\nconventional RoPE-based transformer models in long context tasks but also\nachieves competitive performance on benchmarks requiring shorter context\nlengths.", "published": "2025-01-30 23:05:57", "link": "http://arxiv.org/abs/2501.18795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of\n  Large Language Models", "abstract": "FinanceQA is a testing suite that evaluates LLMs' performance on complex\nnumerical financial analysis tasks that mirror real-world investment work.\nDespite recent advances, current LLMs fail to meet the strict accuracy\nrequirements of financial institutions, with models failing approximately 60%\nof realistic tasks that mimic on-the-job analyses at hedge funds, private\nequity firms, investment banks, and other financial institutions. The primary\nchallenges include hand-spreading metrics, adhering to standard accounting and\ncorporate valuation conventions, and performing analysis under incomplete\ninformation - particularly in multi-step tasks requiring assumption generation.\nThis performance gap highlights the disconnect between existing LLM\ncapabilities and the demands of professional financial analysis that are\ninadequately tested by current testing architectures. Results show that\nhigher-quality training data is needed to support such tasks, which we\nexperiment with using OpenAI's fine-tuning API. FinanceQA is publicly released\nat [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA).", "published": "2025-01-30 00:06:55", "link": "http://arxiv.org/abs/2501.18062v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge", "abstract": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models.", "published": "2025-01-30 02:21:59", "link": "http://arxiv.org/abs/2501.18099v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via\n  Post-fine-tuning Perturbation", "abstract": "Harmful fine-tuning attack introduces significant security risks to the\nfine-tuning services. Mainstream defenses aim to vaccinate the model such that\nthe later harmful fine-tuning attack is less effective. However, our evaluation\nresults show that such defenses are fragile -- with a few fine-tuning steps,\nthe model still can learn the harmful knowledge. To this end, we do further\nexperiment and find that an embarrassingly simple solution -- adding purely\nrandom perturbations to the fine-tuned model, can recover the model from\nharmful behavior, though it leads to a degradation in the model's fine-tuning\nperformance. To address the degradation of fine-tuning performance, we further\npropose Panacea, which optimizes an adaptive perturbation that will be applied\nto the model after fine-tuning. Panacea maintains model's safety alignment\nperformance without compromising downstream fine-tuning performance.\nComprehensive experiments are conducted on different harmful ratios,\nfine-tuning tasks and mainstream LLMs, where the average harmful scores are\nreduced by up-to 21.5%, while maintaining fine-tuning performance. As a\nby-product, we analyze the optimized perturbation and show that different\nlayers in various LLMs have distinct safety coefficients. Source code available\nat https://github.com/w-yibo/Panacea", "published": "2025-01-30 02:47:09", "link": "http://arxiv.org/abs/2501.18100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM\n  Interactions", "abstract": "Traditional text-based human-AI interactions often adhere to a strict\nturn-taking approach. In this research, we propose a novel approach that\nincorporates overlapping messages, mirroring natural human conversations.\nThrough a formative study, we observed that even in text-based contexts, users\ninstinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B:\nyeah.\" To capitalize on these insights, we developed OverlapBot, a prototype\nchatbot where both AI and users can initiate overlapping. Our user study\nrevealed that OverlapBot was perceived as more communicative and immersive than\ntraditional turn-taking chatbot, fostering faster and more natural\ninteractions. Our findings contribute to the understanding of design space for\noverlapping interactions. We also provide recommendations for implementing\noverlap-capable AI interactions to enhance the fluidity and engagement of\ntext-based conversations.", "published": "2025-01-30 03:01:01", "link": "http://arxiv.org/abs/2501.18103v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Self-supervised Quantized Representation for Seamlessly Integrating\n  Knowledge Graphs with Large Language Models", "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.", "published": "2025-01-30 03:40:20", "link": "http://arxiv.org/abs/2501.18119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unraveling the Capabilities of Language Models in News Summarization", "abstract": "Given the recent introduction of multiple language models and the ongoing\ndemand for improved Natural Language Processing tasks, particularly\nsummarization, this work provides a comprehensive benchmarking of 20 recent\nlanguage models, focusing on smaller ones for the news summarization task. In\nthis work, we systematically test the capabilities and effectiveness of these\nmodels in summarizing news article texts which are written in different styles\nand presented in three distinct datasets. Specifically, we focus in this study\non zero-shot and few-shot learning settings and we apply a robust evaluation\nmethodology that combines different evaluation concepts including automatic\nmetrics, human evaluation, and LLM-as-a-judge. Interestingly, including\ndemonstration examples in the few-shot learning setting did not enhance models'\nperformance and, in some cases, even led to worse quality of the generated\nsummaries. This issue arises mainly due to the poor quality of the gold\nsummaries that have been used as reference summaries, which negatively impacts\nthe models' performance. Furthermore, our study's results highlight the\nexceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate\ndue to their advanced capabilities. However, among the public models evaluated,\ncertain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B\nand Zephyr-7B-Beta demonstrated promising results. These models showed\nsignificant potential, positioning them as competitive alternatives to large\nmodels for the task of news summarization.", "published": "2025-01-30 04:20:16", "link": "http://arxiv.org/abs/2501.18128v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Video-grounded Dialogue Dataset and Metric for Event-driven Activities", "abstract": "This paper presents VDAct, a dataset for a Video-grounded Dialogue on\nEvent-driven Activities, alongside VDEval, a session-based context evaluation\nmetric specially designed for the task. Unlike existing datasets, VDAct\nincludes longer and more complex video sequences that depict a variety of\nevent-driven activities that require advanced contextual understanding for\naccurate response generation. The dataset comprises 3,000 dialogues with over\n30,000 question-and-answer pairs, derived from 1,000 videos with diverse\nactivity scenarios. VDAct displays a notably challenging characteristic due to\nits broad spectrum of activity scenarios and wide range of question types.\nEmpirical studies on state-of-the-art vision foundation models highlight their\nlimitations in addressing certain question types on our dataset. Furthermore,\nVDEval, which integrates dialogue session history and video content summaries\nextracted from our supplementary Knowledge Graphs to evaluate individual\nresponses, demonstrates a significantly higher correlation with human\nassessments on the VDAct dataset than existing evaluation metrics that rely\nsolely on the context of single dialogue turns.", "published": "2025-01-30 13:11:19", "link": "http://arxiv.org/abs/2501.18324v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques.", "published": "2025-01-30 14:15:09", "link": "http://arxiv.org/abs/2501.18365v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training", "abstract": "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.", "published": "2025-01-30 17:21:44", "link": "http://arxiv.org/abs/2501.18511v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Differentially Private Steering for Large Language Model Alignment", "abstract": "Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques.", "published": "2025-01-30 17:58:36", "link": "http://arxiv.org/abs/2501.18532v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Spoken Language as a Biomarker for Automated Screening of\n  Cognitive Impairment", "abstract": "Timely and accurate assessment of cognitive impairment is a major unmet need\nin populations at risk. Alterations in speech and language can be early\npredictors of Alzheimer's disease and related dementias (ADRD) before clinical\nsigns of neurodegeneration. Voice biomarkers offer a scalable and non-invasive\nsolution for automated screening. However, the clinical applicability of\nmachine learning (ML) remains limited by challenges in generalisability,\ninterpretability, and access to patient data to train clinically applicable\npredictive models. Using DementiaBank recordings (N=291, 64% female), we\nevaluated ML techniques for ADRD screening and severity prediction from spoken\nlanguage. We validated model generalisability with pilot data collected\nin-residence from older adults (N=22, 59% female). Risk stratification and\nlinguistic feature importance analysis enhanced the interpretability and\nclinical utility of predictions. For ADRD classification, a Random Forest\napplied to lexical features achieved a mean sensitivity of 69.4% (95%\nconfidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On\nreal-world pilot data, this model achieved a mean sensitivity of 70.0%\n(58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using\nMini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved\na mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3\n(3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk\nincluded increased use of pronouns and adverbs, greater disfluency, reduced\nanalytical thinking, lower lexical diversity and fewer words reflecting a\npsychological state of completion. Our interpretable predictive modelling\noffers a novel approach for in-home integration with conversational AI to\nmonitor cognitive health and triage higher-risk individuals, enabling earlier\ndetection and intervention.", "published": "2025-01-30 20:17:17", "link": "http://arxiv.org/abs/2501.18731v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity\n  Recognition in Low-Resource Languages", "abstract": "Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer\nbetween languages to identify and classify named entities, making it\nparticularly useful for low-resource languages. We show that the data-based\ncross-lingual transfer method is an effective technique for crosslingual NER\nand can outperform multilingual language models for low-resource languages.\nThis paper introduces two key enhancements to the annotation projection step in\ncross-lingual NER for low-resource languages. First, we explore refining word\nalignments using back-translation to improve accuracy. Second, we present a\nnovel formalized projection approach of matching source entities with extracted\ntarget candidates. Through extensive experiments on two datasets spanning 57\nlanguages, we demonstrated that our approach surpasses existing projectionbased\nmethods in low-resource settings. These findings highlight the robustness of\nprojection-based data transfer as an alternative to model-based methods for\ncrosslingual named entity recognition in lowresource languages.", "published": "2025-01-30 21:00:47", "link": "http://arxiv.org/abs/2501.18750v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Breaking the Fake News Barrier: Deep Learning Approaches in Bangla\n  Language", "abstract": "The rapid development of digital stages has greatly compounded the dispersal\nof untrue data, dissolving certainty and judgment in society, especially among\nthe Bengali-speaking community. Our ponder addresses this critical issue by\npresenting an interesting strategy that utilizes a profound learning\ninnovation, particularly the Gated Repetitive Unit (GRU), to recognize fake\nnews within the Bangla dialect. The strategy of our proposed work incorporates\nintensive information preprocessing, which includes lemmatization,\ntokenization, and tending to course awkward nature by oversampling. This comes\nabout in a dataset containing 58,478 passages. We appreciate the creation of a\ndemonstration based on GRU (Gated Repetitive Unit) that illustrates remarkable\nexecution with a noteworthy precision rate of 94%. This ponder gives an\nintensive clarification of the methods included in planning the information,\nselecting the show, preparing it, and assessing its execution. The performance\nof the model is investigated by reliable metrics like precision, recall, F1\nscore, and accuracy. The commitment of the work incorporates making a huge fake\nnews dataset in Bangla and a demonstration that has outperformed other Bangla\nfake news location models.", "published": "2025-01-30 21:41:26", "link": "http://arxiv.org/abs/2501.18766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data\n  Contamination's Impact on Machine Translation", "abstract": "Data contamination -- the accidental consumption of evaluation examples\nwithin the pre-training data -- can undermine the validity of evaluation\nbenchmarks. In this paper, we present a rigorous analysis of the effects of\ncontamination on language models at 1B and 8B scales on the machine translation\ntask. Starting from a carefully decontaminated train-test split, we\nsystematically introduce contamination at various stages, scales, and data\nformats to isolate its effect and measure its impact on performance metrics.\nOur experiments reveal that contamination with both source and target\nsubstantially inflates BLEU scores, and this inflation is 2.5 times larger (up\nto 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and\ntarget-only contamination generally produce smaller, less consistent\nover-estimations. Finally, we study how the temporal distribution and frequency\nof contaminated samples influence performance over-estimation across languages\nwith varying degrees of data resources.", "published": "2025-01-30 21:51:18", "link": "http://arxiv.org/abs/2501.18771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-Layered Large Language Model Framework for Disease Prediction", "abstract": "Social telehealth has revolutionized healthcare by enabling patients to share\nsymptoms and receive medical consultations remotely. Users frequently post\nsymptoms on social media and online health platforms, generating a vast\nrepository of medical data that can be leveraged for disease classification and\nsymptom severity assessment. Large language models (LLMs), such as LLAMA3,\nGPT-3.5 Turbo, and BERT, process complex medical data to enhance disease\nclassification. This study explores three Arabic medical text preprocessing\ntechniques: text summarization, text refinement, and Named Entity Recognition\n(NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best\nperformance was achieved using CAMeL-BERT with NER-augmented text (83% type\nclassification, 69% severity assessment). Non-fine-tuned models performed\npoorly (13%-20% type classification, 40%-49% severity assessment). Integrating\nLLMs into social telehealth systems enhances diagnostic accuracy and treatment\noutcomes.", "published": "2025-01-30 18:53:50", "link": "http://arxiv.org/abs/2502.00063v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs can see and hear without any training", "abstract": "We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple,\ntraining-free approach, to imbue multimodal capabilities into your favorite\nLLM. Leveraging their innate ability to perform multi-step reasoning, MILS\nprompts the LLM to generate candidate outputs, each of which are scored and fed\nback iteratively, eventually generating a solution to the task. This enables\nvarious applications that typically require training specialized models on\ntask-specific data. In particular, we establish a new state-of-the-art on\nemergent zero-shot image, video and audio captioning. MILS seamlessly applies\nto media generation as well, discovering prompt rewrites to improve\ntext-to-image generation, and even edit prompts for style transfer! Finally,\nbeing a gradient-free optimization approach, MILS can invert multimodal\nembeddings into text, enabling applications like cross-modal arithmetic.", "published": "2025-01-30 02:16:35", "link": "http://arxiv.org/abs/2501.18096v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Scaling Inference-Efficient Language Models", "abstract": "Scaling laws are powerful tools to predict the performance of large language\nmodels. However, current scaling laws fall short of accounting for inference\ncosts. In this work, we first show that model architecture affects inference\nlatency, where models of the same size can have up to 3.5x difference in\nlatency. To tackle this challenge, we modify the Chinchilla scaling laws to\nco-optimize the model parameter count, the number of training tokens, and the\nmodel architecture. Due to the reason that models of similar training loss\nexhibit gaps in downstream evaluation, we also propose a novel method to train\ninference-efficient models based on the revised scaling laws. We perform\nextensive empirical studies to fit and evaluate our inference-aware scaling\nlaws. We vary model parameters from 80M to 1B, training tokens from 1.6B to\n30B, and model shapes, training a total of 63 models. Guided by our\ninference-efficient scaling law and model selection method, we release the\nMorph-1B model, which improves inference latency by 1.8x while maintaining\naccuracy on downstream tasks compared to open-source models, pushing the Pareto\nfrontier of accuracy-latency tradeoff.", "published": "2025-01-30 03:16:44", "link": "http://arxiv.org/abs/2501.18107v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance", "abstract": "The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs.", "published": "2025-01-30 10:21:10", "link": "http://arxiv.org/abs/2501.18243v1", "categories": ["stat.AP", "cs.CL", "cs.LG"], "primary_category": "stat.AP"}
{"title": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence", "abstract": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.", "published": "2025-01-30 11:04:14", "link": "http://arxiv.org/abs/2501.18265v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models", "abstract": "The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.", "published": "2025-01-30 11:37:40", "link": "http://arxiv.org/abs/2501.18280v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models", "abstract": "This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions.", "published": "2025-01-30 11:55:44", "link": "http://arxiv.org/abs/2501.18287v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Citation Recommendation based on Argumentative Zoning of User Queries", "abstract": "Citation recommendation aims to locate the important papers for scholars to\ncite. When writing the citing sentences, the authors usually hold different\nciting intents, which are referred to citation function in citation analysis.\nSince argumentative zoning is to identify the argumentative and rhetorical\nstructure in scientific literature, we want to use this information to improve\nthe citation recommendation task. In this paper, a multi-task learning model is\nbuilt for citation recommendation and argumentative zoning classification. We\nalso generated an annotated corpus of the data from PubMed Central based on a\nnew argumentative zoning schema. The experimental results show that, by\nconsidering the argumentative information in the citing sentence, citation\nrecommendation model will get better performance.", "published": "2025-01-30 12:08:00", "link": "http://arxiv.org/abs/2501.18292v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence", "abstract": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.", "published": "2025-01-30 14:03:36", "link": "http://arxiv.org/abs/2501.18356v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding", "abstract": "We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.", "published": "2025-01-30 14:07:56", "link": "http://arxiv.org/abs/2501.18362v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable performance\nacross a wide range of tasks. However, their deployment in safety-critical\ndomains poses significant challenges. Existing safety fine-tuning methods,\nwhich focus on textual or multimodal content, fall short in addressing\nchallenging cases or disrupt the balance between helpfulness and harmlessness.\nOur evaluation highlights a safety reasoning gap: these methods lack safety\nvisual reasoning ability, leading to such bottlenecks. To address this\nlimitation and enhance both visual perception and reasoning in safety-critical\ncontexts, we propose a novel dataset that integrates multi-image inputs with\nsafety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve\nmodel performance. Specifically, we introduce the Multi-Image Safety (MIS)\ndataset, an instruction-following dataset tailored for multi-image safety\nscenarios, consisting of training and test splits. Our experiments demonstrate\nthat fine-tuning InternVL2.5-8B with MIS significantly outperforms both\npowerful open-source models and API-based models in challenging multi-image\ntasks requiring safety-related visual reasoning. This approach not only\ndelivers exceptional safety performance but also preserves general capabilities\nwithout any trade-offs. Specifically, fine-tuning with MIS increases average\naccuracy by 0.83% across five general benchmarks and reduces the Attack Success\nRate (ASR) on multiple safety benchmarks by a large margin. Data and Models are\nreleased under:\n\\href{https://dripnowhy.github.io/MIS/}{\\texttt{https://dripnowhy.github.io/MIS/}}", "published": "2025-01-30 17:59:45", "link": "http://arxiv.org/abs/2501.18533v1", "categories": ["cs.CV", "cs.CL", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method", "abstract": "Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches.", "published": "2025-01-30 18:07:19", "link": "http://arxiv.org/abs/2501.18539v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "R.I.P.: Better Models by Survival of the Fittest Prompts", "abstract": "Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard.", "published": "2025-01-30 18:50:25", "link": "http://arxiv.org/abs/2501.18578v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ambisonics Binaural Rendering via Masked Magnitude Least Squares", "abstract": "Ambisonics rendering has become an integral part of 3D audio for headphones.\nIt works well with existing recording hardware, the processing cost is mostly\nindependent of the number of sound sources, and it elegantly allows for\nrotating the scene and listener. One challenge in Ambisonics headphone\nrendering is to find a perceptually well behaved low-order representation of\nthe Head-Related Transfer Functions (HRTFs) that are contained in the rendering\npipe-line. Low-order rendering is of interest, when working with microphone\narrays containing only a few sensors, or for reducing the bandwidth for signal\ntransmission. Magnitude Least Squares rendering became the de facto standard\nfor this, which discards high-frequency interaural phase information in favor\nof reducing magnitude errors. Building upon this idea, we suggest Masked\nMagnitude Least Squares, which optimized the Ambisonics coefficients with a\nneural network and employs a spatio-spectral weighting mask to control the\naccuracy of the magnitude reconstruction. In the tested case, the weighting\nmask helped to maintain high-frequency notches in the low-order HRTFs and\nimproved the modeled median plane localization performance in comparison to\nMagLS, while only marginally affecting the overall accuracy of the magnitude\nreconstruction.", "published": "2025-01-30 09:26:49", "link": "http://arxiv.org/abs/2501.18224v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BSM-iMagLS: ILD Informed Binaural Signal Matching for Reproduction with\n  Head-Mounted Microphone Arrays", "abstract": "Headphone listening in applications such as augmented and virtual reality (AR\nand VR) relies on high-quality spatial audio to ensure immersion, making\naccurate binaural reproduction a critical component. As capture devices,\nwearable arrays with only a few microphones with irregular arrangement face\nchallenges in achieving a reproduction quality comparable to that of arrays\nwith a large number of microphones. Binaural signal matching (BSM) has recently\nbeen presented as a signal-independent approach for generating high-quality\nbinaural signal using only a few microphones, which is further improved using\nmagnitude-least squares (MagLS) optimization at high frequencies. This paper\nextends BSM with MagLS by introducing interaural level difference (ILD) into\nthe MagLS, integrated into BSM (BSM-iMagLS). Using a deep neural network\n(DNN)-based solver, BSM-iMagLS achieves joint optimization of magnitude, ILD,\nand magnitude derivatives, improving spatial fidelity. Performance is validated\nthrough theoretical analysis, numerical simulations with diverse HRTFs and\nhead-mounted array geometries, and listening experiments, demonstrating a\nsubstantial reduction in ILD errors while maintaining comparable magnitude\naccuracy to state-of-the-art solutions. The results highlight the potential of\nBSM-iMagLS to enhance binaural reproduction for wearable and portable devices.", "published": "2025-01-30 09:33:37", "link": "http://arxiv.org/abs/2501.18227v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment", "abstract": "Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%.", "published": "2025-01-30 05:46:30", "link": "http://arxiv.org/abs/2501.18157v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment", "abstract": "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https://agav-rater.github.io.", "published": "2025-01-30 12:43:47", "link": "http://arxiv.org/abs/2501.18314v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Resampling Filter Design for Multirate Neural Audio Effect Processing", "abstract": "Neural networks have become ubiquitous in audio effects modelling, especially\nfor guitar amplifiers and distortion pedals. One limitation of such models is\nthat the sample rate of the training data is implicitly encoded in the model\nweights and therefore not readily adjustable at inference. Recent work explored\nmodifications to recurrent neural network architecture to approximate a sample\nrate independent system, enabling audio processing at a rate that differs from\nthe original training rate. This method works well for integer oversampling and\ncan reduce aliasing caused by nonlinear activation functions. For small\nfractional changes in sample rate, fractional delay filters can be used to\napproximate sample rate independence, but in some cases this method fails\nentirely. Here, we explore the use of signal resampling at the input and output\nof the neural network as an alternative solution. We investigate several\nresampling filter designs and show that a two-stage design consisting of a\nhalf-band IIR filter cascaded with a Kaiser window FIR filter can give similar\nor better results to the previously proposed model adjustment method with many\nfewer operations per sample and less than one millisecond of latency at typical\naudio rates. Furthermore, we investigate interpolation and decimation filters\nfor the task of integer oversampling and show that cascaded half-band IIR and\nFIR designs can be used in conjunction with the model adjustment method to\nreduce aliasing in a range of distortion effect models.", "published": "2025-01-30 16:44:49", "link": "http://arxiv.org/abs/2501.18470v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A General-Purpose Neuromorphic Sensor based on Spiketrum Algorithm:\n  Hardware Details and Real-life Applications", "abstract": "Spiking Neural Networks (SNNs) offer a biologically inspired computational\nparadigm, enabling energy-efficient data processing through spike-based\ninformation transmission. Despite notable advancements in hardware for SNNs,\nspike encoding has largely remained software-dependent, limiting efficiency.\nThis paper addresses the need for adaptable and resource-efficient spike\nencoding hardware by presenting an area-optimized hardware implementation of\nthe Spiketrum algorithm, which encodes time-varying analogue signals into\nspatiotemporal spike patterns. Unlike earlier performance-optimized designs,\nwhich prioritize speed, our approach focuses on reducing hardware footprint,\nachieving a 52% reduction in Block RAMs (BRAMs), 31% fewer Digital Signal\nProcessing (DSP) slices, and a 6% decrease in Look-Up Tables (LUTs). The\nproposed implementation has been verified on an FPGA and successfully\nintegrated into an IC using TSMC180 technology. Experimental results\ndemonstrate the system's effectiveness in real-world applications, including\nsound and ECG classification. This work highlights the trade-offs between\nperformance and resource efficiency, offering a flexible, scalable solution for\nneuromorphic systems in power-sensitive applications like cochlear implants and\nneural devices.", "published": "2025-01-30 23:34:10", "link": "http://arxiv.org/abs/2501.18799v1", "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Multilayered Intelligent Reflecting Surface for Long-Range Underwater\n  Acoustic Communication", "abstract": "This article introduces a multilayered acoustic reconfigurable intelligent\nsurface (ML-ARIS) architecture designed for the next generation of underwater\ncommunications. ML-ARIS incorporates multiple layers of piezoelectric material\nin each acoustic reflector, with the load impedance of each layer independently\nadjustable via a control circuit. This design increases the flexibility in\ngenerating reflected signals with desired amplitudes and orthogonal phases,\nenabling passive in-phase and quadrature (IQ) modulation using a single\nacoustic reflector. Such a feature enables precise beam steering, enhancing\nsound levels in targeted directions while minimizing interference in\nsurrounding environments. Extensive simulations and tank experiments were\nconducted to verify the feasibility of ML-ARIS. The experimental results\nindicate that implementing IQ modulation with a multilayer structure is indeed\npractical in real-world scenarios, making it possible to use a single\nreflection unit to generate reflected waves with high-resolution amplitudes and\nphases.", "published": "2025-01-30 14:02:59", "link": "http://arxiv.org/abs/2501.18355v1", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
{"title": "Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks", "abstract": "The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.", "published": "2025-01-30 20:07:44", "link": "http://arxiv.org/abs/2501.18727v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
