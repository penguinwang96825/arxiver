{"title": "Multilingual Distributed Representations without Word Alignment", "abstract": "Distributed representations of meaning are a natural way to encode covariance\nrelationships between words and phrases in NLP. By overcoming data sparsity\nproblems, as well as providing information about semantic relatedness which is\nnot available in discrete representations, distributed representations have\nproven useful in many NLP tasks. Recent work has shown how compositional\nsemantic representations can successfully be applied to a number of monolingual\napplications such as sentiment analysis. At the same time, there has been some\ninitial success in work on learning shared word-level representations across\nlanguages. We combine these two approaches by proposing a method for learning\ndistributed representations in a multilingual setup. Our model learns to assign\nsimilar embeddings to aligned sentences and dissimilar ones to sentence which\nare not aligned while not requiring word alignments. We show that our\nrepresentations are semantically informative and apply them to a cross-lingual\ndocument classification task where we outperform the previous state of the art.\nFurther, by employing parallel corpora of multiple language pairs we find that\nour model learns representations that capture semantic relationships across\nlanguages for which no parallel data was used.", "published": "2013-12-20 23:13:38", "link": "http://arxiv.org/abs/1312.6173v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Learning for Semantic Utterance Classification", "abstract": "We propose a novel zero-shot learning method for semantic utterance\nclassification (SUC). It learns a classifier $f: X \\to Y$ for problems where\nnone of the semantic categories $Y$ are present in the training set. The\nframework uncovers the link between categories and utterances using a semantic\nspace. We show that this semantic space can be learned by deep neural networks\ntrained on large amounts of search engine query log data. More precisely, we\npropose a novel method that can learn discriminative semantic features without\nsupervision. It uses the zero-shot learning framework to guide the learning of\nthe semantic features. We demonstrate the effectiveness of the zero-shot\nsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).\nFurthermore, we achieve state-of-the-art results by combining the semantic\nfeatures with a supervised method.", "published": "2013-12-20 17:08:26", "link": "http://arxiv.org/abs/1401.0509v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Type-Driven Tensor-Based Meaning Representations", "abstract": "This paper investigates the learning of 3rd-order tensors representing the\nsemantics of transitive verbs. The meaning representations are part of a\ntype-driven tensor-based semantic framework, from the newly emerging field of\ncompositional distributional semantics. Standard techniques from the neural\nnetworks literature are used to learn the tensors, which are tested on a\nselectional preference-style task with a simple 2-dimensional sentence space.\nPromising results are obtained against a competitive corpus-based baseline. We\nargue that extending this work beyond transitive verbs, and to\nhigher-dimensional sentence spaces, is an interesting and challenging problem\nfor the machine learning community to consider.", "published": "2013-12-20 15:21:15", "link": "http://arxiv.org/abs/1312.5985v2", "categories": ["cs.CL", "cs.LG", "H.3.1"], "primary_category": "cs.CL"}
{"title": "Factorial Hidden Markov Models for Learning Representations of Natural\n  Language", "abstract": "Most representation learning algorithms for language and image processing are\nlocal, in that they identify features for a data point based on surrounding\npoints. Yet in language processing, the correct meaning of a word often depends\non its global context. As a step toward incorporating global context into\nrepresentation learning, we develop a representation learning algorithm that\nincorporates joint prediction into its technique for producing features for a\nword. We develop efficient variational methods for learning Factorial Hidden\nMarkov Models from large texts, and use variational distributions to produce\nfeatures for each word that are sensitive to the entire input sequence, not\njust to a local context window. Experiments on part-of-speech tagging and\nchunking indicate that the features are competitive with or better than\nexisting state-of-the-art representation learning methods.", "published": "2013-12-20 22:44:26", "link": "http://arxiv.org/abs/1312.6168v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
