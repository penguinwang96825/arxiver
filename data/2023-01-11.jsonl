{"title": "MGeo: Multi-Modal Geographic Pre-Training Method", "abstract": "As a core task in location-based services (LBS) (e.g., navigation maps),\nquery and point of interest (POI) matching connects users' intent with\nreal-world geographic information. Recently, pre-trained models (PTMs) have\nmade advancements in many natural language processing (NLP) tasks. Generic\ntext-based PTMs do not have enough geographic knowledge for query-POI matching.\nTo overcome this limitation, related literature attempts to employ\ndomain-adaptive pre-training based on geo-related corpus. However, a query\ngenerally contains mentions of multiple geographic objects, such as nearby\nroads and regions of interest (ROIs). The geographic context (GC), i.e., these\ndiverse geographic objects and their relationships, is therefore pivotal to\nretrieving the most relevant POI. Single-modal PTMs can barely make use of the\nimportant GC and therefore have limited performance. In this work, we propose a\nnovel query-POI matching method Multi-modal Geographic language model (MGeo),\nwhich comprises a geographic encoder and a multi-modal interaction module. MGeo\nrepresents GC as a new modality and is able to fully extract multi-modal\ncorrelations for accurate query-POI matching. Besides, there is no publicly\navailable benchmark for this topic. In order to facilitate further research, we\nbuild a new open-source large-scale benchmark Geographic TExtual Similarity\n(GeoTES). The POIs come from an open-source geographic information system\n(GIS). The queries are manually generated by annotators to prevent privacy\nissues. Compared with several strong baselines, the extensive experiment\nresults and detailed ablation analyses on GeoTES demonstrate that our proposed\nmulti-modal pre-training method can significantly improve the query-POI\nmatching capability of generic PTMs, even when the queries' GC is not provided.\nOur code and dataset are publicly available at\nhttps://github.com/PhantomGrapes/MGeo.", "published": "2023-01-11 03:05:12", "link": "http://arxiv.org/abs/2301.04283v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word-Graph2vec: An efficient word embedding approach on word\n  co-occurrence graph using random walk technique", "abstract": "Word embedding has become ubiquitous and is widely used in various natural\nlanguage processing (NLP) tasks, such as web retrieval, web semantic analysis,\nand machine translation, and so on. Unfortunately, training the word embedding\nin a relatively large corpus is prohibitively expensive. We propose a\ngraph-based word embedding algorithm, called Word-Graph2vec, which converts the\nlarge corpus into a word co-occurrence graph, then takes the word sequence\nsamples from this graph by randomly traveling and trains the word embedding on\nthis sampling corpus in the end. We posit that because of the limited\nvocabulary, huge idioms, and fixed expressions in English, the size and density\nof the word co-occurrence graph change slightly with the increase in the\ntraining corpus. So that Word-Graph2vec has stable runtime on the large-scale\ndata set, and its performance advantage becomes more and more obvious with the\ngrowth of the training corpus. Extensive experiments conducted on real-world\ndatasets show that the proposed algorithm outperforms traditional Word2vec four\nto five times in terms of efficiency and two to three times than FastText,\nwhile the error generated by the random walk technique is small.", "published": "2023-01-11 05:21:00", "link": "http://arxiv.org/abs/2301.04312v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counteracts: Testing Stereotypical Representation in Pre-trained\n  Language Models", "abstract": "Recently, language models have demonstrated strong performance on various\nnatural language understanding tasks. Language models trained on large\nhuman-generated corpus encode not only a significant amount of human knowledge,\nbut also the human stereotype. As more and more downstream tasks have\nintegrated language models as part of the pipeline, it is necessary to\nunderstand the internal stereotypical representation in order to design the\nmethods for mitigating the negative effects. In this paper, we use\ncounterexamples to examine the internal stereotypical knowledge in pre-trained\nlanguage models (PLMs) that can lead to stereotypical preference. We mainly\nfocus on gender stereotypes, but the method can be extended to other types of\nstereotype. We evaluate 7 PLMs on 9 types of cloze-style prompt with different\ninformation and base knowledge. The results indicate that PLMs show a certain\namount of robustness against unrelated information and preference of shallow\nlinguistic cues, such as word position and syntactic structure, but a lack of\ninterpreting information by meaning. Such findings shed light on how to\ninteract with PLMs in a neutral approach for both finetuning and evaluation.", "published": "2023-01-11 07:52:59", "link": "http://arxiv.org/abs/2301.04347v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Entity and Relation Extraction from Unified to\n  Language-specific Training", "abstract": "Entity and relation extraction is a key task in information extraction, where\nthe output can be used for downstream NLP tasks. Existing approaches for entity\nand relation extraction tasks mainly focus on the English corpora and ignore\nother languages. Thus, it is critical to improving performance in a\nmultilingual setting. Meanwhile, multilingual training is usually used to boost\ncross-lingual performance by transferring knowledge from languages (e.g.,\nhigh-resource) to other (e.g., low-resource) languages. However, language\ninterference usually exists in multilingual tasks as the model parameters are\nshared among all languages. In this paper, we propose a two-stage multilingual\ntraining method and a joint model called Multilingual Entity and Relation\nExtraction framework (mERE) to mitigate language interference across languages.\nSpecifically, we randomly concatenate sentences in different languages to train\na Language-universal Aggregator (LA), which narrows the distance of embedding\nrepresentations by obtaining the unified language representation. Then, we\nseparate parameters to mitigate interference via tuning a Language-specific\nSwitcher (LS), which includes several independent sub-modules to refine the\nlanguage-specific feature representation. After that, to enhance the relational\ntriple extraction, the sentence representations concatenated with the relation\nfeature are used to recognize the entities. Extensive experimental results show\nthat our method outperforms both the monolingual and multilingual baseline\nmethods. Besides, we also perform detailed analysis to show that mERE is\nlightweight but effective on relational triple extraction and mERE{} is easy to\ntransfer to other backbone models of multi-field tasks, which further\ndemonstrates the effectiveness of our method.", "published": "2023-01-11 12:26:53", "link": "http://arxiv.org/abs/2301.04434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deteksi Depresi dan Kecemasan Pengguna Twitter Menggunakan Bidirectional\n  LSTM", "abstract": "The most common mental disorders experienced by a person in daily life are\ndepression and anxiety. Social stigma makes people with depression and anxiety\nneglected by their surroundings. Therefore, they turn to social media like\nTwitter for support. Detecting users with potential depression and anxiety\ndisorders through textual data is not easy because they do not explicitly\ndiscuss their mental state. It takes a model that can identify potential users\nwho experience depression and anxiety on textual data to get treatment earlier.\nText classification techniques can achieve this. One approach that can be used\nis LSTM as an RNN architecture development in dealing with vanishing gradient\nproblems. Standard LSTM does not capture enough information because it can only\nread sentences from one direction. Meanwhile, Bidirectional LSTM (BiLSTM) is a\ntwo-way LSTM that can capture information without ignoring the context and\nmeaning of a sentence. The proposed BiLSTM model is higher than all traditional\nmachine learning models and standard LSTMs. Based on the test results, the\nhighest accuracy obtained by BiLSTM reached 94.12%. This study has succeeded in\ndeveloping a model for the detection of depression and anxiety in Twitter\nusers.", "published": "2023-01-11 15:37:48", "link": "http://arxiv.org/abs/2301.04521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "tieval: An Evaluation Framework for Temporal Information Extraction\n  Systems", "abstract": "Temporal information extraction (TIE) has attracted a great deal of interest\nover the last two decades, leading to the development of a significant number\nof datasets. Despite its benefits, having access to a large volume of corpora\nmakes it difficult when it comes to benchmark TIE systems. On the one hand,\ndifferent datasets have different annotation schemes, thus hindering the\ncomparison between competitors across different corpora. On the other hand, the\nfact that each corpus is commonly disseminated in a different format requires a\nconsiderable engineering effort for a researcher/practitioner to develop\nparsers for all of them. This constraint forces researchers to select a limited\namount of datasets to evaluate their systems which consequently limits the\ncomparability of the systems. Yet another obstacle that hinders the\ncomparability of the TIE systems is the evaluation metric employed. While most\nresearch works adopt traditional metrics such as precision, recall, and $F_1$,\na few others prefer temporal awareness -- a metric tailored to be more\ncomprehensive on the evaluation of temporal systems. Although the reason for\nthe absence of temporal awareness in the evaluation of most systems is not\nclear, one of the factors that certainly weights this decision is the necessity\nto implement the temporal closure algorithm in order to compute temporal\nawareness, which is not straightforward to implement neither is currently\neasily available. All in all, these problems have limited the fair comparison\nbetween approaches and consequently, the development of temporal extraction\nsystems. To mitigate these problems, we have developed tieval, a Python library\nthat provides a concise interface for importing different corpora and\nfacilitates system evaluation. In this paper, we present the first public\nrelease of tieval and highlight its most relevant features.", "published": "2023-01-11 18:55:22", "link": "http://arxiv.org/abs/2301.04643v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Learning for Large Vocabulary On-Device ASR", "abstract": "Dual learning is a paradigm for semi-supervised machine learning that seeks\nto leverage unsupervised data by solving two opposite tasks at once. In this\nscheme, each model is used to generate pseudo-labels for unlabeled examples\nthat are used to train the other model. Dual learning has seen some use in\nspeech processing by pairing ASR and TTS as dual tasks. However, these results\nmostly address only the case of using unpaired examples to compensate for very\nsmall supervised datasets, and mostly on large, non-streaming models. Dual\nlearning has not yet been proven effective for using unsupervised data to\nimprove realistic on-device streaming models that are already trained on large\nsupervised corpora. We provide this missing piece though an analysis of an\non-device-sized streaming conformer trained on the entirety of Librispeech,\nshowing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4%\nwith an LM.", "published": "2023-01-11 06:32:28", "link": "http://arxiv.org/abs/2301.04327v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topics in Contextualised Attention Embeddings", "abstract": "Contextualised word vectors obtained via pre-trained language models encode a\nvariety of knowledge that has already been exploited in applications.\nComplementary to these language models are probabilistic topic models that\nlearn thematic patterns from the text. Recent work has demonstrated that\nconducting clustering on the word-level contextual representations from a\nlanguage model emulates word clusters that are discovered in latent topics of\nwords from Latent Dirichlet Allocation. The important question is how such\ntopical word clusters are automatically formed, through clustering, in the\nlanguage model when it has not been explicitly designed to model latent topics.\nTo address this question, we design different probe experiments. Using BERT and\nDistilBERT, we find that the attention framework plays a key role in modelling\nsuch word topic clusters. We strongly believe that our work paves way for\nfurther research into the relationships between probabilistic topic models and\npre-trained language models.", "published": "2023-01-11 07:26:19", "link": "http://arxiv.org/abs/2301.04339v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Diving Deep into Modes of Fact Hallucinations in Dialogue Systems", "abstract": "Knowledge Graph(KG) grounded conversations often use large pre-trained models\nand usually suffer from fact hallucination. Frequently entities with no\nreferences in knowledge sources and conversation history are introduced into\nresponses, thus hindering the flow of the conversation -- existing work attempt\nto overcome this issue by tweaking the training procedure or using a multi-step\nrefining method. However, minimal effort is put into constructing an\nentity-level hallucination detection system, which would provide fine-grained\nsignals that control fallacious content while generating responses. As a first\nstep to address this issue, we dive deep to identify various modes of\nhallucination in KG-grounded chatbots through human feedback analysis.\nSecondly, we propose a series of perturbation strategies to create a synthetic\ndataset named FADE (FActual Dialogue Hallucination DEtection Dataset). Finally,\nwe conduct comprehensive data analyses and create multiple baseline models for\nhallucination detection to compare against human-verified data and already\nestablished benchmarks.", "published": "2023-01-11 13:08:57", "link": "http://arxiv.org/abs/2301.04449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Role of Interactive Visualization in Explaining (Large) NLP Models:\n  from Data to Inference", "abstract": "With a constant increase of learned parameters, modern neural language models\nbecome increasingly more powerful. Yet, explaining these complex model's\nbehavior remains a widely unsolved problem. In this paper, we discuss the role\ninteractive visualization can play in explaining NLP models (XNLP). We motivate\nthe use of visualization in relation to target users and common NLP pipelines.\nWe also present several use cases to provide concrete examples on XNLP with\nvisualization. Finally, we point out an extensive list of research\nopportunities in this field.", "published": "2023-01-11 15:46:52", "link": "http://arxiv.org/abs/2301.04528v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Few-shot Learning for Cross-Target Stance Detection by Aggregating\n  Multimodal Embeddings", "abstract": "Despite the increasing popularity of the stance detection task, existing\napproaches are predominantly limited to using the textual content of social\nmedia posts for the classification, overlooking the social nature of the task.\nThe stance detection task becomes particularly challenging in cross-target\nclassification scenarios, where even in few-shot training settings the model\nneeds to predict the stance towards new targets for which the model has only\nseen few relevant samples during training. To address the cross-target stance\ndetection in social media by leveraging the social nature of the task, we\nintroduce CT-TN, a novel model that aggregates multimodal embeddings derived\nfrom both textual and network features of the data. We conduct experiments in a\nfew-shot cross-target scenario on six different combinations of\nsource-destination target pairs. By comparing CT-TN with state-of-the-art\ncross-target stance detection models, we demonstrate the effectiveness of our\nmodel by achieving average performance improvements ranging from 11% to 21%\nacross different baseline models. Experiments with different numbers of shots\nshow that CT-TN can outperform other models after seeing 300 instances of the\ndestination target. Further, ablation experiments demonstrate the positive\ncontribution of each of the components of CT-TN towards the final performance.\nWe further analyse the network interactions between social media users, which\nreveal the potential of using social features for cross-target stance\ndetection.", "published": "2023-01-11 15:52:55", "link": "http://arxiv.org/abs/2301.04535v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning to Exploit Temporal Structure for Biomedical Vision-Language\n  Processing", "abstract": "Self-supervised learning in vision-language processing exploits semantic\nalignment between imaging and text modalities. Prior work in biomedical VLP has\nmostly relied on the alignment of single image and report pairs even though\nclinical notes commonly refer to prior images. This does not only introduce\npoor alignment between the modalities but also a missed opportunity to exploit\nrich self-supervision through existing temporal content in the data. In this\nwork, we explicitly account for prior images and reports when available during\nboth training and fine-tuning. Our approach, named BioViL-T, uses a\nCNN-Transformer hybrid multi-image encoder trained jointly with a text model.\nIt is designed to be versatile to arising challenges such as pose variations\nand missing input images across time. The resulting model excels on downstream\ntasks both in single- and multi-image setups, achieving state-of-the-art\nperformance on (I) progression classification, (II) phrase grounding, and (III)\nreport generation, whilst offering consistent improvements on disease\nclassification and sentence-similarity tasks. We release a novel multi-modal\ntemporal benchmark dataset, MS-CXR-T, to quantify the quality of\nvision-language representations in terms of temporal semantics. Our\nexperimental results show the advantages of incorporating prior images and\nreports to make most use of the data.", "published": "2023-01-11 16:35:33", "link": "http://arxiv.org/abs/2301.04558v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EXIF as Language: Learning Cross-Modal Associations Between Images and\n  Camera Metadata", "abstract": "We learn a visual representation that captures information about the camera\nthat recorded a given photo. To do this, we train a multimodal embedding\nbetween image patches and the EXIF metadata that cameras automatically insert\ninto image files. Our model represents this metadata by simply converting it to\ntext and then processing it with a transformer. The features that we learn\nsignificantly outperform other self-supervised and supervised features on\ndownstream image forensics and calibration tasks. In particular, we\nsuccessfully localize spliced image regions \"zero shot\" by clustering the\nvisual embeddings for all of the patches within an image.", "published": "2023-01-11 18:59:16", "link": "http://arxiv.org/abs/2301.04647v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SensePOLAR: Word sense aware interpretability for pre-trained contextual\n  word embeddings", "abstract": "Adding interpretability to word embeddings represents an area of active\nresearch in text representation. Recent work has explored thepotential of\nembedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.\nwrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,\nand BiImp. Although these approaches provide interpretable dimensions for\nwords, they have not been designed to deal with polysemy, i.e. they can not\neasily distinguish between different senses of words. To address this\nlimitation, we present SensePOLAR, an extension of the original POLAR framework\nthat enables word-sense aware interpretability for pre-trained contextual word\nembeddings. The resulting interpretable word embeddings achieve a level of\nperformance that is comparable to original contextual word embeddings across a\nvariety of natural language processing tasks including the GLUE and SQuAD\nbenchmarks. Our work removes a fundamental limitation of existing approaches by\noffering users sense aware interpretations for contextual word embeddings.", "published": "2023-01-11 20:25:53", "link": "http://arxiv.org/abs/2301.04704v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Web Enabled Geographic Question Answering Framework: GeoTR", "abstract": "With the considerable growth of linked data, researchers have focused on how\nto increase the availability of semantic web technologies to provide practical\nusages for real life systems. Question answering systems are an example of\nreal-life systems that communicate directly with end users, understand user\nintention and generate answers. End users do not care about the structural\nquery language or the vocabulary of the knowledge base where the point of a\nproblem arises. In this study, a question answering framework that converts\nTurkish natural language input into SPARQL queries in the geographical domain\nis proposed. Additionally, a novel Turkish ontology, which covers a 10th grade\ngeography lesson named Spatial Synthesis Turkey, has been developed to be used\nas a linked data provider. Moreover, a gap in the literature on Turkish\nquestion answering systems, which utilizes linked data in the geographical\ndomain, is addressed. A hybrid system architecture that combines natural\nlanguage processing techniques with linked data technologies to generate\nanswers is also proposed. Further related research areas are suggested.", "published": "2023-01-11 23:20:43", "link": "http://arxiv.org/abs/2301.04752v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference", "abstract": "Large-scale language model pretraining is a very successful form of\nself-supervised learning in natural language processing, but it is increasingly\nexpensive to perform as the models and pretraining corpora have become larger\nover time. We propose NarrowBERT, a modified transformer encoder that increases\nthe throughput for masked language model pretraining by more than $2\\times$.\nNarrowBERT sparsifies the transformer model such that the self-attention\nqueries and feedforward layers only operate on the masked tokens of each\nsentence during pretraining, rather than all of the tokens as with the usual\ntransformer encoder. We also show that NarrowBERT increases the throughput at\ninference time by as much as $3.5\\times$ with minimal (or no) performance\ndegradation on sentence encoding tasks like MNLI. Finally, we examine the\nperformance of NarrowBERT on the IMDB and Amazon reviews classification and\nCoNLL NER tasks and show that it is also comparable to standard BERT\nperformance.", "published": "2023-01-11 23:45:50", "link": "http://arxiv.org/abs/2301.04761v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Answering Climate Questionnaires from Unstructured Climate\n  Reports", "abstract": "The topic of Climate Change (CC) has received limited attention in NLP\ndespite its urgency. Activists and policymakers need NLP tools to effectively\nprocess the vast and rapidly growing unstructured textual climate reports into\nstructured form. To tackle this challenge we introduce two new large-scale\nclimate questionnaire datasets and use their existing structure to train\nself-supervised models. We conduct experiments to show that these models can\nlearn to generalize to climate disclosures of different organizations types\nthan seen during training. We then use these models to help align texts from\nunstructured climate documents to the semi-structured questionnaires in a human\npilot study. Finally, to support further NLP research in the climate domain we\nintroduce a benchmark of existing climate text classification datasets to\nbetter evaluate and compare existing models.", "published": "2023-01-11 00:22:56", "link": "http://arxiv.org/abs/2301.04253v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Inverse Cloze Task for Knowledge-based Visual Question\n  Answering", "abstract": "We present a new pre-training method, Multimodal Inverse Cloze Task, for\nKnowledge-based Visual Question Answering about named Entities (KVQAE). KVQAE\nis a recently introduced task that consists in answering questions about named\nentities grounded in a visual context using a Knowledge Base. Therefore, the\ninteraction between the modalities is paramount to retrieve information and\nmust be captured with complex fusion models. As these models require a lot of\ntraining data, we design this pre-training task from existing work in textual\nQuestion Answering. It consists in considering a sentence as a pseudo-question\nand its context as a pseudo-relevant passage and is extended by considering\nimages near texts in multimodal documents. Our method is applicable to\ndifferent neural network architectures and leads to a 9% relative-MRR and 15%\nrelative-F1 gain for retrieval and reading comprehension, respectively, over a\nno-pre-training baseline.", "published": "2023-01-11 09:16:34", "link": "http://arxiv.org/abs/2301.04366v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Perceive and predict: self-supervised speech representation based loss\n  functions for speech enhancement", "abstract": "Recent work in the domain of speech enhancement has explored the use of\nself-supervised speech representations to aid in the training of neural speech\nenhancement models. However, much of this work focuses on using the deepest or\nfinal outputs of self supervised speech representation models, rather than the\nearlier feature encodings. The use of self supervised representations in such a\nway is often not fully motivated. In this work it is shown that the distance\nbetween the feature encodings of clean and noisy speech correlate strongly with\npsychoacoustically motivated measures of speech quality and intelligibility, as\nwell as with human Mean Opinion Score (MOS) ratings. Experiments using this\ndistance as a loss function are performed and improved performance over the use\nof STFT spectrogram distance based loss as well as other common loss functions\nfrom speech enhancement literature is demonstrated using objective measures\nsuch as perceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI).", "published": "2023-01-11 10:20:56", "link": "http://arxiv.org/abs/2301.04388v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities", "abstract": "The global economy is increasingly dependent on knowledge workers to meet the\nneeds of public and private organizations. While there is no single definition\nof knowledge work, organizations and industry groups still attempt to measure\nindividuals' capability to engage in it. The most comprehensive assessment of\ncapability readiness for professional knowledge workers is the Uniform CPA\nExamination developed by the American Institute of Certified Public Accountants\n(AICPA). In this paper, we experimentally evaluate OpenAI's `text-davinci-003`\nand prior versions of GPT on both a sample Regulation (REG) exam and an\nassessment of over 200 multiple-choice questions based on the AICPA Blueprints\nfor legal, financial, accounting, technology, and ethical tasks. First, we find\nthat `text-davinci-003` achieves a correct rate of 14.4% on a sample REG exam\nsection, significantly underperforming human capabilities on quantitative\nreasoning in zero-shot prompts. Second, `text-davinci-003` appears to be\napproaching human-level performance on the Remembering & Understanding and\nApplication skill levels in the Exam absent calculation. For best prompt and\nparameters, the model answers 57.6% of questions correctly, significantly\nbetter than the 25% guessing rate, and its top two answers are correct 82.1% of\nthe time, indicating strong non-entailment. Finally, we find that recent\ngenerations of GPT-3 demonstrate material improvements on this assessment,\nrising from 30% for `text-davinci-001` to 57% for `text-davinci-003`. These\nfindings strongly suggest that large language models have the potential to\ntransform the quality and efficiency of future knowledge work.", "published": "2023-01-11 11:30:42", "link": "http://arxiv.org/abs/2301.04408v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Analyzing And Improving Neural Speaker Embeddings for ASR", "abstract": "Neural speaker embeddings encode the speaker's speech characteristics through\na DNN model and are prevalent for speaker verification tasks. However, few\nstudies have investigated the usage of neural speaker embeddings for an ASR\nsystem. In this work, we present our efforts w.r.t integrating neural speaker\nembeddings into a conformer based hybrid HMM ASR system. For ASR, our improved\nembedding extraction pipeline in combination with the Weighted-Simple-Add\nintegration method results in x-vector and c-vector reaching on par performance\nwith i-vectors. We further compare and analyze different speaker embeddings. We\npresent our acoustic model improvements obtained by switching from newbob\nlearning rate schedule to one cycle learning schedule resulting in a ~3%\nrelative WER reduction on Switchboard, additionally reducing the overall\ntraining time by 17%. By further adding neural speaker embeddings, we gain\nadditional ~3% relative WER improvement on Hub5'00. Our best Conformer-based\nhybrid ASR system with speaker embeddings achieves 9.0% WER on Hub5'00 and\nHub5'01 with training on SWB 300h.", "published": "2023-01-11 16:56:03", "link": "http://arxiv.org/abs/2301.04571v2", "categories": ["cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Modelling low-resource accents without accent-specific TTS frontend", "abstract": "This work focuses on modelling a speaker's accent that does not have a\ndedicated text-to-speech (TTS) frontend, including a grapheme-to-phoneme (G2P)\nmodule. Prior work on modelling accents assumes a phonetic transcription is\navailable for the target accent, which might not be the case for low-resource,\nregional accents. In our work, we propose an approach whereby we first augment\nthe target accent data to sound like the donor voice via voice conversion, then\ntrain a multi-speaker multi-accent TTS model on the combination of recordings\nand synthetic data, to generate the donor's voice speaking in the target\naccent. Throughout the procedure, we use a TTS frontend developed for the same\nlanguage but a different accent. We show qualitative and quantitative analysis\nwhere the proposed strategy achieves state-of-the-art results compared to other\ngenerative models. Our work demonstrates that low resource accents can be\nmodelled with relatively little data and without developing an accent-specific\nTTS frontend. Audio samples of our model converting to multiple accents are\navailable on our web page.", "published": "2023-01-11 18:00:29", "link": "http://arxiv.org/abs/2301.04606v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rethinking complex-valued deep neural networks for monaural speech\n  enhancement", "abstract": "Despite multiple efforts made towards adopting complex-valued deep neural\nnetworks (DNNs), it remains an open question whether complex-valued DNNs are\ngenerally more effective than real-valued DNNs for monaural speech enhancement.\nThis work is devoted to presenting a critical assessment by systematically\nexamining complex-valued DNNs against their real-valued counterparts.\nSpecifically, we investigate complex-valued DNN atomic units, including linear\nlayers, convolutional layers, long short-term memory (LSTM), and gated linear\nunits. By comparing complex- and real-valued versions of fundamental building\nblocks in the recently developed gated convolutional recurrent network (GCRN),\nwe show how different mechanisms for basic blocks affect the performance. We\nalso find that the use of complex-valued operations hinders the model capacity\nwhen the model size is small. In addition, we examine two recent complex-valued\nDNNs, i.e. deep complex convolutional recurrent network (DCCRN) and deep\ncomplex U-Net (DCUNET). Evaluation results show that both DNNs produce\nidentical performance to their real-valued counterparts while requiring much\nmore computation. Based on these comprehensive comparisons, we conclude that\ncomplex-valued DNNs do not provide a performance gain over their real-valued\ncounterparts for monaural speech enhancement, and thus are less desirable due\nto their higher computational costs.", "published": "2023-01-11 05:59:50", "link": "http://arxiv.org/abs/2301.04320v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WuYun: Exploring hierarchical skeleton-guided melody generation using\n  knowledge-enhanced deep learning", "abstract": "Although deep learning has revolutionized music generation, existing methods\nfor structured melody generation follow an end-to-end left-to-right\nnote-by-note generative paradigm and treat each note equally. Here, we present\nWuYun, a knowledge-enhanced deep learning architecture for improving the\nstructure of generated melodies, which first generates the most structurally\nimportant notes to construct a melodic skeleton and subsequently infills it\nwith dynamically decorative notes into a full-fledged melody. Specifically, we\nuse music domain knowledge to extract melodic skeletons and employ sequence\nlearning to reconstruct them, which serve as additional knowledge to provide\nauxiliary guidance for the melody generation process. We demonstrate that WuYun\ncan generate melodies with better long-term structure and musicality and\noutperforms other state-of-the-art methods by 0.51 on average on all subjective\nevaluation metrics. Our study provides a multidisciplinary lens to design\nmelodic hierarchical structures and bridge the gap between data-driven and\nknowledge-based approaches for numerous music generation tasks.", "published": "2023-01-11 14:33:42", "link": "http://arxiv.org/abs/2301.04488v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Topological data analysis hearing the shapes of drums and bells", "abstract": "Mark Kac asked a famous question in 1966 entitled Can one hear the shape of a\ndrum?, a spectral geometry problem that has intrigued mathematicians for the\nlast six decades and is important to many other fields, such as architectural\nacoustics, audio forensics, pattern recognition, radiology, and imaging\nscience. A related question is how to hear the shape of a drum. We show that\nthe answer was given in the set of 65 Zenghouyi chime bells dated back to\n475-433 B.C. in China. The set of chime bells gradually varies their sizes and\nweights to enable melodies, intervals, and temperaments. The same design\nprinciple was used in many other musical instruments, such as xylophones, pan\nflutes, pianos, etc. We reveal that there is a fascinating connection between\nthe progression pattern of many musical instruments and filtration (or spectral\nsequence) in topological data analysis (TDA). We argue that filtration-induced\nevolutionary de Rham-Hodge theory provides a new mathematical foundation for\nmusical instruments. Its discrete counterpart, persistent Laplacians and many\nother persistent topological Laplacians, including persistent sheaf Laplacians\nand persistent path Laplacians are briefly discussed.", "published": "2023-01-11 15:40:53", "link": "http://arxiv.org/abs/2301.05025v1", "categories": ["math.HO", "cs.SD", "eess.AS"], "primary_category": "math.HO"}
