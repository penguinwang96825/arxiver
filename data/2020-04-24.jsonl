{"title": "Generative Data Augmentation for Commonsense Reasoning", "abstract": "Recent advances in commonsense reasoning depend on large-scale\nhuman-annotated training data to achieve peak performance. However, manual\ncuration of training examples is expensive and has been shown to introduce\nannotation artifacts that neural models can readily exploit and overfit on. We\ninvestigate G-DAUG^C, a novel generative data augmentation method that aims to\nachieve more accurate and robust learning in the low-resource setting. Our\napproach generates synthetic examples using pretrained language models, and\nselects the most informative and diverse set of examples for data augmentation.\nIn experiments with multiple commonsense reasoning benchmarks, G-DAUG^C\nconsistently outperforms existing data augmentation methods based on\nback-translation, and establishes a new state-of-the-art on WinoGrande, CODAH,\nand CommonsenseQA. Further, in addition to improvements in in-distribution\naccuracy, G-DAUG^C-augmented training also enhances out-of-distribution\ngeneralization, showing greater robustness against adversarial or perturbed\nexamples. Our analysis demonstrates that G-DAUG^C produces a diverse set of\nfluent training examples, and that its selection and training approaches are\nimportant for performance. Our findings encourage future research toward\ngenerative data augmentation to enhance both in-distribution learning and\nout-of-distribution generalization.", "published": "2020-04-24 06:12:10", "link": "http://arxiv.org/abs/2004.11546v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistically Masked Language Model Capable of Autoregressive\n  Generation in Arbitrary Word Order", "abstract": "Masked language model and autoregressive language model are two types of\nlanguage models. While pretrained masked language models such as BERT overwhelm\nthe line of natural language understanding (NLU) tasks, autoregressive language\nmodels such as GPT are especially capable in natural language generation (NLG).\nIn this paper, we propose a probabilistic masking scheme for the masked\nlanguage model, which we call probabilistically masked language model (PMLM).\nWe implement a specific PMLM with a uniform prior distribution on the masking\nratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive\npermutated language model. One main advantage of the model is that it supports\ntext generation in arbitrary order with surprisingly good quality, which could\npotentially enable new applications over traditional unidirectional generation.\nBesides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU\ntasks.", "published": "2020-04-24 07:38:19", "link": "http://arxiv.org/abs/2004.11579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Customization and modifications of SignWriting by LIS users", "abstract": "Historically, the various sign languages (SL) have not developed an own\nwriting system; nevertheless, some systems exist, among which the SignWriting\n(SW) is a powerful and flexible one. In this paper, we present the mechanisms\nadopted by signers of the Italian Sign Language (LIS), expert users of SW, to\nmodify the standard SW glyphs and increase their writing skills and/or\nrepresent peculiar linguistic phenomena. We identify these glyphs and show\nwhich characteristics make them \"acceptable\" by the expert community.\nEventually, we analyze the potentialities of these glyphs in hand writing and\nin computer-assisted writing, focusing on SWift, a software designed to allow\nthe electronic writing-down of user-modified glyphs.", "published": "2020-04-24 07:49:45", "link": "http://arxiv.org/abs/2004.11583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning the grammar of drug prescription: recurrent neural network\n  grammars for medication information extraction in clinical texts", "abstract": "In this study, we evaluated the RNNG, a neural top-down transition based\nparser, for medication information extraction in clinical texts. We evaluated\nthis model on a French clinical corpus. The task was to extract the name of a\ndrug (or a drug class), as well as attributes informing its administration:\nfrequency, dosage, duration, condition and route of administration. We compared\nthe RNNG model that jointly identifies entities, events and their relations\nwith separate BiLSTMs models for entities, events and relations as baselines.\nWe call seq-BiLSTMs the baseline models for relations extraction that takes as\nextra-input the output of the BiLSTMs for entities and events. Similarly, we\nevaluated seq-RNNG, a hybrid RNNG model that takes as extra-input the output of\nthe BiLSTMs for entities and events. RNNG outperforms seq-BiLSTM for\nidentifying complex relations, with on average 88.1 [84.4-91.6] % versus 69.9\n[64.0-75.4] F-measure. However, RNNG tends to be weaker than the baseline\nBiLSTM on detecting entities, with on average 82.4 [80.8-83.8] versus 84.1\n[82.7-85.6] % F- measure. RNNG trained only for detecting relations tends to be\nweaker than RNNG with the joint modelling objective, 87.4% [85.8-88.8] versus\n88.5% [87.2-89.8]. Seq-RNNG is on par with BiLSTM for entities (84.0\n[82.6-85.4] % F-measure) and with RNNG for relations (88.7 [87.4-90.0] %\nF-measure). The performance of RNNG on relations can be explained both by the\nmodel architecture, which provides inductive bias to capture the hierarchy in\nthe targets, and the joint modeling objective which allows the RNNG to learn\nricher representations. RNNG is efficient for modeling relations between\nentities or/and events in medical texts and its performances are close to those\nof a BiLSTM for entity and event detection.", "published": "2020-04-24 09:43:14", "link": "http://arxiv.org/abs/2004.11622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling", "abstract": "As an essential task in task-oriented dialog systems, slot filling requires\nextensive training data in a certain domain. However, such data are not always\navailable. Hence, cross-domain slot filling has naturally arisen to cope with\nthis data scarcity problem. In this paper, we propose a Coarse-to-fine approach\n(Coach) for cross-domain slot filling. Our model first learns the general\npattern of slot entities by detecting whether the tokens are slot entities or\nnot. It then predicts the specific types for the slot entities. In addition, we\npropose a template regularization approach to improve the adaptation robustness\nby regularizing the representation of utterances based on utterance templates.\nExperimental results show that our model significantly outperforms\nstate-of-the-art approaches in slot filling. Furthermore, our model can also be\napplied to the cross-domain named entity recognition task, and it achieves\nbetter adaptation performance than other existing baselines. The code is\navailable at https://github.com/zliucr/coach.", "published": "2020-04-24 13:07:12", "link": "http://arxiv.org/abs/2004.11727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ST$^2$: Small-data Text Style Transfer via Multi-task Meta-Learning", "abstract": "Text style transfer aims to paraphrase a sentence in one style into another\nstyle while preserving content. Due to lack of parallel training data,\nstate-of-art methods are unsupervised and rely on large datasets that share\ncontent. Furthermore, existing methods have been applied on very limited\ncategories of styles such as positive/negative and formal/informal. In this\nwork, we develop a meta-learning framework to transfer between any kind of text\nstyles, including personal writing styles that are more fine-grained, share\nless content and have much smaller training data. While state-of-art models\nfail in the few-shot style transfer task, our framework effectively utilizes\ninformation from other styles to improve both language fluency and style\ntransfer accuracy.", "published": "2020-04-24 13:36:38", "link": "http://arxiv.org/abs/2004.11742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Explainable Selection to Control Abstractive Summarization", "abstract": "Like humans, document summarization models can interpret a document's\ncontents in a number of ways. Unfortunately, the neural models of today are\nlargely black boxes that provide little explanation of how or why they\ngenerated a summary in the way they did. Therefore, to begin prying open the\nblack box and to inject a level of control into the substance of the final\nsummary, we developed a novel select-and-generate framework that focuses on\nexplainability. By revealing the latent centrality and interactions between\nsentences, along with scores for sentence novelty and relevance, users are\ngiven a window into the choices a model is making and an opportunity to guide\nthose choices in a more desirable direction. A novel pair-wise matrix captures\nthe sentence interactions, centrality, and attribute scores, and a mask with\ntunable attribute thresholds allows the user to control which sentences are\nlikely to be included in the extraction. A sentence-deployed attention\nmechanism in the abstractor ensures the final summary emphasizes the desired\ncontent. Additionally, the encoder is adaptable, supporting both Transformer-\nand BERT-based configurations. In a series of experiments assessed with ROUGE\nmetrics and two human evaluations, ESCA outperformed eight state-of-the-art\nmodels on the CNN/DailyMail and NYT50 benchmark datasets.", "published": "2020-04-24 14:39:34", "link": "http://arxiv.org/abs/2004.11779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FLAT: Chinese NER Using Flat-Lattice Transformer", "abstract": "Recently, the character-word lattice structure has been proved to be\neffective for Chinese named entity recognition (NER) by incorporating the word\ninformation. However, since the lattice structure is complex and dynamic, most\nexisting lattice-based models are hard to fully utilize the parallel\ncomputation of GPUs and usually have a low inference-speed. In this paper, we\npropose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the\nlattice structure into a flat structure consisting of spans. Each span\ncorresponds to a character or latent word and its position in the original\nlattice. With the power of Transformer and well-designed position encoding,\nFLAT can fully leverage the lattice information and has an excellent\nparallelization ability. Experiments on four datasets show FLAT outperforms\nother lexicon-based models in performance and efficiency.", "published": "2020-04-24 15:27:49", "link": "http://arxiv.org/abs/2004.11795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models", "abstract": "Sequence-to-sequence models usually transfer all encoder outputs to the\ndecoder for generation. In this work, by contrast, we hypothesize that these\nencoder outputs can be compressed to shorten the sequence delivered for\ndecoding. We take Transformer as the testbed and introduce a layer of\nstochastic gates in-between the encoder and the decoder. The gates are\nregularized using the expected value of the sparsity-inducing L0penalty,\nresulting in completely masking-out a subset of encoder outputs. In other\nwords, via joint training, the L0DROP layer forces Transformer to route\ninformation through a subset of its encoder states. We investigate the effects\nof this sparsification on two machine translation and two summarization tasks.\nExperiments show that, depending on the task, around 40-70% of source encodings\ncan be pruned without significantly compromising quality. The decrease of the\noutput length endows L0DROP with the potential of improving decoding\nefficiency, where it yields a speedup of up to 1.65x on document summarization\ntasks against the standard Transformer. We analyze the L0DROP behaviour and\nobserve that it exhibits systematic preferences for pruning certain word types,\ne.g., function words and punctuation get pruned most. Inspired by these\nobservations, we explore the feasibility of specifying rule-based patterns that\nmask out encoder outputs based on information such as part-of-speech tags, word\nfrequency and word position.", "published": "2020-04-24 16:57:52", "link": "http://arxiv.org/abs/2004.11854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Massively Multilingual Neural Machine Translation and\n  Zero-Shot Translation", "abstract": "Massively multilingual models for neural machine translation (NMT) are\ntheoretically attractive, but often underperform bilingual models and deliver\npoor zero-shot translations. In this paper, we explore ways to improve them. We\nargue that multilingual NMT requires stronger modeling capacity to support\nlanguage pairs with varying typological characteristics, and overcome this\nbottleneck via language-specific components and deepening NMT architectures. We\nidentify the off-target translation issue (i.e. translating into a wrong target\nlanguage) as the major source of the inferior zero-shot performance, and\npropose random online backtranslation to enforce the translation of unseen\ntraining language pairs. Experiments on OPUS-100 (a novel multilingual dataset\nwith 100 languages) show that our approach substantially narrows the\nperformance gap with bilingual models in both one-to-many and many-to-many\nsettings, and improves zero-shot performance by ~10 BLEU, approaching\nconventional pivot-based methods.", "published": "2020-04-24 17:21:32", "link": "http://arxiv.org/abs/2004.11867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lite Transformer with Long-Short Range Attention", "abstract": "Transformer has become ubiquitous in natural language processing (e.g.,\nmachine translation, question answering); however, it requires enormous amount\nof computations to achieve high performance, which makes it not suitable for\nmobile applications that are tightly constrained by the hardware resources and\nbattery. In this paper, we present an efficient mobile NLP architecture, Lite\nTransformer to facilitate deploying mobile NLP applications on edge devices.\nThe key primitive is the Long-Short Range Attention (LSRA), where one group of\nheads specializes in the local context modeling (by convolution) while another\ngroup specializes in the long-distance relationship modeling (by attention).\nSuch specialization brings consistent improvement over the vanilla transformer\non three well-established language tasks: machine translation, abstractive\nsummarization, and language modeling. Under constrained resources (500M/100M\nMACs), Lite Transformer outperforms transformer on WMT'14 English-French by\n1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of\ntransformer base model by 2.5x with 0.3 BLEU score degradation. Combining with\npruning and quantization, we further compressed the model size of Lite\nTransformer by 18.2x. For language modeling, Lite Transformer achieves 1.8\nlower perplexity than the transformer at around 500M MACs. Notably, Lite\nTransformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU\nfor the mobile NLP setting without the costly architecture search that requires\nmore than 250 GPU years. Code has been made available at\nhttps://github.com/mit-han-lab/lite-transformer.", "published": "2020-04-24 17:52:25", "link": "http://arxiv.org/abs/2004.11886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Template-Based Question Generation from Retrieved Sentences for Improved\n  Unsupervised Question Answering", "abstract": "Question Answering (QA) is in increasing demand as the amount of information\navailable online and the desire for quick access to this content grows. A\ncommon approach to QA has been to fine-tune a pretrained language model on a\ntask-specific labeled dataset. This paradigm, however, relies on scarce, and\ncostly to obtain, large-scale human-labeled data. We propose an unsupervised\napproach to training QA models with generated pseudo-training data. We show\nthat generating questions for QA training by applying a simple template on a\nrelated, retrieved sentence rather than the original context sentence improves\ndownstream QA performance by allowing the model to learn more complex\ncontext-question relationships. Training a QA model on this data gives a\nrelative improvement over a previous unsupervised model in F1 score on the\nSQuAD dataset by about 14%, and 20% when the answer is a named entity,\nachieving state-of-the-art performance on SQuAD for unsupervised QA.", "published": "2020-04-24 17:57:45", "link": "http://arxiv.org/abs/2004.11892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Practical Comparable Data Collection for Low-Resource Languages via\n  Images", "abstract": "We propose a method of curating high-quality comparable training data for\nlow-resource languages with monolingual annotators. Our method involves using a\ncarefully selected set of images as a pivot between the source and target\nlanguages by getting captions for such images in both languages independently.\nHuman evaluations on the English-Hindi comparable corpora created with our\nmethod show that 81.1% of the pairs are acceptable translations, and only 2.47%\nof the pairs are not translations at all. We further establish the potential of\nthe dataset collected through our approach by experimenting on two downstream\ntasks - machine translation and dictionary extraction. All code and data are\navailable at https://github.com/madaan/PML4DC-Comparable-Data-Collection.", "published": "2020-04-24 19:30:38", "link": "http://arxiv.org/abs/2004.11954v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in\n  Arabic", "abstract": "This paper describes our method for the task of Semantic Question Similarity\nin Arabic in the workshop on NLP Solutions for Under-Resourced Languages\n(NSURL). The aim is to build a model that is able to detect similar semantic\nquestions in the Arabic language for the provided dataset. Different methods of\ndetermining questions similarity are explored in this work. The proposed models\nachieved high F1-scores, which range from (88% to 96%). Our official best\nresult is produced from the ensemble model of using a pre-trained multilingual\nBERT model with different random seeds with 95.924% F1-Score, which ranks the\nfirst among nine participants teams.", "published": "2020-04-24 19:52:40", "link": "http://arxiv.org/abs/2004.11964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Protocols and Negative Results for Textual Entailment Data\n  Collection", "abstract": "Natural language inference (NLI) data has proven useful in benchmarking and,\nespecially, as pretraining data for tasks requiring language understanding.\nHowever, the crowdsourcing protocol that was used to collect this data has\nknown issues and was not explicitly optimized for either of these purposes, so\nit is likely far from ideal. We propose four alternative protocols, each aimed\nat improving either the ease with which annotators can produce sound training\nexamples or the quality and diversity of those examples. Using these\nalternatives and a fifth baseline protocol, we collect and compare five new\n8.5k-example training sets. In evaluations focused on transfer learning\napplications, our results are solidly negative, with models trained on our\nbaseline dataset yielding good transfer performance to downstream tasks, but\nnone of our four new methods (nor the recent ANLI) showing any improvements\nover that baseline. In a small silver lining, we observe that all four new\nprotocols, especially those where annotators edit pre-filled text boxes, reduce\npreviously observed issues with annotation artifacts.", "published": "2020-04-24 21:31:57", "link": "http://arxiv.org/abs/2004.11997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Data Augmentation Increases Robustness to Inference Heuristics", "abstract": "Pretrained neural models such as BERT, when fine-tuned to perform natural\nlanguage inference (NLI), often show high accuracy on standard datasets, but\ndisplay a surprising lack of sensitivity to word order on controlled challenge\nsets. We hypothesize that this issue is not primarily caused by the pretrained\nmodel's limitations, but rather by the paucity of crowdsourced NLI examples\nthat might convey the importance of syntactic structure at the fine-tuning\nstage. We explore several methods to augment standard training sets with\nsyntactically informative examples, generated by applying syntactic\ntransformations to sentences from the MNLI corpus. The best-performing\naugmentation method, subject/object inversion, improved BERT's accuracy on\ncontrolled examples that diagnose sensitivity to word order from 0.28 to 0.73,\nwithout affecting performance on the MNLI test set. This improvement\ngeneralized beyond the particular construction used for data augmentation,\nsuggesting that augmentation causes BERT to recruit abstract syntactic\nrepresentations.", "published": "2020-04-24 21:35:26", "link": "http://arxiv.org/abs/2004.11999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Representations Using Textual Encyclopedic Knowledge", "abstract": "We present a method to represent input texts by contextualizing them jointly\nwith dynamically retrieved textual encyclopedic background knowledge from\nmultiple documents. We apply our method to reading comprehension tasks by\nencoding questions and passages together with background sentences about the\nentities they mention. We show that integrating background knowledge from text\nis effective for tasks focusing on factual reasoning and allows direct reuse of\npowerful pretrained BERT-style encoders. Moreover, knowledge integration can be\nfurther improved with suitable pretraining via a self-supervised masked\nlanguage model objective over words in background-augmented input text. On\nTriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable\nRoBERTa models which do not integrate background knowledge dynamically. On\nMRQA, a large collection of diverse QA datasets, we see consistent gains\nin-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2\nF1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).", "published": "2020-04-24 22:08:09", "link": "http://arxiv.org/abs/2004.12006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Annealing for Informal Language Understanding Tasks", "abstract": "There is a huge performance gap between formal and informal language\nunderstanding tasks. The recent pre-trained models that improved the\nperformance of formal language understanding tasks did not achieve a comparable\nresult on informal language. We pro-pose a data annealing transfer learning\nprocedure to bridge the performance gap on informal natural language\nunderstanding tasks. It successfully utilizes a pre-trained model such as BERT\nin informal language. In our data annealing procedure, the training set\ncontains mainly formal text data at first; then, the proportion of the informal\ntext data is gradually increased during the training process. Our data\nannealing procedure is model-independent and can be applied to various tasks.\nWe validate its effectiveness in exhaustive experiments. When BERT is\nimplemented with our learning procedure, it outperforms all the\nstate-of-the-art models on the three common informal language tasks.", "published": "2020-04-24 09:27:09", "link": "http://arxiv.org/abs/2004.13833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Sentiment Classification and Topic Discovery on Novel Coronavirus\n  or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network\n  Approach", "abstract": "Internet forums and public social media, such as online healthcare forums,\nprovide a convenient channel for users (people/patients) concerned about health\nissues to discuss and share information with each other. In late December 2019,\nan outbreak of a novel coronavirus (infection from which results in the disease\nnamed COVID-19) was reported, and, due to the rapid spread of the virus in\nother parts of the world, the World Health Organization declared a state of\nemergency. In this paper, we used automated extraction of COVID-19 related\ndiscussions from social media and a natural language process (NLP) method based\non topic modeling to uncover various issues related to COVID-19 from public\nopinions. Moreover, we also investigate how to use LSTM recurrent neural\nnetwork for sentiment classification of COVID-19 comments. Our findings shed\nlight on the importance of using public opinions and suitable computational\ntechniques to understand issues surrounding COVID-19 and to guide related\ndecision-making.", "published": "2020-04-24 16:29:13", "link": "http://arxiv.org/abs/2004.11695v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Event-QA: A Dataset for Event-Centric Question Answering over Knowledge\n  Graphs", "abstract": "Semantic Question Answering (QA) is a crucial technology to facilitate\nintuitive user access to semantic information stored in knowledge graphs.\nWhereas most of the existing QA systems and datasets focus on entity-centric\nquestions, very little is known about these systems' performance in the context\nof events. As new event-centric knowledge graphs emerge, datasets for such\nquestions gain importance. In this paper, we present the Event-QA dataset for\nanswering event-centric questions over knowledge graphs. Event-QA contains 1000\nsemantic queries and the corresponding English, German and Portuguese\nverbalizations for EventKG - an event-centric knowledge graph with more than\n970 thousand events.", "published": "2020-04-24 17:11:37", "link": "http://arxiv.org/abs/2004.11861v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question Answering over Curated and Open Web Sources", "abstract": "The last few years have seen an explosion of research on the topic of\nautomated question answering (QA), spanning the communities of information\nretrieval, natural language processing, and artificial intelligence. This\ntutorial would cover the highlights of this really active period of growth for\nQA to give the audience a grasp over the families of algorithms that are\ncurrently being used. We partition research contributions by the underlying\nsource from where answers are retrieved: curated knowledge graphs, unstructured\ntext, or hybrid corpora. We choose this dimension of partitioning as it is the\nmost discriminative when it comes to algorithm design. Other key dimensions are\ncovered within each sub-topic: like the complexity of questions addressed, and\ndegrees of explainability and interactivity introduced in the systems. We would\nconclude the tutorial with the most promising emerging trends in the expanse of\nQA, that would help new entrants into this field make the best decisions to\ntake the community forward. Much has changed in the community since the last\ntutorial on QA in SIGIR 2016, and we believe that this timely overview will\nindeed benefit a large number of conference participants.", "published": "2020-04-24 20:35:11", "link": "http://arxiv.org/abs/2004.11980v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Tailored Pre-Training Model for Task-Oriented Dialog Generation", "abstract": "The recent success of large pre-trained language models such as BERT and\nGPT-2 has suggested the effectiveness of incorporating language priors in\ndownstream dialog generation tasks. However, the performance of pre-trained\nmodels on the dialog task is not as optimal as expected. In this paper, we\npropose a Pre-trained Role Alternating Language model (PRAL), designed\nspecifically for task-oriented conversational systems. We adopted (Wu et al.,\n2019) that models two speakers separately. We also design several techniques,\nsuch as start position randomization, knowledge distillation, and history\ndiscount to improve pre-training performance. We introduce a task-oriented\ndialog pretraining dataset by cleaning 13 existing data sets. We test PRAL on\nthree different downstream tasks. The results show that PRAL performs better or\non par with state-of-the-art methods.", "published": "2020-04-24 09:25:45", "link": "http://arxiv.org/abs/2004.13835v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Social Interactions or Business Transactions? What customer reviews\n  disclose about Airbnb marketplace", "abstract": "Airbnb is one of the most successful examples of sharing economy\nmarketplaces. With rapid and global market penetration, understanding its\nattractiveness and evolving growth opportunities is key to plan business\ndecision making. There is an ongoing debate, for example, about whether Airbnb\nis a hospitality service that fosters social exchanges between hosts and\nguests, as the sharing economy manifesto originally stated, or whether it is\n(or is evolving into being) a purely business transaction platform, the way\nhotels have traditionally operated. To answer these questions, we propose a\nnovel market analysis approach that exploits customers' reviews. Key to the\napproach is a method that combines thematic analysis and machine learning to\ninductively develop a custom dictionary for guests' reviews. Based on this\ndictionary, we then use quantitative linguistic analysis on a corpus of 3.2\nmillion reviews collected in 6 different cities, and illustrate how to answer a\nvariety of market research questions, at fine levels of temporal, thematic,\nuser and spatial granularity, such as (i) how the business vs social dichotomy\nis evolving over the years, (ii) what exact words within such top-level\ncategories are evolving, (iii) whether such trends vary across different user\nsegments and (iv) in different neighbourhoods.", "published": "2020-04-24 09:08:46", "link": "http://arxiv.org/abs/2004.11604v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News\n  Detection on Social Media", "abstract": "This paper solves the fake news detection problem under a more realistic\nscenario on social media. Given the source short-text tweet and the\ncorresponding sequence of retweet users without text comments, we aim at\npredicting whether the source tweet is fake or not, and generating explanation\nby highlighting the evidences on suspicious retweeters and the words they\nconcern. We develop a novel neural network-based model, Graph-aware\nCo-Attention Networks (GCAN), to achieve the goal. Extensive experiments\nconducted on real tweet datasets exhibit that GCAN can significantly outperform\nstate-of-the-art methods by 16% in accuracy on average. In addition, the case\nstudies also show that GCAN can produce reasonable explanations.", "published": "2020-04-24 10:42:49", "link": "http://arxiv.org/abs/2004.11648v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Corpus-level and Concept-based Explanations for Interpretable Document\n  Classification", "abstract": "Using attention weights to identify information that is important for models'\ndecision-making is a popular approach to interpret attention-based neural\nnetworks. This is commonly realized in practice through the generation of a\nheat-map for every single document based on attention weights. However, this\ninterpretation method is fragile, and easy to find contradictory examples. In\nthis paper, we propose a corpus-level explanation approach, which aims to\ncapture causal relationships between keywords and model predictions via\nlearning the importance of keywords for predicted labels across a training\ncorpus based on attention weights. Based on this idea, we further propose a\nconcept-based explanation method that can automatically learn higher-level\nconcepts and their importance to model prediction tasks. Our concept-based\nexplanation method is built upon a novel Abstraction-Aggregation Network, which\ncan automatically cluster important keywords during an end-to-end training\nprocess. We apply these methods to the document classification task and show\nthat they are powerful in extracting semantically meaningful keywords and\nconcepts. Our consistency analysis results based on an attention-based Na\\\"ive\nBayes classifier also demonstrate these keywords and concepts are important for\nmodel predictions.", "published": "2020-04-24 20:54:17", "link": "http://arxiv.org/abs/2004.13003v4", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Cross-lingual Information Retrieval with BERT", "abstract": "Multiple neural language models have been developed recently, e.g., BERT and\nXLNet, and achieved impressive results in various NLP tasks including sentence\nclassification, question answering and document ranking. In this paper, we\nexplore the use of the popular bidirectional language model, BERT, to model and\nlearn the relevance between English queries and foreign-language documents in\nthe task of cross-lingual information retrieval. A deep relevance matching\nmodel based on BERT is introduced and trained by finetuning a pretrained\nmultilingual BERT model with weak supervision, using home-made CLIR training\ndata derived from parallel corpora. Experimental results of the retrieval of\nLithuanian documents against short English queries show that our model is\neffective and outperforms the competitive baseline approaches.", "published": "2020-04-24 23:32:13", "link": "http://arxiv.org/abs/2004.13005v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "How fine can fine-tuning be? Learning efficient language models", "abstract": "State-of-the-art performance on language understanding tasks is now achieved\nwith increasingly large networks; the current record holder has billions of\nparameters. Given a language model pre-trained on massive unlabeled text\ncorpora, only very light supervised fine-tuning is needed to learn a task: the\nnumber of fine-tuning steps is typically five orders of magnitude lower than\nthe total parameter count. Does this mean that fine-tuning only introduces\nsmall differences from the pre-trained model in the parameter space? If so, can\none avoid storing and computing an entire model for each task? In this work, we\naddress these questions by using Bidirectional Encoder Representations from\nTransformers (BERT) as an example. As expected, we find that the fine-tuned\nmodels are close in parameter space to the pre-trained one, with the closeness\nvarying from layer to layer. We show that it suffices to fine-tune only the\nmost critical layers. Further, we find that there are surprisingly many good\nsolutions in the set of sparsified versions of the pre-trained model. As a\nresult, fine-tuning of huge language models can be achieved by simply setting a\ncertain number of entries in certain layers of the pre-trained parameters to\nzero, saving both task-specific parameter storage and computational cost.", "published": "2020-04-24 20:31:28", "link": "http://arxiv.org/abs/2004.14129v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Development of a General Purpose Sentiment Lexicon for Igbo Language", "abstract": "There are publicly available general purpose sentiment lexicons in some high\nresource languages but very few exist in the low resource languages. This makes\nit difficult to directly perform sentiment analysis tasks in such languages.\nThe objective of this work is to create a general purpose sentiment lexicon for\nthe Igbo language that can determine the sentiment of documents written in the\nIgbo language without having to translate it to the English language. The\nmaterial used was an automatically translated lexicon by Liu and the manual\naddition of Igbo native words. The result of this work is a general purpose\nlexicon called IgboSentilex. The performance was tested on the BBC Igbo news\nchannel. It returned an average polarity agreement of 95.75 percent with other\ngeneral purpose sentiment lexicons.", "published": "2020-04-24 22:10:34", "link": "http://arxiv.org/abs/2004.14176v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Fast and Accurate Streaming End-to-End ASR", "abstract": "End-to-end (E2E) models fold the acoustic, pronunciation and language models\nof a conventional speech recognition model into one neural network with a much\nsmaller number of parameters than a conventional ASR system, thus making it\nsuitable for on-device applications. For example, recurrent neural network\ntransducer (RNN-T) as a streaming E2E model has shown promising potential for\non-device ASR. For such applications, quality and latency are two critical\nfactors. We propose to reduce E2E model's latency by extending the RNN-T\nendpointer (RNN-T EP) model with additional early and late penalties. By\nfurther applying the minimum word error rate (MWER) training technique, we\nachieved 8.0% relative word error rate (WER) reduction and 130ms 90-percentile\nlatency reduction over on a Voice Search test set. We also experimented with a\nsecond-pass Listen, Attend and Spell (LAS) rescorer . Although it did not\ndirectly improve the first pass latency, the large WER reduction provides extra\nroom to trade WER for latency. RNN-T EP+LAS, together with MWER training brings\nin 18.7% relative WER reduction and 160ms 90-percentile latency reductions\ncompared to the original proposed RNN-T EP model.", "published": "2020-04-24 05:58:30", "link": "http://arxiv.org/abs/2004.11544v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Binaural Audio Source Remixing with Microphone Array Listening Devices", "abstract": "Augmented listening devices, such as hearing aids and augmented reality\nheadsets, enhance human perception by changing the sounds that we hear.\nMicrophone arrays can improve the performance of listening systems in noisy\nenvironments, but most array-based listening systems are designed to isolate a\nsingle sound source from a mixture. This work considers a source-remixing\nfilter that alters the relative level of each source independently. Remixing\nrather than separating sounds can help to improve perceptual transparency: it\ncauses less distortion to the signal spectrum and especially to the interaural\ncues that humans use to localize sounds in space.", "published": "2020-04-24 19:34:38", "link": "http://arxiv.org/abs/2004.11956v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
