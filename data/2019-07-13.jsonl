{"title": "Cross-Lingual Transfer Learning for Question Answering", "abstract": "Deep learning based question answering (QA) on English documents has achieved\nsuccess because there is a large amount of English training examples. However,\nfor most languages, training examples for high-quality QA models are not\navailable. In this paper, we explore the problem of cross-lingual transfer\nlearning for QA, where a source language task with plentiful annotations is\nutilized to improve the performance of a QA model on a target language task\nwith limited available annotations. We examine two different approaches. A\nmachine translation (MT) based approach translates the source language into the\ntarget language, or vice versa. Although the MT-based approach brings\nimprovement, it assumes the availability of a sentence-level translation\nsystem. A GAN-based approach incorporates a language discriminator to learn\nlanguage-universal feature representations, and consequentially transfer\nknowledge from the source language. The GAN-based approach rivals the\nperformance of the MT-based approach with fewer linguistic resources. Applying\nboth approaches simultaneously yield the best results. We use two English\nbenchmark datasets, SQuAD and NewsQA, as source language data, and show\nsignificant improvements over a number of established baselines on a Chinese QA\ntask. We achieve the new state-of-the-art on the Chinese QA dataset.", "published": "2019-07-13 10:04:37", "link": "http://arxiv.org/abs/1907.06042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Relational Memory-based Embedding Model for Triple Classification and\n  Search Personalization", "abstract": "Knowledge graph embedding methods often suffer from a limitation of\nmemorizing valid triples to predict new ones for triple classification and\nsearch personalization problems. To this end, we introduce a novel embedding\nmodel, named R-MeN, that explores a relational memory network to encode\npotential dependencies in relationship triples. R-MeN considers each triple as\na sequence of 3 input vectors that recurrently interact with a memory using a\ntransformer self-attention mechanism. Thus R-MeN encodes new information from\ninteractions between the memory and each input vector to return a corresponding\nvector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional\nneural network-based decoder to produce a scalar score for the triple.\nExperimental results show that our proposed R-MeN obtains state-of-the-art\nresults on SEARCH17 for the search personalization task, and on WN11 and FB13\nfor the triple classification task.", "published": "2019-07-13 14:01:27", "link": "http://arxiv.org/abs/1907.06080v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learn Spelling from Teachers: Transferring Knowledge from Language\n  Models to Sequence-to-Sequence Speech Recognition", "abstract": "Integrating an external language model into a sequence-to-sequence speech\nrecognition system is non-trivial. Previous works utilize linear interpolation\nor a fusion network to integrate external language models. However, these\napproaches introduce external components, and increase decoding computation. In\nthis paper, we instead propose a knowledge distillation based training approach\nto integrating external language models into a sequence-to-sequence model. A\nrecurrent neural network language model, which is trained on large scale\nexternal text, generates soft labels to guide the sequence-to-sequence model\ntraining. Thus, the language model plays the role of the teacher. This approach\ndoes not add any external component to the sequence-to-sequence model during\ntesting. And this approach is flexible to be combined with shallow fusion\ntechnique together for decoding. The experiments are conducted on public\nChinese datasets AISHELL-1 and CLMAD. Our approach achieves a character error\nrate of 9.3%, which is relatively reduced by 18.42% compared with the vanilla\nsequence-to-sequence model.", "published": "2019-07-13 06:27:24", "link": "http://arxiv.org/abs/1907.06017v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Tackling Graphical NLP problems with Graph Recurrent Networks", "abstract": "How to properly model graphs is a long-existing and important problem in NLP\narea, where several popular types of graphs are knowledge graphs, semantic\ngraphs and dependency graphs. Comparing with other data structures, such as\nsequences and trees, graphs are generally more powerful in representing complex\ncorrelations among entities. For example, a knowledge graph stores real-word\nentities (such as \"Barack_Obama\" and \"U.S.\") and their relations (such as\n\"live_in\" and \"lead_by\"). Properly encoding a knowledge graph is beneficial to\nuser applications, such as question answering and knowledge discovery. Modeling\ngraphs is also very challenging, probably because graphs usually contain\nmassive and cyclic relations.\n  Recent years have witnessed the success of deep learning, especially\nRNN-based models, on many NLP problems. Besides, RNNs and their variations have\nbeen extensively studied on several graph problems and showed preliminary\nsuccesses. Despite the successes that have been achieved, RNN-based models\nsuffer from several major drawbacks on graphs. First, they can only consume\nsequential data, thus linearization is required to serialize input graphs,\nresulting in the loss of important structural information. Second, the\nserialization results are usually very long, so it takes a long time for RNNs\nto encode them.\n  In this thesis, we propose a novel graph neural network, named graph\nrecurrent network (GRN). We study our GRN model on 4 very different tasks, such\nas machine reading comprehension, relation extraction and machine translation.\nSome take undirected graphs without edge labels, while the others have directed\nones with edge labels. To consider these important differences, we gradually\nenhance our GRN model, such as further considering edge labels and adding an\nRNN decoder. Carefully designed experiments show the effectiveness of GRN on\nall these tasks.", "published": "2019-07-13 22:48:31", "link": "http://arxiv.org/abs/1907.06142v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Recognition with Random Digit Strings Using Uncertainty\n  Normalized HMM-based i-vectors", "abstract": "In this paper, we combine Hidden Markov Models (HMMs) with i-vector\nextractors to address the problem of text-dependent speaker recognition with\nrandom digit strings. We employ digit-specific HMMs to segment the utterances\ninto digits, to perform frame alignment to HMM states and to extract Baum-Welch\nstatistics. By making use of the natural partition of input features into\ndigits, we train digit-specific i-vector extractors on top of each HMM and we\nextract well-localized i-vectors, each modelling merely the phonetic content\ncorresponding to a single digit. We then examine ways to perform channel and\nuncertainty compensation, and we propose a novel method for using the\nuncertainty in the i-vector estimates. The experiments on RSR2015 part III show\nthat the proposed method attains 1.52\\% and 1.77\\% Equal Error Rate (EER) for\nmale and female respectively, outperforming state-of-the-art methods such as\nx-vectors, trained on vast amounts of data. Furthermore, these results are\nattained by a single system trained entirely on RSR2015, and by a simple\nscore-normalized cosine distance. Moreover, we show that the omission of\nchannel compensation yields only a minor degradation in performance, meaning\nthat the system attains state-of-the-art results even without recordings from\nmultiple handsets per speaker for training or enrolment. Similar conclusions\nare drawn from our experiments on the RedDots corpus, where the same method is\nevaluated on phrases. Finally, we report results with bottleneck features and\nshow that further improvement is attained when fusing them with spectral\nfeatures.", "published": "2019-07-13 17:52:17", "link": "http://arxiv.org/abs/1907.06111v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BUT VOiCES 2019 System Description", "abstract": "This is a description of our effort in VOiCES 2019 Speaker Recognition\nchallenge. All systems in the fixed condition are based on the x-vector\nparadigm with different features and DNN topologies. The single best system\nreaches 1.2% EER and a fusion of 3 systems yields 1.0% EER, which is 15%\nrelative improvement. The open condition allowed us to use external data which\nwe did for the PLDA adaptation and achieved less than ~10% relative\nimprovement. In the submission to open condition, we used 3 x-vector systems\nand also one i-vector based system.", "published": "2019-07-13 17:57:55", "link": "http://arxiv.org/abs/1907.06112v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Task Semi-Supervised Adversarial Autoencoding for Speech Emotion\n  Recognition", "abstract": "Inspite the emerging importance of Speech Emotion Recognition (SER), the\nstate-of-the-art accuracy is quite low and needs improvement to make commercial\napplications of SER viable. A key underlying reason for the low accuracy is the\nscarcity of emotion datasets, which is a challenge for developing any robust\nmachine learning model in general. In this paper, we propose a solution to this\nproblem: a multi-task learning framework that uses auxiliary tasks for which\ndata is abundantly available. We show that utilisation of this additional data\ncan improve the primary task of SER for which only limited labelled data is\navailable. In particular, we use gender identifications and speaker recognition\nas auxiliary tasks, which allow the use of very large datasets, e.g., speaker\nclassification datasets. To maximise the benefit of multi-task learning, we\nfurther use an adversarial autoencoder (AAE) within our framework, which has a\nstrong capability to learn powerful and discriminative features. Furthermore,\nthe unsupervised AAE in combination with the supervised classification networks\nenables semi-supervised learning which incorporates a discriminative component\nin the AAE unsupervised training pipeline. This semi-supervised learning\nessentially helps to improve generalisation of our framework and thus leads to\nimprovements in SER performance. The proposed model is rigorously evaluated for\ncategorical and dimensional emotion, and cross-corpus scenarios. Experimental\nresults demonstrate that the proposed model achieves state-of-the-art\nperformance on two publicly available datasets.", "published": "2019-07-13 13:52:54", "link": "http://arxiv.org/abs/1907.06078v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Adversarial Domain Adaptation for Cross-Lingual Speech\n  Emotion Recognition", "abstract": "Cross-lingual speech emotion recognition (SER) is a crucial task for many\nreal-world applications. The performance of SER systems is often degraded by\nthe differences in the distributions of training and test data. These\ndifferences become more apparent when training and test data belong to\ndifferent languages, which cause a significant performance gap between the\nvalidation and test scores. It is imperative to build more robust models that\ncan fit in practical applications of SER systems. Therefore, in this paper, we\npropose a Generative Adversarial Network (GAN)-based model for multilingual\nSER. Our choice of using GAN is motivated by their great success in learning\nthe underlying data distribution. The proposed model is designed in such a way\nthat can learn language invariant representations without requiring\ntarget-language data labels. We evaluate our proposed model on four different\nlanguage emotional datasets, including an Urdu-language dataset to also\nincorporate alternative languages for which labelled data is difficult to find\nand which have not been studied much by the mainstream community. Our results\nshow that our proposed model can significantly improve the baseline\ncross-lingual SER performance for all the considered datasets including the\nnon-mainstream Urdu language data without requiring any labels.", "published": "2019-07-13 14:12:51", "link": "http://arxiv.org/abs/1907.06083v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Scene Classification Using Fusion of Attentive Convolutional\n  Neural Networks for DCASE2019 Challenge", "abstract": "In this report, the Brno University of Technology (BUT) team submissions for\nTask 1 (Acoustic Scene Classification, ASC) of the DCASE-2019 challenge are\ndescribed. Also, the analysis of different methods is provided. The proposed\napproach is a fusion of three different Convolutional Neural Network (CNN)\ntopologies. The first one is a VGG like two-dimensional CNNs. The second one is\nagain a two-dimensional CNN network which uses Max-Feature-Map activation and\ncalled Light-CNN (LCNN). The third network is a one-dimensional CNN which\nmainly used for speaker verification and called x-vector topology. All proposed\nnetworks use self-attention mechanism for statistic pooling. As a feature, we\nuse a 256-dimensional log Mel-spectrogram. Our submissions are a fusion of\nseveral networks trained on 4-folds generated evaluation setup using different\nfusion strategies.", "published": "2019-07-13 18:10:34", "link": "http://arxiv.org/abs/1907.07127v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Complex Basis Functions for Invariant Representations of Audio", "abstract": "Learning features from data has shown to be more successful than using\nhand-crafted features for many machine learning tasks. In music information\nretrieval (MIR), features learned from windowed spectrograms are highly variant\nto transformations like transposition or time-shift. Such variances are\nundesirable when they are irrelevant for the respective MIR task. We propose an\narchitecture called Complex Autoencoder (CAE) which learns features invariant\nto orthogonal transformations. Mapping signals onto complex basis functions\nlearned by the CAE results in a transformation-invariant \"magnitude space\" and\na transformation-variant \"phase space\". The phase space is useful to infer\ntransformations between data pairs. When exploiting the invariance-property of\nthe magnitude space, we achieve state-of-the-art results in audio-to-score\nalignment and repeated section discovery for audio. A PyTorch implementation of\nthe CAE, including the repeated section discovery method, is available online.", "published": "2019-07-13 00:23:26", "link": "http://arxiv.org/abs/1907.05982v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Voice Pathology Detection", "abstract": "Automatic objective non-invasive detection of pathological voice based on\ncomputerized analysis of acoustic signals can play an important role in early\ndiagnosis, progression tracking and even effective treatment of pathological\nvoices. In search towards such a robust voice pathology detection system we\ninvestigated 3 distinct classifiers within supervised learning and anomaly\ndetection paradigms. We conducted a set of experiments using a variety of input\ndata such as raw waveforms, spectrograms, mel-frequency cepstral coefficients\n(MFCC) and conventional acoustic (dysphonic) features (AF). In comparison with\npreviously published works, this article is the first to utilize combination of\n4 different databases comprising normophonic and pathological recordings of\nsustained phonation of the vowel /a/ unrestricted to a subset of vocal\npathologies. Furthermore, to our best knowledge, this article is the first to\nexplore gradient boosted trees and deep learning for this application. The\nfollowing best classification performances measured by F1 score on dedicated\ntest set were achieved: XGBoost (0.733) using AF and MFCC, DenseNet (0.621)\nusing MFCC, and Isolation Forest (0.610) using AF. Even though these results\nare of exploratory character, conducted experiments do show promising potential\nof gradient boosting and deep learning methods to robustly detect voice\npathologies.", "published": "2019-07-13 21:09:40", "link": "http://arxiv.org/abs/1907.06129v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
