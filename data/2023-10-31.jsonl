{"title": "Keyword-optimized Template Insertion for Clinical Information Extraction\n  via Prompt-based Learning", "abstract": "Clinical note classification is a common clinical NLP task. However,\nannotated data-sets are scarse. Prompt-based learning has recently emerged as\nan effective method to adapt pre-trained models for text classification using\nonly few training examples. A critical component of prompt design is the\ndefinition of the template (i.e. prompt text). The effect of template position,\nhowever, has been insufficiently investigated. This seems particularly\nimportant in the clinical setting, where task-relevant information is usually\nsparse in clinical notes. In this study we develop a keyword-optimized template\ninsertion method (KOTI) and show how optimizing position can improve\nperformance on several clinical tasks in a zero-shot and few-shot training\nsetting.", "published": "2023-10-31 00:07:11", "link": "http://arxiv.org/abs/2310.20089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Large Language Models Better Data Creators", "abstract": "Although large language models (LLMs) have advanced the state-of-the-art in\nNLP significantly, deploying them for downstream applications is still\nchallenging due to cost, responsiveness, control, or concerns around privacy\nand security. As such, trainable models are still the preferred option in some\ncases. However, these models still require human-labeled data for optimal\nperformance, which is expensive and time-consuming to obtain. In order to\naddress this issue, several techniques to reduce human effort involve labeling\nor generating data using LLMs. Although these methods are effective for certain\napplications, in practice they encounter difficulties in real-world scenarios.\nLabeling data requires careful data selection, while generating data\nnecessitates task-specific prompt engineering. In this paper, we propose a\nunified data creation pipeline that requires only a single formatting example,\nand which is applicable to a broad range of tasks, including traditionally\nproblematic ones with semantically devoid label spaces. In our experiments we\ndemonstrate that instruction-following LLMs are highly cost-effective data\ncreators, and that models trained with these data exhibit performance better\nthan those trained with human-labeled data (by up to 17.5%) on\nout-of-distribution evaluation, while maintaining comparable performance on\nin-distribution tasks. These results have important implications for the\nrobustness of NLP systems deployed in the real-world.", "published": "2023-10-31 01:08:34", "link": "http://arxiv.org/abs/2310.20111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ling-CL: Understanding NLP Models through Linguistic Curricula", "abstract": "We employ a characterization of linguistic complexity from psycholinguistic\nand language acquisition research to develop data-driven curricula to\nunderstand the underlying linguistic knowledge that models learn to address NLP\ntasks. The novelty of our approach is in the development of linguistic\ncurricula derived from data, existing knowledge about linguistic complexity,\nand model behavior during training. By analyzing several benchmark NLP\ndatasets, our curriculum learning approaches identify sets of linguistic\nmetrics (indices) that inform the challenges and reasoning required to address\neach task. Our work will inform future research in all NLP areas, allowing\nlinguistic complexity to be considered early in the research and development\nprocess. In addition, our work prompts an examination of gold standards and\nfair evaluation in NLP.", "published": "2023-10-31 01:44:33", "link": "http://arxiv.org/abs/2310.20121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Prompt Tuning with Learned Prompting Layers", "abstract": "Prompt tuning prepends a soft prompt to the input embeddings or hidden states\nand only optimizes the prompt to adapt pretrained models (PTMs) to downstream\ntasks. The previous work manually selects prompt layers which are far from\noptimal and failed to exploit the potential of prompt tuning. In this work, we\npropose a novel framework, \\underline{S}elective \\underline{P}rompt\n\\underline{T}uning (SPT), that learns to select the proper prompt layers by\ninserting a prompt controlled by a learnable probabilistic gate at each\nintermediate layer. We further propose a novel bi-level optimization framework,\nSPT-DARTS, that can better optimize the learnable gates and improve the final\nprompt tuning performances of the learned prompt layer settings. We conduct\nextensive experiments with ten benchmark datasets under the full-data and\nfew-shot scenarios. The results demonstrate that our SPT framework can perform\nbetter than the previous state-of-the-art PETuning baselines with comparable or\nfewer tunable parameters.", "published": "2023-10-31 02:07:51", "link": "http://arxiv.org/abs/2310.20127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval", "abstract": "Given a query and a document corpus, the information retrieval (IR) task is\nto output a ranked list of relevant documents. Combining large language models\n(LLMs) with embedding-based retrieval models, recent work shows promising\nresults on the zero-shot retrieval problem, i.e., no access to labeled data\nfrom the target domain. Two such popular paradigms are generation-augmented\nretrieval or GAR (generate additional context for the query and then retrieve),\nand retrieval-augmented generation or RAG (retrieve relevant documents as\ncontext and then generate answers). The success of these paradigms hinges on\n(i) high-recall retrieval models, which are difficult to obtain in the\nzero-shot setting, and (ii) high-precision (re-)ranking models which typically\nneed a good initialization. In this work, we propose a novel GAR-meets-RAG\nrecurrence formulation that overcomes the challenges of existing paradigms. Our\nmethod iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in\nthe zero-shot setting. A key design principle is that the rewrite-retrieval\nstages improve the recall of the system and a final re-ranking stage improves\nthe precision. We conduct extensive experiments on zero-shot passage retrieval\nbenchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in\nthe BEIR benchmark, outperforming previous best results in Recall@100 and\nnDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the\nprevious best.", "published": "2023-10-31 03:52:08", "link": "http://arxiv.org/abs/2310.20158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain\n  Question Answering over Knowledge Base and Text", "abstract": "Large Language Models (LLMs) have exhibited impressive generation\ncapabilities, but they suffer from hallucinations when solely relying on their\ninternal knowledge, especially when answering questions that require less\ncommonly known information. Retrieval-augmented LLMs have emerged as a\npotential solution to ground LLMs in external knowledge. Nonetheless, recent\napproaches have primarily emphasized retrieval from unstructured text corpora,\nowing to its seamless integration into prompts. When using structured data such\nas knowledge graphs, most methods simplify it into natural text, neglecting the\nunderlying structures. Moreover, a significant gap in the current landscape is\nthe absence of a realistic benchmark for evaluating the effectiveness of\ngrounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and\ntext). To fill this gap, we have curated a comprehensive dataset that poses two\nunique challenges: (1) Two-hop multi-source questions that require retrieving\ninformation from both open-domain structured and unstructured knowledge\nsources; retrieving information from structured knowledge sources is a critical\ncomponent in correctly answering the questions. (2) The generation of symbolic\nqueries (e.g., SPARQL for Wikidata) is a key requirement, which adds another\nlayer of challenge. Our dataset is created using a combination of automatic\ngeneration through predefined reasoning chains and human annotation. We also\nintroduce a novel approach that leverages multiple retrieval tools, including\ntext passage retrieval and symbolic language-assisted retrieval. Our model\noutperforms previous approaches by a significant margin, demonstrating its\neffectiveness in addressing the above-mentioned reasoning challenges.", "published": "2023-10-31 04:37:57", "link": "http://arxiv.org/abs/2310.20170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Video-Helpful Multimodal Machine Translation", "abstract": "Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or instructional video subtitles, which rarely contain\nlinguistic ambiguity, making visual information ineffective in generating\nappropriate translations. Recent work has constructed an ambiguous subtitles\ndataset to alleviate this problem but is still limited to the problem that\nvideos do not necessarily contribute to disambiguation. We introduce EVA\n(Extensive training set and Video-helpful evaluation set for Ambiguous\nsubtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En)\nparallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs,\nand corresponding video clips collected from movies and TV episodes. In\naddition to the extensive training set, EVA contains a video-helpful evaluation\nset in which subtitles are ambiguous, and videos are guaranteed helpful for\ndisambiguation. Furthermore, we propose SAFA, an MMT model based on the\nSelective Attention model with two novel methods: Frame attention loss and\nAmbiguity augmentation, aiming to use videos in EVA for disambiguation fully.\nExperiments on EVA show that visual information and the proposed methods can\nboost translation performance, and our model performs significantly better than\nexisting MMT models. The EVA dataset and the SAFA model are available at:\nhttps://github.com/ku-nlp/video-helpful-MMT.git.", "published": "2023-10-31 05:51:56", "link": "http://arxiv.org/abs/2310.20201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamically Updating Event Representations for Temporal Relation\n  Classification with Multi-category Learning", "abstract": "Temporal relation classification is a pair-wise task for identifying the\nrelation of a temporal link (TLINK) between two mentions, i.e. event, time, and\ndocument creation time (DCT). It leads to two crucial limits: 1) Two TLINKs\ninvolving a common mention do not share information. 2) Existing models with\nindependent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from\nusing the whole data. This paper presents an event centric model that allows to\nmanage dynamic event representations across multiple TLINKs. Our model deals\nwith three TLINK categories with multi-task learning to leverage the full size\nof data. The experimental results show that our proposal outperforms\nstate-of-the-art models and two transfer learning baselines on both the English\nand Japanese data.", "published": "2023-10-31 07:41:24", "link": "http://arxiv.org/abs/2310.20236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for\n  Personality Detection", "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have\nshowcased remarkable zero-shot performance across various NLP tasks. However,\nthe potential of LLMs in personality detection, which involves identifying an\nindividual's personality from their written texts, remains largely unexplored.\nDrawing inspiration from Psychological Questionnaires, which are carefully\ndesigned by psychologists to evaluate individual personality traits through a\nseries of targeted items, we argue that these items can be regarded as a\ncollection of well-structured chain-of-thought (CoT) processes. By\nincorporating these processes, LLMs can enhance their capabilities to make more\nreasonable inferences on personality from textual input. In light of this, we\npropose a novel personality detection method, called PsyCoT, which mimics the\nway individuals complete psychological questionnaires in a multi-turn dialogue\nmanner. In particular, we employ a LLM as an AI assistant with a specialization\nin text analysis. We prompt the assistant to rate individual items at each turn\nand leverage the historical rating results to derive a conclusive personality\npreference. Our experiments demonstrate that PsyCoT significantly improves the\nperformance and robustness of GPT-3.5 in personality detection, achieving an\naverage F1 score improvement of 4.23/10.63 points on two benchmark datasets\ncompared to the standard prompting method. Our code is available at\nhttps://github.com/TaoYang225/PsyCoT.", "published": "2023-10-31 08:23:33", "link": "http://arxiv.org/abs/2310.20256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating\n  Chess Moves based on Sentiment Analysis", "abstract": "Learning chess strategies has been investigated widely, with most studies\nfocussing on learning from previous games using search algorithms. Chess\ntextbooks encapsulate grandmaster knowledge, explain playing strategies and\nrequire a smaller search space compared to traditional chess agents. This paper\nexamines chess textbooks as a new knowledge source for enabling machines to\nlearn how to play chess -- a resource that has not been explored previously. We\ndeveloped the LEAP corpus, a first and new heterogeneous dataset with\nstructured (chess move notations and board states) and unstructured data\n(textual descriptions) collected from a chess textbook containing 1164\nsentences discussing strategic moves from 91 games. We firstly labelled the\nsentences based on their relevance, i.e., whether they are discussing a move.\nEach relevant sentence was then labelled according to its sentiment towards the\ndescribed move. We performed empirical experiments that assess the performance\nof various transformer-based baseline models for sentiment analysis. Our\nresults demonstrate the feasibility of employing transformer-based sentiment\nanalysis models for evaluating chess moves, with the best performing model\nobtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP\ncorpus to create a larger dataset, which can be used as a solution to the\nlimited textual resource in the chess domain.", "published": "2023-10-31 08:26:02", "link": "http://arxiv.org/abs/2310.20260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Erato: Automatizing Poetry Evaluation", "abstract": "We present Erato, a framework designed to facilitate the automated evaluation\nof poetry, including that generated by poetry generation systems. Our framework\nemploys a diverse set of features, and we offer a brief overview of Erato's\ncapabilities and its potential for expansion. Using Erato, we compare and\ncontrast human-authored poetry with automatically-generated poetry,\ndemonstrating its effectiveness in identifying key differences. Our\nimplementation code and software are freely available under the GNU GPLv3\nlicense.", "published": "2023-10-31 10:06:37", "link": "http://arxiv.org/abs/2310.20326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for\n  Computational Linguistics and Cognitive Science", "abstract": "In this resource paper we release ChiSCor, a new corpus containing 619\nfantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was\ncompiled for studying how children render character perspectives, and\nunravelling language and cognition in development, with computational tools.\nUnlike existing resources, ChiSCor's stories were produced in natural contexts,\nin line with recent calls for more ecologically valid datasets. ChiSCor hosts\ntext, audio, and annotations for character complexity and linguistic\ncomplexity. Additional metadata (e.g. education of caregivers) is available for\none third of the Dutch children. ChiSCor also includes a small set of 62\nEnglish stories. This paper details how ChiSCor was compiled and shows its\npotential for future work with three brief case studies: i) we show that the\nsyntactic complexity of stories is strikingly stable across children's ages;\nii) we extend work on Zipfian distributions in free speech and show that\nChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show\nthat even though ChiSCor is relatively small, the corpus is rich enough to\ntrain informative lemma vectors that allow us to analyse children's language\nuse. We end with a reflection on the value of narrative datasets in\ncomputational linguistics.", "published": "2023-10-31 10:15:20", "link": "http://arxiv.org/abs/2310.20328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generators for a Family of Matrix Multiplication Routines with\n  Apache TVM", "abstract": "We explore the utilization of the Apache TVM open source framework to\nautomatically generate a family of algorithms that follow the approach taken by\npopular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in\norder to obtain high-performance blocked formulations of the general matrix\nmultiplication (GEMM). % In addition, we fully automatize the generation\nprocess, by also leveraging the Apache TVM framework to derive a complete\nvariety of the processor-specific micro-kernels for GEMM. This is in contrast\nwith the convention in high performance libraries, which hand-encode a single\nmicro-kernel per architecture using Assembly code. % In global, the combination\nof our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves\nportability, maintainability and, globally, streamlines the software life\ncycle; 2)~provides high flexibility to easily tailor and optimize the solution\nto different data types, processor architectures, and matrix operand shapes,\nyielding performance on a par (or even superior for specific matrix shapes)\nwith that of hand-tuned libraries; and 3)~features a small memory footprint.", "published": "2023-10-31 10:36:26", "link": "http://arxiv.org/abs/2310.20347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMERICANO: Argument Generation with Discourse-driven Decomposition and\n  Agent Interaction", "abstract": "Argument generation is a challenging task in natural language processing,\nwhich requires rigorous reasoning and proper content organization. Inspired by\nrecent chain-of-thought prompting that breaks down a complex task into\nintermediate steps, we propose Americano, a novel framework with agent\ninteraction for argument generation. Our approach decomposes the generation\nprocess into sequential actions grounded on argumentation theory, which first\nexecutes actions sequentially to generate argumentative discourse components,\nand then produces a final argument conditioned on the components. To further\nmimic the human writing process and improve the left-to-right generation\nparadigm of current autoregressive language models, we introduce an argument\nrefinement module which automatically evaluates and refines argument drafts\nbased on feedback received. We evaluate our framework on the task of\ncounterargument generation using a subset of Reddit/CMV dataset. The results\nshow that our method outperforms both end-to-end and chain-of-thought prompting\nmethods and can generate more coherent and persuasive arguments with diverse\nand rich contents.", "published": "2023-10-31 10:47:33", "link": "http://arxiv.org/abs/2310.20352v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark\n  for Large Language Models", "abstract": "The ability to follow instructions is crucial for Large Language Models\n(LLMs) to handle various real-world applications. Existing benchmarks primarily\nfocus on evaluating pure response quality, rather than assessing whether the\nresponse follows constraints stated in the instruction. To fill this research\ngap, in this paper, we propose FollowBench, a Multi-level Fine-grained\nConstraints Following Benchmark for LLMs. FollowBench comprehensively includes\nfive different types (i.e., Content, Situation, Style, Format, and Example) of\nfine-grained constraints. To enable a precise constraint following estimation\non diverse difficulties, we introduce a Multi-level mechanism that\nincrementally adds a single constraint to the initial instruction at each\nincreased level. To assess whether LLMs' outputs have satisfied every\nindividual constraint, we propose to prompt strong LLMs with\nconstraint-evolution paths to handle challenging open-ended instructions. By\nevaluating 13 closed-source and open-source popular LLMs on FollowBench, we\nhighlight the weaknesses of LLMs in instruction following and point towards\npotential avenues for future work. The data and code are publicly available at\nhttps://github.com/YJiangcm/FollowBench.", "published": "2023-10-31 12:32:38", "link": "http://arxiv.org/abs/2310.20410v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating curation into scientific publishing to train AI models", "abstract": "High throughput extraction and structured labeling of data from academic\narticles is critical to enable downstream machine learning applications and\nsecondary analyses. We have embedded multimodal data curation into the academic\npublishing process to annotate segmented figure panels and captions. Natural\nlanguage processing (NLP) was combined with human-in-the-loop feedback from the\noriginal authors to increase annotation accuracy. Annotation included eight\nclasses of bioentities (small molecules, gene products, subcellular components,\ncell lines, cell types, tissues, organisms, and diseases) plus additional\nclasses delineating the entities' roles in experiment designs and\nmethodologies. The resultant dataset, SourceData-NLP, contains more than\n620,000 annotated biomedical entities, curated from 18,689 figures in 3,223\narticles in molecular and cell biology. We evaluate the utility of the dataset\nto train AI models using named-entity recognition, segmentation of figure\ncaptions into their constituent panels, and a novel context-dependent semantic\ntask assessing whether an entity is a controlled intervention target or a\nmeasurement object. We also illustrate the use of our dataset in performing a\nmulti-modal task for segmenting figures into panel images and their\ncorresponding captions.", "published": "2023-10-31 13:22:38", "link": "http://arxiv.org/abs/2310.20440v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Deep Understanding of Multilingual End-to-End Speech\n  Translation", "abstract": "In this paper, we employ Singular Value Canonical Correlation Analysis\n(SVCCA) to analyze representations learnt in a multilingual end-to-end speech\ntranslation model trained over 22 languages. SVCCA enables us to estimate\nrepresentational similarity across languages and layers, enhancing our\nunderstanding of the functionality of multilingual speech translation and its\npotential connection to multilingual neural machine translation. The\nmultilingual speech translation model is trained on the CoVoST 2 dataset in all\npossible directions, and we utilize LASER to extract parallel bitext data for\nSVCCA analysis. We derive three major findings from our analysis: (I)\nLinguistic similarity loses its efficacy in multilingual speech translation\nwhen the training data for a specific language is limited. (II) Enhanced\nencoder representations and well-aligned audio-text data significantly improve\ntranslation quality, surpassing the bilingual counterparts when the training\ndata is not compromised. (III) The encoder representations of multilingual\nspeech translation demonstrate superior performance in predicting phonetic\nfeatures in linguistic typology prediction. With these findings, we propose\nthat releasing the constraint of limited data for low-resource languages and\nsubsequently combining them with linguistically related high-resource languages\ncould offer a more effective approach for multilingual end-to-end speech\ntranslation.", "published": "2023-10-31 13:50:55", "link": "http://arxiv.org/abs/2310.20456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representativeness as a Forgotten Lesson for Multilingual and\n  Code-switched Data Collection and Preparation", "abstract": "Multilingualism is widespread around the world and code-switching (CSW) is a\ncommon practice among different language pairs/tuples across locations and\nregions. However, there is still not much progress in building successful CSW\nsystems, despite the recent advances in Massive Multilingual Language Models\n(MMLMs). We investigate the reasons behind this setback through a critical\nstudy about the existing CSW data sets (68) across language pairs in terms of\nthe collection and preparation (e.g. transcription and annotation) stages. This\nin-depth analysis reveals that \\textbf{a)} most CSW data involves English\nignoring other language pairs/tuples \\textbf{b)} there are flaws in terms of\nrepresentativeness in data collection and preparation stages due to ignoring\nthe location based, socio-demographic and register variation in CSW. In\naddition, lack of clarity on the data selection and filtering stages shadow the\nrepresentativeness of CSW data sets. We conclude by providing a short\ncheck-list to improve the representativeness for forthcoming studies involving\nCSW data collection and preparation.", "published": "2023-10-31 14:04:07", "link": "http://arxiv.org/abs/2310.20470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users", "abstract": "While most task-oriented dialogues assume conversations between the agent and\none user at a time, dialogue systems are increasingly expected to communicate\nwith multiple users simultaneously who make decisions collaboratively. To\nfacilitate development of such systems, we release the Multi-User MultiWOZ\ndataset: task-oriented dialogues among two users and one agent. To collect this\ndataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat\nbetween two users that is semantically and pragmatically consistent with the\noriginal user utterance, thus resulting in the same dialogue state and system\nresponse. These dialogues reflect interesting dynamics of collaborative\ndecision-making in task-oriented scenarios, e.g., social chatter and\ndeliberation. Supported by this data, we propose the novel task of multi-user\ncontextual query rewriting: to rewrite a task-oriented chat between two users\nas a concise task-oriented query that retains only task-relevant information\nand that is directly consumable by the dialogue system. We demonstrate that in\nmulti-user dialogues, using predicted rewrites substantially improves dialogue\nstate tracking without modifying existing dialogue systems that are trained for\nsingle-user dialogues. Further, this method surpasses training a medium-sized\nmodel directly on multi-user dialogues and generalizes to unseen domains.", "published": "2023-10-31 14:12:07", "link": "http://arxiv.org/abs/2310.20479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Word Guessing Games to Assess the Intelligence of Large\n  Language Models", "abstract": "The automatic evaluation of LLM-based agent intelligence is critical in\ndeveloping advanced LLM-based agents. Although considerable effort has been\ndevoted to developing human-annotated evaluation datasets, such as AlpacaEval,\nexisting techniques are costly, time-consuming, and lack adaptability. In this\npaper, inspired by the popular language game ``Who is Spy'', we propose to use\nthe word guessing game to assess the intelligence performance of LLMs. Given a\nword, the LLM is asked to describe the word and determine its identity (spy or\nnot) based on its and other players' descriptions. Ideally, an advanced agent\nshould possess the ability to accurately describe a given word using an\naggressive description while concurrently maximizing confusion in the\nconservative description, enhancing its participation in the game. To this end,\nwe first develop DEEP to evaluate LLMs' expression and disguising abilities.\nDEEP requires LLM to describe a word in aggressive and conservative modes. We\nthen introduce SpyGame, an interactive multi-agent framework designed to assess\nLLMs' intelligence through participation in a competitive language-based board\ngame. Incorporating multi-agent interaction, SpyGame requires the target LLM to\npossess linguistic skills and strategic thinking, providing a more\ncomprehensive evaluation of LLMs' human-like cognitive abilities and\nadaptability in complex communication situations. The proposed evaluation\nframework is very easy to implement. We collected words from multiple sources,\ndomains, and languages and used the proposed evaluation framework to conduct\nexperiments. Extensive experiments demonstrate that the proposed DEEP and\nSpyGame effectively evaluate the capabilities of various LLMs, capturing their\nability to adapt to novel situations and engage in strategic communication.", "published": "2023-10-31 14:37:42", "link": "http://arxiv.org/abs/2310.20499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Increasing The Performance of Cognitively Inspired Data-Efficient\n  Language Models via Implicit Structure Building", "abstract": "In this paper, we describe our submission to the BabyLM Challenge 2023 shared\ntask on data-efficient language model (LM) pretraining (Warstadt et al., 2023).\nWe train transformer-based masked language models that incorporate unsupervised\npredictions about hierarchical sentence structure into the model architecture.\nConcretely, we use the Structformer architecture (Shen et al., 2021) and\nvariants thereof. StructFormer models have been shown to perform well on\nunsupervised syntactic induction based on limited pretraining data, and to\nyield performance improvements over a vanilla transformer architecture (Shen et\nal., 2021). Evaluation of our models on 39 tasks provided by the BabyLM\nchallenge shows promising improvements of models that integrate a hierarchical\nbias into the architecture at some particular tasks, even though they fail to\nconsistently outperform the RoBERTa baseline model provided by the shared task\norganizers on all tasks.", "published": "2023-10-31 16:26:36", "link": "http://arxiv.org/abs/2310.20589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defining a New NLP Playground", "abstract": "The recent explosion of performance of large language models (LLMs) has\nchanged the field of Natural Language Processing (NLP) more abruptly and\nseismically than any other shift in the field's 80-year history. This has\nresulted in concerns that the field will become homogenized and\nresource-intensive. The new status quo has put many academic researchers,\nespecially PhD students, at a disadvantage. This paper aims to define a new NLP\nplayground by proposing 20+ PhD-dissertation-worthy research directions,\ncovering theoretical analysis, new and challenging problems, learning\nparadigms, and interdisciplinary applications.", "published": "2023-10-31 17:02:33", "link": "http://arxiv.org/abs/2310.20633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Compositionality in Sentiment: New Data and Analyses", "abstract": "When natural language phrases are combined, their meaning is often more than\nthe sum of their parts. In the context of NLP tasks such as sentiment analysis,\nwhere the meaning of a phrase is its sentiment, that still applies. Many NLP\nstudies on sentiment analysis, however, focus on the fact that sentiment\ncomputations are largely compositional. We, instead, set out to obtain\nnon-compositionality ratings for phrases with respect to their sentiment. Our\ncontributions are as follows: a) a methodology for obtaining those\nnon-compositionality ratings, b) a resource of ratings for 259 phrases --\nNonCompSST -- along with an analysis of that resource, and c) an evaluation of\ncomputational models for sentiment analysis using this new resource.", "published": "2023-10-31 17:25:07", "link": "http://arxiv.org/abs/2310.20656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy\n  Text", "abstract": "Real-world NLP applications often deal with nonstandard text (e.g.,\ndialectal, informal, or misspelled text). However, language models like BERT\ndeteriorate in the face of dialect variation or noise. How do we push BERT's\nmodeling capabilities to encompass nonstandard text? Fine-tuning helps, but it\nis designed for specializing a model to a task and does not seem to bring about\nthe deeper, more pervasive changes needed to adapt a model to nonstandard\nlanguage. In this paper, we introduce the novel idea of sandwiching BERT's\nencoder stack between additional encoder layers trained to perform masked\nlanguage modeling on noisy text. We find that our approach, paired with recent\nwork on including character-level noise in fine-tuning data, can promote\nzero-shot transfer to dialectal text, as well as reduce the distance in the\nembedding space between words and their noisy counterparts.", "published": "2023-10-31 19:44:50", "link": "http://arxiv.org/abs/2311.00116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B", "abstract": "Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.", "published": "2023-10-31 19:45:15", "link": "http://arxiv.org/abs/2311.00117v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the effect of curriculum learning with developmental data for grammar\n  acquisition", "abstract": "This work explores the degree to which grammar acquisition is driven by\nlanguage `simplicity' and the source modality (speech vs. text) of data. Using\nBabyBERTa as a probe, we find that grammar acquisition is largely driven by\nexposure to speech data, and in particular through exposure to two of the\nBabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this\nfinding by examining various ways of presenting input data to our model. First,\nwe assess the impact of various sequence-level complexity based curricula. We\nthen examine the impact of learning over `blocks' -- covering spans of text\nthat are balanced for the number of tokens in each of the source corpora\n(rather than number of lines). Finally, we explore curricula that vary the\ndegree to which the model is exposed to different corpora. In all cases, we\nfind that over-exposure to AO-Childes and Open Subtitles significantly drives\nperformance. We verify these findings through a comparable control dataset in\nwhich exposure to these corpora, and speech more generally, is limited by\ndesign. Our findings indicate that it is not the proportion of tokens occupied\nby high-utility data that aids acquisition, but rather the proportion of\ntraining steps assigned to such data. We hope this encourages future research\ninto the use of more developmentally plausible linguistic data (which tends to\nbe more scarce) to augment general purpose pre-training regimes.", "published": "2023-10-31 20:05:30", "link": "http://arxiv.org/abs/2311.00128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Longer Fixations, More Computation: Gaze-Guided Recurrent Neural\n  Networks", "abstract": "Humans read texts at a varying pace, while machine learning models treat each\ntoken in the same way in terms of a computational process. Therefore, we ask,\ndoes it help to make models act more like humans? In this paper, we convert\nthis intuition into a set of novel models with fixation-guided parallel RNNs or\nlayers and conduct various experiments on language modeling and sentiment\nanalysis tasks to test their effectiveness, thus providing empirical validation\nfor this intuition. Our proposed models achieve good performance on the\nlanguage modeling task, considerably surpassing the baseline model. In\naddition, we find that, interestingly, the fixation duration predicted by\nneural networks bears some resemblance to humans' fixation. Without any\nexplicit guidance, the model makes similar choices to humans. We also\ninvestigate the reasons for the differences between them, which explain why\n\"model fixations\" are often more suitable than human fixations, when used to\nguide language models.", "published": "2023-10-31 21:32:11", "link": "http://arxiv.org/abs/2311.00159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChipNeMo: Domain-Adapted LLMs for Chip Design", "abstract": "ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: domain-adaptive tokenization, domain-adaptive continued\npretraining, model alignment with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our evaluations demonstrate\nthat domain-adaptive pretraining of language models, can lead to superior\nperformance in domain related downstream tasks compared to their base LLaMA2\ncounterparts, without degradations in generic capabilities. In particular, our\nlargest model, ChipNeMo-70B, outperforms the highly capable GPT-4 on two of our\nuse cases, namely engineering assistant chatbot and EDA scripts generation,\nwhile exhibiting competitive performance on bug summarization and analysis.\nThese results underscore the potential of domain-specific customization for\nenhancing the effectiveness of large language models in specialized\napplications.", "published": "2023-10-31 22:35:58", "link": "http://arxiv.org/abs/2311.00176v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Neural Language Models as Cognitive Models of Language\n  Acquisition", "abstract": "The success of neural language models (LMs) on many technological tasks has\nbrought about their potential relevance as scientific theories of language\ndespite some clear differences between LM training and child language\nacquisition. In this paper we argue that some of the most prominent benchmarks\nfor evaluating the syntactic capacities of LMs may not be sufficiently\nrigorous. In particular, we show that the template-based benchmarks lack the\nstructural diversity commonly found in the theoretical and psychological\nstudies of language. When trained on small-scale data modeling child language\nacquisition, the LMs can be readily matched by simple baseline models. We\nadvocate for the use of the readily available, carefully curated datasets that\nhave been evaluated for gradient acceptability by large pools of native\nspeakers and are designed to probe the structural basis of grammar\nspecifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences\nin a way inconsistent with human language users. We conclude with suggestions\nfor better connecting LMs with the empirical study of child language\nacquisition.", "published": "2023-10-31 00:16:17", "link": "http://arxiv.org/abs/2310.20093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language\n  Models", "abstract": "Large language models pretrained on a huge amount of data capture rich\nknowledge and information in the training data. The ability of data\nmemorization and regurgitation in pretrained language models, revealed in\nprevious studies, brings the risk of data leakage. In order to effectively\nreduce these risks, we propose a framework DEPN to Detect and Edit Privacy\nNeurons in pretrained language models, partially inspired by knowledge neurons\nand model editing. In DEPN, we introduce a novel method, termed as privacy\nneuron detector, to locate neurons associated with private information, and\nthen edit these detected privacy neurons by setting their activations to zero.\nFurthermore, we propose a privacy neuron aggregator dememorize private\ninformation in a batch processing manner. Experimental results show that our\nmethod can significantly and efficiently reduce the exposure of private data\nleakage without deteriorating the performance of the model. Additionally, we\nempirically demonstrate the relationship between model memorization and privacy\nneurons, from multiple perspectives, including model size, training time,\nprompts, privacy neuron distribution, illustrating the robustness of our\napproach.", "published": "2023-10-31 03:09:36", "link": "http://arxiv.org/abs/2310.20138v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs", "abstract": "Large language models (LLMs) have achieved significant progress from\npre-training on and memorizing a wide range of textual data, however, this\nprocess might suffer from privacy issues and violations of data protection\nregulations. As a result, the ability to easily remove data related to\nindividual users from such models while not deteriorating their predictive\nquality after the removal becomes increasingly important. To address these\nissues, in this work, we propose an efficient unlearning framework that could\nefficiently update LLMs without having to retrain the whole model after data\nremovals, by introducing lightweight unlearning layers learned with a selective\nteacher-student objective into the transformers. In addition, we introduce a\nfusion mechanism to effectively combine different unlearning layers that learns\nto forget different sets of data to handle a sequence of forgetting operations.\nExperiments on classification and generation tasks demonstrate the\neffectiveness of our proposed methods compared to the state-of-the-art\nbaselines.", "published": "2023-10-31 03:35:59", "link": "http://arxiv.org/abs/2310.20150v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of\n  Language Model with Sparse Human Supervision", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, their suitability for domain-specific tasks, is limited\ndue to their immense scale at deployment, susceptibility to misinformation, and\nmore importantly, high data annotation costs. We propose a novel Interactive\nMulti-Fidelity Learning (IMFL) framework for the cost-effective development of\nsmall domain-specific LMs under limited annotation budgets. Our approach\nformulates the domain-specific fine-tuning process as a multi-fidelity learning\nproblem, focusing on identifying the optimal acquisition strategy that balances\nbetween low-fidelity automatic LLM annotations and high-fidelity human\nannotations to maximize model performance. We further propose an\nexploration-exploitation query strategy that enhances annotation diversity and\ninformativeness, incorporating two innovative designs: 1) prompt retrieval that\nselects in-context examples from human-annotated samples to improve LLM\nannotation, and 2) variable batch size that controls the order for choosing\neach fidelity to facilitate knowledge distillation, ultimately enhancing\nannotation quality. Extensive experiments on financial and medical tasks\ndemonstrate that IMFL achieves superior performance compared with single\nfidelity annotations. Given a limited budget of human annotation, IMFL\nsignificantly outperforms the human annotation baselines in all four tasks and\nachieves very close performance as human annotations on two of the tasks. These\npromising results suggest that the high human annotation costs in\ndomain-specific tasks can be significantly reduced by employing IMFL, which\nutilizes fewer human annotations, supplemented with cheaper and faster LLM\n(e.g., GPT-3.5) annotations to achieve comparable performance.", "published": "2023-10-31 03:39:23", "link": "http://arxiv.org/abs/2310.20153v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Continuations in Multilingual Idiomatic Contexts", "abstract": "The ability to process idiomatic or literal multiword expressions is a\ncrucial aspect of understanding and generating any language. The task of\ngenerating contextually relevant continuations for narratives containing\nidiomatic (or literal) expressions can allow us to test the ability of\ngenerative language models (LMs) in understanding nuanced language containing\nnon-compositional figurative text. We conduct a series of experiments using\ndatasets in two distinct languages (English and Portuguese) under three\ndifferent training settings (zero-shot, few-shot, and fine-tuned). Our results\nsuggest that the models are only slightly better at generating continuations\nfor literal contexts than idiomatic contexts, with exceedingly small margins.\nFurthermore, the models studied in this work perform equally well across both\nlanguages, indicating the robustness of generative models in performing this\ntask.", "published": "2023-10-31 05:40:33", "link": "http://arxiv.org/abs/2310.20195v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History", "abstract": "Machine learning (ML) has recently shown promising results in medical\npredictions using electronic health records (EHRs). However, since ML models\ntypically have a limited capability in terms of input sizes, selecting specific\nmedical events from EHRs for use as input is necessary. This selection process,\noften relying on expert opinion, can cause bottlenecks in development. We\npropose Retrieval-Enhanced Medical prediction model (REMed) to address such\nchallenges. REMed can essentially evaluate unlimited medical events, select the\nrelevant ones, and make predictions. This allows for an unrestricted input\nsize, eliminating the need for manual event selection. We verified these\nproperties through experiments involving 27 clinical prediction tasks across\nfour independent cohorts, where REMed outperformed the baselines. Notably, we\nfound that the preferences of REMed align closely with those of medical\nexperts. We expect our approach to significantly expedite the development of\nEHR prediction models by minimizing clinicians' need for manual involvement.", "published": "2023-10-31 06:04:18", "link": "http://arxiv.org/abs/2310.20204v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Does GPT-4 pass the Turing test?", "abstract": "We evaluated GPT-4 in a public online Turing test. The best-performing GPT-4\nprompt passed in 49.7% of games, outperforming ELIZA (22%) and GPT-3.5 (20%),\nbut falling short of the baseline set by human participants (66%).\nParticipants' decisions were based mainly on linguistic style (35%) and\nsocioemotional traits (27%), supporting the idea that intelligence, narrowly\nconceived, is not sufficient to pass the Turing test. Participant knowledge\nabout LLMs and number of games played positively correlated with accuracy in\ndetecting AI, suggesting learning and practice as possible strategies to\nmitigate deception. Despite known limitations as a test of intelligence, we\nargue that the Turing test continues to be relevant as an assessment of\nnaturalistic communication and deception. AI models with the ability to\nmasquerade as humans could have widespread societal consequences, and we\nanalyse the effectiveness of different strategies and criteria for judging\nhumanlikeness.", "published": "2023-10-31 06:27:52", "link": "http://arxiv.org/abs/2310.20216v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations", "abstract": "Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at\nhttps://github.com/microsoft/MathOctopus.", "published": "2023-10-31 08:09:20", "link": "http://arxiv.org/abs/2310.20246v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Theory of Mind in Large Language Models: Examining Performance of 11\n  State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests", "abstract": "To what degree should we ascribe cognitive capacities to Large Language\nModels (LLMs), such as the ability to reason about intentions and beliefs known\nas Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11\nbase- and instruction-tuned LLMs on capabilities relevant to ToM beyond the\ndominant false-belief paradigm, including non-literal language usage and\nrecursive intentionality; (ii) using newly rewritten versions of standardized\ntests to gauge LLMs' robustness; (iii) prompting and scoring for open besides\nclosed questions; and (iv) benchmarking LLM performance against that of\nchildren aged 7-10 on the same tasks. We find that instruction-tuned LLMs from\nthe GPT family outperform other models, and often also children. Base-LLMs are\nmostly unable to solve ToM tasks, even with specialized prompting. We suggest\nthat the interlinked evolution and development of language and ToM may help\nexplain what instruction-tuning adds: rewarding cooperative communication that\ntakes into account interlocutor and context. We conclude by arguing for a\nnuanced perspective on ToM in LLMs.", "published": "2023-10-31 09:55:07", "link": "http://arxiv.org/abs/2310.20320v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FA Team at the NTCIR-17 UFO Task", "abstract": "The FA team participated in the Table Data Extraction (TDE) and Text-to-Table\nRelationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of\nNon-Financial Objects in Financial Reports (UFO). This paper reports our\napproach to solving the problems and discusses the official results. We\nsuccessfully utilized various enhancement techniques based on the ELECTRA\nlanguage model to extract valuable data from tables. Our efforts resulted in an\nimpressive TDE accuracy rate of 93.43 %, positioning us in second place on the\nLeaderboard rankings. This outstanding achievement is a testament to our\nproposed approach's effectiveness. In the TTRE task, we proposed the rule-based\nmethod to extract meaningful relationships between the text and tables task and\nconfirmed the performance.", "published": "2023-10-31 09:56:12", "link": "http://arxiv.org/abs/2310.20322v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing", "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal\nwith daily. Despite its relevance and practical usefulness, automatic code\nediting remains an underexplored area in the evolution of deep learning models,\npartly due to data scarcity. In this work, we explore the use of Large Language\nModels (LLMs) to edit code based on user instructions. Evaluated on a novel\nhuman-written execution-based benchmark dubbed EditEval, we found current\nmodels often struggle to fulfill the instructions. In light of this, we\ncontribute InstructCoder, the first instruction-tuning dataset designed to\nadapt LLMs for general-purpose code editing, containing high-diversity\ncode-editing tasks such as comment insertion, code optimization, and code\nrefactoring. It consists of over 114,000 instruction-input-output triplets and\ncovers multiple distinct code editing scenarios. The collection process starts\nwith filtered commit data sourced from GitHub Python repositories as seeds.\nSubsequently, the dataset is systematically expanded through an iterative\nprocess, where both seed and generated tasks are used to prompt ChatGPT for\nmore data. Our findings reveal that open-source LLMs fine-tuned on\nInstructCoder can significantly enhance the accuracy of code edits, exhibiting\nsuperior code-editing performance matching advanced proprietary LLMs. The\ndatasets and the source code are publicly available at\nhttps://github.com/qishenghu/CodeInstruct.", "published": "2023-10-31 10:15:35", "link": "http://arxiv.org/abs/2310.20329v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Do large language models solve verbal analogies like children do?", "abstract": "Analogy-making lies at the heart of human cognition. Adults solve analogies\nsuch as \\textit{Horse belongs to stable like chicken belongs to ...?} by\nmapping relations (\\textit{kept in}) and answering \\textit{chicken coop}. In\ncontrast, children often use association, e.g., answering \\textit{egg}. This\npaper investigates whether large language models (LLMs) solve verbal analogies\nin A:B::C:? form using associations, similar to what children do. We use verbal\nanalogies extracted from an online adaptive learning environment, where 14,002\n7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six\ntested Dutch monolingual and multilingual LLMs performed around the same level\nas children, with MGPT performing worst, around the 7-year-old level, and XLM-V\nand GPT-3 the best, slightly above the 11-year-old level. However, when we\ncontrol for associative processes this picture changes and each model's\nperformance level drops 1-2 years. Further experiments demonstrate that\nassociative processes often underlie correctly solved analogies. We conclude\nthat the LLMs we tested indeed tend to solve verbal analogies by association\nwith C like children do.", "published": "2023-10-31 11:49:11", "link": "http://arxiv.org/abs/2310.20384v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL\n  Anthology", "abstract": "The ACL Anthology is an online repository that serves as a comprehensive\ncollection of publications in the field of natural language processing (NLP)\nand computational linguistics (CL). This paper presents a tool called ``ACL\nAnthology Helper''. It automates the process of parsing and downloading papers\nalong with their meta-information, which are then stored in a local MySQL\ndatabase. This allows for efficient management of the local papers using a wide\nrange of operations, including \"where,\" \"group,\" \"order,\" and more. By\nproviding over 20 operations, this tool significantly enhances the retrieval of\nliterature based on specific conditions. Notably, this tool has been\nsuccessfully utilised in writing a survey paper (Tang et al.,2022a). By\nintroducing the ACL Anthology Helper, we aim to enhance researchers' ability to\neffectively access and organise literature from the ACL Anthology. This tool\noffers a convenient solution for researchers seeking to explore the ACL\nAnthology's vast collection of publications while allowing for more targeted\nand efficient literature retrieval.", "published": "2023-10-31 13:59:05", "link": "http://arxiv.org/abs/2310.20467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Breaking the Token Barrier: Chunking and Convolution for Efficient Long\n  Text Classification with BERT", "abstract": "Transformer-based models, specifically BERT, have propelled research in\nvarious NLP tasks. However, these models are limited to a maximum token limit\nof 512 tokens. Consequently, this makes it non-trivial to apply it in a\npractical setting with long input. Various complex methods have claimed to\novercome this limit, but recent research questions the efficacy of these models\nacross different classification tasks. These complex architectures evaluated on\ncarefully curated long datasets perform at par or worse than simple baselines.\nIn this work, we propose a relatively simple extension to vanilla BERT\narchitecture called ChunkBERT that allows finetuning of any pretrained models\nto perform inference on arbitrarily long text. The proposed method is based on\nchunking token representations and CNN layers, making it compatible with any\npre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for\ncomparing long-text classification models across a variety of tasks (including\nbinary classification, multi-class classification, and multi-label\nclassification). A BERT model finetuned using the ChunkBERT method performs\nconsistently across long samples in the benchmark while utilizing only a\nfraction (6.25\\%) of the original memory footprint. These findings suggest that\nefficient finetuning and inference can be achieved through simple modifications\nto pre-trained BERT models.", "published": "2023-10-31 15:41:08", "link": "http://arxiv.org/abs/2310.20558v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding", "abstract": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.", "published": "2023-10-31 16:26:33", "link": "http://arxiv.org/abs/2310.20588v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Effectiveness of Random Target Embeddings for\n  Continuous-Output Neural Machine Translation", "abstract": "Continuous-output neural machine translation (CoNMT) replaces the discrete\nnext-word prediction problem with an embedding prediction. The semantic\nstructure of the target embedding space (i.e., closeness of related words) is\nintuitively believed to be crucial. We challenge this assumption and show that\ncompletely random output embeddings can outperform laboriously pretrained ones,\nespecially on larger datasets. Further investigation shows this surprising\neffect is strongest for rare words, due to the geometry of their embeddings. We\nshed further light on this finding by designing a mixed strategy that combines\nrandom and pre-trained embeddings for different tokens.", "published": "2023-10-31 16:53:10", "link": "http://arxiv.org/abs/2310.20620v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning From Mistakes Makes LLM Better Reasoner", "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve their reasoning\ncapabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA),\nakin to the human learning process. Consider a human student who failed to\nsolve a math problem, he will learn from what mistake he has made and how to\ncorrect it. Mimicking this error-driven learning process, LEMA incorporates\nmistake-correction data pairs during fine-tuning LLMs. Specifically, we first\ncollect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as\na ''corrector'' to identify the mistake step, explain the reason for the\nmistake, correct the mistake and generate the final answer. In addition, we\napply a correction-centric evolution strategy that effectively expands the\nquestion set for generating correction data. Experiments across various LLMs\nand reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning.\nOur further ablations shed light on the non-homogeneous effectiveness between\nCoT data and correction data. These results suggest a significant potential for\nLLMs to improve through learning from their mistakes. Our code, models and\nprompts are publicly available at https://github.com/microsoft/LEMA.", "published": "2023-10-31 17:52:22", "link": "http://arxiv.org/abs/2310.20689v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text-Transport: Toward Learning Causal Effects of Natural Language", "abstract": "As language technologies gain prominence in real-world settings, it is\nimportant to understand how changes to language affect reader perceptions. This\ncan be formalized as the causal effect of varying a linguistic attribute (e.g.,\nsentiment) on a reader's response to the text. In this paper, we introduce\nText-Transport, a method for estimation of causal effects from natural language\nunder any text distribution. Current approaches for valid causal effect\nestimation require strong assumptions about the data, meaning the data from\nwhich one can estimate valid causal effects often is not representative of the\nactual target domain of interest. To address this issue, we leverage the notion\nof distribution shift to describe an estimator that transports causal effects\nbetween domains, bypassing the need for strong assumptions in the target\ndomain. We derive statistical guarantees on the uncertainty of this estimator,\nand we report empirical results and analyses that support the validity of\nText-Transport across data settings. Finally, we use Text-Transport to study a\nrealistic setting--hate speech on social media--in which causal effects do\nshift significantly between text domains, demonstrating the necessity of\ntransport when conducting causal inference on natural language.", "published": "2023-10-31 17:56:51", "link": "http://arxiv.org/abs/2310.20697v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "What's In My Big Data?", "abstract": "Large text corpora are the backbone of language models. However, we have a\nlimited understanding of the content of these corpora, including general\nstatistics, quality, social factors, and inclusion of evaluation data\n(contamination). In this work, we propose What's In My Big Data? (WIMBD), a\nplatform and a set of sixteen analyses that allow us to reveal and compare the\ncontents of large text corpora. WIMBD builds on two basic capabilities -- count\nand search -- at scale, which allows us to analyze more than 35 terabytes on a\nstandard compute node. We apply WIMBD to ten different corpora used to train\npopular language models, including C4, The Pile, and RedPajama. Our analysis\nuncovers several surprising and previously undocumented findings about these\ncorpora, including the high prevalence of duplicate, synthetic, and low-quality\ncontent, personally identifiable information, toxic language, and benchmark\ncontamination. For instance, we find that about 50% of the documents in\nRedPajama and LAION-2B-en are duplicates. In addition, several datasets used\nfor benchmarking models trained on such corpora are contaminated with respect\nto important benchmarks, including the Winograd Schema Challenge and parts of\nGLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a\nstandard set of evaluations for new text-based corpora and to encourage more\nanalyses and transparency around them.", "published": "2023-10-31 17:59:38", "link": "http://arxiv.org/abs/2310.20707v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Denouncing Hate: Strategies for Countering Implied Biases and\n  Stereotypes in Language", "abstract": "Counterspeech, i.e., responses to counteract potential harms of hateful\nspeech, has become an increasingly popular solution to address online hate\nspeech without censorship. However, properly countering hateful language\nrequires countering and dispelling the underlying inaccurate stereotypes\nimplied by such language. In this work, we draw from psychology and philosophy\nliterature to craft six psychologically inspired strategies to challenge the\nunderlying stereotypical implications of hateful language. We first examine the\nconvincingness of each of these strategies through a user study, and then\ncompare their usages in both human- and machine-generated counterspeech\ndatasets. Our results show that human-written counterspeech uses countering\nstrategies that are more specific to the implied stereotype (e.g., counter\nexamples to the stereotype, external factors about the stereotype's origins),\nwhereas machine-generated counterspeech uses less specific strategies (e.g.,\ngenerally denouncing the hatefulness of speech). Furthermore, machine-generated\ncounterspeech often employs strategies that humans deem less convincing\ncompared to human-produced counterspeech. Our findings point to the importance\nof accounting for the underlying stereotypical implications of speech when\ngenerating counterspeech and for better machine reasoning about\nanti-stereotypical examples.", "published": "2023-10-31 21:33:46", "link": "http://arxiv.org/abs/2311.00161v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt\n  Shield", "abstract": "Large Language Models' safety remains a critical concern due to their\nvulnerability to adversarial attacks, which can prompt these systems to produce\nharmful responses. In the heart of these systems lies a safety classifier, a\ncomputational model trained to discern and mitigate potentially harmful,\noffensive, or unethical outputs. However, contemporary safety classifiers,\ndespite their potential, often fail when exposed to inputs infused with\nadversarial noise. In response, our study introduces the Adversarial Prompt\nShield (APS), a lightweight model that excels in detection accuracy and\ndemonstrates resilience against adversarial prompts. Additionally, we propose\nnovel strategies for autonomously generating adversarial training datasets,\nnamed Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are\ndesigned to fortify the safety classifier's robustness, and we investigate the\nconsequences of incorporating adversarial examples into the training process.\nThrough evaluations involving Large Language Models, we demonstrate that our\nclassifier has the potential to decrease the attack success rate resulting from\nadversarial attacks by up to 60%. This advancement paves the way for the next\ngeneration of more reliable and resilient conversational agents.", "published": "2023-10-31 22:22:10", "link": "http://arxiv.org/abs/2311.00172v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak\n  Supervision", "abstract": "Text classification aims to effectively categorize documents into pre-defined\ncategories. Traditional methods for text classification often rely on large\namounts of manually annotated training data, making the process time-consuming\nand labor-intensive. To address this issue, recent studies have focused on\nweakly-supervised and extremely weakly-supervised settings, which require\nminimal or no human annotation, respectively. In previous methods of weakly\nsupervised text classification, pseudo-training data is generated by assigning\npseudo-labels to documents based on their alignment (e.g., keyword matching)\nwith specific classes. However, these methods ignore the importance of\nincorporating the explanations of the generated pseudo-labels, or saliency of\nindividual words, as additional guidance during the text classification\ntraining process. To address this limitation, we propose XAI-CLASS, a novel\nexplanation-enhanced extremely weakly-supervised text classification method\nthat incorporates word saliency prediction as an auxiliary task. XAI-CLASS\nbegins by employing a multi-round question-answering process to generate\npseudo-training data that promotes the mutual enhancement of class labels and\ncorresponding explanation word generation. This pseudo-training data is then\nused to train a multi-task framework that simultaneously learns both text\nclassification and word saliency prediction. Extensive experiments on several\nweakly-supervised text classification datasets show that XAI-CLASS outperforms\nother weakly-supervised text classification methods significantly. Moreover,\nexperiments demonstrate that XAI-CLASS enhances both model performance and\nexplainability.", "published": "2023-10-31 23:24:22", "link": "http://arxiv.org/abs/2311.00189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relation Extraction from News Articles (RENA): A Tool for Epidemic\n  Surveillance", "abstract": "Relation Extraction from News Articles (RENA) is a browser-based tool\ndesigned to extract key entities and their semantic relationships in English\nlanguage news articles related to infectious diseases. Constructed using the\nReact framework, this system presents users with an elegant and user-friendly\ninterface. It enables users to input a news article and select from a choice of\ntwo models to generate a comprehensive list of relations within the provided\ntext. As a result, RENA allows real-time parsing of news articles to extract\nkey information for epidemic surveillance, contributing to EPIWATCH, an\nopen-source intelligence-based epidemic warning system.", "published": "2023-10-31 23:14:54", "link": "http://arxiv.org/abs/2311.01472v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Classification of Student Help Requests in Programming Courses\n  Using Large Language Models", "abstract": "The accurate classification of student help requests with respect to the type\nof help being sought can enable the tailoring of effective responses.\nAutomatically classifying such requests is non-trivial, but large language\nmodels (LLMs) appear to offer an accessible, cost-effective solution. This\nstudy evaluates the performance of the GPT-3.5 and GPT-4 models for classifying\nhelp requests from students in an introductory programming class. In zero-shot\ntrials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories,\nwhile GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests\nrelated to debugging. Fine-tuning the GPT-3.5 model improved its performance to\nsuch an extent that it approximated the accuracy and consistency across\ncategories observed between two human raters. Overall, this study demonstrates\nthe feasibility of using LLMs to enhance educational systems through the\nautomated classification of student needs.", "published": "2023-10-31 00:56:33", "link": "http://arxiv.org/abs/2310.20105v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "EELBERT: Tiny Models through Dynamic Embeddings", "abstract": "We introduce EELBERT, an approach for compression of transformer-based models\n(e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is\nachieved by replacing the input embedding layer of the model with dynamic, i.e.\non-the-fly, embedding computations. Since the input embedding layer accounts\nfor a significant fraction of the model size, especially for the smaller BERT\nvariants, replacing this layer with an embedding computation function helps us\nreduce the model size significantly. Empirical evaluation on the GLUE benchmark\nshows that our BERT variants (EELBERT) suffer minimal regression compared to\nthe traditional BERT models. Through this approach, we are able to develop our\nsmallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully\ntrained BERT-tiny, while being 15x smaller (1.2 MB) in size.", "published": "2023-10-31 03:28:08", "link": "http://arxiv.org/abs/2310.20144v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Consensus Seeking via Large Language Models", "abstract": "Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/.", "published": "2023-10-31 03:37:11", "link": "http://arxiv.org/abs/2310.20151v2", "categories": ["cs.CL", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Extracting Entities of Interest from Comparative Product Reviews", "abstract": "This paper presents a deep learning based approach to extract product\ncomparison information out of user reviews on various e-commerce websites. Any\ncomparative product review has three major entities of information: the names\nof the products being compared, the user opinion (predicate) and the feature or\naspect under comparison. All these informing entities are dependent on each\nother and bound by the rules of the language, in the review. We observe that\ntheir inter-dependencies can be captured well using LSTMs. We evaluate our\nsystem on existing manually labeled datasets and observe out-performance over\nthe existing Semantic Role Labeling (SRL) framework popular for this task.", "published": "2023-10-31 08:43:11", "link": "http://arxiv.org/abs/2310.20274v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.IR"}
{"title": "Neural Retrievers are Biased Towards LLM-Generated Content", "abstract": "Recently, the emergence of large language models (LLMs) has revolutionized\nthe paradigm of information retrieval (IR) applications, especially in web\nsearch, by generating vast amounts of human-like texts on the Internet. As a\nresult, IR systems in the LLM era are facing a new challenge: the indexed\ndocuments are now not only written by human beings but also automatically\ngenerated by the LLMs. How these LLM-generated documents influence the IR\nsystems is a pressing and still unexplored question. In this work, we conduct a\nquantitative evaluation of IR models in scenarios where both human-written and\nLLM-generated texts are involved. Surprisingly, our findings indicate that\nneural retrieval models tend to rank LLM-generated documents higher. We refer\nto this category of biases in neural retrievers towards the LLM-generated\ncontent as the \\textbf{source bias}. Moreover, we discover that this bias is\nnot confined to the first-stage neural retrievers, but extends to the\nsecond-stage neural re-rankers. Then, in-depth analyses from the perspective of\ntext compression indicate that LLM-generated texts exhibit more focused\nsemantics with less noise, making it easier for neural retrieval models to\nsemantic match. To mitigate the source bias, we also propose a plug-and-play\ndebiased constraint for the optimization objective, and experimental results\nshow its effectiveness. Finally, we discuss the potential severe concerns\nstemming from the observed source bias and hope our findings can serve as a\ncritical wake-up call to the IR community and beyond. To facilitate future\nexplorations of IR in the LLM era, the constructed two new benchmarks are\navailable at https://github.com/KID-22/Source-Bias.", "published": "2023-10-31 14:42:23", "link": "http://arxiv.org/abs/2310.20501v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CapsFusion: Rethinking Image-Text Data at Scale", "abstract": "Large multimodal models demonstrate remarkable generalist ability to perform\ndiverse multimodal tasks in a zero-shot manner. Large-scale web-based\nimage-text pairs contribute fundamentally to this success, but suffer from\nexcessive noise. Recent studies use alternative captions synthesized by\ncaptioning models and have achieved notable benchmark performance. However, our\nexperiments reveal significant Scalability Deficiency and World Knowledge Loss\nissues in models trained with synthetic captions, which have been largely\nobscured by their initial benchmark success. Upon closer examination, we\nidentify the root cause as the overly-simplified language structure and lack of\nknowledge details in existing synthetic captions. To provide higher-quality and\nmore scalable multimodal pretraining data, we propose CapsFusion, an advanced\nframework that leverages large language models to consolidate and refine\ninformation from both web-based image-text pairs and synthetic captions.\nExtensive experiments show that CapsFusion captions exhibit remarkable\nall-round superiority over existing captions in terms of model performance\n(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample\nefficiency (requiring 11-16 times less computation than baselines), world\nknowledge depth, and scalability. These effectiveness, efficiency and\nscalability advantages position CapsFusion as a promising candidate for future\nscaling of LMM training.", "published": "2023-10-31 15:31:39", "link": "http://arxiv.org/abs/2310.20550v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Vanishing Gradients in Reinforcement Finetuning of Language Models", "abstract": "Pretrained language models are commonly aligned with human preferences and\ndownstream tasks via reinforcement finetuning (RFT), which refers to maximizing\na (possibly learned) reward function using policy gradient algorithms. This\nwork identifies a fundamental optimization obstacle in RFT: we prove that the\nexpected gradient for an input vanishes when its reward standard deviation\nunder the model is small, even if the expected reward is far from optimal.\nThrough experiments on an RFT benchmark and controlled environments, as well as\na theoretical analysis, we then demonstrate that vanishing gradients due to\nsmall reward standard deviation are prevalent and detrimental, leading to\nextremely slow reward maximization. Lastly, we explore ways to overcome\nvanishing gradients in RFT. We find the common practice of an initial\nsupervised finetuning (SFT) phase to be the most promising candidate, which\nsheds light on its importance in an RFT pipeline. Moreover, we show that a\nrelatively small number of SFT optimization steps on as few as 1% of the input\nsamples can suffice, indicating that the initial SFT phase need not be\nexpensive in terms of compute and data labeling efforts. Overall, our results\nemphasize that being mindful for inputs whose expected gradient vanishes, as\nmeasured by the reward standard deviation, is crucial for successful execution\nof RFT.", "published": "2023-10-31 17:59:05", "link": "http://arxiv.org/abs/2310.20703v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Grounding Visual Illusions in Language: Do Vision-Language Models\n  Perceive Illusions Like Humans?", "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by\nhumans emulating our understanding of the world. However, known as visual\nillusions, human's perception of reality isn't always faithful to the physical\nworld. This raises a key question: do VLMs have the similar kind of illusions\nas humans do, or do they faithfully learn to represent reality? To investigate\nthis question, we build a dataset containing five types of visual illusions and\nformulate four tasks to examine visual illusions in state-of-the-art VLMs. Our\nfindings have shown that although the overall alignment is low, larger models\nare closer to human perception and more susceptible to visual illusions. Our\ndataset and initial findings will promote a better understanding of visual\nillusions in humans and machines and provide a stepping stone for future\ncomputational models that can better align humans and machines in perceiving\nand communicating about the shared visual world. The code and data are\navailable at https://github.com/vl-illusion/dataset.", "published": "2023-10-31 18:01:11", "link": "http://arxiv.org/abs/2311.00047v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"", "abstract": "The recent wave of generative AI has sparked unprecedented global attention,\nwith both excitement and concern over potentially superhuman levels of\nartificial intelligence: models now take only seconds to produce outputs that\nwould challenge or exceed the capabilities even of expert humans. At the same\ntime, models still show basic errors in understanding that would not be\nexpected even in non-expert humans. This presents us with an apparent paradox:\nhow do we reconcile seemingly superhuman capabilities with the persistence of\nerrors that few humans would make? In this work, we posit that this tension\nreflects a divergence in the configuration of intelligence in today's\ngenerative models relative to intelligence in humans. Specifically, we propose\nand test the Generative AI Paradox hypothesis: generative models, having been\ntrained directly to reproduce expert-like outputs, acquire generative\ncapabilities that are not contingent upon -- and can therefore exceed -- their\nability to understand those same types of outputs. This contrasts with humans,\nfor whom basic understanding almost always precedes the ability to generate\nexpert-level outputs. We test this hypothesis through controlled experiments\nanalyzing generation vs. understanding in generative models, across both\nlanguage and image modalities. Our results show that although models can\noutperform humans in generation, they consistently fall short of human\ncapabilities in measures of understanding, as well as weaker correlation\nbetween generation and understanding performance, and more brittleness to\nadversarial inputs. Our findings support the hypothesis that models' generative\ncapability may not be contingent upon understanding capability, and call for\ncaution in interpreting artificial intelligence by analogy to human\nintelligence.", "published": "2023-10-31 18:07:07", "link": "http://arxiv.org/abs/2311.00059v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Two-Stage Classifier for Campaign Negativity Detection using Axis\n  Embeddings: A Case Study on Tweets of Political Users during 2021\n  Presidential Election in Iran", "abstract": "In elections around the world, the candidates may turn their campaigns toward\nnegativity due to the prospect of failure and time pressure. In the digital\nage, social media platforms such as Twitter are rich sources of political\ndiscourse. Therefore, despite the large amount of data that is published on\nTwitter, the automatic system for campaign negativity detection can play an\nessential role in understanding the strategy of candidates and parties in their\ncampaigns. In this paper, we propose a hybrid model for detecting campaign\nnegativity consisting of a two-stage classifier that combines the strengths of\ntwo machine learning models. Here, we have collected Persian tweets from 50\npolitical users, including candidates and government officials. Then we\nannotated 5,100 of them that were published during the year before the 2021\npresidential election in Iran. In the proposed model, first, the required\ndatasets of two classifiers based on the cosine similarity of tweet embeddings\nwith axis embeddings (which are the average of embedding in positive and\nnegative classes of tweets) from the training set (85\\%) are made, and then\nthese datasets are considered the training set of the two classifiers in the\nhybrid model. Finally, our best model (RF-RF) was able to achieve 79\\% for the\nmacro F1 score and 82\\% for the weighted F1 score. By running the best model on\nthe rest of the tweets of 50 political users that were published one year\nbefore the election and with the help of statistical models, we find that the\npublication of a tweet by a candidate has nothing to do with the negativity of\nthat tweet, and the presence of the names of political persons and political\norganizations in the tweet is directly related to its negativity.", "published": "2023-10-31 20:31:41", "link": "http://arxiv.org/abs/2311.00143v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EIT: Earnest Insight Toolkit for Evaluating Students' Earnestness in\n  Interactive Lecture Participation Exercises", "abstract": "In today's rapidly evolving educational landscape, traditional modes of\npassive information delivery are giving way to transformative pedagogical\napproaches that prioritize active student engagement. Within the context of\nlarge-scale hybrid classrooms, the challenge lies in fostering meaningful and\nactive interaction between students and course content. This study delves into\nthe significance of measuring students' earnestness during interactive lecture\nparticipation exercises. By analyzing students' responses to interactive\nlecture poll questions, establishing a clear rubric for evaluating earnestness,\nand conducting a comprehensive assessment, we introduce EIT (Earnest Insight\nToolkit), a tool designed to assess students' engagement within interactive\nlecture participation exercises - particularly in the context of large-scale\nhybrid classrooms. Through the utilization of EIT, our objective is to equip\neducators with valuable means of identifying at-risk students for enhancing\nintervention and support strategies, as well as measuring students' levels of\nengagement with course content.", "published": "2023-10-31 07:05:00", "link": "http://arxiv.org/abs/2311.10746v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Filter bubbles and affective polarization in user-personalized large\n  language model outputs", "abstract": "Echoing the history of search engines and social media content rankings, the\nadvent of large language models (LLMs) has led to a push for increased\npersonalization of model outputs to individual users. In the past, personalized\nrecommendations and ranking systems have been linked to the development of\nfilter bubbles (serving content that may confirm a user's existing biases) and\naffective polarization (strong negative sentiment towards those with differing\nviews). In this work, we explore how prompting a leading large language model,\nChatGPT-3.5, with a user's political affiliation prior to asking factual\nquestions about public figures and organizations leads to differing results. We\nobserve that left-leaning users tend to receive more positive statements about\nleft-leaning political figures and media outlets, while right-leaning users see\nmore positive statements about right-leaning entities. This pattern holds\nacross presidential candidates, members of the U.S. Senate, and media\norganizations with ratings from AllSides. When qualitatively evaluating some of\nthese outputs, there is evidence that particular facts are included or excluded\nbased on the user's political affiliation. These results illustrate that\npersonalizing LLMs based on user demographics carry the same risks of affective\npolarization and filter bubbles that have been seen in other personalized\ninternet technologies. This ``failure mode\" should be monitored closely as\nthere are more attempts to monetize and personalize these models.", "published": "2023-10-31 18:19:28", "link": "http://arxiv.org/abs/2311.14677v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Study of speaker localization with binaural microphone array\n  incorporating auditory filters and lateral angle estimation", "abstract": "Speaker localization for binaural microphone arrays has been widely studied\nfor applications such as speech communication, video conferencing, and robot\naudition. Many methods developed for this task, including the direct path\ndominance (DPD) test, share common stages in their processing, which include\ntransformation using the short-time Fourier transform (STFT), and a direction\nof arrival (DOA) search that is based on the head related transfer function\n(HRTF) set. In this paper, alternatives to these processing stages, motivated\nby human hearing, are proposed. These include incorporating an auditory filter\nbank to replace the STFT, and a new DOA search based on transformed HRTF as\nsteering vectors. A simulation study and an experimental study are conducted to\nvalidate the proposed alternatives, and both are applied to two binaural DOA\nestimation methods; the results show that the proposed method compares\nfavorably with current methods.", "published": "2023-10-31 07:43:12", "link": "http://arxiv.org/abs/2310.20238v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RIR-SF: Room Impulse Response Based Spatial Feature for Target Speech\n  Recognition in Multi-Channel Multi-Speaker Scenarios", "abstract": "Automatic speech recognition (ASR) on multi-talker recordings is challenging.\nCurrent methods using 3D spatial data from multi-channel audio and visual cues\nfocus mainly on direct waves from the target speaker, overlooking reflection\nwave impacts, which hinders performance in reverberant environments. Our\nresearch introduces RIR-SF, a novel spatial feature based on room impulse\nresponse (RIR) that leverages the speaker's position, room acoustics, and\nreflection dynamics. RIR-SF significantly outperforms traditional 3D spatial\nfeatures, showing superior theoretical and empirical performance. We also\npropose an optimized all-neural multi-channel ASR framework for RIR-SF,\nachieving a relative 21.3\\% reduction in CER for target speaker ASR in\nmulti-channel settings. RIR-SF enhances recognition accuracy and demonstrates\nrobustness in high-reverberation scenarios, overcoming the limitations of\nprevious methods.", "published": "2023-10-31 20:42:08", "link": "http://arxiv.org/abs/2311.00146v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "LAVSS: Location-Guided Audio-Visual Spatial Audio Separation", "abstract": "Existing machine learning research has achieved promising results in monaural\naudio-visual separation (MAVS). However, most MAVS methods purely consider what\nthe sound source is, not where it is located. This can be a problem in VR/AR\nscenarios, where listeners need to be able to distinguish between similar audio\nsources located in different directions. To address this limitation, we have\ngeneralized MAVS to spatial audio separation and proposed LAVSS: a\nlocation-guided audio-visual spatial audio separator. LAVSS is inspired by the\ncorrelation between spatial audio and visual location. We introduce the phase\ndifference carried by binaural audio as spatial cues, and we utilize positional\nrepresentations of sounding objects as additional modality guidance. We also\nleverage multi-level cross-modal attention to perform visual-positional\ncollaboration with audio features. In addition, we adopt a pre-trained monaural\nseparator to transfer knowledge from rich mono sounds to boost spatial audio\nseparation. This exploits the correlation between monaural and binaural\nchannels. Experiments on the FAIR-Play dataset demonstrate the superiority of\nthe proposed LAVSS over existing benchmarks of audio-visual separation. Our\nproject page: https://yyx666660.github.io/LAVSS/.", "published": "2023-10-31 13:30:24", "link": "http://arxiv.org/abs/2310.20446v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
