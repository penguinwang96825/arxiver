{"title": "TensiStrength: Stress and relaxation magnitude detection for social\n  media texts", "abstract": "Computer systems need to be able to react to stress in order to perform\noptimally on some tasks. This article describes TensiStrength, a system to\ndetect the strength of stress and relaxation expressed in social media text\nmessages. TensiStrength uses a lexical approach and a set of rules to detect\ndirect and indirect expressions of stress or relaxation, particularly in the\ncontext of transportation. It is slightly more effective than a comparable\nsentiment analysis program, although their similar performances occur despite\ndifferences on almost half of the tweets gathered. The effectiveness of\nTensiStrength depends on the nature of the tweets classified, with tweets that\nare rich in stress-related terms being particularly problematic. Although\ngeneric machine learning methods can give better performance than TensiStrength\noverall, they exploit topic-related terms in a way that may be undesirable in\npractical applications and that may not work as well in more focused contexts.\nIn conclusion, TensiStrength and generic machine learning approaches work well\nenough to be practical choices for intelligent applications that need to take\nadvantage of stress information, and the decision about which to use depends on\nthe nature of the texts analysed and the purpose of the task.", "published": "2016-07-01 07:50:02", "link": "http://arxiv.org/abs/1607.00139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "abstract": "Most state of the art approaches for Named Entity Recognition rely on hand\ncrafted features and annotated corpora. Recently Neural network based models\nhave been proposed which do not require handcrafted features but still require\nannotated corpora. However, such annotated corpora may not be available for\nmany languages. In this paper, we propose a neural network based model which\nallows sharing the decoder as well as word and character level parameters\nbetween two languages thereby allowing a resource fortunate language to aid a\nresource deprived language. Specifically, we focus on the case when limited\nannotated corpora is available in one language ($L_1$) and abundant annotated\ncorpora is available in another language ($L_2$). Sharing the network\narchitecture and parameters between $L_1$ and $L_2$ leads to improved\nperformance in $L_1$. Further, our approach does not require any hand crafted\nfeatures but instead directly learns meaningful feature representations from\nthe training data itself. We experiment with 4 language pairs and show that\nindeed in a resource constrained setup (lesser annotated corpora), a model\njointly trained with data from another language performs better than a model\ntrained only on the limited corpora in one language.", "published": "2016-07-01 10:35:59", "link": "http://arxiv.org/abs/1607.00198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "abstract": "Word embeddings have recently seen a strong increase in interest as a result\nof strong performance gains on a variety of tasks. However, most of this\nresearch also underlined the importance of benchmark datasets, and the\ndifficulty of constructing these for a variety of language-specific tasks.\nStill, many of the datasets used in these tasks could prove to be fruitful\nlinguistic resources, allowing for unique observations into language use and\nvariability. In this paper we demonstrate the performance of multiple types of\nembeddings, created with both count and prediction-based architectures on a\nvariety of corpora, in two language-specific tasks: relation evaluation, and\ndialect identification. For the latter, we compare unsupervised methods with a\ntraditional, hand-crafted dictionary. With this research, we provide the\nembeddings themselves, the relation evaluation task benchmark for use in\nfurther research, and demonstrate how the benchmarked embeddings prove a useful\nunsupervised linguistic resource, effectively used in a downstream task.", "published": "2016-07-01 12:48:35", "link": "http://arxiv.org/abs/1607.00225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "abstract": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice\nin many fields and applications. It is also at the heart of most\nstate-of-the-art speech recognition systems since the 70's. However, from\nGaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM),\nthe underlying Markovian chain of state-of-the-art models did not changed much.\nThe \"left-to-right\" topology is mostly always employed because very few other\nalternatives exist. In this paper, we propose that finely-tuned HMM topologies\nare essential for precise temporal modelling and that this approach should be\ninvestigated in state-of-the-art HMM system. As such, we propose a\nproof-of-concept framework for learning efficient topologies by pruning down\ncomplex generic models. Speech recognition experiments that were conducted\nindicate that complex time dependencies can be better learned by this approach\nthan with classical \"left-to-right\" models.", "published": "2016-07-01 19:20:50", "link": "http://arxiv.org/abs/1607.00359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Throwing fuel on the embers: Probability or Dichotomy, Cognitive or\n  Linguistic?", "abstract": "Prof. Robert Berwick's abstract for his forthcoming invited talk at the\nACL2016 workshop on Cognitive Aspects of Computational Language Learning\nrevives an ancient debate. Entitled \"Why take a chance?\", Berwick seems to\nrefer implicitly to Chomsky's critique of the statistical approach of Harris as\nwell as the currently dominant paradigms in CoNLL.\n  Berwick avoids Chomsky's use of \"innate\" but states that \"the debate over the\nexistence of sophisticated mental grammars was settled with Chomsky's Logical\nStructure of Linguistic Theory (1957/1975)\", acknowledging that \"this debate\nhas often been revived\".\n  This paper agrees with the view that this debate has long since been settled,\nbut with the opposite outcome! Given the embers have not yet died away, and the\nquestions remain fundamental, perhaps it is appropriate to refuel the debate,\nso I would like to join Bob in throwing fuel on this fire by reviewing the\nevidence against the Chomskian position!", "published": "2016-07-01 10:01:11", "link": "http://arxiv.org/abs/1607.00186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SentiBubbles: Topic Modeling and Sentiment Visualization of\n  Entity-centric Tweets", "abstract": "Social Media users tend to mention entities when reacting to news events. The\nmain purpose of this work is to create entity-centric aggregations of tweets on\na daily basis. By applying topic modeling and sentiment analysis, we create\ndata visualization insights about current events and people reactions to those\nevents from an entity-centric perspective.", "published": "2016-07-01 09:15:13", "link": "http://arxiv.org/abs/1607.00167v2", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Permutation Invariant Training of Deep Models for Speaker-Independent\n  Multi-talker Speech Separation", "abstract": "We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common.", "published": "2016-07-01 17:34:16", "link": "http://arxiv.org/abs/1607.00325v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Neural Networks by Parameter Augmentation", "abstract": "We propose a simple domain adaptation method for neural networks in a\nsupervised setting. Supervised domain adaptation is a way of improving the\ngeneralization performance on the target domain by using the source domain\ndataset, assuming that both of the datasets are labeled. Recently, recurrent\nneural networks have been shown to be successful on a variety of NLP tasks such\nas caption generation; however, the existing domain adaptation techniques are\nlimited to (1) tune the model parameters by the target dataset after the\ntraining by the source dataset, or (2) design the network to have dual output,\none for the source domain and the other for the target domain. Reformulating\nthe idea of the domain adaptation technique proposed by Daume (2007), we\npropose a simple domain adaptation method, which can be applied to neural\nnetworks trained with a cross-entropy loss. On captioning datasets, we show\nperformance improvements over other domain adaptation methods.", "published": "2016-07-01 21:24:21", "link": "http://arxiv.org/abs/1607.00410v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Relational Dependency Networks for Relation Extraction", "abstract": "We consider the task of KBP slot filling -- extracting relation information\nfrom newswire documents for knowledge base construction. We present our\npipeline, which employs Relational Dependency Networks (RDNs) to learn\nlinguistic patterns for relation extraction. Additionally, we demonstrate how\nseveral components such as weak supervision, word2vec features, joint learning\nand the use of human advice, can be incorporated in this relational framework.\nWe evaluate the different components in the benchmark KBP 2015 task and show\nthat RDNs effectively model a diverse set of features and perform competitively\nwith current state-of-the-art relation extraction.", "published": "2016-07-01 22:11:38", "link": "http://arxiv.org/abs/1607.00424v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
