{"title": "Predicting Actions to Help Predict Translations", "abstract": "We address the task of text translation on the How2 dataset using a state of\nthe art transformer-based multimodal approach. The question we ask ourselves is\nwhether visual features can support the translation process, in particular,\ngiven that this is a dataset extracted from videos, we focus on the translation\nof actions, which we believe are poorly captured in current static image-text\ndatasets currently used for multimodal translation. For that purpose, we\nextract different types of action features from the videos and carefully\ninvestigate how helpful this visual information is by testing whether it can\nincrease translation quality when used in conjunction with (i) the original\ntext and (ii) the original text where action-related words (or all verbs) are\nmasked out. The latter is a simulation that helps us assess the utility of the\nimage in cases where the text does not provide enough context about the action,\nor in the presence of noise in the input text.", "published": "2019-08-05 14:56:01", "link": "http://arxiv.org/abs/1908.01665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Processamento de linguagem natural em Portugu\u00eas e aprendizagem\n  profunda para o dom\u00ednio de \u00d3leo e G\u00e1s", "abstract": "Over the last few decades, institutions around the world have been challenged\nto deal with the sheer volume of information captured in unstructured formats,\nespecially in textual documents. The so called Digital Transformation age,\ncharacterized by important technological advances and the advent of disruptive\nmethods in Artificial Intelligence, offers opportunities to make better use of\nthis information. Recent techniques in Natural Language Processing (NLP) with\nDeep Learning approaches allow to efficiently process a large volume of data in\norder to obtain relevant information, to identify patterns, classify text,\namong other applications. In this context, the highly technical vocabulary of\nOil and Gas (O&G) domain represents a challenge for these NLP algorithms, in\nwhich terms can assume a very different meaning in relation to common sense\nunderstanding. The search for suitable mathematical representations and\nspecific models requires a large amount of representative corpora in the O&G\ndomain. However, public access to this material is scarce in the scientific\nliterature, especially considering the Portuguese language. This paper presents\na literature review about the main techniques for deep learning NLP and their\nmajor applications for O&G domain in Portuguese.", "published": "2019-08-05 15:05:48", "link": "http://arxiv.org/abs/1908.01674v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Beyond English-Only Reading Comprehension: Experiments in Zero-Shot\n  Multilingual Transfer for Bulgarian", "abstract": "Recently, reading comprehension models achieved near-human performance on\nlarge-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely\ndue to the release of pre-trained contextualized representations such as BERT\nand ELMo, which can be fine-tuned for the target task. Despite those advances\nand the creation of more challenging datasets, most of the work is still done\nfor English. Here, we study the effectiveness of multilingual BERT fine-tuned\non large-scale English datasets for reading comprehension (e.g., for RACE), and\nwe apply it to Bulgarian multiple-choice reading comprehension. We propose a\nnew dataset containing 2,221 questions from matriculation exams for twelfth\ngrade in various subjects -history, biology, geography and philosophy-, and 412\nadditional questions from online quizzes in history. While the quiz authors\ngave no relevant context, we incorporate knowledge from Wikipedia, retrieving\ndocuments matching the combination of question + each answer option. Moreover,\nwe experiment with different indexing and pre-training strategies. The\nevaluation results show accuracy of 42.23%, which is well above the baseline of\n24.89%.", "published": "2019-08-05 08:45:20", "link": "http://arxiv.org/abs/1908.01519v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Thoth: Improved Rapid Serial Visual Presentation using Natural Language\n  Processing", "abstract": "Thoth is a tool designed to combine many different types of speed reading\ntechnology. The largest insight is using natural language parsing for more\noptimal rapid serial visual presentation and more effective reading\ninformation.", "published": "2019-08-05 15:45:39", "link": "http://arxiv.org/abs/1908.01699v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Performance Evaluation of Supervised Machine Learning Techniques for\n  Efficient Detection of Emotions from Online Content", "abstract": "Emotion detection from the text is an important and challenging problem in\ntext analytics. The opinion-mining experts are focusing on the development of\nemotion detection applications as they have received considerable attention of\nonline community including users and business organization for collecting and\ninterpreting public emotions. However, most of the existing works on emotion\ndetection used less efficient machine learning classifiers with limited\ndatasets, resulting in performance degradation. To overcome this issue, this\nwork aims at the evaluation of the performance of different machine learning\nclassifiers on a benchmark emotion dataset. The experimental results show the\nperformance of different machine learning classifiers in terms of different\nevaluation metrics like precision, recall ad f-measure. Finally, a classifier\nwith the best performance is recommended for the emotion classification.", "published": "2019-08-05 16:48:22", "link": "http://arxiv.org/abs/1908.01587v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "68T01"], "primary_category": "cs.IR"}
{"title": "Unsupervised Context Retrieval for Long-tail Entities", "abstract": "Monitoring entities in media streams often relies on rich entity\nrepresentations, like structured information available in a knowledge base\n(KB). For long-tail entities, such monitoring is highly challenging, due to\ntheir limited, if not entirely missing, representation in the reference KB. In\nthis paper, we address the problem of retrieving textual contexts for\nmonitoring long-tail entities. We propose an unsupervised method to overcome\nthe limited representation of long-tail entities by leveraging established\nentities and their contexts as support information. Evaluation on a\npurpose-built test collection shows the suitability of our approach and its\nrobustness for out-of-KB entities.", "published": "2019-08-05 18:28:09", "link": "http://arxiv.org/abs/1908.01798v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Answering Questions about Data Visualizations using Efficient Bimodal\n  Fusion", "abstract": "Chart question answering (CQA) is a newly proposed visual question answering\n(VQA) task where an algorithm must answer questions about data visualizations,\ne.g. bar charts, pie charts, and line graphs. CQA requires capabilities that\nnatural-image VQA algorithms lack: fine-grained measurements, optical character\nrecognition, and handling out-of-vocabulary words in both questions and\nanswers. Without modifications, state-of-the-art VQA algorithms perform poorly\non this task. Here, we propose a novel CQA algorithm called parallel recurrent\nfusion of image and language (PReFIL). PReFIL first learns bimodal embeddings\nby fusing question and image features and then intelligently aggregates these\nlearned embeddings to answer the given question. Despite its simplicity, PReFIL\ngreatly surpasses state-of-the art systems and human baselines on both the\nFigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be\nused to reconstruct tables by asking a series of questions about a chart.", "published": "2019-08-05 18:47:30", "link": "http://arxiv.org/abs/1908.01801v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Weakly-Supervised Attention-based Visualization Tool for Assessing\n  Political Affiliation", "abstract": "In this work, we seek to finetune a weakly-supervised expert-guided Deep\nNeural Network (DNN) for the purpose of determining political affiliations. In\nthis context, stance detection is used for determining political affiliation or\nideology which is framed in the form of relative proximities between entities\nin a low-dimensional space. An attention-based mechanism is used to provide\nmodel interpretability. A Deep Neural Network for Natural Language\nUnderstanding (NLU) using static and contextual embeddings is trained and\nevaluated. Various techniques to visualize the projections generated from the\nnetwork are evaluated for visualization efficiency. An overview of the pipeline\nfrom data ingestion, processing and generation of visualization is given here.\nA web-based framework created to faciliate this interaction and exploration is\npresented here. Preliminary results of this study are summarized and future\nwork is outlined.", "published": "2019-08-05 18:14:06", "link": "http://arxiv.org/abs/1908.02282v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling with Associated Memory Network", "abstract": "Semantic role labeling (SRL) is a task to recognize all the\npredicate-argument pairs of a sentence, which has been in a performance\nimprovement bottleneck after a series of latest works were presented. This\npaper proposes a novel syntax-agnostic SRL model enhanced by the proposed\nassociated memory network (AMN), which makes use of inter-sentence attention of\nlabel-known associated sentences as a kind of memory to further enhance\ndependency-based SRL. In detail, we use sentences and their labels from train\ndataset as an associated memory cue to help label the target sentence.\nFurthermore, we compare several associated sentences selecting strategies and\nlabel merging methods in AMN to find and utilize the label of associated\nsentences while attending them. By leveraging the attentive memory from known\ntraining data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark\ndatasets for syntax-agnostic setting, showing a new effective research line of\nSRL enhancement other than exploiting external resources such as well\npre-trained language models.", "published": "2019-08-05 09:40:18", "link": "http://arxiv.org/abs/1908.02367v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic Sounds for Wellbeing: A Novel Dataset and Baseline Results", "abstract": "The field of sound healing includes ancient practices coming from a broad\nrange of cultures. Across such practices there is a variety of acoustic\ninstrumentation utilised. Practitioners suggest that sound has the ability to\ntarget both mental and even physical health issues, e.g., chronic-stress, or\njoint-pain. Instruments including the Tibetan singing bowl and vocal chanting,\nare still widely used today. With the noise-floor of modern urban soundscapes\ncontinually increasing and known to impact wellbeing, methods to improve this\nare needed. With that in mind, this study presents the Acoustic Sounds for\nWellbeing (ASW) dataset. The ASW dataset is a dataset gathered from YouTube\nincluding 88\\,+ hrs of audio from 5-classes of acoustic instrumentation (Gongs,\nDrumming, Singing Bowls, and Chanting). We additionally present initial\nbaseline classification results on the dataset, finding that conventional\nMel-Frequency Cepstra coefficient features achieve at best an unweighted\naverage recalled of 57.4 % for a 5-class support vector machine classification\nparadigm.", "published": "2019-08-05 15:01:17", "link": "http://arxiv.org/abs/1908.01671v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-lingual Text-independent Speaker Verification using Unsupervised\n  Adversarial Discriminative Domain Adaptation", "abstract": "Speaker verification systems often degrade significantly when there is a\nlanguage mismatch between training and testing data. Being able to improve\ncross-lingual speaker verification system using unlabeled data can greatly\nincrease the robustness of the system and reduce human labeling costs. In this\nstudy, we introduce an unsupervised Adversarial Discriminative Domain\nAdaptation (ADDA) method to effectively learn an asymmetric mapping that adapts\nthe target domain encoder to the source domain, where the target domain and\nsource domain are speech data from different languages. ADDA, together with a\npopular Domain Adversarial Training (DAT) approach, are evaluated on a\ncross-lingual speaker verification task: the training data is in English from\nNIST SRE04-08, Mixer 6 and Switchboard, and the test data is in Chinese from\nAISHELL-I. We show that with the ADDA adaptation, Equal Error Rate (EER) of the\nx-vector system decreases from 9.331\\% to 7.645\\%, relatively 18.07\\% reduction\nof EER, and 6.32\\% reduction from DAT as well. Further data analysis of ADDA\nadapted speaker embedding shows that the learned speaker embeddings can perform\nwell on speaker classification for the target domain data, and are less\ndependent with respect to the shift in language.", "published": "2019-08-05 02:35:22", "link": "http://arxiv.org/abs/1908.01447v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "V2S attack: building DNN-based voice conversion from automatic speaker\n  verification", "abstract": "This paper presents a new voice impersonation attack using voice conversion\n(VC). Enrolling personal voices for automatic speaker verification (ASV) offers\nnatural and flexible biometric authentication systems. Basically, the ASV\nsystems do not include the users' voice data. However, if the ASV system is\nunexpectedly exposed and hacked by a malicious attacker, there is a risk that\nthe attacker will use VC techniques to reproduce the enrolled user's voices. We\nname this the ``verification-to-synthesis (V2S) attack'' and propose VC\ntraining with the ASV and pre-trained automatic speech recognition (ASR) models\nand without the targeted speaker's voice data. The VC model reproduces the\ntargeted speaker's individuality by deceiving the ASV model and restores\nphonetic property of an input voice by matching phonetic posteriorgrams\npredicted by the ASR model. The experimental evaluation compares converted\nvoices between the proposed method that does not use the targeted speaker's\nvoice data and the standard VC that uses the data. The experimental results\ndemonstrate that the proposed method performs comparably to the existing VC\nmethods that trained using a very small amount of parallel voice data.", "published": "2019-08-05 03:28:13", "link": "http://arxiv.org/abs/1908.01454v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech\n  Recognition Systems", "abstract": "Automatic speech recognition (ASR) systems can be fooled via targeted\nadversarial examples, which induce the ASR to produce arbitrary transcriptions\nin response to altered audio signals. However, state-of-the-art adversarial\nexamples typically have to be fed into the ASR system directly, and are not\nsuccessful when played in a room. The few published over-the-air adversarial\nexamples fall into one of three categories: they are either handcrafted\nexamples, they are so conspicuous that human listeners can easily recognize the\ntarget transcription once they are alerted to its content, or they require\nprecise information about the room where the attack takes place, and are hence\nnot transferable to other rooms. In this paper, we demonstrate the first\nalgorithm that produces generic adversarial examples, which remain robust in an\nover-the-air attack that is not adapted to the specific environment. Hence, no\nprior knowledge of the room characteristics is required. Instead, we use room\nimpulse responses (RIRs) to compute robust adversarial examples for arbitrary\nroom characteristics and employ the ASR system Kaldi to demonstrate the attack.\nFurther, our algorithm can utilize psychoacoustic methods to hide changes of\nthe original audio signal below the human thresholds of hearing. In practical\nexperiments, we show that the adversarial examples work for varying room\nsetups, and that no direct line-of-sight between speaker and microphone is\nnecessary. As a result, an attacker can create inconspicuous adversarial\nexamples for any target transcription and apply these to arbitrary room setups\nwithout any prior knowledge.", "published": "2019-08-05 10:29:40", "link": "http://arxiv.org/abs/1908.01551v5", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
