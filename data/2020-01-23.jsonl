{"title": "Coordinated Reasoning for Cross-Lingual Knowledge Graph Alignment", "abstract": "Existing entity alignment methods mainly vary on the choices of encoding the\nknowledge graph, but they typically use the same decoding method, which\nindependently chooses the local optimal match for each source entity. This\ndecoding method may not only cause the \"many-to-one\" problem but also neglect\nthe coordinated nature of this task, that is, each alignment decision may\nhighly correlate to the other decisions. In this paper, we introduce two\ncoordinated reasoning methods, i.e., the Easy-to-Hard decoding strategy and\njoint entity alignment algorithm. Specifically, the Easy-to-Hard strategy first\nretrieves the model-confident alignments from the predicted results and then\nincorporates them as additional knowledge to resolve the remaining\nmodel-uncertain alignments. To achieve this, we further propose an enhanced\nalignment model that is built on the current state-of-the-art baseline. In\naddition, to address the many-to-one problem, we propose to jointly predict\nentity alignments so that the one-to-one constraint can be naturally\nincorporated into the alignment prediction. Experimental results show that our\nmodel achieves the state-of-the-art performance and our reasoning methods can\nalso significantly improve existing baselines.", "published": "2020-01-23 18:41:21", "link": "http://arxiv.org/abs/2001.08728v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Non-Normative Text Generation from Language Models", "abstract": "Large-scale, transformer-based language models such as GPT-2 are pretrained\non diverse corpora scraped from the internet. Consequently, they are prone to\ngenerating non-normative text (i.e. in violation of social norms). We introduce\na technique for fine-tuning GPT-2, using a policy gradient reinforcement\nlearning technique and a normative text classifier to produce reward and\npunishment values. We evaluate our technique on five data sets using automated\nand human participant experiments. The normative text classifier is 81-90%\naccurate when compared to gold-standard human judgments of normative and\nnon-normative generated text. Our normative fine-tuning technique is able to\nreduce non-normative text by 27-61%, depending on the data set.", "published": "2020-01-23 19:06:18", "link": "http://arxiv.org/abs/2001.08764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training via Leveraging Assisting Languages and Data Selection for\n  Neural Machine Translation", "abstract": "Sequence-to-sequence (S2S) pre-training using large monolingual data is known\nto improve performance for various S2S NLP tasks in low-resource settings.\nHowever, large monolingual corpora might not always be available for the\nlanguages of interest (LOI). To this end, we propose to exploit monolingual\ncorpora of other languages to complement the scarcity of monolingual corpora\nfor the LOI. A case study of low-resource Japanese-English neural machine\ntranslation (NMT) reveals that leveraging large Chinese and French monolingual\ncorpora can help overcome the shortage of Japanese and English monolingual\ncorpora, respectively, for S2S pre-training. We further show how to utilize\nscript mapping (Chinese to Japanese) to increase the similarity between the two\nmonolingual corpora leading to further improvements in translation quality.\nAdditionally, we propose simple data-selection techniques to be used prior to\npre-training that significantly impact the quality of S2S pre-training. An\nempirical comparison of our proposed methods reveals that leveraging assisting\nlanguage monolingual corpora, data selection and script mapping are extremely\nimportant for NMT pre-training in low-resource scenarios.", "published": "2020-01-23 02:47:39", "link": "http://arxiv.org/abs/2001.08353v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Hierarchical Dialog Autoencoder for Dialog State Tracking\n  Data Augmentation", "abstract": "Recent works have shown that generative data augmentation, where synthetic\nsamples generated from deep generative models complement the training dataset,\nbenefit NLP tasks. In this work, we extend this approach to the task of dialog\nstate tracking for goal-oriented dialogs. Due to the inherent hierarchical\nstructure of goal-oriented dialogs over utterances and related annotations, the\ndeep generative model must be capable of capturing the coherence among\ndifferent hierarchies and types of dialog features. We propose the Variational\nHierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of\ngoal-oriented dialogs, including linguistic features and underlying structured\nannotations, namely speaker information, dialog acts, and goals. The proposed\narchitecture is designed to model each aspect of goal-oriented dialogs using\ninter-connected latent variables and learns to generate coherent goal-oriented\ndialogs from the latent spaces. To overcome training issues that arise from\ntraining complex variational models, we propose appropriate training\nstrategies. Experiments on various dialog datasets show that our model improves\nthe downstream dialog trackers' robustness via generative data augmentation. We\nalso discover additional benefits of our unified approach to modeling\ngoal-oriented dialogs: dialog response generation and user simulation, where\nour model outperforms previous strong baselines.", "published": "2020-01-23 15:34:56", "link": "http://arxiv.org/abs/2001.08604v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Action Recognition and State Change Prediction in a Recipe Understanding\n  Task Using a Lightweight Neural Network Model", "abstract": "Consider a natural language sentence describing a specific step in a food\nrecipe. In such instructions, recognizing actions (such as press, bake, etc.)\nand the resulting changes in the state of the ingredients (shape molded,\ncustard cooked, temperature hot, etc.) is a challenging task. One way to cope\nwith this challenge is to explicitly model a simulator module that applies\nactions to entities and predicts the resulting outcome (Bosselut et al. 2018).\nHowever, such a model can be unnecessarily complex. In this paper, we propose a\nsimplified neural network model that separates action recognition and state\nchange prediction, while coupling the two through a novel loss function. This\nallows learning to indirectly influence each other. Our model, although\nsimpler, achieves higher state change prediction performance (67% average\naccuracy for ours vs. 55% in (Bosselut et al. 2018)) and takes fewer samples to\ntrain (10K ours vs. 65K+ by (Bosselut et al. 2018)).", "published": "2020-01-23 17:04:00", "link": "http://arxiv.org/abs/2001.08665v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linguistic Fingerprints of Internet Censorship: the Case of SinaWeibo", "abstract": "This paper studies how the linguistic components of blogposts collected from\nSina Weibo, a Chinese microblogging platform, might affect the blogposts'\nlikelihood of being censored. Our results go along with King et al. (2013)'s\nCollective Action Potential (CAP) theory, which states that a blogpost's\npotential of causing riot or assembly in real life is the key determinant of it\ngetting censored. Although there is not a definitive measure of this construct,\nthe linguistic features that we identify as discriminatory go along with the\nCAP theory. We build a classifier that significantly outperforms non-expert\nhumans in predicting whether a blogpost will be censored. The crowdsourcing\nresults suggest that while humans tend to see censored blogposts as more\ncontroversial and more likely to trigger action in real life than the\nuncensored counterparts, they in general cannot make a better guess than our\nmodel when it comes to `reading the mind' of the censors in deciding whether a\nblogpost should be censored. We do not claim that censorship is only determined\nby the linguistic features. There are many other factors contributing to\ncensorship decisions. The focus of the present paper is on the linguistic form\nof blogposts. Our work suggests that it is possible to use linguistic\nproperties of social media posts to automatically predict if they are going to\nbe censored.", "published": "2020-01-23 23:08:24", "link": "http://arxiv.org/abs/2001.08845v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Traduction des Grammaires Cat\u00e9gorielles de Lambek dans les Grammaires\n  Cat\u00e9gorielles Abstraites", "abstract": "Lambek Grammars (LG) are a computational modelling of natural language, based\non non-commutative compositional types. It has been widely studied, especially\nfor languages where the syntax plays a major role (like English). The goal of\nthis internship report is to demonstrate that every Lambek Grammar can be, not\nentirely but efficiently, expressed in Abstract Categorial Grammars (ACG). The\nlatter is a novel modelling based on higher-order signature homomorphisms\n(using $\\lambda$-calculus), aiming at uniting the currently used models. The\nmain idea is to transform the type rewriting system of LGs into that of\nContext-Free Grammars (CFG) by erasing introduction and elimination rules and\ngenerating enough axioms so that the cut rule suffices. This iterative approach\npreserves the derivations and enables us to stop the possible infinite\ngenerative process at any step. Although the underlying algorithm was not fully\nimplemented, this proof provides another argument in favour of the relevance of\nACGs in Natural Language Processing.", "published": "2020-01-23 18:23:03", "link": "http://arxiv.org/abs/2002.00725v1", "categories": ["cs.CL", "cs.LO", "68Q42 (Primary)", "F.4.2; J.5"], "primary_category": "cs.CL"}
{"title": "Improving speaker discrimination of target speech extraction with\n  time-domain SpeakerBeam", "abstract": "Target speech extraction, which extracts a single target source in a mixture\ngiven clues about the target speaker, has attracted increasing attention. We\nhave recently proposed SpeakerBeam, which exploits an adaptation utterance of\nthe target speaker to extract his/her voice characteristics that are then used\nto guide a neural network towards extracting speech of that speaker.\nSpeakerBeam presents a practical alternative to speech separation as it enables\ntracking speech of a target speaker across utterances, and achieves promising\nspeech extraction performance. However, it sometimes fails when speakers have\nsimilar voice characteristics, such as in same-gender mixtures, because it is\ndifficult to discriminate the target speaker from the interfering speakers. In\nthis paper, we investigate strategies for improving the speaker discrimination\ncapability of SpeakerBeam. First, we propose a time-domain implementation of\nSpeakerBeam similar to that proposed for a time-domain audio separation network\n(TasNet), which has achieved state-of-the-art performance for speech\nseparation. Besides, we investigate (1) the use of spatial features to better\ndiscriminate speakers when microphone array recordings are available, (2)\nadding an auxiliary speaker identification loss for helping to learn more\ndiscriminative voice characteristics. We show experimentally that these\nstrategies greatly improve speech extraction performance, especially for\nsame-gender mixtures, and outperform TasNet in terms of target speech\nextraction.", "published": "2020-01-23 05:36:06", "link": "http://arxiv.org/abs/2001.08378v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Study of the Tasks and Models in Machine Reading Comprehension", "abstract": "To provide a survey on the existing tasks and models in Machine Reading\nComprehension (MRC), this report reviews: 1) the dataset collection and\nperformance evaluation of some representative simple-reasoning and\ncomplex-reasoning MRC tasks; 2) the architecture designs, attention mechanisms,\nand performance-boosting approaches for developing neural-network-based MRC\nmodels; 3) some recently proposed transfer learning approaches to incorporating\ntext-style knowledge contained in external corpora into the neural networks of\nMRC models; 4) some recently proposed knowledge base encoding approaches to\nincorporating graph-style knowledge contained in external knowledge bases into\nthe neural networks of MRC models. Besides, according to what has been achieved\nand what are still deficient, this report also proposes some open problems for\nthe future research.", "published": "2020-01-23 16:11:44", "link": "http://arxiv.org/abs/2001.08635v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EventMapper: Detecting Real-World Physical Events Using Corroborative\n  and Probabilistic Sources", "abstract": "The ubiquity of social media makes it a rich source for physical event\ndetection, such as disasters, and as a potential resource for crisis management\nresource allocation. There have been some recent works on leveraging social\nmedia sources for retrospective, after-the-fact event detection of large events\nsuch as earthquakes or hurricanes. Similarly, there is a long history of using\ntraditional physical sensors such as climate satellites to perform regional\nevent detection. However, combining social media with corroborative physical\nsensors for real-time, accurate, and global physical detection has remained\nunexplored.\n  This paper presents EventMapper, a framework to support event recognition of\nsmall yet equally costly events (landslides, flooding, wildfires). EventMapper\nintegrates high-latency, high-accuracy corroborative sources such as physical\nsensors with low-latency, noisy probabilistic sources such as social media\nstreams to deliver real-time, global event recognition. Furthermore,\nEventMapper is resilient to the concept drift phenomenon, where machine\nlearning models require continuous fine-tuning to maintain high performance.\n  By exploiting the common features of probabilistic and corroborative sources,\nEventMapper automates machine learning model updates, maintenance, and\nfine-tuning. We describe three applications built on EventMapper for landslide,\nwildfire, and flooding detection.", "published": "2020-01-23 17:47:31", "link": "http://arxiv.org/abs/2001.08700v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Semi-Autoregressive Training Improves Mask-Predict Decoding", "abstract": "The recently proposed mask-predict decoding algorithm has narrowed the\nperformance gap between semi-autoregressive machine translation models and the\ntraditional left-to-right approach. We introduce a new training method for\nconditional masked language models, SMART, which mimics the semi-autoregressive\nbehavior of mask-predict, producing training examples that contain model\npredictions as part of their inputs. Models trained with SMART produce\nhigher-quality translations when using mask-predict decoding, effectively\nclosing the remaining performance gap with fully autoregressive models.", "published": "2020-01-23 19:56:35", "link": "http://arxiv.org/abs/2001.08785v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Graph Constrained Reinforcement Learning for Natural Language Action\n  Spaces", "abstract": "Interactive Fiction games are text-based simulations in which an agent\ninteracts with the world purely through natural language. They are ideal\nenvironments for studying how to extend reinforcement learning agents to meet\nthe challenges of natural language understanding, partial observability, and\naction generation in combinatorially-large text-based action spaces. We present\nKG-A2C, an agent that builds a dynamic knowledge graph while exploring and\ngenerates actions using a template-based action space. We contend that the dual\nuses of the knowledge graph to reason about game state and to constrain natural\nlanguage generation are the keys to scalable exploration of combinatorially\nlarge natural language actions. Results across a wide variety of IF games show\nthat KG-A2C outperforms current IF agents despite the exponential increase in\naction space size.", "published": "2020-01-23 22:33:18", "link": "http://arxiv.org/abs/2001.08837v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Uncertainty based Class Activation Maps for Visual Question Answering", "abstract": "Understanding and explaining deep learning models is an imperative task.\nTowards this, we propose a method that obtains gradient-based certainty\nestimates that also provide visual attention maps. Particularly, we solve for\nvisual question answering task. We incorporate modern probabilistic deep\nlearning methods that we further improve by using the gradients for these\nestimates. These have two-fold benefits: a) improvement in obtaining the\ncertainty estimates that correlate better with misclassified samples and b)\nimproved attention maps that provide state-of-the-art results in terms of\ncorrelation with human attention regions. The improved attention maps result in\nconsistent improvement for various methods for visual question answering.\nTherefore, the proposed technique can be thought of as a recipe for obtaining\nimproved certainty estimates and explanations for deep learning models. We\nprovide detailed empirical analysis for the visual question answering task on\nall standard benchmarks and comparison with state of the art methods.", "published": "2020-01-23 19:54:19", "link": "http://arxiv.org/abs/2002.10309v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Robust Explanations for Visual Question Answering", "abstract": "In this paper, we propose a method to obtain robust explanations for visual\nquestion answering(VQA) that correlate well with the answers. Our model\nexplains the answers obtained through a VQA model by providing visual and\ntextual explanations. The main challenges that we address are i) Answers and\ntextual explanations obtained by current methods are not well correlated and\nii) Current methods for visual explanation do not focus on the right location\nfor explaining the answer. We address both these challenges by using a\ncollaborative correlated module which ensures that even if we do not train for\nnoise based attacks, the enhanced correlation ensures that the right\nexplanation and answer can be generated. We further show that this also aids in\nimproving the generated visual and textual explanations. The use of the\ncorrelated module can be thought of as a robust method to verify if the answer\nand explanations are coherent. We evaluate this model using VQA-X dataset. We\nobserve that the proposed method yields better textual and visual justification\nthat supports the decision. We showcase the robustness of the model against a\nnoise-based perturbation attack using corresponding visual and textual\nexplanations. A detailed empirical analysis is shown. Here we provide source\ncode link for our model \\url{https://github.com/DelTA-Lab-IITK/CCM-WACV}.", "published": "2020-01-23 18:43:34", "link": "http://arxiv.org/abs/2001.08730v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Deep Bayesian Network for Visual Question Generation", "abstract": "Generating natural questions from an image is a semantic task that requires\nusing vision and language modalities to learn multimodal representations.\nImages can have multiple visual and language cues such as places, captions, and\ntags. In this paper, we propose a principled deep Bayesian learning framework\nthat combines these cues to produce natural questions. We observe that with the\naddition of more cues and by minimizing uncertainty in the among cues, the\nBayesian network becomes more confident. We propose a Minimizing Uncertainty of\nMixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues\nexperts for generating probabilistic questions. This is a Bayesian framework\nand the results show a remarkable similarity to natural questions as validated\nby a human study. We observe that with the addition of more cues and by\nminimizing uncertainty among the cues, the Bayesian framework becomes more\nconfident. Ablation studies of our model indicate that a subset of cues is\ninferior at this task and hence the principled fusion of cues is preferred.\nFurther, we observe that the proposed approach substantially improves over\nstate-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE,\nand CIDEr). Here we provide project link for Deep Bayesian VQG\n\\url{https://delta-lab-iitk.github.io/BVQG/}", "published": "2020-01-23 19:37:20", "link": "http://arxiv.org/abs/2001.08779v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "LaFurca: Iterative Refined Speech Separation Based on Context-Aware\n  Dual-Path Parallel Bi-LSTM", "abstract": "Deep neural network with dual-path bi-directional long short-term memory\n(BiLSTM) block has been proved to be very effective in sequence modeling,\nespecially in speech separation, e.g. DPRNN-TasNet \\cite{luo2019dual}. In this\npaper, we propose several improvements of dual-path BiLSTM based network for\nend-to-end approach to monaural speech separation. Firstly a dual-path network\nwith intra-parallel BiLSTM and inter-parallel BiLSTM components is introduced\nto reduce performance sub-variances among different branches. Secondly, we\npropose to use global context aware inter-intra cross-parallel BiLSTM to\nfurther perceive the global contextual information. Finally, a spiral\nmulti-stage dual-path BiLSTM is proposed to iteratively refine the separation\nresults of the previous stages. All these networks take the mixed utterance of\ntwo speakers and map it to two separate utterances, where each utterance\ncontains only one speaker's voice. For the objective, we propose to train the\nnetwork by directly optimizing the utterance level scale-invariant\nsignal-to-distortion ratio (SI-SDR) in a permutation invariant training (PIT)\nstyle. Our experiments on the public WSJ0-2mix data corpus results in 20.55dB\nSDR improvement, 20.35dB SI-SDR improvement, 3.69 of PESQ, and 94.86\\% of\nESTOI, which shows our proposed networks can lead to performance improvement on\nthe speaker separation task. We have open-sourced our re-implementation of the\nDPRNN-TasNet in\nhttps://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation,\nand our LaFurca is realized based on this implementation of DPRNN-TasNet, it is\nbelieved that the results in this paper can be reproduced with ease.", "published": "2020-01-23 02:03:26", "link": "http://arxiv.org/abs/2001.08998v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets,\n  Subjective Speech Quality and Testing Framework", "abstract": "The INTERSPEECH 2020 Deep Noise Suppression Challenge is intended to promote\ncollaborative research in real-time single-channel Speech Enhancement aimed to\nmaximize the subjective (perceptual) quality of the enhanced speech. A typical\napproach to evaluate the noise suppression methods is to use objective metrics\non the test set obtained by splitting the original dataset. Many publications\nreport reasonable performance on the synthetic test set drawn from the same\ndistribution as that of the training set. However, often the model performance\ndegrades significantly on real recordings. Also, most of the conventional\nobjective metrics do not correlate well with subjective tests and lab\nsubjective tests are not scalable for a large test set. In this challenge, we\nopen-source a large clean speech and noise corpus for training the noise\nsuppression models and a representative test set to real-world scenarios\nconsisting of both synthetic and real recordings. We also open source an online\nsubjective test framework based on ITU-T P.808 for researchers to quickly test\ntheir developments. The winners of this challenge will be selected based on\nsubjective evaluation on a representative test set using P.808 framework.", "published": "2020-01-23 17:00:21", "link": "http://arxiv.org/abs/2001.08662v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lipreading using Temporal Convolutional Networks", "abstract": "Lip-reading has attracted a lot of research attention lately thanks to\nadvances in deep learning. The current state-of-the-art model for recognition\nof isolated words in-the-wild consists of a residual network and Bidirectional\nGated Recurrent Unit (BGRU) layers. In this work, we address the limitations of\nthis model and we propose changes which further improve its performance.\nFirstly, the BGRU layers are replaced with Temporal Convolutional Networks\n(TCN). Secondly, we greatly simplify the training procedure, which allows us to\ntrain the model in one single stage. Thirdly, we show that the current\nstate-of-the-art methodology produces models that do not generalize well to\nvariations on the sequence length, and we addresses this issue by proposing a\nvariable-length augmentation. We present results on the largest\npublicly-available datasets for isolated word recognition in English and\nMandarin, LRW and LRW1000, respectively. Our proposed model results in an\nabsolute improvement of 1.2% and 3.2%, respectively, in these datasets which is\nthe new state-of-the-art performance.", "published": "2020-01-23 17:49:35", "link": "http://arxiv.org/abs/2001.08702v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Scattering Features for Multimodal Gait Recognition", "abstract": "We consider the problem of identifying people on the basis of their walk\n(gait) pattern. Classical approaches to tackle this problem are based on, e.g.,\nvideo recordings or piezoelectric sensors embedded in the floor. In this work,\nwe rely on acoustic and vibration measurements, obtained from a microphone and\na geophone sensor, respectively. The contribution of this work is twofold.\nFirst, we propose a feature extraction method based on an (untrained) shallow\nscattering network, specially tailored for the gait signals. Second, we\ndemonstrate that fusing the two modalities improves identification in the\npractically relevant open set scenario.", "published": "2020-01-23 22:11:38", "link": "http://arxiv.org/abs/2001.08830v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "On the human evaluation of audio adversarial examples", "abstract": "Human-machine interaction is increasingly dependent on speech communication.\nMachine Learning models are usually applied to interpret human speech commands.\nHowever, these models can be fooled by adversarial examples, which are inputs\nintentionally perturbed to produce a wrong prediction without being noticed.\nWhile much research has been focused on developing new techniques to generate\nadversarial perturbations, less attention has been given to aspects that\ndetermine whether and how the perturbations are noticed by humans. This\nquestion is relevant since high fooling rates of proposed adversarial\nperturbation strategies are only valuable if the perturbations are not\ndetectable. In this paper we investigate to which extent the distortion metrics\nproposed in the literature for audio adversarial examples, and which are\ncommonly applied to evaluate the effectiveness of methods for generating these\nattacks, are a reliable measure of the human perception of the perturbations.\nUsing an analytical framework, and an experiment in which 18 subjects evaluate\naudio adversarial examples, we demonstrate that the metrics employed by\nconvention are not a reliable measure of the perceptual similarity of\nadversarial examples in the audio domain.", "published": "2020-01-23 10:56:50", "link": "http://arxiv.org/abs/2001.08444v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
