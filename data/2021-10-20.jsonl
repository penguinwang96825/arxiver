{"title": "News-based Business Sentiment and its Properties as an Economic Index", "abstract": "This paper presents an approach to measuring business sentiment based on\ntextual data. Business sentiment has been measured by traditional surveys,\nwhich are costly and time-consuming to conduct. To address the issues, we take\nadvantage of daily newspaper articles and adopt a self-attention-based model to\ndefine a business sentiment index, named S-APIR, where outlier detection models\nare investigated to properly handle various genres of news articles. Moreover,\nwe propose a simple approach to temporally analyzing how much any given event\ncontributed to the predicted business sentiment index. To demonstrate the\nvalidity of the proposed approach, an extensive analysis is carried out on 12\nyears' worth of newspaper articles. The analysis shows that the S-APIR index is\nstrongly and positively correlated with established survey-based index (up to\ncorrelation coefficient r=0.937) and that the outlier detection is effective\nespecially for a general newspaper. Also, S-APIR is compared with a variety of\neconomic indices, revealing the properties of S-APIR that it reflects the trend\nof the macroeconomy as well as the economic outlook and sentiment of economic\nagents. Moreover, to illustrate how S-APIR could benefit economists and\npolicymakers, several events are analyzed with respect to their impacts on\nbusiness sentiment over time.", "published": "2021-10-20 02:20:53", "link": "http://arxiv.org/abs/2110.10340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Aspect-guided Explanation Generation for Explainable\n  Recommendation", "abstract": "Explainable recommendation systems provide explanations for recommendation\nresults to improve their transparency and persuasiveness. The existing\nexplainable recommendation methods generate textual explanations without\nexplicitly considering the user's preferences on different aspects of the item.\nIn this paper, we propose a novel explanation generation framework, named\nHierarchical Aspect-guided explanation Generation (HAG), for explainable\nrecommendation. Specifically, HAG employs a review-based syntax graph to\nprovide a unified view of the user/item details. An aspect-guided graph pooling\noperator is proposed to extract the aspect-relevant information from the\nreview-based syntax graphs to model the user's preferences on an item at the\naspect level. Then, a hierarchical explanation decoder is developed to generate\naspects and aspect-relevant explanations based on the attention mechanism. The\nexperimental results on three real datasets indicate that HAG outperforms\nstate-of-the-art explanation generation methods in both single-aspect and\nmulti-aspect explanation generation tasks, and also achieves comparable or even\nbetter preference prediction accuracy than strong baseline methods.", "published": "2021-10-20 03:28:58", "link": "http://arxiv.org/abs/2110.10358v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discontinuous Grammar as a Foreign Language", "abstract": "In order to achieve deep natural language understanding, syntactic\nconstituent parsing is a vital step, highly demanded by many artificial\nintelligence systems to process both text and speech. One of the most recent\nproposals is the use of standard sequence-to-sequence models to perform\nconstituent parsing as a machine translation task, instead of applying\ntask-specific parsers. While they show a competitive performance, these\ntext-to-parse transducers are still lagging behind classic techniques in terms\nof accuracy, coverage and speed. To close the gap, we here extend the framework\nof sequence-to-sequence models for constituent parsing, not only by providing a\nmore powerful neural architecture for improving their performance, but also by\nenlarging their coverage to handle the most complex syntactic phenomena:\ndiscontinuous structures. To that end, we design several novel linearizations\nthat can fully produce discontinuities and, for the first time, we test a\nsequence-to-sequence model on the main discontinuous benchmarks, obtaining\ncompetitive results on par with task-specific discontinuous constituent parsers\nand achieving state-of-the-art scores on the (discontinuous) English Penn\nTreebank.", "published": "2021-10-20 08:58:02", "link": "http://arxiv.org/abs/2110.10431v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Interpreting Deep Learning Models in Natural Language Processing: A\n  Review", "abstract": "Neural network models have achieved state-of-the-art performances in a wide\nrange of natural language processing (NLP) tasks. However, a long-standing\ncriticism against neural network models is the lack of interpretability, which\nnot only reduces the reliability of neural NLP systems but also limits the\nscope of their applications in areas where interpretability is essential (e.g.,\nhealth care applications). In response, the increasing interest in interpreting\nneural NLP models has spurred a diverse array of interpretation methods over\nrecent years. In this survey, we provide a comprehensive review of various\ninterpretation methods for neural models in NLP. We first stretch out a\nhigh-level taxonomy for interpretation methods in NLP, i.e., training-based\napproaches, test-based approaches, and hybrid approaches. Next, we describe\nsub-categories in each category in detail, e.g., influence-function based\nmethods, KNN-based methods, attention-based models, saliency-based methods,\nperturbation-based methods, etc. We point out deficiencies of current methods\nand suggest some avenues for future research.", "published": "2021-10-20 10:17:04", "link": "http://arxiv.org/abs/2110.10470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Unsupervised Neural Machine Translation with Denoising\n  Adapters", "abstract": "We consider the problem of multilingual unsupervised machine translation,\ntranslating to and from languages that only have monolingual data by using\nauxiliary parallel language pairs. For this problem the standard procedure so\nfar to leverage the monolingual data is back-translation, which is\ncomputationally costly and hard to tune.\n  In this paper we propose instead to use denoising adapters, adapter layers\nwith a denoising objective, on top of pre-trained mBART-50. In addition to the\nmodularity and flexibility of such an approach we show that the resulting\ntranslations are on-par with back-translating as measured by BLEU, and\nfurthermore it allows adding unseen languages incrementally.", "published": "2021-10-20 10:18:29", "link": "http://arxiv.org/abs/2110.10472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Learning in Multilingual NMT via Language-Specific Embeddings", "abstract": "This paper proposes a technique for adding a new source or target language to\nan existing multilingual NMT model without re-training it on the initial set of\nlanguages. It consists in replacing the shared vocabulary with a small\nlanguage-specific vocabulary and fine-tuning the new embeddings on the new\nlanguage's parallel data. Some additional language-specific components may be\ntrained to improve performance (e.g., Transformer layers or adapter modules).\nBecause the parameters of the original model are not modified, its performance\non the initial languages does not degrade. We show on two sets of experiments\n(small-scale on TED Talks, and large-scale on ParaCrawl) that this approach\nperforms as well or better as the more costly alternatives; and that it has\nexcellent zero-shot performance: training on English-centric data is enough to\ntranslate between the new language and any of the initial languages.", "published": "2021-10-20 10:38:57", "link": "http://arxiv.org/abs/2110.10478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SocialVisTUM: An Interactive Visualization Toolkit for Correlated Neural\n  Topic Models on Social Media Opinion Mining", "abstract": "Recent research in opinion mining proposed word embedding-based topic\nmodeling methods that provide superior coherence compared to traditional topic\nmodeling. In this paper, we demonstrate how these methods can be used to\ndisplay correlated topic models on social media texts using SocialVisTUM, our\nproposed interactive visualization toolkit. It displays a graph with topics as\nnodes and their correlations as edges. Further details are displayed\ninteractively to support the exploration of large text collections, e.g.,\nrepresentative words and sentences of topics, topic and sentiment\ndistributions, hierarchical topic clustering, and customizable, predefined\ntopic labels. The toolkit optimizes automatically on custom data for optimal\ncoherence. We show a working instance of the toolkit on data crawled from\nEnglish social media discussions about organic food consumption. The\nvisualization confirms findings of a qualitative consumer research study.\nSocialVisTUM and its training procedures are accessible online.", "published": "2021-10-20 14:04:13", "link": "http://arxiv.org/abs/2110.10575v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the 2021 Key Point Analysis Shared Task", "abstract": "We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point\nanalysis that we organized as a part of the 8th Workshop on Argument Mining\n(ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the\nresults of the shared task. We expect the task and the findings reported in\nthis paper to be relevant for researchers working on text summarization and\nargument mining.", "published": "2021-10-20 14:08:14", "link": "http://arxiv.org/abs/2110.10577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Evaluation Metrics for Style Transfer: A Case Study in\n  Multilingual Formality Transfer", "abstract": "While the field of style transfer (ST) has been growing rapidly, it has been\nhampered by a lack of standardized practices for automatic evaluation. In this\npaper, we evaluate leading ST automatic metrics on the oft-researched task of\nformality style transfer. Unlike previous evaluations, which focus solely on\nEnglish, we expand our focus to Brazilian-Portuguese, French, and Italian,\nmaking this work the first multilingual evaluation of metrics in ST. We outline\nbest practices for automatic evaluation in (formality) style transfer and\nidentify several models that correlate well with human judgments and are robust\nacross languages. We hope that this work will help accelerate development in\nST, where human evaluation is often challenging to collect.", "published": "2021-10-20 17:21:09", "link": "http://arxiv.org/abs/2110.10668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Self-Explainable Stylish Image Captioning Framework via\n  Multi-References", "abstract": "In this paper, we propose to build a stylish image captioning model through a\nMulti-style Multi modality mechanism (2M). We demonstrate that with 2M, we can\nbuild an effective stylish captioner and that multi-references produced by the\nmodel can also support explaining the model through identifying erroneous input\nfeatures on faulty examples. We show how this 2M mechanism can be used to build\nstylish captioning models and show how these models can be utilized to provide\nexplanations of likely errors in the models.", "published": "2021-10-20 18:00:40", "link": "http://arxiv.org/abs/2110.10704v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation", "abstract": "Generating texts in scientific papers requires not only capturing the content\ncontained within the given input but also frequently acquiring the external\ninformation called \\textit{context}. We push forward the scientific text\ngeneration by proposing a new task, namely \\textbf{context-aware text\ngeneration} in the scientific domain, aiming at exploiting the contributions of\ncontext in generated texts. To this end, we present a novel challenging\nlarge-scale \\textbf{Sci}entific Paper Dataset for Conte\\textbf{X}t-Aware Text\n\\textbf{Gen}eration (SciXGen), consisting of well-annotated 205,304 papers with\nfull references to widely-used objects (e.g., tables, figures, algorithms) in a\npaper. We comprehensively benchmark, using state-of-the-arts, the efficacy of\nour newly constructed SciXGen dataset in generating description and paragraph.\nOur dataset and benchmarks will be made publicly available to hopefully\nfacilitate the scientific text generation research.", "published": "2021-10-20 20:37:11", "link": "http://arxiv.org/abs/2110.10774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Document Representation Learning with Graph Attention\n  Networks", "abstract": "Recent progress in pretrained Transformer-based language models has shown\ngreat success in learning contextual representation of text. However, due to\nthe quadratic self-attention complexity, most of the pretrained Transformers\nmodels can only handle relatively short text. It is still a challenge when it\ncomes to modeling very long documents. In this work, we propose to use a graph\nattention network on top of the available pretrained Transformers model to\nlearn document embeddings. This graph attention network allows us to leverage\nthe high-level semantic structure of the document. In addition, based on our\ngraph document model, we design a simple contrastive learning strategy to\npretrain our models on a large amount of unlabeled corpus. Empirically, we\ndemonstrate the effectiveness of our approaches in document classification and\ndocument retrieval tasks.", "published": "2021-10-20 21:05:02", "link": "http://arxiv.org/abs/2110.10778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Multilingual Language Model Pretraining for Social Media Text\n  via Translation Pair Prediction", "abstract": "We evaluate a simple approach to improving zero-shot multilingual transfer of\nmBERT on social media corpus by adding a pretraining task called translation\npair prediction (TPP), which predicts whether a pair of cross-lingual texts are\na valid translation. Our approach assumes access to translations (exact or\napproximate) between source-target language pairs, where we fine-tune a model\non source language task data and evaluate the model in the target language. In\nparticular, we focus on language pairs where transfer learning is difficult for\nmBERT: those where source and target languages are different in script,\nvocabulary, and linguistic typology. We show improvements from TPP pretraining\nover mBERT alone in zero-shot transfer from English to Hindi, Arabic, and\nJapanese on two social media tasks: NER (a 37% average relative improvement in\nF1 across target languages) and sentiment classification (12% relative\nimprovement in F1) on social media text, while also benchmarking on a\nnon-social media task of Universal Dependency POS tagging (6.7% relative\nimprovement in accuracy). Our results are promising given the lack of social\nmedia bitext corpus. Our code can be found at:\nhttps://github.com/twitter-research/multilingual-alignment-tpp.", "published": "2021-10-20 00:06:26", "link": "http://arxiv.org/abs/2110.10318v1", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "R$^3$Net:Relation-embedded Representation Reconstruction Network for\n  Change Captioning", "abstract": "Change captioning is to use a natural language sentence to describe the\nfine-grained disagreement between two similar images. Viewpoint change is the\nmost typical distractor in this task, because it changes the scale and location\nof the objects and overwhelms the representation of real change. In this paper,\nwe propose a Relation-embedded Representation Reconstruction Network (R$^3$Net)\nto explicitly distinguish the real change from the large amount of clutter and\nirrelevant changes. Specifically, a relation-embedded module is first devised\nto explore potential changed objects in the large amount of clutter. Then,\nbased on the semantic similarities of corresponding locations in the two\nimages, a representation reconstruction module (RRM) is designed to learn the\nreconstruction representation and further model the difference representation.\nBesides, we introduce a syntactic skeleton predictor (SSP) to enhance the\nsemantic interaction between change localization and caption generation.\nExtensive experiments show that the proposed method achieves the\nstate-of-the-art results on two public datasets.", "published": "2021-10-20 00:57:39", "link": "http://arxiv.org/abs/2110.10328v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text\n  Joint Pre-Training", "abstract": "Unsupervised pre-training is now the predominant approach for both text and\nspeech understanding. Self-attention models pre-trained on large amounts of\nunannotated data have been hugely successful when fine-tuned on downstream\ntasks from a variety of domains and languages. This paper takes the\nuniversality of unsupervised language pre-training one step further, by\nunifying speech and text pre-training within a single model. We build a single\nencoder with the BERT objective on unlabeled text together with the w2v-BERT\nobjective on unlabeled speech. To further align our model representations\nacross modalities, we leverage alignment losses, specifically Translation\nLanguage Modeling (TLM) and Speech Text Matching (STM) that make use of\nsupervised speech-text recognition data. We demonstrate that incorporating both\nspeech and text data during pre-training can significantly improve downstream\nquality on CoVoST~2 speech translation, by around 1 BLEU compared to\nsingle-modality pre-trained models, while retaining close to SotA performance\non LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and\ntext-normalization, we observe evidence of capacity limitations and\ninterference between the two modalities, leading to degraded performance\ncompared to an equivalent text-only model, while still being competitive with\nBERT. Through extensive empirical analysis we also demonstrate the importance\nof the choice of objective function for speech pre-training, and the beneficial\neffect of adding additional supervised signals on the quality of the learned\nrepresentations.", "published": "2021-10-20 00:59:36", "link": "http://arxiv.org/abs/2110.10329v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distributionally Robust Classifiers in Sentiment Analysis", "abstract": "In this paper, we propose sentiment classification models based on BERT\nintegrated with DRO (Distributionally Robust Classifiers) to improve model\nperformance on datasets with distributional shifts. We added 2-Layer Bi-LSTM,\nprojection layer (onto simplex or Lp ball), and linear layer on top of BERT to\nachieve distributionally robustness. We considered one form of distributional\nshift (from IMDb dataset to Rotten Tomatoes dataset). We have confirmed through\nexperiments that our DRO model does improve performance on our test set with\ndistributional shift from the training set.", "published": "2021-10-20 04:52:54", "link": "http://arxiv.org/abs/2110.10372v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Better than Average: Paired Evaluation of NLP Systems", "abstract": "Evaluation in NLP is usually done by comparing the scores of competing\nsystems independently averaged over a common set of test instances. In this\nwork, we question the use of averages for aggregating evaluation scores into a\nfinal number used to decide which system is best, since the average, as well as\nalternatives such as the median, ignores the pairing arising from the fact that\nsystems are evaluated on the same test instances. We illustrate the importance\nof taking the instance-level pairing of evaluation scores into account and\ndemonstrate, both theoretically and empirically, the advantages of aggregation\nmethods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a\nmechanism based on the estimated probability that a given system scores better\nthan another on the test set. By re-evaluating 296 real NLP evaluation setups\nacross four tasks and 18 evaluation metrics, we show that the choice of\naggregation mechanism matters and yields different conclusions as to which\nsystems are state of the art in about 30% of the setups. To facilitate the\nadoption of pairwise evaluation, we release a practical tool for performing the\nfull analysis of evaluation scores with the mean, median, BT, and two variants\nof BT (Elo and TrueSkill), alongside functionality for appropriate statistical\ntesting.", "published": "2021-10-20 19:40:31", "link": "http://arxiv.org/abs/2110.10746v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Open Natural Language Processing Development Framework for EHR-based\n  Clinical Research: A case demonstration using the National COVID Cohort\n  Collaborative (N3C)", "abstract": "While we pay attention to the latest advances in clinical natural language\nprocessing (NLP), we can notice some resistance in the clinical and\ntranslational research community to adopt NLP models due to limited\ntransparency, interpretability, and usability. In this study, we proposed an\nopen natural language processing development framework. We evaluated it through\nthe implementation of NLP algorithms for the National COVID Cohort\nCollaborative (N3C). Based on the interests in information extraction from\nCOVID-19 related clinical notes, our work includes 1) an open data annotation\nprocess using COVID-19 signs and symptoms as the use case, 2) a\ncommunity-driven ruleset composing platform, and 3) a synthetic text data\ngeneration workflow to generate texts for information extraction tasks without\ninvolving human subjects. The corpora were derived from texts from three\ndifferent institutions (Mayo Clinic, University of Kentucky, University of\nMinnesota). The gold standard annotations were tested with a single\ninstitution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706,\nand 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets,\nrespectively. The study as a consortium effort of the N3C NLP subgroup\ndemonstrates the feasibility of creating a federated NLP algorithm development\nand benchmarking platform to enhance multi-institution clinical NLP study and\nadoption. Although we use COVID-19 as a use case in this effort, our framework\nis general enough to be applied to other domains of interest in clinical NLP.", "published": "2021-10-20 21:09:41", "link": "http://arxiv.org/abs/2110.10780v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LMSOC: An Approach for Socially Sensitive Pretraining", "abstract": "While large-scale pretrained language models have been shown to learn\neffective linguistic representations for many NLP tasks, there remain many\nreal-world contextual aspects of language that current approaches do not\ncapture. For instance, consider a cloze-test \"I enjoyed the ____ game this\nweekend\": the correct answer depends heavily on where the speaker is from, when\nthe utterance occurred, and the speaker's broader social milieu and\npreferences. Although language depends heavily on the geographical, temporal,\nand other social contexts of the speaker, these elements have not been\nincorporated into modern transformer-based language models. We propose a simple\nbut effective approach to incorporate speaker social context into the learned\nrepresentations of large-scale language models. Our method first learns dense\nrepresentations of social contexts using graph representation learning\nalgorithms and then primes language model pretraining with these social context\nrepresentations. We evaluate our approach on geographically-sensitive\nlanguage-modeling tasks and show a substantial improvement (more than 100%\nrelative lift on MRR) compared to baselines.", "published": "2021-10-20 00:10:37", "link": "http://arxiv.org/abs/2110.10319v1", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Knowledge distillation from language model to acoustic model: a\n  hierarchical multi-task learning approach", "abstract": "The remarkable performance of the pre-trained language model (LM) using\nself-supervised learning has led to a major paradigm shift in the study of\nnatural language processing. In line with these changes, leveraging the\nperformance of speech recognition systems with massive deep learning-based LMs\nis a major topic of speech recognition research. Among the various methods of\napplying LMs to speech recognition systems, in this paper, we focus on a\ncross-modal knowledge distillation method that transfers knowledge between two\ntypes of deep neural networks with different modalities. We propose an acoustic\nmodel structure with multiple auxiliary output layers for cross-modal\ndistillation and demonstrate that the proposed method effectively compensates\nfor the shortcomings of the existing label-interpolation-based distillation\nmethod. In addition, we extend the proposed method to a hierarchical\ndistillation method using LMs trained in different units (senones, monophones,\nand subwords) and reveal the effectiveness of the hierarchical distillation\nmethod through an ablation study.", "published": "2021-10-20 08:42:10", "link": "http://arxiv.org/abs/2110.10429v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Knowledge Graph informed Fake News Classification via Heterogeneous\n  Representation Ensembles", "abstract": "Increasing amounts of freely available data both in textual and relational\nform offers exploration of richer document representations, potentially\nimproving the model performance and robustness. An emerging problem in the\nmodern era is fake news detection -- many easily available pieces of\ninformation are not necessarily factually correct, and can lead to wrong\nconclusions or are used for manipulation. In this work we explore how different\ndocument representations, ranging from simple symbolic bag-of-words, to\ncontextual, neural language model-based ones can be used for efficient fake\nnews identification. One of the key contributions is a set of novel document\nrepresentation learning methods based solely on knowledge graphs, i.e.\nextensive collections of (grounded) subject-predicate-object triplets. We\ndemonstrate that knowledge graph-based representations already achieve\ncompetitive performance to conventionally accepted representation learners.\nFurthermore, when combined with existing, contextual representations, knowledge\ngraph-based document representations can achieve state-of-the-art performance.\nTo our knowledge this is the first larger-scale evaluation of how knowledge\ngraph-based representations can be systematically incorporated into the process\nof fake news classification.", "published": "2021-10-20 09:41:14", "link": "http://arxiv.org/abs/2110.10457v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SILG: The Multi-environment Symbolic Interactive Language Grounding\n  Benchmark", "abstract": "Existing work in language grounding typically study single environments. How\ndo we build unified models that apply across multiple environments? We propose\nthe multi-environment Symbolic Interactive Language Grounding benchmark (SILG),\nwhich unifies a collection of diverse grounded language learning environments\nunder a common interface. SILG consists of grid-world environments that require\ngeneralization to new dynamics, entities, and partially observed worlds (RTFM,\nMessenger, NetHack), as well as symbolic counterparts of visual worlds that\nrequire interpreting rich natural language with respect to complex scenes\n(ALFWorld, Touchdown). Together, these environments provide diverse grounding\nchallenges in richness of observation space, action space, language\nspecification, and plan complexity. In addition, we propose the first shared\nmodel architecture for RL on these environments, and evaluate recent advances\nsuch as egocentric local convolution, recurrent state-tracking, entity-centric\nattention, and pretrained LM using SILG. Our shared architecture achieves\ncomparable performance to environment-specific architectures. Moreover, we find\nthat many recent modelling advances do not result in significant gains on\nenvironments other than the one they were designed for. This highlights the\nneed for a multi-environment benchmark. Finally, the best models significantly\nunderperform humans on SILG, which suggests ample room for future work. We hope\nSILG enables the community to quickly identify new methodologies for language\ngrounding that generalize to a diverse set of environments and their associated\nchallenges.", "published": "2021-10-20 17:02:06", "link": "http://arxiv.org/abs/2110.10661v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The R package sentometrics to compute, aggregate and predict with\n  textual sentiment", "abstract": "We provide a hands-on introduction to optimized textual sentiment indexation\nusing the R package sentometrics. Textual sentiment analysis is increasingly\nused to unlock the potential information value of textual data. The\nsentometrics package implements an intuitive framework to efficiently compute\nsentiment scores of numerous texts, to aggregate the scores into multiple time\nseries, and to use these time series to predict other variables. The workflow\nof the package is illustrated with a built-in corpus of news articles from two\nmajor U.S. journals to forecast the CBOE Volatility Index.", "published": "2021-10-20 23:24:45", "link": "http://arxiv.org/abs/2110.10817v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.AP"], "primary_category": "stat.ML"}
{"title": "VLDeformer: Vision-Language Decomposed Transformer for Fast Cross-Modal\n  Retrieval", "abstract": "Cross-model retrieval has emerged as one of the most important upgrades for\ntext-only search engines (SE). Recently, with powerful representation for\npairwise text-image inputs via early interaction, the accuracy of\nvision-language (VL) transformers has outperformed existing methods for\ntext-image retrieval. However, when the same paradigm is used for inference,\nthe efficiency of the VL transformers is still too low to be applied in a real\ncross-modal SE. Inspired by the mechanism of human learning and using\ncross-modal knowledge, this paper presents a novel Vision-Language Decomposed\nTransformer (VLDeformer), which greatly increases the efficiency of VL\ntransformers while maintaining their outstanding accuracy. By the proposed\nmethod, the cross-model retrieval is separated into two stages: the VL\ntransformer learning stage, and the VL decomposition stage. The latter stage\nplays the role of single modal indexing, which is to some extent like the term\nindexing of a text SE. The model learns cross-modal knowledge from\nearly-interaction pre-training and is then decomposed into an individual\nencoder. The decomposition requires only small target datasets for supervision\nand achieves both $1000+$ times acceleration and less than $0.6$\\% average\nrecall drop. VLDeformer also outperforms state-of-the-art visual-semantic\nembedding methods on COCO and Flickr30k.", "published": "2021-10-20 09:00:51", "link": "http://arxiv.org/abs/2110.11338v3", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Disentanglement of Emotional Style and Speaker Identity for Expressive\n  Voice Conversion", "abstract": "Expressive voice conversion performs identity conversion for emotional\nspeakers by jointly converting speaker identity and emotional style. Due to the\nhierarchical structure of speech emotion, it is challenging to disentangle the\nemotional style for different speakers. Inspired by the recent success of\nspeaker disentanglement with variational autoencoder (VAE), we propose an\nany-to-any expressive voice conversion framework, that is called StyleVC.\nStyleVC is designed to disentangle linguistic content, speaker identity, pitch,\nand emotional style information. We study the use of style encoder to model\nemotional style explicitly. At run-time, StyleVC converts both speaker identity\nand emotional style for arbitrary speakers. Experiments validate the\neffectiveness of our proposed framework in both objective and subjective\nevaluations.", "published": "2021-10-20 00:49:02", "link": "http://arxiv.org/abs/2110.10326v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "One model to enhance them all: array geometry agnostic multi-channel\n  personalized speech enhancement", "abstract": "With the recent surge of video conferencing tools usage, providing\nhigh-quality speech signals and accurate captions have become essential to\nconduct day-to-day business or connect with friends and families.\nSingle-channel personalized speech enhancement (PSE) methods show promising\nresults compared with the unconditional speech enhancement (SE) methods in\nthese scenarios due to their ability to remove interfering speech in addition\nto the environmental noise. In this work, we leverage spatial information\nafforded by microphone arrays to improve such systems' performance further. We\ninvestigate the relative importance of speaker embeddings and spatial features.\nMoreover, we propose a new causal array-geometry-agnostic multi-channel PSE\nmodel, which can generate a high-quality enhanced signal from arbitrary\nmicrophone geometry. Experimental results show that the proposed geometry\nagnostic model outperforms the model trained on a specific microphone array\ngeometry in both speech quality and automatic speech recognition accuracy. We\nalso demonstrate the effectiveness of the proposed approach for unseen array\ngeometries.", "published": "2021-10-20 01:03:07", "link": "http://arxiv.org/abs/2110.10330v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting Speech Separation to Real-World Meetings Using Mixture\n  Invariant Training", "abstract": "The recently-proposed mixture invariant training (MixIT) is an unsupervised\nmethod for training single-channel sound separation models in the sense that it\ndoes not require ground-truth isolated reference sources. In this paper, we\ninvestigate using MixIT to adapt a separation model on real far-field\noverlapping reverberant and noisy speech data from the AMI Corpus. The models\nare tested on real AMI recordings containing overlapping speech, and are\nevaluated subjectively by human listeners. To objectively evaluate our models,\nwe also devise a synthetic AMI test set. For human evaluations on real\nrecordings, we also propose a modification of the standard MUSHRA protocol to\nhandle imperfect reference signals, which we call MUSHIRA. Holding network\narchitectures constant, we find that a fine-tuned semi-supervised model yields\nthe largest SI-SNR improvement, PESQ scores, and human listening ratings across\nsynthetic and real datasets, outperforming unadapted generalist models trained\non orders of magnitude more data. Our results show that unsupervised learning\nthrough MixIT enables model adaptation on real-world unlabeled spontaneous\nspeech recordings.", "published": "2021-10-20 19:21:41", "link": "http://arxiv.org/abs/2110.10739v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TPARN: Triple-path Attentive Recurrent Network for Time-domain\n  Multichannel Speech Enhancement", "abstract": "In this work, we propose a new model called triple-path attentive recurrent\nnetwork (TPARN) for multichannel speech enhancement in the time domain. TPARN\nextends a single-channel dual-path network to a multichannel network by adding\na third path along the spatial dimension. First, TPARN processes speech signals\nfrom all channels independently using a dual-path attentive recurrent network\n(ARN), which is a recurrent neural network (RNN) augmented with self-attention.\nNext, an ARN is introduced along the spatial dimension for spatial context\naggregation. TPARN is designed as a multiple-input and multiple-output\narchitecture to enhance all input channels simultaneously. Experimental results\ndemonstrate the superiority of TPARN over existing state-of-the-art approaches.", "published": "2021-10-20 20:00:29", "link": "http://arxiv.org/abs/2110.10757v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of Enhancing CTC Model for Triggered Attention-based\n  Streaming ASR", "abstract": "In the present paper, an attempt is made to combine Mask-CTC and the\ntriggered attention mechanism to construct a streaming end-to-end automatic\nspeech recognition (ASR) system that provides high performance with low\nlatency. The triggered attention mechanism, which performs autoregressive\ndecoding triggered by the CTC spike, has shown to be effective in streaming\nASR. However, in order to maintain high accuracy of alignment estimation based\non CTC outputs, which is the key to its performance, it is inevitable that\ndecoding should be performed with some future information input (i.e., with\nhigher latency). It should be noted that in streaming ASR, it is desirable to\nbe able to achieve high recognition accuracy while keeping the latency low.\nTherefore, the present study aims to achieve highly accurate streaming ASR with\nlow latency by introducing Mask-CTC, which is capable of learning feature\nrepresentations that anticipate future information (i.e., that can consider\nlong-term contexts), to the encoder pre-training. Experimental comparisons\nconducted using WSJ data demonstrate that the proposed method achieves higher\naccuracy with lower latency than the conventional triggered attention-based\nstreaming ASR system.", "published": "2021-10-20 06:44:58", "link": "http://arxiv.org/abs/2110.10402v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study On Data Augmentation In Voice Anti-Spoofing", "abstract": "In this paper, we perform an in-depth study of how data augmentation\ntechniques improve synthetic or spoofed audio detection. Specifically, we\npropose methods to deal with channel variability, different audio compressions,\ndifferent band-widths, and unseen spoofing attacks, which have all been shown\nto significantly degrade the performance of audio-based systems and\nAnti-Spoofing systems. Our results are based on the ASVspoof 2021 challenge, in\nthe Logical Access (LA) and Deep Fake (DF) categories. Our study is\nData-Centric, meaning that the models are fixed and we significantly improve\nthe results by making changes in the data. We introduce two forms of data\naugmentation - compression augmentation for the DF part, compression & channel\naugmentation for the LA part. In addition, a new type of online data\naugmentation, SpecAverage, is introduced in which the audio features are masked\nwith their average value in order to improve generalization. Furthermore, we\nintroduce a Log spectrogram feature design that improved the results. Our best\nsingle system and fusion scheme both achieve state-of-the-art performance in\nthe DF category, with an EER of 15.46% and 14.46% respectively. Our best system\nfor the LA task reduced the best baseline EER by 50% and the min t-DCF by 16%.\nOur techniques to deal with spoofed data from a wide variety of distributions\ncan be replicated and can help anti-spoofing and speech-based systems enhance\ntheir results.", "published": "2021-10-20 11:09:05", "link": "http://arxiv.org/abs/2110.10491v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Progressive Learning for Stabilizing Label Selection in Speech\n  Separation with Mapping-based Method", "abstract": "Speech separation has been studied in time domain because of lower latency\nand higher performance compared to time-frequency domain. The masking-based\nmethod has been mostly used in time domain, and the other common method\n(mapping-based) has been inadequately studied. We investigate the use of the\nmapping-based method in the time domain and show that it can perform better on\na large training set than the masking-based method. We also investigate the\nfrequent label-switching problem in permutation invariant training (PIT), which\nresults in suboptimal training because the labels selected by PIT differ across\ntraining epochs. Our experiment results showed that PIT works well in a shallow\nseparation model, and the label switching occurs for a deeper model. We\ninferred that layer decoupling may be the reason for the frequent label\nswitching. Therefore, we propose a training strategy based on progressive\nlearning. This approach significantly reduced inconsistent label assignment\nwithout added computational complexity or training corpus. By combining this\ntraining strategy with the mapping-based method, we significantly improved the\nseparation performance compared to the baseline.", "published": "2021-10-20 14:42:50", "link": "http://arxiv.org/abs/2110.10593v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "REAL-M: Towards Speech Separation on Real Mixtures", "abstract": "In recent years, deep learning based source separation has achieved\nimpressive results. Most studies, however, still evaluate separation models on\nsynthetic datasets, while the performance of state-of-the-art techniques on\nin-the-wild speech data remains an open question. This paper contributes to\nfill this gap in two ways. First, we release the REAL-M dataset, a\ncrowd-sourced corpus of real-life mixtures. Secondly, we address the problem of\nperformance evaluation of real-life mixtures, where the ground truth is not\navailable. We bypass this issue by carefully designing a blind Scale-Invariant\nSignal-to-Noise Ratio (SI-SNR) neural estimator. Through a user study, we show\nthat our estimator reliably evaluates the separation performance on real\nmixtures. The performance predictions of the SI-SNR estimator indeed correlate\nwell with human opinions. Moreover, we observe that the performance trends\npredicted by our estimator on the REAL-M dataset closely follow those achieved\non synthetic benchmarks when evaluating popular speech separation models.", "published": "2021-10-20 22:39:35", "link": "http://arxiv.org/abs/2110.10812v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
