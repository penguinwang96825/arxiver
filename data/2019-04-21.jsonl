{"title": "Few-Shot NLG with Pre-Trained Language Model", "abstract": "Neural-based end-to-end approaches to natural language generation (NLG) from\nstructured data or knowledge are data-hungry, making their adoption for\nreal-world applications difficult with limited data. In this work, we propose\nthe new task of \\textit{few-shot natural language generation}. Motivated by how\nhumans tend to summarize tabular data, we propose a simple yet effective\napproach and show that it not only demonstrates strong performance but also\nprovides good generalization across domains. The design of the model\narchitecture is based on two aspects: content selection from input data and\nlanguage modeling to compose coherent sentences, which can be acquired from\nprior knowledge. With just 200 training examples, across multiple domains, we\nshow that our approach achieves very reasonable performances and outperforms\nthe strongest baseline by an average of over 8.0 BLEU points improvement. Our\ncode and data can be found at \\url{https://github.com/czyssrs/Few-Shot-NLG}", "published": "2019-04-21 00:42:22", "link": "http://arxiv.org/abs/1904.09521v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego", "abstract": "Deep Neural Networks (DNN) have been widely employed in industry to address\nvarious Natural Language Processing (NLP) tasks. However, many engineers find\nit a big overhead when they have to choose from multiple frameworks, compare\ndifferent types of models, and understand various optimization mechanisms. An\nNLP toolkit for DNN models with both generality and flexibility can greatly\nimprove the productivity of engineers by saving their learning cost and guiding\nthem to find optimal solutions to their tasks. In this paper, we introduce\nNeuronBlocks\\footnote{Code: \\url{https://github.com/Microsoft/NeuronBlocks}}\n\\footnote{Demo: \\url{https://youtu.be/x6cOpVSZcdo}}, a toolkit encapsulating a\nsuite of neural network modules as building blocks to construct various DNN\nmodels with complex architecture. This toolkit empowers engineers to build,\ntrain, and test various NLP models through simple configuration of JSON files.\nThe experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003\ndemonstrate the effectiveness of NeuronBlocks.", "published": "2019-04-21 03:11:24", "link": "http://arxiv.org/abs/1904.09535v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact Discovery from Knowledge Base via Facet Decomposition", "abstract": "During the past few decades, knowledge bases (KBs) have experienced rapid\ngrowth. Nevertheless, most KBs still suffer from serious incompletion.\nResearchers proposed many tasks such as knowledge base completion and relation\nprediction to help build the representation of KBs. However, there are some\nissues unsettled towards enriching the KBs. Knowledge base completion and\nrelation prediction assume that we know two elements of the fact triples and we\nare going to predict the missing one. This assumption is too restricted in\npractice and prevents it from discovering new facts directly. To address this\nissue, we propose a new task, namely, fact discovery from knowledge base. This\ntask only requires that we know the head entity and the goal is to discover\nfacts associated with the head entity. To tackle this new problem, we propose a\nnovel framework that decomposes the discovery problem into several facet\ndiscovery components. We also propose a novel auto-encoder based facet\ncomponent to estimate some facets of the fact. Besides, we propose a feedback\nlearning component to share the information between each facet. We evaluate our\nframework using a benchmark dataset and the experimental results show that our\nframework achieves promising results. We also conduct extensive analysis of our\nframework in discovering different kinds of facts. The source code of this\npaper can be obtained from https://github.com/thunlp/FFD.", "published": "2019-04-21 04:13:55", "link": "http://arxiv.org/abs/1904.09540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good-Enough Compositional Data Augmentation", "abstract": "We propose a simple data augmentation protocol aimed at providing a\ncompositional inductive bias in conditional and unconditional sequence models.\nUnder this protocol, synthetic training examples are constructed by taking real\ntraining examples and replacing (possibly discontinuous) fragments with other\nfragments that appear in at least one similar environment. The protocol is\nmodel-agnostic and useful for a variety of tasks. Applied to neural\nsequence-to-sequence models, it reduces error rate by as much as 87% on\ndiagnostic tasks from the SCAN dataset and 16% on a semantic parsing task.\nApplied to n-gram language models, it reduces perplexity by roughly 1% on small\ncorpora in several languages.", "published": "2019-04-21 05:54:30", "link": "http://arxiv.org/abs/1904.09545v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Stability of Medical Concept Embeddings", "abstract": "Frequency is one of the major factors for training quality word embeddings.\nSeveral work has recently discussed the stability of word embeddings in general\ndomain and suggested factors influencing the stability. In this work, we\nconduct a detailed analysis on the stability of concept embeddings in medical\ndomain, particularly the relation with concept frequency. The analysis reveals\nthe surprising high stability of low-frequency concepts: low-frequency (<100)\nconcepts have the same high stability as high-frequency (>1000) concepts. To\ndevelop a deeper understanding of this finding, we propose a new factor, the\nnoisiness of context words, which influences the stability of medical concept\nembeddings, regardless of frequency. We evaluate the proposed factor by showing\nthe linear correlation with the stability of medical concept embeddings. The\ncorrelations are clear and consistent with various groups of medical concepts.\nBased on the linear relations, we make suggestions on ways to adjust the\nnoisiness of context words for the improvement of stability. Finally, we\ndemonstrate that the proposed factor extends to the word embedding stability in\ngeneral domain.", "published": "2019-04-21 07:07:48", "link": "http://arxiv.org/abs/1904.09552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Compression with Multi-Task Knowledge Distillation for Web-scale\n  Question Answering System", "abstract": "Deep pre-training and fine-tuning models (like BERT, OpenAI GPT) have\ndemonstrated excellent results in question answering areas. However, due to the\nsheer amount of model parameters, the inference speed of these models is very\nslow. How to apply these complex models to real business scenarios becomes a\nchallenging but practical problem. Previous works often leverage model\ncompression approaches to resolve this problem. However, these methods usually\ninduce information loss during the model compression procedure, leading to\nincomparable results between compressed model and the original model. To tackle\nthis challenge, we propose a Multi-task Knowledge Distillation Model (MKDM for\nshort) for web-scale Question Answering system, by distilling knowledge from\nmultiple teacher models to a light-weight student model. In this way, more\ngeneralized knowledge can be transferred. The experiment results show that our\nmethod can significantly outperform the baseline methods and even achieve\ncomparable results with the original teacher models, along with significant\nspeedup of model inference.", "published": "2019-04-21 17:46:24", "link": "http://arxiv.org/abs/1904.09636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Past and Future for Neural Machine Translation", "abstract": "Previous studies have shown that neural machine translation (NMT) models can\nbenefit from explicitly modeling translated (Past) and untranslated (Future) to\ngroups of translated and untranslated contents through parts-to-wholes\nassignment. The assignment is learned through a novel variant of\nrouting-by-agreement mechanism (Sabour et al., 2017), namely {\\em Guided\nDynamic Routing}, where the translating status at each decoding step {\\em\nguides} the routing process to assign each source word to its associated group\n(i.e., translated or untranslated content) represented by a capsule, enabling\ntranslation to be made from holistic context. Experiments show that our\napproach achieves substantial improvements over both RNMT and Transformer by\nproducing more adequate translations. Extensive analysis demonstrates that our\nmethod is highly interpretable, which is able to recognize the translated and\nuntranslated contents as expected.", "published": "2019-04-21 19:07:07", "link": "http://arxiv.org/abs/1904.09646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTScore: Evaluating Text Generation with BERT", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation.\nAnalogously to common metrics, BERTScore computes a similarity score for each\ntoken in the candidate sentence with each token in the reference sentence.\nHowever, instead of exact matches, we compute token similarity using contextual\nembeddings. We evaluate using the outputs of 363 machine translation and image\ncaptioning systems. BERTScore correlates better with human judgments and\nprovides stronger model selection performance than existing metrics. Finally,\nwe use an adversarial paraphrase detection task to show that BERTScore is more\nrobust to challenging examples when compared to existing metrics.", "published": "2019-04-21 23:08:53", "link": "http://arxiv.org/abs/1904.09675v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "abstract": "In this paper, we introduce UniSent universal sentiment lexica for $1000+$\nlanguages. Sentiment lexica are vital for sentiment analysis in absence of\ndocument-level annotations, a very common scenario for low-resource languages.\nTo the best of our knowledge, UniSent is the largest sentiment resource to date\nin terms of the number of covered languages, including many low resource ones.\nIn this work, we use a massively parallel Bible corpus to project sentiment\ninformation from English to other languages for sentiment analysis on Twitter\ndata. We introduce a method called DomDrift to mitigate the huge domain\nmismatch between Bible and Twitter by a confidence weighting scheme that uses\ndomain-specific embeddings to compare the nearest neighbors for a candidate\nsentiment word in the source (Bible) and target (Twitter) domain. We evaluate\nthe quality of UniSent in a subset of languages for which manually created\nground truth was available, Macedonian, Czech, German, Spanish, and French. We\nshow that the quality of UniSent is comparable to manually created sentiment\nresources when it is used as the sentiment seed for the task of word sentiment\nprediction on top of embedding representations. In addition, we show that\nemoticon sentiments could be reliably predicted in the Twitter domain using\nonly UniSent and monolingual embeddings in German, Spanish, French, and\nItalian. With the publication of this paper, we release the UniSent sentiment\nlexica.", "published": "2019-04-21 23:37:30", "link": "http://arxiv.org/abs/1904.09678v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading\n  Comprehension", "abstract": "Machine reading comprehension tasks require a machine reader to answer\nquestions relevant to the given document. In this paper, we present the first\nfree-form multiple-Choice Chinese machine reading Comprehension dataset (C^3),\ncontaining 13,369 documents (dialogues or more formally written mixed-genre\ntexts) and their associated 19,577 multiple-choice free-form questions\ncollected from Chinese-as-a-second-language examinations.\n  We present a comprehensive analysis of the prior knowledge (i.e., linguistic,\ndomain-specific, and general world knowledge) needed for these real-world\nproblems. We implement rule-based and popular neural methods and find that\nthere is still a significant performance gap between the best performing model\n(68.5%) and human readers (96.0%), especially on problems that require prior\nknowledge. We further study the effects of distractor plausibility and data\naugmentation based on translated relevant datasets for English on model\nperformance. We expect C^3 to present great challenges to existing systems as\nanswering 86.8% of questions requires both knowledge within and beyond the\naccompanying document, and we hope that C^3 can serve as a platform to study\nhow to leverage various kinds of prior knowledge to better understand a given\nwritten or orally oriented text. C^3 is available at https://dataset.org/c3/.", "published": "2019-04-21 23:49:02", "link": "http://arxiv.org/abs/1904.09679v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PullNet: Open Domain Question Answering with Iterative Retrieval on\n  Knowledge Bases and Text", "abstract": "We consider open-domain queston answering (QA) where answers are drawn from\neither a corpus, a knowledge base (KB), or a combination of both of these. We\nfocus on a setting in which a corpus is supplemented with a large but\nincomplete KB, and on questions that require non-trivial (e.g., ``multi-hop'')\nreasoning. We describe PullNet, an integrated framework for (1) learning what\nto retrieve (from the KB and/or corpus) and (2) reasoning with this\nheterogeneous information to find the best answer. PullNet uses an {iterative}\nprocess to construct a question-specific subgraph that contains information\nrelevant to the question. In each iteration, a graph convolutional network\n(graph CNN) is used to identify subgraph nodes that should be expanded using\nretrieval (or ``pull'') operations on the corpus and/or KB. After the subgraph\nis complete, a similar graph CNN is used to extract the answer from the\nsubgraph. This retrieve-and-reason process allows us to answer multi-hop\nquestions using large KBs and corpora. PullNet is weakly supervised, requiring\nquestion-answer pairs but not gold inference paths. Experimentally PullNet\nimproves over the prior state-of-the art, and in the setting where a corpus is\nused with incomplete KB these improvements are often dramatic. PullNet is also\noften superior to prior systems in a KB-only setting or a text-only setting.", "published": "2019-04-21 03:49:09", "link": "http://arxiv.org/abs/1904.09537v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Study on Agreement in PICO Span Annotations", "abstract": "In evidence-based medicine, relevance of medical literature is determined by\npredefined relevance conditions. The conditions are defined based on PICO\nelements, namely, Patient, Intervention, Comparator, and Outcome. Hence, PICO\nannotations in medical literature are essential for automatic relevant document\nfiltering. However, defining boundaries of text spans for PICO elements is not\nstraightforward. In this paper, we study the agreement of PICO annotations made\nby multiple human annotators, including both experts and non-experts.\nAgreements are estimated by a standard span agreement (i.e., matching both\nlabels and boundaries of text spans), and two types of relaxed span agreement\n(i.e., matching labels without guaranteeing matching boundaries of spans).\nBased on the analysis, we report two observations: (i) Boundaries of PICO span\nannotations by individual human annotators are very diverse. (ii) Despite the\ndisagreement in span boundaries, general areas of the span annotations are\nbroadly agreed by annotators. Our results suggest that applying a standard\nagreement alone may undermine the agreement of PICO spans, and adopting both a\nstandard and a relaxed agreements is more suitable for PICO span evaluation.", "published": "2019-04-21 07:30:35", "link": "http://arxiv.org/abs/1904.09557v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Obfuscation for Privacy-preserving Syntactic Parsing", "abstract": "The goal of homomorphic encryption is to encrypt data such that another party\ncan operate on it without being explicitly exposed to the content of the\noriginal data. We introduce an idea for a privacy-preserving transformation on\nnatural language data, inspired by homomorphic encryption. Our primary tool is\n{\\em obfuscation}, relying on the properties of natural language. Specifically,\na given English text is obfuscated using a neural model that aims to preserve\nthe syntactic relationships of the original sentence so that the obfuscated\nsentence can be parsed instead of the original one. The model works at the word\nlevel, and learns to obfuscate each word separately by changing it into a new\nword that has a similar syntactic role. The text obfuscated by our model leads\nto better performance on three syntactic parsers (two dependency and one\nconstituency parsers) in comparison to an upper-bound random substitution\nbaseline. More specifically, the results demonstrate that as more terms are\nobfuscated (by their part of speech), the substitution upper bound\nsignificantly degrades, while the neural model maintains a relatively high\nperforming parser. All of this is done without much sacrifice of privacy\ncompared to the random substitution upper bound. We also further analyze the\nresults, and discover that the substituted words have similar syntactic\nproperties, but different semantic content, compared to the original words.", "published": "2019-04-21 12:09:39", "link": "http://arxiv.org/abs/1904.09585v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "GAN-based Generation and Automatic Selection of Explanations for Neural\n  Networks", "abstract": "One way to interpret trained deep neural networks (DNNs) is by inspecting\ncharacteristics that neurons in the model respond to, such as by iteratively\noptimising the model input (e.g., an image) to maximally activate specific\nneurons. However, this requires a careful selection of hyper-parameters to\ngenerate interpretable examples for each neuron of interest, and current\nmethods rely on a manual, qualitative evaluation of each setting, which is\nprohibitively slow. We introduce a new metric that uses Fr\\'echet Inception\nDistance (FID) to encourage similarity between model activations for real and\ngenerated data. This provides an efficient way to evaluate a set of generated\nexamples for each setting of hyper-parameters. We also propose a novel\nGAN-based method for generating explanations that enables an efficient search\nthrough the input space and imposes a strong prior favouring realistic outputs.\nWe apply our approach to a classification model trained to predict whether a\nmusic audio recording contains singing voice. Our results suggest that this\nproposed metric successfully selects hyper-parameters leading to interpretable\nexamples, avoiding the need for manual evaluation. Moreover, we see that\nexamples synthesised to maximise or minimise the predicted probability of\nsinging voice presence exhibit vocal or non-vocal characteristics,\nrespectively, suggesting that our approach is able to generate suitable\nexplanations for understanding concepts learned by a neural network.", "published": "2019-04-21 02:54:33", "link": "http://arxiv.org/abs/1904.09533v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
