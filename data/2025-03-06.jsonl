{"title": "A Unified Framework with Novel Metrics for Evaluating the Effectiveness of XAI Techniques in LLMs", "abstract": "The increasing complexity of LLMs presents significant challenges to their\ntransparency and interpretability, necessitating the use of eXplainable AI\n(XAI) techniques to enhance trustworthiness and usability. This study\nintroduces a comprehensive evaluation framework with four novel metrics for\nassessing the effectiveness of five XAI techniques across five LLMs and two\ndownstream tasks. We apply this framework to evaluate several XAI techniques\nLIME, SHAP, Integrated Gradients, Layer-wise Relevance Propagation (LRP), and\nAttention Mechanism Visualization (AMV) using the IMDB Movie Reviews and Tweet\nSentiment Extraction datasets. The evaluation focuses on four key metrics:\nHuman-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. Our\nresults show that LIME consistently achieves high scores across multiple LLMs\nand evaluation metrics, while AMV demonstrates superior Robustness and\nnear-perfect Consistency. LRP excels in Contrastivity, particularly with more\ncomplex models. Our findings provide valuable insights into the strengths and\nlimitations of different XAI methods, offering guidance for developing and\nselecting appropriate XAI techniques for LLMs.", "published": "2025-03-06 23:59:50", "link": "http://arxiv.org/abs/2503.05050v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering Datasets", "abstract": "As question answering (QA) systems advance alongside the rapid evolution of\nfoundation models, the need for robust, adaptable, and large-scale evaluation\nbenchmarks becomes increasingly critical. Traditional QA benchmarks are often\nstatic and publicly available, making them susceptible to data contamination\nand memorization by large language models (LLMs). Consequently, static\nbenchmarks may overestimate model generalization and hinder a reliable\nassessment of real-world performance. In this work, we introduce Dynamic-KGQA,\na scalable framework for generating adaptive QA datasets from knowledge graphs\n(KGs), designed to mitigate memorization risks while maintaining statistical\nconsistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generates\na new dataset variant on every run while preserving the underlying\ndistribution, enabling fair and reproducible evaluations. Furthermore, our\nframework provides fine-grained control over dataset characteristics,\nsupporting domain-specific and topic-focused QA dataset generation.\nAdditionally, Dynamic-KGQA produces compact, semantically coherent subgraphs\nthat facilitate both training and evaluation of KGQA models, enhancing their\nability to leverage structured knowledge effectively. To align with existing\nevaluation protocols, we also provide static large-scale train/test/validation\nsplits, ensuring comparability with prior methods. By introducing a dynamic,\ncustomizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous and\nadaptable evaluation of QA systems.", "published": "2025-03-06 23:58:01", "link": "http://arxiv.org/abs/2503.05049v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Biases in Large Language Model-Elicited Text: A Case Study in Natural Language Inference", "abstract": "We test whether NLP datasets created with Large Language Models (LLMs)\ncontain annotation artifacts and social biases like NLP datasets elicited from\ncrowd-source workers. We recreate a portion of the Stanford Natural Language\nInference corpus using GPT-4, Llama-2 70b for Chat, and Mistral 7b Instruct. We\ntrain hypothesis-only classifiers to determine whether LLM-elicited NLI\ndatasets contain annotation artifacts. Next, we use pointwise mutual\ninformation to identify the words in each dataset that are associated with\ngender, race, and age-related terms. On our LLM-generated NLI datasets,\nfine-tuned BERT hypothesis-only classifiers achieve between 86-96% accuracy.\nOur analyses further characterize the annotation artifacts and stereotypical\nbiases in LLM-generated datasets.", "published": "2025-03-06 23:49:30", "link": "http://arxiv.org/abs/2503.05047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning", "abstract": "Automata-conditioned reinforcement learning (RL) has given promising results\nfor learning multi-task policies capable of performing temporally extended\nobjectives given at runtime, done by pretraining and freezing automata\nembeddings prior to training the downstream policy. However, no theoretical\nguarantees were given. This work provides a theoretical framework for the\nautomata-conditioned RL problem and shows that it is probably approximately\ncorrect learnable. We then present a technique for learning provably correct\nautomata embeddings, guaranteeing optimal multi-task policy learning. Our\nexperimental evaluation confirms these theoretical results.", "published": "2025-03-06 23:37:05", "link": "http://arxiv.org/abs/2503.05042v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence", "abstract": "Dense retrieval models are commonly used in Information Retrieval (IR)\napplications, such as Retrieval-Augmented Generation (RAG). Since they often\nserve as the first step in these systems, their robustness is critical to avoid\nfailures. In this work, by repurposing a relation extraction dataset (e.g.\nRe-DocRED), we design controlled experiments to quantify the impact of\nheuristic biases, such as favoring shorter documents, in retrievers like\nDragon+ and Contriever. Our findings reveal significant vulnerabilities:\nretrievers often rely on superficial patterns like over-prioritizing document\nbeginnings, shorter documents, repeated entities, and literal matches.\nAdditionally, they tend to overlook whether the document contains the query's\nanswer, lacking deep semantic understanding. Notably, when multiple biases\ncombine, models exhibit catastrophic performance degradation, selecting the\nanswer-containing document in less than 3% of cases over a biased document\nwithout the answer. Furthermore, we show that these biases have direct\nconsequences for downstream applications like RAG, where retrieval-preferred\ndocuments can mislead LLMs, resulting in a 34% performance drop than not\nproviding any documents at all.", "published": "2025-03-06 23:23:13", "link": "http://arxiv.org/abs/2503.05037v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Early Detection of Mental Health Issues Using Social Media Posts", "abstract": "The increasing prevalence of mental health disorders, such as depression,\nanxiety, and bipolar disorder, calls for immediate need in developing tools for\nearly detection and intervention. Social media platforms, like Reddit,\nrepresent a rich source of user-generated content, reflecting emotional and\nbehavioral patterns. In this work, we propose a multi-modal deep learning\nframework that integrates linguistic and temporal features for early detection\nof mental health crises. Our approach is based on the method that utilizes a\nBiLSTM network both for text and temporal feature analysis, modeling sequential\ndependencies in a different manner, capturing contextual patterns quite well.\nThis work includes a cross-modal attention approach that allows fusion of such\noutputs into context-aware classification of mental health conditions. The\nmodel was then trained and evaluated on a dataset of labeled Reddit posts\npreprocessed using text preprocessing, scaling of temporal features, and\nencoding of labels. Experimental results indicate that the proposed\narchitecture performs better compared to traditional models with a validation\naccuracy of 74.55% and F1-Score of 0.7376. This study presents the importance\nof multi-modal learning for mental health detection and provides a baseline for\nfurther improvements by using more advanced attention mechanisms and other data\nmodalities.", "published": "2025-03-06 23:08:08", "link": "http://arxiv.org/abs/2503.07653v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Continual Pre-training of MoEs: How robust is your router?", "abstract": "Sparsely-activated Mixture of Experts (MoE) transformers are promising\narchitectures for foundation models. Compared to dense transformers that\nrequire the same amount of floating point operations (FLOPs) per forward pass,\nMoEs benefit from improved sample efficiency at training time and achieve much\nstronger performance. Many closed-source and open-source frontier language\nmodels have thus adopted an MoE architecture. Naturally, practitioners will\nwant to extend the capabilities of these models with large amounts of newly\ncollected data without completely re-training them. Prior work has shown that a\nsimple combination of replay and learning rate re-warming and re-decaying can\nenable the continual pre-training (CPT) of dense decoder-only transformers with\nminimal performance degradation compared to full re-training. In the case of\ndecoder-only MoE transformers, however, it is unclear how the routing algorithm\nwill impact continual pre-training performance: 1) do the MoE transformer's\nrouters exacerbate forgetting relative to a dense model?; 2) do the routers\nmaintain a balanced load on previous distributions after CPT?; 3) are the same\nstrategies applied to dense models sufficient to continually pre-train MoE\nLLMs? In what follows, we conduct a large-scale (>2B parameter switch and\nDeepSeek MoE LLMs trained for 600B tokens) empirical study across four MoE\ntransformers to answer these questions. Our results establish a surprising\nrobustness to distribution shifts for both Sinkhorn-Balanced and\nZ-and-Aux-loss-balanced routing algorithms, even in MoEs continually\npre-trained without replay. Moreover, we show that MoE LLMs maintain their\nsample efficiency (relative to a FLOP-matched dense model) during CPT and that\nthey can match the performance of a fully re-trained MoE at a fraction of the\ncost.", "published": "2025-03-06 22:55:01", "link": "http://arxiv.org/abs/2503.05029v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit\nweaknesses in traditional safety alignment, which often relies on rigid refusal\nheuristics or representation engineering to block harmful outputs. While they\nare effective for direct adversarial attacks, they fall short of broader safety\nchallenges requiring nuanced, context-aware decision-making. To address this,\nwe propose Reasoning-enhanced Finetuning for interpretable LLM Safety\n(Rational), a novel framework that trains models to engage in explicit safe\nreasoning before response. Fine-tuned models leverage the extensive pretraining\nknowledge in self-generated reasoning to bootstrap their own safety through\nstructured reasoning, internalizing context-sensitive decision-making. Our\nfindings suggest that safety extends beyond refusal, requiring context\nawareness for more robust, interpretable, and adaptive responses. Reasoning is\nnot only a core capability of LLMs but also a fundamental mechanism for LLM\nsafety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject\nharmful prompts while providing meaningful and context-aware responses in\ncomplex scenarios.", "published": "2025-03-06 22:47:45", "link": "http://arxiv.org/abs/2503.05021v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Leveraging Domain Knowledge at Inference Time for LLM Translation: Retrieval versus Generation", "abstract": "While large language models (LLMs) have been increasingly adopted for machine\ntranslation (MT), their performance for specialist domains such as medicine and\nlaw remains an open challenge. Prior work has shown that LLMs can be\ndomain-adapted at test-time by retrieving targeted few-shot demonstrations or\nterminologies for inclusion in the prompt. Meanwhile, for general-purpose LLM\nMT, recent studies have found some success in generating similarly useful\ndomain knowledge from an LLM itself, prior to translation. Our work studies\ndomain-adapted MT with LLMs through a careful prompting setup, finding that\ndemonstrations consistently outperform terminology, and retrieval consistently\noutperforms generation. We find that generating demonstrations with weaker\nmodels can close the gap with larger model's zero-shot performance. Given the\neffectiveness of demonstrations, we perform detailed analyses to understand\ntheir value. We find that domain-specificity is particularly important, and\nthat the popular multi-domain benchmark is testing adaptation to a particular\nwriting style more so than to a specific domain.", "published": "2025-03-06 22:23:07", "link": "http://arxiv.org/abs/2503.05010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "abstract": "Deploying large language models (LLMs) in real-world applications is often\nhindered by strict computational and latency constraints. While dynamic\ninference offers the flexibility to adjust model behavior based on varying\nresource budgets, existing methods are frequently limited by hardware\ninefficiencies or performance degradation. In this paper, we introduce Balcony,\na simple yet highly effective framework for depth-based dynamic inference. By\nfreezing the pretrained LLM and inserting additional transformer layers at\nselected exit points, Balcony maintains the full model's performance while\nenabling real-time adaptation to different computational budgets. These\nadditional layers are trained using a straightforward self-distillation loss,\naligning the sub-model outputs with those of the full model. This approach\nrequires significantly fewer training tokens and tunable parameters,\ndrastically reducing computational costs compared to prior methods. When\napplied to the LLaMA3-8B model, using only 0.2% of the original pretraining\ndata, Balcony achieves minimal performance degradation while enabling\nsignificant speedups. Remarkably, we show that Balcony outperforms\nstate-of-the-art methods such as Flextron and Layerskip as well as other\nleading compression techniques on multiple models and at various scales, across\na variety of benchmarks.", "published": "2025-03-06 22:09:55", "link": "http://arxiv.org/abs/2503.05005v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HieroLM: Egyptian Hieroglyph Recovery with Next Word Prediction Language Model", "abstract": "Egyptian hieroglyphs are found on numerous ancient Egyptian artifacts, but it\nis common that they are blurry or even missing due to erosion. Existing efforts\nto restore blurry hieroglyphs adopt computer vision techniques such as CNNs and\nmodel hieroglyph recovery as an image classification task, which suffers from\ntwo major limitations: (i) They cannot handle severely damaged or completely\nmissing hieroglyphs. (ii) They make predictions based on a single hieroglyph\nwithout considering contextual and grammatical information. This paper proposes\na novel approach to model hieroglyph recovery as a next word prediction task\nand use language models to address it. We compare the performance of different\nSOTA language models and choose LSTM as the architecture of our HieroLM due to\nthe strong local affinity of semantics in Egyptian hieroglyph texts.\nExperiments show that HieroLM achieves over 44% accuracy and maintains notable\nperformance on multi-shot predictions and scarce data, which makes it a\npragmatic tool to assist scholars in inferring missing hieroglyphs. It can also\ncomplement CV-based models to significantly reduce perplexity in recognizing\nblurry hieroglyphs. Our code is available at\nhttps://github.com/Rick-Cai/HieroLM/.", "published": "2025-03-06 21:53:49", "link": "http://arxiv.org/abs/2503.04996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wanda++: Pruning Large Language Models via Regional Gradients", "abstract": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU.", "published": "2025-03-06 21:42:35", "link": "http://arxiv.org/abs/2503.04992v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DP-GTR: Differentially Private Prompt Protection via Group Text Rewriting", "abstract": "Prompt privacy is crucial, especially when using online large language models\n(LLMs), due to the sensitive information often contained within prompts. While\nLLMs can enhance prompt privacy through text rewriting, existing methods\nprimarily focus on document-level rewriting, neglecting the rich,\nmulti-granular representations of text. This limitation restricts LLM\nutilization to specific tasks, overlooking their generalization and in-context\nlearning capabilities, thus hindering practical application. To address this\ngap, we introduce DP-GTR, a novel three-stage framework that leverages local\ndifferential privacy (DP) and the composition theorem via group text rewriting.\nDP-GTR is the first framework to integrate both document-level and word-level\ninformation while exploiting in-context learning to simultaneously improve\nprivacy and utility, effectively bridging local and global DP mechanisms at the\nindividual data point level. Experiments on CommonSense QA and DocVQA\ndemonstrate that DP-GTR outperforms existing approaches, achieving a superior\nprivacy-utility trade-off. Furthermore, our framework is compatible with\nexisting rewriting techniques, serving as a plug-in to enhance privacy\nprotection. Our code is publicly available at\nhttps://github.com/FatShion-FTD/DP-GTR for reproducibility.", "published": "2025-03-06 21:39:42", "link": "http://arxiv.org/abs/2503.04990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application of integrated gradients explainability to sociopsychological semantic markers", "abstract": "Classification of textual data in terms of sentiment, or more nuanced\nsociopsychological markers (e.g., agency), is now a popular approach commonly\napplied at the sentence level. In this paper, we exploit the integrated\ngradient (IG) method to capture the classification output at the word level,\nrevealing which words actually contribute to the classification process. This\napproach improves explainability and provides in-depth insights into the text.\nWe focus on sociopsychological markers beyond sentiment and investigate how to\neffectively train IG in agency, one of the very few markers for which a\nverified deep learning classifier, BERTAgent, is currently available.\nPerformance and system parameters are carefully tested, alternatives to the IG\napproach are evaluated, and the usefulness of the result is verified in a\nrelevant application scenario. The method is also applied in a scenario where\nonly a small labeled dataset is available, with the aim of exploiting IG to\nidentify the salient words that contribute to building the different classes\nthat relate to relevant sociopsychological markers. To achieve this, an\nuncommon training procedure that encourages overfitting is employed to enhance\nthe distinctiveness of each class. The results are analyzed through the lens of\nsocial psychology, offering valuable insights.", "published": "2025-03-06 21:35:24", "link": "http://arxiv.org/abs/2503.04989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large Vision-Language Model Compression", "abstract": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.", "published": "2025-03-06 21:21:18", "link": "http://arxiv.org/abs/2503.04982v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning", "abstract": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.", "published": "2025-03-06 21:07:41", "link": "http://arxiv.org/abs/2503.04973v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Answer Reranking Strategies in Time-sensitive Question Answering", "abstract": "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.", "published": "2025-03-06 21:06:35", "link": "http://arxiv.org/abs/2503.04972v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL", "abstract": "Recent text-to-SQL systems powered by large language models (LLMs) have\ndemonstrated remarkable performance in translating natural language queries\ninto SQL. However, these systems often struggle with complex database\nstructures and domain-specific queries, as they primarily focus on enhancing\nlogical reasoning and SQL syntax while overlooking the critical need for\ncomprehensive database understanding. To address this limitation, we propose\nDB-Explore, a novel framework that systematically aligns LLMs with database\nknowledge through automated exploration and instruction synthesis. DB-Explore\nconstructs database graphs to capture complex relational schemas, leverages\nGPT-4 to systematically mine structural patterns and semantic knowledge, and\nsynthesizes instructions to distill this knowledge for efficient fine-tuning of\nLLMs. Our framework enables comprehensive database understanding through\ndiverse sampling strategies and automated instruction generation, bridging the\ngap between database structures and language models. Experiments conducted on\nthe SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore,\nachieving an execution accuracy of 52.1% on BIRD and 84.0% on SPIDER. Notably,\nour open-source implementation, based on the Qwen2.5-coder-7B model,\noutperforms multiple GPT-4-driven text-to-SQL systems in comparative\nevaluations, and achieves near state-of-the-art performance with minimal\ncomputational cost.", "published": "2025-03-06 20:46:43", "link": "http://arxiv.org/abs/2503.04959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SafeArena: Evaluating the Safety of Autonomous Web Agents", "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based\ntasks. With this capability comes a greater risk of misuse for malicious\npurposes, such as posting misinformation in an online forum or selling illicit\nsubstances on a website. To evaluate these risks, we propose SafeArena, the\nfirst benchmark to focus on the deliberate misuse of web agents. SafeArena\ncomprises 250 safe and 250 harmful tasks across four websites. We classify the\nharmful tasks into five harm categories -- misinformation, illegal activity,\nharassment, cybercrime, and social bias, designed to assess realistic misuses\nof web agents. We evaluate leading LLM-based web agents, including GPT-4o,\nClaude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To\nsystematically assess their susceptibility to harmful tasks, we introduce the\nAgent Risk Assessment framework that categorizes agent behavior across four\nrisk levels. We find agents are surprisingly compliant with malicious requests,\nwith GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,\nrespectively. Our findings highlight the urgent need for safety alignment\nprocedures for web agents. Our benchmark is available here:\nhttps://safearena.github.io", "published": "2025-03-06 20:43:14", "link": "http://arxiv.org/abs/2503.04957v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems", "abstract": "The proliferation of generative models has presented significant challenges\nin distinguishing authentic human-authored content from deepfake content.\nCollaborative human efforts, augmented by AI tools, present a promising\nsolution. In this study, we explore the potential of DeepFakeDeLiBot, a\ndeliberation-enhancing chatbot, to support groups in detecting deepfake text.\nOur findings reveal that group-based problem-solving significantly improves the\naccuracy of identifying machine-generated paragraphs compared to individual\nefforts. While engagement with DeepFakeDeLiBot does not yield substantial\nperformance gains overall, it enhances group dynamics by fostering greater\nparticipant engagement, consensus building, and the frequency and diversity of\nreasoning-based utterances. Additionally, participants with higher perceived\neffectiveness of group collaboration exhibited performance benefits from\nDeepFakeDeLiBot. These findings underscore the potential of deliberative\nchatbots in fostering interactive and productive group dynamics while ensuring\naccuracy in collaborative deepfake text detection. \\textit{Dataset and source\ncode used in this study will be made publicly available upon acceptance of the\nmanuscript.", "published": "2025-03-06 20:19:38", "link": "http://arxiv.org/abs/2503.04945v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games", "abstract": "In the field of emergent language, efforts have traditionally focused on\ndeveloping communication protocols through interactions between agents in\nreferential games. However, the aspect of internal language learning, where\nlanguage serves not only as a communicative tool with others but also as a\nmeans for individual thinking, self-reflection, and problem-solving remains\nunderexplored. Developing a language through self-play, without another agent's\ninvolvement, poses a unique challenge. It requires an agent to craft symbolic\nrepresentations and train them using direct gradient methods. The challenge\nhere is that if an agent attempts to learn symbolic representations through\nself-play using conventional modeling and techniques such as REINFORCE, the\nsolution will offer no advantage over previous multi-agent approaches. We\nintroduce VQEL, a novel method that incorporates Vector Quantization into the\nagents' architecture, enabling them to autonomously invent and develop discrete\nsymbolic representations in a self-play referential game. Following the\nself-play phase, agents can enhance their language through reinforcement\nlearning and interactions with other agents in the mutual-play phase. Our\nexperiments across various datasets demonstrate that VQEL not only outperforms\nthe traditional REINFORCE method but also benefits from improved control and\nreduced susceptibility to collapse, thanks to the incorporation of vector\nquantization.", "published": "2025-03-06 20:15:51", "link": "http://arxiv.org/abs/2503.04940v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Factorio Learning Environment", "abstract": "Large Language Models (LLMs) are rapidly saturating existing benchmarks,\nnecessitating new open-ended evaluations. We introduce the Factorio Learning\nEnvironment (FLE), based on the game of Factorio, that tests agents in\nlong-term planning, program synthesis, and resource optimization. FLE provides\nexponentially scaling challenges -- from basic automation to complex factories\nprocessing millions of resource units per second. We provide two settings: (1)\nlab-play consisting of eight structured tasks with fixed resources, and (2)\nopen-play with the unbounded task of building the largest factory on an\nprocedurally generated map. We demonstrate across both settings that models\nstill lack strong spatial reasoning. In lab-play, we find that LLMs exhibit\npromising short-horizon skills, yet are unable to operate effectively in\nconstrained environments, reflecting limitations in error analysis. In\nopen-play, while LLMs discover automation strategies that improve growth (e.g\nelectric-powered drilling), they fail to achieve complex automation (e.g\nelectronic-circuit manufacturing).", "published": "2025-03-06 20:13:02", "link": "http://arxiv.org/abs/2503.09617v1", "categories": ["cs.MA", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "HILGEN: Hierarchically-Informed Data Generation for Biomedical NER Using Knowledgebases and Large Language Models", "abstract": "We present HILGEN, a Hierarchically-Informed Data Generation approach that\ncombines domain knowledge from the Unified Medical Language System (UMLS) with\nsynthetic data generated by large language models (LLMs), specifically GPT-3.5.\nOur approach leverages UMLS's hierarchical structure to expand training data\nwith related concepts, while incorporating contextual information from LLMs\nthrough targeted prompts aimed at automatically generating synthetic examples\nfor sparsely occurring named entities. The performance of the HILGEN approach\nwas evaluated across four biomedical NER datasets (MIMIC III, BC5CDR,\nNCBI-Disease, and Med-Mentions) using BERT-Large and DANN (Data Augmentation\nwith Nearest Neighbor Classifier) models, applying various data generation\nstrategies, including UMLS, GPT-3.5, and their best ensemble. For the\nBERT-Large model, incorporating UMLS led to an average F1 score improvement of\n40.36%, while using GPT-3.5 resulted in a comparable average increase of\n40.52%. The Best-Ensemble approach using BERT-Large achieved the highest\nimprovement, with an average increase of 42.29%. DANN model's F1 score improved\nby 22.74% on average using the UMLS-only approach. The GPT-3.5-based method\nresulted in a 21.53% increase, and the Best-Ensemble DANN model showed a more\nnotable improvement, with an average increase of 25.03%. Our proposed HILGEN\napproach improves NER performance in few-shot settings without requiring\nadditional manually annotated data. Our experiments demonstrate that an\neffective strategy for optimizing biomedical NER is to combine biomedical\nknowledge curated in the past, such as the UMLS, and generative LLMs to create\nsynthetic training instances. Our future research will focus on exploring\nadditional innovative synthetic data generation strategies for further\nimproving NER performance.", "published": "2025-03-06 20:02:19", "link": "http://arxiv.org/abs/2503.04930v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maximizing Signal in Human-Model Preference Alignment", "abstract": "The emergence of powerful LLMs has led to a paradigm shift in Natural\nLanguage Understanding and Natural Language Generation. The properties that\nmake LLMs so valuable for these tasks -- creativity, ability to produce fluent\nspeech, and ability to quickly and effectively abstract information from large\ncorpora -- also present new challenges to evaluating their outputs. The rush to\nmarket has led teams to fall back on quick, cost-effective automatic\nevaluations which offer value, but do not obviate the need for human judgments\nin model training and evaluation. This paper argues that in cases in which end\nusers need to agree with the decisions made by ML models -- e.g. in toxicity\ndetection or extraction of main points for summarization -- models should be\ntrained and evaluated on data that represent the preferences of those users. We\nsupport this argument by explicating the role of human feedback in labeling and\njudgment tasks for model training and evaluation. First, we propose methods for\ndisentangling noise from signal in labeling tasks. Then we show that noise in\nlabeling disagreement can be minimized by adhering to proven methodological\nbest practices, while signal can be maximized to play an integral role in model\ntraining and evaluation tasks. Finally, we illustrate best practices by\nproviding a case study in which two guardrails classifiers are evaluated using\nhuman judgments to align final model behavior to user preferences. We aim for\nthis paper to provide researchers and professionals with guidelines to\nintegrating human judgments into their ML and generative AI evaluation toolkit,\nparticularly when working toward achieving accurate and unbiased features that\nalign with users' needs and expectations.", "published": "2025-03-06 19:10:57", "link": "http://arxiv.org/abs/2503.04910v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling", "abstract": "We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.", "published": "2025-03-06 18:59:48", "link": "http://arxiv.org/abs/2503.04725v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM", "abstract": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .", "published": "2025-03-06 18:59:38", "link": "http://arxiv.org/abs/2503.04724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shifting Long-Context LLMs Research from Input to Output", "abstract": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.", "published": "2025-03-06 18:59:37", "link": "http://arxiv.org/abs/2503.04723v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enough Coin Flips Can Make LLMs Act Bayesian", "abstract": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.", "published": "2025-03-06 18:59:23", "link": "http://arxiv.org/abs/2503.04722v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities", "abstract": "Spoken dialogue modeling introduces unique challenges beyond text-based\nlanguage modeling, demanding robust turn-taking, backchanneling, and real-time\ninteraction. Although most Spoken Dialogue Models (SDMs) rely on half-duplex\nprocessing (handling speech one turn at a time), emerging full-duplex SDMs can\nlisten and speak simultaneously, enabling more natural and engaging\nconversations. However, current evaluations of such models remain limited,\noften focusing on turn-based metrics or high-level corpus analyses (e.g., turn\ngaps, pauses). To address this gap, we present Full-Duplex-Bench, a new\nbenchmark that systematically evaluates key conversational behaviors: pause\nhandling, backchanneling, turn-taking, and interruption management. Our\nframework uses automatic metrics for consistent and reproducible assessments of\nSDMs' interactive performance. By offering an open and standardized evaluation\nbenchmark, we aim to advance spoken dialogue modeling and encourage the\ndevelopment of more interactive and natural dialogue systems.", "published": "2025-03-06 18:59:16", "link": "http://arxiv.org/abs/2503.04721v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scaling Rich Style-Prompted Text-to-Speech Datasets", "abstract": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\ndataset that annotates speech utterances with rich style captions. While rich\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\nembedders, classifiers and an audio language model to automatically scale rich\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\ntags, including both speaker-level intrinsic tags and utterance-level\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\n(+15.5% Naturalness MOS) over the best performing baseline that combines\nexisting rich style tag datasets. We ablate several of our dataset design\nchoices to lay the foundation for future work in this space. Our dataset,\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .", "published": "2025-03-06 18:57:40", "link": "http://arxiv.org/abs/2503.04713v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning", "abstract": "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps://www.cmu-l3.github.io/l1", "published": "2025-03-06 18:43:29", "link": "http://arxiv.org/abs/2503.04697v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets", "abstract": "Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark.", "published": "2025-03-06 18:40:00", "link": "http://arxiv.org/abs/2503.04693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases", "abstract": "Recent advancements in reasoning-enhanced large language models (LLMs), such\nas DeepSeek-R1 and OpenAI-o3, have demonstrated significant progress. However,\ntheir application in professional medical contexts remains underexplored,\nparticularly in evaluating the quality of their reasoning processes alongside\nfinal outputs. Here, we introduce MedR-Bench, a benchmarking dataset of 1,453\nstructured patient cases, annotated with reasoning references derived from\nclinical case reports. Spanning 13 body systems and 10 specialties, it includes\nboth common and rare diseases. To comprehensively evaluate LLM performance, we\npropose a framework encompassing three critical examination recommendation,\ndiagnostic decision-making, and treatment planning, simulating the entire\npatient care journey. To assess reasoning quality, we present the Reasoning\nEvaluator, a novel automated system that objectively scores free-text reasoning\nresponses based on efficiency, actuality, and completeness using dynamic\ncross-referencing and evidence checks. Using this benchmark, we evaluate five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nGemini-2.0-Flash Thinking, etc. Our results show that current LLMs achieve over\n85% accuracy in relatively simple diagnostic tasks when provided with\nsufficient examination results. However, performance declines in more complex\ntasks, such as examination recommendation and treatment planning. While\nreasoning outputs are generally reliable, with factuality scores exceeding 90%,\ncritical reasoning steps are frequently missed. These findings underscore both\nthe progress and limitations of clinical LLMs. Notably, open-source models like\nDeepSeek-R1 are narrowing the gap with proprietary systems, highlighting their\npotential to drive accessible and equitable advancements in healthcare.", "published": "2025-03-06 18:35:39", "link": "http://arxiv.org/abs/2503.04691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module", "abstract": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples.", "published": "2025-03-06 18:27:41", "link": "http://arxiv.org/abs/2503.04685v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue", "abstract": "Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase.", "published": "2025-03-06 18:12:33", "link": "http://arxiv.org/abs/2503.04675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Information-theoretic Multi-task Representation Learning Framework for Natural Language Understanding", "abstract": "This paper proposes a new principled multi-task representation learning\nframework (InfoMTL) to extract noise-invariant sufficient representations for\nall tasks. It ensures sufficiency of shared representations for all tasks and\nmitigates the negative effect of redundant features, which can enhance language\nunderstanding of pre-trained language models (PLMs) under the multi-task\nparadigm. Firstly, a shared information maximization principle is proposed to\nlearn more sufficient shared representations for all target tasks. It can avoid\nthe insufficiency issue arising from representation compression in the\nmulti-task paradigm. Secondly, a task-specific information minimization\nprinciple is designed to mitigate the negative effect of potential redundant\nfeatures in the input for each task. It can compress task-irrelevant redundant\ninformation and preserve necessary information relevant to the target for\nmulti-task prediction. Experiments on six classification benchmarks show that\nour method outperforms 12 comparative multi-task methods under the same\nmulti-task settings, especially in data-constrained and noisy scenarios.\nExtensive experiments demonstrate that the learned representations are more\nsufficient, data-efficient, and robust.", "published": "2025-03-06 17:59:51", "link": "http://arxiv.org/abs/2503.04667v1", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding", "published": "2025-03-06 17:33:01", "link": "http://arxiv.org/abs/2503.04647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval", "abstract": "We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development.", "published": "2025-03-06 17:32:22", "link": "http://arxiv.org/abs/2503.04644v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking", "abstract": "As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.", "published": "2025-03-06 17:24:06", "link": "http://arxiv.org/abs/2503.04636v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing", "abstract": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.", "published": "2025-03-06 17:15:48", "link": "http://arxiv.org/abs/2503.04629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "START: Self-taught Reasoner with Tools", "abstract": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.", "published": "2025-03-06 17:11:51", "link": "http://arxiv.org/abs/2503.04625v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling", "abstract": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.", "published": "2025-03-06 17:05:33", "link": "http://arxiv.org/abs/2503.04619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Process Supervision with Bi-directional Rewarding Signals", "abstract": "Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.", "published": "2025-03-06 17:03:17", "link": "http://arxiv.org/abs/2503.04618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!", "abstract": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.", "published": "2025-03-06 16:59:18", "link": "http://arxiv.org/abs/2503.04615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning", "abstract": "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes.", "published": "2025-03-06 16:57:26", "link": "http://arxiv.org/abs/2503.04611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation", "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Kling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.", "published": "2025-03-06 16:53:14", "link": "http://arxiv.org/abs/2503.04606v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Architecture for a Trustworthy Quantum Chatbot", "abstract": "Large language model (LLM)-based tools such as ChatGPT seem useful for\nclassical programming assignments. The more specialized the field, the more\nlikely they lack reliability because of the lack of data to train them. In the\ncase of quantum computing, the quality of answers of generic chatbots is low.\n  C4Q is a chatbot focused on quantum programs that addresses this challenge\nthrough a software architecture that integrates specialized LLMs to classify\nrequests and specialized question answering modules with a deterministic\nlogical engine to provide trustworthy quantum computing support. This article\ndescribes the latest version (2.0) of C4Q, which delivers several enhancements:\nready-to-run Qiskit code for gate definitions and circuit operations, expanded\nfeatures to solve software engineering tasks such as the travelling salesperson\nproblem and the knapsack problem, and a feedback mechanism for iterative\nimprovement.\n  Extensive testing of the backend confirms the system's reliability, while\nempirical evaluations show that C4Q 2.0's classification LLM reaches\nnear-perfect accuracy. The evaluation of the result consists in a comparative\nstudy with three existing chatbots highlighting C4Q 2.0's maintainability and\ncorrectness, reflecting on how software architecture decisions, such as\nseparating deterministic logic from probabilistic text generation impact the\nquality of the results.", "published": "2025-03-06 16:43:23", "link": "http://arxiv.org/abs/2503.04875v1", "categories": ["cs.CL", "quant-ph", "81P68, 68T20"], "primary_category": "cs.CL"}
{"title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization", "abstract": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.", "published": "2025-03-06 16:40:48", "link": "http://arxiv.org/abs/2503.04598v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks", "abstract": "Generative Large Language Models (LLMs) have shown promising results in text\nannotation using zero-shot and few-shot learning. Yet these approaches do not\nallow the model to retain information from previous annotations, making each\nresponse independent from the preceding ones. This raises the question of\nwhether model memory -- the LLM having knowledge about its own previous\nannotations in the same task -- affects performance. In this article, using\nOpenAI's GPT-4o and Meta's Llama 3.1 on two political science datasets, we\ndemonstrate that allowing the model to retain information about its own\nprevious classifications yields significant performance improvements: between 5\nand 25\\% when compared to zero-shot and few-shot learning. Moreover, memory\nreinforcement, a novel approach we propose that combines model memory and\nreinforcement learning, yields additional performance gains in three out of our\nfour tests. These findings have important implications for applied researchers\nlooking to improve performance and efficiency in LLM annotation tasks.", "published": "2025-03-06 16:39:18", "link": "http://arxiv.org/abs/2503.04874v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Good In-context Learners for Financial Sentiment Analysis?", "abstract": "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.", "published": "2025-03-06 16:38:12", "link": "http://arxiv.org/abs/2503.04873v1", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation", "abstract": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.", "published": "2025-03-06 16:25:53", "link": "http://arxiv.org/abs/2503.04872v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional Causal Reasoning Evaluation in Language Models", "abstract": "Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.", "published": "2025-03-06 15:47:19", "link": "http://arxiv.org/abs/2503.04556v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation", "abstract": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation", "published": "2025-03-06 15:37:31", "link": "http://arxiv.org/abs/2503.04554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models", "abstract": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.", "published": "2025-03-06 15:34:27", "link": "http://arxiv.org/abs/2503.04548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model", "abstract": "Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.", "published": "2025-03-06 15:29:13", "link": "http://arxiv.org/abs/2503.04543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Label Distribution Learning-Enhanced Dual-KNN for Text Classification", "abstract": "Many text classification methods usually introduce external information\n(e.g., label descriptions and knowledge bases) to improve the classification\nperformance. Compared to external information, some internal information\ngenerated by the model itself during training, like text embeddings and\npredicted label probability distributions, are exploited poorly when predicting\nthe outcomes of some texts. In this paper, we focus on leveraging this internal\ninformation, proposing a dual $k$ nearest neighbor (D$k$NN) framework with two\n$k$NN modules, to retrieve several neighbors from the training set and augment\nthe distribution of labels. For the $k$NN module, it is easily confused and may\ncause incorrect predictions when retrieving some nearest neighbors from noisy\ndatasets (datasets with labeling errors) or similar datasets (datasets with\nsimilar labels). To address this issue, we also introduce a label distribution\nlearning module that can learn label similarity, and generate a better label\ndistribution to help models distinguish texts more effectively. This module\neases model overfitting and improves final classification performance, hence\nenhancing the quality of the retrieved neighbors by $k$NN modules during\ninference. Extensive experiments on the benchmark datasets verify the\neffectiveness of our method.", "published": "2025-03-06 15:15:26", "link": "http://arxiv.org/abs/2503.04869v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Bioinformatics: A Survey", "abstract": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.", "published": "2025-03-06 14:38:20", "link": "http://arxiv.org/abs/2503.04490v1", "categories": ["cs.CL", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "Generalized Interpolating Discrete Diffusion", "abstract": "While state-of-the-art language models achieve impressive results through\nnext-token prediction, they have inherent limitations such as the inability to\nrevise already generated tokens. This has prompted exploration of alternative\napproaches such as discrete diffusion. However, masked diffusion, which has\nemerged as a popular choice due to its simplicity and effectiveness,\nreintroduces this inability to revise words. To overcome this, we generalize\nmasked diffusion and derive the theoretical backbone of a family of general\ninterpolating discrete diffusion (GIDD) processes offering greater flexibility\nin the design of the noising processes. Leveraging a novel diffusion ELBO, we\nachieve compute-matched state-of-the-art performance in diffusion language\nmodeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining\nmasking and uniform noise, leading to improved sample quality and unlocking the\nability for the model to correct its own mistakes, an area where autoregressive\nmodels notoriously have struggled. Our code and models are open-source:\nhttps://github.com/dvruette/gidd/", "published": "2025-03-06 14:30:55", "link": "http://arxiv.org/abs/2503.04482v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual Explanations for Text Classification", "abstract": "The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier.", "published": "2025-03-06 14:15:07", "link": "http://arxiv.org/abs/2503.04463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying patterns of punctuation in modern Chinese prose", "abstract": "Recent research shows that punctuation patterns in texts exhibit universal\nfeatures across languages. Analysis of Western classical literature reveals\nthat the distribution of spaces between punctuation marks aligns with a\ndiscrete Weibull distribution, typically used in survival analysis. By\nextending this analysis to Chinese literature represented here by three notable\ncontemporary works, it is shown that Zipf's law applies to Chinese texts\nsimilarly to Western texts, where punctuation patterns also improve adherence\nto the law. Additionally, the distance distribution between punctuation marks\nin Chinese texts follows the Weibull model, though larger spacing is less\nfrequent than in English translations. Sentence-ending punctuation,\nrepresenting sentence length, diverges more from this pattern, reflecting\ngreater flexibility in sentence length. This variability supports the formation\nof complex, multifractal sentence structures, particularly evident in Gao\nXingjian's \"Soul Mountain\". These findings demonstrate that both Chinese and\nWestern texts share universal punctuation and word distribution patterns,\nunderscoring their broad applicability across languages.", "published": "2025-03-06 14:04:30", "link": "http://arxiv.org/abs/2503.04449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Analysing News Framing in Chinese Media", "abstract": "Framing is an essential device in news reporting, allowing the writer to\ninfluence public perceptions of current affairs. While there are existing\nautomatic news framing detection datasets in various languages, none of them\nfocus on news framing in the Chinese language which has complex character\nmeanings and unique linguistic features. This study introduces the first\nChinese News Framing dataset, to be used as either a stand-alone dataset or a\nsupplementary resource to the SemEval-2023 task 3 dataset. We detail its\ncreation and we run baseline experiments to highlight the need for such a\ndataset and create benchmarks for future research, providing results obtained\nthrough fine-tuning XLM-RoBERTa-Base and using GPT-4o in the zero-shot setting.\nWe find that GPT-4o performs significantly worse than fine-tuned XLM-RoBERTa\nacross all languages. For the Chinese language, we obtain an F1-micro (the\nperformance metric for SemEval task 3, subtask 2) score of 0.719 using only\nsamples from our Chinese News Framing dataset and a score of 0.753 when we\naugment the SemEval dataset with Chinese news framing samples. With positive\nnews frame detection results, this dataset is a valuable resource for detecting\nnews frames in the Chinese language and is a valuable supplement to the\nSemEval-2023 task 3 dataset.", "published": "2025-03-06 13:55:33", "link": "http://arxiv.org/abs/2503.04439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Othello World Model Hypothesis", "abstract": "Li et al. (2023) used the Othello board game as a test case for the ability\nof GPT-2 to induce world models, and were followed up by Nanda et al. (2023b).\nWe briefly discuss the original experiments, expanding them to include more\nlanguage models with more comprehensive probing. Specifically, we analyze\nsequences of Othello board states and train the model to predict the next move\nbased on previous moves. We evaluate seven language models (GPT-2, T5, Bart,\nFlan-T5, Mistral, LLaMA-2, and Qwen2.5) on the Othello task and conclude that\nthese models not only learn to play Othello, but also induce the Othello board\nlayout. We find that all models achieve up to 99% accuracy in unsupervised\ngrounding and exhibit high similarity in the board features they learned. This\nprovides considerably stronger evidence for the Othello World Model Hypothesis\nthan previous works.", "published": "2025-03-06 13:26:58", "link": "http://arxiv.org/abs/2503.04421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Predict Antimicrobial Resistance Gene?", "abstract": "This study demonstrates that generative large language models can be utilized\nin a more flexible manner for DNA sequence analysis and classification tasks\ncompared to traditional transformer encoder-based models. While recent\nencoder-based models such as DNABERT and Nucleotide Transformer have shown\nsignificant performance in DNA sequence classification, transformer\ndecoder-based generative models have not yet been extensively explored in this\nfield. This study evaluates how effectively generative Large Language Models\nhandle DNA sequences with various labels and analyzes performance changes when\nadditional textual information is provided. Experiments were conducted on\nantimicrobial resistance genes, and the results show that generative Large\nLanguage Models can offer comparable or potentially better predictions,\ndemonstrating flexibility and accuracy when incorporating both sequence and\ntextual information. The code and data used in this work are available at the\nfollowing GitHub repository: https://github.com/biocomgit/llm4dna.", "published": "2025-03-06 13:10:57", "link": "http://arxiv.org/abs/2503.04413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Zero-Shot Cross-Lingual Transfer for Bodo POS and NER Tagging Using Gemini 2.0 Flash Thinking Experimental Model", "abstract": "Named Entity Recognition (NER) and Part-of-Speech (POS) tagging are critical\ntasks for Natural Language Processing (NLP), yet their availability for\nlow-resource languages (LRLs) like Bodo remains limited. This article presents\na comparative empirical study investigating the effectiveness of Google's\nGemini 2.0 Flash Thinking Experiment model for zero-shot cross-lingual transfer\nof POS and NER tagging to Bodo. We explore two distinct methodologies: (1)\ndirect translation of English sentences to Bodo followed by tag transfer, and\n(2) prompt-based tag transfer on parallel English-Bodo sentence pairs. Both\nmethods leverage the machine translation and cross-lingual understanding\ncapabilities of Gemini 2.0 Flash Thinking Experiment to project English POS and\nNER annotations onto Bodo text in CONLL-2003 format. Our findings reveal the\ncapabilities and limitations of each approach, demonstrating that while both\nmethods show promise for bootstrapping Bodo NLP, prompt-based transfer exhibits\nsuperior performance, particularly for NER. We provide a detailed analysis of\nthe results, highlighting the impact of translation quality, grammatical\ndivergences, and the inherent challenges of zero-shot cross-lingual transfer.\nThe article concludes by discussing future research directions, emphasizing the\nneed for hybrid approaches, few-shot fine-tuning, and the development of\ndedicated Bodo NLP resources to achieve high-accuracy POS and NER tagging for\nthis low-resource language.", "published": "2025-03-06 12:59:11", "link": "http://arxiv.org/abs/2503.04405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models", "abstract": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.", "published": "2025-03-06 12:50:14", "link": "http://arxiv.org/abs/2503.04396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication", "abstract": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction.", "published": "2025-03-06 12:47:54", "link": "http://arxiv.org/abs/2503.04395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG", "abstract": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .", "published": "2025-03-06 12:38:17", "link": "http://arxiv.org/abs/2503.04388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge", "abstract": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT.", "published": "2025-03-06 12:33:20", "link": "http://arxiv.org/abs/2503.04381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks", "abstract": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicated Feedback and Edit Models that are capable of performing\ninference-time scaling for open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo\ncan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.", "published": "2025-03-06 12:30:24", "link": "http://arxiv.org/abs/2503.04378v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assumed Identities: Quantifying Gender Bias in Machine Translation of Ambiguous Occupational Terms", "abstract": "Machine Translation (MT) systems frequently encounter ambiguous scenarios\nwhere they must assign gender to certain occupations when translating without\nexplicit guidance or contextual cues. While individual translations in such\ncases may not be inherently biased, systematic patterns-such as the repeated\nassociation of certain professions with specific genders-can emerge, reflecting\nand perpetuating societal stereotypes. This ambiguity challenges traditional\ninstance-level single-answer evaluation approaches, as no single gold standard\ntranslation exists. To address this, we propose an approach that evaluates\ngender bias through aggregated model responses. Specifically, we introduce a\nmethodology to detect gender imbalances between source texts and translations,\na benchmarking dataset with ambiguous English inputs, and probability-based\nmetrics to quantify a model's divergence from normative standards or reference\ndistributions.", "published": "2025-03-06 12:16:14", "link": "http://arxiv.org/abs/2503.04372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Literalism: How Supervised Training Shapes Translationese in LLMs", "abstract": "Large language models (LLMs) have achieved remarkable success in machine\ntranslation, demonstrating impressive performance across diverse languages.\nHowever, translationese, characterized by overly literal and unnatural\ntranslations, remains a persistent challenge in LLM-based translation systems.\nDespite their pre-training on vast corpora of natural utterances, LLMs exhibit\ntranslationese errors and generate unexpected unnatural translations, stemming\nfrom biases introduced during supervised fine-tuning (SFT). In this work, we\nsystematically evaluate the prevalence of translationese in LLM-generated\ntranslations and investigate its roots during supervised training. We introduce\nmethods to mitigate these biases, including polishing golden references and\nfiltering unnatural training instances. Empirical evaluations demonstrate that\nthese approaches significantly reduce translationese while improving\ntranslation naturalness, validated by human evaluations and automatic metrics.\nOur findings highlight the need for training-aware adjustments to optimize LLM\ntranslation outputs, paving the way for more fluent and\ntarget-language-consistent translations. We release the data and code at\nhttps://github.com/yafuly/LLM_Translationese.", "published": "2025-03-06 12:14:45", "link": "http://arxiv.org/abs/2503.04369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Multilingual NLG Evaluation Abilities of LLM-Based Evaluators", "abstract": "Previous research has shown that LLMs have potential in multilingual NLG\nevaluation tasks. However, existing research has not fully explored the\ndifferences in the evaluation capabilities of LLMs across different languages.\nTo this end, this study provides a comprehensive analysis of the multilingual\nevaluation performance of 10 recent LLMs, spanning high-resource and\nlow-resource languages through correlation analysis, perturbation attacks, and\nfine-tuning. We found that 1) excluding the reference answer from the prompt\nand using large-parameter LLM-based evaluators leads to better performance\nacross various languages; 2) most LLM-based evaluators show a higher\ncorrelation with human judgments in high-resource languages than in\nlow-resource languages; 3) in the languages where they are most sensitive to\nsuch attacks, they also tend to exhibit the highest correlation with human\njudgments; and 4) fine-tuning with data from a particular language yields a\nbroadly consistent enhancement in the model's evaluation performance across\ndiverse languages. Our findings highlight the imbalance in LLMs'evaluation\ncapabilities across different languages and suggest that low-resource language\nscenarios deserve more attention.", "published": "2025-03-06 12:04:29", "link": "http://arxiv.org/abs/2503.04360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer-Specific Scaling of Positional Encodings for Superior Long-Context Modeling", "abstract": "Although large language models (LLMs) have achieved significant progress in\nhandling long-context inputs, they still suffer from the ``lost-in-the-middle''\nproblem, where crucial information in the middle of the context is often\nunderrepresented or lost. Our extensive experiments reveal that this issue may\narise from the rapid long-term decay in Rotary Position Embedding (RoPE). To\naddress this problem, we propose a layer-specific positional encoding scaling\nmethod that assigns distinct scaling factors to each layer, slowing down the\ndecay rate caused by RoPE to make the model pay more attention to the middle\ncontext. A specially designed genetic algorithm is employed to efficiently\nselect the optimal scaling factors for each layer by incorporating Bezier\ncurves to reduce the search space. Through comprehensive experimentation, we\ndemonstrate that our method significantly alleviates the ``lost-in-the-middle''\nproblem. Our approach results in an average accuracy improvement of up to 20%\non the Key-Value Retrieval dataset. Furthermore, we show that layer-specific\ninterpolation, as opposed to uniform interpolation across all layers, enhances\nthe model's extrapolation capabilities when combined with PI and Dynamic-NTK\npositional encoding schemes.", "published": "2025-03-06 11:59:55", "link": "http://arxiv.org/abs/2503.04355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adding Alignment Control to Language Models", "abstract": "Post-training alignment has increasingly become a crucial factor in enhancing\nthe usability of language models (LMs). However, the strength of alignment\nvaries depending on individual preferences. This paper proposes a method to\nincorporate alignment control into a single model, referred to as CLM. This\napproach adds one identity layer preceding the initial layers and performs\npreference learning only on this layer to map unaligned input token embeddings\ninto the aligned space. Experimental results demonstrate that this efficient\nfine-tuning method performs comparable to full fine-tuning. During inference,\nthe input embeddings are processed through the aligned and unaligned layers,\nwhich are then merged through the interpolation coefficient. By controlling\nthis parameter, the alignment exhibits a clear interpolation and extrapolation\nphenomenon.", "published": "2025-03-06 11:42:03", "link": "http://arxiv.org/abs/2503.04346v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-depth Analysis of Graph-based RAG in a Unified Framework", "abstract": "Graph-based Retrieval-Augmented Generation (RAG) has proven effective in\nintegrating external knowledge into large language models (LLMs), improving\ntheir factual accuracy, adaptability, interpretability, and trustworthiness. A\nnumber of graph-based RAG methods have been proposed in the literature.\nHowever, these methods have not been systematically and comprehensively\ncompared under the same experimental settings. In this paper, we first\nsummarize a unified framework to incorporate all graph-based RAG methods from a\nhigh-level perspective. We then extensively compare representative graph-based\nRAG methods over a range of questing-answering (QA) datasets -- from specific\nquestions to abstract questions -- and examine the effectiveness of all\nmethods, providing a thorough analysis of graph-based RAG approaches. As a\nbyproduct of our experimental analysis, we are also able to identify new\nvariants of the graph-based RAG methods over specific QA and abstract QA tasks\nrespectively, by combining existing techniques, which outperform the\nstate-of-the-art methods. Finally, based on these findings, we offer promising\nresearch opportunities. We believe that a deeper understanding of the behavior\nof existing methods can provide new valuable insights for future research.", "published": "2025-03-06 11:34:49", "link": "http://arxiv.org/abs/2503.04338v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Solving Word-Sense Disambiguation and Word-Sense Induction with Dictionary Examples", "abstract": "Many less-resourced languages struggle with a lack of large, task-specific\ndatasets that are required for solving relevant tasks with modern\ntransformer-based large language models (LLMs). On the other hand, many\nlinguistic resources, such as dictionaries, are rarely used in this context\ndespite their large information contents. We show how LLMs can be used to\nextend existing language resources in less-resourced languages for two\nimportant tasks: word-sense disambiguation (WSD) and word-sense induction\n(WSI). We approach the two tasks through the related but much more accessible\nword-in-context (WiC) task where, given a pair of sentences and a target word,\na classification model is tasked with predicting whether the sense of a given\nword differs between sentences. We demonstrate that a well-trained model for\nthis task can distinguish between different word senses and can be adapted to\nsolve the WSD and WSI tasks. The advantage of using the WiC task, instead of\ndirectly predicting senses, is that the WiC task does not need pre-constructed\nsense inventories with a sufficient number of examples for each sense, which\nare rarely available in less-resourced languages. We show that sentence pairs\nfor the WiC task can be successfully generated from dictionary examples using\nLLMs. The resulting prediction models outperform existing models on WiC, WSD,\nand WSI tasks. We demonstrate our methodology on the Slovene language, where a\nmonolingual dictionary is available, but word-sense resources are tiny.", "published": "2025-03-06 11:27:55", "link": "http://arxiv.org/abs/2503.04328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computational Law: Datasets, Benchmarks, and Ontologies", "abstract": "Recent developments in computer science and artificial intelligence have also\ncontributed to the legal domain, as revealed by the number and range of related\npublications and applications. Machine and deep learning models require\nconsiderable amount of domain-specific data for training and comparison\npurposes, in order to attain high-performance in the legal domain.\nAdditionally, semantic resources such as ontologies are valuable for building\nlarge-scale computational legal systems, in addition to ensuring\ninteroperability of such systems. Considering these aspects, we present an\nup-to-date review of the literature on datasets, benchmarks, and ontologies\nproposed for computational law. We believe that this comprehensive and recent\nreview will help researchers and practitioners when developing and testing\napproaches and systems for computational law.", "published": "2025-03-06 10:46:15", "link": "http://arxiv.org/abs/2503.04305v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual-Class Prompt Generation: Enhancing Indonesian Gender-Based Hate Speech Detection through Data Augmentation", "abstract": "Detecting gender-based hate speech in Indonesian social media remains\nchallenging due to limited labeled datasets. While binary hate speech\nclassification has advanced, a more granular category like gender-targeted hate\nspeech is understudied because of class imbalance issues. This paper addresses\nthis gap by comparing three data augmentation techniques for Indonesian\ngender-based hate speech detection. We evaluate backtranslation, single-class\nprompt generation (using only hate speech examples), and our proposed\ndual-class prompt generation (using both hate speech and non-hate speech\nexamples). Experiments show all augmentation methods improve classification\nperformance, with our dual-class approach achieving the best results (88.5%\naccuracy, 88.1% F1-score using Random Forest). Semantic similarity analysis\nreveals dual-class prompt generation produces the most novel content, while\nT-SNE visualizations confirm these samples occupy distinct feature space\nregions while maintaining class characteristics. Our findings suggest that\nincorporating examples from both classes helps language models generate more\ndiverse yet representative samples, effectively addressing limited data\nchallenges in specialized hate speech detection.", "published": "2025-03-06 10:07:51", "link": "http://arxiv.org/abs/2503.04279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Fact and Frequency: LLM Responses to Misinformation Expressed with Uncertainty", "abstract": "We study LLM judgments of misinformation expressed with uncertainty. Our\nexperiments study the response of three widely used LLMs (GPT-4o, LlaMA3,\nDeepSeek-v2) to misinformation propositions that have been verified false and\nthen are transformed into uncertain statements according to an uncertainty\ntypology. Our results show that after transformation, LLMs change their\nfactchecking classification from false to not-false in 25% of the cases.\nAnalysis reveals that the change cannot be explained by predictors to which\nhumans are expected to be sensitive, i.e., modality, linguistic cues, or\nargumentation strategy. The exception is doxastic transformations, which use\nlinguistic cue phrases such as \"It is believed ...\".To gain further insight, we\nprompt the LLM to make another judgment about the transformed misinformation\nstatements that is not related to truth value. Specifically, we study LLM\nestimates of the frequency with which people make the uncertain statement. We\nfind a small but significant correlation between judgment of fact and\nestimation of frequency.", "published": "2025-03-06 10:02:25", "link": "http://arxiv.org/abs/2503.04271v1", "categories": ["cs.CL", "cs.CY", "91F20", "I.2.7; I.2.4; K.4; J.4"], "primary_category": "cs.CL"}
{"title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models", "abstract": "Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B.", "published": "2025-03-06 09:21:54", "link": "http://arxiv.org/abs/2503.04240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tgea: An error-annotated dataset and benchmark tasks for text generation from pretrained language models", "abstract": "In order to deeply understand the capability of pretrained language models in\ntext generation and conduct a diagnostic evaluation, we propose TGEA, an\nerror-annotated dataset with multiple benchmark tasks for text generation from\npretrained language models (PLMs). We use carefully selected prompt words to\nguide GPT-2 to generate candidate sentences, from which we select 47K for error\nannotation. Crowdsourced workers manually check each of these sentences and\ndetect 12k erroneous sentences. We create an error taxonomy to cover 24 types\nof errors occurring in these erroneous sentences according to the nature of\nerrors with respect to linguistics and knowledge (eg, common sense). For each\nerroneous span in PLM-generated sentences, we also detect another span that is\nclosely associated with it. Each error is hence manually labeled with\ncomprehensive annotations, including the span of the error, the associated\nspan, minimal correction to the error, the type of the error, and rationale\nbehind the error. Apart from the fully annotated dataset, we also present a\ndetailed description of the data collection procedure, statistics and analysis\nof the dataset. This is the first dataset with comprehensive annotations for\nPLM-generated texts, which facilitates the diagnostic evaluation of PLM-based\ntext generation. Furthermore, we use TGEA as a benchmark dataset and propose a\nseries of automatic diagnosis tasks, including error detection, error type\nclassification, associated span detection, error rationale generation, to\nfurther promote future study on the automatic error detection and correction on\ntexts generated by pretrained language models.", "published": "2025-03-06 09:14:02", "link": "http://arxiv.org/abs/2503.04232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion", "abstract": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed\nby integrating the strengths of heterogeneous source LLMs into more compact\ntarget LLMs. Our source models include the powerful Gemma-2-27B-it,\nMistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct.\nFor target models, we focus on three widely-used smaller\nvariants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along\nwith two ultra-compact options, Llama-3.2-3B-Instruct and\nLlama-3.2-1B-Instruct. To leverage the diverse capabilities of these source\nmodels, we develop a specialized data construction protocol tailored to various\ntasks and domains. The FuseChat-3.0 training pipeline consists of two key\nstages: (1) supervised fine-tuning (SFT) to align the target and source model\ndistributions, and (2) Direct Preference Optimization (DPO) to apply\npreferences from multiple source LLMs to fine-tune the target model. The\nresulting FuseChat-3.0 models exhibit significant performance gains across\ntasks such as instruction following, general knowledge, mathematics, and\ncoding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target\nmodel, our fusion approach achieves an average improvement of 6.8 points across\n14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and\n30.1 points on the instruction-following benchmarks AlpacaEval-2 and\nArena-Hard, respectively. Our code, models, and datasets are available at\nhttps://github.com/SLIT-AI/FuseChat-3.0.", "published": "2025-03-06 09:03:36", "link": "http://arxiv.org/abs/2503.04222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Codebook Reduction and Saturation: Novel observations on Inductive Thematic Saturation for Large Language Models and initial coding in Thematic Analysis", "abstract": "This paper reflects on the process of performing Thematic Analysis with Large\nLanguage Models (LLMs). Specifically, the paper deals with the problem of\nanalytical saturation of initial codes, as produced by LLMs. Thematic Analysis\nis a well-established qualitative analysis method composed of interlinked\nphases. A key phase is the initial coding, where the analysts assign labels to\ndiscrete components of a dataset. Saturation is a way to measure the validity\nof a qualitative analysis and relates to the recurrence and repetition of\ninitial codes. In the paper we reflect on how well LLMs achieve analytical\nsaturation and propose also a novel technique to measure Inductive Thematic\nSaturation (ITS). This novel technique leverages a programming framework called\nDSPy. The proposed novel approach allows a precise measurement of ITS.", "published": "2025-03-06 08:52:03", "link": "http://arxiv.org/abs/2503.04859v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative Approach to Few-shot Multimodal Dialogue Intention Recognition", "abstract": "Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.", "published": "2025-03-06 08:28:44", "link": "http://arxiv.org/abs/2503.04201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring temporal effects of agent knowledge by date-controlled tool use", "abstract": "Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nan improper configuration affects the quality of the agent's responses. Here,\nwe assess the agent behavior using distinct date-controlled tools (DCTs) as\nstress test to measure the knowledge variability of large language model (LLM)\nagents. We demonstrate the temporal effects of an LLM agent as a writing\nassistant, which uses web search to complete scientific publication abstracts.\nWe show that the temporality of search engine translates into tool-dependent\nagent performance but can be alleviated with base model choice and explicit\nreasoning instructions such as chain-of-thought prompting. Our results indicate\nthat agent design and evaluations should take a dynamical view and implement\nmeasures to account for the temporal influence of external resources to ensure\nreliability.", "published": "2025-03-06 08:03:51", "link": "http://arxiv.org/abs/2503.04188v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large-Scale AI in Telecom: Charting the Roadmap for Innovation, Scalability, and Enhanced Digital Experiences", "abstract": "This white paper discusses the role of large-scale AI in the\ntelecommunications industry, with a specific focus on the potential of\ngenerative AI to revolutionize network functions and user experiences,\nespecially in the context of 6G systems. It highlights the development and\ndeployment of Large Telecom Models (LTMs), which are tailored AI models\ndesigned to address the complex challenges faced by modern telecom networks.\nThe paper covers a wide range of topics, from the architecture and deployment\nstrategies of LTMs to their applications in network management, resource\nallocation, and optimization. It also explores the regulatory, ethical, and\nstandardization considerations for LTMs, offering insights into their future\nintegration into telecom infrastructure. The goal is to provide a comprehensive\nroadmap for the adoption of LTMs to enhance scalability, performance, and\nuser-centric innovation in telecom networks.", "published": "2025-03-06 07:53:24", "link": "http://arxiv.org/abs/2503.04184v1", "categories": ["cs.NI", "cs.AI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "TIMER: Temporal Instruction Modeling and Evaluation for Longitudinal Clinical Records", "abstract": "Large language models (LLMs) have emerged as promising tools for assisting in\nmedical tasks, yet processing Electronic Health Records (EHRs) presents unique\nchallenges due to their longitudinal nature. While LLMs' capabilities to\nperform medical tasks continue to improve, their ability to reason over\ntemporal dependencies across multiple patient visits and time frames remains\nunexplored. We introduce TIMER (Temporal Instruction Modeling and Evaluation\nfor Longitudinal Clinical Records), a framework that incorporate\ninstruction-response pairs grounding to different parts of a patient's record\nas a critical dimension in both instruction evaluation and tuning for\nlongitudinal clinical records. We develop TIMER-Bench, the first time-aware\nbenchmark that evaluates temporal reasoning capabilities over longitudinal\nEHRs, as well as TIMER-Instruct, an instruction-tuning methodology for LLMs to\nlearn reasoning over time. We demonstrate that models fine-tuned with\nTIMER-Instruct improve performance by 7.3% on human-generated benchmarks and\n9.2% on TIMER-Bench, indicating that temporal instruction-tuning improves model\nperformance for reasoning over EHR.", "published": "2025-03-06 07:44:17", "link": "http://arxiv.org/abs/2503.04176v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.LG", "68T50, 68T37", "I.2.7; J.3"], "primary_category": "cs.AI"}
{"title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs", "abstract": "Despite extensive safety enhancements in large language models (LLMs),\nmulti-turn \"jailbreak\" conversations crafted by skilled human adversaries can\nstill breach even the most sophisticated guardrails. However, these multi-turn\nattacks demand considerable manual effort, limiting their scalability. In this\nwork, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that\nsystematically converts multi-turn jailbreak prompts into single-turn attacks.\nSpecifically, we propose three conversion strategies - Hyphenize, Numberize,\nand Pythonize - each preserving sequential context yet packaging it in a single\nquery. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show\nthat M2S often increases or maintains high Attack Success Rates (ASRs) compared\nto original multi-turn conversations. Notably, using a StrongREJECT-based\nevaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and\noutperforms original multi-turn prompts by as much as 17.5% in absolute\nimprovement on GPT-4o. Further analysis reveals that certain adversarial\ntactics, when consolidated into a single prompt, exploit structural formatting\ncues to evade standard policy checks. These findings underscore that\nsingle-turn attacks - despite being simpler and cheaper to conduct - can be\njust as potent, if not more, than their multi-turn counterparts. Our findings\nunderscore the urgent need to reevaluate and reinforce LLM safety strategies,\ngiven how adversarial queries can be compacted into a single prompt while still\nretaining sufficient complexity to bypass existing safety measures.", "published": "2025-03-06 07:34:51", "link": "http://arxiv.org/abs/2503.04856v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BPQA Dataset: Evaluating How Well Language Models Leverage Blood Pressures to Answer Biomedical Questions", "abstract": "Clinical measurements such as blood pressures and respiration rates are\ncritical in diagnosing and monitoring patient outcomes. It is an important\ncomponent of biomedical data, which can be used to train transformer-based\nlanguage models (LMs) for improving healthcare delivery. It is, however,\nunclear whether LMs can effectively interpret and use clinical measurements. We\ninvestigate two questions: First, can LMs effectively leverage clinical\nmeasurements to answer related medical questions? Second, how to enhance an\nLM's performance on medical question-answering (QA) tasks that involve\nmeasurements? We performed a case study on blood pressure readings (BPs), a\nvital sign routinely monitored by medical professionals. We evaluated the\nperformance of four LMs: BERT, BioBERT, MedAlpaca, and GPT-3.5, on our newly\ndeveloped dataset, BPQA (Blood Pressure Question Answering). BPQA contains\n$100$ medical QA pairs that were verified by medical students and designed to\nrely on BPs . We found that GPT-3.5 and MedAlpaca (larger and medium sized LMs)\nbenefit more from the inclusion of BPs than BERT and BioBERT (small sized LMs).\nFurther, augmenting measurements with labels improves the performance of\nBioBERT and Medalpaca (domain specific LMs), suggesting that retrieval may be\nuseful for improving domain-specific LMs.", "published": "2025-03-06 07:06:46", "link": "http://arxiv.org/abs/2503.04155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ticktack : Long Span Temporal Alignment of Large Language Models Leveraging Sexagenary Cycle Time Expression", "abstract": "Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal.", "published": "2025-03-06 06:59:09", "link": "http://arxiv.org/abs/2503.04150v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination", "abstract": "The rapid evolution of code largelanguage models underscores the need for\neffective and transparent benchmarking of their reasoning capabilities.\nHowever, the current benchmarking approach heavily depends on publicly\navailable, human-created datasets. The widespread use of these fixed benchmark\ndatasets makes the benchmarking process to be static and thus particularly\nsusceptible to data contamination, an unavoidable consequence of the extensive\ndata collection processes used to train Code LLMs. Existing approaches that\naddress data contamination often suffer from human effort limitations and\nimbalanced problem complexity. To tackle these challenges, we propose \\tool, a\nnovel benchmarking suite for evaluating Code LLMs under potential data\ncontamination. Given a seed programming problem, \\tool employs multiple agents\nto extract and modify the context without altering the core logic, generating\nsemantically equivalent variations. We introduce a dynamic data generation\nmethods and conduct empirical studies on two seed datasets across 21 Code LLMs.\nResults show that \\tool effectively benchmarks reasoning capabilities under\ncontamination risks while generating diverse problem sets to ensure consistent\nand reliable evaluations.", "published": "2025-03-06 06:56:59", "link": "http://arxiv.org/abs/2503.04149v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using LLMs", "abstract": "The growth of conversational AI services has increased demand for effective\ninformation retrieval from dialogue data. However, existing methods often face\nchallenges in capturing semantic intent or require extensive labeling and\nfine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted\nSemantic Indexing for Retrieval), a novel framework that enhances semantic\nunderstanding in conversational data retrieval through optimized data\ningestion, eliminating the need for resource-intensive labeling or model\nadaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets\nFormulation and (2) Adjunct Augmentation, creating semantic indices consisting\nof Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured\nrepresentation effectively captures the underlying semantic information from\ndialogue content. HEISIR achieves high retrieval performance while maintaining\nlow latency during the actual retrieval process. Our experimental results\ndemonstrate that HEISIR outperforms fine-tuned models across various embedding\ntypes and language models. Beyond improving retrieval capabilities, HEISIR also\noffers opportunities for intent and topic analysis in conversational data,\nproviding a versatile solution for dialogue systems.", "published": "2025-03-06 06:39:25", "link": "http://arxiv.org/abs/2503.04141v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Biological Sequence with Language Model Prompting: A Survey", "abstract": "Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.", "published": "2025-03-06 06:28:36", "link": "http://arxiv.org/abs/2503.04135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Gaps in How Humans and LLMs Interpret Subjective Language", "abstract": "Humans often rely on subjective natural language to direct language models\n(LLMs); for example, users might instruct the LLM to write an enthusiastic\nblogpost, while developers might train models to be helpful and harmless using\nLLM-based edits. The LLM's operational semantics of such subjective phrases --\nhow it adjusts its behavior when each phrase is included in the prompt -- thus\ndictates how aligned it is with human intent. In this work, we uncover\ninstances of misalignment between LLMs' actual operational semantics and what\nhumans expect. Our method, TED (thesaurus error detector), first constructs a\nthesaurus that captures whether two phrases have similar operational semantics\naccording to the LLM. It then elicits failures by unearthing disagreements\nbetween this thesaurus and a human-constructed reference. TED routinely\nproduces surprising instances of misalignment; for example, Mistral 7B Instruct\nproduces more harassing outputs when it edits text to be witty, and Llama 3 8B\nInstruct produces dishonest articles when instructed to make the articles\nenthusiastic. Our results demonstrate that humans can uncover unexpected LLM\nbehavior by scrutinizing relationships between abstract concepts, without\nsupervising outputs directly.", "published": "2025-03-06 05:43:35", "link": "http://arxiv.org/abs/2503.04113v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs Can Generate a Better Answer by Aggregating Their Own Responses", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across tasks,\nyet they often require additional prompting techniques when facing complex\nproblems. While approaches like self-correction and response selection have\nemerged as popular solutions, recent studies have shown these methods perform\npoorly when relying on the LLM itself to provide feedback or selection\ncriteria. We argue this limitation stems from the fact that common LLM\npost-training procedures lack explicit supervision for discriminative judgment\ntasks. In this paper, we propose Generative Self-Aggregation (GSA), a novel\nprompting method that improves answer quality without requiring the model's\ndiscriminative capabilities. GSA first samples multiple diverse responses from\nthe LLM, then aggregates them to obtain an improved solution. Unlike previous\napproaches, our method does not require the LLM to correct errors or compare\nresponse quality; instead, it leverages the model's generative abilities to\nsynthesize a new response based on the context of multiple samples. While GSA\nshares similarities with the self-consistency (SC) approach for response\naggregation, SC requires specific verifiable tokens to enable majority voting.\nIn contrast, our approach is more general and can be applied to open-ended\ntasks. Empirical evaluation demonstrates that GSA effectively improves response\nquality across various tasks, including mathematical reasoning, knowledge-based\nproblems, and open-ended generation tasks such as code synthesis and\nconversational responses.", "published": "2025-03-06 05:25:43", "link": "http://arxiv.org/abs/2503.04104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on African American English", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https://github.com/Runtaozhou/dialect_bias_eval.", "published": "2025-03-06 05:15:34", "link": "http://arxiv.org/abs/2503.04099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts", "abstract": "Multimodal Large Language Models (MLLMs) have garnered significant attention\nfor their strong visual-semantic understanding. Most existing chart benchmarks\nevaluate MLLMs' ability to parse information from charts to answer questions.\nHowever, they overlook the inherent output biases of MLLMs, where models rely\non their parametric memory to answer questions rather than genuinely\nunderstanding the chart content. To address this limitation, we introduce a\nnovel Chart Hypothetical Question Answering (HQA) task, which imposes\nassumptions on the same question to compel models to engage in counterfactual\nreasoning based on the chart content. Furthermore, we introduce HAI, a human-AI\ninteractive data synthesis approach that leverages the efficient text-editing\ncapabilities of LLMs alongside human expert knowledge to generate diverse and\nhigh-quality HQA data at a low cost. Using HAI, we construct Chart-HQA, a\nchallenging benchmark synthesized from publicly available data sources.\nEvaluation results on 18 MLLMs of varying model sizes reveal that current\nmodels face significant generalization challenges and exhibit imbalanced\nreasoning performance on the HQA task.", "published": "2025-03-06 05:08:40", "link": "http://arxiv.org/abs/2503.04095v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks", "abstract": "With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "published": "2025-03-06 03:43:21", "link": "http://arxiv.org/abs/2503.04065v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Uncovering inequalities in new knowledge learning by large language models across different languages", "abstract": "As large language models (LLMs) gradually become integral tools for problem\nsolving in daily life worldwide, understanding linguistic inequality is\nbecoming increasingly important. Existing research has primarily focused on\nstatic analyses that assess the disparities in the existing knowledge and\ncapabilities of LLMs across languages. However, LLMs are continuously evolving,\nacquiring new knowledge to generate up-to-date, domain-specific responses.\nInvestigating linguistic inequalities within this dynamic process is,\ntherefore, also essential. In this paper, we explore inequalities in new\nknowledge learning by LLMs across different languages and four key dimensions:\neffectiveness, transferability, prioritization, and robustness. Through\nextensive experiments under two settings (in-context learning and fine-tuning)\nusing both proprietary and open-source models, we demonstrate that low-resource\nlanguages consistently face disadvantages across all four dimensions. By\nshedding light on these disparities, we aim to raise awareness of linguistic\ninequalities in LLMs' new knowledge learning, fostering the development of more\ninclusive and equitable future LLMs.", "published": "2025-03-06 03:41:47", "link": "http://arxiv.org/abs/2503.04064v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "abstract": "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.", "published": "2025-03-06 02:40:51", "link": "http://arxiv.org/abs/2503.04036v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Benchmarking Large Language Models on Multiple Tasks in Bioinformatics NLP with Prompting", "abstract": "Large language models (LLMs) have become important tools in solving\nbiological problems, offering improvements in accuracy and adaptability over\nconventional methods. Several benchmarks have been proposed to evaluate the\nperformance of these LLMs. However, current benchmarks can hardly evaluate the\nperformance of these models across diverse tasks effectively. In this paper, we\nintroduce a comprehensive prompting-based benchmarking framework, termed\nBio-benchmark, which includes 30 key bioinformatics tasks covering areas such\nas proteins, RNA, drugs, electronic health records, and traditional Chinese\nmedicine. Using this benchmark, we evaluate six mainstream LLMs, including\nGPT-4o and Llama-3.1-70b, etc., using 0-shot and few-shot Chain-of-Thought\n(CoT) settings without fine-tuning to reveal their intrinsic capabilities. To\nimprove the efficiency of our evaluations, we demonstrate BioFinder, a new tool\nfor extracting answers from LLM responses, which increases extraction accuracy\nby round 30% compared to existing methods. Our benchmark results show the\nbiological tasks suitable for current LLMs and identify specific areas\nrequiring enhancement. Furthermore, we propose targeted prompt engineering\nstrategies for optimizing LLM performance in these contexts. Based on these\nfindings, we provide recommendations for the development of more robust LLMs\ntailored for various biological applications. This work offers a comprehensive\nevaluation framework and robust tools to support the application of LLMs in\nbioinformatics.", "published": "2025-03-06 02:01:59", "link": "http://arxiv.org/abs/2503.04013v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention for their remarkable ability to process and analyze non-textual data,\nsuch as images, videos, and audio. Notably, several adaptations of\ngeneral-domain MLLMs to the medical field have been explored, including\nLLaVA-Med. However, these medical adaptations remain insufficiently advanced in\nunderstanding and interpreting retinal images. In contrast, medical experts\nemphasize the importance of quantitative analyses for disease detection and\ninterpretation. This underscores a gap between general-domain and\nmedical-domain MLLMs: while general-domain MLLMs excel in broad applications,\nthey lack the specialized knowledge necessary for precise diagnostic and\ninterpretative tasks in the medical field. To address these challenges, we\nintroduce \\textit{RetinalGPT}, a multimodal conversational assistant for\nclinically preferred quantitative analysis of retinal images. Specifically, we\nachieve this by compiling a large retinal image dataset, developing a novel\ndata pipeline, and employing customized visual instruction tuning to enhance\nboth retinal analysis and enrich medical knowledge. In particular, RetinalGPT\noutperforms MLLM in the generic domain by a large margin in the diagnosis of\nretinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis,\nRetinalGPT features quantitative analyses and lesion localization, representing\na pioneering step in leveraging LLMs for an interpretable and end-to-end\nclinical research framework. The code is available at\nhttps://github.com/Retinal-Research/RetinalGPT", "published": "2025-03-06 00:19:54", "link": "http://arxiv.org/abs/2503.03987v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities", "abstract": "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps://research.nvidia.com/labs/adlr/AF2/.", "published": "2025-03-06 00:10:26", "link": "http://arxiv.org/abs/2503.03983v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReasonGraph: Visualisation of Reasoning Paths", "abstract": "Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.", "published": "2025-03-06 00:03:55", "link": "http://arxiv.org/abs/2503.03979v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Foundations of block-parallel automata networks", "abstract": "We settle the theoretical ground for the study of automata networks under\nblock-parallel update schedules, which are somehow dual to the block-sequential\nones, but allow for repetitions of automaton updates. This gain in expressivity\nbrings new challenges, and we analyse natural equivalence classes of update\nschedules: those leading to the same dynamics, and to the same limit dynamics,\nfor any automata network. Countings and enumeration algorithms are provided,\nfor their numerical study. We also prove computational complexity bounds for\nmany classical problems, involving fixed points, limit cycles, the recognition\nof subdynamics, reachability, etc. The PSPACE-completeness of computing the\nimage of a single configuration lifts the complexity of most problems, but the\nlandscape keeps some relief, in particular for reversible computations.", "published": "2025-03-06 16:28:02", "link": "http://arxiv.org/abs/2503.04591v2", "categories": ["cs.DM", "68R01, 68R05", "F.1.1; F.1.3"], "primary_category": "cs.DM"}
{"title": "Source-Oblivious Broadcast", "abstract": "This paper revisits the study of (minimum) broadcast graphs, i.e., graphs\nenabling fast information dissemination from every source node to all the other\nnodes (and having minimum number of edges for this property). This study is\nperformed in the framework of compact distributed data structures, that is,\nwhen the broadcast protocols are bounded to be encoded at each node as an\nordered list of neighbors specifying, upon reception of a message, in which\norder this message must be passed to these neighbors. We show that this\nconstraint does not limit the power of broadcast protocols, as far as the\ndesign of (minimum) broadcast graphs is concerned. Specifically, we show that,\nfor every~$n$, there are $n$-node graphs for which it is possible to design\nprotocols encoded by lists yet enabling broadcast in $\\lceil\\log_2n\\rceil$\nrounds from every source, which is optimal even for general (i.e., non\nspace-constrained) broadcast protocols. Moreover, we show that, for every~$n$,\nthere exist such graphs with the additional property that they are\nasymptotically as sparse as the sparsest graphs for which\n$\\lceil\\log_2n\\rceil$-round broadcast protocols exist, up to a constant\nmultiplicative factor. Concretely, these graphs have $O(n\\cdot L(n))$ edges,\nwhere $L(n)$ is the number of leading~1s in the binary representation of $n-1$,\nand general minimum broadcast graphs are known to have $\\Omega(n\\cdot L(n))$\nedges.", "published": "2025-03-06 14:57:56", "link": "http://arxiv.org/abs/2503.04511v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Positionality of Dumont--Thomas numeration systems for integers", "abstract": "Introduced in 2001 by Lecomte and Rigo, abstract numeration systems provide a\nway of expressing natural numbers with words from a language $L$ accepted by a\nfinite automaton. As it turns out, these numeration systems are not necessarily\npositional, i.e., we cannot always find a sequence $U=(U_i)_{i\\ge 0}$ of\nintegers such that the value of every word in the language $L$ is determined by\nthe position of its letters and the first few values of $U$. Finding the\nconditions under which an abstract numeration system is positional seems\ndifficult in general. In this paper, we thus consider this question for a\nparticular sub-family of abstract numeration systems called Dumont--Thomas\nnumeration systems. They are derived from substitutions and were introduced in\n1989 by Dumont and Thomas. We exhibit conditions on the underlying substitution\nso that the corresponding Dumont--Thomas numeration is positional. We first\nwork in the most general setting, then particularize our results to some\npractical cases. Finally, we link our numeration systems to existing\nliterature, notably properties studied by R\\'{e}nyi in 1957, Parry in 1960,\nBertrand-Mathis in 1989, and Fabre in 1995.", "published": "2025-03-06 14:33:39", "link": "http://arxiv.org/abs/2503.04487v3", "categories": ["math.CO", "cs.DM", "cs.FL", "11A67, 11K16, 68R15, 68Q45"], "primary_category": "math.CO"}
{"title": "Polynomial Bounds in the Apex Minor Theorem", "abstract": "A graph $A$ is \"apex\" if $A-z$ is planar for some vertex $z\\in V(A)$.\nEppstein [Algorithmica, 2000] showed that for a minor-closed class\n$\\mathcal{G}$, the graphs in $\\mathcal{G}$ with bounded radius have bounded\ntreewidth if and only if some apex graph is not in $\\mathcal{G}$. In\nparticular, for every apex graph $A$ and integer $r$, there is a minimum\ninteger $g(A,r)$ such that every $A$-minor-free graph with radius $r$ has\ntreewidth at most $g(A,r)$. We show that if $t=|V(A)|$ then $g(A,r)\\in\nO^\\ast(r^9t^{18})$ which is the first upper bound on $g(A,r)$ with polynomial\ndependence on both $r$ and $t$. More precisely, we show that every\n$A$-minor-free graph with radius $r$ has no $16rt^2 \\times 16rt^2$ grid minor,\nwhich implies the first result via the Polynomial Grid Minor Theorem. A key\nexample of an apex graph is the complete bipartite graph $K_{3,t}$, since\n$K_{3,t}$-minor-free graphs include and generalise graphs embeddable in any\nfixed surface. In this case, we prove that every $K_{3,t}$-minor-free graph\nwith radius $r$ has no $4r(1+\\sqrt{t})\\times 4r(1+\\sqrt{t})$ grid minor, which\nis tight up to a constant factor.", "published": "2025-03-06 09:08:45", "link": "http://arxiv.org/abs/2503.04228v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge", "abstract": "Foundation models (FMs) such as GPT-4 exhibit exceptional generative\ncapabilities across diverse downstream tasks through fine-tuning. Split\nFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning on\nresource-constrained local devices by offloading partial FM computations to\nedge servers, enabling device-edge synergistic fine-tuning. Practical edge\nnetworks often host multiple SFL tenants to support diversified downstream\ntasks. However, existing research primarily focuses on single-tenant SFL\nscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,\nwhich are essential to effectively coordinate self-interested local devices for\nparticipation in various downstream tasks, ensuring that each SFL tenant's\ndistinct FM fine-tuning requirements (e.g., FM types, performance targets, and\nfine-tuning deadlines) are met. To address this gap, we propose a novel\nPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer\nstrategic price incentives, which solicit high-quality device participation for\nefficient FM fine-tuning. Specifically, we first develop a bias-resilient\nglobal SFL model aggregation scheme to eliminate model biases caused by\nindependent device participation. We then derive a rigorous SFL convergence\nbound to evaluate the contributions of heterogeneous devices to FM performance\nimprovements, guiding the incentive strategies of SFL tenants. Furthermore, we\nmodel inter-tenant device competition as a congestion game for Stackelberg\nequilibrium (SE) analysis, deriving each SFL tenant's optimal incentive\nstrategy. Extensive simulations involving four representative SFL tenant types\n(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,\nand audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x\ncompared to state-of-the-art approaches, while consistently meeting fine-tuning\nperformance targets.", "published": "2025-03-06 21:06:27", "link": "http://arxiv.org/abs/2503.04971v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "primary_category": "cs.LG"}
{"title": "A Case Study of Counting the Number of Unique Users in Linear and Non-Linear Trails -- A Multi-Agent System Approach", "abstract": "Parks play a crucial role in enhancing the quality of life by providing\nrecreational spaces and environmental benefits. Understanding the patterns of\npark usage, including the number of visitors and their activities, is essential\nfor effective security measures, infrastructure maintenance, and resource\nallocation. Traditional methods rely on single-entry sensors that count total\nvisits but fail to distinguish unique users, limiting their effectiveness due\nto manpower and cost constraints.With advancements in affordable video\nsurveillance and networked processing, more comprehensive park usage analysis\nis now feasible. This study proposes a multi-agent system leveraging low-cost\ncameras in a distributed network to track and analyze unique users. As a case\nstudy, we deployed this system at the Jack A. Markell (JAM) Trail in\nWilmington, Delaware, and Hall Trail in Newark, Delaware. The system captures\nvideo data, autonomously processes it using existing algorithms, and extracts\nuser attributes such as speed, direction, activity type, clothing color, and\ngender. These attributes are shared across cameras to construct movement trails\nand accurately count unique visitors. Our approach was validated through\ncomparison with manual human counts and simulated scenarios under various\nconditions. The results demonstrate a 72% success rate in identifying unique\nusers, setting a benchmark in automated park activity monitoring. Despite\nchallenges such as camera placement and environmental factors, our findings\nsuggest that this system offers a scalable, cost-effective solution for\nreal-time park usage analysis and visitor behavior tracking.", "published": "2025-03-06 18:43:51", "link": "http://arxiv.org/abs/2503.07651v1", "categories": ["cs.MA", "cs.CV"], "primary_category": "cs.MA"}
{"title": "Multi-Agent Inverse Q-Learning from Demonstrations", "abstract": "When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql .", "published": "2025-03-06 18:22:29", "link": "http://arxiv.org/abs/2503.04679v1", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.MA"}
{"title": "From Idea to CAD: A Language Model-Driven Multi-Agent System for Collaborative Design", "abstract": "Creating digital models using Computer Aided Design (CAD) is a process that\nrequires in-depth expertise. In industrial product development, this process\ntypically involves entire teams of engineers, spanning requirements\nengineering, CAD itself, and quality assurance. We present an approach that\nmirrors this team structure with a Vision Language Model (VLM)-based Multi\nAgent System, with access to parametric CAD tooling and tool documentation.\nCombining agents for requirements engineering, CAD engineering, and\nvision-based quality assurance, a model is generated automatically from\nsketches and/ or textual descriptions. The resulting model can be refined\ncollaboratively in an iterative validation loop with the user. Our approach has\nthe potential to increase the effectiveness of design processes, both for\nindustry experts and for hobbyists who create models for 3D printing. We\ndemonstrate the potential of the architecture at the example of various design\ntasks and provide several ablations that show the benefits of the\narchitecture's individual components.", "published": "2025-03-06 13:21:27", "link": "http://arxiv.org/abs/2503.04417v1", "categories": ["cs.AI", "cs.MA", "J.6; I.6.5; I.2.1; I.2.11; I.2.8"], "primary_category": "cs.AI"}
{"title": "DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems", "abstract": "Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple\nagents to work together in mapping unknown environments while simultaneously\nestimating their own positions. This approach enhances robustness, scalability,\nand accuracy by sharing information between agents, reducing drift, and\nenabling collective exploration of larger areas. In this paper, we present\nDecentralized Visual Monocular SLAM (DVM-SLAM), the first open-source\ndecentralized monocular C-SLAM system. By only utilizing low-cost and\nlight-weight monocular vision sensors, our system is well suited for small\nrobots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is\nvalidated on physical robots with a custom collision avoidance framework,\nshowcasing its potential in real-time multi-agent autonomous navigation\nscenarios. We also demonstrate comparable accuracy to state-of-the-art\ncentralized monocular C-SLAM systems. We open-source our code and provide\nsupplementary material online.", "published": "2025-03-06 06:10:21", "link": "http://arxiv.org/abs/2503.04126v1", "categories": ["cs.RO", "cs.CV", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Pok\u00e9Champ: an Expert-level Minimax Language Agent", "abstract": "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models\n(LLMs) for Pok\\'emon battles. Built on a general framework for two-player\ncompetitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to\nenhance minimax tree search. Specifically, LLMs replace three key modules: (1)\nplayer action sampling, (2) opponent modeling, and (3) value function\nestimation, enabling the agent to effectively utilize gameplay history and\nhuman knowledge to reduce the search space and address partial observability.\nNotably, our framework requires no additional LLM training. We evaluate\nPok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves\na win rate of 76% against the best existing LLM-based bot and 84% against the\nstrongest rule-based bot, demonstrating its superior performance. Even with an\nopen-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently\noutperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o,\nwith a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the\nPok\\'emon Showdown online ladder, placing it among the top 30%-10% of human\nplayers. In addition, this work compiles the largest real-player Pok\\'emon\nbattle dataset, featuring over 3 million games, including more than 500k\nhigh-Elo matches. Based on this dataset, we establish a series of battle\nbenchmarks and puzzles to evaluate specific battling skills. We further provide\nkey updates to the local game engine. We hope this work fosters further\nresearch that leverage Pok\\'emon battle as benchmark to integrate LLM\ntechnologies with game-theoretic algorithms addressing general multiagent\nproblems. Videos, code, and dataset available at\nhttps://sites.google.com/view/pokechamp-llm.", "published": "2025-03-06 05:06:27", "link": "http://arxiv.org/abs/2503.04094v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Risk-aware Trading Portfolio Optimization", "abstract": "We investigate portfolio optimization in financial markets from a trading and\nrisk management perspective. We term this task Risk-Aware Trading Portfolio\nOptimization (RATPO), formulate the corresponding optimization problem, and\npropose an efficient Risk-Aware Trading Swarm (RATS) algorithm to solve it. The\nkey elements of RATPO are a generic initial portfolio P, a specific set of\nUnique Eligible Instruments (UEIs), their combination into an Eligible\nOptimization Strategy (EOS), an objective function, and a set of constraints.\nRATS searches for an optimal EOS that, added to P, improves the objective\nfunction repecting the constraints.\n  RATS is a specialized Particle Swarm Optimization method that leverages the\nparameterization of P in terms of UEIs, enables parallel computation with a\nlarge number of particles, and is fully general with respect to specific\nchoices of the key elements, which can be customized to encode financial\nknowledge and needs of traders and risk managers.\n  We showcase two RATPO applications involving a real trading portfolio made of\nhundreds of different financial instruments, an objective function combining\nboth market risk (VaR) and profit&loss measures, constrains on market\nsensitivities and UEIs trading costs. In the case of small-sized EOS, RATS\nsuccessfully identifies the optimal solution and demonstrates robustness with\nrespect to hyper-parameters tuning. In the case of large-sized EOS, RATS\nmarkedly improves the portfolio objective value, optimizing risk and capital\ncharge while respecting risk limits and preserving expected profits.\n  Our work bridges the gap between the implementation of effective trading\nstrategies and compliance with stringent regulatory and economic capital\nrequirements, allowing a better alignment of business and risk management\nobjectives.", "published": "2025-03-06 17:56:57", "link": "http://arxiv.org/abs/2503.04662v1", "categories": ["q-fin.RM", "q-fin.CP", "q-fin.PM", "91G10, 91G60 (primary) 90-08 (secondary)"], "primary_category": "q-fin.RM"}
{"title": "Hedging with Sparse Reward Reinforcement Learning", "abstract": "Derivatives, as a critical class of financial instruments, isolate and trade\nthe price attributes of risk assets such as stocks, commodities, and indices,\naiding risk management and enhancing market efficiency. However, traditional\nhedging models, constrained by assumptions such as continuous trading and zero\ntransaction costs, fail to satisfy risk control requirements in complex and\nuncertain real-world markets.\n  With advances in computing technology and deep learning, data-driven trading\nstrategies are becoming increasingly prevalent. This thesis proposes a\nderivatives hedging framework integrating deep learning and reinforcement\nlearning. The framework comprises a probabilistic forecasting model and a\nhedging agent, enabling market probability prediction, derivative pricing, and\nhedging.\n  Specifically, we design a spatiotemporal attention-based probabilistic\nfinancial time series forecasting Transformer to address the scarcity of\nderivatives hedging data. A low-rank attention mechanism compresses\nhigh-dimensional assets into a low-dimensional latent space, capturing\nnonlinear asset relationships. The Transformer models sequential dependencies\nwithin this latent space, improving market probability forecasts and\nconstructing an online training environment for downstream hedging tasks.\n  Additionally, we incorporate generalized geometric Brownian motion to develop\na risk-neutral pricing approach for derivatives. We model derivatives hedging\nas a reinforcement learning problem with sparse rewards and propose a behavior\ncloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This\npretraining-finetuning framework significantly enhances the hedging agent's\nperformance. Numerical experiments in the U.S. and Chinese financial markets\ndemonstrate our method's superiority over traditional approaches.", "published": "2025-03-06 08:53:28", "link": "http://arxiv.org/abs/2503.04218v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "CoFinDiff: Controllable Financial Diffusion Model for Time Series Generation", "abstract": "The generation of synthetic financial data is a critical technology in the\nfinancial domain, addressing challenges posed by limited data availability.\nTraditionally, statistical models have been employed to generate synthetic\ndata. However, these models fail to capture the stylized facts commonly\nobserved in financial data, limiting their practical applicability. Recently,\nmachine learning models have been introduced to address the limitations of\nstatistical models; however, controlling synthetic data generation remains\nchallenging. We propose CoFinDiff (Controllable Financial Diffusion model), a\nsynthetic financial data generation model based on conditional diffusion models\nthat accept conditions about the synthetic time series. By incorporating\nconditions derived from price data into the conditional diffusion model via\ncross-attention, CoFinDiff learns the relationships between the conditions and\nthe data, generating synthetic data that align with arbitrary conditions.\nExperimental results demonstrate that: (i) synthetic data generated by\nCoFinDiff capture stylized facts; (ii) the generated data accurately meet\nspecified conditions for trends and volatility; (iii) the diversity of the\ngenerated data surpasses that of the baseline models; and (iv) models trained\non CoFinDiff-generated data achieve improved performance in deep hedging task.", "published": "2025-03-06 07:25:37", "link": "http://arxiv.org/abs/2503.04164v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Fredholm Approach to Nonlinear Propagator Models", "abstract": "We formulate and solve an optimal trading problem with alpha signals, where\ntransactions induce a nonlinear transient price impact described by a general\npropagator model, including power-law decay. Using a variational approach, we\ndemonstrate that the optimal trading strategy satisfies a nonlinear stochastic\nFredholm equation with both forward and backward coefficients. We prove the\nexistence and uniqueness of the solution under a monotonicity condition\nreflecting the nonlinearity of the price impact. Moreover, we derive an\nexistence result for the optimal strategy beyond this condition when the\nunderlying probability space is countable. In addition, we introduce a novel\niterative scheme and establish its convergence to the optimal trading strategy.\nFinally, we provide a numerical implementation of the scheme that illustrates\nits convergence, stability, and the effects of concavity on optimal execution\nstrategies under exponential and power-law decay.", "published": "2025-03-06 11:15:23", "link": "http://arxiv.org/abs/2503.04323v1", "categories": ["q-fin.MF", "math.OC", "q-fin.TR", "93E20, 60H30, 91G80"], "primary_category": "q-fin.MF"}
{"title": "Wasserstein Robust Market Making via Entropy Regularization", "abstract": "In this paper, we introduce a robust market making framework based on\nWasserstein distance, utilizing a stochastic policy approach enhanced by\nentropy regularization. We demonstrate that, under mild assumptions, the robust\nmarket making problem can be reformulated as a convex optimization question.\nAdditionally, we outline a methodology for selecting the optimal radius of the\nWasserstein ball, further refining our framework's effectiveness.", "published": "2025-03-06 04:08:10", "link": "http://arxiv.org/abs/2503.04072v1", "categories": ["q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Matrix H-theory approach to stock market fluctuations", "abstract": "We introduce matrix H theory, a framework for analyzing collective behavior\narising from multivariate stochastic processes with hierarchical structure. The\ntheory models the joint distribution of the multiple variables (the measured\nsignal) as a compound of a large-scale multivariate distribution with the\ndistribution of a slowly fluctuating background. The background is\ncharacterized by a hierarchical stochastic evolution of internal degrees of\nfreedom, representing the correlations between stocks at different time scales.\nAs in its univariate version, the matrix H-theory formalism also has two\nuniversality classes: Wishart and inverse Wishart, enabling a concise\ndescription of both the background and the signal probability distributions in\nterms of Meijer G-functions with matrix argument. Empirical analysis of daily\nreturns of stocks within the S&P500 demonstrates the effectiveness of matrix H\ntheory in describing fluctuations in stock markets. These findings contribute\nto a deeper understanding of multivariate hierarchical processes and offer\npotential for developing more informed portfolio strategies in financial\nmarkets.", "published": "2025-03-06 01:40:16", "link": "http://arxiv.org/abs/2503.08697v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Musical Source Separation of Brazilian Percussion", "abstract": "Musical source separation (MSS) has recently seen a big breakthrough in\nseparating instruments from a mixture in the context of Western music, but\nresearch on non-Western instruments is still limited due to a lack of data. In\nthis demo, we use an existing dataset of Brazilian sama percussion to create\nartificial mixtures for training a U-Net model to separate the surdo drum, a\ntraditional instrument in samba. Despite limited training data, the model\neffectively isolates the surdo, given the drum's repetitive patterns and its\ncharacteristic low-pitched timbre. These results suggest that MSS systems can\nbe successfully harnessed to work in more culturally-inclusive scenarios\nwithout the need of collecting extensive amounts of data.", "published": "2025-03-06 21:49:28", "link": "http://arxiv.org/abs/2503.04995v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Unified Framework with Novel Metrics for Evaluating the Effectiveness\n  of XAI Techniques in LLMs", "abstract": "The increasing complexity of LLMs presents significant challenges to their\ntransparency and interpretability, necessitating the use of eXplainable AI\n(XAI) techniques to enhance trustworthiness and usability. This study\nintroduces a comprehensive evaluation framework with four novel metrics for\nassessing the effectiveness of five XAI techniques across five LLMs and two\ndownstream tasks. We apply this framework to evaluate several XAI techniques\nLIME, SHAP, Integrated Gradients, Layer-wise Relevance Propagation (LRP), and\nAttention Mechanism Visualization (AMV) using the IMDB Movie Reviews and Tweet\nSentiment Extraction datasets. The evaluation focuses on four key metrics:\nHuman-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. Our\nresults show that LIME consistently achieves high scores across multiple LLMs\nand evaluation metrics, while AMV demonstrates superior Robustness and\nnear-perfect Consistency. LRP excels in Contrastivity, particularly with more\ncomplex models. Our findings provide valuable insights into the strengths and\nlimitations of different XAI methods, offering guidance for developing and\nselecting appropriate XAI techniques for LLMs.", "published": "2025-03-06 23:59:50", "link": "http://arxiv.org/abs/2503.05050v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Voice to Safety: Language AI Powered Pilot-ATC Communication\n  Understanding for Airport Surface Movement Collision Risk Assessment", "abstract": "This work integrates language AI-based voice communication understanding with\ncollision risk assessment. The proposed framework consists of two major parts,\n(a) Automatic Speech Recognition (ASR); (b) surface collision risk modeling.\nASR module generates information tables by processing voice communication\ntranscripts, which serve as references for producing potential taxi plans and\ncalculating the surface movement collision risk. For ASR, we collect and\nannotate our own Named Entity Recognition (NER) dataset based on open-sourced\nvideo recordings and safety investigation reports. Additionally, we refer to\nFAA Order JO 7110.65W and FAA Order JO 7340.2N to get the list of heuristic\nrules and phase contractions of communication between the pilot and the Air\nTraffic Controller (ATCo) used in daily aviation operations. Then, we propose\nthe novel ATC Rule-Enhanced NER method, which integrates the heuristic rules\ninto the model training and inference stages, resulting into hybrid rule-based\nNER model. We show the effectiveness of this hybrid approach by comparing\ndifferent setups with different token-level embedding models. For the risk\nmodeling, we adopt the node-link airport layout graph from NASA FACET and model\nthe aircraft taxi speed at each link as a log-normal distribution and derive\nthe total taxi time distribution. Then, we propose a spatiotemporal formulation\nof the risk probability of two aircraft moving across potential collision nodes\nduring ground movement. We show the effectiveness of our approach by simulating\ntwo case studies, (a) the Henada airport runway collision accident happened in\nJanuary 2024; (b) the KATL taxiway collision happened in September 2024. We\nshow that, by understanding the pilot-ATC communication transcripts and\nanalyzing surface movement patterns, the proposed model improves airport safety\nby providing risk assessment in time.", "published": "2025-03-06 21:08:07", "link": "http://arxiv.org/abs/2503.04974v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frequency-Based Alignment of EEG and Audio Signals Using Contrastive\n  Learning and SincNet for Auditory Attention Detection", "abstract": "Humans exhibit a remarkable ability to focus auditory attention in complex\nacoustic environments, such as cocktail parties. Auditory attention detection\n(AAD) aims to identify the attended speaker by analyzing brain signals, such as\nelectroencephalography (EEG) data. Existing AAD algorithms often leverage deep\nlearning's powerful nonlinear modeling capabilities, few consider the neural\nmechanisms underlying auditory processing in the brain. In this paper, we\npropose SincAlignNet, a novel network based on an improved SincNet and\ncontrastive learning, designed to align audio and EEG features for auditory\nattention detection. The SincNet component simulates the brain's processing of\naudio during auditory attention, while contrastive learning guides the model to\nlearn the relationship between EEG signals and attended speech. During\ninference, we calculate the cosine similarity between EEG and audio features\nand also explore direct inference of the attended speaker using EEG data.\nCross-trial evaluations results demonstrate that SincAlignNet outperforms\nstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,\nachieving average accuracies of 78.3% and 92.2%, respectively, with a 1-second\ndecision window. The model exhibits strong interpretability, revealing that the\nleft and right temporal lobes are more active during both male and female\nspeaker scenarios. Furthermore, we found that using data from only six\nelectrodes near the temporal lobes maintains similar or even better performance\ncompared to using 64 electrodes. These findings indicate that efficient\nlow-density EEG online decoding is achievable, marking an important step toward\nthe practical implementation of neuro-guided hearing aids in real-world\napplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.", "published": "2025-03-06 07:11:01", "link": "http://arxiv.org/abs/2503.04156v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "TAIL: Text-Audio Incremental Learning", "abstract": "Many studies combine text and audio to capture multi-modal information but\nthey overlook the model's generalization ability on new datasets. Introducing\nnew datasets may affect the feature space of the original dataset, leading to\ncatastrophic forgetting. Meanwhile, large model parameters can significantly\nimpact training performance. To address these limitations, we introduce a novel\ntask called Text-Audio Incremental Learning (TAIL) task for text-audio\nretrieval, and propose a new method, PTAT, Prompt Tuning for Audio-Text\nincremental learning. This method utilizes prompt tuning to optimize the model\nparameters while incorporating an audio-text similarity and feature\ndistillation module to effectively mitigate catastrophic forgetting. We\nbenchmark our method and previous incremental learning methods on AudioCaps,\nClotho, BBC Sound Effects and Audioset datasets, and our method outperforms\nprevious methods significantly, particularly demonstrating stronger resistance\nto forgetting on older datasets. Compared to the full-parameters Finetune\n(Sequential) method, our model only requires 2.42\\% of its parameters,\nachieving 4.46\\% higher performance.", "published": "2025-03-06 09:39:36", "link": "http://arxiv.org/abs/2503.04258v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "I.2"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Models for Phoneme Recognition: Applications in\n  Children's Speech for Reading Learning", "abstract": "Child speech recognition is still an underdeveloped area of research due to\nthe lack of data (especially on non-English languages) and the specific\ndifficulties of this task. Having explored various architectures for child\nspeech recognition in previous work, in this article we tackle recent\nself-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models\nadapted to phoneme recognition in French child speech, and continue our\nexperiments with the best of them, WavLM base+. We then further adapt it by\nunfreezing its transformer blocks during fine-tuning on child speech, which\ngreatly improves its performance and makes it significantly outperform our base\nmodel, a Transformer+CTC. Finally, we study in detail the behaviour of these\ntwo models under the real conditions of our application, and show that WavLM\nbase+ is more robust to various reading tasks and noise levels. Index Terms:\nspeech recognition, child speech, self-supervised learning", "published": "2025-03-06 18:57:16", "link": "http://arxiv.org/abs/2503.04710v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Cascaded Architecture for Extractive Summarization of Multimedia\n  Content via Audio-to-Text Alignment", "abstract": "This study presents a cascaded architecture for extractive summarization of\nmultimedia content via audio-to-text alignment. The proposed framework\naddresses the challenge of extracting key insights from multimedia sources like\nYouTube videos. It integrates audio-to-text conversion using Microsoft Azure\nSpeech with advanced extractive summarization models, including Whisper,\nPegasus, and Facebook BART XSum. The system employs tools such as Pytube,\nPydub, and SpeechRecognition for content retrieval, audio extraction, and\ntranscription. Linguistic analysis is enhanced through named entity recognition\nand semantic role labeling. Evaluation using ROUGE and F1 scores demonstrates\nthat the cascaded architecture outperforms conventional summarization methods,\ndespite challenges like transcription errors. Future improvements may include\nmodel fine-tuning and real-time processing. This study contributes to\nmultimedia summarization by improving information retrieval, accessibility, and\nuser experience.", "published": "2025-03-06 13:59:14", "link": "http://arxiv.org/abs/2504.06275v1", "categories": ["cs.IR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
