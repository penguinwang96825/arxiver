{"title": "Technical Progress Analysis Using a Dynamic Topic Model for Technical\n  Terms to Revise Patent Classification Codes", "abstract": "Japanese patents are assigned a patent classification code, FI (File Index),\nthat is unique to Japan. FI is a subdivision of the IPC, an international\npatent classification code, that is related to Japanese technology. FIs are\nrevised to keep up with technological developments. These revisions have\nalready established more than 30,000 new FIs since 2006. However, these\nrevisions require a lot of time and workload. Moreover, these revisions are not\nautomated and are thus inefficient. Therefore, using machine learning to assist\nin the revision of patent classification codes (FI) will lead to improved\naccuracy and efficiency. This study analyzes patent documents from this new\nperspective of assisting in the revision of patent classification codes with\nmachine learning. To analyze time-series changes in patents, we used the\ndynamic topic model (DTM), which is an extension of the latent Dirichlet\nallocation (LDA). Also, unlike English, the Japanese language requires\nmorphological analysis. Patents contain many technical words that are not used\nin everyday life, so morphological analysis using a common dictionary is not\nsufficient. Therefore, we used a technique for extracting technical terms from\ntext. After extracting technical terms, we applied them to DTM. In this study,\nwe determined the technological progress of the lighting class F21 for 14 years\nand compared it with the actual revision of patent classification codes. In\nother words, we extracted technical terms from Japanese patents and applied DTM\nto determine the progress of Japanese technology. Then, we analyzed the results\nfrom the new perspective of revising patent classification codes with machine\nlearning. As a result, it was found that those whose topics were on the rise\nwere judged to be new technologies.", "published": "2020-12-18 09:24:01", "link": "http://arxiv.org/abs/2012.10120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularized Attentive Capsule Network for Overlapped Relation Extraction", "abstract": "Distantly supervised relation extraction has been widely applied in knowledge\nbase construction due to its less requirement of human efforts. However, the\nautomatically established training datasets in distant supervision contain\nlow-quality instances with noisy words and overlapped relations, introducing\ngreat challenges to the accurate extraction of relations. To address this\nproblem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet)\nto better identify highly overlapped relations in each informal sentence. To\ndiscover multiple relation features in an instance, we embed multi-head\nattention into the capsule network as the low-level capsules, where the\nsubtraction of two entities acts as a new form of relation query to select\nsalient features regardless of their positions. To further discriminate\noverlapped relation features, we devise disagreement regularization to\nexplicitly encourage the diversity among both multiple attention heads and\nlow-level capsules. Extensive experiments conducted on widely used datasets\nshow that our model achieves significant improvements in relation extraction.", "published": "2020-12-18 12:17:08", "link": "http://arxiv.org/abs/2012.10187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Open Intent Classification with Adaptive Decision Boundary", "abstract": "Open intent classification is a challenging task in dialogue systems. On the\none hand, it should ensure the quality of known intent identification. On the\nother hand, it needs to detect the open (unknown) intent without prior\nknowledge. Current models are limited in finding the appropriate decision\nboundary to balance the performances of both known intents and the open intent.\nIn this paper, we propose a post-processing method to learn the adaptive\ndecision boundary (ADB) for open intent classification. We first utilize the\nlabeled known intent samples to pre-train the model. Then, we automatically\nlearn the adaptive spherical decision boundary for each known class with the\naid of well-trained features. Specifically, we propose a new loss function to\nbalance both the empirical risk and the open space risk. Our method does not\nneed open intent samples and is free from modifying the model architecture.\nMoreover, our approach is surprisingly insensitive with less labeled data and\nfewer known intents. Extensive experiments on three benchmark datasets show\nthat our method yields significant improvements compared with the\nstate-of-the-art methods. The codes are released at\nhttps://github.com/thuiar/Adaptive-Decision-Boundary.", "published": "2020-12-18 13:05:11", "link": "http://arxiv.org/abs/2012.10209v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdvExpander: Generating Natural Language Adversarial Examples by\n  Expanding Text", "abstract": "Adversarial examples are vital to expose the vulnerability of machine\nlearning models. Despite the success of the most popular substitution-based\nmethods which substitutes some characters or words in the original examples,\nonly substitution is insufficient to uncover all robustness issues of models.\nIn this paper, we present AdvExpander, a method that crafts new adversarial\nexamples by expanding text, which is complementary to previous\nsubstitution-based methods. We first utilize linguistic rules to determine\nwhich constituents to expand and what types of modifiers to expand with. We\nthen expand each constituent by inserting an adversarial modifier searched from\na CVAE-based generative model which is pre-trained on a large scale corpus. To\nsearch adversarial modifiers, we directly search adversarial latent codes in\nthe latent space without tuning the pre-trained parameters. To ensure that our\nadversarial examples are label-preserving for text matching, we also constrain\nthe modifications with a heuristic rule. Experiments on three classification\ntasks verify the effectiveness of AdvExpander and the validity of our\nadversarial examples. AdvExpander crafts a new type of adversarial examples by\ntext expansion, thereby promising to reveal new robustness issues.", "published": "2020-12-18 13:50:17", "link": "http://arxiv.org/abs/2012.10235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReINTEL Challenge 2020: A Multimodal Ensemble Model for Detecting\n  Unreliable Information on Vietnamese SNS", "abstract": "In this paper, we present our methods for unrealiable information\nidentification task at VLSP 2020 ReINTEL Challenge. The task is to classify a\npiece of information into reliable or unreliable category. We propose a novel\nmultimodal ensemble model which combines two multimodal models to solve the\ntask. In each multimodal model, we combined feature representations acquired\nfrom three different data types: texts, images, and metadata. Multimodal\nfeatures are derived from three neural networks and fused for classification.\nExperimental results showed that our proposed multimodal ensemble model\nimproved against single models in term of ROC AUC score. We obtained 0.9445 AUC\nscore on the private test of the challenge.", "published": "2020-12-18 14:33:08", "link": "http://arxiv.org/abs/2012.10267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Using Pre-trained BERT Models for Vietnamese\n  Relation Extraction Task at VLSP 2020", "abstract": "In this paper, we present an empirical study of using pre-trained BERT models\nfor the relation extraction task at the VLSP 2020 Evaluation Campaign. We\napplied two state-of-the-art BERT-based models: R-BERT and BERT model with\nentity starts. For each model, we compared two pre-trained BERT models:\nFPTAI/vibert and NlpHUST/vibert4news. We found that NlpHUST/vibert4news model\nsignificantly outperforms FPTAI/vibert for the Vietnamese relation extraction\ntask. Finally, we proposed an ensemble model that combines R-BERT and BERT with\nentity starts. Our proposed ensemble model slightly improved against two single\nmodels on the development data and the test data provided by the task\norganizers.", "published": "2020-12-18 14:53:49", "link": "http://arxiv.org/abs/2012.10275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Contextual Representations for Semantic Parsing with\n  Generation-Augmented Pre-Training", "abstract": "Most recently, there has been significant interest in learning contextual\nrepresentations for various NLP tasks, by leveraging large scale text corpora\nto train large neural language models with self-supervised learning objectives,\nsuch as Masked Language Model (MLM). However, based on a pilot study, we\nobserve three issues of existing general-purpose language models when they are\napplied to text-to-SQL semantic parsers: fail to detect column mentions in the\nutterances, fail to infer column mentions from cell values, and fail to compose\ncomplex SQL queries. To mitigate these issues, we present a model pre-training\nframework, Generation-Augmented Pre-training (GAP), that jointly learns\nrepresentations of natural language utterances and table schemas by leveraging\ngeneration models to generate pre-train data. GAP MODEL is trained on 2M\nutterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances\nare produced by generative models. Based on experimental results, neural\nsemantic parsers that leverage GAP MODEL as a representation encoder obtain new\nstate-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks.", "published": "2020-12-18 15:53:50", "link": "http://arxiv.org/abs/2012.10309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Event Specific and Chunk Span features to Extract COVID\n  Events from tweets", "abstract": "Twitter has acted as an important source of information during disasters and\npandemic, especially during the times of COVID-19. In this paper, we describe\nour system entry for WNUT 2020 Shared Task-3. The task was aimed at automating\nthe extraction of a variety of COVID-19 related events from Twitter, such as\nindividuals who recently contracted the virus, someone with symptoms who were\ndenied testing and believed remedies against the infection. The system consists\nof separate multi-task models for slot-filling subtasks and\nsentence-classification subtasks while leveraging the useful sentence-level\ninformation for the corresponding event. The system uses COVID-Twitter-Bert\nwith attention-weighted pooling of candidate slot-chunk features to capture the\nuseful information chunks. The system ranks 1st at the leader-board with F1 of\n0.6598, without using any ensembles or additional datasets. The code and\ntrained models are available at this https URL.", "published": "2020-12-18 04:49:32", "link": "http://arxiv.org/abs/2012.10052v1", "categories": ["cs.CL", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Attention-Based LSTM Network for COVID-19 Clinical Trial Parsing", "abstract": "COVID-19 clinical trial design is a critical task in developing therapeutics\nfor the prevention and treatment of COVID-19. In this study, we apply a deep\nlearning approach to extract eligibility criteria variables from COVID-19\ntrials to enable quantitative analysis of trial design and optimization.\nSpecifically, we train attention-based bidirectional Long Short-Term Memory\n(Att-BiLSTM) models and use the optimal model to extract entities (i.e.,\nvariables) from the eligibility criteria of COVID-19 trials. We compare the\nperformance of Att-BiLSTM with traditional ontology-based method. The result on\na benchmark dataset shows that Att-BiLSTM outperforms the ontology model.\nAtt-BiLSTM achieves a precision of 0.942, recall of 0.810, and F1 of 0.871,\nwhile the ontology model only achieves a precision of 0.715, recall of 0.659,\nand F1 of 0.686. Our analyses demonstrate that Att-BiLSTM is an effective\napproach for characterizing patient populations in COVID-19 clinical trials.", "published": "2020-12-18 05:55:52", "link": "http://arxiv.org/abs/2012.10063v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mention Extraction and Linking for SQL Query Generation", "abstract": "On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take\na slot-filling approach by building several dedicated models for each type of\nslots. Such modularized systems are not only complex butalso of limited\ncapacity for capturing inter-dependencies among SQL clauses. To solve these\nproblems, this paper proposes a novel extraction-linking approach, where a\nunified extractor recognizes all types of slot mentions appearing in the\nquestion sentence before a linker maps the recognized columns to the table\nschema to generate executable SQL queries. Trained with automatically generated\nannotations, the proposed method achieves the first place on the WikiSQL\nbenchmark.", "published": "2020-12-18 06:51:23", "link": "http://arxiv.org/abs/2012.10074v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Should I visit this place? Inclusion and Exclusion Phrase Mining from\n  Reviews", "abstract": "Although several automatic itinerary generation services have made travel\nplanning easy, often times travellers find themselves in unique situations\nwhere they cannot make the best out of their trip. Visitors differ in terms of\nmany factors such as suffering from a disability, being of a particular dietary\npreference, travelling with a toddler, etc. While most tourist spots are\nuniversal, others may not be inclusive for all. In this paper, we focus on the\nproblem of mining inclusion and exclusion phrases associated with 11 such\nfactors, from reviews related to a tourist spot. While existing work on tourism\ndata mining mainly focuses on structured extraction of trip related\ninformation, personalized sentiment analysis, and automatic itinerary\ngeneration, to the best of our knowledge this is the first work on\ninclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000\nreviews related to 1000 tourist spots, our broad level classifier provides a\nbinary overlap F1 of $\\sim$80 and $\\sim$82 to classify a phrase as inclusion or\nexclusion respectively. Further, our inclusion/exclusion classifier provides an\nF1 of $\\sim$98 and $\\sim$97 for 11-class inclusion and exclusion classification\nrespectively. We believe that our work can significantly improve the quality of\nan automatic itinerary generation service.", "published": "2020-12-18 13:43:13", "link": "http://arxiv.org/abs/2012.10226v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understood in Translation, Transformers for Domain Understanding", "abstract": "Knowledge acquisition is the essential first step of any Knowledge Graph (KG)\napplication. This knowledge can be extracted from a given corpus (KG generation\nprocess) or specified from an existing KG (KG specification process). Focusing\non domain specific solutions, knowledge acquisition is a labor intensive task\nusually orchestrated and supervised by subject matter experts. Specifically,\nthe domain of interest is usually manually defined and then the needed\ngeneration or extraction tools are utilized to produce the KG. Herein, we\npropose a supervised machine learning method, based on Transformers, for domain\ndefinition of a corpus. We argue why such automated definition of the domain's\nstructure is beneficial both in terms of construction time and quality of the\ngenerated graph. The proposed method is extensively validated on three public\ndatasets (WebNLG, NYT and DocRED) by comparing it with two reference methods\nbased on CNNs and RNNs models. The evaluation shows the efficiency of our model\nin this task. Focusing on scientific document understanding, we present a new\nhealth domain dataset based on publications extracted from PubMed and we\nsuccessfully utilize our method on this. Lastly, we demonstrate how this work\nlays the foundation for fully automated and unsupervised KG generation.", "published": "2020-12-18 14:47:47", "link": "http://arxiv.org/abs/2012.10271v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trying Bilinear Pooling in Video-QA", "abstract": "Bilinear pooling (BLP) refers to a family of operations recently developed\nfor fusing features from different modalities predominantly developed for VQA\nmodels. A bilinear (outer-product) expansion is thought to encourage models to\nlearn interactions between two feature spaces and has experimentally\noutperformed `simpler' vector operations (concatenation and\nelement-wise-addition/multiplication) on VQA benchmarks. Successive BLP\ntechniques have yielded higher performance with lower computational expense and\nare often implemented alongside attention mechanisms. However, despite\nsignificant progress in VQA, BLP methods have not been widely applied to more\nrecently explored video question answering (video-QA) tasks. In this paper, we\nbegin to bridge this research gap by applying BLP techniques to various\nvideo-QA benchmarks, namely: TVQA, TGIF-QA, Ego-VQA and MSVD-QA. We share our\nresults on the TVQA baseline model, and the recently proposed\nheterogeneous-memory-enchanced multimodal attention (HME) model. Our\nexperiments include both simply replacing feature concatenation in the existing\nmodels with BLP, and a modified version of the TVQA baseline to accommodate BLP\nwe name the `dual-stream' model. We find that our relatively simple integration\nof BLP does not increase, and mostly harms, performance on these video-QA\nbenchmarks. Using recently proposed theoretical multimodal fusion taxonomies,\nwe offer insight into why BLP-driven performance gain for video-QA benchmarks\nmay be more difficult to achieve than in earlier VQA models. We suggest a few\nadditional `best-practices' to consider when applying BLP to video-QA. We\nstress that video-QA models should carefully consider where the complex\nrepresentational potential from BLP is actually needed to avoid computational\nexpense on `redundant' fusion.", "published": "2020-12-18 15:01:50", "link": "http://arxiv.org/abs/2012.10285v1", "categories": ["cs.CV", "cs.CL", "68T99", "I.2.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "Efficient Object-Level Visual Context Modeling for Multimodal Machine\n  Translation: Masking Irrelevant Objects Helps Grounding", "abstract": "Visual context provides grounding information for multimodal machine\ntranslation (MMT). However, previous MMT models and probing studies on visual\nfeatures suggest that visual information is less explored in MMT as it is often\nredundant to textual information. In this paper, we propose an object-level\nvisual context modeling framework (OVC) to efficiently capture and explore\nvisual information for multimodal machine translation. With detected objects,\nthe proposed OVC encourages MMT to ground translation on desirable visual\nobjects by masking irrelevant objects in the visual modality. We equip the\nproposed with an additional object-masking loss to achieve this goal. The\nobject-masking loss is estimated according to the similarity between masked\nobjects and the source texts so as to encourage masking source-irrelevant\nobjects. Additionally, in order to generate vision-consistent target words, we\nfurther propose a vision-weighted translation loss for OVC. Experiments on MMT\ndatasets demonstrate that the proposed OVC model outperforms state-of-the-art\nMMT models and analyses show that masking irrelevant objects helps grounding in\nMMT.", "published": "2020-12-18 11:10:00", "link": "http://arxiv.org/abs/2101.05208v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NeurST: Neural Speech Translation Toolkit", "abstract": "NeurST is an open-source toolkit for neural speech translation. The toolkit\nmainly focuses on end-to-end speech translation, which is easy to use, modify,\nand extend to advanced speech translation research and products. NeurST aims at\nfacilitating the speech translation research for NLP researchers and building\nreliable benchmarks for this field. It provides step-by-step recipes for\nfeature extraction, data preprocessing, distributed training, and evaluation.\nIn this paper, we will introduce the framework design of NeurST and show\nexperimental results for different benchmark datasets, which can be regarded as\nreliable baselines for future research. The toolkit is publicly available at\nhttps://github.com/bytedance/neurst/ and we will continuously update the\nperformance of NeurST with other counterparts and studies at\nhttps://st-benchmark.github.io/.", "published": "2020-12-18 02:33:58", "link": "http://arxiv.org/abs/2012.10018v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Fluent Query Reformulations with Text-to-Text Transformers and\n  Reinforcement Learning", "abstract": "Query reformulation aims to alter noisy or ambiguous text sequences into\ncoherent ones closer to natural language questions. This is to prevent errors\nfrom propagating in a client-facing pipeline and promote better communication\nwith users. Besides, it is crucial to maintain performance in downstream\nenvironments like question answering when rephrased queries are given as input.\nWe show that under the previous framework (AQA), attempts to alter RL\nalgorithms do not bring significant benefits to either reward acquisition or\nsequence fluency. Instead, we leverage a query-reformulating text-to-text\ntransformer (QRT5) and apply policy-based RL algorithms to further nudge this\nreformulator and obtain better answers downstream by generating\nreward-acquiring query trajectories. QRT5 shows better sample efficiency in RL\nto achieve the same level of QA performance as the previous approach. It can\ngenerate reformulations with more readability based on query well-formedness\nevaluations and can generalize to out-of-sample data. Our framework is\ndemonstrated to be flexible, allowing reward signals to be sourced from\ndifferent downstream environments such as intent classification.", "published": "2020-12-18 03:16:37", "link": "http://arxiv.org/abs/2012.10033v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Speaker Diarization as Post-Processing", "abstract": "This paper investigates the utilization of an end-to-end diarization model as\npost-processing of conventional clustering-based diarization. Clustering-based\ndiarization methods partition frames into clusters of the number of speakers;\nthus, they typically cannot handle overlapping speech because each frame is\nassigned to one speaker. On the other hand, some end-to-end diarization methods\ncan handle overlapping speech by treating the problem as multi-label\nclassification. Although some methods can treat a flexible number of speakers,\nthey do not perform well when the number of speakers is large. To compensate\nfor each other's weakness, we propose to use a two-speaker end-to-end\ndiarization method as post-processing of the results obtained by a\nclustering-based method. We iteratively select two speakers from the results\nand update the results of the two speakers to improve the overlapped region.\nExperimental results show that the proposed algorithm consistently improved the\nperformance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II\ndatasets.", "published": "2020-12-18 05:31:07", "link": "http://arxiv.org/abs/2012.10055v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Modality Bias in the TVQA Dataset", "abstract": "TVQA is a large scale video question answering (video-QA) dataset based on\npopular TV shows. The questions were specifically designed to require \"both\nvision and language understanding to answer\". In this work, we demonstrate an\ninherent bias in the dataset towards the textual subtitle modality. We infer\nsaid bias both directly and indirectly, notably finding that models trained\nwith subtitles learn, on-average, to suppress video feature contribution. Our\nresults demonstrate that models trained on only the visual information can\nanswer ~45% of the questions, while using only the subtitles achieves ~68%. We\nfind that a bilinear pooling based joint representation of modalities damages\nmodel performance by 9% implying a reliance on modality specific information.\nWe also show that TVQA fails to benefit from the RUBi modality bias reduction\ntechnique popularised in VQA. By simply improving text processing using BERT\nembeddings with the simple model first proposed for TVQA, we achieve\nstate-of-the-art results (72.13%) compared to the highly complex STAGE model\n(70.50%). We recommend a multimodal evaluation framework that can highlight\nbiases in models and isolate visual and textual reliant subsets of data. Using\nthis framework we propose subsets of TVQA that respond exclusively to either or\nboth modalities in order to facilitate multimodal modelling as TVQA originally\nintended.", "published": "2020-12-18 13:06:23", "link": "http://arxiv.org/abs/2012.10210v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T99", "I.2.10; I.2.7; I.2.4"], "primary_category": "cs.CV"}
{"title": "A Benchmark Arabic Dataset for Commonsense Explanation", "abstract": "Language comprehension and commonsense knowledge validation by machines are\nchallenging tasks that are still under researched and evaluated for Arabic\ntext. In this paper, we present a benchmark Arabic dataset for commonsense\nexplanation. The dataset consists of Arabic sentences that does not make sense\nalong with three choices to select among them the one that explains why the\nsentence is false. Furthermore, this paper presents baseline results to assist\nand encourage the future evaluation of research in this field. The dataset is\ndistributed under the Creative Commons CC-BY-SA 4.0 license and can be found on\nGitHub", "published": "2020-12-18 14:07:10", "link": "http://arxiv.org/abs/2012.10251v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection", "abstract": "Hate speech is a challenging issue plaguing the online social media. While\nbetter models for hate speech detection are continuously being developed, there\nis little research on the bias and interpretability aspects of hate speech. In\nthis paper, we introduce HateXplain, the first benchmark hate speech dataset\ncovering multiple aspects of the issue. Each post in our dataset is annotated\nfrom three different perspectives: the basic, commonly used 3-class\nclassification (i.e., hate, offensive or normal), the target community (i.e.,\nthe community that has been the victim of hate speech/offensive speech in the\npost), and the rationales, i.e., the portions of the post on which their\nlabelling decision (as hate, offensive or normal) is based. We utilize existing\nstate-of-the-art models and observe that even models that perform very well in\nclassification do not score high on explainability metrics like model\nplausibility and faithfulness. We also observe that models, which utilize the\nhuman rationales for training, perform better in reducing unintended bias\ntowards target communities. We have made our code and dataset public at\nhttps://github.com/punyajoy/HateXplain", "published": "2020-12-18 15:12:14", "link": "http://arxiv.org/abs/2012.10289v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Toward Streaming ASR with Non-Autoregressive Insertion-based Model", "abstract": "Neural end-to-end (E2E) models have become a promising technique to realize\npractical automatic speech recognition (ASR) systems. When realizing such a\nsystem, one important issue is the segmentation of audio to deal with streaming\ninput or long recording. After audio segmentation, the ASR model with a small\nreal-time factor (RTF) is preferable because the latency of the system can be\nfaster. Recently, E2E ASR based on non-autoregressive models becomes a\npromising approach since it can decode an $N$-length token sequence with less\nthan $N$ iterations. We propose a system to concatenate audio segmentation and\nnon-autoregressive ASR to realize high accuracy and low RTF ASR. As a\nnon-autoregressive ASR, the insertion-based model is used. In addition, instead\nof concatenating separated models for segmentation and ASR, we introduce a new\narchitecture that realizes audio segmentation and non-autoregressive ASR by a\nsingle neural network. Experimental results on Japanese and English dataset\nshow that the method achieved a reasonable trade-off between accuracy and RTF\ncompared with baseline autoregressive Transformer and connectionist temporal\nclassification.", "published": "2020-12-18 09:37:13", "link": "http://arxiv.org/abs/2012.10128v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Resource-efficient DNNs for Keyword Spotting using Neural Architecture\n  Search and Quantization", "abstract": "This paper introduces neural architecture search (NAS) for the automatic\ndiscovery of small models for keyword spotting (KWS) in limited resource\nenvironments. We employ a differentiable NAS approach to optimize the structure\nof convolutional neural networks (CNNs) to maximize the classification accuracy\nwhile minimizing the number of operations per inference. Using NAS only, we\nwere able to obtain a highly efficient model with 95.4% accuracy on the Google\nspeech commands dataset with 494.8 kB of memory usage and 19.6 million\noperations. Additionally, weight quantization is used to reduce the memory\nconsumption even further. We show that weight quantization to low bit-widths\n(e.g. 1 bit) can be used without substantial loss in accuracy. By increasing\nthe number of input features from 10 MFCC to 20 MFCC we were able to increase\nthe accuracy to 96.3% at 340.1 kB of memory usage and 27.1 million operations.", "published": "2020-12-18 09:53:55", "link": "http://arxiv.org/abs/2012.10138v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
