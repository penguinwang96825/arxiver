{"title": "Deep Learning for Options Trading: An End-To-End Approach", "abstract": "We introduce a novel approach to options trading strategies using a highly\nscalable and data-driven machine learning algorithm. In contrast to traditional\napproaches that often require specifications of underlying market dynamics or\nassumptions on an option pricing model, our models depart fundamentally from\nthe need for these prerequisites, directly learning non-trivial mappings from\nmarket data to optimal trading signals. Backtesting on more than a decade of\noption contracts for equities listed on the S&P 100, we demonstrate that deep\nlearning models trained according to our end-to-end approach exhibit\nsignificant improvements in risk-adjusted performance over existing rules-based\ntrading strategies. We find that incorporating turnover regularization into the\nmodels leads to further performance enhancements at prohibitively high levels\nof transaction costs.", "published": "2024-07-31 17:59:09", "link": "http://arxiv.org/abs/2407.21791v1", "categories": ["q-fin.PM", "cs.LG", "q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.PM"}
{"title": "Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning", "abstract": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.", "published": "2024-07-31 00:56:09", "link": "http://arxiv.org/abs/2407.21264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance of Recent Large Language Models for a Low-Resourced Language", "abstract": "Large Language Models (LLMs) have shown significant advances in the past\nyear. In addition to new versions of GPT and Llama, several other LLMs have\nbeen introduced recently. Some of these are open models available for download\nand modification.\n  Although multilingual large language models have been available for some\ntime, their performance on low-resourced languages such as Sinhala has been\npoor. We evaluated four recent LLMs on their performance directly in the\nSinhala language, and by translation to and from English. We also evaluated\ntheir fine-tunability with a small amount of fine-tuning data. Claude and GPT\n4o perform well out-of-the-box and do significantly better than previous\nversions. Llama and Mistral perform poorly but show some promise of improvement\nwith fine tuning.", "published": "2024-07-31 04:38:07", "link": "http://arxiv.org/abs/2407.21330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in\n  Language Models", "abstract": "Modern language models (LMs) need to follow human instructions while being\nfaithful; yet, they often fail to achieve both. Here, we provide concrete\nevidence of a trade-off between instruction following (i.e., follow open-ended\ninstructions) and faithfulness (i.e., ground responses in given context) when\ntraining LMs with these objectives. For instance, fine-tuning LLaMA-7B on\ninstruction following datasets renders it less faithful. Conversely,\ninstruction-tuned Vicuna-7B shows degraded performance at following\ninstructions when further optimized on tasks that require contextual grounding.\nOne common remedy is multi-task learning (MTL) with data mixing, yet it remains\nfar from achieving a synergic outcome. We propose a simple yet effective method\nthat relies on Rejection Sampling for Continued Self-instruction Tuning\n(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find\nthat less is more, as training ReSet with high-quality, yet substantially\nsmaller data (three-fold less) yields superior results. Our findings offer a\nbetter understanding of objective discrepancies in alignment training of LMs.", "published": "2024-07-31 08:05:04", "link": "http://arxiv.org/abs/2407.21417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuestGen: Effectiveness of Question Generation Methods for Fact-Checking\n  Applications", "abstract": "Verifying fact-checking claims poses a significant challenge, even for\nhumans. Recent approaches have demonstrated that decomposing claims into\nrelevant questions to gather evidence enhances the efficiency of the\nfact-checking process. In this paper, we provide empirical evidence showing\nthat this question decomposition can be effectively automated. We demonstrate\nthat smaller generative models, fine-tuned for the question generation task\nusing data augmentation from various datasets, outperform large language models\nby up to 8%. Surprisingly, in some cases, the evidence retrieved using\nmachine-generated questions proves to be significantly more effective for\nfact-checking than that obtained from human-written questions. We also perform\nmanual evaluation of the decomposed questions to assess the quality of the\nquestions generated.", "published": "2024-07-31 08:44:29", "link": "http://arxiv.org/abs/2407.21441v2", "categories": ["cs.CL", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment\n  Dynamics for Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition in conversation (MERC) has garnered\nsubstantial research attention recently. Existing MERC methods face several\nchallenges: (1) they fail to fully harness direct inter-modal cues, possibly\nleading to less-than-thorough cross-modal modeling; (2) they concurrently\nextract information from the same and different modalities at each network\nlayer, potentially triggering conflicts from the fusion of multi-source data;\n(3) they lack the agility required to detect dynamic sentimental changes,\nperhaps resulting in inaccurate classification of utterances with abrupt\nsentiment shifts. To address these issues, a novel approach named GraphSmile is\nproposed for tracking intricate emotional cues in multimodal dialogues.\nGraphSmile comprises two key components, i.e., GSF and SDP modules. GSF\ningeniously leverages graph structures to alternately assimilate inter-modal\nand intra-modal emotional dependencies layer by layer, adequately capturing\ncross-modal cues while effectively circumventing fusion conflicts. SDP is an\nauxiliary task to explicitly delineate the sentiment dynamics between\nutterances, promoting the model's ability to distinguish sentimental\ndiscrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal\nsentiment analysis in conversation (MSAC), forging a unified multimodal\naffective model capable of executing MERC and MSAC tasks. Empirical results on\nmultiple benchmarks demonstrate that GraphSmile can handle complex emotional\nand sentimental patterns, significantly outperforming baseline models.", "published": "2024-07-31 11:47:36", "link": "http://arxiv.org/abs/2407.21536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization\n  Methods", "abstract": "Authorship obfuscation aims to disguise the identity of an author within a\ntext by altering the writing style, vocabulary, syntax, and other linguistic\nfeatures associated with the text author. This alteration needs to balance\nprivacy and utility. While strong obfuscation techniques can effectively hide\nthe author's identity, they often degrade the quality and usefulness of the\ntext for its intended purpose. Conversely, maintaining high utility tends to\nprovide insufficient privacy, making it easier for an adversary to de-anonymize\nthe author. Thus, achieving an optimal trade-off between these two conflicting\nobjectives is crucial. In this paper, we propose TAROT: Task-Oriented\nAuthorship Obfuscation Using Policy Optimization, a new unsupervised authorship\nobfuscation method whose goal is to optimize the privacy-utility trade-off by\nregenerating the entire text considering its downstream utility. Our approach\nleverages policy optimization as a fine-tuning paradigm over small language\nmodels in order to rewrite texts by preserving author identity and downstream\ntask utility. We show that our approach largely reduces the accuracy of\nattackers while preserving utility. We make our code and models publicly\navailable.", "published": "2024-07-31 14:24:01", "link": "http://arxiv.org/abs/2407.21630v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank\n  Adaptation", "abstract": "Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to\ntransition to unfamiliar domains without manual annotation or extensive\nretraining. Prior research has approached this objective by embedding prompts\ninto language models (LMs). Common methodologies include integrating prompts at\nthe input layer or introducing learnable variables at each transformer layer.\nNonetheless, each strategy exhibits inherent limitations. Prompts integrated at\nthe input layer risk underutilization, with their impact potentially\ndiminishing across successive transformer layers. Conversely, the addition of\nlearnable variables to each layer can complicate the training process and\nincrease inference latency. To tackle the issues mentioned above, this paper\nproposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture\ndesigned for zero-shot DST. DualLoRA incorporates two distinct Low-Rank\nAdaptation (LoRA) components, targeting both dialogue context processing and\nprompt optimization, to ensure the comprehensive influence of prompts\nthroughout the transformer model layers. This is achieved without incurring\nadditional inference latency, showcasing an efficient integration into existing\narchitectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,\nDualLoRA demonstrates notable improvements across multiple domains,\noutperforming traditional baseline methods in zero-shot settings. Our code is\naccessible at: \\url{https://github.com/suntea233/DualLoRA}.", "published": "2024-07-31 14:26:41", "link": "http://arxiv.org/abs/2407.21633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-modality Information Check for Detecting Jailbreaking in\n  Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) extend the capacity of LLMs to\nunderstand multimodal information comprehensively, achieving remarkable\nperformance in many vision-centric tasks. Despite that, recent studies have\nshown that these models are susceptible to jailbreak attacks, which refer to an\nexploitative technique where malicious users can break the safety alignment of\nthe target model and generate misleading and harmful answers. This potential\nthreat is caused by both the inherent vulnerabilities of LLM and the larger\nattack scope introduced by vision input. To enhance the security of MLLMs\nagainst jailbreak attacks, researchers have developed various defense\ntechniques. However, these methods either require modifications to the model's\ninternal structure or demand significant computational resources during the\ninference phase. Multimodal information is a double-edged sword. While it\nincreases the risk of attacks, it also provides additional data that can\nenhance safeguards. Inspired by this, we propose Cross-modality Information\nDEtectoR (CIDER), a plug-and-play jailbreaking detector designed to identify\nmaliciously perturbed image inputs, utilizing the cross-modal similarity\nbetween harmful queries and adversarial images. CIDER is independent of the\ntarget MLLMs and requires less computation cost. Extensive experimental results\ndemonstrate the effectiveness and efficiency of CIDER, as well as its\ntransferability to both white-box and black-box MLLMs.", "published": "2024-07-31 15:02:46", "link": "http://arxiv.org/abs/2407.21659v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Course Shared Task on Evaluating LLM Output for Clinical Questions", "abstract": "This paper presents a shared task that we organized at the Foundations of\nLanguage Technology (FoLT) course in 2023/2024 at the Technical University of\nDarmstadt, which focuses on evaluating the output of Large Language Models\n(LLMs) in generating harmful answers to health-related clinical questions. We\ndescribe the task design considerations and report the feedback we received\nfrom the students. We expect the task and the findings reported in this paper\nto be relevant for instructors teaching natural language processing (NLP) and\ndesigning course assignments.", "published": "2024-07-31 19:24:40", "link": "http://arxiv.org/abs/2408.00122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented\n  Generation", "abstract": "This paper addresses the need for improved precision in existing\nknowledge-enhanced question-answering frameworks, specifically\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395% F1 gain by improving its performance from 0.1636 to 0.8109.", "published": "2024-07-31 01:51:24", "link": "http://arxiv.org/abs/2407.21276v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances", "abstract": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%).", "published": "2024-07-31 03:53:14", "link": "http://arxiv.org/abs/2407.21315v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided\n  Attention for Enhanced Document-level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) aims to extract relations between\nentities from unstructured document text. Compared to sentence-level relation\nextraction, it requires more complex semantic understanding from a broader text\ncontext. Currently, some studies are utilizing logical rules within evidence\nsentences to enhance the performance of DocRE. However, in the data without\nprovided evidence sentences, researchers often obtain a list of evidence\nsentences for the entire document through evidence retrieval (ER). Therefore,\nDocRE suffers from two challenges: firstly, the relevance between evidence and\nentity pairs is weak; secondly, there is insufficient extraction of complex\ncross-relations between long-distance multi-entities. To overcome these\nchallenges, we propose GEGA, a novel model for DocRE. The model leverages graph\nneural networks to construct multiple weight matrices, guiding attention\nallocation to evidence sentences. It also employs multi-scale representation\naggregation to enhance ER. Subsequently, we integrate the most efficient\nevidence information to implement both fully supervised and weakly supervised\ntraining processes for the model. We evaluate the GEGA model on three widely\nused benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The\nexperimental results indicate that our model has achieved comprehensive\nimprovements compared to the existing SOTA model.", "published": "2024-07-31 07:15:33", "link": "http://arxiv.org/abs/2407.21384v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards interfacing large language models with ASR systems using\n  confidence measures and prompting", "abstract": "As large language models (LLMs) grow in parameter size and capabilities, such\nas interaction through prompting, they open up new ways of interfacing with\nautomatic speech recognition (ASR) systems beyond rescoring n-best lists. This\nwork investigates post-hoc correction of ASR transcripts with LLMs. To avoid\nintroducing errors into likely accurate transcripts, we propose a range of\nconfidence-based filtering methods. Our results indicate that this can improve\nthe performance of less competitive ASR systems.", "published": "2024-07-31 08:00:41", "link": "http://arxiv.org/abs/2407.21414v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improving Faithfulness of Large Language Models in Summarization via\n  Sliding Generation and Self-Consistency", "abstract": "Despite large language models (LLMs) have demonstrated impressive performance\nin various tasks, they are still suffering from the factual inconsistency\nproblem called hallucinations. For instance, LLMs occasionally generate content\nthat diverges from source article, and prefer to extract information that\nappears at the beginning and end of the context, especially in long document\nsummarization. Inspired by these findings, we propose to improve the\nfaithfulness of LLMs in summarization by impelling them to process the entire\narticle more fairly and faithfully. We present a novel summary generation\nstrategy, namely SliSum, which exploits the ideas of sliding windows and\nself-consistency. Specifically, SliSum divides the source article into\noverlapping windows, and utilizes LLM to generate local summaries for the\ncontent in the windows. Finally, SliSum aggregates all local summaries using\nclustering and majority voting algorithm to produce more faithful summary of\nentire article. Extensive experiments demonstrate that SliSum significantly\nimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and\nGPT-3.5 in both short and long text summarization, while maintaining their\nfluency and informativeness and without additional fine-tuning and resources.\nWe further conduct qualitative and quantitative studies to investigate why\nSliSum works and impacts of hyperparameters in SliSum on performance.", "published": "2024-07-31 08:48:48", "link": "http://arxiv.org/abs/2407.21443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent\n  Trends", "abstract": "Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref.", "published": "2024-07-31 09:58:48", "link": "http://arxiv.org/abs/2407.21489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Contamination Report from the 2024 CONDA Shared Task", "abstract": "The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.", "published": "2024-07-31 11:26:57", "link": "http://arxiv.org/abs/2407.21530v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Sentiment Analysis via Latent Category Distribution and\n  Constrained Decoding", "abstract": "Fine-grained sentiment analysis involves extracting and organizing sentiment\nelements from textual data. However, existing approaches often overlook issues\nof category semantic inclusion and overlap, as well as inherent structural\npatterns within the target sequence. This study introduces a generative\nsentiment analysis model. To address the challenges related to category\nsemantic inclusion and overlap, a latent category distribution variable is\nintroduced. By reconstructing the input of a variational autoencoder, the model\nlearns the intensity of the relationship between categories and text, thereby\nimproving sequence generation. Additionally, a trie data structure and\nconstrained decoding strategy are utilized to exploit structural patterns,\nwhich in turn reduces the search space and regularizes the generation process.\nExperimental results on the Restaurant-ACOS and Laptop-ACOS datasets\ndemonstrate a significant performance improvement compared to baseline models.\nAblation experiments further confirm the effectiveness of latent category\ndistribution and constrained decoding strategy.", "published": "2024-07-31 12:29:17", "link": "http://arxiv.org/abs/2407.21560v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PMoE: Progressive Mixture of Experts with Asymmetric Transformer for\n  Continual Learning", "abstract": "Large Language Models (LLMs) encounter significant challenges in continual\nlearning due to catastrophic forgetting, where new information overwrites\npreviously acquired knowledge. This limitation leads to substantial\nenvironmental and economic waste. In this study, we introduce the PMoE,\nProgressive Mixture of Experts with Asymmetric Transformer, which aims to\nminimize forgetting by utilizing an asymmetric design with shallow layers\ndedicated to general knowledge and deep layers for new knowledge. PMoE\nincorporates progressively added experts in deep layers and a router that\nallocates new knowledge to the appropriate experts efficiently. The router,\npositioned adjacent to the deep layers, utilizes deep features aggregating\nconsolidated information. This enables the router to perform efficiently,\nallocating new knowledge to the appropriate experts, which progressively\nincrease in the deep layers. Extensive experiments on TRACE datasets and\ngeneral language understanding datasets demonstrate that the proposed PMoE\noutperforms previous state-of-the-art approaches.", "published": "2024-07-31 12:56:14", "link": "http://arxiv.org/abs/2407.21571v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synth-Empathy: Towards High-Quality Synthetic Empathy Data", "abstract": "In recent years, with the rapid advancements in large language models (LLMs),\nachieving excellent empathetic response capabilities has become a crucial\nprerequisite. Consequently, managing and understanding empathetic datasets have\ngained increasing significance. However, empathetic data are typically\nhuman-labeled, leading to insufficient datasets and wasted human labor. In this\nwork, we present Synth-Empathy, an LLM-based data generation and quality and\ndiversity selection pipeline that automatically generates high-quality\nempathetic data while discarding low-quality data. With the data generated from\na low empathetic model, we are able to further improve empathetic response\nperformance and achieve state-of-the-art (SoTA) results across multiple\nbenchmarks. Moreover, our model achieves SoTA performance on various human\nevaluation benchmarks, demonstrating its effectiveness and robustness in\nreal-world applications. Furthermore, we show the trade-off between data\nquantity and quality, providing insights into empathetic data generation and\nselection.", "published": "2024-07-31 15:12:24", "link": "http://arxiv.org/abs/2407.21669v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Retrieval-Augmented Generation for Conversational Systems", "abstract": "Despite the success of integrating large language models into the development\nof conversational systems, many studies have shown the effectiveness of\nretrieving and augmenting external knowledge for informative responses. Hence,\nmany existing studies commonly assume the always need for Retrieval Augmented\nGeneration (RAG) in a conversational system without explicit control. This\nraises a research question about such a necessity. In this study, we propose to\ninvestigate the need for each turn of system response to be augmented with\nexternal knowledge. In particular, by leveraging human judgements on the binary\nchoice of adaptive augmentation, we develop RAGate, a gating model, which\nmodels conversation context and relevant inputs to predict if a conversational\nsystem requires RAG for improved responses. We conduct extensive experiments on\ndevising and applying RAGate to conversational models and well-rounded analyses\nof different conversational scenarios. Our experimental results and analysis\nindicate the effective application of RAGate in RAG-based conversational\nsystems in identifying system responses for appropriate RAG with high-quality\nresponses and a high generation confidence. This study also identifies the\ncorrelation between the generation's confidence level and the relevance of the\naugmented knowledge.", "published": "2024-07-31 16:04:03", "link": "http://arxiv.org/abs/2407.21712v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ShieldGemma: Generative AI Content Moderation Based on Gemma", "abstract": "We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.", "published": "2024-07-31 17:48:14", "link": "http://arxiv.org/abs/2407.21772v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation\n  Extraction on an Academic Budget", "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.", "published": "2024-07-31 18:25:49", "link": "http://arxiv.org/abs/2408.00103v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gemma 2: Improving Open Language Models at a Practical Size", "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.", "published": "2024-07-31 19:13:07", "link": "http://arxiv.org/abs/2408.00118v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correcting Negative Bias in Large Language Models through Negative\n  Attention Score Alignment", "abstract": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities. Our codes are available\nat \\url{https://github.com/ysw1021/NASA}.", "published": "2024-07-31 19:50:57", "link": "http://arxiv.org/abs/2408.00137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distributed In-Context Learning under Non-IID Among Clients", "abstract": "Advancements in large language models (LLMs) have shown their effectiveness\nin multiple complicated natural language reasoning tasks. A key challenge\nremains in adapting these models efficiently to new or unfamiliar tasks.\nIn-context learning (ICL) provides a promising solution for few-shot adaptation\nby retrieving a set of data points relevant to a query, called in-context\nexamples (ICE), from a training dataset and providing them during the inference\nas context. Most existing studies utilize a centralized training dataset, yet\nmany real-world datasets may be distributed among multiple clients, and remote\ndata retrieval can be associated with costs. Especially when the client data\nare non-identical independent distributions (non-IID), retrieving from clients\na proper set of ICEs needed for a test query presents critical challenges. In\nthis paper, we first show that in this challenging setting, test queries will\nhave different preferences among clients because of non-IIDness, and equal\ncontribution often leads to suboptimal performance. We then introduce a novel\napproach to tackle the distributed non-IID ICL problem when a data usage budget\nis present. The principle is that each client's proper contribution (budget)\nshould be designed according to the preference of each query for that client.\nOur approach uses a data-driven manner to allocate a budget for each client,\ntailored to each test query. Through extensive empirical studies on diverse\ndatasets, our framework demonstrates superior performance relative to competing\nbaselines.", "published": "2024-07-31 20:06:25", "link": "http://arxiv.org/abs/2408.00144v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LADDER: Language Driven Slice Discovery and Error Rectification", "abstract": "Error slice discovery is crucial to diagnose and mitigate model errors.\nCurrent clustering or discrete attribute-based slice discovery methods face key\nlimitations: 1) clustering results in incoherent slices, while assigning\ndiscrete attributes to slices leads to incomplete coverage of error patterns\ndue to missing or insufficient attributes; 2) these methods lack complex\nreasoning, preventing them from fully explaining model biases; 3) they fail to\nintegrate \\textit{domain knowledge}, limiting their usage in specialized fields\n\\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven\n\\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to\naddress the limitations by: (1) leveraging the flexibility of natural language\nto address incompleteness, (2) employing LLM's latent \\textit{domain knowledge}\nand advanced reasoning to analyze sentences and derive testable hypotheses\ndirectly, identifying biased attributes, and form coherent error slices without\nclustering. Existing mitigation methods typically address only the\nworst-performing group, often amplifying errors in other subgroups. In\ncontrast,\\ladder generates pseudo attributes from the discovered hypotheses to\nmitigate errors across all biases without explicit attribute annotations or\nprior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural\nand medical images -- comparing 200+ classifiers with diverse architectures,\npretraining strategies, and LLMs -- show that\\ladder consistently outperforms\nexisting baselines in discovering and mitigating biases.", "published": "2024-07-31 14:49:35", "link": "http://arxiv.org/abs/2408.07832v9", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.", "published": "2024-07-31 06:34:38", "link": "http://arxiv.org/abs/2407.21368v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cost-Effective Hallucination Detection for LLMs", "abstract": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.", "published": "2024-07-31 08:19:06", "link": "http://arxiv.org/abs/2407.21424v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented\n  Generation via Knowledge-enhanced Reranking and Noise-injected Training", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in processing and generating content across multiple data\nmodalities. However, a significant drawback of MLLMs is their reliance on\nstatic training data, leading to outdated information and limited contextual\nawareness. This static nature hampers their ability to provide accurate and\nup-to-date responses, particularly in dynamic or rapidly evolving contexts.\nThough integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)\noffers a promising solution, the system would inevitably encounter the\nmulti-granularity noisy correspondence (MNC) problem, which hinders accurate\nretrieval and generation. In this work, we propose RagVL, a novel framework\nwith knowledge-enhanced reranking and noise-injected training, to address these\nlimitations. We instruction-tune the MLLM with a simple yet effective\ninstruction template to induce its ranking ability and serve it as a reranker\nto precisely filter the top-k retrieved images. For generation, we inject\nvisual noise during training at the data and token levels to enhance the\ngenerator's robustness. Extensive experiments on the subsets of two datasets\nthat require retrieving and reasoning over images to answer a given query\nverify the effectiveness of our method. Code and models are available at\nhttps://github.com/IDEA-FinAI/RagVL.", "published": "2024-07-31 08:43:17", "link": "http://arxiv.org/abs/2407.21439v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Navigating Beyond Instructions: Vision-and-Language Navigation in\n  Obstructed Environments", "abstract": "Real-world navigation often involves dealing with unexpected obstructions\nsuch as closed doors, moved objects, and unpredictable entities. However,\nmainstream Vision-and-Language Navigation (VLN) tasks typically assume\ninstructions perfectly align with the fixed and predefined navigation graphs\nwithout any obstructions. This assumption overlooks potential discrepancies in\nactual navigation graphs and given instructions, which can cause major failures\nfor both indoor and outdoor agents. To address this issue, we integrate diverse\nobstructions into the R2R dataset by modifying both the navigation graphs and\nvisual observations, introducing an innovative dataset and task, R2R with\nUNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers\nof path obstructions to generate instruction-reality mismatches for VLN\nresearch. Experiments on R2R-UNO reveal that state-of-the-art VLN methods\ninevitably encounter significant challenges when facing such mismatches,\nindicating that they rigidly follow instructions rather than navigate\nadaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),\nwhich includes a curriculum training strategy and virtual graph construction to\nhelp agents effectively adapt to obstructed environments. Empirical results\nshow that ObVLN not only maintains robust performance in unobstructed scenarios\nbut also achieves a substantial performance advantage with unexpected\nobstructions.", "published": "2024-07-31 08:55:57", "link": "http://arxiv.org/abs/2407.21452v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "On the Problem of Text-To-Speech Model Selection for Synthetic Data\n  Generation in Automatic Speech Recognition", "abstract": "The rapid development of neural text-to-speech (TTS) systems enabled its\nusage in other areas of natural language processing such as automatic speech\nrecognition (ASR) or spoken language translation (SLT). Due to the large number\nof different TTS architectures and their extensions, selecting which TTS\nsystems to use for synthetic data creation is not an easy task. We use the\ncomparison of five different TTS decoder architectures in the scope of\nsynthetic data generation to show the impact on CTC-based speech recognition\ntraining. We compare the recognition results to computable metrics like NISQA\nMOS and intelligibility, finding that there are no clear relations to the ASR\nperformance. We also observe that for data generation auto-regressive decoding\nperforms better than non-autoregressive decoding, and propose an approach to\nquantify TTS generalization capabilities.", "published": "2024-07-31 09:37:27", "link": "http://arxiv.org/abs/2407.21476v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generative Expressive Conversational Speech Synthesis", "abstract": "Conversational Speech Synthesis (CSS) aims to express a target utterance with\nthe proper speaking style in a user-agent conversation setting. Existing CSS\nmethods employ effective multi-modal context modeling techniques to achieve\nempathy understanding and expression. However, they often need to design\ncomplex network architectures and meticulously optimize the modules within\nthem. In addition, due to the limitations of small-scale datasets containing\nscripted recording styles, they often fail to simulate real natural\nconversational styles. To address the above issues, we propose a novel\ngenerative expressive CSS system, termed GPT-Talker.We transform the multimodal\ninformation of the multi-turn dialogue history into discrete token sequences\nand seamlessly integrate them to form a comprehensive user-agent dialogue\ncontext. Leveraging the power of GPT, we predict the token sequence, that\nincludes both semantic and style knowledge, of response for the agent. After\nthat, the expressive conversational speech is synthesized by the\nconversation-enriched VITS to deliver feedback to the user.Furthermore, we\npropose a large-scale Natural CSS Dataset called NCSSD, that includes both\nnaturally recorded conversational speech in improvised styles and dialogues\nextracted from TV shows. It encompasses both Chinese and English languages,\nwith a total duration of 236 hours.We conducted comprehensive experiments on\nthe reliability of the NCSSD and the effectiveness of our GPT-Talker. Both\nsubjective and objective evaluations demonstrate that our model outperforms\nother state-of-the-art CSS systems significantly in terms of naturalness and\nexpressiveness. The Code, Dataset, and Pre-trained Model are available at:\nhttps://github.com/AI-S2-Lab/GPT-Talker.", "published": "2024-07-31 10:02:21", "link": "http://arxiv.org/abs/2407.21491v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interpreting and learning voice commands with a Large Language Model for\n  a robot system", "abstract": "Robots are increasingly common in industry and daily life, such as in nursing\nhomes where they can assist staff. A key challenge is developing intuitive\ninterfaces for easy communication. The use of Large Language Models (LLMs) like\nGPT-4 has enhanced robot capabilities, allowing for real-time interaction and\ndecision-making. This integration improves robots' adaptability and\nfunctionality. This project focuses on merging LLMs with databases to improve\ndecision-making and enable knowledge acquisition for request interpretation\nproblems.", "published": "2024-07-31 10:30:31", "link": "http://arxiv.org/abs/2407.21512v1", "categories": ["cs.RO", "cs.CL", "cs.NE"], "primary_category": "cs.RO"}
{"title": "Can LLMs \"Reason\" in Music? An Evaluation of LLMs' Capability of Music\n  Understanding and Generation", "abstract": "Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians.", "published": "2024-07-31 11:29:46", "link": "http://arxiv.org/abs/2407.21531v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech\n  Translation via LLM Agent", "abstract": "In this paper, we present Cross Language Agent -- Simultaneous\nInterpretation, CLASI, a high-quality and human-like Simultaneous Speech\nTranslation (SiST) System. Inspired by professional human interpreters, we\nutilize a novel data-driven read-write strategy to balance the translation\nquality and latency. To address the challenge of translating in-domain\nterminologies, CLASI employs a multi-modal retrieving module to obtain relevant\ninformation to augment the translation. Supported by LLMs, our approach can\ngenerate error-tolerated translation by considering the input audio, historical\ncontext, and retrieved information. Experimental results show that our system\noutperforms other systems by significant margins. Aligned with professional\nhuman interpreters, we evaluate CLASI with a better human evaluation metric,\nvalid information proportion (VIP), which measures the amount of information\nthat can be successfully conveyed to the listeners. In the real-world\nscenarios, where the speeches are often disfluent, informal, and unclear, CLASI\nachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese\ntranslation directions, respectively. In contrast, state-of-the-art commercial\nor open-source systems only achieve 35.4% and 41.6%. On the extremely hard\ndataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%\nVIP.", "published": "2024-07-31 14:48:27", "link": "http://arxiv.org/abs/2407.21646v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Characterizing User Archetypes and Discussions on Scored.co", "abstract": "In recent years, the proliferation of social platforms has drastically\ntransformed the way individuals interact, organize, and share information. In\nthis scenario, we experience an unprecedented increase in the scale and\ncomplexity of interactions and, at the same time, little to no research about\nsome fringe social platforms. In this paper, we present a multi-dimensional\nframework for characterizing nodes and hyperedges in social hypernetworks, with\na focus on the understudied alt-right platform Scored.co. Our approach\nintegrates the possibility of studying higher-order interactions, thanks to the\nhypernetwork representation, and various node features such as user activity,\nsentiment, and toxicity, with the aim to define distinct user archetypes and\nunderstand their roles within the network. Utilizing a comprehensive dataset\nfrom Scored.co, we analyze the dynamics of these archetypes over time and\nexplore their interactions and influence within the community. The framework's\nversatility allows for detailed analysis of both individual user behaviors and\nbroader social structures. Our findings highlight the importance of\nhigher-order interactions in understanding social dynamics, offering new\ninsights into the roles and behaviors that emerge in complex online\nenvironments.", "published": "2024-07-31 17:18:25", "link": "http://arxiv.org/abs/2407.21753v2", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "The Llama 3 Herd of Models", "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.", "published": "2024-07-31 17:54:27", "link": "http://arxiv.org/abs/2407.21783v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Vision-Language Model Based Handwriting Verification", "abstract": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.", "published": "2024-07-31 17:57:32", "link": "http://arxiv.org/abs/2407.21788v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?", "abstract": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with both upstream model capabilities and training\ncompute, potentially enabling \"safetywashing\"--where capability improvements\nare misrepresented as safety advancements. Based on these findings, we propose\nan empirical foundation for developing more meaningful safety metrics and\ndefine AI safety in a machine learning research context as a set of clearly\ndelineated research goals that are empirically separable from generic\ncapabilities advancements. In doing so, we aim to provide a more rigorous\nframework for AI safety research, advancing the science of safety evaluations\nand clarifying the path towards measurable progress.", "published": "2024-07-31 17:59:24", "link": "http://arxiv.org/abs/2407.21792v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Measuring Progress in Dictionary Learning for Language Model\n  Interpretability with Board Game Models", "abstract": "What latent features are encoded in language model (LM) representations?\nRecent work on training sparse autoencoders (SAEs) to disentangle interpretable\nfeatures in LM representations has shown significant promise. However,\nevaluating the quality of these SAEs is difficult because we lack a\nground-truth collection of interpretable features that we expect good SAEs to\nrecover. We thus propose to measure progress in interpretable dictionary\nlearning by working in the setting of LMs trained on chess and Othello\ntranscripts. These settings carry natural collections of interpretable features\n-- for example, \"there is a knight on F3\" -- which we leverage into\n$\\textit{supervised}$ metrics for SAE quality. To guide progress in\ninterpretable dictionary learning, we introduce a new SAE training technique,\n$\\textit{p-annealing}$, which improves performance on prior unsupervised\nmetrics as well as our new metrics.", "published": "2024-07-31 18:45:13", "link": "http://arxiv.org/abs/2408.00113v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting", "abstract": "Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.", "published": "2024-07-31 21:12:21", "link": "http://arxiv.org/abs/2408.00161v2", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Taxonomy of Stereotype Content in Large Language Models", "abstract": "This study introduces a taxonomy of stereotype content in contemporary large\nlanguage models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three\npowerful and widely used LLMs, for the characteristics associated with 87\nsocial categories (e.g., gender, race, occupations). We identify 14 stereotype\ndimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for\n~90% of LLM stereotype associations. Warmth and Competence facets were the most\nfrequent content, but all other dimensions were significantly prevalent.\nStereotypes were more positive in LLMs (vs. humans), but there was significant\nvariability across categories and dimensions. Finally, the taxonomy predicted\nthe LLMs' internal evaluations of social categories (e.g., how\npositively/negatively the categories were represented), supporting the\nrelevance of a multidimensional taxonomy for characterizing LLM stereotypes.\nOur findings suggest that high-dimensional human stereotypes are reflected in\nLLMs and must be considered in AI auditing and debiasing to minimize\nunidentified harms from reliance in low-dimensional views of bias in LLMs.", "published": "2024-07-31 21:14:41", "link": "http://arxiv.org/abs/2408.00162v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Automated Software Vulnerability Static Code Analysis Using Generative\n  Pre-Trained Transformer Models", "abstract": "Generative Pre-Trained Transformer models have been shown to be surprisingly\neffective at a variety of natural language processing tasks -- including\ngenerating computer code. We evaluate the effectiveness of open source GPT\nmodels for the task of automatic identification of the presence of vulnerable\ncode syntax (specifically targeting C and C++ source code). This task is\nevaluated on a selection of 36 source code examples from the NIST SARD dataset,\nwhich are specifically curated to not contain natural English that indicates\nthe presence, or lack thereof, of a particular vulnerability. The NIST SARD\nsource code dataset contains identified vulnerable lines of source code that\nare examples of one out of the 839 distinct Common Weakness Enumerations (CWE),\nallowing for exact quantification of the GPT output classification error rate.\nA total of 5 GPT models are evaluated, using 10 different inference\ntemperatures and 100 repetitions at each setting, resulting in 5,000 GPT\nqueries per vulnerable source code analyzed. Ultimately, we find that the GPT\nmodels that we evaluated are not suitable for fully automated vulnerability\nscanning because the false positive and false negative rates are too high to\nlikely be useful in practice. However, we do find that the GPT models perform\nsurprisingly well at automated vulnerability detection for some of the test\ncases, in particular surpassing random sampling, and being able to identify the\nexact lines of code that are vulnerable albeit at a low success rate. The best\nperforming GPT model result found was Llama-2-70b-chat-hf with inference\ntemperature of 0.1 applied to NIST SARD test case 149165 (which is an example\nof a buffer overflow vulnerability), which had a binary classification recall\nscore of 1.0 and a precision of 1.0 for correctly and uniquely identifying the\nvulnerable line of code and the correct CWE number.", "published": "2024-07-31 23:33:26", "link": "http://arxiv.org/abs/2408.00197v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "GPT-3 Powered Information Extraction for Building Robust Knowledge Bases", "abstract": "This work uses the state-of-the-art language model GPT-3 to offer a novel\nmethod of information extraction for knowledge base development. The suggested\nmethod attempts to solve the difficulties associated with obtaining relevant\nentities and relationships from unstructured text in order to extract\nstructured information. We conduct experiments on a huge corpus of text from\ndiverse fields to assess the performance of our suggested technique. The\nevaluation measures, which are frequently employed in information extraction\ntasks, include precision, recall, and F1-score. The findings demonstrate that\nGPT-3 can be used to efficiently and accurately extract pertinent and correct\ninformation from text, hence increasing the precision and productivity of\nknowledge base creation. We also assess how well our suggested approach\nperforms in comparison to the most advanced information extraction techniques\nalready in use. The findings show that by utilizing only a small number of\ninstances in in-context learning, our suggested strategy yields competitive\noutcomes with notable savings in terms of data annotation and engineering\nexpense. Additionally, we use our proposed method to retrieve Biomedical\ninformation, demonstrating its practicality in a real-world setting. All things\nconsidered, our suggested method offers a viable way to overcome the\ndifficulties involved in obtaining structured data from unstructured text in\norder to create knowledge bases. It can greatly increase the precision and\neffectiveness of information extraction, which is necessary for many\napplications including chatbots, recommendation engines, and question-answering\nsystems.", "published": "2024-07-31 14:59:29", "link": "http://arxiv.org/abs/2408.04641v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards EMG-to-Speech with a Necklace Form Factor", "abstract": "Electrodes for decoding speech from electromyography (EMG) are typically\nplaced on the face, requiring adhesives that are inconvenient and\nskin-irritating if used regularly. We explore a different device form factor,\nwhere dry electrodes are placed around the neck instead. 11-word, multi-speaker\nvoiced EMG classifiers trained on data recorded with this device achieve 92.7%\naccuracy. Ablation studies reveal the importance of having more than two\nelectrodes on the neck, and phonological analyses reveal similar classification\nconfusions between neck-only and neck-and-face form factors. Finally,\nspeech-EMG correlation experiments demonstrate a linear relationship between\nmany EMG spectrogram frequency bins and self-supervised speech representation\ndimensions.", "published": "2024-07-31 05:28:50", "link": "http://arxiv.org/abs/2407.21345v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Robust Lossy Audio Compression Identification", "abstract": "Previous research contributions on blind lossy compression identification\nreport near perfect performance metrics on their test set, across a variety of\ncodecs and bit rates. However, we show that such results can be deceptive and\nmay not accurately represent true ability of the system to tackle the task at\nhand. In this article, we present an investigation into the robustness and\ngeneralisation capability of a lossy audio identification model. Our\ncontributions are as follows. (1) We show the lack of robustness to codec\nparameter variations of a model equivalent to prior art. In particular, when\nnaively training a lossy compression detection model on a dataset of music\nrecordings processed with a range of codecs and their lossless counterparts, we\nobtain near perfect performance metrics on the held-out test set, but severely\ndegraded performance on lossy tracks produced with codec parameters not seen in\ntraining. (2) We propose and show the effectiveness of an improved training\nstrategy to significantly increase the robustness and generalisation capability\nof the model beyond codec configurations seen during training. Namely we apply\na random mask to the input spectrogram to encourage the model not to rely\nsolely on the training set's codec cutoff frequency.", "published": "2024-07-31 12:09:31", "link": "http://arxiv.org/abs/2407.21545v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Design and Development of Laughter Recognition System Based on\n  Multimodal Fusion and Deep Learning", "abstract": "This study aims to design and implement a laughter recognition system based\non multimodal fusion and deep learning, leveraging image and audio processing\ntechnologies to achieve accurate laughter recognition and emotion analysis.\nFirst, the system loads video files and uses the OpenCV library to extract\nfacial information while employing the Librosa library to process audio\nfeatures such as MFCC. Then, multimodal fusion techniques are used to integrate\nimage and audio features, followed by training and prediction using deep\nlearning models. Evaluation results indicate that the model achieved 80%\naccuracy, precision, and recall on the test dataset, with an F1 score of 80%,\ndemonstrating robust performance and the ability to handle real-world data\nvariability. This study not only verifies the effectiveness of multimodal\nfusion methods in laughter recognition but also highlights their potential\napplications in affective computing and human-computer interaction. Future work\nwill focus on further optimizing feature extraction and model architecture to\nimprove recognition accuracy and expand application scenarios, promoting the\ndevelopment of laughter recognition technology in fields such as mental health\nmonitoring and educational activity evaluation", "published": "2024-07-31 07:31:13", "link": "http://arxiv.org/abs/2407.21391v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware\n  Attention Mechanism", "abstract": "The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.", "published": "2024-07-31 13:49:17", "link": "http://arxiv.org/abs/2407.21611v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and\n  Human-Composed Progressive Metal Music", "abstract": "Generative AI models have recently blossomed, significantly impacting\nartistic and musical traditions. Research investigating how humans interact\nwith and deem these models is therefore crucial. Through a listening and\nreflection study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format, using rock music as a\ncontrol group. AI-generated examples were produced by ProgGP, a\nTransformer-based model. We propose a mixed methods approach to assess the\neffects of generation type (human vs. AI), genre (progressive metal vs. rock),\nand curation process (random vs. cherry-picked). This combines quantitative\nfeedback on genre congruence, preference, creativity, consistency, playability,\nhumanness, and repeatability, and qualitative feedback to provide insights into\nlisteners' experiences. A total of 32 progressive metal fans completed the\nstudy. Our findings validate the use of fine-tuning to achieve genre-specific\nspecialization in AI music generation, as listeners could distinguish between\nAI-generated rock and progressive metal. Despite some AI-generated excerpts\nreceiving similar ratings to human music, listeners exhibited a preference for\nhuman compositions. Thematic analysis identified key features for genre and AI\nvs. human distinctions. Finally, we consider the ethical implications of our\nwork in promoting musical data diversity within MIR research by focusing on an\nunder-explored genre.", "published": "2024-07-31 14:03:45", "link": "http://arxiv.org/abs/2407.21615v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beat this! Accurate beat tracking without DBN postprocessing", "abstract": "We propose a system for tracking beats and downbeats with two objectives:\ngenerality across a diverse music range, and high accuracy. We achieve\ngenerality by training on multiple datasets -- including solo instrument\nrecordings, pieces with time signature changes, and classical music with high\ntempo variations -- and by removing the commonly used Dynamic Bayesian Network\n(DBN) postprocessing, which introduces constraints on the meter and tempo. For\nhigh accuracy, among other improvements, we develop a loss function tolerant to\nsmall time shifts of annotations, and an architecture alternating convolutions\nwith transformers either over frequency or time. Our system surpasses the\ncurrent state of the art in F1 score despite using no DBN. However, it can\nstill fail, especially for difficult and underrepresented genres, and performs\nworse on continuity metrics, so we publish our model, code, and preprocessed\ndatasets, and invite others to beat this.", "published": "2024-07-31 14:59:17", "link": "http://arxiv.org/abs/2407.21658v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Combining audio control and style transfer using latent diffusion", "abstract": "Deep generative models are now able to synthesize high-quality audio signals,\nshifting the critical aspect in their development from audio quality to control\ncapabilities. Although text-to-music generation is getting largely adopted by\nthe general public, explicit control and example-based style transfer are more\nadequate modalities to capture the intents of artists and musicians.\n  In this paper, we aim to unify explicit control and style transfer within a\nsingle model by separating local and global information to capture musical\nstructure and timbre respectively. To do so, we leverage the capabilities of\ndiffusion autoencoders to extract semantic features, in order to build two\nrepresentation spaces. We enforce disentanglement between those spaces using an\nadversarial criterion and a two-stage training strategy. Our resulting model\ncan generate audio matching a timbre target, while specifying structure either\nwith explicit controls or through another audio example. We evaluate our model\non one-shot timbre transfer and MIDI-to-audio tasks on instrumental recordings\nand show that we outperform existing baselines in terms of audio quality and\ntarget fidelity. Furthermore, we show that our method can generate cover\nversions of complete musical pieces by transferring rhythmic and melodic\ncontent to the style of a target audio in a different genre.", "published": "2024-07-31 23:27:27", "link": "http://arxiv.org/abs/2408.00196v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "TinyChirp: Bird Song Recognition Using TinyML Models on Low-power\n  Wireless Acoustic Sensors", "abstract": "Monitoring biodiversity at scale is challenging. Detecting and identifying\nspecies in fine grained taxonomies requires highly accurate machine learning\n(ML) methods. Training such models requires large high quality data sets. And\ndeploying these models to low power devices requires novel compression\ntechniques and model architectures. While species classification methods have\nprofited from novel data sets and advances in ML methods, in particular neural\nnetworks, deploying these state of the art models to low power devices remains\ndifficult. Here we present a comprehensive empirical comparison of various\ntinyML neural network architectures and compression techniques for species\nclassification. We focus on the example of bird song detection, more concretely\na data set curated for studying the corn bunting bird species. The data set is\nreleased along with all code and experiments of this study. In our experiments\nwe compare predictive performance, memory and time complexity of classical\nspectrogram based methods and recent approaches operating on raw audio signal.\nOur results indicate that individual bird species can be robustly detected with\nrelatively simple architectures that can be readily deployed to low power\ndevices.", "published": "2024-07-31 08:57:42", "link": "http://arxiv.org/abs/2407.21453v2", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
