{"title": "ISQA: Informative Factuality Feedback for Scientific Summarization", "abstract": "We propose Iterative Facuality Refining on Informative Scientific\nQuestion-Answering (ISQA) feedback\\footnote{Code is available at\n\\url{https://github.com/lizekai-richard/isqa}}, a method following human\nlearning theories that employs model-generated feedback consisting of both\npositive and negative information. Through iterative refining of summaries, it\nprobes for the underlying rationale of statements to enhance the factuality of\nscientific summarization. ISQA does this in a fine-grained manner by asking a\nsummarization agent to reinforce validated statements in positive feedback and\nfix incorrect ones in negative feedback. Our findings demonstrate that the ISQA\nfeedback mechanism significantly improves the factuality of various open-source\nLLMs on the summarization task, as evaluated across multiple scientific\ndatasets.", "published": "2024-04-20 03:16:13", "link": "http://arxiv.org/abs/2404.13246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE\n  Questions", "abstract": "GPT-4 demonstrates high accuracy in medical QA tasks, leading with an\naccuracy of 86.70%, followed by Med-PaLM 2 at 86.50%. However, around 14% of\nerrors remain. Additionally, current works use GPT-4 to only predict the\ncorrect option without providing any explanation and thus do not provide any\ninsight into the thinking process and reasoning used by GPT-4 or other LLMs.\nTherefore, we introduce a new domain-specific error taxonomy derived from\ncollaboration with medical students. Our GPT-4 USMLE Error (G4UE) dataset\ncomprises 4153 GPT-4 correct responses and 919 incorrect responses to the\nUnited States Medical Licensing Examination (USMLE) respectively. These\nresponses are quite long (258 words on average), containing detailed\nexplanations from GPT-4 justifying the selected option. We then launch a\nlarge-scale annotation study using the Potato annotation platform and recruit\n44 medical experts through Prolific, a well-known crowdsourcing platform. We\nannotated 300 out of these 919 incorrect data points at a granular level for\ndifferent classes and created a multi-label span to identify the reasons behind\nthe error. In our annotated dataset, a substantial portion of GPT-4's incorrect\nresponses is categorized as a \"Reasonable response by GPT-4,\" by annotators.\nThis sheds light on the challenge of discerning explanations that may lead to\nincorrect options, even among trained medical professionals. We also provide\nmedical concepts and medical semantic predications extracted using the SemRep\ntool for every data point. We believe that it will aid in evaluating the\nability of LLMs to answer complex medical questions. We make the resources\navailable at https://github.com/roysoumya/usmle-gpt4-error-taxonomy .", "published": "2024-04-20 07:29:06", "link": "http://arxiv.org/abs/2404.13307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Swa Bhasha: Message-Based Singlish to Sinhala Transliteration", "abstract": "Machine Transliteration provides the ability to transliterate a basic\nlanguage into different languages in a computational way. Transliteration is an\nimportant technical process that has caught the attention most recently. The\nSinhala transliteration has many constraints because of the insufficiency of\nresources in the Sinhala language. Due to these limitations, Sinhala\nTransliteration is highly complex and time-consuming. Therefore, the majority\nof the Sri Lankans uses non-formal texting language named 'Singlish' to make\nthat process simple. This study has focused on the transliteration of the\nSinglish language at the word level by reducing the complication in the\ntransliteration. A new approach of coding system has invented with the\nrule-based approach that can map the matching Sinhala words even without the\nvowels. Various typing patterns were collected by different communities for\nthis. The collected data have analyzed with every Sinhala character and unique\nSinglish patterns related to them were generated. The system has introduced a\nnewly initiated numeric coding system to use with the Singlish letters by\nmatching with the recognized typing patterns. For the mapping process, fuzzy\nlogic-based implementation has used. A codified dictionary has also implemented\nincluding unique numeric values. In this system, Each Romanized English letter\nwas assigned with a unique numeric code that can construct a unique pattern for\neach word. The system can identify the most relevant Sinhala word that matches\nwith the pattern of the Singlish word or it gives the most related word\nsuggestions. For example, the word 'kiyanna,kianna, kynna, kynn, kiynna' have\nmapped with the accurate Sinhala word \"kiyanna\". These results revealed that\nthe 'Swa Bhasha' transliteration system has the ability to enhance the Sinhala\nusers' experience while conducting the texting in Singlish to Sinhala.", "published": "2024-04-20 11:10:37", "link": "http://arxiv.org/abs/2404.13350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation based Bias Decoupling Regularization for Natural Language\n  Inference", "abstract": "The robustness of Transformer-based Natural Language Inference encoders is\nfrequently compromised as they tend to rely more on dataset biases than on the\nintended task-relevant features. Recent studies have attempted to mitigate this\nby reducing the weight of biased samples during the training process. However,\nthese debiasing methods primarily focus on identifying which samples are biased\nwithout explicitly determining the biased components within each case. This\nlimitation restricts those methods' capability in out-of-distribution\ninference. To address this issue, we aim to train models to adopt the logic\nhumans use in explaining causality. We propose a simple, comprehensive, and\ninterpretable method: Explanation based Bias Decoupling Regularization\n(EBD-Reg). EBD-Reg employs human explanations as criteria, guiding the encoder\nto establish a tripartite parallel supervision of Distinguishing, Decoupling\nand Aligning. This method enables encoders to identify and focus on keywords\nthat represent the task-relevant features during inference, while discarding\nthe residual elements acting as biases. Empirical evidence underscores that\nEBD-Reg effectively guides various Transformer-based encoders to decouple\nbiases through a human-centric lens, significantly surpassing other methods in\nterms of out-of-distribution inference capabilities.", "published": "2024-04-20 14:20:24", "link": "http://arxiv.org/abs/2404.13390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Named Entities for Corona News", "abstract": "Information resources such as newspapers have produced unstructured text data\nin various languages related to the corona outbreak since December 2019.\nAnalyzing these unstructured texts is time-consuming without representing them\nin a structured format; therefore, representing them in a structured format is\ncrucial. An information extraction pipeline with essential tasks -- named\nentity tagging and relation extraction -- to accomplish this goal might be\napplied to these texts. This study proposes a data annotation pipeline to\ngenerate training data from corona news articles, including generic and\ndomain-specific entities. Named entity recognition models are trained on this\nannotated corpus and then evaluated on test sentences manually annotated by\ndomain experts evaluating the performance of a trained model. The code base and\ndemonstration are available at https://github.com/sefeoglu/coronanews-ner.git.", "published": "2024-04-20 18:22:49", "link": "http://arxiv.org/abs/2404.13439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Subword Tokenization: Alien Subword Composition and OOV\n  Generalization Challenge", "abstract": "The popular subword tokenizers of current language models, such as Byte-Pair\nEncoding (BPE), are known not to respect morpheme boundaries, which affects the\ndownstream performance of the models. While many improved tokenization\nalgorithms have been proposed, their evaluation and cross-comparison is still\nan open problem. As a solution, we propose a combined intrinsic-extrinsic\nevaluation framework for subword tokenization. Intrinsic evaluation is based on\nour new UniMorph Labeller tool that classifies subword tokenization as either\nmorphological or alien. Extrinsic evaluation, in turn, is performed via the\nOut-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of\nthree newly specified downstream text classification tasks. Our empirical\nfindings show that the accuracy of UniMorph Labeller is 98%, and that, in all\nlanguage models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien\ntokenization leads to poorer generalizations compared to morphological\ntokenization for semantic compositionality of word meanings.", "published": "2024-04-20 06:49:15", "link": "http://arxiv.org/abs/2404.13292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MahaSQuAD: Bridging Linguistic Divides in Marathi Question-Answering", "abstract": "Question-answering systems have revolutionized information retrieval, but\nlinguistic and cultural boundaries limit their widespread accessibility. This\nresearch endeavors to bridge the gap of the absence of efficient QnA datasets\nin low-resource languages by translating the English Question Answering Dataset\n(SQuAD) using a robust data curation approach. We introduce MahaSQuAD, the\nfirst-ever full SQuAD dataset for the Indic language Marathi, consisting of\n118,516 training, 11,873 validation, and 11,803 test samples. We also present a\ngold test set of manually verified 500 examples. Challenges in maintaining\ncontext and handling linguistic nuances are addressed, ensuring accurate\ntranslations. Moreover, as a QnA dataset cannot be simply converted into any\nlow-resource language using translation, we need a robust method to map the\nanswer translation to its span in the translated passage. Hence, to address\nthis challenge, we also present a generic approach for translating SQuAD into\nany low-resource language. Thus, we offer a scalable approach to bridge\nlinguistic and cultural gaps present in low-resource languages, in the realm of\nquestion-answering systems. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/MarathiNLP .", "published": "2024-04-20 12:16:35", "link": "http://arxiv.org/abs/2404.13364v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generation-based Relation Extraction", "abstract": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.", "published": "2024-04-20 14:42:43", "link": "http://arxiv.org/abs/2404.13397v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do \"English\" Named Entity Recognizers Work Well on Global Englishes?", "abstract": "The vast majority of the popular English named entity recognition (NER)\ndatasets contain American or British English data, despite the existence of\nmany global varieties of English. As such, it is unclear whether they\ngeneralize for analyzing use of English globally. To test this, we build a\nnewswire dataset, the Worldwide English NER Dataset, to analyze NER model\nperformance on low-resource English variants from around the world. We test\nwidely used NER toolkits and transformer models, including models using the\npre-trained contextual models RoBERTa and ELECTRA, on three datasets: a\ncommonly used British English newswire dataset, CoNLL 2003, a more American\nfocused dataset OntoNotes, and our global dataset. All models trained on the\nCoNLL or OntoNotes datasets experienced significant performance drops-over 10\nF1 in some cases-when tested on the Worldwide English dataset. Upon examination\nof region-specific errors, we observe the greatest performance drops for\nOceania and Africa, while Asia and the Middle East had comparatively strong\nperformance. Lastly, we find that a combined model trained on the Worldwide\ndataset and either CoNLL or OntoNotes lost only 1-2 F1 on both test sets.", "published": "2024-04-20 20:48:42", "link": "http://arxiv.org/abs/2404.13465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of Machine Translation Based on Semantic Dependencies and\n  Keywords", "abstract": "In view of the fact that most of the existing machine translation evaluation\nalgorithms only consider the lexical and syntactic information, but ignore the\ndeep semantic information contained in the sentence, this paper proposes a\ncomputational method for evaluating the semantic correctness of machine\ntranslations based on reference translations and incorporating semantic\ndependencies and sentence keyword information. Use the language technology\nplatform developed by the Social Computing and Information Retrieval Research\nCenter of Harbin Institute of Technology to conduct semantic dependency\nanalysis and keyword analysis on sentences, and obtain semantic dependency\ngraphs, keywords, and weight information corresponding to keywords. It includes\nall word information with semantic dependencies in the sentence and keyword\ninformation that affects semantic information. Construct semantic association\npairs including word and dependency multi-features. The key semantics of the\nsentence cannot be highlighted in the semantic information extracted through\nsemantic dependence, resulting in vague semantics analysis. Therefore, the\nsentence keyword information is also included in the scope of machine\ntranslation semantic evaluation. To achieve a comprehensive and in-depth\nevaluation of the semantic correctness of sentences, the experimental results\nshow that the accuracy of the evaluation algorithm has been improved compared\nwith similar methods, and it can more accurately measure the semantic\ncorrectness of machine translation.", "published": "2024-04-20 04:14:28", "link": "http://arxiv.org/abs/2404.14443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting Question Quality on StackOverflow with Neural Networks", "abstract": "The wealth of information available through the Internet and social media is\nunprecedented. Within computing fields, websites such as Stack Overflow are\nconsidered important sources for users seeking solutions to their computing and\nprogramming issues. However, like other social media platforms, Stack Overflow\ncontains a mixture of relevant and irrelevant information. In this paper, we\nevaluated neural network models to predict the quality of questions on Stack\nOverflow, as an example of Question Answering (QA) communities. Our results\ndemonstrate the effectiveness of neural network models compared to baseline\nmachine learning models, achieving an accuracy of 80%. Furthermore, our\nfindings indicate that the number of layers in the neural network model can\nsignificantly impact its performance.", "published": "2024-04-20 16:48:18", "link": "http://arxiv.org/abs/2404.14449v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Personalized Wireless Federated Learning for Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their deployment in wireless networks still face challenges,\ni.e., a lack of privacy and security protection mechanisms. Federated Learning\n(FL) has emerged as a promising approach to address these challenges. Yet, it\nsuffers from issues including inefficient handling with big and heterogeneous\ndata, resource-intensive training, and high communication overhead. To tackle\nthese issues, we first compare different learning stages and their features of\nLLMs in wireless networks. Next, we introduce two personalized wireless\nfederated fine-tuning methods with low communication overhead, i.e., (1)\nPersonalized Federated Instruction Tuning (PFIT), which employs reinforcement\nlearning to fine-tune local LLMs with diverse reward models to achieve\npersonalization; (2) Personalized Federated Task Tuning (PFTT), which can\nleverage global adapters and local Low-Rank Adaptations (LoRA) to\ncollaboratively fine-tune local LLMs, where the local LoRAs can be applied to\nachieve personalization without aggregation. Finally, we perform simulations to\ndemonstrate the effectiveness of the proposed two methods and comprehensively\ndiscuss open issues.", "published": "2024-04-20 02:30:21", "link": "http://arxiv.org/abs/2404.13238v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Double Mixture: Towards Continual Event Detection from Speech", "abstract": "Speech event detection is crucial for multimedia retrieval, involving the\ntagging of both semantic and acoustic events. Traditional ASR systems often\noverlook the interplay between these events, focusing solely on content, even\nthough the interpretation of dialogue can vary with environmental context. This\npaper tackles two primary challenges in speech event detection: the continual\nintegration of new events without forgetting previous ones, and the\ndisentanglement of semantic from acoustic events. We introduce a new task,\ncontinual event detection from speech, for which we also provide two benchmark\ndatasets. To address the challenges of catastrophic forgetting and effective\ndisentanglement, we propose a novel method, 'Double Mixture.' This method\nmerges speech expertise with robust memory mechanisms to enhance adaptability\nand prevent forgetting. Our comprehensive experiments show that this task\npresents significant challenges that are not effectively addressed by current\nstate-of-the-art methods in either computer vision or natural language\nprocessing. Our approach achieves the lowest rates of forgetting and the\nhighest levels of generalization, proving robust across various continual\nlearning sequences. Our code and data are available at\nhttps://anonymous.4open.science/status/Continual-SpeechED-6461.", "published": "2024-04-20 06:32:00", "link": "http://arxiv.org/abs/2404.13289v2", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty\n  and Response Time for Multiple-Choice Questions", "abstract": "This work explores a novel data augmentation method based on Large Language\nModels (LLMs) for predicting item difficulty and response time of retired USMLE\nMultiple-Choice Questions (MCQs) in the BEA 2024 Shared Task. Our approach is\nbased on augmenting the dataset with answers from zero-shot LLMs (Falcon,\nMeditron, Mistral) and employing transformer-based models based on six\nalternative feature combinations. The results suggest that predicting the\ndifficulty of questions is more challenging. Notably, our top performing\nmethods consistently include the question text, and benefit from the\nvariability of LLM answers, highlighting the potential of LLMs for improving\nautomated assessment in medical licensing exams. We make our code available\nhttps://github.com/ana-rogoz/BEA-2024.", "published": "2024-04-20 10:41:02", "link": "http://arxiv.org/abs/2404.13343v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantically Corrected Amharic Automatic Speech Recognition", "abstract": "Automatic Speech Recognition (ASR) can play a crucial role in enhancing the\naccessibility of spoken languages worldwide. In this paper, we build a set of\nASR tools for Amharic, a language spoken by more than 50 million people\nprimarily in eastern Africa. Amharic is written in the Ge'ez script, a sequence\nof graphemes with spacings denoting word boundaries. This makes computational\nprocessing of Amharic challenging since the location of spacings can\nsignificantly impact the meaning of formed sentences. We find that existing\nbenchmarks for Amharic ASR do not account for these spacings and only measure\nindividual grapheme error rates, leading to significantly inflated measurements\nof in-the-wild performance. In this paper, we first release corrected\ntranscriptions of existing Amharic ASR test datasets, enabling the community to\naccurately evaluate progress. Furthermore, we introduce a post-processing\napproach using a transformer encoder-decoder architecture to organize raw ASR\noutputs into a grammatically complete and semantically meaningful Amharic\nsentence. Through experiments on the corrected test dataset, our model enhances\nthe semantic correctness of Amharic speech recognition systems, achieving a\nCharacter Error Rate (CER) of 5.5\\% and a Word Error Rate (WER) of 23.3\\%.", "published": "2024-04-20 12:08:00", "link": "http://arxiv.org/abs/2404.13362v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Movie101v2: Improved Movie Narration Benchmark", "abstract": "Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.", "published": "2024-04-20 13:15:27", "link": "http://arxiv.org/abs/2404.13370v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Intrusion Detection at Scale with the Assistance of a Command-line\n  Language Model", "abstract": "Intrusion detection is a long standing and crucial problem in security. A\nsystem capable of detecting intrusions automatically is on great demand in\nenterprise security solutions. Existing solutions rely heavily on hand-crafted\nrules designed by security operators, which suffer from high false negative\nrates and poor generalization ability to new, zero-day attacks at scale. AI and\nmachine learning offer promising solutions to address the issues, by inspecting\nabnormal user behaviors intelligently and automatically from data. However,\nexisting learning-based intrusion detection systems in the literature are\nmostly designed for small data, and they lack the ability to leverage the power\nof big data in cloud environments. In this paper, we target at this problem and\nintroduce an intrusion detection system which incorporates large-scale\npre-training, so as to train a large language model based on tens of millions\nof command lines for AI-based intrusion detection. Experiments performed on 30\nmillion training samples and 10 million test samples verify the effectiveness\nof our solution.", "published": "2024-04-20 15:04:25", "link": "http://arxiv.org/abs/2404.13402v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A Multi-Faceted Evaluation Framework for Assessing Synthetic Data\n  Generated by Large Language Models", "abstract": "The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy.", "published": "2024-04-20 08:08:28", "link": "http://arxiv.org/abs/2404.14445v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interactive tools for making temporally variable, multiple-attributes,\n  and multiple-instances morphing accessible: Flexible manipulation of\n  divergent speech instances for explorational research and education", "abstract": "We generalized a voice morphing algorithm capable of handling temporally\nvariable, multiple-attributes, and multiple instances. The generalized morphing\nprovides a new strategy for investigating speech diversity. However, excessive\ncomplexity and the difficulty of preparation have prevented researchers and\nstudents from enjoying its benefits. To address this issue, we introduced a set\nof interactive tools to make preparation and tests less cumbersome. These tools\nare integrated into our previously reported interactive tools as extensions.\nThe introduction of the extended tools in lessons in graduate education was\nsuccessful. Finally, we outline further extensions to explore excessively\ncomplex morphing parameter settings.", "published": "2024-04-20 16:13:52", "link": "http://arxiv.org/abs/2404.13418v1", "categories": ["cs.HC", "eess.AS", "68-04", "K.3.1"], "primary_category": "cs.HC"}
{"title": "Track Role Prediction of Single-Instrumental Sequences", "abstract": "In the composition process, selecting appropriate single-instrumental music\nsequences and assigning their track-role is an indispensable task. However,\nmanually determining the track-role for a myriad of music samples can be\ntime-consuming and labor-intensive. This study introduces a deep learning model\ndesigned to automatically predict the track-role of single-instrumental music\nsequences. Our evaluations show a prediction accuracy of 87% in the symbolic\ndomain and 84% in the audio domain. The proposed track-role prediction methods\nhold promise for future applications in AI music generation and analysis.", "published": "2024-04-20 06:22:07", "link": "http://arxiv.org/abs/2404.13286v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Consistency Models", "abstract": "Consistency models have exhibited remarkable capabilities in facilitating\nefficient image/video generation, enabling synthesis with minimal sampling\nsteps. It has proven to be advantageous in mitigating the computational burdens\nassociated with diffusion models. Nevertheless, the application of consistency\nmodels in music generation remains largely unexplored. To address this gap, we\npresent Music Consistency Models (\\texttt{MusicCM}), which leverages the\nconcept of consistency models to efficiently synthesize mel-spectrogram for\nmusic clips, maintaining high quality while minimizing the number of sampling\nsteps. Building upon existing text-to-music diffusion models, the\n\\texttt{MusicCM} model incorporates consistency distillation and adversarial\ndiscriminator training. Moreover, we find it beneficial to generate extended\ncoherent music by incorporating multiple diffusion processes with shared\nconstraints. Experimental results reveal the effectiveness of our model in\nterms of computational efficiency, fidelity, and naturalness. Notable,\n\\texttt{MusicCM} achieves seamless music synthesis with a mere four sampling\nsteps, e.g., only one second per minute of the music clip, showcasing the\npotential for real-time application.", "published": "2024-04-20 11:52:30", "link": "http://arxiv.org/abs/2404.13358v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-dependent Speaker Verification (TdSV) Challenge 2024: Challenge\n  Evaluation Plan", "abstract": "This document outlines the Text-dependent Speaker Verification (TdSV)\nChallenge 2024, which centers on analyzing and exploring novel approaches for\ntext-dependent speaker verification. The primary goal of this challenge is to\nmotive participants to develop single yet competitive systems, conduct thorough\nanalyses, and explore innovative concepts such as multi-task learning,\nself-supervised learning, few-shot learning, and others, for text-dependent\nspeaker verification.", "published": "2024-04-20 17:26:59", "link": "http://arxiv.org/abs/2404.13428v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
