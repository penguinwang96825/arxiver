{"title": "Otem&Utem: Over- and Under-Translation Evaluation Metric for NMT", "abstract": "Although neural machine translation(NMT) yields promising translation\nperformance, it unfortunately suffers from over- and under-translation is- sues\n[Tu et al., 2016], of which studies have become research hotspots in NMT. At\npresent, these studies mainly apply the dominant automatic evaluation metrics,\nsuch as BLEU, to evaluate the overall translation quality with respect to both\nadequacy and uency. However, they are unable to accurately measure the ability\nof NMT systems in dealing with the above-mentioned issues. In this paper, we\npropose two quantitative metrics, the Otem and Utem, to automatically evaluate\nthe system perfor- mance in terms of over- and under-translation respectively.\nBoth metrics are based on the proportion of mismatched n-grams between gold\nref- erence and system translation. We evaluate both metrics by comparing their\nscores with human evaluations, where the values of Pearson Cor- relation\nCoefficient reveal their strong correlation. Moreover, in-depth analyses on\nvarious translation systems indicate some inconsistency be- tween BLEU and our\nproposed metrics, highlighting the necessity and significance of our metrics.", "published": "2018-07-24 08:09:22", "link": "http://arxiv.org/abs/1807.08945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Argumentation Mining: Machine Translation (and a bit of\n  Projection) is All You Need!", "abstract": "Argumentation mining (AM) requires the identification of complex discourse\nstructures and has lately been applied with success monolingually. In this\nwork, we show that the existing resources are, however, not adequate for\nassessing cross-lingual AM, due to their heterogeneity or lack of complexity.\nWe therefore create suitable parallel corpora by (human and machine)\ntranslating a popular AM dataset consisting of persuasive student essays into\nGerman, French, Spanish, and Chinese. We then compare (i) annotation projection\nand (ii) bilingual word embeddings based direct transfer strategies for\ncross-lingual AM, finding that the former performs considerably better and\nalmost eliminates the loss from cross-lingual transfer. Moreover, we find that\nannotation projection works equally well when using either costly human or\ncheap machine translations. Our code and data are available at\n\\url{http://github.com/UKPLab/coling2018-xling_argument_mining}.", "published": "2018-07-24 09:48:43", "link": "http://arxiv.org/abs/1807.08998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The division of labor in communication: Speakers help listeners account\n  for asymmetries in visual perspective", "abstract": "Recent debates over adults' theory of mind use have been fueled by surprising\nfailures of perspective-taking in communication, suggesting that\nperspective-taking can be relatively effortful. How, then, should speakers and\nlisteners allocate their resources to achieve successful communication? We\nbegin with the observation that this shared goal induces a natural division of\nlabor: the resources one agent chooses to allocate toward perspective-taking\nshould depend on their expectations about the other's allocation. We formalize\nthis idea in a resource-rational model augmenting recent probabilistic\nweighting accounts with a mechanism for (costly) control over the degree of\nperspective-taking. In a series of simulations, we first derive an intermediate\ndegree of perspective weighting as an optimal tradeoff between expected costs\nand benefits of perspective-taking. We then present two behavioral experiments\ntesting novel predictions of our model. In Experiment 1, we manipulated the\npresence or absence of occlusions in a director-matcher task and found that\nspeakers spontaneously produced more informative descriptions to account for\n\"known unknowns\" in their partner's private view. In Experiment 2, we compared\nthe scripted utterances used by confederates in prior work with those produced\nin interactions with unscripted directors. We found that confederates were\nsystematically less informative than listeners would initially expect given the\npresence of occlusions, but listeners used violations to adaptively make fewer\nerrors over time. Taken together, our work suggests that people are not simply\n\"mindblind\"; they use contextually appropriate expectations to navigate the\ndivision of labor with their partner. We discuss how a resource rational\nframework may provide a more deeply explanatory foundation for understanding\nflexible perspective-taking under processing constraints.", "published": "2018-07-24 09:56:53", "link": "http://arxiv.org/abs/1807.09000v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and representing the semantics of large structured\n  documents", "abstract": "Understanding large, structured documents like scholarly articles, requests\nfor proposals or business reports is a complex and difficult task. It involves\ndiscovering a document's overall purpose and subject(s), understanding the\nfunction and meaning of its sections and subsections, and extracting low level\nentities and facts about them. In this research, we present a deep learning\nbased document ontology to capture the general purpose semantic structure and\ndomain specific semantic concepts from a large number of academic articles and\nbusiness documents. The ontology is able to describe different functional parts\nof a document, which can be used to enhance semantic indexing for a better\nunderstanding by human beings and machines. We evaluate our models through\nextensive experiments on datasets of scholarly articles from arXiv and Request\nfor Proposal documents.", "published": "2018-07-24 04:14:51", "link": "http://arxiv.org/abs/1807.09842v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Joint Time-Frequency Scattering", "abstract": "In time series classification and regression, signals are typically mapped\ninto some intermediate representation used for constructing models. Since the\nunderlying task is often insensitive to time shifts, these representations are\nrequired to be time-shift invariant. We introduce the joint time-frequency\nscattering transform, a time-shift invariant representation which characterizes\nthe multiscale energy distribution of a signal in time and frequency. It is\ncomputed through wavelet convolutions and modulus non-linearities and may\ntherefore be implemented as a deep convolutional neural network whose filters\nare not learned but calculated from wavelets. We consider the progression from\nmel-spectrograms to time scattering and joint time-frequency scattering\ntransforms, illustrating the relationship between increased discriminability\nand refinements of convolutional network architectures. The suitability of the\njoint time-frequency scattering transform for time-shift invariant\ncharacterization of time series is demonstrated through applications to chirp\nsignals and audio synthesis experiments. The proposed transform also obtains\nstate-of-the-art results on several audio classification tasks, outperforming\ntime scattering transforms and achieving accuracies comparable to those of\nfully learned networks.", "published": "2018-07-24 01:01:35", "link": "http://arxiv.org/abs/1807.08869v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Extractor Network for Target Speaker Recovery From Single Channel\n  Speech Mixtures", "abstract": "Speaker-aware source separation methods are promising workarounds for major\ndifficulties such as arbitrary source permutation and unknown number of\nsources. However, it remains challenging to achieve satisfying performance\nprovided a very short available target speaker utterance (anchor). Here we\npresent a novel \"deep extractor network\" which creates an extractor point for\nthe target speaker in a canonical high dimensional embedding space, and pulls\ntogether the time-frequency bins corresponding to the target speaker. The\nproposed model is different from prior works in that the canonical embedding\nspace encodes knowledges of both the anchor and the mixture during an\nend-to-end training phase: First, embeddings for the anchor and mixture speech\nare separately constructed in a primary embedding space, and then combined as\nan input to feed-forward layers to transform to a canonical embedding space\nwhich we discover more stable than the primary one. Experimental results show\nthat given a very short utterance, the proposed model can efficiently recover\nhigh quality target speech from a mixture, which outperforms various baseline\nmodels, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively\ncompared with a baseline oracle deep attracor model. Meanwhile, we show it can\nbe generalized well to more than one interfering speaker.", "published": "2018-07-24 09:08:29", "link": "http://arxiv.org/abs/1807.08974v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hybrid of Deep Audio Feature and i-vector for Artist Recognition", "abstract": "Artist recognition is a task of modeling the artist's musical style. This\nproblem is challenging because there is no clear standard. We propose a hybrid\nmethod of the generative model i-vector and the discriminative model deep\nconvolutional neural network. We show that this approach achieves\nstate-of-the-art performance by complementing each other. In addition, we\nbriefly explain the advantages and disadvantages of each approach.", "published": "2018-07-24 16:14:09", "link": "http://arxiv.org/abs/1807.09208v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
