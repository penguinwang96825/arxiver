{"title": "Noise Contrastive Priors for Functional Uncertainty", "abstract": "Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of an independent normal prior in weight space imposes relatively weak constraints on the function posterior, allowing it to generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.", "published": "2018-07-24 18:08:45", "link": "http://arxiv.org/abs/1807.09289v3", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Decision Variance in Online Learning", "abstract": "Online learning has traditionally focused on the expected rewards. In this paper, a risk-averse online learning problem under the performance measure of the mean-variance of the rewards is studied. Both the bandit and full information settings are considered. The performance of several existing policies is analyzed, and new fundamental limitations on risk-averse learning is established. In particular, it is shown that although a logarithmic distribution-dependent regret in time $T$ is achievable (similar to the risk-neutral problem), the worst-case (i.e. minimax) regret is lower bounded by $\u03a9(T)$ (in contrast to the $\u03a9(\\sqrt{T})$ lower bound in the risk-neutral problem). This sharp difference from the risk-neutral counterpart is caused by the the variance in the player's decisions, which, while absent in the regret under the expected reward criterion, contributes to excess mean-variance due to the non-linearity of this risk measure. The role of the decision variance in regret performance reflects a risk-averse player's desire for robust decisions and outcomes.", "published": "2018-07-24 13:20:49", "link": "http://arxiv.org/abs/1807.09089v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Multi-Agent Coverage Control with Energy Depletion and Repletion", "abstract": "We develop a hybrid system model to describe the behavior of multiple agents cooperatively solving an optimal coverage problem under energy depletion and repletion constraints. The model captures the controlled switching of agents between coverage (when energy is depleted) and battery charging (when energy is replenished) modes. It guarantees the feasibility of the coverage problem by defining a guard function on each agent's battery level to prevent it from dying on its way to a charging station. The charging station plays the role of a centralized scheduler to solve the contention problem of agents competing for the only charging resource in the mission space. The optimal coverage problem is transformed into a parametric optimization problem to determine an optimal recharging policy. This problem is solved through the use of Infinitesimal Perturbation Analysis (IPA), with simulation results showing that a full recharging policy is optimal.", "published": "2018-07-24 21:14:04", "link": "http://arxiv.org/abs/1807.09359v1", "categories": ["eess.SP", "eess.SY"], "primary_category": "eess.SP"}
