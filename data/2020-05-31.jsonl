{"title": "Learning to refer informatively by amortizing pragmatic reasoning", "abstract": "A hallmark of human language is the ability to effectively and efficiently\nconvey contextually relevant information. One theory for how humans reason\nabout language is presented in the Rational Speech Acts (RSA) framework, which\ncaptures pragmatic phenomena via a process of recursive social reasoning\n(Goodman & Frank, 2016). However, RSA represents ideal reasoning in an\nunconstrained setting. We explore the idea that speakers might learn to\namortize the cost of RSA computation over time by directly optimizing for\nsuccessful communication with an internal listener model. In simulations with\ngrounded neural speakers and listeners across two communication game datasets\nrepresenting synthetic and human-generated data, we find that our amortized\nmodel is able to quickly generate language that is effective and concise across\na range of contexts, without the need for explicit pragmatic reasoning.", "published": "2020-05-31 02:52:22", "link": "http://arxiv.org/abs/2006.00418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Group Beliefs Related to 2018's Brazilian Elections in Tweets\n  A Combined Study on Modeling Topics and Sentiment Analysis", "abstract": "2018's Brazilian presidential elections highlighted the influence of\nalternative media and social networks, such as Twitter. In this work, we\nperform an analysis covering politically motivated discourses related to the\nsecond round in Brazilian elections. In order to verify whether similar\ndiscourses reinforce group engagement to personal beliefs, we collected a set\nof tweets related to political hashtags at that moment. To this end, we have\nused a combination of topic modeling approach with opinion mining techniques to\nanalyze the motivated political discourses. Using SentiLex-PT, a Portuguese\nsentiment lexicon, we extracted from the dataset the top 5 most frequent group\nof words related to opinions. Applying a bag-of-words model, the cosine\nsimilarity calculation was performed between each opinion and the observed\ngroups. This study allowed us to observe an exacerbated use of passionate\ndiscourses in the digital political scenario as a form of appreciation and\nengagement to the groups which convey similar beliefs.", "published": "2020-05-31 10:58:35", "link": "http://arxiv.org/abs/2006.00490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Recognise Words using Visually Grounded Speech", "abstract": "We investigated word recognition in a Visually Grounded Speech model. The\nmodel has been trained on pairs of images and spoken captions to create\nvisually grounded embeddings which can be used for speech to image retrieval\nand vice versa. We investigate whether such a model can be used to recognise\nwords by embedding isolated words and using them to retrieve images of their\nvisual referents. We investigate the time-course of word recognition using a\ngating paradigm and perform a statistical analysis to see whether well known\nword competition effects in human speech processing influence word recognition.\nOur experiments show that the model is able to recognise words, and the gating\nparadigm reveals that words can be recognised from partial input as well and\nthat recognition is negatively influenced by word competition from the word\ninitial cohort.", "published": "2020-05-31 12:48:37", "link": "http://arxiv.org/abs/2006.00512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Entity Linking: A Survey of Models Based on Deep Learning", "abstract": "This survey presents a comprehensive description of recent neural entity\nlinking (EL) systems developed since 2015 as a result of the \"deep learning\nrevolution\" in natural language processing. Its goal is to systemize design\nfeatures of neural entity linking systems and compare their performance to the\nremarkable classic methods on common benchmarks. This work distills a generic\narchitecture of a neural EL system and discusses its components, such as\ncandidate generation, mention-context encoding, and entity ranking, summarizing\nprominent methods for each of them. The vast variety of modifications of this\ngeneral architecture are grouped by several common themes: joint entity mention\ndetection and disambiguation, models for global linking, domain-independent\ntechniques including zero-shot and distant supervision methods, and\ncross-lingual approaches. Since many neural models take advantage of entity and\nmention/context embeddings to represent their meaning, this work also overviews\nprominent entity embedding techniques. Finally, the survey touches on\napplications of entity linking, focusing on the recently emerged use-case of\nenhancing deep pre-trained masked language models based on the Transformer\narchitecture.", "published": "2020-05-31 18:02:26", "link": "http://arxiv.org/abs/2006.00575v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Judge me by my size (noun), do you?'' YodaLib: A Demographic-Aware\n  Humor Generation Framework", "abstract": "The subjective nature of humor makes computerized humor generation a\nchallenging task. We propose an automatic humor generation framework for\nfilling the blanks in Mad Libs stories, while accounting for the demographic\nbackgrounds of the desired audience. We collect a dataset consisting of such\nstories, which are filled in and judged by carefully selected workers on Amazon\nMechanical Turk. We build upon the BERT platform to predict location-biased\nword fillings in incomplete sentences, and we fine tune BERT to classify\nlocation-specific humor in a sentence. We leverage these components to produce\nYodaLib, a fully-automated Mad Libs style humor generation framework, which\nselects and ranks appropriate candidate words and sentences in order to\ngenerate a coherent and funny story tailored to certain demographics. Our\nexperimental results indicate that YodaLib outperforms a previous\nsemi-automated approach proposed for this task, while also surpassing human\nannotators in both qualitative and quantitative analyses.", "published": "2020-05-31 18:11:52", "link": "http://arxiv.org/abs/2006.00578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Deployment of Conversational Natural Language Interfaces over\n  Databases", "abstract": "Many users communicate with chatbots and AI assistants in order to help them\nwith various tasks. A key component of the assistant is the ability to\nunderstand and answer a user's natural language questions for\nquestion-answering (QA). Because data can be usually stored in a structured\nmanner, an essential step involves turning a natural language question into its\ncorresponding query language. However, in order to train most natural\nlanguage-to-query-language state-of-the-art models, a large amount of training\ndata is needed first. In most domains, this data is not available and\ncollecting such datasets for various domains can be tedious and time-consuming.\nIn this work, we propose a novel method for accelerating the training dataset\ncollection for developing the natural language-to-query-language machine\nlearning models. Our system allows one to generate conversational multi-term\ndata, where multiple turns define a dialogue session, enabling one to better\nutilize chatbot interfaces. We train two current state-of-the-art NL-to-QL\nmodels, on both an SQL and SPARQL-based datasets in order to showcase the\nadaptability and efficacy of our created data.", "published": "2020-05-31 19:16:27", "link": "http://arxiv.org/abs/2006.00591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BPGC at SemEval-2020 Task 11: Propaganda Detection in News Articles with\n  Multi-Granularity Knowledge Sharing and Linguistic Features based Ensemble\n  Learning", "abstract": "Propaganda spreads the ideology and beliefs of like-minded people,\nbrainwashing their audiences, and sometimes leading to violence. SemEval 2020\nTask-11 aims to design automated systems for news propaganda detection. Task-11\nconsists of two sub-tasks, namely, Span Identification - given any news\narticle, the system tags those specific fragments which contain at least one\npropaganda technique; and Technique Classification - correctly classify a given\npropagandist statement amongst 14 propaganda techniques. For sub-task 1, we use\ncontextual embeddings extracted from pre-trained transformer models to\nrepresent the text data at various granularities and propose a\nmulti-granularity knowledge sharing approach. For sub-task 2, we use an\nensemble of BERT and logistic regression classifiers with linguistic features.\nOur results reveal that the linguistic features are the strong indicators for\ncovering minority classes in a highly imbalanced dataset.", "published": "2020-05-31 19:35:53", "link": "http://arxiv.org/abs/2006.00593v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative\n  Models to Perform Short-Edits based Humor Grading", "abstract": "In this paper, we assess the ability of BERT and its derivative models\n(RoBERTa, DistilBERT, and ALBERT) for short-edits based humor grading. We test\nthese models for humor grading and classification tasks on the Humicroedit and\nthe FunLines dataset. We perform extensive experiments with these models to\ntest their language modeling and generalization abilities via zero-shot\ninference and cross-dataset inference based approaches. Further, we also\ninspect the role of self-attention layers in humor-grading by performing a\nqualitative analysis over the self-attention weights from the final layer of\nthe trained BERT model. Our experiments show that all the pre-trained BERT\nderivative models show significant generalization capabilities for\nhumor-grading related tasks.", "published": "2020-05-31 20:55:08", "link": "http://arxiv.org/abs/2006.00607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with\n  Multi-Head Self-Attention Weights based Counterfactual Detection", "abstract": "In this paper, we describe an approach for modelling causal reasoning in\nnatural language by detecting counterfactuals in text using multi-head\nself-attention weights. We use pre-trained transformer models to extract\ncontextual embeddings and self-attention weights from the text. We show the use\nof convolutional layers to extract task-specific features from these\nself-attention weights. Further, we describe a fine-tuning approach with a\ncommon base model for knowledge sharing between the two closely related\nsub-tasks for counterfactual detection. We analyze and compare the performance\nof various transformer models in our experiments. Finally, we perform a\nqualitative analysis with the multi-head self-attention weights to interpret\nour models' dynamics.", "published": "2020-05-31 21:02:25", "link": "http://arxiv.org/abs/2006.00609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Unsupervised Domain Adaptation in NLP---A Survey", "abstract": "Deep neural networks excel at learning from labeled data and achieve\nstate-of-the-art resultson a wide array of Natural Language Processing tasks.\nIn contrast, learning from unlabeled data, especially under domain shift,\nremains a challenge. Motivated by the latest advances, in this survey we review\nneural unsupervised domain adaptation techniques which do not require labeled\ntarget domain data. This is a more challenging yet a more widely applicable\nsetup. We outline methods, from early traditional non-neural methods to\npre-trained model transfer. We also revisit the notion of domain, and we\nuncover a bias in the type of Natural Language Processing tasks which received\nmost attention. Lastly, we outline future directions, particularly the broader\nneed for out-of-distribution generalization of future NLP.", "published": "2020-05-31 22:34:14", "link": "http://arxiv.org/abs/2006.00632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Feature Representation for Lexical Connotations", "abstract": "Ideological attitudes and stance are often expressed through subtle meanings\nof words and phrases. Understanding these connotations is critical to\nrecognizing the cultural and emotional perspectives of the speaker. In this\npaper, we use distant labeling to create a new lexical resource representing\nconnotation aspects for nouns and adjectives. Our analysis shows that it aligns\nwell with human judgments. Additionally, we present a method for creating\nlexical representations that captures connotations within the embedding space\nand show that using the embeddings provides a statistically significant\nimprovement on the task of stance detection when data is limited.", "published": "2020-05-31 23:14:02", "link": "http://arxiv.org/abs/2006.00635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Reward Estimator Bottleneck: Learning Robust Reward\n  Estimator for Multi-Domain Task-Oriented Dialog", "abstract": "Despite its notable success in adversarial learning approaches to\nmulti-domain task-oriented dialog system, training the dialog policy via\nadversarial inverse reinforcement learning often fails to balance the\nperformance of the policy generator and reward estimator. During optimization,\nthe reward estimator often overwhelms the policy generator and produces\nexcessively uninformative gradients. We proposes the Variational Reward\nestimator Bottleneck (VRB), which is an effective regularization method that\naims to constrain unproductive information flows between inputs and the reward\nestimator. The VRB focuses on capturing discriminative features, by exploiting\ninformation bottleneck on mutual information. Empirical results on a\nmulti-domain task-oriented dialog dataset demonstrate that the VRB\nsignificantly outperforms previous methods.", "published": "2020-05-31 02:44:36", "link": "http://arxiv.org/abs/2006.00417v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SANA : Sentiment Analysis on Newspapers comments in Algeria", "abstract": "It is very current in today life to seek for tracking the people opinion from\ntheir interaction with occurring events. A very common way to do that is\ncomments in articles published in newspapers web sites dealing with\ncontemporary events. Sentiment analysis or opinion mining is an emergent field\nwho is the purpose is finding the behind phenomenon masked in opinionated\ntexts. We are interested in our work by comments in Algerian newspaper\nwebsites. For this end, two corpora were used SANA and OCA. SANA corpus is\ncreated by collection of comments from three Algerian newspapers, and annotated\nby two Algerian Arabic native speakers, while OCA is a freely available corpus\nfor sentiment analysis. For the classification we adopt Supports vector\nmachines, naive Bayes and knearest neighbors. Obtained results are very\npromising and show the different effects of stemming in such domain, also\nknearest neighbors give important improvement comparing to other classifiers\nunlike similar works where SVM is the most dominant. From this study we observe\nthe importance of dedicated resources and methods the newspaper comments\nsentiment analysis which we look forward in future works.", "published": "2020-05-31 08:02:23", "link": "http://arxiv.org/abs/2006.00459v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recognizing Chinese Judicial Named Entity using BiLSTM-CRF", "abstract": "Named entity recognition (NER) plays an essential role in natural language\nprocessing systems. Judicial NER is a fundamental component of judicial\ninformation retrieval, entity relation extraction, and knowledge map building.\nHowever, Chinese judicial NER remains to be more challenging due to the\ncharacteristics of Chinese and high accuracy requirements in the judicial\nfiled. Thus, in this paper, we propose a deep learning-based method named\nBiLSTM-CRF which consists of bi-directional long short-term memory (BiLSTM) and\nconditional random fields (CRF). For further accuracy promotion, we propose to\nuse Adaptive moment estimation (Adam) for optimization of the model. To\nvalidate our method, we perform experiments on judgment documents including\ncommutation, parole and temporary service outside prison, which is acquired\nfrom China Judgments Online. Experimental results achieve the accuracy of\n0.876, recall of 0.856 and F1 score of 0.855, which suggests the superiority of\nthe proposed BiLSTM-CRF with Adam optimizer.", "published": "2020-05-31 08:13:00", "link": "http://arxiv.org/abs/2006.00464v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BiERU: Bidirectional Emotional Recurrent Unit for Conversational\n  Sentiment Analysis", "abstract": "Sentiment analysis in conversations has gained increasing attention in recent\nyears for the growing amount of applications it can serve, e.g., sentiment\nanalysis, recommender systems, and human-robot interaction. The main difference\nbetween conversational sentiment analysis and single sentence sentiment\nanalysis is the existence of context information which may influence the\nsentiment of an utterance in a dialogue. How to effectively encode contextual\ninformation in dialogues, however, remains a challenge. Existing approaches\nemploy complicated deep learning structures to distinguish different parties in\na conversation and then model the context information. In this paper, we\npropose a fast, compact and parameter-efficient party-ignorant framework named\nbidirectional emotional recurrent unit for conversational sentiment analysis.\nIn our system, a generalized neural tensor block followed by a two-channel\nclassifier is designed to perform context compositionality and sentiment\nclassification, respectively. Extensive experiments on three standard datasets\ndemonstrate that our model outperforms the state of the art in most cases.", "published": "2020-05-31 11:13:13", "link": "http://arxiv.org/abs/2006.00492v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLP Scholar: An Interactive Visual Explorer for Natural Language\n  Processing Literature", "abstract": "As part of the NLP Scholar project, we created a single unified dataset of\nNLP papers and their meta-information (including citation numbers), by\nextracting and aligning information from the ACL Anthology and Google Scholar.\nIn this paper, we describe several interconnected interactive visualizations\n(dashboards) that present various aspects of the data. Clicking on an item\nwithin a visualization or entering query terms in the search boxes filters the\ndata in all visualizations in the dashboard. This allows users to search for\npapers in the area of their interest, published within specific time periods,\npublished by specified authors, etc. The interactive visualizations presented\nhere, and the associated dataset of papers mapped to citations, have additional\nuses as well including understanding how the field is growing (both overall and\nacross sub-areas), as well as quantifying the impact of different types of\npapers on subsequent publications.", "published": "2020-05-31 17:12:37", "link": "http://arxiv.org/abs/2006.01131v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Crossed-Time Delay Neural Network for Speaker Recognition", "abstract": "Time Delay Neural Network (TDNN) is a well-performing structure for DNN-based\nspeaker recognition systems. In this paper we introduce a novel structure\nCrossed-Time Delay Neural Network (CTDNN) to enhance the performance of current\nTDNN. Inspired by the multi-filters setting of convolution layer from\nconvolution neural network, we set multiple time delay units each with\ndifferent context size at the bottom layer and construct a multilayer parallel\nnetwork. The proposed CTDNN gives significant improvements over original TDNN\non both speaker verification and identification tasks. It outperforms in\nVoxCeleb1 dataset in verification experiment with a 2.6% absolute Equal Error\nRate improvement. In few shots condition CTDNN reaches 90.4% identification\naccuracy, which doubles the identification accuracy of original TDNN. We also\ncompare the proposed CTDNN with another new variant of TDNN, FTDNN, which shows\nthat our model has a 36% absolute identification accuracy improvement under few\nshots condition and can better handle training of a larger batch in a shorter\ntraining time, which better utilize the calculation resources. The code of the\nnew model is released at https://github.com/chenllliang/CTDNN", "published": "2020-05-31 06:57:34", "link": "http://arxiv.org/abs/2006.00452v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data-driven Detection and Analysis of the Patterns of Creaky Voice", "abstract": "This paper investigates the temporal excitation patterns of creaky voice.\nCreaky voice is a voice quality frequently used as a phrase-boundary marker,\nbut also as a means of portraying attitude, affective states and even social\nstatus. Consequently, the automatic detection and modelling of creaky voice may\nhave implications for speech technology applications. The acoustic\ncharacteristics of creaky voice are, however, rather distinct from modal\nphonation. Further, several acoustic patterns can bring about the perception of\ncreaky voice, thereby complicating the strategies used for its automatic\ndetection, analysis and modelling. The present study is carried out using a\nvariety of languages, speakers, and on both read and conversational data and\ninvolves a mutual information-based assessment of the various acoustic features\nproposed in the literature for detecting creaky voice. These features are then\nexploited in classification experiments where we achieve an appreciable\nimprovement in detection accuracy compared to the state of the art. Both\nexperiments clearly highlight the presence of several creaky patterns. A\nsubsequent qualitative and quantitative analysis of the identified patterns is\nprovided, which reveals a considerable speaker-dependent variability in the\nusage of these creaky patterns. We also investigate how creaky voice detection\nsystems perform across creaky patterns.", "published": "2020-05-31 13:34:30", "link": "http://arxiv.org/abs/2006.00518v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Maximum Voiced Frequency Estimation: Exploiting Amplitude and Phase\n  Spectra", "abstract": "Maximum Voiced Frequency (MVF) is used in various speech models as the\nspectral boundary separating periodic and aperiodic components during the\nproduction of voiced sounds. Recent studies have shown that its proper\nestimation and modeling enhance the quality of statistical parametric speech\nsynthesizers. Contrastingly, these same methods of MVF estimation have been\nreported to degrade the performance of singing voice synthesizers. This paper\nproposes a new approach for MVF estimation which exploits both amplitude and\nphase spectra. It is shown that phase conveys relevant information about the\nharmonicity of the voice signal, and that it can be jointly used with features\nderived from the amplitude spectrum. This information is further integrated\ninto a maximum likelihood criterion which provides a decision about the MVF\nestimate. The proposed technique is compared to two state-of-the-art methods,\nand shows a superior performance in both objective and subjective evaluations.\nPerceptual tests indicate a drastic improvement in high-pitched voices.", "published": "2020-05-31 13:40:46", "link": "http://arxiv.org/abs/2006.00521v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Residual Excitation Skewness for Automatic Speech Polarity Detection", "abstract": "Detecting the correct speech polarity is a necessary step prior to several\nspeech processing techniques. An error on its determination could have a\ndramatic detrimental impact on their performance. As current systems have to\ndeal with increasing amounts of data stemming from multiple devices, the\nautomatic detection of speech polarity has become a crucial problem. For this\npurpose, we here propose a very simple algorithm based on the skewness of two\nexcitation signals. The method is shown on 10 speech corpora (8545 files) to\nlead to an error rate of only 0.06% in clean conditions and to clearly\noutperform four state-of-the-art methods. Besides it significantly reduces the\ncomputational load through its simplicity and is observed to exhibit the\nstrongest robustness in both noisy and reverberant environments.", "published": "2020-05-31 13:56:07", "link": "http://arxiv.org/abs/2006.00525v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Benchmarking BioRelEx for Entity Tagging and Relation Extraction", "abstract": "Extracting relationships and interactions between different biological\nentities is still an extremely challenging problem but has not received much\nattention as much as extraction in other generic domains. In addition to the\nlack of annotated data, low benchmarking is still a major reason for slow\nprogress. In order to fill this gap, we compare multiple existing entity and\nrelation extraction models over a recently introduced public dataset, BioRelEx\nof sentences annotated with biological entities and relations. Our\nstraightforward benchmarking shows that span-based multi-task architectures\nlike DYGIE show 4.9% and 6% absolute improvements in entity tagging and\nrelation extraction respectively over the previous state-of-art and that\nincorporating domain-specific information like embeddings pre-trained over\nrelated domains boosts performance.", "published": "2020-05-31 14:45:28", "link": "http://arxiv.org/abs/2006.00533v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improve Document Embedding for Text Categorization Through Deep Siamese\n  Neural Network", "abstract": "Due to the increasing amount of data on the internet, finding a\nhighly-informative, low-dimensional representation for text is one of the main\nchallenges for efficient natural language processing tasks including text\nclassification. This representation should capture the semantic information of\nthe text while retaining their relevance level for document classification.\nThis approach maps the documents with similar topics to a similar space in\nvector space representation. To obtain representation for large text, we\npropose the utilization of deep Siamese neural networks. To embed document\nrelevance in topics in the distributed representation, we use a Siamese neural\nnetwork to jointly learn document representations. Our Siamese network consists\nof two sub-network of multi-layer perceptron. We examine our representation for\nthe text categorization task on BBC news dataset. The results show that the\nproposed representations outperform the conventional and state-of-the-art\nrepresentations in the text classification task on this dataset.", "published": "2020-05-31 17:51:08", "link": "http://arxiv.org/abs/2006.00572v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Transfer Learning in Natural Language Processing", "abstract": "Deep learning models usually require a huge amount of data. However, these\nlarge datasets are not always attainable. This is common in many challenging\nNLP tasks. Consider Neural Machine Translation, for instance, where curating\nsuch large datasets may not be possible specially for low resource languages.\nAnother limitation of deep learning models is the demand for huge computing\nresources. These obstacles motivate research to question the possibility of\nknowledge transfer using large trained models. The demand for transfer learning\nis increasing as many large models are emerging. In this survey, we feature the\nrecent transfer learning advances in the field of NLP. We also provide a\ntaxonomy for categorizing different transfer learning approaches from the\nliterature.", "published": "2020-05-31 21:52:31", "link": "http://arxiv.org/abs/2007.04239v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Introducing Latent Timbre Synthesis", "abstract": "We present the Latent Timbre Synthesis (LTS), a new audio synthesis method\nusing Deep Learning. The synthesis method allows composers and sound designers\nto interpolate and extrapolate between the timbre of multiple sounds using the\nlatent space of audio frames. We provide the details of two Variational\nAutoencoder architectures for LTS, and compare their advantages and drawbacks.\nThe implementation includes a fully working application with graphical user\ninterface, called \\textit{interpolate\\_two}, which enables practitioners to\nexplore the timbre between two audio excerpts of their selection using\ninterpolation and extrapolation in the latent space of audio frames. Our\nimplementation is open-source, and we aim to improve the accessibility of this\ntechnology by providing a guide for users with any technical background.", "published": "2020-05-31 01:54:50", "link": "http://arxiv.org/abs/2006.00408v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
