{"title": "An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using\n  MIMIC-III Clinical Notes", "abstract": "Background and Objective: Code assignment is of paramount importance in many\nlevels in modern hospitals, from ensuring accurate billing process to creating\na valid record of patient care history. However, the coding process is tedious\nand subjective, and it requires medical coders with extensive training. This\nstudy aims to evaluate the performance of deep-learning-based systems to\nautomatically map clinical notes to ICD-9 medical codes. Methods: The\nevaluations of this research are focused on end-to-end learning methods without\nmanually defined rules. Traditional machine learning algorithms, as well as\nstate-of-the-art deep learning methods such as Recurrent Neural Networks and\nConvolution Neural Networks, were applied to the Medical Information Mart for\nIntensive Care (MIMIC-III) dataset. An extensive number of experiments was\napplied to different settings of the tested algorithm. Results: Findings showed\nthat the deep learning-based methods outperformed other conventional machine\nlearning methods. From our assessment, the best models could predict the top 10\nICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10\nICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also\noutperformed existing work under certain evaluation metrics. Conclusion: A set\nof standard metrics was utilized in assessing the performance of ICD-9 code\nassignment on MIMIC-III dataset. All the developed evaluation tools and\nresources are available online, which can be used as a baseline for further\nresearch.", "published": "2018-02-07 05:23:21", "link": "http://arxiv.org/abs/1802.02311v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised word sense disambiguation in dynamic semantic spaces", "abstract": "In this paper, we are mainly concerned with the ability to quickly and\nautomatically distinguish word senses in dynamic semantic spaces in which new\nterms and new senses appear frequently. Such spaces are built '\"on the fly\"\nfrom constantly evolving data sets such as Wikipedia, repositories of patent\ngrants and applications, or large sets of legal documents for Technology\nAssisted Review and e-discovery. This immediacy rules out supervision as well\nas the use of a priori training sets. We show that the various senses of a term\ncan be automatically made apparent with a simple clustering algorithm, each\nsense being a vector in the semantic space. While we only consider here\nsemantic spaces built by using random vectors, this algorithm should work with\nany kind of embedding, provided meaningful similarities between terms can be\ncomputed and do fulfill at least the two basic conditions that terms which\nclose meanings have high similarities and terms with unrelated meanings have\nnear-zero similarities.", "published": "2018-02-07 19:27:27", "link": "http://arxiv.org/abs/1802.02605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhance word representation for out-of-vocabulary on Ubuntu dialogue\n  corpus", "abstract": "Ubuntu dialogue corpus is the largest public available dialogue corpus to\nmake it feasible to build end-to-end deep neural network models directly from\nthe conversation data. One challenge of Ubuntu dialogue corpus is the large\nnumber of out-of-vocabulary words. In this paper we proposed a method which\ncombines the general pre-trained word embedding vectors with those generated on\nthe task-specific training set to address this issue. We integrated character\nembedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate\nthe effectiveness of our proposed method. For the task of next utterance\nselection, the proposed method has demonstrated a significant performance\nimprovement against original ESIM and the new model has achieved\nstate-of-the-art results on both Ubuntu dialogue corpus and Douban conversation\ncorpus. In addition, we investigated the performance impact of end-of-utterance\nand end-of-turn token tags.", "published": "2018-02-07 19:41:02", "link": "http://arxiv.org/abs/1802.02614v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Amortized Variational Autoencoders", "abstract": "Amortized variational inference (AVI) replaces instance-specific local\ninference with a global inference network. While AVI has enabled efficient\ntraining of deep generative models such as variational autoencoders (VAE),\nrecent empirical work suggests that inference networks can produce suboptimal\nvariational parameters. We propose a hybrid approach, to use AVI to initialize\nthe variational parameters and run stochastic variational inference (SVI) to\nrefine them. Crucially, the local SVI procedure is itself differentiable, so\nthe inference network and generative model can be trained end-to-end with\ngradient-based optimization. This semi-amortized approach enables the use of\nrich generative models without experiencing the posterior-collapse phenomenon\ncommon in training VAEs for problems like text generation. Experiments show\nthis approach outperforms strong autoregressive and variational baselines on\nstandard text and image datasets.", "published": "2018-02-07 18:06:42", "link": "http://arxiv.org/abs/1802.02550v7", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Polisis: Automated Analysis and Presentation of Privacy Policies Using\n  Deep Learning", "abstract": "Privacy policies are the primary channel through which companies inform users\nabout their data collection and sharing practices. These policies are often\nlong and difficult to comprehend. Short notices based on information extracted\nfrom privacy policies have been shown to be useful but face a significant\nscalability hurdle, given the number of policies and their evolution over time.\nCompanies, users, researchers, and regulators still lack usable and scalable\ntools to cope with the breadth and depth of privacy policies. To address these\nhurdles, we propose an automated framework for privacy policy analysis\n(Polisis). It enables scalable, dynamic, and multi-dimensional queries on\nnatural language privacy policies. At the core of Polisis is a privacy-centric\nlanguage model, built with 130K privacy policies, and a novel hierarchy of\nneural-network classifiers that accounts for both high-level aspects and\nfine-grained details of privacy practices. We demonstrate Polisis' modularity\nand utility with two applications supporting structured and free-form querying.\nThe structured querying application is the automated assignment of privacy\nicons from privacy policies. With Polisis, we can achieve an accuracy of 88.4%\non this task. The second application, PriBot, is the first freeform\nquestion-answering system for privacy policies. We show that PriBot can produce\na correct answer among its top-3 results for 82% of the test questions. Using\nan MTurk user study with 700 participants, we show that at least one of\nPriBot's top-3 answers is relevant to users for 89% of the test questions.", "published": "2018-02-07 18:36:38", "link": "http://arxiv.org/abs/1802.02561v2", "categories": ["cs.CL", "cs.CR", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning from Past Mistakes: Improving Automatic Speech Recognition\n  Output via Noisy-Clean Phrase Context Modeling", "abstract": "Automatic speech recognition (ASR) systems often make unrecoverable errors\ndue to subsystem pruning (acoustic, language and pronunciation models); for\nexample pruning words due to acoustics using short-term context, prior to\nrescoring with long-term context based on linguistics. In this work we model\nASR as a phrase-based noisy transformation channel and propose an error\ncorrection system that can learn from the aggregate errors of all the\nindependent modules constituting the ASR and attempt to invert those. The\nproposed system can exploit long-term context using a neural network language\nmodel and can better choose between existing ASR output possibilities as well\nas re-introduce previously pruned or unseen (out-of-vocabulary) phrases. It\nprovides corrections under poorly performing ASR conditions without degrading\nany accurate transcriptions; such corrections are greater on top of\nout-of-domain and mismatched data ASR. Our system consistently provides\nimprovements over the baseline ASR, even when baseline is further optimized\nthrough recurrent neural network language model rescoring. This demonstrates\nthat any ASR improvements can be exploited independently and that our proposed\nsystem can potentially still provide benefits on highly optimized ASR. Finally,\nwe present an extensive analysis of the type of errors corrected by our system.", "published": "2018-02-07 19:30:17", "link": "http://arxiv.org/abs/1802.02607v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Joint Modeling of Accents and Acoustics for Multi-Accent Speech\n  Recognition", "abstract": "The performance of automatic speech recognition systems degrades with\nincreasing mismatch between the training and testing scenarios. Differences in\nspeaker accents are a significant source of such mismatch. The traditional\napproach to deal with multiple accents involves pooling data from several\naccents during training and building a single model in multi-task fashion,\nwhere tasks correspond to individual accents. In this paper, we explore an\nalternate model where we jointly learn an accent classifier and a multi-task\nacoustic model. Experiments on the American English Wall Street Journal and\nBritish English Cambridge corpora demonstrate that our joint model outperforms\nthe strong multi-task acoustic model baseline. We obtain a 5.94% relative\nimprovement in word error rate on British English, and 9.47% relative\nimprovement on American English. This illustrates that jointly modeling with\naccent information improves acoustic model performance.", "published": "2018-02-07 22:05:18", "link": "http://arxiv.org/abs/1802.02656v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Divide and Conquer Strategy for Musical Noise-free Speech Enhancement\n  in Adverse Environments", "abstract": "A divide and conquer strategy for enhancement of noisy speeches in adverse\nenvironments involving lower levels of SNR is presented in this paper, where\nthe total system of speech enhancement is divided into two separate steps. The\nfirst step is based on noise compensation on short time magnitude and the\nsecond step is based on phase compensation. The magnitude spectrum is\ncompensated based on a modified spectral subtraction method where the\ncross-terms containing spectra of noise and clean speech are taken into\nconsideration, which are neglected in the traditional spectral subtraction\nmethods. By employing the modified magnitude and unchanged phase, a procedure\nis formulated to compensate the overestimation or underestimation of noise by\nphase compensation method based on the probability of speech presence. A\nmodified complex spectrum based on these two steps are obtained to synthesize a\nmusical noise free enhanced speech. Extensive simulations are carried out using\nthe speech files available in the NOIZEUS database in order to evaluate the\nperformance of the proposed method. It is shown in terms of the objective\nmeasures, spectrogram analysis and formal subjective listening tests that the\nproposed method consistently outperforms some of the state-of-the-art methods\nof speech enhancement for noisy speech corrupted by street or babble noise at\nvery low as well as medium levels of SNR.", "published": "2018-02-07 22:48:08", "link": "http://arxiv.org/abs/1802.02665v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recognition of Acoustic Events Using Masked Conditional Neural Networks", "abstract": "Automatic feature extraction using neural networks has accomplished\nremarkable success for images, but for sound recognition, these models are\nusually modified to fit the nature of the multi-dimensional temporal\nrepresentation of the audio signal in spectrograms. This may not efficiently\nharness the time-frequency representation of the signal. The ConditionaL Neural\nNetwork (CLNN) takes into consideration the interrelation between the temporal\nframes, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN\nby forcing a systematic sparseness over the network's weights using a binary\nmask. The masking allows the network to learn about frequency bands rather than\nbins, mimicking a filterbank used in signal transformations such as MFCC.\nAdditionally, the Mask is designed to consider various combinations of\nfeatures, which automates the feature hand-crafting process. We applied the\nMCLNN for the Environmental Sound Recognition problem using the Urbansound8k,\nYorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive\nperformance compared to state-of-the-art Convolutional Neural Networks and\nhand-crafted attempts.", "published": "2018-02-07 19:58:50", "link": "http://arxiv.org/abs/1802.02617v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
