{"title": "What Should/Do/Can LSTMs Learn When Parsing Auxiliary Verb\n  Constructions?", "abstract": "There is a growing interest in investigating what neural NLP models learn\nabout language. A prominent open question is the question of whether or not it\nis necessary to model hierarchical structure. We present a linguistic\ninvestigation of a neural parser adding insights to this question. We look at\ntransitivity and agreement information of auxiliary verb constructions (AVCs)\nin comparison to finite main verbs (FMVs). This comparison is motivated by\ntheoretical work in dependency grammar and in particular the work of Tesni\\`ere\n(1959) where AVCs and FMVs are both instances of a nucleus, the basic unit of\nsyntax. An AVC is a dissociated nucleus, it consists of at least two words, and\nan FMV is its non-dissociated counterpart, consisting of exactly one word. We\nsuggest that the representation of AVCs and FMVs should capture similar\ninformation. We use diagnostic classifiers to probe agreement and transitivity\ninformation in vectors learned by a transition-based neural parser in four\ntypologically different languages. We find that the parser learns different\ninformation about AVCs and FMVs if only sequential models (BiLSTMs) are used in\nthe architecture but similar information when a recursive layer is used. We\nfind explanations for why this is the case by looking closely at how\ninformation is learned in the network and looking at what happens with\ndifferent dependency representations of AVCs. We conclude that there may be\nbenefits to using a recursive layer in dependency parsing and that we have not\nyet found the best way to integrate it in our parsers.", "published": "2019-07-18 09:37:38", "link": "http://arxiv.org/abs/1907.07950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Neural Machine Translation by Simplification: The Case of\n  Encoder-free Models", "abstract": "In this paper, we try to understand neural machine translation (NMT) via\nsimplifying NMT architectures and training encoder-free NMT models. In an\nencoder-free model, the sums of word embeddings and positional embeddings\nrepresent the source. The decoder is a standard Transformer or recurrent neural\nnetwork that directly attends to embeddings via attention mechanisms.\nExperimental results show (1) that the attention mechanism in encoder-free\nmodels acts as a strong feature extractor, (2) that the word embeddings in\nencoder-free models are competitive to those in conventional models, (3) that\nnon-contextualized source representations lead to a big performance drop, and\n(4) that encoder-free models have different effects on alignment quality for\nGerman-English and Chinese-English.", "published": "2019-07-18 16:59:40", "link": "http://arxiv.org/abs/1907.08158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Utility of Document Embedding Vector Difference for\n  Relation Learning", "abstract": "Recent work has demonstrated that vector offsets obtained by subtracting\npretrained word embedding vectors can be used to predict lexical relations with\nsurprising accuracy. Inspired by this finding, in this paper, we extend the\nidea to the document level, in generating document-level embeddings,\ncalculating the distance between them, and using a linear classifier to\nclassify the relation between the documents. In the context of duplicate\ndetection and dialogue act tagging tasks, we show that document-level\ndifference vectors have utility in assessing document-level similarity, but\nperform less well in multi-relational classification.", "published": "2019-07-18 17:47:22", "link": "http://arxiv.org/abs/1907.08184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLATE: A Super-Lightweight Annotation Tool for Experts", "abstract": "Many annotation tools have been developed, covering a wide variety of tasks\nand providing features like user management, pre-processing, and automatic\nlabeling. However, all of these tools use Graphical User Interfaces, and often\nrequire substantial effort to install and configure. This paper presents a new\nannotation tool that is designed to fill the niche of a lightweight interface\nfor users with a terminal-based workflow. Slate supports annotation at\ndifferent scales (spans of characters, tokens, and lines, or a document) and of\ndifferent types (free text, labels, and links), with easily customisable\nkeybindings, and unicode support. In a user study comparing with other tools it\nwas consistently the easiest to install and use. Slate fills a need not met by\nexisting systems, and has already been used to annotate two corpora, one of\nwhich involved over 250 hours of annotation effort.", "published": "2019-07-18 18:32:16", "link": "http://arxiv.org/abs/1907.08236v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Named Entity Recognition and Entity Linking", "abstract": "Named entity recognition (NER) and entity linking (EL) are two fundamentally\nrelated tasks, since in order to perform EL, first the mentions to entities\nhave to be detected. However, most entity linking approaches disregard the\nmention detection part, assuming that the correct mentions have been previously\ndetected. In this paper, we perform joint learning of NER and EL to leverage\ntheir relatedness and obtain a more robust and generalisable system. For that,\nwe introduce a model inspired by the Stack-LSTM approach (Dyer et al., 2015).\nWe observe that, in fact, doing multi-task learning of NER and EL improves the\nperformance in both tasks when comparing with models trained with individual\nobjectives. Furthermore, we achieve results competitive with the\nstate-of-the-art in both NER and EL.", "published": "2019-07-18 18:47:33", "link": "http://arxiv.org/abs/1907.08243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis", "abstract": "Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324", "published": "2019-07-18 01:00:42", "link": "http://arxiv.org/abs/1907.07826v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ELG: An Event Logic Graph", "abstract": "The evolution and development of events have their own basic principles,\nwhich make events happen sequentially. Therefore, the discovery of such\nevolutionary patterns among events are of great value for event prediction,\ndecision-making and scenario design of dialog systems. However, conventional\nknowledge graph mainly focuses on the entities and their relations, which\nneglects the real world events. In this paper, we present a novel type of\nknowledge base - Event Logic Graph (ELG), which can reveal evolutionary\npatterns and development logics of real world events. Specifically, ELG is a\ndirected cyclic graph, whose nodes are events, and edges stand for the\nsequential, causal, conditional or hypernym-hyponym (is-a) relations between\nevents. We constructed two domain ELG: financial domain ELG, which consists of\nmore than 1.5 million of event nodes and more than 1.8 million of directed\nedges, and travel domain ELG, which consists of about 30 thousand of event\nnodes and more than 234 thousand of directed edges. Experimental results show\nthat ELG is effective for the task of script event prediction.", "published": "2019-07-18 12:39:12", "link": "http://arxiv.org/abs/1907.08015v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "OCC: A Smart Reply System for Efficient In-App Communications", "abstract": "Smart reply systems have been developed for various messaging platforms. In\nthis paper, we introduce Uber's smart reply system: one-click-chat (OCC), which\nis a key enhanced feature on top of the Uber in-app chat system. It enables\ndriver-partners to quickly respond to rider messages using smart replies. The\nsmart replies are dynamically selected according to conversation content using\nmachine learning algorithms. Our system consists of two major components:\nintent detection and reply retrieval, which are very different from standard\nsmart reply systems where the task is to directly predict a reply. It is\ndesigned specifically for mobile applications with short and non-canonical\nmessages. Reply retrieval utilizes pairings between intent and reply based on\ntheir popularity in chat messages as derived from historical data. For intent\ndetection, a set of embedding and classification techniques are experimented\nwith, and we choose to deploy a solution using unsupervised distributed\nembedding and nearest-neighbor classifier. It has the advantage of only\nrequiring a small amount of labeled training data, simplicity in developing and\ndeploying to production, and fast inference during serving and hence highly\nscalable. At the same time, it performs comparably with deep learning\narchitectures such as word-level convolutional neural network. Overall, the\nsystem achieves a high accuracy of 76% on intent detection. Currently, the\nsystem is deployed in production for English-speaking countries and 71% of\nin-app communications between riders and driver-partners adopted the smart\nreplies to speedup the communication process.", "published": "2019-07-18 17:19:30", "link": "http://arxiv.org/abs/1907.08167v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Querying Knowledge via Multi-Hop English Questions", "abstract": "The inherent difficulty of knowledge specification and the lack of trained\nspecialists are some of the key obstacles on the way to making intelligent\nsystems based on the knowledge representation and reasoning (KRR) paradigm\ncommonplace. Knowledge and query authoring using natural language, especially\ncontrolled natural language (CNL), is one of the promising approaches that\ncould enable domain experts, who are not trained logicians, to both create\nformal knowledge and query it. In previous work, we introduced the KALM system\n(Knowledge Authoring Logic Machine) that supports knowledge authoring (and\nsimple querying) with very high accuracy that at present is unachievable via\nmachine learning approaches. The present paper expands on the question\nanswering aspect of KALM and introduces KALM-QA (KALM for Question Answering)\nthat is capable of answering much more complex English questions. We show that\nKALM-QA achieves 100% accuracy on an extensive suite of movie-related\nquestions, called MetaQA, which contains almost 29,000 test questions and over\n260,000 training questions. We contrast this with a published machine learning\napproach, which falls far short of this high mark.", "published": "2019-07-18 17:37:13", "link": "http://arxiv.org/abs/1907.08176v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Neural Models for Medical Concept Normalization in User-Generated\n  Texts", "abstract": "In this work, we consider the medical concept normalization problem, i.e.,\nthe problem of mapping a health-related entity mention in a free-form text to a\nconcept in a controlled vocabulary, usually to the standard thesaurus in the\nUnified Medical Language System (UMLS). This is a challenging task since\nmedical terminology is very different when coming from health care\nprofessionals or from the general public in the form of social media texts. We\napproach it as a sequence learning problem with powerful neural networks such\nas recurrent neural networks and contextualized word representation models\ntrained to obtain semantic representations of social media expressions. Our\nexperimental evaluation over three different benchmarks shows that neural\narchitectures leverage the semantic meaning of the entity mention and\nsignificantly outperform an existing state of the art models.", "published": "2019-07-18 10:36:03", "link": "http://arxiv.org/abs/1907.07972v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WriterForcing: Generating more interesting story endings", "abstract": "We study the problem of generating interesting endings for stories. Neural\ngenerative models have shown promising results for various text generation\nproblems. Sequence to Sequence (Seq2Seq) models are typically trained to\ngenerate a single output sequence for a given input sequence. However, in the\ncontext of a story, multiple endings are possible. Seq2Seq models tend to\nignore the context and generate generic and dull responses. Very few works have\nstudied generating diverse and interesting story endings for a given story\ncontext. In this paper, we propose models which generate more diverse and\ninteresting outputs by 1) training models to focus attention on important\nkeyphrases of the story, and 2) promoting generation of non-generic words. We\nshow that the combination of the two leads to more diverse and interesting\nendings.", "published": "2019-07-18 19:29:29", "link": "http://arxiv.org/abs/1907.08259v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SentiMATE: Learning to play Chess through Natural Language Processing", "abstract": "We present SentiMATE, a novel end-to-end Deep Learning model for Chess,\nemploying Natural Language Processing that aims to learn an effective\nevaluation function assessing move quality. This function is pre-trained on the\nsentiment of commentary associated with the training moves and is used to guide\nand optimize the agent's game-playing decision making. The contributions of\nthis research are three-fold: we build and put forward both a classifier which\nextracts commentary describing the quality of Chess moves in vast commentary\ndatasets, and a Sentiment Analysis model trained on Chess commentary to\naccurately predict the quality of said moves, to then use those predictions to\nevaluate the optimal next move of a Chess agent. Both classifiers achieve over\n90 % classification accuracy. Lastly, we present a Chess engine, SentiMATE,\nwhich evaluates Chess moves based on a pre-trained sentiment evaluation\nfunction. Our results exhibit strong evidence to support our initial hypothesis\n- \"Can Natural Language Processing be used to train a novel and sample\nefficient evaluation function in Chess Engines?\" - as we integrate our\nevaluation function into modern Chess engines and play against agents with\ntraditional Chess move evaluation functions, beating both random agents and a\nDeepChess implementation at a level-one search depth - representing the number\nof moves a traditional Chess agent (employing the alpha-beta search algorithm)\nlooks ahead in order to evaluate a given chess state.", "published": "2019-07-18 23:48:21", "link": "http://arxiv.org/abs/1907.08321v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Forward-Backward Decoding for Regularizing End-to-End TTS", "abstract": "Neural end-to-end TTS can generate very high-quality synthesized speech, and\neven close to human recording within similar domain text. However, it performs\nunsatisfactory when scaling it to challenging test sets. One concern is that\nthe encoder-decoder with attention-based network adopts autoregressive\ngenerative sequence model with the limitation of \"exposure bias\" To address\nthis issue, we propose two novel methods, which learn to predict future by\nimproving agreement between forward and backward decoding sequence. The first\none is achieved by introducing divergence regularization terms into model\ntraining objective to reduce the mismatch between two directional models,\nnamely L2R and R2L (which generates targets from left-to-right and\nright-to-left, respectively). While the second one operates on decoder-level\nand exploits the future information during decoding. In addition, we employ a\njoint training strategy to allow forward and backward decoding to improve each\nother in an interactive process. Experimental results show our proposed methods\nespecially the second one (bidirectional decoder regularization), leads a\nsignificantly improvement on both robustness and overall naturalness, as\noutperforming baseline (the revised version of Tacotron2) with a MOS gap of\n0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49\nin MOS) on general test.", "published": "2019-07-18 12:24:30", "link": "http://arxiv.org/abs/1907.09006v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic vocal tract landmark localization from midsagittal MRI data", "abstract": "The various speech sounds of a language are obtained by varying the shape and\nposition of the articulators surrounding the vocal tract. Analyzing their\nvariations is crucial for understanding speech production, diagnosing speech\ndisorders and planning therapy. Identifying key anatomical landmarks of these\nstructures on medical images is a pre-requisite for any quantitative analysis\nand the rising amount of data generated in the field calls for an automatic\nsolution. The challenge lies in the high inter- and intra-speaker variability,\nthe mutual interaction between the articulators and the moderate quality of the\nimages. This study addresses this issue for the first time and tackles it by\nmeans by means of Deep Learning. It proposes a dedicated network architecture\nnamed Flat-net and its performance are evaluated and compared with eleven\nstate-of-the-art methods from the literature. The dataset contains midsagittal\nanatomical Magnetic Resonance Images for 9 speakers sustaining 62 articulations\nwith 21 annotated anatomical landmarks per image. Results show that the\nFlat-net approach outperforms the former methods, leading to an overall Root\nMean Square Error of 3.6 pixels/0.36 cm obtained in a leave-one-out procedure\nover the speakers. The implementation codes are also shared publicly on GitHub.", "published": "2019-07-18 09:38:09", "link": "http://arxiv.org/abs/1907.07951v2", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
{"title": "Leveraging Knowledge Bases And Parallel Annotations For Music Genre\n  Translation", "abstract": "Prevalent efforts have been put in automatically inferring genres of musical\nitems. Yet, the propose solutions often rely on simplifications and fail to\naddress the diversity and subjectivity of music genres. Accounting for these\nhas, though, many benefits for aligning knowledge sources, integrating data and\nenriching musical items with tags. Here, we choose a new angle for the genre\nstudy by seeking to predict what would be the genres of musical items in a\ntarget tag system, knowing the genres assigned to them within source tag\nsystems. We call this a translation task and identify three cases: 1) no common\nannotated corpus between source and target tag systems exists, 2) such a large\ncorpus exists, 3) only few common annotations exist. We propose the related\nsolutions: a knowledge-based translation modeled as taxonomy mapping, a\nstatistical translation modeled with maximum likelihood logistic regression; a\nhybrid translation modeled with maximum a posteriori logistic regression with\npriors given by the knowledge-based translation. During evaluation, the\nsolutions fit well the identified cases and the hybrid translation is\nsystematically the most effective w.r.t. multilabel classification metrics.\nThis is a first attempt to unify genre tag systems by leveraging both\nrepresentation and interpretation diversity.", "published": "2019-07-18 15:23:15", "link": "http://arxiv.org/abs/1907.08698v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
