{"title": "Modeling Context With Linear Attention for Scalable Document-Level\n  Translation", "abstract": "Document-level machine translation leverages inter-sentence dependencies to\nproduce more coherent and consistent translations. However, these models,\npredominantly based on transformers, are difficult to scale to long documents\nas their attention layers have quadratic complexity in the sequence length.\nRecent efforts on efficient attention improve scalability, but their effect on\ndocument translation remains unexplored. In this work, we investigate the\nefficacy of a recent linear attention model by Peng et al. (2021) on document\ntranslation and augment it with a sentential gate to promote a recency\ninductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018\nagainst the transformer, demonstrating substantially increased decoding speed\non long sequences with similar or better BLEU scores. We show that sentential\ngating further improves translation quality on IWSLT.", "published": "2022-10-16 03:41:50", "link": "http://arxiv.org/abs/2210.08431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning", "abstract": "Existing automatic story evaluation methods place a premium on story lexical\nlevel coherence, deviating from human preference. We go beyond this limitation\nby considering a novel \\textbf{Story} \\textbf{E}valuation method that mimics\nhuman preference when judging a story, namely \\textbf{StoryER}, which consists\nof three sub-tasks: \\textbf{R}anking, \\textbf{R}ating and \\textbf{R}easoning.\nGiven either a machine-generated or a human-written story, StoryER requires the\nmachine to output 1) a preference score that corresponds to human preference,\n2) specific ratings and their corresponding confidences and 3) comments for\nvarious aspects (e.g., opening, character-shaping). To support these tasks, we\nintroduce a well-annotated dataset comprising (i) 100k ranked story pairs; and\n(ii) a set of 46k ratings and comments on various aspects of the story. We\nfinetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the\nencoder responsible for preference score and aspect prediction and the decoder\nfor comment generation. Our comprehensive experiments result in a competitive\nbenchmark for each task, showing the high correlation to human preference. In\naddition, we have witnessed the joint learning of the preference scores, the\naspect ratings, and the comments brings gain in each single task. Our dataset\nand benchmarks are publicly available to advance the research of story\nevaluation tasks.\\footnote{Dataset and pre-trained model demo are available at\nanonymous website \\url{http://storytelling-lab.com/eval} and\n\\url{https://github.com/sairin1202/StoryER}}", "published": "2022-10-16 06:27:02", "link": "http://arxiv.org/abs/2210.08459v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Representation Learning with Generative Objective rather than\n  Contrastive Objective", "abstract": "Though offering amazing contextualized token-level representations, current\npre-trained language models take less attention on accurately acquiring\nsentence-level representation during their self-supervised pre-training.\nHowever, contrastive objectives which dominate the current sentence\nrepresentation learning bring little linguistic interpretability and no\nperformance guarantee on downstream semantic tasks. We instead propose a novel\ngenerative self-supervised learning objective based on phrase reconstruction.\nTo overcome the drawbacks of previous generative methods, we carefully model\nintra-sentence structure by breaking down one sentence into pieces of important\nphrases. Empirical studies show that our generative learning achieves powerful\nenough performance improvement and outperforms the current state-of-the-art\ncontrastive methods not only on the STS benchmarks, but also on downstream\nsemantic retrieval and reranking tasks. Our code is available at\nhttps://github.com/chengzhipanpan/PaSeR.", "published": "2022-10-16 07:47:46", "link": "http://arxiv.org/abs/2210.08474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This Patient Looks Like That Patient: Prototypical Networks for\n  Interpretable Diagnosis Prediction from Clinical Text", "abstract": "The use of deep neural models for diagnosis prediction from clinical text has\nshown promising results. However, in clinical practice such models must not\nonly be accurate, but provide doctors with interpretable and helpful results.\nWe introduce ProtoPatient, a novel method based on prototypical networks and\nlabel-wise attention with both of these abilities. ProtoPatient makes\npredictions based on parts of the text that are similar to prototypical\npatients - providing justifications that doctors understand. We evaluate the\nmodel on two publicly available clinical datasets and show that it outperforms\nexisting baselines. Quantitative and qualitative evaluations with medical\ndoctors further demonstrate that the model provides valuable explanations for\nclinical decision support.", "published": "2022-10-16 10:12:07", "link": "http://arxiv.org/abs/2210.08500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CDConv: A Benchmark for Contradiction Detection in Chinese Conversations", "abstract": "Dialogue contradiction is a critical issue in open-domain dialogue systems.\nThe contextualization nature of conversations makes dialogue contradiction\ndetection rather challenging. In this work, we propose a benchmark for\nContradiction Detection in Chinese Conversations, namely CDConv. It contains\n12K multi-turn conversations annotated with three typical contradiction\ncategories: Intra-sentence Contradiction, Role Confusion, and History\nContradiction. To efficiently construct the CDConv conversations, we devise a\nseries of methods for automatic conversation generation, which simulate common\nuser behaviors that trigger chatbots to make contradictions. We conduct careful\nmanual quality screening of the constructed conversations and show that\nstate-of-the-art Chinese chatbots can be easily goaded into making\ncontradictions. Experiments on CDConv show that properly modeling contextual\ninformation is critical for dialogue contradiction detection, but there are\nstill unresolved challenges that require future research.", "published": "2022-10-16 11:37:09", "link": "http://arxiv.org/abs/2210.08511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Some Languages are More Equal than Others: Probing Deeper into the\n  Linguistic Disparity in the NLP World", "abstract": "Linguistic disparity in the NLP world is a problem that has been widely\nacknowledged recently. However, different facets of this problem, or the\nreasons behind this disparity are seldom discussed within the NLP community.\nThis paper provides a comprehensive analysis of the disparity that exists\nwithin the languages of the world. We show that simply categorising languages\nconsidering data availability may not be always correct. Using an existing\nlanguage categorisation based on speaker population and vitality, we analyse\nthe distribution of language data resources, amount of NLP/CL research,\ninclusion in multilingual web-based platforms and the inclusion in pre-trained\nmultilingual models. We show that many languages do not get covered in these\nresources or platforms, and even within the languages belonging to the same\nlanguage group, there is wide disparity. We analyse the impact of family,\ngeographical location, GDP and the speaker population of languages and provide\npossible reasons for this disparity, along with some suggestions to overcome\nthe same.", "published": "2022-10-16 12:50:30", "link": "http://arxiv.org/abs/2210.08523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Prompting in Pre-trained Language Model for Natural Language\n  Understanding", "abstract": "Knowledge-enhanced Pre-trained Language Model (PLM) has recently received\nsignificant attention, which aims to incorporate factual knowledge into PLMs.\nHowever, most existing methods modify the internal structures of fixed types of\nPLMs by stacking complicated modules, and introduce redundant and irrelevant\nfactual knowledge from knowledge bases (KBs). In this paper, to address these\nproblems, we introduce a seminal knowledge prompting paradigm and further\npropose a knowledge-prompting-based PLM framework KP-PLM. This framework can be\nflexibly combined with existing mainstream PLMs. Specifically, we first\nconstruct a knowledge sub-graph from KBs for each context. Then we design\nmultiple continuous prompts rules and transform the knowledge sub-graph into\nnatural language prompts. To further leverage the factual knowledge from these\nprompts, we propose two novel knowledge-aware self-supervised tasks including\nprompt relevance inspection and masked prompt modeling. Extensive experiments\non multiple natural language understanding (NLU) tasks show the superiority of\nKP-PLM over other state-of-the-art methods in both full-resource and\nlow-resource settings.", "published": "2022-10-16 13:36:57", "link": "http://arxiv.org/abs/2210.08536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Learners for Natural Language Understanding via a Unified\n  Multiple Choice Perspective", "abstract": "We propose a new paradigm for zero-shot learners that is format agnostic,\ni.e., it is compatible with any format and applicable to a list of language\ntasks, such as text classification, commonsense reasoning, coreference\nresolution, and sentiment analysis. Zero-shot learning aims to train a model on\na given task such that it can address new learning tasks without any additional\ntraining. Our approach converts zero-shot learning into multiple-choice tasks,\navoiding problems in commonly used large-scale generative models such as FLAN.\nIt not only adds generalization ability to models but also significantly\nreduces the number of parameters. Our method shares the merits of efficient\ntraining and deployment. Our approach shows state-of-the-art performance on\nseveral benchmarks and produces satisfactory results on tasks such as natural\nlanguage inference and text classification. Our model achieves this success\nwith only 235M parameters, which is substantially smaller than state-of-the-art\nmodels with billions of parameters. The code and pre-trained models are\navailable at https://github.com/IDEA-CCNL/Fengshenbang-LM .", "published": "2022-10-16 17:24:06", "link": "http://arxiv.org/abs/2210.08590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventGraph: Event Extraction as Semantic Graph Parsing", "abstract": "Event extraction involves the detection and extraction of both the event\ntriggers and corresponding event arguments. Existing systems often decompose\nevent extraction into multiple subtasks, without considering their possible\ninteractions. In this paper, we propose EventGraph, a joint framework for event\nextraction, which encodes events as graphs. We represent event triggers and\narguments as nodes in a semantic graph. Event extraction therefore becomes a\ngraph parsing problem, which provides the following advantages: 1) performing\nevent detection and argument extraction jointly; 2) detecting and extracting\nmultiple events from a piece of text; and 3) capturing the complicated\ninteraction between event arguments and triggers. Experimental results on\nACE2005 show that our model is competitive to state-of-the-art systems and has\nsubstantially improved the results on argument extraction. Additionally, we\ncreate two new datasets from ACE2005 where we keep the entire text spans for\nevent arguments, instead of just the head word(s). Our code and models are\nreleased as open-source.", "published": "2022-10-16 22:11:46", "link": "http://arxiv.org/abs/2210.08646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answer ranking in Community Question Answering: a deep learning approach", "abstract": "Community Question Answering is the field of computational linguistics that\ndeals with problems derived from the questions and answers posted to websites\nsuch as Quora or Stack Overflow. Among some of these problems we find the issue\nof ranking the multiple answers posted in reply to each question by how\ninformative they are in the attempt to solve the original question. This work\ntries to advance the state of the art on answer ranking for community Question\nAnswering by proceeding with a deep learning approach. We started off by\ncreating a large data set of questions and answers posted to the Stack Overflow\nwebsite.\n  We then leveraged the natural language processing capabilities of dense\nembeddings and LSTM networks to produce a prediction for the accepted answer\nattribute, and present the answers in a ranked form ordered by how likely they\nare to be marked as accepted by the question asker. We also produced a set of\nnumerical features to assist with the answer ranking task. These numerical\nfeatures were either extracted from metadata found in the Stack Overflow posts\nor derived from the questions and answers texts. We compared the performance of\nour deep learning models against a set of forest and boosted trees ensemble\nmethods and found that our models could not improve the best baseline results.\nWe speculate that this lack of performance improvement versus the baseline\nmodels may be caused by the large number of out of vocabulary words present in\nthe programming code snippets found in the questions and answers text. We\nconclude that while a deep learning approach may be helpful in answer ranking\nproblems new methods should be developed to assist with the large number of out\nof vocabulary words present in the programming code snippets", "published": "2022-10-16 18:47:41", "link": "http://arxiv.org/abs/2212.01218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Centric Story Visualization via Visual Planning and Token\n  Alignment", "abstract": "Story visualization advances the traditional text-to-image generation by\nenabling multiple image generation based on a complete story. This task\nrequires machines to 1) understand long text inputs and 2) produce a globally\nconsistent image sequence that illustrates the contents of the story. A key\nchallenge of consistent story visualization is to preserve characters that are\nessential in stories. To tackle the challenge, we propose to adapt a recent\nwork that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a\ntext-tovisual-token (transformer) architecture. Specifically, we modify the\ntext-to-visual-token module with a two-stage framework: 1) character token\nplanning model that predicts the visual tokens for characters only; 2) visual\ntoken completion model that generates the remaining visual token sequence,\nwhich is sent to VQ-VAE for finalizing image generations. To encourage\ncharacters to appear in the images, we further train the two-stage framework\nwith a character-token alignment objective. Extensive experiments and\nevaluations demonstrate that the proposed method excels at preserving\ncharacters and can produce higher quality image sequences compared with the\nstrong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV", "published": "2022-10-16 06:50:39", "link": "http://arxiv.org/abs/2210.08465v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Semantic Matching through Dependency-Enhanced Pre-trained\n  Model with Adaptive Fusion", "abstract": "Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.", "published": "2022-10-16 07:17:27", "link": "http://arxiv.org/abs/2210.08471v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RedApt: An Adaptor for wav2vec 2 Encoding \\\\ Faster and Smaller Speech\n  Translation without Quality Compromise", "abstract": "Pre-trained speech Transformers in speech translation (ST) have facilitated\nstate-of-the-art (SotA) results; yet, using such encoders is computationally\nexpensive. To improve this, we present a novel Reducer Adaptor block, RedApt,\nthat could be seamlessly integrated within any Transformer-based speech\nencoding architecture. Integrating the pretrained wav2vec 2 speech encoder with\nRedAptbrings 41% speedup, 33% memory reduction with 24% fewer FLOPs at\ninference. To our positive surprise, our ST model with RedApt outperforms the\nSotA architecture by an average of 0.68 BLEU score on 8 language pairs from\nMust-C.", "published": "2022-10-16 07:58:25", "link": "http://arxiv.org/abs/2210.08475v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Increasing Visual Awareness in Multimodal Neural Machine Translation\n  from an Information Theoretic Perspective", "abstract": "Multimodal machine translation (MMT) aims to improve translation quality by\nequipping the source sentence with its corresponding image. Despite the\npromising performance, MMT models still suffer the problem of input\ndegradation: models focus more on textual information while visual information\nis generally overlooked. In this paper, we endeavor to improve MMT performance\nby increasing visual awareness from an information theoretic perspective. In\ndetail, we decompose the informative visual signals into two parts:\nsource-specific information and target-specific information. We use mutual\ninformation to quantify them and propose two methods for objective optimization\nto better leverage visual signals. Experiments on two datasets demonstrate that\nour approach can effectively enhance the visual awareness of MMT model and\nachieve superior results against strong baselines.", "published": "2022-10-16 08:11:44", "link": "http://arxiv.org/abs/2210.08478v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TransAlign: Fully Automatic and Effective Entity Alignment for Knowledge\n  Graphs", "abstract": "The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named TransAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nTransAlign constructs a predicate-proximity-graph to automatically capture the\nsimilarity between predicates across two KGs by learning the attention of\nentity types. For entity embeddings, TransAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nTransAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that TransAlign improves the accuracy of entity\nalignment significantly compared to state-of-the-art methods.", "published": "2022-10-16 13:48:00", "link": "http://arxiv.org/abs/2210.08540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Robustness of Natural Language Generation from Logical\n  Forms via Counterfactual Samples", "abstract": "The aim of Logic2Text is to generate controllable and faithful texts\nconditioned on tables and logical forms, which not only requires a deep\nunderstanding of the tables and logical forms, but also warrants symbolic\nreasoning over the tables. State-of-the-art methods based on pre-trained models\nhave achieved remarkable performance on the standard test dataset. However, we\nquestion whether these methods really learn how to perform logical reasoning,\nrather than just relying on the spurious correlations between the headers of\nthe tables and operators of the logical form. To verify this hypothesis, we\nmanually construct a set of counterfactual samples, which modify the original\nlogical forms to generate counterfactual logical forms with rarely co-occurred\ntable headers and logical operators. SOTA methods give much worse results on\nthese counterfactual samples compared with the results on the original test\ndataset, which verifies our hypothesis. To deal with this problem, we firstly\nanalyze this bias from a causal perspective, based on which we propose two\napproaches to reduce the model's reliance on the shortcut. The first one\nincorporates the hierarchical structure of the logical forms into the model.\nThe second one exploits automatically generated counterfactual data for\ntraining. Automatic and manual experimental results on the original test\ndataset and the counterfactual dataset show that our method is effective to\nalleviate the spurious correlation. Our work points out the weakness of\nprevious methods and takes a further step toward developing Logic2Text models\nwith real logical reasoning ability.", "published": "2022-10-16 14:14:53", "link": "http://arxiv.org/abs/2210.08548v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COFAR: Commonsense and Factual Reasoning in Image Search", "abstract": "One characteristic that makes humans superior to modern artificially\nintelligent models is the ability to interpret images beyond what is visually\napparent. Consider the following two natural language search queries - (i) \"a\nqueue of customers patiently waiting to buy ice cream\" and (ii) \"a queue of\ntourists going to see a famous Mughal architecture in India.\" Interpreting\nthese queries requires one to reason with (i) Commonsense such as interpreting\npeople as customers or tourists, actions as waiting to buy or going to see; and\n(ii) Fact or world knowledge associated with named visual entities, for\nexample, whether the store in the image sells ice cream or whether the landmark\nin the image is a Mughal architecture located in India. Such reasoning goes\nbeyond just visual recognition. To enable both commonsense and factual\nreasoning in the image search, we present a unified framework, namely Knowledge\nRetrieval-Augmented Multimodal Transformer (KRAMT), that treats the named\nvisual entities in an image as a gateway to encyclopedic knowledge and\nleverages them along with natural language query to ground relevant knowledge.\nFurther, KRAMT seamlessly integrates visual content and grounded knowledge to\nlearn alignment between images and search queries. This unified framework is\nthen used to perform image search requiring commonsense and factual reasoning.\nThe retrieval performance of KRAMT is evaluated and compared with related\napproaches on a new dataset we introduce - namely COFAR. We make our code and\ndataset available at https://vl2g.github.io/projects/cofar", "published": "2022-10-16 14:43:13", "link": "http://arxiv.org/abs/2210.08554v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Coordinated Topic Modeling", "abstract": "We propose a new problem called coordinated topic modeling that imitates\nhuman behavior while describing a text corpus. It considers a set of\nwell-defined topics like the axes of a semantic space with a reference\nrepresentation. It then uses the axes to model a corpus for easily\nunderstandable representation. This new task helps represent a corpus more\ninterpretably by reusing existing knowledge and benefits the corpora comparison\ntask. We design ECTM, an embedding-based coordinated topic model that\neffectively uses the reference representation to capture the target\ncorpus-specific aspects while maintaining each topic's global semantics. In\nECTM, we introduce the topic- and document-level supervision with a\nself-training mechanism to solve the problem. Finally, extensive experiments on\nmultiple domains show the superiority of our model over other baselines.", "published": "2022-10-16 15:10:54", "link": "http://arxiv.org/abs/2210.08559v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations\n  On-the-Fly", "abstract": "Norm discovery is important for understanding and reasoning about the\nacceptable behaviors and potential violations in human communication and\ninteractions. We introduce NormSage, a framework for addressing the novel task\nof conversation-grounded multi-lingual, multi-cultural norm discovery, based on\nlanguage model prompting and self-verification. NormSAGE leverages the\nexpressiveness and implicit knowledge of the pretrained GPT-3 language model\nbackbone, to elicit knowledge about norms through directed questions\nrepresenting the norm discovery task and conversation context. It further\naddresses the risk of language model hallucination with a self-verification\nmechanism ensuring that the norms discovered are correct and are substantially\ngrounded to their source conversations. Evaluation results show that our\napproach discovers significantly more relevant and insightful norms for\nconversations on-the-fly compared to baselines (>10+% in Likert scale rating).\nThe norms discovered from Chinese conversation are also comparable to the norms\ndiscovered from English conversation in terms of insightfulness and correctness\n(<3% difference). In addition, the culture-specific norms are promising\nquality, allowing for 80% accuracy in culture pair human identification.\nFinally, our grounding process in norm discovery self-verification can be\nextended for instantiating the adherence and violation of any norm for a given\nconversation on-the-fly, with explainability and transparency. NormSAGE\nachieves an AUC of 95.4% in grounding, with natural language explanation\nmatching human-written quality.", "published": "2022-10-16 18:30:05", "link": "http://arxiv.org/abs/2210.08604v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tracing Semantic Variation in Slang", "abstract": "The meaning of a slang term can vary in different communities. However, slang\nsemantic variation is not well understood and under-explored in the natural\nlanguage processing of slang. One existing view argues that slang semantic\nvariation is driven by culture-dependent communicative needs. An alternative\nview focuses on slang's social functions suggesting that the desire to foster\nsemantic distinction may have led to the historical emergence of\ncommunity-specific slang senses. We explore these theories using computational\nmodels and test them against historical slang dictionary entries, with a focus\non characterizing regularity in the geographical variation of slang usages\nattested in the US and the UK over the past two centuries. We show that our\nmodels are able to predict the regional identity of emerging slang word\nmeanings from historical slang records. We offer empirical evidence that both\ncommunicative need and semantic distinction play a role in the variation of\nslang meaning yet their relative importance fluctuates over the course of\nhistory. Our work offers an opportunity for incorporating historical cultural\nelements into the natural language processing of slang.", "published": "2022-10-16 20:51:14", "link": "http://arxiv.org/abs/2210.08635v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable Causal Analysis of Mental Health on Social Media Data", "abstract": "With recent developments in Social Computing, Natural Language Processing and\nClinical Psychology, the social NLP research community addresses the challenge\nof automation in mental illness on social media. A recent extension to the\nproblem of multi-class classification of mental health issues is to identify\nthe cause behind the user's intention. However, multi-class causal\ncategorization for mental health issues on social media has a major challenge\nof wrong prediction due to the overlapping problem of causal explanations.\nThere are two possible mitigation techniques to solve this problem: (i)\nInconsistency among causal explanations/ inappropriate human-annotated\ninferences in the dataset, (ii) in-depth analysis of arguments and stances in\nself-reported text using discourse analysis. In this research work, we\nhypothesise that if there exists the inconsistency among F1 scores of different\nclasses, there must be inconsistency among corresponding causal explanations as\nwell. In this task, we fine tune the classifiers and find explanations for\nmulti-class causal categorization of mental illness on social media with LIME\nand Integrated Gradient (IG) methods. We test our methods with CAMS dataset and\nvalidate with annotated interpretations. A key contribution of this research\nwork is to find the reason behind inconsistency in accuracy of multi-class\ncausal categorization. The effectiveness of our methods is evident with the\nresults obtained having category-wise average scores of $81.29 \\%$ and $0.906$\nusing cosine similarity and word mover's distance, respectively.", "published": "2022-10-16 03:34:47", "link": "http://arxiv.org/abs/2210.08430v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Model Criticism for Long-Form Text Generation", "abstract": "Language models have demonstrated the ability to generate highly fluent text;\nhowever, it remains unclear whether their output retains coherent high-level\nstructure (e.g., story progression). Here, we propose to apply a statistical\ntool, model criticism in latent space, to evaluate the high-level structure of\nthe generated text. Model criticism compares the distributions between real and\ngenerated data in a latent space obtained according to an assumptive generative\nprocess. Different generative processes identify specific failure modes of the\nunderlying model. We perform experiments on three representative aspects of\nhigh-level discourse -- coherence, coreference, and topicality -- and find that\ntransformer-based language models are able to capture topical structures but\nhave a harder time maintaining structural coherence or modeling coreference.", "published": "2022-10-16 04:35:58", "link": "http://arxiv.org/abs/2210.08444v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "TLDW: Extreme Multimodal Summarisation of News Videos", "abstract": "Multimodal summarisation with multimodal output is drawing increasing\nattention due to the rapid growth of multimedia data. While several methods\nhave been proposed to summarise visual-text contents, their multimodal outputs\nare not succinct enough at an extreme level to address the information overload\nissue. To the end of extreme multimodal summarisation, we introduce a new task,\neXtreme Multimodal Summarisation with Multimodal Output (XMSMO) for the\nscenario of TL;DW - Too Long; Didn't Watch, akin to TL;DR. XMSMO aims to\nsummarise a video-document pair into a summary with an extremely short length,\nwhich consists of one cover frame as the visual summary and one sentence as the\ntextual summary. We propose a novel unsupervised Hierarchical Optimal Transport\nNetwork (HOT-Net) consisting of three components: hierarchical multimodal\nencoders, hierarchical multimodal fusion decoders, and optimal transport\nsolvers. Our method is trained, without using reference summaries, by\noptimising the visual and textual coverage from the perspectives of the\ndistance between the semantic distributions under optimal transport plans. To\nfacilitate the study on this task, we collect a large-scale dataset XMSMO-News\nby harvesting 4,891 video-document pairs. The experimental results show that\nour method achieves promising performance in terms of ROUGE and IoU metrics.", "published": "2022-10-16 08:19:59", "link": "http://arxiv.org/abs/2210.08481v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "AskYourDB: An end-to-end system for querying and visualizing relational\n  databases using natural language", "abstract": "Querying databases for the right information is a time consuming and\nerror-prone task and often requires experienced professionals for the job.\nFurthermore, the user needs to have some prior knowledge about the database.\nThere have been various efforts to develop an intelligence which can help\nbusiness users to query databases directly. However, there has been some\nsuccesses, but very little in terms of testing and deploying those for real\nworld users. In this paper, we propose a semantic parsing approach to address\nthe challenge of converting complex natural language into SQL and institute a\nproduct out of it. For this purpose, we modified state-of-the-art models, by\nvarious pre and post processing steps which make the significant part when a\nmodel is deployed in production. To make the product serviceable to businesses\nwe added an automatic visualization framework over the queried results.", "published": "2022-10-16 13:31:32", "link": "http://arxiv.org/abs/2210.08532v1", "categories": ["cs.DB", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB"}
{"title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of\n  Self-Supervised Speech Representation Learning", "abstract": "We present the SUPERB challenge at SLT 2022, which aims at learning\nself-supervised speech representation for better performance, generalization,\nand efficiency. The challenge builds upon the SUPERB benchmark and implements\nmetrics to measure the computation requirements of self-supervised learning\n(SSL) representation and to evaluate its generalizability and performance\nacross the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive\ncoverage of popular speech processing tasks, from speech and speaker\nrecognition to audio generation and semantic understanding. As SSL has gained\ninterest in the speech community and showed promising outcomes, we envision the\nchallenge to uplevel the impact of SSL techniques by motivating more practical\ndesigns of techniques beyond task performance. We summarize the results of 14\nsubmitted models in this paper. We also discuss the main findings from those\nsubmissions and the future directions of SSL research.", "published": "2022-10-16 20:50:04", "link": "http://arxiv.org/abs/2210.08634v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Sample and Aggregate: Few-shot Reasoning over Temporal\n  Knowledge Graphs", "abstract": "In this paper, we investigate a realistic but underexplored problem, called\nfew-shot temporal knowledge graph reasoning, that aims to predict future facts\nfor newly emerging entities based on extremely limited observations in evolving\ngraphs. It offers practical value in applications that need to derive instant\nnew knowledge about new entities in temporal knowledge graphs (TKGs) with\nminimal supervision. The challenges mainly come from the few-shot and time\nshift properties of new entities. First, the limited observations associated\nwith them are insufficient for training a model from scratch. Second, the\npotentially dynamic distributions from the initially observable facts to the\nfuture facts ask for explicitly modeling the evolving characteristics of new\nentities. We correspondingly propose a novel Meta Temporal Knowledge Graph\nReasoning (MetaTKGR) framework. Unlike prior work that relies on rigid\nneighborhood aggregation schemes to enhance low-data entity representation,\nMetaTKGR dynamically adjusts the strategies of sampling and aggregating\nneighbors from recent facts for new entities, through temporally supervised\nsignals on future facts as instant feedback. Besides, such a meta temporal\nreasoning procedure goes beyond existing meta-learning paradigms on static\nknowledge graphs that fail to handle temporal adaptation with large entity\nvariance. We further provide a theoretical analysis and propose a temporal\nadaptation regularizer to stabilize the meta temporal reasoning over time.\nEmpirically, extensive experiments on three real-world TKGs demonstrate the\nsuperiority of MetaTKGR over state-of-the-art baselines by a large margin.", "published": "2022-10-16 22:40:33", "link": "http://arxiv.org/abs/2210.08654v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "End-to-end Two-dimensional Sound Source Localization With Ad-hoc\n  Microphone Arrays", "abstract": "Conventional sound source localization methods are mostly based on a single\nmicrophone array that consists of multiple microphones. They are usually\nformulated as the estimation of the direction of arrival problem. In this\npaper, we propose a deep-learning-based end-to-end sound source localization\nmethod with ad-hoc microphone arrays, where an ad-hoc microphone array is a set\nof randomly distributed microphone arrays that collaborate with each other. It\ncan produce two-dimensional locations of speakers with only a single microphone\nper node. Specifically, we divide a targeted indoor space into multiple local\nareas. We encode each local area by a one-hot code, therefore, the node and\nspeaker locations can be represented by the one-hot codes. Accordingly, the\nsound source localization problem is formulated as such a classification task\nof recognizing the one-hot code of the speaker given the one hot codes of the\nmicrophone nodes and their speech recordings. An end-to-end spatial-temporal\ndeep model is designed for the classification problem. It utilizes a\nspatial-temporal attention architecture with a fusion layer inserted in the\nmiddle of the architecture, which is able to handle arbitrarily different\nnumbers of microphone nodes during the model training and test. Experimental\nresults show that the proposed method yields good performance in highly\nreverberant and noisy environments.", "published": "2022-10-16 08:28:40", "link": "http://arxiv.org/abs/2210.08484v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Policy-based Approach to the SpecAugment Method for Low Resource E2E\n  ASR", "abstract": "SpecAugment is a very effective data augmentation method for both HMM and\nE2E-based automatic speech recognition (ASR) systems. Especially, it also works\nin low-resource scenarios. However, SpecAugment masks the spectrum of time or\nthe frequency domain in a fixed augmentation policy, which may bring relatively\nless data diversity to the low-resource ASR. In this paper, we propose a\npolicy-based SpecAugment (Policy-SpecAugment) method to alleviate the above\nproblem. The idea is to use the augmentation-select policy and the\naugmentation-parameter changing policy to solve the fixed way. These policies\nare learned based on the loss of validation set, which is applied to the\ncorresponding augmentation policies. It aims to encourage the model to learn\nmore diverse data, which the model relatively requires. In experiments, we\nevaluate the effectiveness of our approach in low-resource scenarios, i.e., the\n100 hours librispeech task. According to the results and analysis, we can see\nthat the above issue can be obviously alleviated using our proposal. In\naddition, the experimental results show that, compared with the\nstate-of-the-art SpecAugment, the proposed Policy-SpecAugment has a relative\nWER reduction of more than 10% on the Test/Dev-clean set, more than 5% on the\nTest/Dev-other set, and an absolute WER reduction of more than 1% on all test\nsets.", "published": "2022-10-16 12:39:02", "link": "http://arxiv.org/abs/2210.08520v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CTCBERT: Advancing Hidden-unit BERT with CTC Objectives", "abstract": "In this work, we present a simple but effective method, CTCBERT, for\nadvancing hidden-unit BERT (HuBERT). HuBERT applies a frame-level cross-entropy\n(CE) loss, which is similar to most acoustic model training. However, CTCBERT\nperforms the model training with the Connectionist Temporal Classification\n(CTC) objective after removing duplicated IDs in each masked region. The idea\nstems from the observation that there can be significant errors in alignments\nwhen using clustered or aligned IDs. CTC learns alignments implicitly,\nindicating that learning with CTC can be more flexible when misalignment\nexists. We examine CTCBERT on IDs from HuBERT Iter1, HuBERT Iter2, and PBERT.\nThe CTC training brings consistent improvements compared to the CE training.\nFurthermore, when loading blank-related parameters during finetuning, slight\nimprovements are observed. Evaluated on the Librispeech 960-100h setting, the\nrelative WER improvements of CTCBERT are 2%-11% over HuBERT and PERT on\ntest-other data.", "published": "2022-10-16 18:20:01", "link": "http://arxiv.org/abs/2210.08603v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention-Based Audio Embeddings for Query-by-Example", "abstract": "An ideal audio retrieval system efficiently and robustly recognizes a short\nquery snippet from an extensive database. However, the performance of\nwell-known audio fingerprinting systems falls short at high signal distortion\nlevels. This paper presents an audio retrieval system that generates noise and\nreverberation robust audio fingerprints using the contrastive learning\nframework. Using these fingerprints, the method performs a comprehensive search\nto identify the query audio and precisely estimate its timestamp in the\nreference audio. Our framework involves training a CNN to maximize the\nsimilarity between pairs of embeddings extracted from clean audio and its\ncorresponding distorted and time-shifted version. We employ a channel-wise\nspectral-temporal attention mechanism to better discriminate the audio by\ngiving more weight to the salient spectral-temporal patches in the signal.\nExperimental results indicate that our system is efficient in computation and\nmemory usage while being more accurate, particularly at higher distortion\nlevels, than competing state-of-the-art systems and scalable to a larger\ndatabase.", "published": "2022-10-16 19:37:55", "link": "http://arxiv.org/abs/2210.08624v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic-aware Non-autoregressive Spell Correction with Mask Sample\n  Decoding", "abstract": "Masked language model (MLM) has been widely used for understanding tasks,\ne.g. BERT. Recently, MLM has also been used for generation tasks. The most\npopular one in speech is using Mask-CTC for non-autoregressive speech\nrecognition. In this paper, we take one step further, and explore the\npossibility of using MLM as a non-autoregressive spell correction (SC) model\nfor transformer-transducer (TT), denoted as MLM-SC. Our initial experiments\nshow that MLM-SC provides no improvements on Librispeech data. The problem\nmight be the choice of modeling units (word pieces) and the inaccuracy of the\nTT confidence scores for English data. To solve the problem, we propose a mask\nsample decoding (MS-decode) method where the masked tokens can have the choice\nof being masked or not to compensate for the inaccuracy. As a result, we reduce\nthe WER of a streaming TT from 7.6% to 6.5% on the Librispeech test-other data\nand the CER from 7.3% to 6.1% on the Aishell test data, respectively.", "published": "2022-10-16 23:53:58", "link": "http://arxiv.org/abs/2210.08665v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust, General, and Low Complexity Acoustic Scene Classification\n  Systems and An Effective Visualization for Presenting a Sound Scene Context", "abstract": "In this paper, we present a comprehensive analysis of Acoustic Scene\nClassification (ASC), the task of identifying the scene of an audio recording\nfrom its acoustic signature. In particular, we firstly propose an\ninception-based and low footprint ASC model, referred to as the ASC baseline.\nThe proposed ASC baseline is then compared with benchmark and high-complexity\nnetwork architectures of MobileNetV1, MobileNetV2, VGG16, VGG19, ResNet50V2,\nResNet152V2, DenseNet121, DenseNet201, and Xception. Next, we improve the ASC\nbaseline by proposing a novel deep neural network architecture which leverages\nresidual-inception architectures and multiple kernels. Given the novel\nresidual-inception (NRI) model, we further evaluate the trade off between the\nmodel complexity and the model accuracy performance. Finally, we evaluate\nwhether sound events occurring in a sound scene recording can help to improve\nASC accuracy, then indicate how a sound scene context is well presented by\ncombining both sound scene and sound event information. We conduct extensive\nexperiments on various ASC datasets, including Crowded Scenes, IEEE AASP\nChallenge on Detection and Classification of Acoustic Scenes and Events (DCASE)\n2018 Task 1A and 1B, 2019 Task 1A and 1B, 2020 Task 1A, 2021 Task 1A, 2022 Task\n1. The experimental results on several different ASC challenges highlight two\nmain achievements; the first is to propose robust, general, and low complexity\nASC systems which are suitable for real-life applications on a wide range of\nedge devices and mobiles; the second is to propose an effective visualization\nmethod for comprehensively presenting a sound scene context.", "published": "2022-10-16 19:07:21", "link": "http://arxiv.org/abs/2210.08610v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
