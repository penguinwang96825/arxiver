{"title": "Balancing Lexical and Semantic Quality in Abstractive Summarization", "abstract": "An important problem of the sequence-to-sequence neural models widely used in\nabstractive summarization is exposure bias. To alleviate this problem,\nre-ranking systems have been applied in recent years. Despite some performance\nimprovements, this approach remains underexplored. Previous works have mostly\nspecified the rank through the ROUGE score and aligned candidate summaries, but\nthere can be quite a large gap between the lexical overlap metric and semantic\nsimilarity. In this paper, we propose a novel training method in which a\nre-ranker balances the lexical and semantic quality. We further newly define\nfalse positives in ranking and present a strategy to reduce their influence.\nExperiments on the CNN/DailyMail and XSum datasets show that our method can\nestimate the meaning of summaries without seriously degrading the lexical\naspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail\ndataset, reaching new state-of-the-art performance. Our code is publicly\navailable at https://github.com/jeewoo1025/BalSum.", "published": "2023-05-17 02:18:31", "link": "http://arxiv.org/abs/2305.09898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized\n  Language Models", "abstract": "By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.", "published": "2023-05-17 05:25:27", "link": "http://arxiv.org/abs/2305.09955v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smart Word Suggestions for Writing Assistance", "abstract": "Enhancing word usage is a desired feature for writing assistance. To further\nadvance research in this area, this paper introduces \"Smart Word Suggestions\"\n(SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end\nevaluation and presents a more realistic writing assistance scenario. This task\ninvolves identifying words or phrases that require improvement and providing\nsubstitution suggestions. The benchmark includes human-labeled data for\ntesting, a large distantly supervised dataset for training, and the framework\nfor evaluation. The test data includes 1,000 sentences written by English\nlearners, accompanied by over 16,000 substitution suggestions annotated by 10\nnative speakers. The training dataset comprises over 3.7 million sentences and\n12.7 million suggestions generated through rules. Our experiments with seven\nbaselines demonstrate that SWS is a challenging task. Based on experimental\nanalysis, we suggest potential directions for future research on SWS. The\ndataset and related codes is available at\nhttps://github.com/microsoft/SmartWordSuggestions.", "published": "2023-05-17 06:15:41", "link": "http://arxiv.org/abs/2305.09975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DinoSR: Self-Distillation and Online Clustering for Self-supervised\n  Speech Representation Learning", "abstract": "In this paper, we introduce self-distillation and online clustering for\nself-supervised speech representation learning (DinoSR) which combines masked\nlanguage modeling, self-distillation, and online clustering. We show that these\nconcepts complement each other and result in a strong representation learning\nmodel for speech. DinoSR first extracts contextualized embeddings from the\ninput audio with a teacher network, then runs an online clustering system on\nthe embeddings to yield a machine-discovered phone inventory, and finally uses\nthe discretized tokens to guide a student network. We show that DinoSR\nsurpasses previous state-of-the-art performance in several downstream tasks,\nand provide a detailed analysis of the model and the learned discrete units.", "published": "2023-05-17 07:23:46", "link": "http://arxiv.org/abs/2305.10005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model\n  Compression", "abstract": "Knowledge distillation has attracted a great deal of interest recently to\ncompress pre-trained language models. However, existing knowledge distillation\nmethods suffer from two limitations. First, the student model simply imitates\nthe teacher's behavior while ignoring the underlying reasoning. Second, these\nmethods usually focus on the transfer of sophisticated model-specific knowledge\nbut overlook data-specific knowledge. In this paper, we present a novel\nattribution-driven knowledge distillation approach, which explores the\ntoken-level rationale behind the teacher model based on Integrated Gradients\n(IG) and transfers attribution knowledge to the student model. To enhance the\nknowledge transfer of model reasoning and generalization, we further explore\nmulti-view attribution distillation on all potential decisions of the teacher.\nComprehensive experiments are conducted with BERT on the GLUE benchmark. The\nexperimental results demonstrate the superior performance of our approach to\nseveral state-of-the-art methods.", "published": "2023-05-17 07:40:12", "link": "http://arxiv.org/abs/2305.10010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Analysis of Oral and Nasal Vowels of Konkani", "abstract": "Konkani is a highly nasalised language which makes it unique among Indo-Aryan\nlanguages. This work investigates the acoustic-phonetic properties of Konkani\noral and nasal vowels. For this study, speech samples from six speakers (3 male\nand 3 female) were collected. A total of 74 unique sentences were used as a\npart of the recording script, 37 each for oral and nasal vowels, respectively.\nThe final data set consisted of 1135 vowel phonemes. A comparative F1-F2 plot\nof Konkani oral and nasal vowels is presented with an experimental result and\nformant analysis. The average F1, F2 and F3 values are also reported for the\nfirst time through experimentation for all nasal and oral vowels. This study\ncan be helpful for the linguistic research on vowels and speech synthesis\nsystems specific to the Konkani language.", "published": "2023-05-17 11:01:38", "link": "http://arxiv.org/abs/2305.10122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Language Model Negotiation with Self-Play and In-Context\n  Learning from AI Feedback", "abstract": "We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player's negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model's negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game's rules or cannot incorporate AI feedback for further\nimprovement. (2) Models' abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.", "published": "2023-05-17 11:55:32", "link": "http://arxiv.org/abs/2305.10142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog", "abstract": "Retrieving proper domain knowledge from an external database lies at the\nheart of end-to-end task-oriented dialog systems to generate informative\nresponses. Most existing systems blend knowledge retrieval with response\ngeneration and optimize them with direct supervision from reference responses,\nleading to suboptimal retrieval performance when the knowledge base becomes\nlarge-scale. To address this, we propose to decouple knowledge retrieval from\nresponse generation and introduce a multi-grained knowledge retriever (MAKER)\nthat includes an entity selector to search for relevant entities and an\nattribute selector to filter out irrelevant attributes. To train the retriever,\nwe propose a novel distillation objective that derives supervision signals from\nthe response generator. Experiments conducted on three standard benchmarks with\nboth small and large-scale knowledge bases demonstrate that our retriever\nperforms knowledge retrieval more effectively than existing methods. Our code\nhas been made publicly\navailable.\\footnote{https://github.com/18907305772/MAKER}", "published": "2023-05-17 12:12:46", "link": "http://arxiv.org/abs/2305.10149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural evolution via iterated learning and communication explains\n  efficient color naming systems", "abstract": "It has been argued that semantic systems reflect pressure for efficiency, and\na current debate concerns the cultural evolutionary process that produces this\npattern. We consider efficiency as instantiated in the Information Bottleneck\n(IB) principle, and a model of cultural evolution that combines iterated\nlearning and communication. We show that this model, instantiated in neural\nnetworks, converges to color naming systems that are efficient in the IB sense\nand similar to human color naming systems. We also show that some other\nproposals such as iterated learning alone, communication alone, or the greater\nlearnability of convex categories, do not yield the same outcome as clearly. We\nconclude that the combination of iterated learning and communication provides a\nplausible means by which human semantic systems become efficient.", "published": "2023-05-17 12:18:44", "link": "http://arxiv.org/abs/2305.10154v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variable-length Neural Interlingua Representations for Zero-shot Neural\n  Machine Translation", "abstract": "The language-independency of encoded representations within multilingual\nneural machine translation (MNMT) models is crucial for their generalization\nability on zero-shot translation. Neural interlingua representations have been\nshown as an effective method for achieving this. However, fixed-length neural\ninterlingua representations introduced in previous work can limit its\nflexibility and representation ability. In this study, we introduce a novel\nmethod to enhance neural interlingua representations by making their length\nvariable, thereby overcoming the constraint of fixed-length neural interlingua\nrepresentations. Our empirical results on zero-shot translation on OPUS, IWSLT,\nand Europarl datasets demonstrate stable model convergence and superior\nzero-shot translation results compared to fixed-length neural interlingua\nrepresentations. However, our analysis reveals the suboptimal efficacy of our\napproach in translating from certain source languages, wherein we pinpoint the\ndefective model component in our proposed method.", "published": "2023-05-17 13:15:10", "link": "http://arxiv.org/abs/2305.10190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Distress Support Dialogue Responses with Motivational\n  Interviewing Strategy", "abstract": "AI-driven chatbots have become an emerging solution to address psychological\ndistress. Due to the lack of psychotherapeutic data, researchers use dialogues\nscraped from online peer support forums to train them. But since the responses\nin such platforms are not given by professionals, they contain both conforming\nand non-conforming responses. In this work, we attempt to recognize these\nconforming and non-conforming response types present in online distress-support\ndialogues using labels adapted from a well-established behavioral coding scheme\nnamed Motivational Interviewing Treatment Integrity (MITI) code and show how\nsome response types could be rephrased into a more MI adherent form that can,\nin turn, enable chatbot responses to be more compliant with the MI strategy. As\na proof of concept, we build several rephrasers by fine-tuning Blender and GPT3\nto rephrase MI non-adherent \"Advise without permission\" responses into \"Advise\nwith permission\". We show how this can be achieved with the construction of\npseudo-parallel corpora avoiding costs for human labor. Through automatic and\nhuman evaluation we show that in the presence of less training data, techniques\nsuch as prompting and data augmentation can be used to produce substantially\ngood rephrasings that reflect the intended style and preserve the content of\nthe original text.", "published": "2023-05-17 13:18:28", "link": "http://arxiv.org/abs/2305.10195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken\n  Language Understanding", "abstract": "Spoken Language Understanding (SLU) is one of the core components of a\ntask-oriented dialogue system, which aims to extract the semantic meaning of\nuser queries (e.g., intents and slots). In this work, we introduce OpenSLU, an\nopen-source toolkit to provide a unified, modularized, and extensible toolkit\nfor spoken language understanding. Specifically, OpenSLU unifies 10 SLU models\nfor both single-intent and multi-intent scenarios, which support both\nnon-pretrained and pretrained models simultaneously. Additionally, OpenSLU is\nhighly modularized and extensible by decomposing the model architecture,\ninference, and learning process into reusable modules, which allows researchers\nto quickly set up SLU experiments with highly flexible configurations. OpenSLU\nis implemented based on PyTorch, and released at\n\\url{https://github.com/LightChen233/OpenSLU}.", "published": "2023-05-17 14:12:29", "link": "http://arxiv.org/abs/2305.10231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark\n  for Chinese Large Language Models", "abstract": "Large language models have recently made tremendous progress in a variety of\naspects, e.g., cross-task generalization, instruction following.\nComprehensively evaluating the capability of large language models in multiple\ntasks is of great importance. In this paper, we propose M3KE, a Massive\nMulti-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to\nmeasure knowledge acquired by Chinese large language models by testing their\nmultitask accuracy in zero- and few-shot settings. We have collected 20,477\nquestions from 71 tasks. Our selection covers all major levels of Chinese\neducation system, ranging from the primary school to college, as well as a wide\nvariety of subjects, including humanities, history, politics, law, education,\npsychology, science, technology, art and religion. All questions are\nmultiple-choice questions with four options, hence guaranteeing a standardized\nand unified assessment process. We've assessed a number of state-of-the-art\nopen-source Chinese large language models on the proposed benchmark. The size\nof these models varies from 335M to 130B parameters. Experiment results\ndemonstrate that they perform significantly worse than GPT-3.5 that reaches an\naccuracy of ~ 48% on M3KE. The dataset is available at\nhttps://github.com/tjunlp-lab/M3KE.", "published": "2023-05-17 14:56:31", "link": "http://arxiv.org/abs/2305.10263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Needles in a Haystack: On the Role of Incidental\n  Bilingualism in PaLM's Translation Capability", "abstract": "Large, multilingual language models exhibit surprisingly good zero- or\nfew-shot machine translation capabilities, despite having never seen the\nintentionally-included translation examples provided to typical neural\ntranslation systems. We investigate the role of incidental bilingualism -- the\nunintentional consumption of bilingual signals, including translation examples\n-- in explaining the translation capabilities of large language models, taking\nthe Pathways Language Model (PaLM) as a case study. We introduce a mixed-method\napproach to measure and understand incidental bilingualism at scale. We show\nthat PaLM is exposed to over 30 million translation pairs across at least 44\nlanguages. Furthermore, the amount of incidental bilingual content is highly\ncorrelated with the amount of monolingual in-language content for non-English\nlanguages. We relate incidental bilingual content to zero-shot prompts and show\nthat it can be used to mine new prompts to improve PaLM's out-of-English\nzero-shot translation quality. Finally, in a series of small-scale ablations,\nwe show that its presence has a substantial impact on translation capabilities,\nalthough this impact diminishes with model scale.", "published": "2023-05-17 14:58:06", "link": "http://arxiv.org/abs/2305.10266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models", "abstract": "In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning", "published": "2023-05-17 15:07:50", "link": "http://arxiv.org/abs/2305.10276v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FACE: Evaluating Natural Language Generation with Fourier Analysis of\n  Cross-Entropy", "abstract": "Measuring the distance between machine-produced and human language is a\ncritical open problem. Inspired by empirical findings from psycholinguistics on\nthe periodicity of entropy in language, we propose FACE, a set of metrics based\non Fourier Analysis of the estimated Cross-Entropy of language, for measuring\nthe similarity between model-generated and human-written languages. Based on an\nopen-ended generation task and the experimental data from previous studies, we\nfind that FACE can effectively identify the human-model gap, scales with model\nsize, reflects the outcomes of different sampling methods for decoding,\ncorrelates well with other evaluation metrics and with human judgment scores.", "published": "2023-05-17 15:44:57", "link": "http://arxiv.org/abs/2305.10307v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elaborative Simplification as Implicit Questions Under Discussion", "abstract": "Automated text simplification, a technique useful for making text more\naccessible to people such as children and emergent bilinguals, is often thought\nof as a monolingual translation task from complex sentences to simplified\nsentences using encoder-decoder models. This view fails to account for\nelaborative simplification, where new information is added into the simplified\ntext. This paper proposes to view elaborative simplification through the lens\nof the Question Under Discussion (QUD) framework, providing a robust way to\ninvestigate what writers elaborate upon, how they elaborate, and how\nelaborations fit into the discourse context by viewing elaborations as explicit\nanswers to implicit questions. We introduce ElabQUD, consisting of 1.3K\nelaborations accompanied with implicit QUDs, to study these phenomena. We show\nthat explicitly modeling QUD (via question generation) not only provides\nessential understanding of elaborative simplification and how the elaborations\nconnect with the rest of the discourse, but also substantially improves the\nquality of elaboration generation.", "published": "2023-05-17 17:26:16", "link": "http://arxiv.org/abs/2305.10387v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAD: BiAs Detection for Large Language Models in the context of\n  candidate screening", "abstract": "Application Tracking Systems (ATS) have allowed talent managers, recruiters,\nand college admissions committees to process large volumes of potential\ncandidate applications efficiently. Traditionally, this screening process was\nconducted manually, creating major bottlenecks due to the quantity of\napplications and introducing many instances of human bias. The advent of large\nlanguage models (LLMs) such as ChatGPT and the potential of adopting methods to\ncurrent automated application screening raises additional bias and fairness\nissues that must be addressed. In this project, we wish to identify and\nquantify the instances of social bias in ChatGPT and other OpenAI LLMs in the\ncontext of candidate screening in order to demonstrate how the use of these\nmodels could perpetuate existing biases and inequalities in the hiring process.", "published": "2023-05-17 17:47:31", "link": "http://arxiv.org/abs/2305.10407v1", "categories": ["cs.CL", "I.2, I.2.7", "F.2.2, I.2.7"], "primary_category": "cs.CL"}
{"title": "Bring More Attention to Syntactic Symmetry for Automatic Postediting of\n  High-Quality Machine Translations", "abstract": "Automatic postediting (APE) is an automated process to refine a given machine\ntranslation (MT). Recent findings present that existing APE systems are not\ngood at handling high-quality MTs even for a language pair with abundant data\nresources, English-to-German: the better the given MT is, the harder it is to\ndecide what parts to edit and how to fix these errors. One possible solution to\nthis problem is to instill deeper knowledge about the target language into the\nmodel. Thus, we propose a linguistically motivated method of regularization\nthat is expected to enhance APE models' understanding of the target language: a\nloss function that encourages symmetric self-attention on the given MT. Our\nanalysis of experimental results demonstrates that the proposed method helps\nimproving the state-of-the-art architecture's APE quality for high-quality MTs.", "published": "2023-05-17 20:25:19", "link": "http://arxiv.org/abs/2305.10557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multi-Lingual Event Understanding: Extraction, Visualization,\n  and Search", "abstract": "In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,\nzero-shot event extraction system and accompanying user interface for event\nvisualization & search. Using only English training data, ISI-Clear makes\nglobal events available on-demand, processing user-supplied text in 100\nlanguages ranging from Afrikaans to Yiddish. We provide multiple event-centric\nviews of extracted events, including both a graphical representation and a\ndocument-level summary. We also integrate existing cross-lingual search\nalgorithms with event extraction capabilities to provide cross-lingual\nevent-centric search, allowing English-speaking users to search over events\nautomatically extracted from a corpus of non-English documents, using either\nEnglish natural language queries (e.g. cholera outbreaks in Iran) or structured\nqueries (e.g. find all events of type Disease-Outbreak with agent cholera and\nlocation Iran).", "published": "2023-05-17 20:41:51", "link": "http://arxiv.org/abs/2305.10561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From chocolate bunny to chocolate crocodile: Do Language Models\n  Understand Noun Compounds?", "abstract": "Noun compound interpretation is the task of expressing a noun compound (e.g.\nchocolate bunny) in a free-text paraphrase that makes the relationship between\nthe constituent nouns explicit (e.g. bunny-shaped chocolate). We propose\nmodifications to the data and evaluation setup of the standard task (Hendrickx\net al., 2013), and show that GPT-3 solves it almost perfectly. We then\ninvestigate the task of noun compound conceptualization, i.e. paraphrasing a\nnovel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped\nchocolate. This task requires creativity, commonsense, and the ability to\ngeneralize knowledge about similar concepts. While GPT-3's performance is not\nperfect, it is better than that of humans -- likely thanks to its access to\nvast amounts of knowledge, and because conceptual processing is effortful for\npeople (Connell and Lynott, 2012). Finally, we estimate the extent to which\nGPT-3 is reasoning about the world vs. parroting its training data. We find\nthat the outputs from GPT-3 often have significant overlap with a large web\ncorpus, but that the parroting strategy is less beneficial for novel noun\ncompounds.", "published": "2023-05-17 21:05:23", "link": "http://arxiv.org/abs/2305.10568v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Better Way to Do Masked Language Model Scoring", "abstract": "Estimating the log-likelihood of a given sentence under an autoregressive\nlanguage model is straightforward: one can simply apply the chain rule and sum\nthe log-likelihood values for each successive token. However, for masked\nlanguage models (MLMs), there is no direct way to estimate the log-likelihood\nof a sentence. To address this issue, Salazar et al. (2020) propose to estimate\nsentence pseudo-log-likelihood (PLL) scores, computed by successively masking\neach sentence token, retrieving its score using the rest of the sentence as\ncontext, and summing the resulting values. Here, we demonstrate that the\noriginal PLL method yields inflated scores for out-of-vocabulary words and\npropose an adapted metric, in which we mask not only the target token, but also\nall within-word tokens to the right of the target. We show that our adapted\nmetric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric\nin which all within-word tokens are masked. In particular, it better satisfies\ntheoretical desiderata and better correlates with scores from autoregressive\nmodels. Finally, we show that the choice of metric affects even tightly\ncontrolled, minimal pair evaluation benchmarks (such as BLiMP), underscoring\nthe importance of selecting an appropriate scoring metric for evaluating MLM\nproperties.", "published": "2023-05-17 21:51:58", "link": "http://arxiv.org/abs/2305.10588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Cosine Similarity Underestimation between High Frequency Words\n  by L2 Norm Discounting", "abstract": "Cosine similarity between two words, computed using their contextualised\ntoken embeddings obtained from masked language models (MLMs) such as BERT has\nshown to underestimate the actual similarity between those words (Zhou et al.,\n2022). This similarity underestimation problem is particularly severe for\nhighly frequent words. Although this problem has been noted in prior work, no\nsolution has been proposed thus far. We observe that the L2 norm of\ncontextualised embeddings of a word correlates with its log-frequency in the\npretraining corpus. Consequently, the larger L2 norms associated with the\nhighly frequent words reduce the cosine similarity values measured between\nthem, thus underestimating the similarity scores. To solve this issue, we\npropose a method to discount the L2 norm of a contextualised word embedding by\nthe frequency of that word in a corpus when measuring the cosine similarities\nbetween words. We show that the so called stop words behave differently from\nthe rest of the words, which require special consideration during their\ndiscounting process. Experimental results on a contextualised word similarity\ndataset show that our proposed discounting method accurately solves the\nsimilarity underestimation problem.", "published": "2023-05-17 23:41:30", "link": "http://arxiv.org/abs/2305.10610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context\n  Learning", "abstract": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to\npredict future facts using knowledge of past facts. In this paper, we apply\nlarge language models (LLMs) to these benchmarks using in-context learning\n(ICL). We investigate whether and to what extent LLMs can be used for TKG\nforecasting, especially without any fine-tuning or explicit modules for\ncapturing structural and temporal information. For our experiments, we present\na framework that converts relevant historical facts into prompts and generates\nranked predictions using token probabilities. Surprisingly, we observe that\nLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully\ndesigned and trained for TKG forecasting. Our extensive evaluation presents\nperformances across several models and datasets with different characteristics,\ncompares alternative heuristics for preparing contextual information, and\ncontrasts to prominent TKG methods and simple frequency and recency baselines.\nWe also discover that using numerical indices instead of entity/relation names,\ni.e., hiding semantic information, does not significantly affect the\nperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is\nunnecessary; instead, LLMs can leverage the existing patterns in the context to\nachieve such performance. Our analysis also reveals that ICL enables LLMs to\nlearn irregular patterns from the historical context, going beyond simple\npredictions based on common or recent information.", "published": "2023-05-17 23:50:28", "link": "http://arxiv.org/abs/2305.10613v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Tuned Models are Quick Learners", "abstract": "Instruction tuning of language models has demonstrated the ability to enhance\nmodel generalization to unseen tasks via in-context learning using a few\nexamples. However, typical supervised learning still requires a plethora of\ndownstream training data for finetuning. Often in real-world situations, there\nis a scarcity of data available for finetuning, falling somewhere between few\nshot inference and fully supervised finetuning. In this work, we demonstrate\nthe sample efficiency of instruction tuned models over various tasks by\nestimating the minimal downstream training data required by them to perform\ntransfer learning and match the performance of state-of-the-art (SOTA)\nsupervised models. We conduct experiments on 119 tasks from Super Natural\nInstructions (SuperNI) in both the single task learning (STL) and multi task\nlearning (MTL) settings. Our findings reveal that, in the STL setting,\ninstruction tuned models equipped with 25% of the downstream train data surpass\nthe SOTA performance on the downstream tasks. In the MTL setting, an\ninstruction tuned model trained on only 6% of downstream training data achieve\nSOTA, while using 100% of the training data results in a 3.69% points\nimprovement (ROUGE-L 74.68) over the previous SOTA. We conduct an analysis on\nT5 vs Tk-Instruct by developing several baselines to demonstrate that\ninstruction tuning aids in increasing both sample efficiency and transfer\nlearning. Additionally, we observe a consistent ~4% performance increase in\nboth settings when pre-finetuning is performed with instructions. Finally, we\nconduct a categorical study and find that contrary to previous results, tasks\nin the question rewriting and title generation categories suffer from\ninstruction tuning.", "published": "2023-05-17 22:30:01", "link": "http://arxiv.org/abs/2306.05539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning", "abstract": "We introduce CoEdIT, a state-of-the-art text editing system for writing\nassistance. CoEdIT takes instructions from the user specifying the attributes\nof the desired text, such as \"Make the sentence simpler\" or \"Write it in a more\nneutral style,\" and outputs the edited text. We present a large language model\nfine-tuned on a diverse collection of task-specific instructions for text\nediting (a total of 82K instructions). Our model (1) achieves state-of-the-art\nperformance on various text editing benchmarks, (2) is competitive with\npublicly available largest-sized LLMs trained on instructions while being\nnearly 60x smaller, (3) is capable of generalizing to unseen edit instructions,\nand (4) exhibits abilities to generalize to composite instructions containing\ndifferent combinations of edit actions. Through extensive qualitative and\nquantitative analysis, we show that writers prefer the edits suggested by\nCoEdIT relative to other state-of-the-art text editing models. Our code, data,\nand models are publicly available at https://github.com/vipulraheja/coedit.", "published": "2023-05-17 00:05:24", "link": "http://arxiv.org/abs/2305.09857v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Smaller Language Models are Better Black-box Machine-Generated Text\n  Detectors", "abstract": "With the advent of fluent generative language models that can produce\nconvincing utterances very similar to those written by humans, distinguishing\nwhether a piece of text is machine-generated or human-written becomes more\nchallenging and more important, as such models could be used to spread\nmisinformation, fake news, fake reviews and to mimic certain authors and\nfigures. To this end, there have been a slew of methods proposed to detect\nmachine-generated text. Most of these methods need access to the logits of the\ntarget model or need the ability to sample from the target. One such black-box\ndetection method relies on the observation that generated text is locally\noptimal under the likelihood function of the generator, while human-written\ntext is not. We find that overall, smaller and partially-trained models are\nbetter universal text detectors: they can more precisely detect text generated\nfrom both small and larger models. Interestingly, we find that whether the\ndetector and generator were trained on the same data is not critically\nimportant to the detection success. For instance the OPT-125M model has an AUC\nof 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT\nfamily, GPTJ-6B, has AUC of 0.45.", "published": "2023-05-17 00:09:08", "link": "http://arxiv.org/abs/2305.09859v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clustering-Aware Negative Sampling for Unsupervised Sentence\n  Representation", "abstract": "Contrastive learning has been widely studied in sentence representation\nlearning. However, earlier works mainly focus on the construction of positive\nexamples, while in-batch samples are often simply treated as negative examples.\nThis approach overlooks the importance of selecting appropriate negative\nexamples, potentially leading to a scarcity of hard negatives and the inclusion\nof false negatives. To address these issues, we propose ClusterNS\n(Clustering-aware Negative Sampling), a novel method that incorporates cluster\ninformation into contrastive learning for unsupervised sentence representation\nlearning. We apply a modified K-means clustering algorithm to supply hard\nnegatives and recognize in-batch false negatives during training, aiming to\nsolve the two issues in one unified framework. Experiments on semantic textual\nsimilarity (STS) tasks demonstrate that our proposed ClusterNS compares\nfavorably with baselines in unsupervised sentence representation learning. Our\ncode has been made publicly available.", "published": "2023-05-17 02:06:47", "link": "http://arxiv.org/abs/2305.09892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual Semantic Knowledge Composed Multimodal Dialog Systems", "abstract": "Textual response generation is an essential task for multimodal task-oriented\ndialog systems.Although existing studies have achieved fruitful progress, they\nstill suffer from two critical limitations: 1) focusing on the attribute\nknowledge but ignoring the relation knowledge that can reveal the correlations\nbetween different entities and hence promote the response generation}, and 2)\nonly conducting the cross-entropy loss based output-level supervision but\nlacking the representation-level regularization. To address these limitations,\nwe devise a novel multimodal task-oriented dialog system (named MDS-S2).\nSpecifically, MDS-S2 first simultaneously acquires the context related\nattribute and relation knowledge from the knowledge base, whereby the\nnon-intuitive relation knowledge is extracted by the n-hop graph walk.\nThereafter, considering that the attribute knowledge and relation knowledge can\nbenefit the responding to different levels of questions, we design a\nmulti-level knowledge composition module in MDS-S2 to obtain the latent\ncomposed response representation. Moreover, we devise a set of latent query\nvariables to distill the semantic information from the composed response\nrepresentation and the ground truth response representation, respectively, and\nthus conduct the representation-level semantic regularization. Extensive\nexperiments on a public dataset have verified the superiority of our proposed\nMDS-S2. We have released the codes and parameters to facilitate the research\ncommunity.", "published": "2023-05-17 06:33:26", "link": "http://arxiv.org/abs/2305.09990v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made\n  in Black-Box Scenario", "abstract": "Large pre-trained language models (PLMs) have garnered significant attention\nfor their versatility and potential for solving a wide spectrum of natural\nlanguage processing (NLP) tasks. However, the cost of running these PLMs may be\nprohibitive. Furthermore, PLMs may not be open-sourced due to commercial\nconsiderations and potential risks of misuse, such as GPT-3. The parameters and\ngradients of PLMs are unavailable in this scenario. To solve the issue,\nblack-box tuning has been proposed, which utilizes derivative-free optimization\n(DFO), instead of gradient descent, for training task-specific continuous\nprompts. However, these gradient-free methods still exhibit a significant gap\ncompared to gradient-based methods. In this paper, we introduce gradient\ndescent into black-box tuning scenario through knowledge distillation.\nFurthermore, we propose a novel method GDFO, which integrates gradient descent\nand derivative-free optimization to optimize task-specific continuous prompts\nin a harmonized manner. Experimental results show that GDFO can achieve\nsignificant performance gains over previous state-of-the-art methods.", "published": "2023-05-17 07:48:28", "link": "http://arxiv.org/abs/2305.10013v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are You Copying My Model? Protecting the Copyright of Large Language\n  Models for EaaS via Backdoor Watermark", "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.", "published": "2023-05-17 08:28:54", "link": "http://arxiv.org/abs/2305.10036v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Can Language Models Solve Graph Problems in Natural Language?", "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.", "published": "2023-05-17 08:29:21", "link": "http://arxiv.org/abs/2305.10037v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing the Role of Positional Information in Vision-Language Models", "abstract": "In most Vision-Language models (VL), the understanding of the image structure\nis enabled by injecting the position information (PI) about objects in the\nimage. In our case study of LXMERT, a state-of-the-art VL model, we probe the\nuse of the PI in the representation and study its effect on Visual Question\nAnswering. We show that the model is not capable of leveraging the PI for the\nimage-text matching task on a challenge set where only position differs. Yet,\nour experiments with probing confirm that the PI is indeed present in the\nrepresentation. We introduce two strategies to tackle this: (i) Positional\nInformation Pre-training and (ii) Contrastive Learning on PI using\nCross-Modality Matching. Doing so, the model can correctly classify if images\nwith detailed PI statements match. Additionally to the 2D information from\nbounding boxes, we introduce the object's depth as new feature for a better\nobject localization in the space. Even though we were able to improve the model\nproperties as defined by our probes, it only has a negligible effect on the\ndownstream performance. Our results thus highlight an important issue of\nmultimodal modeling: the mere presence of information detectable by a probing\nclassifier is not a guarantee that the information is available in a\ncross-modal setup.", "published": "2023-05-17 08:38:59", "link": "http://arxiv.org/abs/2305.10046v1", "categories": ["cs.CL", "cs.CV", "I.4; I.7"], "primary_category": "cs.CL"}
{"title": "Use of a Taxonomy of Empathetic Response Intents to Control and\n  Interpret Empathy in Neural Chatbots", "abstract": "A recent trend in the domain of open-domain conversational agents is enabling\nthem to converse empathetically to emotional prompts. Current approaches either\nfollow an end-to-end approach or condition the responses on similar emotion\nlabels to generate empathetic responses. But empathy is a broad concept that\nrefers to the cognitive and emotional reactions of an individual to the\nobserved experiences of another and it is more complex than mere mimicry of\nemotion. Hence, it requires identifying complex human conversational strategies\nand dynamics in addition to generic emotions to control and interpret\nempathetic responding capabilities of chatbots. In this work, we make use of a\ntaxonomy of eight empathetic response intents in addition to generic emotion\ncategories in building a dialogue response generation model capable of\ngenerating empathetic responses in a controllable and interpretable manner. It\nconsists of two modules: 1) a response emotion/intent prediction module; and 2)\na response generation module. We propose several rule-based and neural\napproaches to predict the next response's emotion/intent and generate responses\nconditioned on these predicted emotions/intents. Automatic and human evaluation\nresults emphasize the importance of the use of the taxonomy of empathetic\nresponse intents in producing more diverse and empathetically more appropriate\nresponses than end-to-end models.", "published": "2023-05-17 10:03:03", "link": "http://arxiv.org/abs/2305.10096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Additive manifesto decomposition: A policy domain aware method for\n  understanding party positioning", "abstract": "Automatic extraction of party (dis)similarities from texts such as party\nelection manifestos or parliamentary speeches plays an increasing role in\ncomputational political science. However, existing approaches are fundamentally\nlimited to targeting only global party (dis)-similarity: they condense the\nrelationship between a pair of parties into a single figure, their similarity.\nIn aggregating over all policy domains (e.g., health or foreign policy), they\ndo not provide any qualitative insights into which domains parties agree or\ndisagree on. This paper proposes a workflow for estimating policy domain aware\nparty similarity that overcomes this limitation. The workflow covers (a)\ndefinition of suitable policy domains; (b) automatic labeling of domains, if no\nmanual labels are available; (c) computation of domain-level similarities and\naggregation at a global level; (d) extraction of interpretable party positions\non major policy axes via multidimensional scaling. We evaluate our workflow on\nmanifestos from the German federal elections. We find that our method (a)\nyields high correlation when predicting party similarity at a global level and\n(b) provides accurate party-specific positions, even with automatically\nlabelled policy domains.", "published": "2023-05-17 11:39:31", "link": "http://arxiv.org/abs/2305.10136v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Personality Understanding of Fictional Characters during Book Reading", "abstract": "Comprehending characters' personalities is a crucial aspect of story reading.\nAs readers engage with a story, their understanding of a character evolves\nbased on new events and information; and multiple fine-grained aspects of\npersonalities can be perceived. This leads to a natural problem of situated and\nfine-grained personality understanding. The problem has not been studied in the\nNLP field, primarily due to the lack of appropriate datasets mimicking the\nprocess of book reading. We present the first labeled dataset PersoNet for this\nproblem. Our novel annotation strategy involves annotating user notes from\nonline reading apps as a proxy for the original books. Experiments and human\nstudies indicate that our dataset construction is both efficient and accurate;\nand our task heavily relies on long-term context to achieve accurate\npredictions for both machines and humans. The dataset is available at\nhttps://github.com/Gorov/personet_acl23.", "published": "2023-05-17 12:19:11", "link": "http://arxiv.org/abs/2305.10156v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stop Uploading Test Data in Plain Text: Practical Strategies for\n  Mitigating Data Contamination by Evaluation Benchmarks", "abstract": "Data contamination has become prevalent and challenging with the rise of\nmodels pretrained on large automatically-crawled corpora. For closed models,\nthe training data becomes a trade secret, and even for open models, it is not\ntrivial to detect contamination. Strategies such as leaderboards with hidden\nanswers, or using test data which is guaranteed to be unseen, are expensive and\nbecome fragile with time. Assuming that all relevant actors value clean test\ndata and will cooperate to mitigate data contamination, what can be done? We\npropose three strategies that can make a difference: (1) Test data made public\nshould be encrypted with a public key and licensed to disallow derivative\ndistribution; (2) demand training exclusion controls from closed API holders,\nand protect your test data by refusing to evaluate without them; (3) avoid data\nwhich appears with its solution on the internet, and release the web-page\ncontext of internet-derived data along with the data. These strategies are\npractical and can be effective in preventing data contamination.", "published": "2023-05-17 12:23:38", "link": "http://arxiv.org/abs/2305.10160v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pragmatic Reasoning in Structured Signaling Games", "abstract": "In this work we introduce a structured signaling game, an extension of the\nclassical signaling game with a similarity structure between meanings in the\ncontext, along with a variant of the Rational Speech Act (RSA) framework which\nwe call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We\nexplore the behavior of the sRSA in the domain of color and show that pragmatic\nagents using sRSA on top of semantic representations, derived from the World\nColor Survey, attain efficiency very close to the information theoretic limit\nafter only 1 or 2 levels of recursion. We also explore the interaction between\npragmatic reasoning and learning in multi-agent reinforcement learning\nframework. Our results illustrate that artificial agents using sRSA develop\ncommunication closer to the information theoretic frontier compared to agents\nusing RSA and just reinforcement learning. We also find that the ambiguity of\nthe semantic representation increases as the pragmatic agents are allowed to\nperform deeper reasoning about each other during learning.", "published": "2023-05-17 12:43:29", "link": "http://arxiv.org/abs/2305.10167v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Knowledge-enhanced Mixed-initiative Dialogue System for Emotional\n  Support Conversations", "abstract": "Unlike empathetic dialogues, the system in emotional support conversations\n(ESC) is expected to not only convey empathy for comforting the help-seeker,\nbut also proactively assist in exploring and addressing their problems during\nthe conversation. In this work, we study the problem of mixed-initiative ESC\nwhere the user and system can both take the initiative in leading the\nconversation. Specifically, we conduct a novel analysis on mixed-initiative ESC\nsystems with a tailor-designed schema that divides utterances into different\ntypes with speaker roles and initiative types. Four emotional support metrics\nare proposed to evaluate the mixed-initiative interactions. The analysis\nreveals the necessity and challenges of building mixed-initiative ESC systems.\nIn the light of this, we propose a knowledge-enhanced mixed-initiative\nframework (KEMI) for ESC, which retrieves actual case knowledge from a\nlarge-scale mental health knowledge graph for generating mixed-initiative\nresponses. Experimental results on two ESC datasets show the superiority of\nKEMI in both content-preserving evaluation and mixed initiative related\nanalyses.", "published": "2023-05-17 12:55:52", "link": "http://arxiv.org/abs/2305.10172v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Survey on Zero Pronoun Translation", "abstract": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.\nChinese, Hungarian, and Hindi), but should be recalled in non-pro-drop\nlanguages (e.g. English). This phenomenon has been studied extensively in\nmachine translation (MT), as it poses a significant challenge for MT systems\ndue to the difficulty in determining the correct antecedent for the pronoun.\nThis survey paper highlights the major works that have been undertaken in zero\npronoun translation (ZPT) after the neural revolution, so that researchers can\nrecognise the current state and future directions of this field. We provide an\norganisation of the literature based on evolution, dataset, method and\nevaluation. In addition, we compare and analyze competing models and evaluation\nmetrics on different benchmarks. We uncover a number of insightful findings\nsuch as: 1) ZPT is in line with the development trend of large language model;\n2) data limitation causes learning bias in languages and domains; 3)\nperformance improvements are often reported on single benchmarks, but advanced\nmethods are still far from real-world use; 4) general-purpose metrics are not\nreliable on nuances and complexities of ZPT, emphasizing the necessity of\ntargeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of\ngender bias.", "published": "2023-05-17 13:19:01", "link": "http://arxiv.org/abs/2305.10196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shielded Representations: Protecting Sensitive Attributes Through\n  Iterative Gradient-Based Projection", "abstract": "Natural language processing models tend to learn and encode social biases\npresent in the data. One popular approach for addressing such biases is to\neliminate encoded information from the model's representations. However,\ncurrent methods are restricted to removing only linearly encoded information.\nIn this work, we propose Iterative Gradient-Based Projection (IGBP), a novel\nmethod for removing non-linear encoded concepts from neural representations.\nOur method consists of iteratively training neural classifiers to predict a\nparticular attribute we seek to eliminate, followed by a projection of the\nrepresentation on a hypersurface, such that the classifiers become oblivious to\nthe target attribute. We evaluate the effectiveness of our method on the task\nof removing gender and race information as sensitive attributes. Our results\ndemonstrate that IGBP is effective in mitigating bias through intrinsic and\nextrinsic evaluations, with minimal impact on downstream task accuracy.", "published": "2023-05-17 13:26:57", "link": "http://arxiv.org/abs/2305.10204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A quantitative study of NLP approaches to question difficulty estimation", "abstract": "Recent years witnessed an increase in the amount of research on the task of\nQuestion Difficulty Estimation from Text QDET with Natural Language Processing\n(NLP) techniques, with the goal of targeting the limitations of traditional\napproaches to question calibration. However, almost the entirety of previous\nresearch focused on single silos, without performing quantitative comparisons\nbetween different models or across datasets from different educational domains.\nIn this work, we aim at filling this gap, by quantitatively analyzing several\napproaches proposed in previous research, and comparing their performance on\nthree publicly available real world datasets containing questions of different\ntypes from different educational domains. Specifically, we consider reading\ncomprehension Multiple Choice Questions (MCQs), science MCQs, and math\nquestions. We find that Transformer based models are the best performing across\ndifferent educational domains, with DistilBERT performing almost as well as\nBERT, and that they outperform other approaches even on smaller datasets. As\nfor the other models, the hybrid ones often outperform the ones based on a\nsingle type of features, the ones based on linguistic features perform well on\nreading comprehension questions, while frequency based features (TF-IDF) and\nword embeddings (word2vec) perform better in domain knowledge assessment.", "published": "2023-05-17 14:26:00", "link": "http://arxiv.org/abs/2305.10236v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory", "abstract": "Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.", "published": "2023-05-17 14:40:29", "link": "http://arxiv.org/abs/2305.10250v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in\n  Benchmarks", "abstract": "The evaluation of natural language processing (NLP) systems is crucial for\nadvancing the field, but current benchmarking approaches often assume that all\nsystems have scores available for all tasks, which is not always practical. In\nreality, several factors such as the cost of running baseline, private systems,\ncomputational limitations, or incomplete data may prevent some systems from\nbeing evaluated on entire tasks. This paper formalize an existing problem in\nNLP research: benchmarking when some systems scores are missing on the task,\nand proposes a novel approach to address it. Our method utilizes a compatible\npartial ranking approach to impute missing data, which is then aggregated using\nthe Borda count method. It includes two refinements designed specifically for\nscenarios where either task-level or instance-level scores are available. We\nalso introduce an extended benchmark, which contains over 131 million scores,\nan order of magnitude larger than existing benchmarks. We validate our methods\nand demonstrate their effectiveness in addressing the challenge of missing\nsystem evaluation on an entire task. This work highlights the need for more\ncomprehensive benchmarking approaches that can handle real-world scenarios\nwhere not all systems are evaluated on the entire task.", "published": "2023-05-17 15:20:31", "link": "http://arxiv.org/abs/2305.10284v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniEX: An Effective and Efficient Framework for Unified Information\n  Extraction via a Span-extractive Perspective", "abstract": "We propose a new paradigm for universal information extraction (IE) that is\ncompatible with any schema format and applicable to a list of IE tasks, such as\nnamed entity recognition, relation extraction, event extraction and sentiment\nanalysis. Our approach converts the text-based IE tasks as the token-pair\nproblem, which uniformly disassembles all extraction targets into joint span\ndetection, classification and association problems with a unified extractive\nframework, namely UniEX. UniEX can synchronously encode schema-based prompt and\ntextual information, and collaboratively learn the generalized knowledge from\npre-defined information using the auto-encoder language models. We develop a\ntraffine attention mechanism to integrate heterogeneous factors including\ntasks, labels and inside tokens, and obtain the extraction target via a scoring\nmatrix. Experiment results show that UniEX can outperform generative universal\nIE models in terms of performance and inference-speed on $14$ benchmarks IE\ndatasets with the supervised setting. The state-of-the-art performance in\nlow-resource scenarios also verifies the transferability and effectiveness of\nUniEX.", "published": "2023-05-17 15:44:12", "link": "http://arxiv.org/abs/2305.10306v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large-Scale Text Analysis Using Generative Language Models: A Case Study\n  in Discovering Public Value Expressions in AI Patents", "abstract": "Labeling data is essential for training text classifiers but is often\ndifficult to accomplish accurately, especially for complex and abstract\nconcepts. Seeking an improved method, this paper employs a novel approach using\na generative language model (GPT-4) to produce labels and rationales for\nlarge-scale text analysis. We apply this approach to the task of discovering\npublic value expressions in US AI patents. We collect a database comprising\n154,934 patent documents using an advanced Boolean query submitted to\nInnovationQ+. The results are merged with full patent text from the USPTO,\nresulting in 5.4 million sentences. We design a framework for identifying and\nlabeling public value expressions in these AI patent sentences. A prompt for\nGPT-4 is developed which includes definitions, guidelines, examples, and\nrationales for text classification. We evaluate the quality of the labels and\nrationales produced by GPT-4 using BLEU scores and topic modeling and find that\nthey are accurate, diverse, and faithful. These rationales also serve as a\nchain-of-thought for the model, a transparent mechanism for human verification,\nand support for human annotators to overcome cognitive limitations. We conclude\nthat GPT-4 achieved a high-level of recognition of public value theory from our\nframework, which it also uses to discover unseen public value expressions. We\nuse the labels produced by GPT-4 to train BERT-based classifiers and predict\nsentences on the entire database, achieving high F1 scores for the 3-class\n(0.85) and 2-class classification (0.91) tasks. We discuss the implications of\nour approach for conducting large-scale text analyses with complex and abstract\nconcepts and suggest that, with careful framework design and interactive human\noversight, generative language models can offer significant advantages in\nquality and in reduced time and costs for producing labels and rationales.", "published": "2023-05-17 17:18:26", "link": "http://arxiv.org/abs/2305.10383v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Logit-Based Ensemble Distribution Distillation for Robust Autoregressive\n  Sequence Uncertainties", "abstract": "Efficiently and reliably estimating uncertainty is an important objective in\ndeep learning. It is especially pertinent to autoregressive sequence tasks,\nwhere training and inference costs are typically very high. However, existing\nresearch has predominantly focused on tasks with static data such as image\nclassification. In this work, we investigate Ensemble Distribution Distillation\n(EDD) applied to large-scale natural language sequence-to-sequence data. EDD\naims to compress the superior uncertainty performance of an expensive (teacher)\nensemble into a cheaper (student) single model. Importantly, the ability to\nseparate knowledge (epistemic) and data (aleatoric) uncertainty is retained.\nExisting probability-space approaches to EDD, however, are difficult to scale\nto large vocabularies. We show, for modern transformer architectures on\nlarge-scale translation tasks, that modelling the ensemble logits, instead of\nsoftmax probabilities, leads to significantly better students. Moreover, the\nstudents surprisingly even outperform Deep Ensembles by up to ~10% AUROC on\nout-of-distribution detection, whilst matching them at in-distribution\ntranslation.", "published": "2023-05-17 17:21:10", "link": "http://arxiv.org/abs/2305.10384v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What You See is What You Read? Improving Text-Image Alignment Evaluation", "abstract": "Automatically determining whether a text and a corresponding image are\nsemantically aligned is a significant challenge for vision-language models,\nwith applications in generative text-to-image and image-to-text tasks. In this\nwork, we study methods for automatic text-image alignment evaluation. We first\nintroduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets\nfrom both text-to-image and image-to-text generation tasks, with human\njudgements for whether a given text-image pair is semantically aligned. We then\ndescribe two automatic methods to determine alignment: the first involving a\npipeline based on question generation and visual question answering models, and\nthe second employing an end-to-end classification approach by finetuning\nmultimodal pretrained models. Both methods surpass prior approaches in various\ntext-image alignment tasks, with significant improvements in challenging cases\nthat involve complex composition or unnatural images. Finally, we demonstrate\nhow our approaches can localize specific misalignments between an image and a\ngiven text, and how they can be used to automatically re-rank candidates in\ntext-to-image generation.", "published": "2023-05-17 17:43:38", "link": "http://arxiv.org/abs/2305.10400v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PaLM 2 Technical Report", "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.", "published": "2023-05-17 17:46:53", "link": "http://arxiv.org/abs/2305.10403v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback", "abstract": "Learning from human feedback has been shown to be effective at aligning\nlanguage models with human preferences. Past work has often relied on\nReinforcement Learning from Human Feedback (RLHF), which optimizes the language\nmodel using reward scores assigned from a reward model trained on human\npreference data. In this work we show how the recently introduced Sequence\nLikelihood Calibration (SLiC), can also be used to effectively learn from human\npreferences (SLiC-HF). Furthermore, we demonstrate this can be done with human\nfeedback data collected for a different model, similar to off-policy, offline\nRL data. Automatic and human evaluation experiments on the TL;DR summarization\ntask show that SLiC-HF significantly improves supervised fine-tuning baselines.\nFurthermore, SLiC-HF presents a competitive alternative to the PPO RLHF\nimplementation used in past work while being much simpler to implement, easier\nto tune and more computationally efficient in practice.", "published": "2023-05-17 17:57:10", "link": "http://arxiv.org/abs/2305.10425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining", "abstract": "The mixture proportions of pretraining data domains (e.g., Wikipedia, books,\nweb text) greatly affect language model (LM) performance. In this paper, we\npropose Domain Reweighting with Minimax Optimization (DoReMi), which first\ntrains a small proxy model using group distributionally robust optimization\n(Group DRO) over domains to produce domain weights (mixture proportions)\nwithout knowledge of downstream tasks. We then resample a dataset with these\ndomain weights and train a larger, full-sized model. In our experiments, we use\nDoReMi on a 280M-parameter proxy model to set the domain weights for training\nan 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi\nimproves perplexity across all domains, even when it downweights a domain.\nDoReMi improves average few-shot downstream accuracy by 6.5% points over a\nbaseline model trained using The Pile's default domain weights and reaches the\nbaseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi,\nwhich has no knowledge of downstream tasks, even matches the performance of\nusing domain weights tuned on downstream tasks.", "published": "2023-05-17 17:58:13", "link": "http://arxiv.org/abs/2305.10429v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IMAD: IMage-Augmented multi-modal Dialogue", "abstract": "Currently, dialogue systems have achieved high performance in processing\ntext-based communication. However, they have not yet effectively incorporated\nvisual information, which poses a significant challenge. Furthermore, existing\nmodels that incorporate images in dialogue generation focus on discussing the\nimage itself. Our proposed approach presents a novel perspective on multi-modal\ndialogue systems, which interprets the image in the context of the dialogue. By\ndoing so, we aim to expand the capabilities of current dialogue systems and\ntransition them from single modality (text) to multi-modality. However, there\nis a lack of validated English datasets that contain both images and dialogue\ncontexts for this task. Thus, we propose a two-stage approach to automatically\nconstruct a multi-modal dialogue dataset. In the first stage, we utilize\ntext-to-image similarity and sentence similarity to identify which utterances\ncould be replaced with an image. In the second stage, we replace those\nutterances by selecting a subset of relevant images and filtering them with a\nvisual question answering model. We used this approach, along with additional\nlabeling, to create the IMage Augmented multi-modal Dialogue dataset (IMAD),\nwhich can serve as a validated dataset for this task. Furthermore, we propose a\nbaseline model trained on this dataset, which outperforms model trained on the\nsame data without images and BlenderBot.", "published": "2023-05-17 18:38:10", "link": "http://arxiv.org/abs/2305.10512v2", "categories": ["cs.CL", "cs.HC", "I.4.10; I.7.m"], "primary_category": "cs.CL"}
{"title": "Statistical Knowledge Assessment for Large Language Models", "abstract": "Given varying prompts regarding a factoid question, can a large language\nmodel (LLM) reliably generate factually correct answers? Existing LLMs may\ngenerate distinct responses for different prompts. In this paper, we study the\nproblem of quantifying knowledge contained in an LLM regarding a given set of\nfacts. We propose KaRR, a statistical approach to assess factual knowledge for\nLLMs. The main idea is to estimate the ratio of LLM generating text\ncorresponding to the answer entity given diverse prompts of the subject and the\nquerying relation, versus it generating by random chances. Our assessment suite\ncontains a comprehensive set of 994,123 entities and 600 relations, with\n1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes,\nincluding LLaMA, Alpaca, OPT, etc. Experiments show that our results have a\nstrong correlation (0.43 Kendall's $\\tau$) with the results of human assessment\non LLMs. Our results reveal that the knowledge in LLMs with the same backbone\narchitecture adheres to the scaling law, while tuning on instruction-following\ndata sometimes compromises the model's capability to generate factually correct\ntext reliably.", "published": "2023-05-17 18:54:37", "link": "http://arxiv.org/abs/2305.10519v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding", "abstract": "We consider a contrastive learning approach to knowledge graph embedding\n(KGE) via InfoNCE. For KGE, efficient learning relies on augmenting the\ntraining data with negative triples. However, most KGE works overlook the bias\nfrom generating the negative triples-false negative triples (factual triples\nmissing from the knowledge graph). We argue that the generation of high-quality\n(i.e., hard) negative triples might lead to an increase in false negative\ntriples. To mitigate the impact of false negative triples during the generation\nof hard negative triples, we propose the Hardness and Structure-aware\n(\\textbf{HaSa}) contrastive KGE method, which alleviates the effect of false\nnegative triples while generating the hard negative triples. Experiments show\nthat HaSa improves the performance of InfoNCE-based KGE approaches and achieves\nstate-of-the-art results in several metrics for WN18RR datasets and competitive\nresults for FB15k-237 datasets compared to both classic and pre-trained\nLM-based KGE methods.", "published": "2023-05-17 20:46:18", "link": "http://arxiv.org/abs/2305.10563v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Token-wise Decomposition of Autoregressive Language Model Hidden States\n  for Analyzing Model Predictions", "abstract": "While there is much recent interest in studying why Transformer-based large\nlanguage models make predictions the way they do, the complex computations\nperformed within each layer have made their behavior somewhat opaque. To\nmitigate this opacity, this work presents a linear decomposition of final\nhidden states from autoregressive language models based on each initial input\ntoken, which is exact for virtually all contemporary Transformer architectures.\nThis decomposition allows the definition of probability distributions that\nablate the contribution of specific input tokens, which can be used to analyze\ntheir influence on model probabilities over a sequence of upcoming words with\nonly one forward pass from the model. Using the change in next-word probability\nas a measure of importance, this work first examines which context words make\nthe biggest contribution to language model predictions. Regression experiments\nsuggest that Transformer-based language models rely primarily on collocational\nassociations, followed by linguistic factors such as syntactic dependencies and\ncoreference relationships in making next-word predictions. Additionally,\nanalyses using these measures to predict syntactic dependencies and coreferent\nmention spans show that collocational association and repetitions of the same\ntoken largely explain the language models' predictions on these tasks.", "published": "2023-05-17 23:55:32", "link": "http://arxiv.org/abs/2305.10614v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM\n  Inference with Transferable Prompt", "abstract": "While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.", "published": "2023-05-17 20:45:13", "link": "http://arxiv.org/abs/2305.11186v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Model Tokenizers Introduce Unfairness Between Languages", "abstract": "Recent language models have shown impressive multilingual performance, even\nwhen not explicitly trained for it. Despite this, there are concerns about the\nquality of their outputs across different languages. In this paper, we show how\ndisparity in the treatment of different languages arises at the tokenization\nstage, well before a model is even invoked. The same text translated into\ndifferent languages can have drastically different tokenization lengths, with\ndifferences up to 15 times in some cases. These disparities persist even for\ntokenizers that are intentionally trained for multilingual support.\nCharacter-level and byte-level models also exhibit over 4 times the difference\nin the encoding length for some language pairs. This induces unfair treatment\nfor some language communities in regard to the cost of accessing commercial\nlanguage services, the processing time and latency, as well as the amount of\ncontent that can be provided as context to the models. Therefore, we make the\ncase that we should train future language models using multilingually fair\nsubword tokenizers.", "published": "2023-05-17 14:17:57", "link": "http://arxiv.org/abs/2305.15425v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical\n  Study of Relation Labeling in E-commerce with LLMs", "abstract": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system\nperformance by providing structured information about entities and their\nrelationships, such as complementary or substitutable relations between\nproducts or product types, which can be utilized in recommender systems.\nHowever, relation labeling in KGs remains a challenging task due to the dynamic\nnature of e-commerce domains and the associated cost of human labor. Recently,\nbreakthroughs in Large Language Models (LLMs) have shown surprising results in\nnumerous natural language processing tasks. In this paper, we conduct an\nempirical study of LLMs for relation labeling in e-commerce KGs, investigating\ntheir powerful learning capabilities in natural language and effectiveness in\npredicting relations between product types with limited labeled data. We\nevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,\ndemonstrating their ability to achieve competitive performance compared to\nhumans on relation labeling tasks using just 1 to 5 labeled examples per\nrelation. Additionally, we experiment with different prompt engineering\ntechniques to examine their impact on model performance. Our results show that\nLLMs significantly outperform existing KG completion models in relation\nlabeling for e-commerce KGs and exhibit performance strong enough to replace\nhuman labeling.", "published": "2023-05-17 00:08:36", "link": "http://arxiv.org/abs/2305.09858v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum\n  Bayes Risk Decoding for Machine Translation", "abstract": "Recent advances in machine translation (MT) have shown that Minimum Bayes\nRisk (MBR) decoding can be a powerful alternative to beam search decoding,\nespecially when combined with neural-based utility functions. However, the\nperformance of MBR decoding depends heavily on how and how many candidates are\nsampled from the model. In this paper, we explore how different sampling\napproaches for generating candidate lists for MBR decoding affect performance.\nWe evaluate popular sampling approaches, such as ancestral, nucleus, and top-k\nsampling. Based on our insights into their limitations, we experiment with the\nrecently proposed epsilon-sampling approach, which prunes away all tokens with\na probability smaller than epsilon, ensuring that each token in a sample\nreceives a fair probability mass. Through extensive human evaluations, we\ndemonstrate that MBR decoding based on epsilon-sampling significantly\noutperforms not only beam search decoding, but also MBR decoding with all other\ntested sampling methods across four language pairs.", "published": "2023-05-17 00:11:38", "link": "http://arxiv.org/abs/2305.09860v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining black box text modules in natural language with language\n  models", "abstract": "Large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their rapid proliferation\nand increasing opaqueness have created a growing need for interpretability.\nHere, we ask whether we can automatically obtain natural language explanations\nfor black box text modules. A \"text module\" is any function that maps text to a\nscalar continuous value, such as a submodule within an LLM or a fitted model of\na brain region. \"Black box\" indicates that we only have access to the module's\ninputs/outputs.\n  We introduce Summarize and Score (SASC), a method that takes in a text module\nand returns a natural language explanation of the module's selectivity along\nwith a score for how reliable the explanation is. We study SASC in 3 contexts.\nFirst, we evaluate SASC on synthetic modules and find that it often recovers\nground truth explanations. Second, we use SASC to explain modules found within\na pre-trained BERT model, enabling inspection of the model's internals.\nFinally, we show that SASC can generate explanations for the response of\nindividual fMRI voxels to language stimuli, with potential applications to\nfine-grained brain mapping. All code for using SASC and reproducing results is\nmade available on Github.", "published": "2023-05-17 00:29:18", "link": "http://arxiv.org/abs/2305.09863v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "The Jaseci Programming Paradigm and Runtime Stack: Building Scale-out\n  Production Applications Easy and Fast", "abstract": "Today's production scale-out applications include many sub-application\ncomponents, such as storage backends, logging infrastructure and AI models.\nThese components have drastically different characteristics, are required to\nwork in collaboration, and interface with each other as microservices. This\nleads to increasingly high complexity in developing, optimizing, configuring,\nand deploying scale-out applications, raising the barrier to entry for most\nindividuals and small teams. We developed a novel co-designed runtime system,\nJaseci, and programming language, Jac, which aims to reduce this complexity.\nThe key design principle throughout Jaseci's design is to raise the level of\nabstraction by moving as much of the scale-out data management, microservice\ncomponentization, and live update complexity into the runtime stack to be\nautomated and optimized automatically. We use real-world AI applications to\ndemonstrate Jaseci's benefit for application performance and developer\nproductivity.", "published": "2023-05-17 00:34:36", "link": "http://arxiv.org/abs/2305.09864v1", "categories": ["cs.CL", "cs.DC", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity Measure of Natural Language Text through Machine\n  Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study\n  Using UCGIS GIS&T Body of Knowledge", "abstract": "Initiated by the University Consortium of Geographic Information Science\n(UCGIS), GIS&T Body of Knowledge (BoK) is a community-driven endeavor to\ndefine, develop, and document geospatial topics related to geographic\ninformation science and technologies (GIS&T). In recent years, GIS&T BoK has\nundergone rigorous development in terms of its topic re-organization and\ncontent updating, resulting in a new digital version of the project. While the\nBoK topics provide useful materials for researchers and students to learn about\nGIS, the semantic relationships among the topics, such as semantic similarity,\nshould also be identified so that a better and automated topic navigation can\nbe achieved. Currently, the related topics are either defined manually by\neditors or authors, which may result in an incomplete assessment of topic\nrelationship. To address this challenge, our research evaluates the\neffectiveness of multiple natural language processing (NLP) techniques in\nextracting semantics from text, including both deep neural networks and\ntraditional machine learning approaches. Besides, a novel text summarization -\nKACERS (Keyword-Aware Cross-Encoder-Ranking Summarizer) - is proposed to\ngenerate a semantic summary of scientific publications. By identifying the\nsemantic linkages among key topics, this work provides guidance for future\ndevelopment and content organization of the GIS&T BoK project. It also offers a\nnew perspective on the use of machine learning techniques for analyzing\nscientific publications, and demonstrate the potential of KACERS summarizer in\nsemantic understanding of long text documents.", "published": "2023-05-17 01:17:57", "link": "http://arxiv.org/abs/2305.09877v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Equivariant Transfer Learning from Pretrained Models", "abstract": "Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of Basu\net al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and\noptimization-based methods, respectively, over features from group-transformed\ninputs to obtain equivariant outputs from non-equivariant neural networks.\nWhile Kaba et al. (2022) are only concerned with training from scratch, we find\nthat equitune performs poorly on equivariant zero-shot tasks despite good\nfinetuning results. We hypothesize that this is because pretrained models\nprovide better quality features for certain transformations than others and\nsimply averaging them is deleterious. Hence, we propose {\\lambda}-equitune that\naverages the features using importance weights, {\\lambda}s. These weights are\nlearned directly from the data using a small neural network, leading to\nexcellent zero-shot and finetuned results that outperform equitune. Further, we\nprove that {\\lambda}-equitune is equivariant and a universal approximator of\nequivariant functions. Additionally, we show that the method of Kaba et al.\n(2022) used with appropriate loss functions, which we call equizero, also gives\nexcellent zero-shot and finetuned performance. Both equitune and equizero are\nspecial cases of {\\lambda}-equitune. To show the simplicity and generality of\nour method, we validate on a wide range of diverse applications and models such\nas 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.", "published": "2023-05-17 02:20:34", "link": "http://arxiv.org/abs/2305.09900v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "\"I'm fully who I am\": Towards Centering Transgender and Non-Binary\n  Voices to Measure Biases in Open Language Generation", "abstract": "Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization of TGNB persons\ncontributes to and persists within Open Language Generation (OLG). This social\nknowledge serves as a guide for evaluating popular large language models (LLMs)\non two key aspects: (1) misgendering and (2) harmful responses to gender\ndisclosure. To do this, we introduce TANGO, a dataset of template-based\nreal-world text curated from a TGNB-oriented community. We discover a dominance\nof binary gender norms reflected by the models; LLMs least misgendered subjects\nin generated text when triggered by prompts whose subjects used binary\npronouns. Meanwhile, misgendering was most prevalent when triggering generation\nwith singular they and neopronouns. When prompted with gender disclosures, TGNB\ndisclosure generated the most stigmatizing language and scored most toxic, on\naverage. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.", "published": "2023-05-17 04:21:45", "link": "http://arxiv.org/abs/2305.09941v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2; I.7; K.4"], "primary_category": "cs.CL"}
{"title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs\n  Sampling", "abstract": "We introduce Reprompting, an iterative sampling algorithm that automatically\nlearns the Chain-of-Thought (CoT) recipes for a given task without human\nintervention. Through Gibbs sampling, Reprompting infers the CoT recipes that\nwork consistently well for a set of training samples by iteratively sampling\nnew recipes using previously sampled recipes as parent prompts to solve other\ntraining problems. We conduct extensive experiments on 20 challenging reasoning\ntasks. Results show that Reprompting outperforms human-written CoT prompts\nsubstantially by +9.4 points on average. It also achieves consistently better\nperformance than the state-of-the-art prompt optimization and decoding\nalgorithms.", "published": "2023-05-17 06:35:43", "link": "http://arxiv.org/abs/2305.09993v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EfficientSCI: Densely Connected Network with Space-time Factorization\n  for Large-scale Video Snapshot Compressive Imaging", "abstract": "Video snapshot compressive imaging (SCI) uses a two-dimensional detector to\ncapture consecutive video frames during a single exposure time. Following this,\nan efficient reconstruction algorithm needs to be designed to reconstruct the\ndesired video frames. Although recent deep learning-based state-of-the-art\n(SOTA) reconstruction algorithms have achieved good results in most tasks, they\nstill face the following challenges due to excessive model complexity and GPU\nmemory limitations: 1) these models need high computational cost, and 2) they\nare usually unable to reconstruct large-scale video frames at high compression\nratios. To address these issues, we develop an efficient network for video SCI\nby using dense connections and space-time factorization mechanism within a\nsingle residual block, dubbed EfficientSCI. The EfficientSCI network can well\nestablish spatial-temporal correlation by using convolution in the spatial\ndomain and Transformer in the temporal domain, respectively. We are the first\ntime to show that an UHD color video with high compression ratio can be\nreconstructed from a snapshot 2D measurement using a single end-to-end deep\nlearning model with PSNR above 32 dB. Extensive results on both simulation and\nreal data show that our method significantly outperforms all previous SOTA\nalgorithms with better real-time performance. The code is at\nhttps://github.com/ucaswangls/EfficientSCI.git.", "published": "2023-05-17 07:28:46", "link": "http://arxiv.org/abs/2305.10006v2", "categories": ["cs.CV", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Large Language Models Leverage External Knowledge to Extend Clinical\n  Insight Beyond Language Boundaries", "abstract": "$\\textbf{Objectives}$: Large Language Models (LLMs) such as ChatGPT and\nMed-PaLM have excelled in various medical question-answering tasks. However,\nthese English-centric models encounter challenges in non-English clinical\nsettings, primarily due to limited clinical knowledge in respective languages,\na consequence of imbalanced training corpora. We systematically evaluate LLMs\nin the Chinese medical context and develop a novel in-context learning\nframework to enhance their performance.\n  $\\textbf{Materials and Methods}$: The latest China National Medical Licensing\nExamination (CNMLE-2022) served as the benchmark. We collected 53 medical books\nand 381,149 medical questions to construct the medical knowledge base and\nquestion bank. The proposed Knowledge and Few-shot Enhancement In-context\nLearning (KFE) framework leverages the in-context learning ability of LLMs to\nintegrate diverse external clinical knowledge sources. We evaluated KFE with\nChatGPT(GPT3.5), GPT4, Baichuan2(BC2)-7B, and BC2-13B in CNMLE-2022 and\ninvestigated the effectiveness of different pathways for incorporating LLMs\nwith medical knowledge from 7 perspectives.\n  $\\textbf{Results}$: Directly applying ChatGPT failed to qualify for the\nCNMLE-2022 at a score of 51. Cooperated with the KFE, the LLMs with varying\nsizes yielded consistent and significant improvements. The ChatGPT's\nperformance surged to 70.04 and GPT-4 achieved the highest score of 82.59. This\nsurpasses the qualification threshold (60) and exceeds the average human score\nof 68.70. It also enabled a smaller BC2-13B to pass the examination, showcasing\nthe great potential in low-resource settings.\n  $\\textbf{Conclusion}$: By synergizing medical knowledge through in-context\nlearning, LLM can extend clinical insight beyond language barriers,\nsignificantly reducing language-related disparities of LLM applications and\nensuring global benefit in healthcare.", "published": "2023-05-17 12:31:26", "link": "http://arxiv.org/abs/2305.10163v4", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Boosting Local Spectro-Temporal Features for Speech Analysis", "abstract": "We introduce the problem of phone classification in the context of speech\nrecognition, and explore several sets of local spectro-temporal features that\ncan be used for phone classification. In particular, we present some\npreliminary results for phone classification using two sets of features that\nare commonly used for object detection: Haar features and SVM-classified\nHistograms of Gradients (HoG).", "published": "2023-05-17 15:02:20", "link": "http://arxiv.org/abs/2305.10270v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LeTI: Learning to Generate from Textual Interactions", "abstract": "Fine-tuning pre-trained language models (LMs) is essential for enhancing\ntheir capabilities. Existing techniques commonly fine-tune on input-output\npairs (e.g., instruction tuning) or with numerical rewards that gauge the\noutput quality (e.g., RLHF). We explore LMs' potential to learn from textual\ninteractions (LETI) that not only check their correctness with binary labels\nbut also pinpoint and explain errors in their outputs through textual feedback.\nOur focus is the code generation task, where the model produces code based on\nnatural language instructions. This setting invites a natural and scalable way\nto acquire textual feedback: the error messages and stack traces from code\nexecution using a Python interpreter. LETI iteratively fine-tunes the model,\nusing the LM objective, on a concatenation of natural language instructions,\nLM-generated programs, and textual feedback. Prepended to this fine-tuning\ntext, a binary reward token is used to differentiate correct and buggy\nsolutions. LETI requires no ground-truth outputs for training and even\noutperforms a fine-tuned baseline that does. LETI not only improves the\nperformance of LMs on a code generation dataset MBPP, but also generalizes to\nother datasets. Trained on MBPP, it achieves comparable or better performance\nthan the base LMs on unseen problems in HumanEval. Furthermore, compared to\nbinary feedback, we observe that textual feedback leads to improved generation\nquality and sample efficiency, achieving the same performance with fewer than\nhalf of the gradient steps. LETI is equally applicable in natural language\ntasks when they can be formulated as code generation, which we empirically\nverified on event argument extraction.", "published": "2023-05-17 15:53:31", "link": "http://arxiv.org/abs/2305.10314v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Controllable Speaking Styles Using a Large Language Model", "abstract": "Reference-based Text-to-Speech (TTS) models can generate multiple,\nprosodically-different renditions of the same target text. Such models jointly\nlearn a latent acoustic space during training, which can be sampled from during\ninference. Controlling these models during inference typically requires finding\nan appropriate reference utterance, which is non-trivial.\n  Large generative language models (LLMs) have shown excellent performance in\nvarious language-related tasks. Given only a natural language query text (the\nprompt), such models can be used to solve specific, context-dependent tasks.\nRecent work in TTS has attempted similar prompt-based control of novel speaking\nstyle generation. Those methods do not require a reference utterance and can,\nunder ideal conditions, be controlled with only a prompt. But existing methods\ntypically require a prompt-labelled speech corpus for jointly training a\nprompt-conditioned encoder.\n  In contrast, we instead employ an LLM to directly suggest prosodic\nmodifications for a controllable TTS model, using contextual information\nprovided in the prompt. The prompt can be designed for a multitude of tasks.\nHere, we give two demonstrations: control of speaking style; prosody\nappropriate for a given dialogue context. The proposed method is rated most\nappropriate in 50% of cases vs. 31% for a baseline model.", "published": "2023-05-17 16:01:50", "link": "http://arxiv.org/abs/2305.10321v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interactive Learning of Hierarchical Tasks from Dialog with GPT", "abstract": "We present a system for interpretable, symbolic, interactive task learning\nfrom dialog using a GPT model as a conversational front-end. The learned tasks\nare represented as hierarchical decompositions of predicate-argument structures\nwith scoped variable arguments. By using a GPT model to convert interactive\ndialog into a semantic representation, and then recursively asking for\ndefinitions of unknown steps, we show that hierarchical task knowledge can be\nacquired and re-used in a natural and unrestrained conversational environment.\nWe compare our system to a similar architecture using a more conventional\nparser and show that our system tolerates a much wider variety of linguistic\nvariance.", "published": "2023-05-17 16:32:40", "link": "http://arxiv.org/abs/2305.10349v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Evaluating Object Hallucination in Large Vision-Language Models", "abstract": "Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.", "published": "2023-05-17 16:34:01", "link": "http://arxiv.org/abs/2305.10355v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Accelerating Transformer Inference for Translation via Parallel Decoding", "abstract": "Autoregressive decoding limits the efficiency of transformers for Machine\nTranslation (MT). The community proposed specific network architectures and\nlearning-based methods to solve this issue, which are expensive and require\nchanges to the MT model, trading inference speed at the cost of the translation\nquality. In this paper, we propose to address the problem from the point of\nview of decoding algorithms, as a less explored but rather compelling\ndirection. We propose to reframe the standard greedy autoregressive decoding of\nMT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point\niteration methods for fast inference. This formulation allows to speed up\nexisting models without training or modifications while retaining translation\nquality. We present three parallel decoding algorithms and test them on\ndifferent languages and models showing how the parallelization introduces a\nspeedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x\nwhen scaling the method on parallel resources. Finally, we introduce a decoding\ndependency graph visualizer (DDGviz) that let us see how the model has learned\nthe conditional dependence between tokens and inspect the decoding procedure.", "published": "2023-05-17 17:57:34", "link": "http://arxiv.org/abs/2305.10427v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incorporating Attribution Importance for Improving Faithfulness Metrics", "abstract": "Feature attribution methods (FAs) are popular approaches for providing\ninsights into the model reasoning process of making predictions. The more\nfaithful a FA is, the more accurately it reflects which parts of the input are\nmore important for the prediction. Widely used faithfulness metrics, such as\nsufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely\nremoving or retaining the top most important tokens ranked by a given FA and\nobserving the changes in predictive likelihood. However, this hard criterion\nignores the importance of each individual token, treating them all equally for\ncomputing sufficiency and comprehensiveness. In this paper, we propose a simple\nyet effective soft erasure criterion. Instead of entirely removing or retaining\ntokens from the input, we randomly mask parts of the token vector\nrepresentations proportionately to their FA importance. Extensive experiments\nacross various natural language processing tasks and different FAs show that\nour soft-sufficiency and soft-comprehensiveness metrics consistently prefer\nmore faithful explanations compared to hard sufficiency and comprehensiveness.\nOur code: https://github.com/casszhao/SoftFaith", "published": "2023-05-17 18:05:49", "link": "http://arxiv.org/abs/2305.10496v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores\n  Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource\n  Languages", "abstract": "In this multicultural age, language translation is one of the most performed\ntasks, and it is becoming increasingly AI-moderated and automated. As a novel\nAI system, ChatGPT claims to be proficient in such translation tasks and in\nthis paper, we put that claim to the test. Specifically, we examine ChatGPT's\naccuracy in translating between English and languages that exclusively use\ngender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most\nspoken language globally, but also generalize our findings across five other\nlanguages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT\nperpetuates gender defaults and stereotypes assigned to certain occupations\n(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to\nwork), as it converts gender-neutral pronouns in languages to `he' or `she'. We\nalso observe ChatGPT completely failing to translate the English gender-neutral\npronoun `they' into equivalent gender-neutral pronouns in other languages, as\nit produces translations that are incoherent and incorrect. While it does\nrespect and provide appropriately gender-marked versions of Bengali words when\nprompted with gender information in English, ChatGPT appears to confer a higher\nrespect to men than to women in the same occupation. We conclude that ChatGPT\nexhibits the same gender biases which have been demonstrated for tools like\nGoogle Translate or MS Translator, as we provide recommendations for a human\ncentered approach for future designers of AIs that perform language translation\nto better accommodate such low-resource languages.", "published": "2023-05-17 18:30:05", "link": "http://arxiv.org/abs/2305.10510v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Scalable and Safe Remediation of Defective Actions in Self-Learning\n  Conversational Systems", "abstract": "Off-Policy reinforcement learning has been a driving force for the\nstate-of-the-art conversational AIs leading to more natural humanagent\ninteractions and improving the user satisfaction for goal-oriented agents.\nHowever, in large-scale commercial settings, it is often challenging to balance\nbetween policy improvements and experience continuity on the broad spectrum of\napplications handled by such system. In the literature, off-policy evaluation\nand guard-railing on aggregate statistics has been commonly used to address\nthis problem. In this paper, we propose a method for curating and leveraging\nhigh-precision samples sourced from historical regression incident reports to\nvalidate, safe-guard, and improve policies prior to the online deployment. We\nconducted extensive experiments using data from a real-world conversational\nsystem and actual regression incidents. The proposed method is currently\ndeployed in our production system to protect customers against broken\nexperiences and enable long-term policy improvements.", "published": "2023-05-17 19:22:24", "link": "http://arxiv.org/abs/2305.10528v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.", "published": "2023-05-17 23:16:17", "link": "http://arxiv.org/abs/2305.10601v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Speaker Verification with Self-Pretrained Transformer Models", "abstract": "Recently, fine-tuning large pre-trained Transformer models using downstream\ndatasets has received a rising interest. Despite their success, it is still\nchallenging to disentangle the benefits of large-scale datasets and Transformer\nstructures from the limitations of the pre-training. In this paper, we\nintroduce a hierarchical training approach, named self-pretraining, in which\nTransformer models are pretrained and finetuned on the same dataset. Three\npre-trained models including HuBERT, Conformer and WavLM are evaluated on four\ndifferent speaker verification datasets with varying sizes. Our experiments\nshow that these self-pretrained models achieve competitive performance on\ndownstream speaker verification tasks with only one-third of the data compared\nto Librispeech pretraining, such as VoxCeleb1 and CNCeleb1. Furthermore, when\npre-training only on the VoxCeleb2-dev, the Conformer model outperforms the one\npre-trained on 94k hours of data using the same fine-tuning settings.", "published": "2023-05-17 18:52:11", "link": "http://arxiv.org/abs/2305.10517v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of\n  Synthesized Speech", "abstract": "Mean Opinion Score (MOS) is a popular measure for evaluating synthesized\nspeech. However, the scores obtained in MOS tests are heavily dependent upon\nmany contextual factors. One such factor is the overall range of quality of the\nsamples presented in the test -- listeners tend to try to use the entire range\nof scoring options available to them regardless of this, a phenomenon which is\nknown as range-equalizing bias. In this paper, we systematically investigate\nthe effects of range-equalizing bias on MOS tests for synthesized speech by\nconducting a series of listening tests in which we progressively \"zoom in\" on a\nsmaller number of systems in the higher-quality range. This allows us to better\nunderstand and quantify the effects of range-equalizing bias in MOS tests.", "published": "2023-05-17 23:30:18", "link": "http://arxiv.org/abs/2305.10608v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with\n  Convolutional Cross Attention in Multi-talker Conditions", "abstract": "Time-domain single-channel speech enhancement (SE) still remains challenging\nto extract the target speaker without any prior information on multi-talker\nconditions. It has been shown via auditory attention decoding that the brain\nactivity of the listener contains the auditory information of the attended\nspeaker. In this paper, we thus propose a novel time-domain brain-assisted SE\nnetwork (BASEN) incorporating electroencephalography (EEG) signals recorded\nfrom the listener for extracting the target speaker from monaural speech\nmixtures. The proposed BASEN is based on the fully-convolutional time-domain\naudio separation network. In order to fully leverage the complementary\ninformation contained in the EEG signals, we further propose a convolutional\nmulti-layer cross attention module to fuse the dual-branch features.\nExperimental results on a public dataset show that the proposed model\noutperforms the state-of-the-art method in several evaluation metrics. The\nreproducible code is available at https://github.com/jzhangU/Basen.git.", "published": "2023-05-17 06:40:31", "link": "http://arxiv.org/abs/2305.09994v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
