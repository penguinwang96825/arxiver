{"title": "Hrebs and Cohesion Chains as similar tools for semantic text properties\n  research", "abstract": "In this study it is proven that the Hrebs used in Denotation analysis of\ntexts and Cohesion Chains (defined as a fusion between Lexical Chains and\nCoreference Chains) represent similar linguistic tools. This result gives us\nthe possibility to extend to Cohesion Chains (CCs) some important indicators\nas, for example the Kernel of CCs, the topicality of a CC, text concentration,\nCC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in\nthe Lexical Chains or Coreference Chains literature these kinds of indicators\nare introduced and used since now. Similarly, some applications of CCs in the\nstudy of a text (as for example segmentation or summarization of a text) could\nbe realized starting from hrebs. As an illustration of the similarity between\nHrebs and CCs a detailed analyze of the poem \"Lacul\" by Mihai Eminescu is\ngiven.", "published": "2014-01-15 17:01:36", "link": "http://arxiv.org/abs/1401.3669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Compression as Tree Transduction", "abstract": "This paper presents a tree-to-tree transduction method for sentence\ncompression. Our model is based on synchronous tree substitution grammar, a\nformalism that allows local distortion of the tree topology and can thus\nnaturally capture structural mismatches. We describe an algorithm for decoding\nin this framework and show how the model can be trained discriminatively within\na large margin framework. Experimental results on sentence compression bring\nsignificant improvements over a state-of-the-art model.", "published": "2014-01-15 05:19:15", "link": "http://arxiv.org/abs/1401.5693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Annotation Projection for Semantic Roles", "abstract": "This article considers the task of automatically inducing role-semantic\nannotations in the FrameNet paradigm for new languages. We propose a general\nframework that is based on annotation projection, phrased as a graph\noptimization problem. It is relatively inexpensive and has the potential to\nreduce the human effort involved in creating role-semantic resources. Within\nthis framework, we present projection models that exploit lexical and syntactic\ninformation. We provide an experimental evaluation on an English-German\nparallel corpus which demonstrates the feasibility of inducing high-precision\nGerman semantic role annotation both for manually and automatically annotated\nEnglish data.", "published": "2014-01-15 05:40:37", "link": "http://arxiv.org/abs/1401.5694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches", "abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases.", "published": "2014-01-15 05:39:01", "link": "http://arxiv.org/abs/1401.5695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Methods for Determining Object and Relation Synonyms on the\n  Web", "abstract": "The task of identifying synonymous relations and objects, or synonym\nresolution, is critical for high-quality information extraction. This paper\ninvestigates synonym resolution in the context of unsupervised information\nextraction, where neither hand-tagged training examples nor domain knowledge is\navailable. The paper presents a scalable, fully-implemented system that runs in\nO(KN log N) time in the number of extractions, N, and the maximum number of\nsynonyms per word, K. The system, called Resolver, introduces a probabilistic\nrelational model for predicting whether two strings are co-referential based on\nthe similarity of the assertions containing them. On a set of two million\nassertions extracted from the Web, Resolver resolves objects with 78% precision\nand 68% recall, and resolves relations with 90% precision and 35% recall.\nSeveral variations of resolvers probabilistic model are explored, and\nexperiments demonstrate that under appropriate conditions these variations can\nimprove F1 by 5%. An extension to the basic Resolver system allows it to handle\npolysemous names with 97% precision and 95% recall on a data set from the TREC\ncorpus.", "published": "2014-01-15 05:33:07", "link": "http://arxiv.org/abs/1401.5696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "abstract": "Adequate representation of natural language semantics requires access to vast\namounts of common sense and domain-specific world knowledge. Prior work in the\nfield was based on purely statistical techniques that did not make use of\nbackground knowledge, on limited lexicographic knowledge bases such as WordNet,\nor on huge manual efforts such as the CYC project. Here we propose a novel\nmethod, called Explicit Semantic Analysis (ESA), for fine-grained semantic\ninterpretation of unrestricted natural language texts. Our method represents\nmeaning in a high-dimensional space of concepts derived from Wikipedia, the\nlargest encyclopedia in existence. We explicitly represent the meaning of any\ntext in terms of Wikipedia-based concepts. We evaluate the effectiveness of our\nmethod on text categorization and on computing the degree of semantic\nrelatedness between fragments of natural language text. Using ESA results in\nsignificant improvements over the previous state of the art in both tasks.\nImportantly, due to the use of natural concepts, the ESA model is easy to\nexplain to human users.", "published": "2014-01-15 05:21:01", "link": "http://arxiv.org/abs/1401.5697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identification of Pleonastic It Using the Web", "abstract": "In a significant minority of cases, certain pronouns, especially the pronoun\nit, can be used without referring to any specific entity. This phenomenon of\npleonastic pronoun usage poses serious problems for systems aiming at even a\nshallow understanding of natural language texts. In this paper, a novel\napproach is proposed to identify such uses of it: the extrapositional cases are\nidentified using a series of queries against the web, and the cleft cases are\nidentified using a simple set of syntactic rules. The system is evaluated with\nfour sets of news articles containing 679 extrapositional cases as well as 78\ncleft constructs. The identification results are comparable to those obtained\nby human efforts.", "published": "2014-01-15 05:11:43", "link": "http://arxiv.org/abs/1401.5698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Relatedness Based on a Word Thesaurus", "abstract": "The computation of relatedness between two fragments of text in an automated\nmanner requires taking into account a wide range of factors pertaining to the\nmeaning the two fragments convey, and the pairwise relations between their\nwords. Without doubt, a measure of relatedness between text segments must take\ninto account both the lexical and the semantic relatedness between words. Such\na measure that captures well both aspects of text relatedness may help in many\ntasks, such as text retrieval, classification and clustering. In this paper we\npresent a new approach for measuring the semantic relatedness between words\nbased on their implicit semantic links. The approach exploits only a word\nthesaurus in order to devise implicit semantic links between words. Based on\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\nbetween texts which capitalizes on the word-to-word semantic relatedness\nmeasure (SR) and extends it to measure the relatedness between texts. We\ngradually validate our method: we first evaluate the performance of the\nsemantic relatedness measure between individual words, covering word-to-word\nsimilarity and relatedness, synonym identification and word analogy; then, we\nproceed with evaluating the performance of our method in measuring text-to-text\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\nparaphrase recognition. Experimental evaluation shows that the proposed method\noutperforms every lexicon-based method of semantic relatedness in the selected\ntasks and the used data sets, and competes well against corpus-based and hybrid\napproaches.", "published": "2014-01-15 05:41:08", "link": "http://arxiv.org/abs/1401.5699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel\n  Corpora", "abstract": "This paper describes a method for the automatic inference of structural\ntransfer rules to be used in a shallow-transfer machine translation (MT) system\nfrom small parallel corpora. The structural transfer rules are based on\nalignment templates, like those used in statistical MT. Alignment templates are\nextracted from sentence-aligned parallel corpora and extended with a set of\nrestrictions which are derived from the bilingual dictionary of the MT system\nand control their application as transfer rules. The experiments conducted\nusing three different language pairs in the free/open-source MT platform\nApertium show that translation quality is improved as compared to word-for-word\ntranslation (when no transfer rules are used), and that the resulting\ntranslation quality is close to that obtained using hand-coded transfer rules.\nThe method we present is entirely unsupervised and benefits from information in\nthe rest of modules of the MT system in which the inferred rules are applied.", "published": "2014-01-15 05:28:26", "link": "http://arxiv.org/abs/1401.5700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "abstract": "This paper presents a new method for inferring the semantic properties of\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\nbecoming increasingly abundant due to the recent dramatic growth in\nsemi-structured, user-generated online content. One especially relevant domain\nis product reviews, which are often annotated by their authors with pros/cons\nkeyphrases such as a real bargain or good value. These annotations are\nrepresentative of the underlying semantic properties; however, unlike expert\nannotations, they are noisy: lay authors may use different labels to denote the\nsame property, and some labels may be missing. To learn using such noisy\nannotations, we find a hidden paraphrase structure which clusters the\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\nreview texts, enabling the system to predict the properties of unannotated\ndocuments and to effectively aggregate the semantic properties of multiple\nreviews. Our approach is implemented as a hierarchical Bayesian model with\njoint inference. We find that joint inference increases the robustness of the\nkeyphrase clustering and encourages the latent topics to correlate with\nsemantically meaningful properties. Multiple evaluations demonstrate that our\nmodel substantially outperforms alternative approaches for summarizing single\nand multiple documents into a set of semantically salient keyphrases.", "published": "2014-01-15 05:14:31", "link": "http://arxiv.org/abs/1401.3457v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Performance Of English-Hindi Cross Language Information\n  Retrieval Using Transliteration Of Query Terms", "abstract": "The main issue in Cross Language Information Retrieval (CLIR) is the poor\nperformance of retrieval in terms of average precision when compared to\nmonolingual retrieval performance. The main reasons behind poor performance of\nCLIR are mismatching of query terms, lexical ambiguity and un-translated query\nterms. The existing problems of CLIR are needed to be addressed in order to\nincrease the performance of the CLIR system. In this paper, we are putting our\neffort to solve the given problem by proposed an algorithm for improving the\nperformance of English-Hindi CLIR system. We used all possible combination of\nHindi translated query using transliteration of English query terms and\nchoosing the best query among them for retrieval of documents. The experiment\nis performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.\nThe experimental result show that the proposed approach gives better\nperformance of English-Hindi CLIR system and also helps in overcoming existing\nproblems and outperforms the existing English-Hindi CLIR system in terms of\naverage precision.", "published": "2014-01-15 08:07:08", "link": "http://arxiv.org/abs/1401.3510v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Complex Question Answering: Unsupervised Learning Approaches and\n  Experiments", "abstract": "Complex questions that require inferencing and synthesizing information from\nmultiple documents can be seen as a kind of topic-oriented, informative\nmulti-document summarization where the goal is to produce a single text as a\ncompressed version of a set of documents with a minimum loss of relevant\ninformation. In this paper, we experiment with one empirical method and two\nunsupervised statistical machine learning techniques: K-means and Expectation\nMaximization (EM), for computing relative importance of the sentences. We\ncompare the results of these approaches. Our experiments show that the\nempirical approach outperforms the other two techniques and EM performs better\nthan K-means. However, the performance of these approaches depends entirely on\nthe feature set used and the weighting of these features. In order to measure\nthe importance and relevance to the user query we extract different kinds of\nfeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,\ntree kernel based syntactic and shallow-semantic) for each of the document\nsentences. We use a local search technique to learn the weights of the\nfeatures. To the best of our knowledge, no study has used tree kernel functions\nto encode syntactic/semantic information for more complex tasks such as\ncomputing the relatedness between the query sentences and the document\nsentences in order to generate query-focused summaries (or answers to complex\nquestions). For each of our methods of generating summaries (i.e. empirical,\nK-means and EM) we show the effects of syntactic and shallow-semantic features\nover the bag-of-words (BOW) features.", "published": "2014-01-15 05:33:57", "link": "http://arxiv.org/abs/1401.3479v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing QA Systems with Complex Temporal Question Processing\n  Capabilities", "abstract": "This paper presents a multilayered architecture that enhances the\ncapabilities of current QA systems and allows different types of complex\nquestions or queries to be processed. The answers to these questions need to be\ngathered from factual information scattered throughout different documents.\nSpecifically, we designed a specialized layer to process the different types of\ntemporal questions. Complex temporal questions are first decomposed into simple\nquestions, according to the temporal relations expressed in the original\nquestion. In the same way, the answers to the resulting simple questions are\nrecomposed, fulfilling the temporal restrictions of the original complex\nquestion. A novel aspect of this approach resides in the decomposition which\nuses a minimal quantity of resources, with the final aim of obtaining a\nportable platform that is easily extensible to other languages. In this paper\nwe also present a methodology for evaluation of the decomposition of the\nquestions as well as the ability of the implemented temporal layer to perform\nat a multilingual level. The temporal layer was first performed for English,\nthen evaluated and compared with: a) a general purpose QA system (F-measure\n65.47% for QA plus English temporal layer vs. 38.01% for the general QA\nsystem), and b) a well-known QA system. Much better results were obtained for\ntemporal questions with the multilayered system. This system was therefore\nextended to Spanish and very good results were again obtained in the evaluation\n(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general\nQA system).", "published": "2014-01-15 05:35:49", "link": "http://arxiv.org/abs/1401.3482v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Content Modeling Using Latent Permutations", "abstract": "We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.", "published": "2014-01-15 05:38:17", "link": "http://arxiv.org/abs/1401.3488v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
