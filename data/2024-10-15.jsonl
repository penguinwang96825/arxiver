{"title": "Quantum Computing for Multi Period Asset Allocation", "abstract": "Portfolio construction has been a long-standing topic of research in finance.\nThe computational complexity and the time taken both increase rapidly with the\nnumber of investments in the portfolio. It becomes difficult, even impossible\nfor classic computers to solve. Quantum computing is a new way of computing\nwhich takes advantage of quantum superposition and entanglement. It changes how\nsuch problems are approached and is not constrained by some of the classic\ncomputational complexity. Studies have shown that quantum computing can offer\nsignificant advantages over classical computing in many fields. The application\nof quantum computing has been constrained by the unavailability of actual\nquantum computers. In the past decade, there has been the rapid development of\nthe large-scale quantum computer. However, software development for quantum\ncomputing is slow in many fields. In our study, we apply quantum computing to a\nmulti-asset portfolio simulation. The simulation is based on historic data,\ncovariance, and expected returns, all calculated using quantum computing.\nAlthough technically a solvable problem for classical computing, we believe the\nsoftware development is important to the future application of quantum\ncomputing in finance. We conducted this study through simulation of a quantum\ncomputer and the use of Rensselaer Polytechnic Institute's IBM quantum\ncomputer.", "published": "2024-10-15 19:04:29", "link": "http://arxiv.org/abs/2410.11997v1", "categories": ["cs.CE", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "Solving The Dynamic Volatility Fitting Problem: A Deep Reinforcement Learning Approach", "abstract": "The volatility fitting is one of the core problems in the equity derivatives\nbusiness. Through a set of deterministic rules, the degrees of freedom in the\nimplied volatility surface encoding (parametrization, density, diffusion) are\ndefined. Whilst very effective, this approach widespread in the industry is not\nnatively tailored to learn from shifts in market regimes and discover\nunsuspected optimal behaviors. In this paper, we change the classical paradigm\nand apply the latest advances in Deep Reinforcement Learning(DRL) to solve the\nfitting problem. In particular, we show that variants of Deep Deterministic\nPolicy Gradient (DDPG) and Soft Actor Critic (SAC) can achieve at least as good\nas standard fitting algorithms. Furthermore, we explain why the reinforcement\nlearning framework is appropriate to handle complex objective functions and is\nnatively adapted for online learning.", "published": "2024-10-15 17:10:54", "link": "http://arxiv.org/abs/2410.11789v1", "categories": ["q-fin.CP", "math.OC", "math.PR", "q-fin.RM", "stat.ML"], "primary_category": "q-fin.CP"}
{"title": "Exploiting Risk-Aversion and Size-dependent fees in FX Trading with Fitted Natural Actor-Critic", "abstract": "In recent years, the popularity of artificial intelligence has surged due to\nits widespread application in various fields. The financial sector has\nharnessed its advantages for multiple purposes, including the development of\nautomated trading systems designed to interact autonomously with markets to\npursue different aims. In this work, we focus on the possibility of recognizing\nand leveraging intraday price patterns in the Foreign Exchange market, known\nfor its extensive liquidity and flexibility. Our approach involves the\nimplementation of a Reinforcement Learning algorithm called Fitted Natural\nActor-Critic. This algorithm allows the training of an agent capable of\neffectively trading by means of continuous actions, which enable the\npossibility of executing orders with variable trading sizes. This feature is\ninstrumental to realistically model transaction costs, as they typically depend\non the order size. Furthermore, it facilitates the integration of risk-averse\napproaches to induce the agent to adopt more conservative behavior. The\nproposed approaches have been empirically validated on EUR-USD historical data.", "published": "2024-10-15 13:13:07", "link": "http://arxiv.org/abs/2410.23294v1", "categories": ["q-fin.TR", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Generalized Distribution Prediction for Asset Returns", "abstract": "We present a novel approach for predicting the distribution of asset returns\nusing a quantile-based method with Long Short-Term Memory (LSTM) networks. Our\nmodel is designed in two stages: the first focuses on predicting the quantiles\nof normalized asset returns using asset-specific features, while the second\nstage incorporates market data to adjust these predictions for broader economic\nconditions. This results in a generalized model that can be applied across\nvarious asset classes, including commodities, cryptocurrencies, as well as\nsynthetic datasets. The predicted quantiles are then converted into full\nprobability distributions through kernel density estimation, allowing for more\nprecise return distribution predictions and inferencing. The LSTM model\nsignificantly outperforms a linear quantile regression baseline by 98% and a\ndense neural network model by over 50%, showcasing its ability to capture\ncomplex patterns in financial return distributions across both synthetic and\nreal-world data. By using exclusively asset-class-neutral features, our model\nachieves robust, generalizable results.", "published": "2024-10-15 15:31:44", "link": "http://arxiv.org/abs/2410.23296v2", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm\n  Intelligence", "abstract": "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via\nswarm intelligence, the collective behavior guiding individual systems.\nSpecifically, Model Swarms starts with a pool of LLM experts and a utility\nfunction. Guided by the best-found checkpoints across models, diverse LLM\nexperts collaboratively move in the weight space and optimize a utility\nfunction representing model adaptation objectives. Compared to existing model\ncomposition approaches, Model Swarms offers tuning-free model adaptation, works\nin low-data regimes with as few as 200 examples, and does not require\nassumptions about specific experts in the swarm or how they should be composed.\nExtensive experiments demonstrate that Model Swarms could flexibly adapt LLM\nexperts to a single task, multi-task domains, reward models, as well as diverse\nhuman interests, improving over 12 model composition baselines by up to 21.0%\nacross tasks and contexts. Further analysis reveals that LLM experts discover\npreviously unseen capabilities in initial checkpoints and that Model Swarms\nenable the weak-to-strong transition of experts through the collaborative\nsearch process.", "published": "2024-10-15 00:59:17", "link": "http://arxiv.org/abs/2410.11163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experiences from Creating a Benchmark for Sentiment Classification for\n  Varieties of English", "abstract": "Existing benchmarks often fail to account for linguistic diversity, like\nlanguage variants of English. In this paper, we share our experiences from our\nongoing project of building a sentiment classification benchmark for three\nvariants of English: Australian (en-AU), Indian (en-IN), and British (en-UK)\nEnglish. Using Google Places reviews, we explore the effects of various\nsampling techniques based on label semantics, review length, and sentiment\nproportion and report performances on three fine-tuned BERT-based models. Our\ninitial evaluation reveals significant performance variations influenced by\nsample characteristics, label semantics, and language variety, highlighting the\nneed for nuanced benchmark design. We offer actionable insights for researchers\nto create robust benchmarks, emphasising the importance of diverse sampling,\ncareful label definition, and comprehensive evaluation across linguistic\nvarieties.", "published": "2024-10-15 03:02:03", "link": "http://arxiv.org/abs/2410.11216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Is Hate Lost in Translation?\": Evaluation of Multilingual LGBTQIA+ Hate\n  Speech Detection", "abstract": "This paper explores the challenges of detecting LGBTQIA+ hate speech of large\nlanguage models across multiple languages, including English, Italian, Chinese\nand (code-switched) English-Tamil, examining the impact of machine translation\nand whether the nuances of hate speech are preserved across translation. We\nexamine the hate speech detection ability of zero-shot and fine-tuned GPT. Our\nfindings indicate that: (1) English has the highest performance and the\ncode-switching scenario of English-Tamil being the lowest, (2) fine-tuning\nimproves performance consistently across languages whilst translation yields\nmixed results. Through simple experimentation with original text and\nmachine-translated text for hate speech detection along with a qualitative\nerror analysis, this paper sheds light on the socio-cultural nuances and\ncomplexities of languages that may not be captured by automatic translation.", "published": "2024-10-15 03:24:03", "link": "http://arxiv.org/abs/2410.11230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GT2Vec: Large Language Models as Multi-Modal Encoders for Text and\n  Graph-Structured Data", "abstract": "Graph-structured information offers rich contextual information that can\nenhance language models by providing structured relationships and hierarchies,\nleading to more expressive embeddings for various applications such as\nretrieval, question answering, and classification. However, existing methods\nfor integrating graph and text embeddings, often based on Multi-layer\nPerceptrons (MLPs) or shallow transformers, are limited in their ability to\nfully exploit the heterogeneous nature of these modalities. To overcome this,\nwe propose GT2Vec, a simple yet effective framework that leverages Large\nLanguage Models (LLMs) to jointly encode text and graph data. Specifically,\nGT2Vec employs an MLP adapter to project graph embeddings into the same space\nas text embeddings, allowing the LLM to process both modalities jointly. Unlike\nprior work, we also introduce contrastive learning to align the graph and text\nspaces more effectively, thereby improving the quality of learned joint\nembeddings. Empirical results across six datasets spanning three tasks,\nknowledge graph-contextualized question answering, graph-text pair\nclassification, and retrieval, demonstrate that GT2Vec consistently outperforms\nexisting baselines, achieving significant improvements across multiple\ndatasets. These results highlight GT2Vec's effectiveness in integrating graph\nand text data. Ablation studies further validate the effectiveness of our\nmethod.", "published": "2024-10-15 03:40:20", "link": "http://arxiv.org/abs/2410.11235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cognitive Overload Attack:Prompt Injection for Long Context", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming tasks across various domains without needing explicit retraining.\nThis capability, known as In-Context Learning (ICL), while impressive, exposes\nLLMs to a variety of adversarial prompts and jailbreaks that manipulate\nsafety-trained LLMs into generating undesired or harmful output. In this paper,\nwe propose a novel interpretation of ICL in LLMs through the lens of cognitive\nneuroscience, by drawing parallels between learning in human cognition with\nICL. We applied the principles of Cognitive Load Theory in LLMs and empirically\nvalidate that similar to human cognition, LLMs also suffer from cognitive\noverload a state where the demand on cognitive processing exceeds the available\ncapacity of the model, leading to potential errors. Furthermore, we\ndemonstrated how an attacker can exploit ICL to jailbreak LLMs through\ndeliberately designed prompts that induce cognitive overload on LLMs, thereby\ncompromising the safety mechanisms of LLMs. We empirically validate this threat\nmodel by crafting various cognitive overload prompts and show that advanced\nmodels such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, Llama-3-70B-Instruct,\nGemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack\nsuccess rates of up to 99.99%. Our findings highlight critical vulnerabilities\nin LLMs and underscore the urgency of developing robust safeguards. We propose\nintegrating insights from cognitive load theory into the design and evaluation\nof LLMs to better anticipate and mitigate the risks of adversarial attacks. By\nexpanding our experiments to encompass a broader range of models and by\nhighlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of\nsafer and more reliable AI systems.", "published": "2024-10-15 04:53:34", "link": "http://arxiv.org/abs/2410.11272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented\n  Generation", "abstract": "Recent studies in Retrieval-Augmented Generation (RAG) have investigated\nextracting evidence from retrieved passages to reduce computational costs and\nenhance the final RAG performance, yet it remains challenging. Existing methods\nheavily rely on heuristic-based augmentation, encountering several issues: (1)\nPoor generalization due to hand-crafted context filtering; (2) Semantics\ndeficiency due to rule-based context chunking; (3) Skewed length due to\nsentence-wise filter learning. To address these issues, we propose a\nmodel-based evidence extraction learning framework, SEER, optimizing a vanilla\nmodel as an evidence extractor with desired properties through self-aligned\nlearning. Extensive experiments show that our method largely improves the final\nRAG performance, enhances the faithfulness, helpfulness, and conciseness of the\nextracted evidence, and reduces the evidence length by 9.25 times. The code\nwill be available at https://github.com/HITsz-TMG/SEER.", "published": "2024-10-15 06:26:24", "link": "http://arxiv.org/abs/2410.11315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-adaptive Multimodal Retrieval-Augmented Generation", "abstract": "Traditional Retrieval-Augmented Generation (RAG) methods are limited by their\nreliance on a fixed number of retrieved documents, often resulting in\nincomplete or noisy information that undermines task performance. Although\nrecent adaptive approaches alleviated these problems, their application in\nintricate and real-world multimodal tasks remains limited. To address these, we\npropose a new approach called Self-adaptive Multimodal Retrieval-Augmented\nGeneration (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG\nnot only dynamically filters relevant documents based on the input query,\nincluding image captions when needed, but also verifies the quality of both the\nretrieved documents and the output. Extensive experimental results show that\nSAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy\nand response generation. By further ablation experiments and effectiveness\nanalysis, SAM-RAG maintains high recall quality while improving overall task\nperformance in multimodal RAG task. Our codes are available at\nhttps://github.com/SAM-RAG/SAM_RAG.", "published": "2024-10-15 06:39:35", "link": "http://arxiv.org/abs/2410.11321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LargePiG: Your Large Language Model is Secretly a Pointer Generator", "abstract": "Recent research on query generation has focused on using Large Language\nModels (LLMs), which despite bringing state-of-the-art performance, also\nintroduce issues with hallucinations in the generated queries. In this work, we\nintroduce relevance hallucination and factuality hallucination as a new\ntypology for hallucination problems brought by query generation based on LLMs.\nWe propose an effective way to separate content from form in LLM-generated\nqueries, which preserves the factual knowledge extracted and integrated from\nthe inputs and compiles the syntactic structure, including function words,\nusing the powerful linguistic capabilities of the LLM. Specifically, we\nintroduce a model-agnostic and training-free method that turns the Large\nLanguage Model into a Pointer-Generator (LargePiG), where the pointer attention\ndistribution leverages the LLM's inherent attention weights, and the copy\nprobability is derived from the difference between the vocabulary distribution\nof the model's high layers and the last layer. To validate the effectiveness of\nLargePiG, we constructed two datasets for assessing the hallucination problems\nin query generation, covering both document and video scenarios. Empirical\nstudies on various LLMs demonstrated the superiority of LargePiG on both\ndatasets. Additional experiments also verified that LargePiG could reduce\nhallucination in large vision language models and improve the accuracy of\ndocument-based question-answering and factuality evaluation tasks.", "published": "2024-10-15 07:41:40", "link": "http://arxiv.org/abs/2410.11366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Have the Generalization Ability in Conducting Causal Inference?", "abstract": "In causal inference, generalization capability refers to the ability to\nconduct causal inference methods on new data to estimate the causal-effect\nbetween unknown phenomenon, which is crucial for expanding the boundaries of\nknowledge. Studies have evaluated the causal inference capabilities of Large\nLanguage Models (LLMs) concerning known phenomena, yet the generalization\ncapabilities of LLMs concerning unseen phenomena remain unexplored. In this\npaper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment\n(BA), Factual Inference (FI), and Counterfactual Inference (CI) as\nrepresentatives of causal inference tasks. To generate evaluation questions\nabout previously unseen phenomena in new data on the four tasks, we propose a\nbenchmark generation framework, which employs randomly generated graphs and\nnode names to formulate questions within hypothetical new causal scenarios.\nBased on this framework, we compile a benchmark dataset of varying levels of\nquestion complexity. We extensively tested the generalization capabilities of\nfive leading LLMs across four tasks. Experiment results reveal that while LLMs\nexhibit good generalization performance in solving simple CP, FI, and complex\nCI questions, they encounter difficulties when tackling BA questions and face\nobvious performance fluctuations as the problem complexity changes.\nFurthermore, when the names of phenomena incorporate existing terms, even if\nthese names are entirely novel, their generalization performance can still be\nhindered by interference from familiar terms.", "published": "2024-10-15 08:23:31", "link": "http://arxiv.org/abs/2410.11385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability", "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads.", "published": "2024-10-15 09:02:09", "link": "http://arxiv.org/abs/2410.11414v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Titanic Calling: Low Bandwidth Video Conference from the Titanic Wreck", "abstract": "In this paper, we report on communication experiments conducted in the summer\nof 2022 during a deep dive to the wreck of the Titanic. Radio transmission is\nnot possible in deep sea water, and communication links rely on sonar signals.\nDue to the low bandwidth of sonar signals and the need to communicate readable\ndata, text messaging is used in deep-sea missions. In this paper, we report\nresults and experiences from a messaging system that converts speech to text in\na submarine, sends text messages to the surface, and reconstructs those\nmessages as synthetic lip-synchronous videos of the speakers. The resulting\nsystem was tested during an actual dive to Titanic in the summer of 2022. We\nachieved an acceptable latency for a system of such complexity as well as good\nquality. The system demonstration video can be found at the following link:\nhttps://youtu.be/C4lyM86-5Ig", "published": "2024-10-15 09:35:07", "link": "http://arxiv.org/abs/2410.11434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a\n  simple RAG task", "abstract": "This paper describes our $3^{rd}$ place submission in the AVeriTeC shared\ntask in which we attempted to address the challenge of fact-checking with\nevidence retrieved in the wild using a simple scheme of Retrieval-Augmented\nGeneration (RAG) designed for the task, leveraging the predictive power of\nLarge Language Models. We release our codebase and explain its two modules -\nthe Retriever and the Evidence & Label generator - in detail, justifying their\nfeatures such as MMR-reranking and Likert-scale confidence estimation. We\nevaluate our solution on AVeriTeC dev and test set and interpret the results,\npicking the GPT-4o as the most appropriate model for our pipeline at the time\nof our publication, with Llama 3.1 70B being a promising open-source\nalternative. We perform an empirical error analysis to see that faults in our\npredictions often coincide with noise in the data or ambiguous fact-checks,\nprovoking further research and data augmentation.", "published": "2024-10-15 09:50:19", "link": "http://arxiv.org/abs/2410.11446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal\n  Studies", "abstract": "This paper introduces a cross-lingual statutory article retrieval (SAR)\ndataset designed to enhance legal information retrieval in multilingual\nsettings. Our dataset features spoken-language-style legal inquiries in\nEnglish, paired with corresponding Chinese versions and relevant statutes,\ncovering all Taiwanese civil, criminal, and administrative laws. This dataset\naims to improve access to legal information for non-native speakers,\nparticularly for foreign nationals in Taiwan. We propose several LLM-based\nmethods as baselines for evaluating retrieval effectiveness, focusing on\nmitigating translation errors and improving cross-lingual retrieval\nperformance. Our work provides a valuable resource for developing inclusive\nlegal information retrieval systems.", "published": "2024-10-15 09:53:40", "link": "http://arxiv.org/abs/2410.11450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tending Towards Stability: Convergence Challenges in Small Language\n  Models", "abstract": "Increasing the number of parameters in language models is a common strategy\nto enhance their performance. However, smaller language models remain valuable\ndue to their lower operational costs. Despite their advantages, smaller models\nfrequently underperform compared to their larger counterparts, even when\nprovided with equivalent data and computational resources. Specifically, their\nperformance tends to degrade in the late pretraining phase. This is anecdotally\nattributed to their reduced representational capacity. Yet, the exact causes of\nthis performance degradation remain unclear. We use the Pythia model suite to\nanalyse the training dynamics that underlie this phenomenon. Across different\nmodel sizes, we investigate the convergence of the Attention and MLP\nactivations to their final state and examine how the effective rank of their\nparameters influences this process. We find that nearly all layers in larger\nmodels stabilise early in training - within the first 20% - whereas layers in\nsmaller models exhibit slower and less stable convergence, especially when\ntheir parameters have lower effective rank. By linking the convergence of\nlayers' activations to their parameters' effective rank, our analyses can guide\nfuture work to address inefficiencies in the learning dynamics of small models.", "published": "2024-10-15 09:57:19", "link": "http://arxiv.org/abs/2410.11451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language\n  Models", "abstract": "Large language models (LLMs) have exhibited outstanding performance in\nengaging with humans and addressing complex questions by leveraging their vast\nimplicit knowledge and robust reasoning capabilities. However, such models are\nvulnerable to jailbreak attacks, leading to the generation of harmful\nresponses. Despite recent research on single-turn jailbreak strategies to\nfacilitate the development of defence mechanisms, the challenge of revealing\nvulnerabilities under multi-turn setting remains relatively under-explored. In\nthis work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective\nmulti-turn jailbreak strategy against the advanced LLMs. JSP splits questions\ninto harmless fractions as the input of each turn, and requests LLMs to\nreconstruct and respond to questions under multi-turn interaction. Our\nexperimental results demonstrate that the proposed JSP jailbreak bypasses\noriginal safeguards against explicitly harmful content, achieving an average\nattack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs\n(Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP\nachieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful\nquery benchmark, and exhibits strong resistant to defence strategies. Warning:\nthis paper contains offensive examples.", "published": "2024-10-15 10:07:15", "link": "http://arxiv.org/abs/2410.11459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training\n  with Syntactic Smoothing", "abstract": "Language models strongly rely on frequency information because they maximize\nthe likelihood of tokens during pre-training. As a consequence, language models\ntend to not generalize well to tokens that are seldom seen during training.\nMoreover, maximum likelihood training has been discovered to give rise to\nanisotropy: representations of tokens in a model tend to cluster tightly in a\nhigh-dimensional cone, rather than spreading out over their representational\ncapacity.\n  Our work introduces a method for quantifying the frequency bias of a language\nmodel by assessing sentence-level perplexity with respect to token-level\nfrequency. We then present a method for reducing the frequency bias of a\nlanguage model by inducing a syntactic prior over token representations during\npre-training. Our Syntactic Smoothing method adjusts the maximum likelihood\nobjective function to distribute the learning signal to syntactically similar\ntokens. This approach results in better performance on infrequent English\ntokens and a decrease in anisotropy. We empirically show that the degree of\nanisotropy in a model correlates with its frequency bias.", "published": "2024-10-15 10:09:57", "link": "http://arxiv.org/abs/2410.11462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "O-Edit: Orthogonal Subspace Editing for Language Model Sequential\n  Editing", "abstract": "Large language models (LLMs) acquire knowledge during pre-training, but over\ntime, this knowledge may become incorrect or outdated, necessitating updates\nafter training. Knowledge editing techniques address this issue without the\nneed for costly re-training. However, most existing methods are designed for\nsingle edits, and as the number of edits increases, they often cause a decline\nin the model's overall performance, posing significant challenges for\nsequential editing. To overcome this, we propose Orthogonal Subspace Editing,\nO-Edit. This algorithm orthogonalizes the direction of each knowledge update,\nminimizing interference between successive updates and reducing the impact of\nnew updates on unrelated knowledge. Our approach does not require replaying\npreviously edited data and processes each edit knowledge on time. It can\nperform thousands of edits on mainstream LLMs, achieving an average performance\nimprovement that is 4.2 times better than existing methods while effectively\npreserving the model's performance on downstream tasks, all with minimal\nadditional parameter overhead.", "published": "2024-10-15 10:16:45", "link": "http://arxiv.org/abs/2410.11469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TopoLM: brain-like spatio-functional organization in a topographic\n  language model", "abstract": "Neurons in the brain are spatially organized such that neighbors on tissue\noften exhibit similar response profiles. In the human language system,\nexperimental studies have observed clusters for syntactic and semantic\ncategories, but the mechanisms underlying this functional organization remain\nunclear. Here, building on work from the vision literature, we develop TopoLM,\na transformer language model with an explicit two-dimensional spatial\nrepresentation of model units. By combining a next-token prediction objective\nwith a spatial smoothness loss, representations in this model assemble into\nclusters that correspond to semantically interpretable groupings of text and\nclosely match the functional organization in the brain's language system.\nTopoLM successfully predicts the emergence of the spatio-functional\norganization of a cortical language system as well as the organization of\nfunctional clusters selective for fine-grained linguistic features empirically\nobserved in human cortex. Our results suggest that the functional organization\nof the human language system is driven by a unified spatial objective, and\nprovide a functionally and spatially aligned model of language processing in\nthe brain.", "published": "2024-10-15 11:37:21", "link": "http://arxiv.org/abs/2410.11516v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Reasoning in Large Language Models: A Knowledge Graph Approach", "abstract": "Large language models (LLMs) typically improve performance by either\nretrieving semantically similar information, or enhancing reasoning abilities\nthrough structured prompts like chain-of-thought. While both strategies are\nconsidered crucial, it remains unclear which has a greater impact on model\nperformance or whether a combination of both is necessary. This paper answers\nthis question by proposing a knowledge graph (KG)-based random-walk reasoning\napproach that leverages causal relationships. We conduct experiments on the\ncommonsense question answering task that is based on a KG. The KG inherently\nprovides both relevant information, such as related entity keywords, and a\nreasoning structure through the connections between nodes. Experimental results\nshow that the proposed KG-based random-walk reasoning method improves the\nreasoning ability and performance of LLMs. Interestingly, incorporating three\nseemingly irrelevant sentences into the query using KG-based random-walk\nreasoning enhances LLM performance, contrary to conventional wisdom. These\nfindings suggest that integrating causal structures into prompts can\nsignificantly improve reasoning capabilities, providing new insights into the\nrole of causality in optimizing LLM performance.", "published": "2024-10-15 13:24:44", "link": "http://arxiv.org/abs/2410.11588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the WMT 2024 Shared Task on Chat Translation", "abstract": "This paper presents the findings from the third edition of the Chat\nTranslation Shared Task. As with previous editions, the task involved\ntranslating bilingual customer support conversations, specifically focusing on\nthe impact of conversation context in translation quality and evaluation. We\nalso include two new language pairs: English-Korean and English-Dutch, in\naddition to the set of language pairs from previous editions: English-German,\nEnglish-French, and English-Brazilian Portuguese. We received 22 primary\nsubmissions and 32 contrastive submissions from eight teams, with each language\npair having participation from at least three teams. We evaluated the systems\ncomprehensively using both automatic metrics and human judgments via a direct\nassessment framework. The official rankings for each language pair were\ndetermined based on human evaluation scores, considering performance in both\ntranslation directions--agent and customer. Our analysis shows that while the\nsystems excelled at translating individual turns, there is room for improvement\nin overall conversation-level translation quality.", "published": "2024-10-15 14:13:17", "link": "http://arxiv.org/abs/2410.11624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization and Morphology in Multilingual Language Models: A\n  Comparative Analysis of mT5 and ByT5", "abstract": "Morphology is a crucial factor for multilingual language modeling as it poses\ndirect challenges for tokenization. Here, we seek to understand how\ntokenization influences the morphological knowledge encoded in multilingual\nlanguage models. Specifically, we capture the impact of tokenization by\ncontrasting two multilingual language models: mT5 and ByT5. The two models\nshare the same architecture, training objective, and training data and only\ndiffer in their tokenization strategies: subword tokenization vs.\\@\ncharacter-level tokenization. Probing the morphological knowledge encoded in\nthese models on four tasks and 17 languages, our analyses show that the models\nlearn the morphological systems of some languages better than others and that\nmorphological information is encoded in the middle and late layers. Finally, we\nshow that languages with more irregularities benefit more from having a higher\nshare of the pre-training data.", "published": "2024-10-15 14:14:19", "link": "http://arxiv.org/abs/2410.11627v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Measuring Spiritual Values and Bias of Large Language Models", "abstract": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.", "published": "2024-10-15 14:33:23", "link": "http://arxiv.org/abs/2410.11647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of\n  Large Language Models", "abstract": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters.", "published": "2024-10-15 14:41:44", "link": "http://arxiv.org/abs/2410.11654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliciting Textual Descriptions from Representations of Continuous\n  Prompts", "abstract": "Continuous prompts, or \"soft prompts\", are a widely-adopted\nparameter-efficient tuning strategy for large language models, but are often\nless favorable due to their opaque nature. Prior attempts to interpret\ncontinuous prompts relied on projecting individual prompt tokens onto the\nvocabulary space. However, this approach is problematic as performant prompts\ncan yield arbitrary or contradictory text, and it interprets prompt tokens\nindividually. In this work, we propose a new approach to interpret continuous\nprompts that elicits textual descriptions from their representations during\nmodel inference. Using a Patchscopes variant (Ghandeharioun et al., 2024)\ncalled InSPEcT over various tasks, we show our method often yields accurate\ntask descriptions which become more faithful as task performance increases.\nMoreover, an elaborated version of InSPEcT reveals biased features in\ncontinuous prompts, whose presence correlates with biased model predictions.\nProviding an effective interpretability solution, InSPEcT can be leveraged to\ndebug unwanted properties in continuous prompts and inform developers on ways\nto mitigate them.", "published": "2024-10-15 14:46:11", "link": "http://arxiv.org/abs/2410.11660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with\n  Sentence Interpolation and Gradual MT", "abstract": "Recent Large Language Models (LLMs) have demonstrated strong performance in\ntranslation without needing to be finetuned on additional parallel corpora.\nHowever, they still underperform for low-resource language pairs. Previous\nworks have focused on mitigating this issue by leveraging relevant few-shot\nexamples or external resources such as dictionaries or grammar books, making\nmodels heavily reliant on these nonparametric sources of information. In this\npaper, we propose a novel method named IntGrad MT that focuses on fully\nexploiting an LLM's inherent translation capability. IntGrad MT achieves this\nby constructing a chain of few-shot examples, each consisting of a source\nsentence and the model's own translation, that rise incrementally in\ndifficulty. IntGrad MT employs two techniques: Sentence Interpolation, which\ngenerates a sequence of sentences that gradually change from an easy sentence\nto translate to a difficult one, and Gradual MT, which sequentially translates\nthis chain using translations of earlier sentences as few-shot examples for the\ntranslation of subsequent ones. With this approach, we observe a substantial\nenhancement in the xCOMET scores of various LLMs for multiple languages,\nespecially in low-resource languages such as Hindi(8.26), Swahili(7.10),\nBengali(6.97) and Marathi(13.03). Our approach presents a practical way of\nenhancing LLMs' performance without extra training.", "published": "2024-10-15 15:26:28", "link": "http://arxiv.org/abs/2410.11693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language\n  Models", "abstract": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git.", "published": "2024-10-15 15:46:17", "link": "http://arxiv.org/abs/2410.11710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Converging to a Lingua Franca: Evolution of Linguistic Regions and\n  Semantics Alignment in Multilingual Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable performance,\nparticularly in multilingual contexts. While recent studies suggest that LLMs\ncan transfer skills learned in one language to others, the internal mechanisms\nbehind this ability remain unclear. We observed that the neuron activation\npatterns of LLMs exhibit similarities when processing the same language,\nrevealing the existence and location of key linguistic regions. Additionally,\nwe found that neuron activation patterns are similar when processing sentences\nwith the same semantic meaning in different languages. This indicates that LLMs\nmap semantically identical inputs from different languages into a \"Lingua\nFranca\", a common semantic latent space that allows for consistent processing\nacross languages. This semantic alignment becomes more pronounced with training\nand increased model size, resulting in a more language-agnostic activation\npattern. Moreover, we found that key linguistic neurons are concentrated in the\nfirst and last layers of LLMs, becoming denser in the first layers as training\nprogresses. Experiments on BLOOM and LLaMA2 support these findings,\nhighlighting the structural evolution of multilingual LLMs during training and\nscaling up. This paper provides insights into the internal workings of LLMs,\noffering a foundation for future improvements in their cross-lingual\ncapabilities.", "published": "2024-10-15 15:49:15", "link": "http://arxiv.org/abs/2410.11718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models", "abstract": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task.", "published": "2024-10-15 17:33:43", "link": "http://arxiv.org/abs/2410.11805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Holistic Reasoning with Long-Context LMs: A Benchmark for Database\n  Operations on Massive Textual Data", "abstract": "The rapid increase in textual information means we need more efficient\nmethods to sift through, organize, and understand it all. While\nretrieval-augmented generation (RAG) models excel in accessing information from\nlarge document collections, they struggle with complex tasks that require\naggregation and reasoning over information spanning across multiple\ndocuments--what we call holistic reasoning. Long-context language models\n(LCLMs) have great potential for managing large-scale documents, but their\nholistic reasoning capabilities remain unclear. In this work, we introduce\nHoloBench, a novel framework that brings database reasoning operations into\ntext-based contexts, making it easier to systematically evaluate how LCLMs\nhandle holistic reasoning across large documents. Our approach adjusts key\nfactors such as context length, information density, distribution of\ninformation, and query complexity to evaluate LCLMs comprehensively. Our\nexperiments show that the amount of information in the context has a bigger\ninfluence on LCLM performance than the actual context length. Furthermore, the\ncomplexity of queries affects performance more than the amount of information,\nparticularly for different types of queries. Interestingly, queries that\ninvolve finding maximum or minimum values are easier for LCLMs and are less\naffected by context length, even though they pose challenges for RAG systems.\nHowever, tasks requiring the aggregation of multiple pieces of information show\na noticeable drop in accuracy as context length increases. Additionally, we\nfind that while grouping relevant information generally improves performance,\nthe optimal positioning varies across models. Our findings surface both the\nadvancements and the ongoing challenges in achieving a holistic understanding\nof long contexts.", "published": "2024-10-15 19:04:13", "link": "http://arxiv.org/abs/2410.11996v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent\n  Representations of Human-Defined Legal Concepts", "abstract": "This paper aims to offer AI & Law researchers and practitioners a more\ndetailed understanding of whether and how continued pre-training and\ninstruction fine-tuning (IFT) of large language models (LLMs) on legal corpora\nincreases their utilization of human-defined legal concepts when developing\nglobal contextual representations of input sequences. We compared three models:\nMistral 7B, SaulLM-7B-Base (Mistral 7B with continued pre-training on legal\ncorpora), and SaulLM-7B-Instruct (with further IFT). This preliminary\nassessment examined 7 distinct text sequences from recent AI & Law literature,\neach containing a human-defined legal concept. We first compared the\nproportions of total attention the models allocated to subsets of tokens\nrepresenting the legal concepts. We then visualized patterns of raw attention\nscore alterations, evaluating whether legal training introduced novel attention\npatterns corresponding to structures of human legal knowledge. This inquiry\nrevealed that (1) the impact of legal training was unevenly distributed across\nthe various human-defined legal concepts, and (2) the contextual\nrepresentations of legal knowledge learned during legal training did not\ncoincide with structures of human-defined legal concepts. We conclude with\nsuggestions for further investigation into the dynamics of legal LLM training.", "published": "2024-10-15 19:06:14", "link": "http://arxiv.org/abs/2410.12001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option", "abstract": "The recently proposed ToolkenGPT tool learning paradigm demonstrates\npromising performance but suffers from two major issues: first, it cannot\nbenefit from tool documentation, and second, it often makes mistakes in whether\nto use a tool at all. We introduce Toolken+ that mitigates the first problem by\nreranking top $k$ tools selected by ToolkenGPT and the second problem with a\nspecial \"Reject\" option such that the model will generate a vocabulary token if\n\"Reject\" is ranked first. We demonstrate the effectiveness of Toolken+ on\nmultistep numerical reasoning and tool selection tasks.", "published": "2024-10-15 19:09:03", "link": "http://arxiv.org/abs/2410.12004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based\n  Language Models", "abstract": "Pixel-based language models have emerged as a compelling alternative to\nsubword-based language modelling, particularly because they can represent\nvirtually any script. PIXEL, a canonical example of such a model, is a vision\ntransformer that has been pre-trained on rendered text. While PIXEL has shown\npromising cross-script transfer abilities and robustness to orthographic\nperturbations, it falls short of outperforming monolingual subword counterparts\nlike BERT in most other contexts. This discrepancy raises questions about the\namount of linguistic knowledge learnt by these models and whether their\nperformance in language tasks stems more from their visual capabilities than\ntheir linguistic ones. To explore this, we probe PIXEL using a variety of\nlinguistic and visual tasks to assess its position on the vision-to-language\nspectrum. Our findings reveal a substantial gap between the model's visual and\nlinguistic understanding. The lower layers of PIXEL predominantly capture\nsuperficial visual features, whereas the higher layers gradually learn more\nsyntactic and semantic abstractions. Additionally, we examine variants of PIXEL\ntrained with different text rendering strategies, discovering that introducing\ncertain orthographic constraints at the input level can facilitate earlier\nlearning of surface-level features. With this study, we hope to provide\ninsights that aid the further development of pixel-based language models.", "published": "2024-10-15 19:21:23", "link": "http://arxiv.org/abs/2410.12011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree", "abstract": "Logical fallacy uses invalid or faulty reasoning in the construction of a\nstatement. Despite the prevalence and harmfulness of logical fallacies,\ndetecting and classifying logical fallacies still remains a challenging task.\nWe observe that logical fallacies often use connective words to indicate an\nintended logical relation between two arguments, while the argument semantics\ndoes not actually support the logical relation. Inspired by this observation,\nwe propose to build a logical structure tree to explicitly represent and track\nthe hierarchical logic flow among relation connectives and their arguments in a\nstatement. Specifically, this logical structure tree is constructed in an\nunsupervised manner guided by the constituency tree and a taxonomy of\nconnectives for ten common logical relations, with relation connectives as\nnon-terminal nodes and textual arguments as terminal nodes, and the latter are\nmostly elementary discourse units. We further develop two strategies to\nincorporate the logical structure tree into LLMs for fallacy reasoning.\nFirstly, we transform the tree into natural language descriptions and feed the\ntextualized tree into LLMs as a part of the hard text prompt. Secondly, we\nderive a relation-aware tree embedding and insert the tree embedding into LLMs\nas a soft prompt. Experiments on benchmark datasets demonstrate that our\napproach based on logical structure tree significantly improves precision and\nrecall for both fallacy detection and fallacy classification.", "published": "2024-10-15 20:35:50", "link": "http://arxiv.org/abs/2410.12048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skill-LLM: Repurposing General-Purpose LLMs for Skill Extraction", "abstract": "Accurate skill extraction from job descriptions is crucial in the hiring\nprocess but remains challenging. Named Entity Recognition (NER) is a common\napproach used to address this issue. With the demonstrated success of large\nlanguage models (LLMs) in various NLP tasks, including NER, we propose\nfine-tuning a specialized Skill-LLM and a light weight model to improve the\nprecision and quality of skill extraction. In our study, we evaluated the\nfine-tuned Skill-LLM and the light weight model using a benchmark dataset and\ncompared its performance against state-of-the-art (SOTA) methods. Our results\nshow that this approach outperforms existing SOTA techniques.", "published": "2024-10-15 20:41:18", "link": "http://arxiv.org/abs/2410.12052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A State-of-the-Art Morphosyntactic Parser and Lemmatizer for Ancient\n  Greek", "abstract": "This paper presents an experiment consisting in the comparison of six models\nto identify a state-of-the-art morphosyntactic parser and lemmatizer for\nAncient Greek capable of annotating according to the Ancient Greek Dependency\nTreebank annotation scheme. A normalized version of the major collections of\nannotated texts was used to (i) train the baseline model Dithrax with randomly\ninitialized character embeddings and (ii) fine-tune Trankit and four recent\nmodels pretrained on Ancient Greek texts, i.e., GreBERTa and PhilBERTa for\nmorphosyntactic annotation and GreTA and PhilTa for lemmatization. A Bayesian\nanalysis shows that Dithrax and Trankit annotate morphology practically\nequivalently, while syntax is best annotated by Trankit and lemmata by GreTa.\nThe results of the experiment suggest that token embeddings are not sufficient\nto achieve high UAS and LAS scores unless they are coupled with a modeling\nstrategy specifically designed to capture syntactic relationships. The dataset\nand best-performing models are made available online for reuse.", "published": "2024-10-15 20:49:48", "link": "http://arxiv.org/abs/2410.12055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring transfer learning for Deep NLP systems on rarely annotated\n  languages", "abstract": "Natural language processing (NLP) has experienced rapid advancements with the\nrise of deep learning, significantly outperforming traditional rule-based\nmethods. By capturing hidden patterns and underlying structures within data,\ndeep learning has improved performance across various NLP tasks, overcoming the\nlimitations of rule-based systems. However, most research and development in\nNLP has been concentrated on a select few languages, primarily those with large\nnumbers of speakers or financial significance, leaving many others\nunderexplored. This lack of research is often attributed to the scarcity of\nadequately annotated datasets essential for training deep learning models.\nDespite this challenge, there is potential in leveraging the linguistic\nsimilarities between unexplored and well-studied languages, particularly those\nin close geographic and linguistic proximity. This thesis investigates the\napplication of transfer learning for Part-of-Speech (POS) tagging between Hindi\nand Nepali, two highly similar languages belonging to the Indo-Aryan language\nfamily. Specifically, the work explores whether joint training of a POS tagging\nmodel for both languages enhances performance. Additionally, we assess whether\nmultitask learning in Hindi, with auxiliary tasks such as gender and\nsingular/plural tagging, can contribute to improved POS tagging accuracy. The\ndeep learning architecture employed is the BLSTM-CNN-CRF model, trained under\ndifferent conditions: monolingual word embeddings, vector-mapped embeddings,\nand jointly trained Hindi-Nepali word embeddings. Varying dropout rates (0.25\nto 0.5) and optimizers (ADAM and AdaDelta) are also evaluated. Results indicate\nthat jointly trained Hindi-Nepali word embeddings improve performance across\nall models compared to monolingual and vector-mapped embeddings.", "published": "2024-10-15 13:33:54", "link": "http://arxiv.org/abs/2410.12879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Athena: Retrieval-augmented Legal Judgment Prediction with Large\n  Language Models", "abstract": "Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have\nprevailed in countless domains, including legal scenarios. With LLMs' rapid\ntechnological progress, the development of prompt engineering (PE) as an\ninterface between the LLMs and real-world applications has drawn the attention\nof all developers. Various PE methods have been proposed to overcome real-world\nchallenges, such as few-shot prompting, chain-of-thought, and\nretrieval-augmented generation (RAG). However, RAG for legal judgment\nprediction (LJP) is still underexplored. To address this, we propose \"Athena\",\na novel framework cultivating RAG as a core preprocess component to enhance\nLLMs' performance on specialized tasks. Athena constructs a knowledge base for\naccusations, attached with a semantic retrieval mechanism through\nvectorization. Our experiments show that Athena's overall performance has\nimproved significantly, achieving state-of-the-art results on the CAIL2018\ndataset. Our ablation study on the in-context window size parameter further\nreproduces LLMs' \"lost-in-the-middle\" phenomenon with a relative positional\nvariation. And with moderate hyper-parameter-tuning, we can achieve at most 95%\nof accuracy accordingly. We also study the impact of query rewriting and data\ndistribution, providing possible directions for future research based on former\nanalyses.", "published": "2024-10-15 02:18:01", "link": "http://arxiv.org/abs/2410.11195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR\n  Applications", "abstract": "Recent LLM (Large Language Models) advancements benefit many fields such as\neducation and finance, but HR has hundreds of repetitive processes, such as\naccess requests, medical claim filing and time-off submissions, which are\nunaddressed. We relate these tasks to the LLM agent, which has addressed tasks\nsuch as writing assisting and customer support. We present HR-Agent, an\nefficient, confidential, and HR-specific LLM-based task-oriented dialogue\nsystem tailored for automating repetitive HR processes such as medical claims\nand access requests. Since conversation data is not sent to an LLM during\ninference, it preserves confidentiality required in HR-related tasks.", "published": "2024-10-15 03:51:08", "link": "http://arxiv.org/abs/2410.11239v1", "categories": ["cs.CL", "cs.AI", "68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "In-Context Learning for Long-Context Sentiment Analysis on\n  Infrastructure Project Opinions", "abstract": "Large language models (LLMs) have achieved impressive results across various\ntasks. However, they still struggle with long-context documents. This study\nevaluates the performance of three leading LLMs: GPT-4o, Claude 3.5 Sonnet, and\nGemini 1.5 Pro on lengthy, complex, and opinion-varying documents concerning\ninfrastructure projects, under both zero-shot and few-shot scenarios. Our\nresults indicate that GPT-4o excels in zero-shot scenarios for simpler, shorter\ndocuments, while Claude 3.5 Sonnet surpasses GPT-4o in handling more complex,\nsentiment-fluctuating opinions. In few-shot scenarios, Claude 3.5 Sonnet\noutperforms overall, while GPT-4o shows greater stability as the number of\ndemonstrations increases.", "published": "2024-10-15 04:42:21", "link": "http://arxiv.org/abs/2410.11265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Process Reward Model with Q-Value Rankings", "abstract": "Process Reward Modeling (PRM) is critical for complex reasoning and\ndecision-making tasks where the accuracy of intermediate steps significantly\ninfluences the overall outcome. Existing PRM approaches, primarily framed as\nclassification problems, employ cross-entropy loss to independently evaluate\neach step's correctness. This method can lead to suboptimal reward distribution\nand does not adequately address the interdependencies among steps. To address\nthese limitations, we introduce the Process Q-value Model (PQM), a novel\nframework that redefines PRM in the context of a Markov Decision Process. PQM\noptimizes Q-value rankings based on a novel comparative loss function,\nenhancing the model's ability to capture the intricate dynamics among\nsequential decisions. This approach provides a more granular and theoretically\ngrounded methodology for process rewards. Our extensive empirical evaluations\nacross various sampling policies, language model backbones, and multi-step\nreasoning benchmarks show that PQM outperforms classification-based PRMs. The\neffectiveness of the comparative loss function is highlighted in our\ncomprehensive ablation studies, confirming PQM's practical efficacy and\ntheoretical advantage.", "published": "2024-10-15 05:10:34", "link": "http://arxiv.org/abs/2410.11287v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset\n  Repository", "abstract": "This paper introduces a centralized, open-source dataset repository designed\nto advance NLP and NMT for Assamese, a low-resource language. The repository,\navailable at GitHub, supports various tasks like sentiment analysis, named\nentity recognition, and machine translation by providing both pre-training and\nfine-tuning corpora. We review existing datasets, highlighting the need for\nstandardized resources in Assamese NLP, and discuss potential applications in\nAI-driven research, such as LLMs, OCR, and chatbots. While promising,\nchallenges like data scarcity and linguistic diversity remain. The repository\naims to foster collaboration and innovation, promoting Assamese language\nresearch in the digital age.", "published": "2024-10-15 05:26:57", "link": "http://arxiv.org/abs/2410.11291v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap\n  Through Interleaved Sampling", "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student\nmodels to approach the performance of larger teacher models. However, popular\nmethods such as supervised KD and on-policy KD, are adversely impacted by the\nknowledge gaps between teacher-student in practical scenarios. Supervised KD\nsuffers from a distribution mismatch between training with a static dataset and\ninference over final student-generated outputs. Conversely, on-policy KD, which\nuses student-generated samples for training, can suffer from low-quality\ntraining examples with which teacher models are not familiar, resulting in\ninaccurate teacher feedback. To address these limitations, we introduce\nSpeculative Knowledge Distillation (SKD), a novel approach that leverages\ncooperation between student and teacher models to generate high-quality\ntraining data on-the-fly while aligning with the student's inference-time\ndistribution. In SKD, the student proposes tokens, and the teacher replaces\npoorly ranked ones based on its own distribution, transferring high-quality\nknowledge adaptively. We evaluate SKD on various text generation tasks,\nincluding translation, summarization, math, and instruction following, and show\nthat SKD consistently outperforms existing KD methods across different domains,\ndata sizes, and model initialization strategies.", "published": "2024-10-15 06:51:25", "link": "http://arxiv.org/abs/2410.11325v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RATE: Causal Explainability of Reward Models with Imperfect\n  Counterfactuals", "abstract": "Reward models are widely used as proxies for human preferences when aligning\nor evaluating LLMs. However, reward models are black boxes, and it is often\nunclear what, exactly, they are actually rewarding. In this paper we develop\nRewrite-based Attribute Treatment Estimator (RATE) as an effective method for\nmeasuring the sensitivity of a reward model to high-level attributes of\nresponses, such as sentiment, helpfulness, or complexity. Importantly, RATE\nmeasures the causal effect of an attribute on the reward. RATE uses LLMs to\nrewrite responses to produce imperfect counterfactual examples that can be used\nto measure causal effects. A key challenge is that these rewrites are imperfect\nin a manner that can induce substantial bias in the estimated sensitivity of\nthe reward model to the attribute. The core idea of RATE is to adjust for this\nimperfect-rewrite effect by rewriting twice. We establish the validity of the\nRATE procedure and show empirically that it is an effective estimator.", "published": "2024-10-15 07:22:16", "link": "http://arxiv.org/abs/2410.11348v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhance Graph Alignment for Large Language Models", "abstract": "Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.", "published": "2024-10-15 07:50:34", "link": "http://arxiv.org/abs/2410.11370v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation\n  of Autoregressive Language Models for Text-to-SQL", "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL,\nwhich involves translating natural language questions into SQL queries.\nHowever, current text-to-SQL LLMs are computationally expensive and challenging\nto deploy in real-world applications, highlighting the importance of\ncompressing them. To achieve this goal, knowledge distillation (KD) is a common\napproach, which aims to distill the larger teacher model into a smaller student\nmodel. While numerous KD methods for autoregressive LLMs have emerged recently,\nit is still under-explored whether they work well in complex text-to-SQL\nscenarios. To this end, we conduct a series of analyses and reveal that these\nKD methods generally fall short in balancing performance and efficiency. In\nresponse to this problem, we propose to improve the KD with Imperfect Data,\nnamely KID, which effectively boosts the performance without introducing much\ntraining budget. The core of KID is to efficiently mitigate the\ntraining-inference mismatch by simulating the cascading effect of inference in\nthe imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks\nshow that, KID can not only achieve consistent and significant performance\ngains (up to +5.83% average score) across all model types and sizes, but also\neffectively improve the training efficiency.", "published": "2024-10-15 07:51:00", "link": "http://arxiv.org/abs/2410.11371v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "PMMT: Preference Alignment in Multilingual Machine Translation via LLM\n  Distillation", "abstract": "Translation is important for cross-language communication, and many efforts\nhave been made to improve its accuracy. However, less investment is conducted\nin aligning translations with human preferences, such as translation tones or\nstyles. In this paper, a new method is proposed to effectively generate\nlarge-scale multilingual parallel corpora with specific translation preferences\nusing Large Language Models (LLMs). Meanwhile, an automatic pipeline is\ndesigned to distill human preferences into smaller Machine Translation (MT)\nmodels for efficiently and economically supporting large-scale calls in online\nservices. Experiments indicate that the proposed method takes the lead in\ntranslation tasks with aligned human preferences by a large margin. Meanwhile,\non popular public benchmarks like WMT and Flores, on which our models were not\ntrained, the proposed method also shows a competitive performance compared to\nSOTA works.", "published": "2024-10-15 08:54:27", "link": "http://arxiv.org/abs/2410.11410v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in\n  Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding\nof the real world and can even handle complex tasks. However, they still fail\non some straightforward visual question-answering (VQA) problems. This paper\ndives deeper into this issue, revealing that models tend to err when answering\neasy questions (e.g. Yes/No questions) about an image, even though they can\ncorrectly describe it. We refer to this model behavior discrepancy between\ndifficult and simple questions as model laziness. To systematically investigate\nmodel laziness, we manually construct LazyBench, a benchmark that includes\nYes/No, multiple choice, short answer questions, and image description tasks\nthat are related to the same subjects in the images. Based on LazyBench, we\nobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,\nGemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on\nstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find\nthat about half of its failure cases are caused by model laziness, which\nfurther highlights the importance of ensuring that the model fully utilizes its\ncapability. To this end, we conduct preliminary exploration on how to mitigate\nlaziness and find that chain of thought (CoT) can effectively address this\nissue.", "published": "2024-10-15 09:40:50", "link": "http://arxiv.org/abs/2410.11437v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "abstract": "In the rapidly evolving landscape of language, resolving new linguistic\nexpressions in continuously updating knowledge bases remains a formidable\nchallenge. This challenge becomes critical in retrieval-augmented generation\n(RAG) with knowledge bases, as emerging expressions hinder the retrieval of\nrelevant documents, leading to generator hallucinations. To address this issue,\nwe introduce a novel task aimed at resolving emerging mentions to dynamic\nentities and present DynamicER benchmark. Our benchmark includes dynamic entity\nmention resolution and entity-centric knowledge-intensive QA task, evaluating\nentity linking and RAG model's adaptability to new expressions, respectively.\nWe discovered that current entity linking models struggle to link these new\nexpressions to entities. Therefore, we propose a temporal segmented clustering\nmethod with continual adaptation, effectively managing the temporal dynamics of\nevolving entities and emerging mentions. Extensive experiments demonstrate that\nour method outperforms existing baselines, enhancing RAG model performance on\nQA task with resolved mentions.", "published": "2024-10-15 10:57:12", "link": "http://arxiv.org/abs/2410.11494v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs", "abstract": "While various vertical domain large language models (LLMs) have been\ndeveloped, automatically evaluating their performance across different domains\nremains a critical challenge. Current benchmark-based methods often rely on\nstatic and costly datasets, are misaligned with practical user needs, and lack\nflexibility across domains. To address these limitations, we revisit the\nevaluation process and introduce two key concepts: Benchmark+, which extends\nthe traditional question-answer benchmark into a more flexible\n``strategy-criterion'' format; and Assessment+, which enhances the interaction\nprocess, enabling deeper exploration and supporting analysis from broader\nperspectives. We propose TestAgent, an agent-based evaluation framework that\nimplements these concepts using retrieval-augmented generation and\nreinforcement learning. TestAgent enables automatic dynamic benchmark\ngeneration and in-depth assessment across diverse vertical domain scenarios.\nExperiments on tasks ranging from constructing multiple vertical domain\nevaluations to converting static benchmarks into dynamic forms demonstrate the\neffectiveness of TestAgent. This work offers an interesting perspective on\nautomatic evaluation for LLMs and highlights a pathway for dynamic and\ndomain-adaptive assessments.", "published": "2024-10-15 11:20:42", "link": "http://arxiv.org/abs/2410.11507v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nlanguage understanding and generation. Advanced utilization of the knowledge\nembedded in LLMs for automated annotation has consistently been explored. This\nstudy proposed to develop an emotion lexicon for Cantonese, a low-resource\nlanguage, through collaborative efforts between LLM and human annotators. By\nintegrating emotion labels provided by LLM and human annotators, the study\nleveraged existing linguistic resources including lexicons in other languages\nand local forums to construct a Cantonese emotion lexicon enriched with\ncolloquial expressions. The consistency of the proposed emotion lexicon in\nemotion extraction was assessed through modification and utilization of three\ndistinct emotion text datasets. This study not only validates the efficacy of\nthe constructed lexicon but also emphasizes that collaborative annotation\nbetween human and artificial intelligence can significantly enhance the quality\nof emotion labels, highlighting the potential of such partnerships in\nfacilitating natural language processing tasks for low-resource languages.", "published": "2024-10-15 11:57:34", "link": "http://arxiv.org/abs/2410.11526v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Multi-round jailbreak attack on large language models", "abstract": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters.", "published": "2024-10-15 12:08:14", "link": "http://arxiv.org/abs/2410.11533v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for\n  Drug Development", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in general tasks across various fields. However, their\neffectiveness within specific domains such as drug development remains\nchallenges. To solve these challenges, we introduce \\textbf{Y-Mol}, forming a\nwell-established LLM paradigm for the flow of drug development. Y-Mol is a\nmultiscale biomedical knowledge-guided LLM designed to accomplish tasks across\nlead compound discovery, pre-clinic, and clinic prediction. By integrating\nmillions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,\nY-Mol augments the reasoning capability in the biomedical domain by learning\nfrom a corpus of publications, knowledge graphs, and expert-designed synthetic\ndata. The capability is further enriched with three types of drug-oriented\ninstructions: description-based prompts from processed publications,\nsemantic-based prompts for extracting associations from knowledge graphs, and\ntemplate-based prompts for understanding expert knowledge from biomedical\ntools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously\nexecute the downstream tasks across the entire process of drug development,\nincluding virtual screening, drug design, pharmacological properties\nprediction, and drug-related interaction prediction. Our extensive evaluations\nof various biomedical sources demonstrate that Y-Mol significantly outperforms\ngeneral-purpose LLMs in discovering lead compounds, predicting molecular\nproperties, and identifying drug interaction events.", "published": "2024-10-15 12:39:20", "link": "http://arxiv.org/abs/2410.11550v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video\n  Retrieval", "abstract": "Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\n$\\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation.", "published": "2024-10-15 13:56:34", "link": "http://arxiv.org/abs/2410.11619v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Retrieval Augmented Spelling Correction for E-Commerce Applications", "abstract": "The rapid introduction of new brand names into everyday language poses a\nunique challenge for e-commerce spelling correction services, which must\ndistinguish genuine misspellings from novel brand names that use unconventional\nspelling. We seek to address this challenge via Retrieval Augmented Generation\n(RAG). On this approach, product names are retrieved from a catalog and\nincorporated into the context used by a large language model (LLM) that has\nbeen fine-tuned to do contextual spelling correction. Through quantitative\nevaluation and qualitative error analyses, we find improvements in spelling\ncorrection utilizing the RAG framework beyond a stand-alone LLM. We also\ndemonstrate the value of additional finetuning of the LLM to incorporate\nretrieved context.", "published": "2024-10-15 14:42:18", "link": "http://arxiv.org/abs/2410.11655v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling the Mystery of Visual Attributes of Concrete and Abstract\n  Concepts: Variability, Nearest Neighbors, and Challenging Categories", "abstract": "The visual representation of a concept varies significantly depending on its\nmeaning and the context where it occurs; this poses multiple challenges both\nfor vision and multimodal models. Our study focuses on concreteness, a\nwell-researched lexical-semantic variable, using it as a case study to examine\nthe variability in visual representations. We rely on images associated with\napproximately 1,000 abstract and concrete concepts extracted from two different\ndatasets: Bing and YFCC. Our goals are: (i) evaluate whether visual diversity\nin the depiction of concepts can reliably distinguish between concrete and\nabstract concepts; (ii) analyze the variability of visual features across\nmultiple images of the same concept through a nearest neighbor analysis; and\n(iii) identify challenging factors contributing to this variability by\ncategorizing and annotating images. Our findings indicate that for classifying\nimages of abstract versus concrete concepts, a combination of basic visual\nfeatures such as color and texture is more effective than features extracted by\nmore complex models like Vision Transformer (ViT). However, ViTs show better\nperformances in the nearest neighbor analysis, emphasizing the need for a\ncareful selection of visual features when analyzing conceptual variables\nthrough modalities other than text.", "published": "2024-10-15 14:44:36", "link": "http://arxiv.org/abs/2410.11657v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers", "abstract": "The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them.", "published": "2024-10-15 15:05:41", "link": "http://arxiv.org/abs/2410.11672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting", "abstract": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting.", "published": "2024-10-15 15:08:57", "link": "http://arxiv.org/abs/2410.11674v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Personas with Attitudes: Controlling LLMs for Diverse Data Annotation", "abstract": "We present a novel approach for enhancing diversity and control in data\nannotation tasks by personalizing large language models (LLMs). We investigate\nthe impact of injecting diverse persona descriptions into LLM prompts across\ntwo studies, exploring whether personas increase annotation diversity and\nwhether the impacts of individual personas on the resulting annotations are\nconsistent and controllable. Our results show that persona-prompted LLMs\nproduce more diverse annotations than LLMs prompted without personas and that\nthese effects are both controllable and repeatable, making our approach a\nsuitable tool for improving data annotation in subjective NLP tasks like\ntoxicity detection.", "published": "2024-10-15 16:22:49", "link": "http://arxiv.org/abs/2410.11745v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.", "published": "2024-10-15 16:53:26", "link": "http://arxiv.org/abs/2410.11772v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language\n  Models", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, including language modeling, understanding,\nand generation. However, the increased memory and computational costs\nassociated with these models pose significant challenges for deployment on\nresource-limited devices. Structural pruning has emerged as a promising\nsolution to reduce the costs of LLMs without requiring post-processing steps.\nPrior structural pruning methods either follow the dependence of structures at\nthe cost of limiting flexibility, or introduce non-trivial additional\nparameters by incorporating different projection matrices. In this work, we\npropose a novel approach that relaxes the constraint imposed by regular\nstructural pruning methods and eliminates the structural dependence along the\nembedding dimension. Our dimension-independent structural pruning method offers\nseveral benefits. Firstly, our method enables different blocks to utilize\ndifferent subsets of the feature maps. Secondly, by removing structural\ndependence, we facilitate each block to possess varying widths along its input\nand output dimensions, thereby significantly enhancing the flexibility of\nstructural pruning. We evaluate our method on various LLMs, including OPT,\nLLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our\napproach outperforms other state-of-the-art methods, showing for the first time\nthat structural pruning can achieve an accuracy similar to semi-structural\npruning.", "published": "2024-10-15 18:51:18", "link": "http://arxiv.org/abs/2410.11988v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Classification with Large Language Models in Cultural Analytics", "abstract": "In this work, we survey the way in which classification is used as a\nsensemaking practice in cultural analytics, and assess where large language\nmodels can fit into this landscape. We identify ten tasks supported by publicly\navailable datasets on which we empirically assess the performance of LLMs\ncompared to traditional supervised methods, and explore the ways in which LLMs\ncan be employed for sensemaking goals beyond mere accuracy. We find that\nprompt-based LLMs are competitive with traditional supervised models for\nestablished tasks, but perform less well on de novo tasks. In addition, LLMs\ncan assist sensemaking by acting as an intermediary input to formal theory\ntesting.", "published": "2024-10-15 20:00:59", "link": "http://arxiv.org/abs/2410.12029v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Concept-Reversed Winograd Schema Challenge: Evaluating and Improving\n  Robust Reasoning in Large Language Models via Abstraction", "abstract": "While Large Language Models (LLMs) have showcased remarkable proficiency in\nreasoning, there is still a concern about hallucinations and unreliable\nreasoning issues due to semantic associations and superficial logical chains.\nTo evaluate the extent to which LLMs perform robust reasoning instead of\nrelying on superficial logical chains, we propose a new evaluation dataset, the\nConcept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous\nWinograd Schema Challenge (WSC) dataset. By simply reversing the concepts to\nthose that are more associated with the wrong answer, we find that the\nperformance of LLMs drops significantly despite the rationale of reasoning\nremaining the same. Furthermore, we propose Abstraction-of-Thought (AoT), a\nnovel prompt method for recovering adversarial cases to normal cases using\nconceptual abstraction to improve LLMs' robustness and consistency in\nreasoning, as demonstrated by experiments on CR-WSC.", "published": "2024-10-15 20:19:27", "link": "http://arxiv.org/abs/2410.12040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sabi\u00e1-3 Technical Report", "abstract": "This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization.", "published": "2024-10-15 20:37:34", "link": "http://arxiv.org/abs/2410.12049v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large-scale cloze evaluation reveals that token prediction tasks are\n  neither lexically nor semantically aligned", "abstract": "In this work we compare the generative behavior at the next token prediction\nlevel in several language models by comparing them to human productions in the\ncloze task. We find that while large models trained for longer are typically\nbetter estimators of human productions, but they reliably under-estimate the\nprobabilities of human responses, over-rank rare responses, under-rank top\nresponses, and produce highly distinct semantic spaces. Altogether, this work\ndemonstrates in a tractable, interpretable domain that LM generations can not\nbe used as replacements of or models of the cloze task.", "published": "2024-10-15 20:52:09", "link": "http://arxiv.org/abs/2410.12057v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LegalLens Shared Task 2024: Legal Violation Identification in\n  Unstructured Text", "abstract": "This paper presents the results of the LegalLens Shared Task, focusing on\ndetecting legal violations within text in the wild across two sub-tasks:\nLegalLens-NER for identifying legal violation entities and LegalLens-NLI for\nassociating these violations with relevant legal contexts and affected\nindividuals. Using an enhanced LegalLens dataset covering labor, privacy, and\nconsumer protection domains, 38 teams participated in the task. Our analysis\nreveals that while a mix of approaches was used, the top-performing teams in\nboth tasks consistently relied on fine-tuning pre-trained language models,\noutperforming legal-specific models and few-shot methods. The top-performing\nteam achieved a 7.11% improvement in NER over the baseline, while NLI saw a\nmore marginal improvement of 5.7%. Despite these gains, the complexity of legal\ntexts leaves room for further advancements.", "published": "2024-10-15 21:02:44", "link": "http://arxiv.org/abs/2410.12064v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OMCAT: Omni Context Aware Transformer", "abstract": "Large Language Models (LLMs) have made significant strides in text generation\nand comprehension, with recent advancements extending into multimodal LLMs that\nintegrate visual and audio inputs. However, these models continue to struggle\nwith fine-grained, cross-modal temporal understanding, particularly when\ncorrelating events across audio and video streams. We address these challenges\nwith two key contributions: a new dataset and model, called OCTAV and OMCAT\nrespectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset\ndesigned to capture event transitions across audio and video. Second, OMCAT\n(Omni Context Aware Transformer) is a powerful model that leverages RoTE\n(Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal\ngrounding and computational efficiency in time-anchored tasks. Through a robust\nthree-stage training pipeline-feature alignment, instruction tuning, and\nOCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our\nmodel demonstrates state-of-the-art performance on Audio-Visual Question\nAnswering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in\ntemporal reasoning and cross-modal alignment, as validated through\ncomprehensive experiments and ablation studies. Our dataset and code will be\nmade publicly available. The link to our demo page is https://om-cat.github.io.", "published": "2024-10-15 23:16:28", "link": "http://arxiv.org/abs/2410.12109v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming", "abstract": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.", "published": "2024-10-15 23:20:54", "link": "http://arxiv.org/abs/2410.12112v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Scaling Laws for Post Training Quantized Large Language Models", "abstract": "Generalization abilities of well-trained large language models (LLMs) are\nknown to scale predictably as a function of model size. In contrast to the\nexistence of practical scaling laws governing pre-training, the quality of LLMs\nafter post-training compression remains highly unpredictable, often requiring\ncase-by-case validation in practice. In this work, we attempted to close this\ngap for post-training weight quantization of LLMs by conducting a systematic\nempirical study on multiple LLM families quantized to numerous low-precision\ntensor data types using popular weight quantization techniques. We identified\nkey scaling factors pertaining to characteristics of the local loss landscape,\nbased on which the performance of quantized LLMs can be reasonably well\npredicted by a statistical model.", "published": "2024-10-15 23:34:22", "link": "http://arxiv.org/abs/2410.12119v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In-context KV-Cache Eviction for LLMs via Attention-Gate", "abstract": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.", "published": "2024-10-15 05:01:19", "link": "http://arxiv.org/abs/2410.12876v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence\nto improve downstream task accuracy has been widely explored in recent large\nlanguage models (LLMs). Yet, these approaches fall inadequate in complex,\nmulti-hop and mathematical reasoning tasks as the synthetic data typically\nfails to add complementary knowledge to the existing raw corpus. In this work,\nwe propose a novel large-scale and diverse Math Informed syNthetic Dialogue\n(MIND) generation method that improves the mathematical reasoning ability of\nLLMs. Specifically, using MIND, we generate synthetic conversations based on\nOpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments\nwith different conversational settings reveal that incorporating knowledge gaps\nbetween dialog participants is essential for generating high-quality math data.\nWe further identify an effective way to format and integrate synthetic and raw\ndata during pretraining to maximize the gain in mathematical reasoning,\nemphasizing the need to restructure raw data rather than use it as-is. Compared\nto pretraining just on raw data, a model pretrained on MIND-OWM shows\nsignificant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%),\nincluding superior performance in specialized knowledge (MMLU: +4.55%,\nMMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING:\n+2.51%).", "published": "2024-10-15 18:25:53", "link": "http://arxiv.org/abs/2410.12881v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Scaling Laws for Multilingual Language Models", "abstract": "We propose a novel scaling law for general-purpose decoder-only language\nmodels (LMs) trained on multilingual data, tackling the problem of balancing\nlanguages during multilingual pretraining. A primary challenge in studying\nmultilingual scaling is the difficulty of analyzing individual language\nperformance due to cross-lingual transfer. To address this, we shift the focus\nfrom individual languages to language families. We introduce and validate a\nhypothesis that the test cross-entropy loss for each language family is\ndetermined solely by its own sampling ratio, independent of other languages in\nthe mixture. This insight simplifies the complexity of multilingual scaling and\nmake the analysis scalable to an arbitrary number of languages. Building on\nthis hypothesis, we derive a power-law relationship that links performance with\ndataset size, model size and sampling ratios. This relationship enables us to\npredict performance across various combinations of the above three quantities,\nand derive the optimal sampling ratios at different model scales. To\ndemonstrate the effectiveness and accuracy of our proposed scaling law, we\nperform a large-scale empirical study, training more than 100 models on 23\nlanguages spanning 5 language families. Our experiments show that the optimal\nsampling ratios derived from small models (85M parameters) generalize\neffectively to models that are several orders of magnitude larger (1.2B\nparameters), offering a resource-efficient approach for multilingual LM\ntraining at scale.", "published": "2024-10-15 20:29:38", "link": "http://arxiv.org/abs/2410.12883v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Capacity of Citation Generation by Large Language Models", "abstract": "Retrieval-augmented generation (RAG) appears as a promising method to\nalleviate the \"hallucination\" problem in large language models (LLMs), since it\ncan incorporate external traceable resources for response generation. The\nessence of RAG in combating the hallucination issue lies in accurately\nattributing claims in responses to the corresponding retrieved documents.\nHowever, most of existing works focus on improving the quality of generated\nresponses from the LLM, while largely overlooked its ability to attribute\nsources accurately. In this study, we conduct a systematic analysis about the\ncapabilities of LLMs in generating citations within response generation, and\nfurther introduce a novel method to enhance their citation generation\nabilities. Specifically, we evaluate both the correctness and citation quality\nfor seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce\nnew citation evaluation metrics to eliminate the over-penalization of\nunnecessary and excessive citations in existing metrics. Furthermore, we\npropose a Generate-then-Refine method that completes relevant citations and\nremoves irrelevant ones without altering the response text. The results on\nWebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves\nthe quality of citations in responses generated by LLMs.", "published": "2024-10-15 03:04:26", "link": "http://arxiv.org/abs/2410.11217v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Investigation of Speaker Representation for Target-Speaker Speech\n  Processing", "abstract": "Target-speaker speech processing (TS) tasks, such as target-speaker automatic\nspeech recognition (TS-ASR), target speech extraction (TSE), and personal voice\nactivity detection (p-VAD), are important for extracting information about a\ndesired speaker's speech even when it is corrupted by interfering speakers.\nWhile most studies have focused on training schemes or system architectures for\neach specific task, the auxiliary network for embedding target-speaker cues has\nnot been investigated comprehensively in a unified cross-task evaluation.\nTherefore, this paper aims to address a fundamental question: what is the\npreferred speaker embedding for TS tasks? To this end, for the TS-ASR, TSE, and\np-VAD tasks, we compare pre-trained speaker encoders (i.e., self-supervised or\nspeaker recognition models) that compute speaker embeddings from pre-recorded\nenrollment speech of the target speaker with ideal speaker embeddings derived\ndirectly from the target speaker's identity in the form of a one-hot vector. To\nfurther understand the properties of ideal speaker embedding, we optimize it\nusing a gradient-based approach to improve performance on the TS task. Our\nanalysis reveals that speaker verification performance is somewhat unrelated to\nTS task performances, the one-hot vector outperforms enrollment-based ones, and\nthe optimal embedding depends on the input mixture.", "published": "2024-10-15 03:58:13", "link": "http://arxiv.org/abs/2410.11243v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix", "abstract": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices.", "published": "2024-10-15 04:35:56", "link": "http://arxiv.org/abs/2410.11261v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs", "abstract": "In the study of LLMs, sycophancy represents a prevalent hallucination that\nposes significant challenges to these models. Specifically, LLMs often fail to\nadhere to original correct responses, instead blindly agreeing with users'\nopinions, even when those opinions are incorrect or malicious. However,\nresearch on sycophancy in visual language models (VLMs) has been scarce. In\nthis work, we extend the exploration of sycophancy from LLMs to VLMs,\nintroducing the MM-SY benchmark to evaluate this phenomenon. We present\nevaluation results from multiple representative models, addressing the gap in\nsycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic\ndataset for training and employ methods based on prompts, supervised\nfine-tuning, and DPO. Our experiments demonstrate that these methods\neffectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess\nthe semantic impact of sycophancy and analyze the attention distribution of\nvisual tokens. Our findings indicate that the ability to prevent sycophancy is\npredominantly observed in higher layers of the model. The lack of attention to\nimage knowledge in these higher layers may contribute to sycophancy, and\nenhancing image attention at high layers proves beneficial in mitigating this\nissue.", "published": "2024-10-15 05:48:14", "link": "http://arxiv.org/abs/2410.11302v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "TSDS: Data Selection for Task-Specific Model Finetuning", "abstract": "Finetuning foundation models for specific tasks is an emerging paradigm in\nmodern machine learning. The efficacy of task-specific finetuning largely\ndepends on the selection of appropriate training data. We present TSDS\n(Task-Specific Data Selection), a framework to select data for task-specific\nmodel finetuning, guided by a small but representative set of examples from the\ntarget task. To do so, we formulate data selection for task-specific finetuning\nas an optimization problem with a distribution alignment loss based on optimal\ntransport to capture the discrepancy between the selected data and the target\ndistribution. In addition, we add a regularizer to encourage the diversity of\nthe selected data and incorporate kernel density estimation into the\nregularizer to reduce the negative effects of near-duplicates among the\ncandidate data. We connect our optimization problem to nearest neighbor search\nand design efficient algorithms to compute the optimal solution based on\napproximate nearest neighbor search techniques. We evaluate our method on data\nselection for both continued pretraining and instruction tuning of language\nmodels. We show that instruction tuning using data selected by our method with\na 1% selection ratio often outperforms using the full dataset and beats the\nbaseline selection methods by 1.5 points in F1 score on average.", "published": "2024-10-15 05:54:17", "link": "http://arxiv.org/abs/2410.11303v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50, 68T01", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial\n  Prompt Translation", "abstract": "Automatic adversarial prompt generation provides remarkable success in\njailbreaking safely-aligned large language models (LLMs). Existing\ngradient-based attacks, while demonstrating outstanding performance in\njailbreaking white-box LLMs, often generate garbled adversarial prompts with\nchaotic appearance. These adversarial prompts are difficult to transfer to\nother LLMs, hindering their performance in attacking unknown victim models. In\nthis paper, for the first time, we delve into the semantic meaning embedded in\ngarbled adversarial prompts and propose a novel method that \"translates\" them\ninto coherent and human-readable natural language adversarial prompts. In this\nway, we can effectively uncover the semantic information that triggers\nvulnerabilities of the model and unambiguously transfer it to the victim model,\nwithout overlooking the adversarial information hidden in the garbled text, to\nenhance jailbreak attacks. It also offers a new approach to discovering\neffective designs for jailbreak prompts, advancing the understanding of\njailbreak attacks. Experimental results demonstrate that our method\nsignificantly improves the success rate of jailbreak attacks against various\nsafety-aligned LLMs and outperforms state-of-the-arts by large margins. With at\nmost 10 queries, our method achieves an average attack success rate of 81.8% in\nattacking 7 commercial closed-source LLMs, including GPT and Claude-3 series,\non HarmBench. Our method also achieves over 90% attack success rates against\nLlama-2-Chat models on AdvBench, despite their outstanding resistance to\njailbreak attacks. Code at:\nhttps://github.com/qizhangli/Adversarial-Prompt-Translator.", "published": "2024-10-15 06:31:04", "link": "http://arxiv.org/abs/2410.11317v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Sequential LLM Framework for Fashion Recommendation", "abstract": "The fashion industry is one of the leading domains in the global e-commerce\nsector, prompting major online retailers to employ recommendation systems for\nproduct suggestions and customer convenience. While recommendation systems have\nbeen widely studied, most are designed for general e-commerce problems and\nstruggle with the unique challenges of the fashion domain. To address these\nissues, we propose a sequential fashion recommendation framework that leverages\na pre-trained large language model (LLM) enhanced with recommendation-specific\nprompts. Our framework employs parameter-efficient fine-tuning with extensive\nfashion data and introduces a novel mix-up-based retrieval technique for\ntranslating text into relevant product suggestions. Extensive experiments show\nour proposed framework significantly enhances fashion recommendation\nperformance.", "published": "2024-10-15 06:54:27", "link": "http://arxiv.org/abs/2410.11327v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge\n  AI and Low-Resource Environments", "abstract": "We introduce Shakti, a 2.5 billion parameter language model specifically\noptimized for resource-constrained environments such as edge devices, including\nsmartphones, wearables, and IoT systems. Shakti combines high-performance NLP\nwith optimized efficiency and precision, making it ideal for real-time AI\napplications where computational resources and memory are limited. With support\nfor vernacular languages and domain-specific tasks, Shakti excels in industries\nsuch as healthcare, finance, and customer service. Benchmark evaluations\ndemonstrate that Shakti performs competitively against larger models while\nmaintaining low latency and on-device efficiency, positioning it as a leading\nsolution for edge AI.", "published": "2024-10-15 06:59:44", "link": "http://arxiv.org/abs/2410.11331v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised\n  Learning", "abstract": "Labeling datasets is a noteworthy challenge in machine learning, both in\nterms of cost and time. This research, however, leverages an efficient answer.\nBy exploring label propagation in semi-supervised learning, we can\nsignificantly reduce the number of labels required compared to traditional\nmethods. We employ a transductive label propagation method based on the\nmanifold assumption for text classification. Our approach utilizes a\ngraph-based method to generate pseudo-labels for unlabeled data for the text\nclassification task, which are then used to train deep neural networks. By\nextending labels based on cosine proximity within a nearest neighbor graph from\nnetwork embeddings, we combine unlabeled data into supervised learning, thereby\nreducing labeling costs. Based on previous successes in other domains, this\nstudy builds and evaluates this approach's effectiveness in sentiment analysis,\npresenting insights into semi-supervised learning.", "published": "2024-10-15 07:25:33", "link": "http://arxiv.org/abs/2410.11355v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "primary_category": "cs.LG"}
{"title": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups", "abstract": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes.", "published": "2024-10-15 08:16:43", "link": "http://arxiv.org/abs/2410.11377v2", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Survey and Evaluation of Converging Architecture in LLMs based on\n  Footsteps of Operations", "abstract": "The advent of the Attention mechanism and Transformer architecture enables\ncontextually natural text generation and compresses the burden of processing\nentire source information into singular vectors. Based on these two main ideas,\nmodel sizes gradually increases to accommodate more precise and comprehensive\ninformation, leading to the current state-of-the-art LLMs being very large,\nwith parameters around 70 billion. As the model sizes are growing, the demand\nfor substantial storage and computational capacity increases. This leads to the\ndevelopment of high-bandwidth memory and accelerators, as well as a variety of\nmodel architectures designed to meet these requirements. We note that LLM\narchitectures have increasingly converged. This paper analyzes how these\nconverged architectures perform in terms of layer configurations, operational\nmechanisms, and model sizes, considering various hyperparameter settings. In\nthis paper, we conduct a concise survey of the history of LLMs by tracing the\nevolution of their operational improvements. Furthermore, we summarize the\nperformance trends of LLMs under various hyperparameter settings using the RTX\n6000, which features the state-of-the-art Ada Lovelace architecture. We\nconclude that even the same model can exhibit different behaviors depending on\nthe hyperparameters or whether it is deployed in server or edge environments.", "published": "2024-10-15 08:19:24", "link": "http://arxiv.org/abs/2410.11381v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under\n  Low-Resource Scenarios", "abstract": "Large language models revolutionize Text2SQL through supervised fine-tuning,\nyet a crucial limitation is overlooked: the complexity of databases leads to an\nincreased context length, consequently resulting in higher GPU memory demands\nfor model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL\ncomprises two supervised fine-tuning models: the schema\\_link model and the\nSQL\\_generation model, with the schema\\_link model serving as the focal point\nfor streamlining the overall process. During the fine-tuning of the\nschema\\_link model, LR-SQL breaks down the complete database into flexible\ncombinations of tables with adjustable quantities, enabling the model to learn\nthe relationships within the entire database from these dispersed slices.\nFurthermore, to enhance the model's ability to perceive the relationships among\nvarious discrete slices during inference, LR-SQL trains the model's\nChain-of-Thought capability for this task. Experimental results demonstrate\nthat LR-SQL can reduce the total GPU memory usage by 40\\% compared to existing\nfine-tuning methods, while only losing 2\\% of table prediction accuracy in\nschema\\_link task. For the overall Text2SQL task, the Execution Accuracy\ndecrease by 0.6\\%.Our project is now available on\nhttps://github.com/hongWin/LR-SQL", "published": "2024-10-15 10:02:55", "link": "http://arxiv.org/abs/2410.11457v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for\n  Embodied AI", "abstract": "Recent advancements in Multi-modal Large Language Models (MLLMs) have opened\nnew avenues for applications in Embodied AI. Building on previous work,\nEgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating\negocentric video understanding capabilities. To bridge the gap between MLLMs\nand low-level control in Embodied AI, we design four key interrelated tasks:\nvideo question-answering, hierarchy planning, visual grounding and reward\nmodeling. To minimize manual annotation costs, we develop an automatic data\ngeneration pipeline based on the Ego4D dataset, leveraging the prior knowledge\nand multimodal capabilities of GPT-4o. Three human annotators then filter the\ngenerated data to ensure diversity and quality, resulting in the VidEgoThink\nbenchmark. We conduct extensive experiments with three types of models:\nAPI-based MLLMs, open-source image-based MLLMs, and open-source video-based\nMLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform\npoorly across all tasks related to egocentric video understanding. These\nfindings suggest that foundation models still require significant advancements\nto be effectively applied to first-person scenarios in Embodied AI. In\nconclusion, VidEgoThink reflects a research trend towards employing MLLMs for\negocentric vision, akin to human capabilities, enabling active observation and\ninteraction in the complex real-world environments.", "published": "2024-10-15 14:08:53", "link": "http://arxiv.org/abs/2410.11623v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual\n  Language Models", "abstract": "Accurately understanding complex visual information is crucial for visual\nlanguage models (VLMs). Enhancing image resolution can improve visual\nperception capabilities, not only reducing hallucinations but also boosting\nperformance in tasks that demand high resolution, such as text-rich or document\nanalysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two\nadvancements in the VisualRWKV model family, specifically designed to process\nhigh-resolution visual inputs. For VisualRWKV-HD, we developed a lossless\ndownsampling method to effectively integrate a high-resolution vision encoder\nwith low-resolution encoders, without extending the input sequence length. For\nthe VisualRWKV-UHD model, we enhanced image representation by dividing the\nimage into four segments, which are then recombined with the original image.\nThis technique allows the model to incorporate both high-resolution and\nlow-resolution features, effectively balancing coarse and fine-grained\ninformation. As a result, the model supports resolutions up to 4096 x 4096\npixels, offering a more detailed and comprehensive visual processing\ncapability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong\nresults on VLM benchmarks but also show marked improvements in performance for\ntext-rich tasks.", "published": "2024-10-15 14:49:19", "link": "http://arxiv.org/abs/2410.11665v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms", "abstract": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.", "published": "2024-10-15 15:14:22", "link": "http://arxiv.org/abs/2410.11677v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are UFOs Driving Innovation? The Illusion of Causality in Large Language\n  Models", "abstract": "Illusions of causality occur when people develop the belief that there is a\ncausal connection between two variables with no supporting evidence. This\ncognitive bias has been proposed to underlie many societal problems including\nsocial prejudice, stereotype formation, misinformation and superstitious\nthinking. In this research we investigate whether large language models develop\nthe illusion of causality in real-world settings. We evaluated and compared\nnews headlines generated by GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro\nto determine whether the models incorrectly framed correlations as causal\nrelationships. In order to also measure sycophantic behavior, which occurs when\na model aligns with a user's beliefs in order to look favorable even if it is\nnot objectively correct, we additionally incorporated the bias into the\nprompts, observing if this manipulation increases the likelihood of the models\nexhibiting the illusion of causality. We found that Claude-3.5-Sonnet is the\nmodel that presents the lowest degree of causal illusion aligned with\nexperiments on Correlation-to-Causation Exaggeration in human-written press\nreleases. On the other hand, our findings suggest that while mimicry sycophancy\nincreases the likelihood of causal illusions in these models, especially in\nGPT-4o-Mini, Claude-3.5-Sonnet remains the most robust against this cognitive\nbias.", "published": "2024-10-15 15:20:49", "link": "http://arxiv.org/abs/2410.11684v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions", "abstract": "Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.", "published": "2024-10-15 15:39:37", "link": "http://arxiv.org/abs/2410.11701v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Latent Action Pretraining from Videos", "abstract": "We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.", "published": "2024-10-15 16:28:09", "link": "http://arxiv.org/abs/2410.11758v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.", "published": "2024-10-15 17:05:25", "link": "http://arxiv.org/abs/2410.11786v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hitchhiker's Guide to Scaling Law Estimation", "abstract": "Scaling laws predict the loss of a target machine learning model by\nextrapolating from easier-to-train models with fewer parameters or smaller\ntraining sets. This provides an efficient way for practitioners and researchers\nalike to compare pretraining decisions involving optimizers, datasets, and\nmodel architectures. Despite the widespread use of scaling laws to model the\ndynamics of language model training, there has been little work on\nunderstanding how to best estimate and interpret them. We collect (and release)\na large-scale dataset containing losses and downstream evaluations for 485\npreviously published pretrained models. We use these to estimate more than 1000\nscaling laws, then derive a set of best practices for estimating scaling laws\nin new model families. We find that fitting scaling laws to intermediate\ncheckpoints of training runs (and not just their final losses) substantially\nimproves accuracy, and that -- all else equal -- estimates of performance are\ngenerally most accurate when derived from other models of similar sizes.\nHowever, because there is a significant degree of variability across model\nseeds, training multiple small models is sometimes more useful than training a\nsingle large one. Moreover, while different model families differ scaling\nbehavior, they are often similar enough that a target model's behavior can be\npredicted from a single model with the same architecture, along with scaling\nparameter estimates derived from other model families.", "published": "2024-10-15 17:59:10", "link": "http://arxiv.org/abs/2410.11840v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Fair Language Model Paradox", "abstract": "Large Language Models (LLMs) are widely deployed in real-world applications,\nyet little is known about their training dynamics at the token level.\nEvaluation typically relies on aggregated training loss, measured at the batch\nlevel, which overlooks subtle per-token biases arising from (i) varying\ntoken-level dynamics and (ii) structural biases introduced by hyperparameters.\nWhile weight decay is commonly used to stabilize training, we reveal that it\nsilently introduces performance biases detectable only at the token level. In\nfact, we empirically show across different dataset sizes, model architectures\nand sizes ranging from 270M to 3B parameters that as weight decay increases,\nlow-frequency tokens are disproportionately depreciated. This is particularly\nconcerning, as these neglected low-frequency tokens represent the vast majority\nof the token distribution in most languages, calling for novel regularization\ntechniques that ensure fairness across all available tokens.", "published": "2024-10-15 18:47:12", "link": "http://arxiv.org/abs/2410.11985v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bias Similarity Across Large Language Models", "abstract": "Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs.", "published": "2024-10-15 19:21:14", "link": "http://arxiv.org/abs/2410.12010v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the\n  Hints from Its Router", "abstract": "Mixture-of-Experts (MoE) architectures face challenges such as high memory\nconsumption and redundancy in experts. Pruning MoE can reduce network weights\nwhile maintaining model performance. Motivated by the recent observation of\nemergent large magnitude features in Large Language Models (LLM) and MoE\nrouting policy, we propose MoE-Pruner, a method that prunes weights with the\nsmallest magnitudes multiplied by the corresponding input activations and\nrouter weights, on each output neuron. Our pruning method is one-shot,\nrequiring no retraining or weight updates. We evaluate our method on\nMixtral-8x7B and Mixtral-8x22B across multiple language benchmarks.\nExperimental results show that our pruning method significantly outperforms\nstate-of-the-art LLM pruning methods. Furthermore, our pruned MoE models can\nbenefit from a pretrained teacher model through expert-wise knowledge\ndistillation, improving performance post-pruning. Experimental results\ndemonstrate that the Mixtral-8x7B model with 50% sparsity maintains 99% of the\nperformance of the original model after the expert-wise knowledge distillation.", "published": "2024-10-15 19:22:27", "link": "http://arxiv.org/abs/2410.12013v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LocoMotion: Learning Motion-Focused Video-Language Representations", "abstract": "This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/", "published": "2024-10-15 19:33:57", "link": "http://arxiv.org/abs/2410.12018v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "De-jargonizing Science for Journalists with GPT-4: A Pilot Study", "abstract": "This study offers an initial evaluation of a human-in-the-loop system\nleveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented\nGeneration (RAG) to identify and define jargon terms in scientific abstracts,\nbased on readers' self-reported knowledge. The system achieves fairly high\nrecall in identifying jargon and preserves relative differences in readers'\njargon identification, suggesting personalization as a feasible use-case for\nLLMs to support sense-making of complex information. Surprisingly, using only\nabstracts for context to generate definitions yields slightly more accurate and\nhigher quality definitions than using RAG-based context from the fulltext of an\narticle. The findings highlight the potential of generative AI for assisting\nscience reporters, and can inform future work on developing tools to simplify\ndense documents.", "published": "2024-10-15 21:10:01", "link": "http://arxiv.org/abs/2410.12069v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "H.4; H.5"], "primary_category": "cs.CL"}
{"title": "Improving Instruction-Following in Language Models through Activation\n  Steering", "abstract": "The ability to follow instructions is crucial for numerous real-world\napplications of language models. In pursuit of deeper insights and more\npowerful capabilities, we derive instruction-specific vector representations\nfrom language models and use them to steer models accordingly. These vectors\nare computed as the difference in activations between inputs with and without\ninstructions, enabling a modular approach to activation steering. We\ndemonstrate how this method can enhance model adherence to constraints such as\noutput format, length, and word inclusion, providing inference-time control\nover instruction following. Our experiments across four models demonstrate how\nwe can use the activation vectors to guide models to follow constraints even\nwithout explicit instructions and to enhance performance when instructions are\npresent. Additionally, we explore the compositionality of activation steering,\nsuccessfully applying multiple instructions simultaneously. Finally, we\ndemonstrate that steering vectors computed on instruction-tuned models can\ntransfer to improve base models. Our findings demonstrate that activation\nsteering offers a practical and scalable approach for fine-grained control in\nlanguage generation.", "published": "2024-10-15 08:38:20", "link": "http://arxiv.org/abs/2410.12877v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards More Effective Table-to-Text Generation: Assessing In-Context\n  Learning and Self-Evaluation with Open-Source Models", "abstract": "Table processing, a key task in natural language processing, has\nsignificantly benefited from recent advancements in language models (LMs).\nHowever, the capabilities of LMs in table-to-text generation, which transforms\nstructured data into coherent narrative text, require an in-depth\ninvestigation, especially with current open-source models. This study explores\nthe effectiveness of various in-context learning strategies in LMs across\nbenchmark datasets, focusing on the impact of providing examples to the model.\nMore importantly, we examine a real-world use case, offering valuable insights\ninto practical applications. To complement traditional evaluation metrics, we\nemploy a large language model (LLM) self-evaluation approach using\nchain-of-thought reasoning and assess its correlation with human-aligned\nmetrics like BERTScore. Our findings highlight the significant impact of\nexamples in improving table-to-text generation and suggest that, while LLM\nself-evaluation has potential, its current alignment with human judgment could\nbe enhanced. This points to the need for more reliable evaluation methods.", "published": "2024-10-15 09:19:42", "link": "http://arxiv.org/abs/2410.12878v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\n  Sensitivity in Large Language Models", "abstract": "As LLMs are increasingly deployed in global applications, the importance of\ncultural sensitivity becomes paramount, ensuring that users from diverse\nbackgrounds feel respected and understood. Cultural harm can arise when these\nmodels fail to align with specific cultural norms, resulting in\nmisrepresentations or violations of cultural values. This work addresses the\nchallenges of ensuring cultural sensitivity in LLMs, especially in\nsmall-parameter models that often lack the extensive training data needed to\ncapture global cultural nuances. We present two key contributions: (1) A\ncultural harm test dataset, created to assess model outputs across different\ncultural contexts through scenarios that expose potential cultural\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\nrestoring cultural sensitivity through fine-tuning based on feedback from\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\nLLMs, ensuring their ethical and safe deployment across different cultural\nlandscapes. Our results show that integrating culturally aligned feedback leads\nto a marked improvement in model behavior, significantly reducing the\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\nthis work paves the way for more inclusive and respectful AI systems, fostering\na future where LLMs can safely and ethically navigate the complexities of\ndiverse cultural landscapes.", "published": "2024-10-15 18:13:10", "link": "http://arxiv.org/abs/2410.12880v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP\n  Technical Specifications", "abstract": "This paper proposes a Question-Answering (QA) system for the telecom domain\nusing 3rd Generation Partnership Project (3GPP) technical documents. Alongside,\na hybrid dataset, Telco-DPR, which consists of a curated 3GPP corpus in a\nhybrid format, combining text and tables, is presented. Additionally, the\ndataset includes a set of synthetic question/answer pairs designed to evaluate\nthe retrieval performance of QA systems on this type of data. The retrieval\nmodels, including the sparse model, Best Matching 25 (BM25), as well as dense\nmodels, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval\n(DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank\n(MRR). The results show that DHR, a retriever model utilising hierarchical\npassage selection through fine-tuning at both the document and passage levels,\noutperforms traditional methods in retrieving relevant technical information,\nachieving a Top-10 accuracy of 86.2%. Additionally, the Retriever-Augmented\nGeneration (RAG) technique, used in the proposed QA system, is evaluated to\ndemonstrate the benefits of using the hybrid dataset and the DHR. The proposed\nQA system, using the developed RAG model and the Generative Pretrained\nTransformer (GPT)-4, achieves a 14% improvement in answer accuracy, when\ncompared to a previous benchmark on the same dataset.", "published": "2024-10-15 16:37:18", "link": "http://arxiv.org/abs/2410.19790v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.NI"], "primary_category": "cs.IR"}
{"title": "RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models\n  for Question Answering", "abstract": "Retrieval-augmented generation (RAG) has shown promising potential in\nknowledge intensive question answering (QA). However, existing approaches only\nconsider the query itself, neither specifying the retrieval preferences for the\nretrievers nor informing the generators of how to refer to the retrieved\ndocuments for the answers, which poses a significant challenge to the QA\nperformance. To address these issues, we propose Rule-guided\nRetrieval-Augmented Generation with LMs, which explicitly introduces rules for\nin-context learning (RuleRAG-ICL) to guide retrievers to recall related\ndocuments in the directions of rules and uniformly guide generators to reason\nattributed by the same rules. Moreover, most existing RAG datasets were\nconstructed without considering rules and Knowledge Graphs (KGs) are recognized\nas providing high-quality rules. Therefore, we construct five rule-aware RAG\nbenchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval\nand reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL\nimproves the retrieval quality of +89.2% in Recall@10 and answer accuracy of\n+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,\nexperiments on four existing RAG datasets show RuleRAG is also effective by\noffering rules in RuleQA to them, further proving the generalization of rule\nguidance in RuleRAG.", "published": "2024-10-15 14:51:45", "link": "http://arxiv.org/abs/2410.22353v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation", "abstract": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs DeCo, which adaptively selects the\nappropriate preceding layers and proportionally integrates knowledge into the\nfinal layer to adjust the output logits. Note that DeCo is model agnostic and\ncan be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.", "published": "2024-10-15 16:57:44", "link": "http://arxiv.org/abs/2410.11779v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Search Engines in an AI Era: The False Promise of Factual and Verifiable\n  Source-Cited Responses", "abstract": "Large Language Model (LLM)-based applications are graduating from research\nprototypes to products serving millions of users, influencing how people write\nand consume information. A prominent example is the appearance of Answer\nEngines: LLM-based generative search engines supplanting traditional search\nengines. Answer engines not only retrieve relevant sources to a user query but\nsynthesize answer summaries that cite the sources. To understand these systems'\nlimitations, we first conducted a study with 21 participants, evaluating\ninteractions with answer vs. traditional search engines and identifying 16\nanswer engine limitations. From these insights, we propose 16 answer engine\ndesign recommendations, linked to 8 metrics. An automated evaluation\nimplementing our metrics on three popular engines (You.com, Perplexity.ai,\nBingChat) quantifies common limitations (e.g., frequent hallucination,\ninaccurate citation) and unique features (e.g., variation in answer\nconfidence), with results mirroring user study insights. We release our Answer\nEngine Evaluation benchmark (AEE) to facilitate transparent evaluation of\nLLM-based applications.", "published": "2024-10-15 00:50:31", "link": "http://arxiv.org/abs/2410.22349v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models", "abstract": "Spatial audio is a crucial component in creating immersive experiences.\nTraditional simulation-based approaches to generate spatial audio rely on\nexpertise, have limited scalability, and assume independence between semantic\nand spatial information. To address these issues, we explore end-to-end spatial\naudio generation. We introduce and formulate a new task of generating\nfirst-order Ambisonics (FOA) given a sound category and sound source spatial\nlocation. We propose Diff-SAGe, an end-to-end, flow-based diffusion-transformer\nmodel for this task. Diff-SAGe utilizes a complex spectrogram representation\nfor FOA, preserving the phase information crucial for accurate spatial cues.\nAdditionally, a multi-conditional encoder integrates the input conditions into\na unified representation, guiding the generation of FOA waveforms from noise.\nThrough extensive evaluations on two datasets, we demonstrate that our method\nconsistently outperforms traditional simulation-based baselines across both\nobjective and subjective metrics.", "published": "2024-10-15 05:37:22", "link": "http://arxiv.org/abs/2410.11299v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The importance of spatial and spectral information in multiple speaker\n  tracking", "abstract": "Multi-speaker localization and tracking using microphone array recording is\nof importance in a wide range of applications. One of the challenges with\nmulti-speaker tracking is to associate direction estimates with the correct\nspeaker. Most existing association approaches rely on spatial or spectral\ninformation alone, leading to performance degradation when one of these\ninformation channels is partially known or missing. This paper studies a joint\nprobability data association (JPDA)-based method that facilitates association\nbased on joint spatial-spectral information. This is achieved by integrating\nspeaker time-frequency (TF) masks, estimated based on spectral information, in\nthe association probabilities calculation. An experimental study that tested\nthe proposed method on recordings from the LOCATA challenge demonstrates the\nenhanced performance obtained by using joint spatial-spectral information in\nthe association.", "published": "2024-10-15 10:00:06", "link": "http://arxiv.org/abs/2410.11453v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDMD: AI-Powered Digital Drug Music Detector", "abstract": "We present the first version of DDMD (Digital Drug Music Detector), a binary\nclassifier that distinguishes digital drug music from normal music. In the\nliterature, digital drug music is primarily explored regarding its\npsychological, neurological, or social impact. However, despite numerous\nstudies on using machine learning in Music Information Retrieval (MIR),\nincluding music genre classification, digital drug music has not been\nconsidered in this field. In this study, we initially collected a dataset of\n3,176 audio files divided into two classes (1,676 digital drugs and 1,500\nnon-digital drugs). We extracted machine learning features, including MFCCs,\nchroma, spectral contrast, and frequency analysis metrics (mean and standard\ndeviation of detected frequencies). Using a Random Forest classifier, we\nachieved an accuracy of 93%. Finally, we developed a web application to deploy\nthe model, enabling end users to detect digital drug music.", "published": "2024-10-15 07:57:11", "link": "http://arxiv.org/abs/2410.23293v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DARNet: Dual Attention Refinement Network with Spatiotemporal\n  Construction for Auditory Attention Detection", "abstract": "At a cocktail party, humans exhibit an impressive ability to direct their\nattention. The auditory attention detection (AAD) approach seeks to identify\nthe attended speaker by analyzing brain signals, such as EEG signals. However,\ncurrent AAD algorithms overlook the spatial distribution information within EEG\nsignals and lack the ability to capture long-range latent dependencies,\nlimiting the model's ability to decode brain activity. To address these issues,\nthis paper proposes a dual attention refinement network with spatiotemporal\nconstruction for AAD, named DARNet, which consists of the spatiotemporal\nconstruction module, dual attention refinement module, and feature fusion \\&\nclassifier module. Specifically, the spatiotemporal construction module aims to\nconstruct more expressive spatiotemporal feature representations, by capturing\nthe spatial distribution characteristics of EEG signals. The dual attention\nrefinement module aims to extract different levels of temporal patterns in EEG\nsignals and enhance the model's ability to capture long-range latent\ndependencies. The feature fusion \\& classifier module aims to aggregate\ntemporal patterns and dependencies from different levels and obtain the final\nclassification results. The experimental results indicate that compared to the\nstate-of-the-art models, DARNet achieves an average classification accuracy\nimprovement of 5.9\\% for 0.1s, 4.6\\% for 1s, and 3.9\\% for 2s on the DTU\ndataset. While maintaining excellent classification performance, DARNet\nsignificantly reduces the number of required parameters. Compared to the\nstate-of-the-art models, DARNet reduces the parameter count by 91\\%. Code is\navailable at: https://github.com/fchest/DARNet.git.", "published": "2024-10-15 01:51:29", "link": "http://arxiv.org/abs/2410.11181v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction", "abstract": "In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.", "published": "2024-10-15 11:48:31", "link": "http://arxiv.org/abs/2410.11522v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmotionCaps: Enhancing Audio Captioning Through Emotion-Augmented Data\n  Generation", "abstract": "Recent progress in audio-language modeling, such as automated audio\ncaptioning, has benefited from training on synthetic data generated with the\naid of large-language models. However, such approaches for environmental sound\ncaptioning have primarily focused on audio event tags and have not explored\nleveraging emotional information that may be present in recordings. In this\nwork, we explore the benefit of generating emotion-augmented synthetic audio\ncaption data by instructing ChatGPT with additional acoustic information in the\nform of estimated soundscape emotion. To do so, we introduce EmotionCaps, an\naudio captioning dataset comprised of approximately 120,000 audio clips with\npaired synthetic descriptions enriched with soundscape emotion recognition\n(SER) information. We hypothesize that this additional information will result\nin higher-quality captions that match the emotional tone of the audio\nrecording, which will, in turn, improve the performance of captioning models\ntrained with this data. We test this hypothesis through both objective and\nsubjective evaluation, comparing models trained with the EmotionCaps dataset to\nmultiple baseline models. Our findings challenge current approaches to\ncaptioning and suggest new directions for developing and assessing captioning\nmodels.", "published": "2024-10-15 19:57:37", "link": "http://arxiv.org/abs/2410.12028v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Learning to rumble: Automated elephant call classification, detection\n  and endpointing using deep architectures", "abstract": "We consider the problem of detecting, isolating and classifying elephant\ncalls in continuously recorded audio. Such automatic call characterisation can\nassist conservation efforts and inform environmental management strategies. In\ncontrast to previous work in which call detection was performed at a segment\nlevel, we perform call detection at a frame level which implicitly also allows\ncall endpointing, the isolation of a call in a longer recording. For\nexperimentation, we employ two annotated datasets, one containing Asian and the\nother African elephant vocalisations. We evaluate several shallow and deep\nclassifier models, and show that the current best performance can be improved\nby using an audio spectrogram transformer (AST), a neural architecture which\nhas not been used for this purpose before, and which we have configured in a\nnovel sequence-to-sequence manner. We also show that using transfer learning by\npre-training leads to further improvements both in terms of computational\ncomplexity and performance. Finally, we consider sub-call classification using\nan accepted taxonomy of call types, a task which has not previously been\nconsidered. We show that also in this case the transformer architectures\nprovide the best performance. Our best classifiers achieve an average precision\n(AP) of 0.962 for framewise binary call classification, and an area under the\nreceiver operating characteristic (AUC) of 0.957 and 0.979 for call\nclassification with 5 classes and sub-call classification with 7 classes\nrespectively. All of these represent either new benchmarks (sub-call\nclassifications) or improvements on previously best systems. We conclude that a\nfully-automated elephant call detection and subcall classification system is\nwithin reach. Such a system would provide valuable information on the behaviour\nand state of elephant herds for the purposes of conservation and management.", "published": "2024-10-15 21:56:40", "link": "http://arxiv.org/abs/2410.12082v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Quality-Aware End-to-End Audio-Visual Neural Speaker Diarization", "abstract": "In this paper, we propose a quality-aware end-to-end audio-visual neural\nspeaker diarization framework, which comprises three key techniques. First, our\naudio-visual model takes both audio and visual features as inputs, utilizing a\nseries of binary classification output layers to simultaneously identify the\nactivities of all speakers. This end-to-end framework is meticulously designed\nto effectively handle situations of overlapping speech, providing accurate\ndiscrimination between speech and non-speech segments through the utilization\nof multi-modal information. Next, we employ a quality-aware audio-visual fusion\nstructure to address signal quality issues for both audio degradations, such as\nnoise, reverberation and other distortions, and video degradations, such as\nocclusions, off-screen speakers, or unreliable detection. Finally, a cross\nattention mechanism applied to multi-speaker embedding empowers the network to\nhandle scenarios with varying numbers of speakers. Our experimental results,\nobtained from various data sets, demonstrate the robustness of our proposed\ntechniques in diverse acoustic environments. Even in scenarios with severely\ndegraded video quality, our system attains performance levels comparable to the\nbest available audio-visual systems.", "published": "2024-10-15 05:48:30", "link": "http://arxiv.org/abs/2410.22350v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities", "abstract": "GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.", "published": "2024-10-15 02:10:45", "link": "http://arxiv.org/abs/2410.11190v3", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single-word Auditory Attention Decoding Using Deep Learning Model", "abstract": "Identifying auditory attention by comparing auditory stimuli and\ncorresponding brain responses, is known as auditory attention decoding (AAD).\nThe majority of AAD algorithms utilize the so-called envelope entrainment\nmechanism, whereby auditory attention is identified by how the envelope of the\nauditory stream drives variation in the electroencephalography (EEG) signal.\nHowever, neural processing can also be decoded based on endogenous cognitive\nresponses, in this case, neural responses evoked by attention to specific words\nin a speech stream. This approach is largely unexplored in the field of AAD but\nleads to a single-word auditory attention decoding problem in which an epoch of\nan EEG signal timed to a specific word is labeled as attended or unattended.\nThis paper presents a deep learning approach, based on EEGNet, to address this\nchallenge. We conducted a subject-independent evaluation on an event-based AAD\ndataset with three different paradigms: word category oddball, word category\nwith competing speakers, and competing speech streams with targets. The results\ndemonstrate that the adapted model is capable of exploiting cognitive-related\nspatiotemporal EEG features and achieving at least 58% accuracy on the most\nrealistic competing paradigm for the unseen subjects. To our knowledge, this is\nthe first study dealing with this problem.", "published": "2024-10-15 21:57:19", "link": "http://arxiv.org/abs/2410.19793v1", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "eess.SP"}
