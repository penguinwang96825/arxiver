{"title": "Supertagging: Introduction, learning, and application", "abstract": "Supertagging is an approach originally developed by Bangalore and Joshi\n(1999) to improve the parsing efficiency. In the beginning, the scholars used\nsmall training datasets and somewhat na\\\"ive smoothing techniques to learn the\nprobability distributions of supertags. Since its inception, the applicability\nof Supertags has been explored for TAG (tree-adjoining grammar) formalism as\nwell as other related yet, different formalisms such as CCG. This article will\ntry to summarize the various chapters, relevant to statistical parsing, from\nthe most recent edited book volume (Bangalore and Joshi, 2010). The chapters\nwere selected so as to blend the learning of supertags, its integration into\nfull-scale parsing, and in semantic parsing.", "published": "2014-12-19 09:53:57", "link": "http://arxiv.org/abs/1412.6264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N-gram-Based Low-Dimensional Representation for Document Classification", "abstract": "The bag-of-words (BOW) model is the common approach for classifying\ndocuments, where words are used as feature for training a classifier. This\ngenerally involves a huge number of features. Some techniques, such as Latent\nSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been\ndesigned to summarize documents in a lower dimension with the least semantic\ninformation loss. Some semantic information is nevertheless always lost, since\nonly words are considered. Instead, we aim at using information coming from\nn-grams to overcome this limitation, while remaining in a low-dimension space.\nMany approaches, such as the Skip-gram model, provide good word vector\nrepresentations very quickly. We propose to average these representations to\nobtain representations of n-grams. All n-grams are thus embedded in a same\nsemantic space. A K-means clustering can then group them into semantic\nconcepts. The number of features is therefore dramatically reduced and\ndocuments can be represented as bag of semantic concepts. We show that this\nmodel outperforms LSA and LDA on a sentiment classification task, and yields\nsimilar results than a traditional BOW-model with far less features.", "published": "2014-12-19 10:29:33", "link": "http://arxiv.org/abs/1412.6277v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Monolingual Data for Crosslingual Compositional Word\n  Representations", "abstract": "In this work, we present a novel neural network based architecture for\ninducing compositional crosslingual word representations. Unlike previously\nproposed methods, our method fulfills the following three criteria; it\nconstrains the word-level representations to be compositional, it is capable of\nleveraging both bilingual and monolingual data, and it is scalable to large\nvocabularies and large quantities of data. The key component of our approach is\nwhat we refer to as a monolingual inclusion criterion, that exploits the\nobservation that phrases are more closely semantically related to their\nsub-phrases than to other randomly sampled phrases. We evaluate our method on a\nwell-established crosslingual document classification task and achieve results\nthat are either comparable, or greatly improve upon previous state-of-the-art\nmethods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for\nthe English to German and German to English sub-tasks respectively. The former\nadvances the state of the art by 0.9% points of accuracy, the latter is an\nabsolute improvement upon the previous state of the art by 7.7% points of\naccuracy and an improvement of 33.0% in error reduction.", "published": "2014-12-19 13:23:35", "link": "http://arxiv.org/abs/1412.6334v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Word Similarity with Neural Machine Translation", "abstract": "Neural language models learn word representations, or embeddings, that\ncapture rich linguistic and conceptual information. Here we investigate the\nembeddings learned by neural machine translation models, a recently-developed\nclass of neural language model. We show that embeddings from translation models\noutperform those learned by monolingual models at tasks that require knowledge\nof both conceptual similarity and lexical-syntactic role. We further show that\nthese effects hold when translating from both English to French and English to\nGerman, and argue that the desirable properties of translation embeddings\nshould emerge largely independently of the source and target languages.\nFinally, we apply a new method for training neural translation models with very\nlarge vocabularies, and show that this vocabulary expansion algorithm results\nin minimal degradation of embedding quality. Our embedding spaces can be\nqueried in an online demo and downloaded from our web page. Overall, our\nanalyses indicate that translation-based embeddings should be used in\napplications that require concepts to be organised according to similarity\nand/or lexical function, while monolingual embeddings are better suited to\nmodelling (nonspecific) inter-word relatedness.", "published": "2014-12-19 17:22:03", "link": "http://arxiv.org/abs/1412.6448v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Authors Detection: A Quantitative Analysis of Dream of the Red\n  Chamber", "abstract": "Inspired by the authorship controversy of Dream of the Red Chamber and the\napplication of machine learning in the study of literary stylometry, we develop\na rigorous new method for the mathematical analysis of authorship by testing\nfor a so-called chrono-divide in writing styles. Our method incorporates some\nof the latest advances in the study of authorship attribution, particularly\ntechniques from support vector machines. By introducing the notion of relative\nfrequency as a feature ranking metric our method proves to be highly effective\nand robust.\n  Applying our method to the Cheng-Gao version of Dream of the Red Chamber has\nled to convincing if not irrefutable evidence that the first $80$ chapters and\nthe last $40$ chapters of the book were written by two different authors.\nFurthermore, our analysis has unexpectedly provided strong support to the\nhypothesis that Chapter 67 was not the work of Cao Xueqin either.\n  We have also tested our method to the other three Great Classical Novels in\nChinese. As expected no chrono-divides have been found. This provides further\nevidence of the robustness of our method.", "published": "2014-12-19 04:31:11", "link": "http://arxiv.org/abs/1412.6211v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations", "abstract": "In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage.", "published": "2014-12-19 16:30:33", "link": "http://arxiv.org/abs/1412.6418v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
