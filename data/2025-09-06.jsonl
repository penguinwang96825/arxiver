{"title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "abstract": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.", "published": "2025-09-06 23:48:46", "link": "http://arxiv.org/abs/2509.05867v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "abstract": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work.", "published": "2025-09-06 23:36:30", "link": "http://arxiv.org/abs/2509.05863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "abstract": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation.", "published": "2025-09-06 15:07:59", "link": "http://arxiv.org/abs/2509.05741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "abstract": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.", "published": "2025-09-06 14:25:09", "link": "http://arxiv.org/abs/2509.05729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "abstract": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP.", "published": "2025-09-06 13:55:34", "link": "http://arxiv.org/abs/2509.05719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "abstract": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.", "published": "2025-09-06 13:38:03", "link": "http://arxiv.org/abs/2509.05716v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "abstract": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content.", "published": "2025-09-06 11:44:26", "link": "http://arxiv.org/abs/2509.05691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "abstract": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "published": "2025-09-06 10:12:52", "link": "http://arxiv.org/abs/2509.05668v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "abstract": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse.", "published": "2025-09-06 09:34:47", "link": "http://arxiv.org/abs/2509.05660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "abstract": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "published": "2025-09-06 09:26:39", "link": "http://arxiv.org/abs/2509.05657v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "abstract": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.", "published": "2025-09-06 07:41:47", "link": "http://arxiv.org/abs/2509.05635v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Contribution of Lexical Features to Speech Emotion Recognition", "abstract": "Although paralinguistic cues are often considered the primary drivers of\nspeech emotion recognition (SER), we investigate the role of lexical content\nextracted from speech and show that it can achieve competitive and in some\ncases higher performance compared to acoustic models. On the MELD dataset, our\nlexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to\n49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,\nwe analyze different self-supervised (SSL) speech and text representations,\nconduct a layer-wise study of transformer-based encoders, and evaluate the\neffect of audio denoising.", "published": "2025-09-06 07:40:27", "link": "http://arxiv.org/abs/2509.05634v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "abstract": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.", "published": "2025-09-06 06:28:28", "link": "http://arxiv.org/abs/2509.05617v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "abstract": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "published": "2025-09-06 05:58:52", "link": "http://arxiv.org/abs/2509.05609v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints", "abstract": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.", "published": "2025-09-06 05:57:20", "link": "http://arxiv.org/abs/2509.05608v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "abstract": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "published": "2025-09-06 05:46:38", "link": "http://arxiv.org/abs/2509.05607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "abstract": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "published": "2025-09-06 05:38:47", "link": "http://arxiv.org/abs/2509.05605v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "abstract": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "published": "2025-09-06 05:33:17", "link": "http://arxiv.org/abs/2509.05602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ad hoc conventions generalize to new referents", "abstract": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "published": "2025-09-06 02:36:12", "link": "http://arxiv.org/abs/2509.05566v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "abstract": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "published": "2025-09-06 00:44:34", "link": "http://arxiv.org/abs/2509.05553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Golden Ratio Growth and Phase Transitions in Chromatic Counts of Circular Chord Graphs", "abstract": "We study generalized circular chord graphs $\\mathcal C^{(k)}_n$, formed from\na cycle $C_n$ by adding fixed-offset chords of length $k$ and, for even $n$,\ndiameters. Using transfer matrix methods, we derive exact formulas for\n3-colorings when $k=3$: for odd $n$, we obtain \\[ P(\\mathcal{C}_n^{(3)},3) =\nL_n + 2\\cos\\left(\\frac{2\\pi n}{3}\\right) + 2s_n + 2 \\] where $L_n$ is the Lucas\nsequence and $(s_n)$ satisfies $s_{n+3} = -s_{n+2} - s_n$, yielding\ngolden-ratio asymptotic growth $\\varphi^n + O(\\rho^n)$ along odd indices. For\neven $n$, we construct a paired-window transfer matrix that exactly enumerates\n$P(\\mathcal{C}_{2m}^{(3)},3)$ while capturing diameter constraints. The\nchromatic counts exhibit pronounced modular patterns across residue classes\nwithout universal vanishing rules (see OEIS A383733). We provide efficient\nalgorithms for exact enumeration and demonstrate applications to cyclic\nscheduling problems where these results serve as feasibility engines for\nairline gate assignment, wireless sensor networks, and multiprocessor task\ncoordination.", "published": "2025-09-06 22:00:26", "link": "http://arxiv.org/abs/2509.05845v1", "categories": ["math.CO", "cs.DM", "68R10, 05C85", "G.2.1; G.2.2; G.2.3; I.1.2"], "primary_category": "math.CO"}
{"title": "Diagonal Frobenius Number via Gomory's Relaxation and Discrepancy", "abstract": "For a matrix $A \\in Z^{k \\times n}$ of rank $k$, the diagonal Frobenius\nnumber $F_{\\text{diag}}(A)$ is defined as the minimum $t \\in Z_{\\geq 1}$, such\nthat, for any $b \\in \\text{span}_{Z}(A)$, the condition \\begin{equation*}\n  \\exists x \\in R_{\\geq 0}^n,\\, x \\geq t \\cdot 1 \\colon \\quad b = A x\n\\end{equation*} implies that \\begin{equation*}\n  \\exists z \\in Z_{\\geq 0}^n \\colon\\quad b = A z. \\end{equation*}\n  In this work, we show that \\begin{equation*}\n  F_{\\text{diag}}(A) = \\Delta + O(\\log k), \\end{equation*} where $\\Delta$\ndenotes the maximum absolute value of $k \\times k$ sub-determinants of $A$.\n  From the computational complexity perspective, we show that the integer\nvector $z$ can be found by a polynomial-time algorithm for some weaker values\nof $t$ in the described condition. For example, we can choose $t = O( \\Delta\n\\cdot \\log k)$ or $t = \\Delta + O(\\sqrt{k} \\cdot \\log k)$. Additionally, in the\nassumption that a $2^k$-time preprocessing is allowed or a base $J$ with\n$|{\\det A_{J}}| = \\Delta$ is given, we can choose $t = \\Delta + O(\\log k)$.\n  Finally, we define a more general notion of the diagonal Frobenius number for\nslacks $F_{\\text{slack}}(A)$, which is a generalization of $F_{\\text{diag}}(A)$\nfor canonical-form systems, like $A x \\leq b$. All the proofs are mainly done\nwith respect to $F_{\\text{slack}}(A)$. The proof technique uses some properties\nof the Gomory's corner polyhedron relaxation and tools from discrepancy theory.", "published": "2025-09-06 07:25:37", "link": "http://arxiv.org/abs/2509.05629v1", "categories": ["cs.DM", "cs.CC", "cs.CG", "math.NT"], "primary_category": "cs.DM"}
{"title": "Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search", "abstract": "Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. Although several paradigms (seed selection, incremental insertion,\nneighborhood propagation, neighborhood diversification, and divide-and-conquer)\nhave been employed to design in-memory graph-based vector search algorithms, a\nsystematic comparison of the key algorithmic advances is still missing. We\nconduct an exhaustive experimental evaluation of twelve state-of-the-art\nmethods on seven real data collections, with sizes up to 1 billion vectors. We\nshare key insights about the strengths and limitations of these methods; e.g.,\nthe best approaches are typically based on incremental insertion and\nneighborhood diversification, and the choice of the base graph can hurt\nscalability. Finally, we discuss open research directions, such as the\nimportance of devising more sophisticated data adaptive seed selection and\ndiversification strategies.", "published": "2025-09-06 15:43:36", "link": "http://arxiv.org/abs/2509.05750v1", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.PF"], "primary_category": "cs.IR"}
{"title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis", "abstract": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.", "published": "2025-09-06 12:36:59", "link": "http://arxiv.org/abs/2509.05703v1", "categories": ["cs.CV", "cs.AI", "cs.IR"], "primary_category": "cs.CV"}
{"title": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce", "abstract": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems.", "published": "2025-09-06 02:54:13", "link": "http://arxiv.org/abs/2509.05570v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models", "abstract": "Complementary recommendations play a crucial role in e-commerce by enhancing\nuser experience through suggestions of compatible items. Accurate\nclassification of complementary item relationships requires reliable labels,\nbut their creation presents a dilemma. Behavior-based labels are widely used\nbecause they can be easily generated from interaction logs; however, they often\ncontain significant noise and lack reliability. While function-based labels\n(FBLs) provide high-quality definitions of complementary relationships by\ncarefully articulating them based on item functions, their reliance on costly\nmanual annotation severely limits a model's ability to generalize to diverse\nitems. To resolve this trade-off, we propose Knowledge-Augmented Relation\nLearning (KARL), a framework that strategically fuses active learning with\nlarge language models (LLMs). KARL efficiently expands a high-quality FBL\ndataset at a low cost by selectively sampling data points that the classifier\nfinds the most difficult and uses the label extension of the LLM. Our\nexperiments showed that in out-of-distribution (OOD) settings, an unexplored\nitem feature space, KARL improved the baseline accuracy by up to 37%. In\ncontrast, in in-distribution (ID) settings, the learned item feature space, the\nimprovement was less than 0.5%, with prolonged learning could degrade accuracy.\nThese contrasting results are due to the data diversity driven by KARL's\nknowledge expansion, suggesting the need for a dynamic sampling strategy that\nadjusts diversity based on the prediction context (ID or OOD).", "published": "2025-09-06 02:20:20", "link": "http://arxiv.org/abs/2509.05564v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "abstract": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness.", "published": "2025-09-06 01:07:08", "link": "http://arxiv.org/abs/2509.05554v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Multiport Network Modeling and Optimization for Reconfigurable Pinching-Antenna Systems", "abstract": "A reconfigurable pinching-antenna system (PASS) is presented, endowing\npinching antennas (PAs) with both amplitude- and phase-controllable radiation\nbeyond conventional implementations. To characterize this feature, a general\nand physically consistent model is established for PASS via multiport network\ntheory. Within this model, the fundamental constraint of ideal\nreconfigurability of PAs is identified, allowing the full control of signal\namplitudes and phases. A practical directional-coupler (DC)-based PA model is\nthen proposed, enabling both amplitude-only control and amplitude-constrained\nphase control. Beamforming optimization is investigated for both ideal and\npractical cases: an optimal solution is obtained for ideal PAs, whereas a\nhigh-quality iterative algorithm is developed for DC-based PAs. Numerical\nresults suggest that in single-user scenarios: (i) with optimized PA positions,\nperformance gains arise primarily from amplitude reconfigurability and DC-based\nPAs approach ideal performance, and (ii) with fixed PA positions, both\namplitude and phase reconfigurability are critical and DC-based PAs incur\nnon-negligible loss.", "published": "2025-09-06 06:14:30", "link": "http://arxiv.org/abs/2509.05612v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sparse Sensor Allocation for Inverse Problems of Detecting Sparse Leaking Emission Sources", "abstract": "This paper investigates the sparse optimal allocation of sensors for\ndetecting sparse leaking emission sources. Because of the non-negativity of\nemission rates, uncertainty associated with parameters in the forward model,\nand sparsity of leaking emission sources, the classical linear Gaussian\nBayesian inversion setup is limited and no closed-form solutions are available.\nBy incorporating the non-negativity constraints on emission rates, relaxing the\nGaussian distributional assumption, and considering the parameter uncertainties\nassociated with the forward model, this paper provides comprehensive\ninvestigations, technical details, in-depth discussions and implementation of\nthe optimal sensor allocation problem leveraging a bilevel optimization\nframework. The upper-level problem determines the optimal sensor locations by\nminimizing the Integrated Mean Squared Error (IMSE) of the estimated emission\nrates over uncertain wind conditions, while the lower-level problem solves an\ninverse problem that estimates the emission rates. Two algorithms, including\nthe repeated Sample Average Approximation (rSAA) and the Stochastic Gradient\nDescent based bilevel approximation (SBA), are thoroughly investigated. It is\nshown that the proposed approach can further reduce the IMSE of the estimated\nemission rates starting from various initial sensor deployment generated by\nexisting approaches. Convergence analysis is performed to obtain the\nperformance guarantee, and numerical investigations show that the proposed\napproach can allocate sensors according to the parameters and output of the\nforward model. Computationally efficient code with GPU acceleration is\navailable on GitHub so that the approach readily applicable.", "published": "2025-09-06 01:53:14", "link": "http://arxiv.org/abs/2509.05559v1", "categories": ["stat.AP", "cs.IT", "math.IT", "stat.ME"], "primary_category": "stat.AP"}
{"title": "Hierarchical Decision-Making in Population Games", "abstract": "This paper introduces a hierarchical framework for population games, where\nindividuals delegate decision-making to proxies that act within their own\nstrategic interests. This framework extends classical population games, where\nindividuals are assumed to make decisions directly, to capture various\nreal-world scenarios involving multiple decision layers. We establish\nequilibrium properties and provide convergence results for the proposed\nhierarchical structure. Additionally, based on these results, we develop a\nsystematic approach to analyze population games with general convex\nconstraints, without requiring individuals to have full knowledge of the\nconstraints as in existing methods. We present a navigation application with\ncapacity constraints as a case study.", "published": "2025-09-06 18:55:03", "link": "http://arxiv.org/abs/2509.05808v1", "categories": ["eess.SY", "cs.MA", "cs.SY"], "primary_category": "eess.SY"}
{"title": "InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios", "abstract": "We address the problem of accurate capture of interactive behaviors between\ntwo people in daily scenarios. Most previous works either only consider one\nperson or solely focus on conversational gestures of two people, assuming the\nbody orientation and/or position of each actor are constant or barely change\nover each interaction. In contrast, we propose to simultaneously model two\npeople's activities, and target objective-driven, dynamic, and semantically\nconsistent interactions which often span longer duration and cover bigger\nspace. To this end, we capture a new multi-modal dataset dubbed InterAct, which\nis composed of 241 motion sequences where two people perform a realistic and\ncoherent scenario for one minute or longer over a complete interaction. For\neach sequence, two actors are assigned different roles and emotion labels, and\ncollaborate to finish one task or conduct a common interaction activity. The\naudios, body motions, and facial expressions of both persons are captured.\nInterAct contains diverse and complex motions of individuals and interesting\nand relatively long-term interaction patterns barely seen before. We also\ndemonstrate a simple yet effective diffusion-based method that estimates\ninteractive face expressions and body motions of two people from speech inputs.\nOur method regresses the body motions in a hierarchical manner, and we also\npropose a novel fine-tuning mechanism to improve the lip accuracy of facial\nexpressions. To facilitate further research, the data and code is made\navailable at https://hku-cg.github.io/interact/ .", "published": "2025-09-06 15:36:47", "link": "http://arxiv.org/abs/2509.05747v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO", "I.5.4"], "primary_category": "cs.CV"}
{"title": "Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks", "abstract": "Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS)\ndue to partial observability and suboptimal coordination. We propose\nOrchestrator, a novel MAS framework that leverages attention-inspired\nself-emergent coordination and reflective benchmarking to optimize global task\nperformance. Orchestrator introduces a monitoring mechanism to track\nagent-environment dynamics, using active inference benchmarks to optimize\nsystem behavior. By tracking agent-to-agent and agent-to-environment\ninteraction, Orchestrator mitigates the effects of partial observability and\nenables agents to approximate global task solutions more efficiently. We\nevaluate the framework on a series of maze puzzles of increasing complexity,\ndemonstrating its effectiveness in enhancing coordination and performance in\ndynamic, non-linear environments with long-horizon objectives.", "published": "2025-09-06 09:03:36", "link": "http://arxiv.org/abs/2509.05651v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Systematic Evaluation of Multi-modal Approaches to Complex Player Profile Classification", "abstract": "Modern adaptive games require nuanced player understanding, yet most models\nuse simplified 5-10 category taxonomies that fail to capture diversity.\nBehavioral clustering cannot distinguish players with different motivations who\nact similarly. We present a systematic evaluation of multi-modal classification\nat scale, combining behavioral telemetry with semantic context to support 36\nplayer profiles. Using 19,413 gameplay sessions from an AI-controlled\ntext-based RPG, we compared behavioral-only baselines with multi-modal\napproaches that integrate action sequences and semantic descriptions.\nTraditional clustering achieved only 10% accuracy for 36-category\nclassification, limited by semantic conflation where opposite actions produced\nidentical features. Our multi-modal LSTM processing action-text pairs improved\naccuracy to 21%, showing both potential and limits of non-conversational data.\nAnalysis by behavioral complexity revealed that non-neutral profiles reached\n42% accuracy (15x above random), while neutral profiles dropped to 25% (9x\nabove random). Identical actions such as \"help the merchant\" cannot reveal\nwhether a player is neutral or strategically waiting. Without access to\nreasoning, even multi-modal models struggle, though above-baseline results\nconfirm a meaningful signal. Since prediction beyond 20 categories remains\nunexplored, our findings establish benchmarks for complex player modeling.\nBehavioral data alone plateaus near 10% for 36 categories, while multi-modal\nintegration enables 25%. For designers, this shows that personality-based\nadaptation requires conversational interaction, as predefined choices cannot\ncapture intent. Our evaluation at 36-category scale offers guidance for\nbuilding adaptive games that better understand their players.", "published": "2025-09-06 07:03:41", "link": "http://arxiv.org/abs/2509.05624v1", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation", "abstract": "The demand for high-fidelity numerical simulations in soil-structure\ninteraction analysis is on the rise, yet a standardized workflow to guide the\ncreation of such simulations remains elusive. This paper aims to bridge this\ngap by presenting a step-by-step guideline proposing a workflow for dynamic\nanalysis of structures with pile foundations. The proposed workflow encompasses\ninstructions on how to use Domain Reduction Method for loading, Perfectly\nMatched Layer elements for wave absorption, soil-structure interaction modeling\nusing Embedded interface elements, and domain decomposition for efficient use\nof processing units. Through a series of numerical simulations, we showcase the\npractical application of this workflow. Our results reveal the efficacy of the\nDomain Reduction Method in reducing simulation size without compromising model\nfidelity, show the precision of Perfectly Matched Layer elements in modeling\ninfinite domains, highlight the efficiency of Embedded Interface elements in\nestablishing connections between structures and the soil domain, and\ndemonstrate the overall effectiveness of the proposed workflow in conducting\nhigh-fidelity simulations. While our study focuses on simplified geometries and\nloading scenarios, it serves as a foundational framework for future research\nendeavors aimed at exploring more intricate structural configurations and\ndynamic loading conditions", "published": "2025-09-06 10:49:30", "link": "http://arxiv.org/abs/2509.05675v1", "categories": ["math.NA", "cs.DC", "cs.NA", "74S05", "G.1.8"], "primary_category": "math.NA"}
{"title": "Energy Transfer Dynamics Generated by Non-Axisymmetric Tornado-Type Flows", "abstract": "The energy cascade in turbulence, first statistically described by Richardson\n(1922) and Kolmogorov (1941), lacked connection to the underlying fluid\ndynamics. Recent numerical studies of Goto et al. (2017) and Yoneda et al.\n(2022) revealed scale-local energy transfer via vortex stretching but remained\nwithin spatial statistics. This study aims to uncover the time-dependent\nelementary process behind the energy cascade by constructing a tornado-type\nflow in a non-axisymmetric curved cylindrical domain. Our approach reveals\nspecific vortex dynamics responsible for energy transfer, offering new insight\ninto the physical mechanisms of turbulence.", "published": "2025-09-06 00:11:29", "link": "http://arxiv.org/abs/2509.05546v1", "categories": ["math.AP", "cs.NA", "math.NA", "physics.flu-dyn"], "primary_category": "math.AP"}
{"title": "Design and hedging of unit linked life insurance with environmental factors", "abstract": "We study the problem of designing and hedging unit-linked life policies whose\nbenefits depend on an investment fund that incorporates environmental criteria\nin its selection process. Offering these products poses two key challenges:\nconstructing a green investment fund and developing a hedging strategy for\npolicies written on that fund. We address these two problems separately. First,\nwe design a portfolio selection rule driven by firms' carbon intensity that\nendogenously selects assets and avoids ad hoc pre-screens based on ESG scores.\nThe effectiveness of our new portfolio selection method is tested using real\nmarket data. Second, we adopt the perspective of an insurance company issuing\nunit-linked policies written on this fund. Such contracts are exposed to\nmarket, carbon, and mortality risk, which the insurer seeks to hedge. Due to\nmarket incompleteness, we address the hedging problem via a quadratic approach\naimed at minimizing the tracking error. We also make a numerical analysis to\nassess the performance of the hedging strategy. For our simulation study, we\nuse an efficient weak second-order scheme that allows for variance reduction.", "published": "2025-09-06 10:55:09", "link": "http://arxiv.org/abs/2509.05676v1", "categories": ["q-fin.RM", "q-fin.CP", "q-fin.PM", "49L12, 60J76, 91B16, 91G20"], "primary_category": "q-fin.RM"}
{"title": "Volatility Modeling via EWMA-Driven Time-Dependent Hurst Parameters", "abstract": "We introduce a novel rough Bergomi (rBergomi) model featuring a\nvariance-driven exponentially weighted moving average (EWMA) time-dependent\nHurst parameter $H_t$, fundamentally distinct from recent machine learning and\nwavelet-based approaches in the literature. Our framework pioneers a unified\nrough differential equation (RDE) formulation grounded in rough path theory,\nwhere the Hurst parameter dynamically adapts to evolving volatility regimes\nthrough a continuous EWMA mechanism tied to instantaneous variance. Unlike\ndiscrete model-switching or computationally intensive forecasting methods, our\napproach provides mathematical tractability while capturing volatility\nclustering and roughness bursts. We rigorously establish existence and\nuniqueness of solutions via rough path theory and derive martingale properties.\nEmpirical validation on diverse asset classes including equities,\ncryptocurrencies, and commodities demonstrates superior performance in\ncapturing dynamics and out-of-sample pricing accuracy. Our results show\nsignificant improvements over traditional constant-Hurst models.", "published": "2025-09-06 19:55:52", "link": "http://arxiv.org/abs/2509.05820v1", "categories": ["q-fin.MF", "cs.LG", "Primary 60G22, Secondary 91G20"], "primary_category": "q-fin.MF"}
{"title": "Rethinking Beta: A Causal Take on CAPM", "abstract": "The CAPM regression is typically interpreted as if the market return\ncontemporaneously \\emph{causes} individual returns, motivating beta-neutral\nportfolios and factor attribution. For realized equity returns, however, this\ninterpretation is inconsistent: a same-period arrow $R_{m,t} \\to R_{i,t}$\nconflicts with the fact that $R_m$ is itself a value-weighted aggregate of its\nconstituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator\ncontradiction.'' We formalize CAPM as a structural causal model and analyze the\nadmissible three-node graphs linking an external driver $Z$, the market $R_m$,\nand an asset $R_i$. The empirically plausible baseline is a \\emph{fork}, $Z \\to\n\\{R_m, R_i\\}$, not $R_m \\to R_i$. In this setting, OLS beta reflects not a\ncausal transmission, but an attenuated proxy for how well $R_m$ captures the\nunderlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain\nexposed to macro or sectoral shocks, and hedging on $R_m$ can import\nindex-specific noise. Using stylized models and large-cap U.S.\\ equity data, we\nshow that contemporaneous betas act like proxies rather than mechanisms; any\ngenuine market-to-stock channel, if at all, appears only at a lag and with\nmodest economic significance. The practical message is clear: CAPM should be\nread as associational. Risk management and attribution should shift from fixed\nfactor menus to explicitly declared causal paths, with ``alpha'' reserved for\nwhat remains invariant once those causal paths are explicitly blocked.", "published": "2025-09-06 16:05:17", "link": "http://arxiv.org/abs/2509.05760v1", "categories": ["econ.TH", "q-fin.PR", "q-fin.ST", "stat.AP"], "primary_category": "econ.TH"}
{"title": "The Measure of Deception: An Analysis of Data Forging in Machine Unlearning", "abstract": "Motivated by privacy regulations and the need to mitigate the effects of\nharmful data, machine unlearning seeks to modify trained models so that they\neffectively ``forget'' designated data. A key challenge in verifying unlearning\nis forging -- adversarially crafting data that mimics the gradient of a target\npoint, thereby creating the appearance of unlearning without actually removing\ninformation. To capture this phenomenon, we consider the collection of data\npoints whose gradients approximate a target gradient within tolerance\n$\\epsilon$ -- which we call an $\\epsilon$-forging set -- and develop a\nframework for its analysis. For linear regression and one-layer neural\nnetworks, we show that the Lebesgue measure of this set is small. It scales on\nthe order of $\\epsilon$, and when $\\epsilon$ is small enough, $\\epsilon^d$.\nMore generally, under mild regularity assumptions, we prove that the forging\nset measure decays as $\\epsilon^{(d-r)/2}$, where $d$ is the data dimension and\n$r<d$ is the nullity of a variation matrix defined by the model gradients.\nExtensions to batch SGD and almost-everywhere smooth loss functions yield the\nsame asymptotic scaling. In addition, we establish probability bounds showing\nthat, under non-degenerate data distributions, the likelihood of randomly\nsampling a forging point is vanishingly small. These results provide evidence\nthat adversarial forging is fundamentally limited and that false unlearning\nclaims can, in principle, be detected.", "published": "2025-09-06 23:44:05", "link": "http://arxiv.org/abs/2509.05865v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Beyond ATE: Multi-Criteria Design for A/B Testing", "abstract": "A/B testing is a widely adopted methodology for estimating conditional\naverage treatment effects (CATEs) in both clinical trials and online platforms.\nWhile most existing research has focused primarily on maximizing estimation\naccuracy, practical applications must also account for additional\nobjectives-most notably welfare or revenue loss. In many settings, it is\ncritical to administer treatments that improve patient outcomes or to implement\nplans that generate greater revenue from customers. Within a machine learning\nframework, such objectives are naturally captured through the notion of\ncumulative regret. In this paper, we investigate the fundamental trade-off\nbetween social welfare loss and statistical accuracy in (adaptive) experiments\nwith heterogeneous treatment effects. We establish matching upper and lower\nbounds for the resulting multi-objective optimization problem and employ the\nconcept of Pareto optimality to characterize the necessary and sufficient\nconditions for optimal experimental designs. Beyond estimating CATEs,\npractitioners often aim to deploy treatment policies that maximize welfare\nacross the entire population. We demonstrate that our Pareto-optimal adaptive\ndesign achieves optimal post-experiment welfare, irrespective of the\nin-experiment trade-off between accuracy and welfare. Furthermore, since\nclinical and commercial data are often highly sensitive, it is essential to\nincorporate robust privacy guarantees into any treatment-allocation mechanism.\nTo this end, we develop differentially private algorithms that continue to\nachieve our established lower bounds, showing that privacy can be attained at\nnegligible cost.", "published": "2025-09-06 23:42:22", "link": "http://arxiv.org/abs/2509.05864v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation", "abstract": "Motivated by the need for rigorous and scalable evaluation of large language\nmodels, we study contextual preference inference for pairwise comparison\nfunctionals of context-dependent preference score functions across domains.\nFocusing on the contextual Bradley-Terry-Luce model, we develop a\nsemiparametric efficient estimator that automates the debiased estimation\nthrough aggregating weighted residual balancing terms across the comparison\ngraph. We show that the efficiency is achieved when the weights are derived\nfrom a novel strategy called Fisher random walk. We also propose a\ncomputationally feasible method to compute the weights by a potential\nrepresentation of nuisance weight functions. We show our inference procedure is\nvalid for general score function estimators accommodating the practitioners'\nneed to implement flexible deep learning methods. We extend the procedure to\nmultiple hypothesis testing using a Gaussian multiplier bootstrap that controls\nfamilywise error and to distributional shift via a cross-fitted\nimportance-sampling adjustment for target-domain inference. Numerical studies,\nincluding language model evaluations under diverse contexts, corroborate the\naccuracy, efficiency, and practical utility of our method.", "published": "2025-09-06 22:29:17", "link": "http://arxiv.org/abs/2509.05852v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection", "abstract": "Out-of-distribution (OOD) detection plays a key role in enhancing the\nrobustness of artificial intelligence systems by identifying inputs that differ\nsignificantly from the training distribution, thereby preventing unreliable\npredictions and enabling appropriate fallback mechanisms. Developing reliable\nOOD detection methods is a significant challenge, and rigorous evaluation of\nthese techniques is essential for ensuring their effectiveness, as it allows\nresearchers to assess their performance under diverse conditions and to\nidentify potential limitations or failure modes. Cross-validation (CV) has\nproven to be a highly effective tool for providing a reasonable estimate of the\nperformance of a learning algorithm. Although OOD scenarios exhibit particular\ncharacteristics, an appropriate adaptation of CV can lead to a suitable\nevaluation framework for this setting. This work proposes a dual CV framework\nfor robust evaluation of OOD detection models, aimed at improving the\nreliability of their assessment. The proposed evaluation framework aims to\neffectively integrate in-distribution (ID) and OOD data while accounting for\ntheir differing characteristics. To achieve this, ID data are partitioned using\na conventional approach, whereas OOD data are divided by grouping samples based\non their classes. Furthermore, we analyze the context of data with class\nhierarchy to propose a data splitting that considers the entire class hierarchy\nto obtain fair ID-OOD partitions to apply the proposed evaluation framework.\nThis framework is called Dual Cross-Validation for Robust Out-of-Distribution\nDetection (DCV-ROOD). To test the validity of the evaluation framework, we\nselected a set of state-of-the-art OOD detection methods, both with and without\noutlier exposure. The results show that the method achieves very fast\nconvergence to the true performance.", "published": "2025-09-06 17:20:09", "link": "http://arxiv.org/abs/2509.05778v1", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2"], "primary_category": "cs.LG"}
{"title": "Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery", "abstract": "Estimating heterogeneous treatment effects is critical in domains such as\npersonalized medicine, resource allocation, and policy evaluation. A central\nchallenge lies in identifying subpopulations that respond differently to\ninterventions, thereby enabling more targeted and effective decision-making.\nWhile clustering methods are well-studied in unsupervised learning, their\nintegration with causal inference remains limited. We propose a novel framework\nthat clusters individuals based on estimated treatment effects using a learned\nkernel derived from causal forests, revealing latent subgroup structures. Our\napproach consists of two main steps. First, we estimate debiased Conditional\nAverage Treatment Effects (CATEs) using orthogonalized learners via the\nRobinson decomposition, yielding a kernel matrix that encodes sample-level\nsimilarities in treatment responsiveness. Second, we apply kernelized\nclustering to this matrix to uncover distinct, treatment-sensitive\nsubpopulations and compute cluster-level average CATEs. We present this\nkernelized clustering step as a form of regularization within the\nresidual-on-residual regression framework. Through extensive experiments on\nsemi-synthetic and real-world datasets, supported by ablation studies and\nexploratory analyses, we demonstrate the effectiveness of our method in\ncapturing meaningful treatment effect heterogeneity.", "published": "2025-09-06 17:01:23", "link": "http://arxiv.org/abs/2509.05775v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Risk-averse Fair Multi-class Classification", "abstract": "We develop a new classification framework based on the theory of coherent\nrisk measures and systemic risk. The proposed approach is suitable for\nmulti-class problems when the data is noisy, scarce (relative to the dimension\nof the problem), and the labeling might be unreliable. In the first part of our\npaper, we provide the foundation of the use of systemic risk models and show\nhow to apply it in the context of linear and kernel-based multi-class problems.\nMore advanced formulation via a system-theoretic approach with non-linear\naggregation is proposed, which leads to a two-stage stochastic programming\nproblem. A risk-averse regularized decomposition method is designed to solve\nthe problem. We use a popular multi-class method as a benchmark in the\nperformance analysis of the proposed classification methods. We illustrate our\nideas by proposing several generalization of that method by the use of coherent\nmeasures of risk. The viability of the proposed risk-averse methods are\nsupported theoretically and numerically. Additionally, we demonstrate that the\napplication of systemic risk measures facilitates enforcing fairness in\nclassification. Analysis and experiments regarding the fairness of the proposed\nmodels are carefully conducted. For all methods, our numerical experiments\ndemonstrate that they are robust in the presence of unreliable training data\nand perform better on unknown data than the methods minimizing expected\nclassification errors. Furthermore, the performance improves when the number of\nclasses increases.", "published": "2025-09-06 16:54:00", "link": "http://arxiv.org/abs/2509.05771v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "stat.ML"}
{"title": "Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders", "abstract": "Anomaly detection underpins critical applications from network security and\nintrusion detection to fraud prevention, where recognizing aberrant patterns\nrapidly is indispensable. Progress in this area is routinely impeded by two\nobstacles: extreme class imbalance and the curse of dimensionality. To combat\nthe former, we previously introduced Precision-Recall Curve (PRC)\nclassification trees and their ensemble extension, the PRC Random Forest\n(PRC-RF). Building on that foundation, we now propose a hybrid framework that\nintegrates PRC-RF with autoencoders, unsupervised machine learning methods that\nlearn compact latent representations, to confront both challenges\nsimultaneously. Extensive experiments across diverse benchmark datasets\ndemonstrate that the resulting Autoencoder-PRC-RF model achieves superior\naccuracy, scalability, and interpretability relative to prior methods,\naffirming its potential for high-stakes anomaly-detection tasks.", "published": "2025-09-06 16:39:22", "link": "http://arxiv.org/abs/2509.05766v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Robust variational neural posterior estimation for simulation-based inference", "abstract": "Recent advances in neural density estimation have enabled powerful\nsimulation-based inference (SBI) methods that can flexibly approximate Bayesian\ninference for intractable stochastic models. Although these methods have\ndemonstrated reliable posterior estimation when the simulator accurately\nrepresents the underlying data generative process (GDP), recent work has shown\nthat they perform poorly in the presence of model misspecification. This poses\na significant problem for their use on real-world problems, due to simulators\nalways misrepresenting the true DGP to a certain degree. In this paper, we\nintroduce robust variational neural posterior estimation (RVNP), a method which\naddresses the problem of misspecification in amortised SBI by bridging the\nsimulation-to-reality gap using variational inference and error modelling. We\ntest RVNP on multiple benchmark tasks, including using real data from\nastronomy, and show that it can recover robust posterior inference in a\ndata-driven manner without adopting tunable hyperparameters or priors governing\nthe misspecification.", "published": "2025-09-06 14:10:49", "link": "http://arxiv.org/abs/2509.05724v1", "categories": ["stat.ML", "astro-ph.GA", "cs.LG"], "primary_category": "stat.ML"}
{"title": "GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR", "abstract": "Human Activity Recognition (HAR) using multimodal sensor data remains\nchallenging due to noisy or incomplete measurements, scarcity of labeled\nexamples, and privacy concerns. Traditional centralized deep learning\napproaches are often constrained by infrastructure availability, network\nlatency, and data sharing restrictions. While federated learning (FL) addresses\nprivacy by training models locally and sharing only model parameters, it still\nhas to tackle issues arising from the use of heterogeneous multimodal data and\ndifferential privacy requirements. In this article, a Graph-based Multimodal\nFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse\nsensor streams such as a pressure mat, depth camera, and multiple\naccelerometers are modeled as modality-specific graphs, processed through\nresidual Graph Convolutional Neural Networks (GCNs), and fused via\nattention-based weighting rather than simple concatenation. The fused\nembeddings enable robust activity classification, while differential privacy\nsafeguards data during federated aggregation. Experimental results show that\nthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with\nup to 2 percent higher accuracy in non-DP settings in both centralized and\nfederated paradigms. More importantly, significant improvements are observed\nunder differential privacy constraints: MultiModalGCN consistently surpasses\nMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on\nthe privacy budget and setting. These results highlight the robustness of\ngraph-based modeling in multimodal learning, where GNNs prove more resilient to\nthe performance degradation introduced by DP noise.", "published": "2025-09-06 10:23:17", "link": "http://arxiv.org/abs/2509.05671v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives", "abstract": "AI audits play a critical role in AI accountability and safety. One branch of\nthe law for which AI audits are particularly salient is anti-discrimination\nlaw. Several areas of anti-discrimination law implicate the \"less\ndiscriminatory alternative\" (LDA) requirement, in which a protocol (e.g.,\nmodel) is defensible if no less discriminatory protocol that achieves\ncomparable performance can be found with a reasonable amount of effort.\nNotably, the burden of proving an LDA exists typically falls on the claimant\n(the party alleging discrimination). This creates a significant hurdle in AI\ncases, as the claimant would seemingly need to train a less discriminatory yet\nhigh-performing model, a task requiring resources and expertise beyond most\nlitigants. Moreover, developers often shield information about and access to\ntheir model and training data as trade secrets, making it difficult to\nreproduce a similar model from scratch.\n  In this work, we present a procedure enabling claimants to determine if an\nLDA exists, even when they have limited compute, data, information, and model\naccess. We focus on the setting in which fairness is given by demographic\nparity and performance by binary cross-entropy loss. As our main result, we\nprovide a novel closed-form upper bound for the loss-fairness Pareto frontier\n(PF). We show how the claimant can use it to fit a PF in the \"low-resource\nregime,\" then extrapolate the PF that applies to the (large) model being\ncontested, all without training a single large model. The expression thus\nserves as a scaling law for loss-fairness PFs. To use this scaling law, the\nclaimant would require a small subsample of the train/test data. Then, the\nclaimant can fit the context-specific PF by training as few as 7 (small)\nmodels. We stress test our main result in simulations, finding that our scaling\nlaw holds even when the exact conditions of our theory do not.", "published": "2025-09-06 07:23:25", "link": "http://arxiv.org/abs/2509.05627v1", "categories": ["cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CY"}
{"title": "Interpretable dimension reduction for compositional data", "abstract": "High-dimensional compositional data, such as those from human microbiome\nstudies, pose unique statistical challenges due to the simplex constraint and\nexcess zeros. While dimension reduction is indispensable for analyzing such\ndata, conventional approaches often rely on log-ratio transformations that\ncompromise interpretability and distort the data through ad hoc zero\nreplacements. We introduce a novel framework for interpretable dimension\nreduction of compositional data that avoids extra transformations and zero\nimputations. Our approach generalizes the concept of amalgamation by softening\nits operation, mapping high-dimensional compositions directly to a\nlower-dimensional simplex, which can be visualized in ternary plots. The\nframework further provides joint visualization of the reduction matrix,\nenabling intuitive, at-a-glance interpretation. To achieve optimal reduction\nwithin our framework, we incorporate sufficient dimension reduction, which\ndefines a new identifiable objective: the central compositional subspace. For\nestimation, we propose a compositional kernel dimension reduction (CKDR)\nmethod. The estimator is provably consistent, exhibits sparsity that reveals\nunderlying amalgamation structures, and comes with an intrinsic predictive\nmodel for downstream analyses. Applications to real microbiome datasets\ndemonstrate that our approach provides a powerful graphical exploration tool\nfor uncovering meaningful biological patterns, opening a new pathway for\nanalyzing high-dimensional compositional data.", "published": "2025-09-06 02:16:21", "link": "http://arxiv.org/abs/2509.05563v1", "categories": ["stat.ME", "math.ST", "stat.AP", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model", "abstract": "Human infants face a formidable challenge in speech acquisition: mapping\nextremely variable acoustic inputs into appropriate articulatory movements\nwithout explicit instruction. We present a computational model that addresses\nthe acoustic-to-articulatory mapping problem through self-supervised learning.\nOur model comprises a feature extractor that transforms speech into latent\nrepresentations, an inverse model that maps these representations to\narticulatory parameters, and a synthesizer that generates speech outputs.\nExperiments conducted in both single- and multi-speaker settings reveal that\nintermediate layers of a pre-trained wav2vec 2.0 model provide optimal\nrepresentations for articulatory learning, significantly outperforming MFCC\nfeatures. These representations enable our model to learn articulatory\ntrajectories that correlate with human patterns, discriminate between places of\narticulation, and produce intelligible speech. Critical to successful\narticulatory learning are representations that balance phonetic\ndiscriminability with speaker invariance -- precisely the characteristics of\nself-supervised representation learning models. Our findings provide\ncomputational evidence consistent with developmental theories proposing that\nperceptual learning of phonetic categories guides articulatory development,\noffering insights into how infants might acquire speech production capabilities\ndespite the complex mapping problem they face.", "published": "2025-09-06 22:10:24", "link": "http://arxiv.org/abs/2509.05849v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Yours or Mine? Overwriting Attacks against Neural Audio Watermarking", "abstract": "As generative audio models are rapidly evolving, AI-generated audios\nincreasingly raise concerns about copyright infringement and misinformation\nspread. Audio watermarking, as a proactive defense, can embed secret messages\ninto audio for copyright protection and source verification. However, current\nneural audio watermarking methods focus primarily on the imperceptibility and\nrobustness of watermarking, while ignoring its vulnerability to security\nattacks. In this paper, we develop a simple yet powerful attack: the\noverwriting attack that overwrites the legitimate audio watermark with a forged\none and makes the original legitimate watermark undetectable. Based on the\naudio watermarking information that the adversary has, we propose three\ncategories of overwriting attacks, i.e., white-box, gray-box, and black-box\nattacks. We also thoroughly evaluate the proposed attacks on state-of-the-art\nneural audio watermarking methods. Experimental results demonstrate that the\nproposed overwriting attacks can effectively compromise existing watermarking\nschemes across various settings and achieve a nearly 100% attack success rate.\nThe practicality and effectiveness of the proposed overwriting attacks expose\nsecurity flaws in existing neural audio watermarking systems, underscoring the\nneed to enhance security in future audio watermarking designs.", "published": "2025-09-06 21:23:44", "link": "http://arxiv.org/abs/2509.05835v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Effectively obtaining acoustic, visual and textual data from videos", "abstract": "The increasing use of machine learning models has amplified the demand for\nhigh-quality, large-scale multimodal datasets. However, the availability of\nsuch datasets, especially those combining acoustic, visual and textual data,\nremains limited. This paper addresses this gap by proposing a method to extract\nrelated audio-image-text observations from videos. We detail the process of\nselecting suitable videos, extracting relevant data pairs, and generating\ndescriptive texts using image-to-text models. Our approach ensures a robust\nsemantic connection between modalities, enhancing the utility of the created\ndatasets for various applications. We also discuss the challenges encountered\nand propose solutions to improve data quality. The resulting datasets, publicly\navailable, aim to support and advance research in multimodal data analysis and\nmachine learning.", "published": "2025-09-06 17:42:22", "link": "http://arxiv.org/abs/2509.05786v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Time-domain sound field estimation using kernel ridge regression", "abstract": "Sound field estimation methods based on kernel ridge regression have proven\neffective, allowing for strict enforcement of physical properties, in addition\nto the inclusion of prior knowledge such as directionality of the sound field.\nThese methods have been formulated for single-frequency sound fields,\nrestricting the types of data and prior knowledge that can be used. In this\npaper, the kernel ridge regression approach is generalized to consider\ndiscrete-time sound fields. The proposed method provides time-domain sound\nfield estimates that can be computed in closed form, are guaranteed to be\nphysically realizable, and for which time-domain properties of the sound fields\ncan be exploited to improve estimation performance. Exploiting prior\ninformation on the time-domain behaviour of room impulse responses, the\nestimation performance of the proposed method is shown to be improved using a\ntime-domain data weighting, demonstrating the usefulness of the proposed\napproach. It is further shown using both simulated and real data that the\ntime-domain data weighting can be combined with a directional weighting,\nexploiting prior knowledge of both spatial and temporal properties of the room\nimpulse responses. The theoretical framework of the proposed method enables\nsolving a broader class of sound field estimation problems using kernel ridge\nregression where it would be required to consider the time-domain response\nrather than the frequency-domain response of each frequency separately.", "published": "2025-09-06 13:56:10", "link": "http://arxiv.org/abs/2509.05720v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Resource Allocation and Beamforming in FIM-Assisted BS and STAR-BD-RIS-Aided NOMA: A Meta-Learning Approach", "abstract": "This study explores a flexible intelligent metasurface (FIM)-based wireless\ncommunication system that integrates simultaneously transmitting and reflecting\nbeyond diagonal reconfigurable intelligent surfaces (STAR-BD-RIS) with\nnon-orthogonal multiple access (NOMA). The system features a multi-antenna\nFIM-assisted base station (BS) aided by dual-sector BD-RIS. The FIM consists of\ncost-effective radiating elements that can independently emit signals and\ndynamically adjust their vertical positions (\"morphing\"). The goal is to\nmaximize energy efficiency by jointly optimizing BS beamforming, the\nSTAR-BD-RIS matrix, NOMA constraints, and the FIM surface shape under power\nlimits. Due to the problem's non-convexity, a meta-soft actor-critic (Meta-SAC)\nalgorithm is proposed for adaptive optimization. Simulation results show that\nMeta-SAC outperforms the Meta-DDPG algorithm, and FIM-assisted designs yield\nsubstantial energy efficiency gains over benchmark schemes.", "published": "2025-09-06 11:46:54", "link": "http://arxiv.org/abs/2509.05692v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Affine Filter Bank Modulation (AFBM): A Novel 6G ISAC Waveform with Low PAPR and OOBE", "abstract": "We propose the affine filter bank modulation (AFBM) waveform for enhanced\nintegrated sensing and communications (ISAC) in sixth generation (6G), designed\nby drawing on concepts from classical filter bank multicarrier modulation\n(FBMC) theory and recent advances in chirp-domain waveforms, particularly\naffine frequency division multiplexing (AFDM). Specifically, AFBM exhibits\nseveral desirable properties, with emphasis on its remarkably low\npeak-to-average power ratio (PAPR) and reduced out-of-band emission (OOBE) when\nbenchmarked against the conventional AFDM waveform under doubly-dispersive (DD)\nchannel conditions. In the communications setting, reliable symbol detection is\nachieved using a tailored low-complexity Gaussian belief propagation\n(GaBP)-based algorithm, while in the sensing setting, a range and velocity\nestimation approach is developed that integrates an expectation maximization\n(EM)-assisted probabilistic data association (PDA) framework to accurately\nidentify surrounding targets. The highlighted performance and benefits of AFBM\nare validated through analytical and numerical evaluations, including\nconventional metrics such as ambiguity function (AF), bit error rate (BER), and\nroot mean square error (RMSE), consolidating its position as a promising\nwaveform for next-generation wireless systems.", "published": "2025-09-06 11:17:38", "link": "http://arxiv.org/abs/2509.05683v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Full-Angle Ray Antenna Array and Omnicell Wireless Communication System", "abstract": "Ray antenna array (RAA) was recently proposed as a novel multi-antenna\narchitecture that arranges multiple massive cheap antenna elements into simple\nuniform linear arrays (sULAs) with different orientations. Compared with\ntraditional architectures like hybrid analog/digital beamforming with uniform\nlinear array (ULA) and uniform circular array (UCA), RAA has several promising\nadvantages such as significantly reduced hardware cost, higher beamforming\ngains and the ability of providing uniform angular resolution for all\ndirections. In this paper, we propose a full-angle RAA architecture and an\ninnovative omnicell wireless communication paradigm enabled by full-angle RAA.\nThe proposed full-angle RAA expands RAA's orientation angle to the full angle\ndomain, such that the RAA's advantages can be exploited to all directions. This\nfurther enables the new concept of omnicell wireless communication system, with\nthe base station equipped by full-angle RAA and deployed at the center of each\ncell. Compared to the conventional cell sectoring wireless communication\nsystem, the proposed omnicell system is expected to not only significantly\nreduce the inter-user interference, but also improve the cost efficiency.\nExtensive analytical and numerical results are provided to compare those key\nperformance indicators such as the spatial resolution and the communication\nrate of the proposed full-angle RAA based omnicell wireless communication\nsystem against the conventional ULA/UCA-based cell sectoring systems.", "published": "2025-09-06 10:57:02", "link": "http://arxiv.org/abs/2509.05677v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Power-Measurement-Based Channel Estimation for Beyond Diagonal RIS", "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS), with its\nenhanced degrees of freedom compared to conventional RIS, has demonstrated\nnotable potential for enhancing wireless communication performance. However, a\nkey challenge in employing BD-RIS lies in accurately acquiring its channel\nstate information (CSI) with both the base station (BS) and users. Existing\nBD-RIS channel estimation methods rely mainly on dedicated pilot signals, which\nincrease system overhead and may be incompatible with current communication\nprotocols. To overcome these limitations, this letter proposes a new\nsingle-layer neural network (NN)-enabled channel estimation method utilizing\nonly the easily accessible received power measurements at user terminals. In\nparticular, we show that the received signal power can be expressed in a form\nsimilar to a single-layer NN, where the weights represent the BD-RIS's CSI.\nThis structure enables the recovery of CSI using the backward propagation,\nbased on power measurements collected under varying training reflection\ncoefficients. Numerical results show that our proposed method can achieve a\nsmall normalized mean square error (NMSE), particularly when the number of\ntraining reflections is large.", "published": "2025-09-06 08:14:06", "link": "http://arxiv.org/abs/2509.05639v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Time-Modulated Intelligent Reflecting Surfaces for Integrated Sensing, Communication and Security: A Generative AI Design Framework", "abstract": "We propose a novel approach to achieve physical layer security for integrated\nsensing and communication (ISAC) systems operating in the presence of targets\nthat may be eavesdroppers. The system is aided by a time-modulated intelligent\nreflecting surface (TM-IRS), which is configured to preserve the integrity of\nthe transmitted data at one or more legitimate communication users (CUs) while\nmaking them appear scrambled in all other directions. The TM-IRS design\nleverages a generative flow network (GFlowNet) framework to learn a stochastic\npolicy that samples high-performing TM-IRS configurations from a vast discrete\nparameter space. Specifically, we begin by formulating the achievable sum rate\nfor the legitimate CUs and the beampattern gain toward the target direction,\nbased on which we construct reward functions for GFlowNets that jointly capture\nboth communication and sensing performance. The TM-IRS design is modeled as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a complete configuration of TM-IRS parameters. GFlowNets,\nparametrized by deep neural networks are employed to learn a stochastic policy\nthat samples TM-IRS parameter sets with probability proportional to their\nassociated reward. Experimental results demonstrate the effectiveness of the\nproposed GFlowNet-based method in integrating sensing, communication and\nsecurity simultaneously, and also exhibit significant sampling efficiency as\ncompared to the exhaustive combinatorial search and enhanced robustness against\nthe rule-based TM-IRS design method.", "published": "2025-09-06 02:34:44", "link": "http://arxiv.org/abs/2509.05565v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "abstract": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "published": "2025-09-06 09:26:39", "link": "http://arxiv.org/abs/2509.05657v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "abstract": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "published": "2025-09-06 05:33:17", "link": "http://arxiv.org/abs/2509.05602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
