{"title": "Redwood: Using Collision Detection to Grow a Large-Scale Intent\n  Classification Dataset", "abstract": "Dialog systems must be capable of incorporating new skills via updates over\ntime in order to reflect new use cases or deployment scenarios. Similarly,\ndevelopers of such ML-driven systems need to be able to add new training data\nto an already-existing dataset to support these new skills. In intent\nclassification systems, problems can arise if training data for a new skill's\nintent overlaps semantically with an already-existing intent. We call such\ncases collisions. This paper introduces the task of intent collision detection\nbetween multiple datasets for the purposes of growing a system's skillset. We\nintroduce several methods for detecting collisions, and evaluate our methods on\nreal datasets that exhibit collisions. To highlight the need for intent\ncollision detection, we show that model performance suffers if new data is\nadded in such a way that does not arbitrate colliding intents. Finally, we use\ncollision detection to construct and benchmark a new dataset, Redwood, which is\ncomposed of 451 ntent categories from 13 original intent classification\ndatasets, making it the largest publicly available intent classification\nbenchmark.", "published": "2022-04-12 02:28:23", "link": "http://arxiv.org/abs/2204.05483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overlapping Word Removal is All You Need: Revisiting Data Imbalance in\n  Hope Speech Detection", "abstract": "Hope Speech Detection, a task of recognizing positive expressions, has made\nsignificant strides recently. However, much of the current works focus on model\ndevelopment without considering the issue of inherent imbalance in the data.\nOur work revisits this issue in hope-speech detection by introducing focal\nloss, data augmentation, and pre-processing strategies. Accordingly, we find\nthat introducing focal loss as part of Multilingual-BERT's (M-BERT) training\nprocess mitigates the effect of class imbalance and improves overall F1-Macro\nby 0.11. At the same time, contextual and back-translation-based word\naugmentation with M-BERT improves results by 0.10 over baseline despite\nimbalance. Finally, we show that overlapping word removal based on\npre-processing, though simple, improves F1-Macro by 0.28. In due process, we\npresent detailed studies depicting various behaviors of each of these\nstrategies and summarize key findings from our empirical results for those\ninterested in getting the most out of M-BERT for hope speech detection under\nreal-world conditions of data imbalance.", "published": "2022-04-12 02:38:54", "link": "http://arxiv.org/abs/2204.05488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for\n  Multimodal Sentiment Detection", "abstract": "Compared with unimodal data, multimodal data can provide more features to\nhelp the model analyze the sentiment of data. Previous research works rarely\nconsider token-level feature fusion, and few works explore learning the common\nfeatures related to sentiment in multimodal data to help the model fuse\nmultimodal features. In this paper, we propose a Contrastive Learning and\nMulti-Layer Fusion (CLMLF) method for multimodal sentiment detection.\nSpecifically, we first encode text and image to obtain hidden representations,\nand then use a multi-layer fusion module to align and fuse the token-level\nfeatures of text and image. In addition to the sentiment analysis task, we also\ndesigned two contrastive learning tasks, label based contrastive learning and\ndata based contrastive learning tasks, which will help the model learn common\nfeatures related to sentiment in multimodal data. Extensive experiments\nconducted on three publicly available multimodal datasets demonstrate the\neffectiveness of our approach for multimodal sentiment detection compared with\nexisting methods. The codes are available for use at\nhttps://github.com/Link-Li/CLMLF", "published": "2022-04-12 04:03:06", "link": "http://arxiv.org/abs/2204.05515v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not always about you: Prioritizing community needs when developing\n  endangered language technology", "abstract": "Languages are classified as low-resource when they lack the quantity of data\nnecessary for training statistical and machine learning tools and models.\nCauses of resource scarcity vary but can include poor access to technology for\ndeveloping these resources, a relatively small population of speakers, or a\nlack of urgency for collecting such resources in bilingual populations where\nthe second language is high-resource. As a result, the languages described as\nlow-resource in the literature are as different as Finnish on the one hand,\nwith millions of speakers using it in every imaginable domain, and Seneca, with\nonly a small-handful of fluent speakers using the language primarily in a\nrestricted domain. While issues stemming from the lack of resources necessary\nto train models unite this disparate group of languages, many other issues cut\nacross the divide between widely-spoken low resource languages and endangered\nlanguages. In this position paper, we discuss the unique technological,\ncultural, practical, and ethical challenges that researchers and indigenous\nspeech community members face when working together to develop language\ntechnology to support endangered language documentation and revitalization. We\nreport the perspectives of language teachers, Master Speakers and elders from\nindigenous communities, as well as the point of view of academics. We describe\nan ongoing fruitful collaboration and make recommendations for future\npartnerships between academic researchers and language community stakeholders.", "published": "2022-04-12 05:59:39", "link": "http://arxiv.org/abs/2204.05541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delving Deep into Regularity: A Simple but Effective Method for Chinese\n  Named Entity Recognition", "abstract": "Recent years have witnessed the improving performance of Chinese Named Entity\nRecognition (NER) from proposing new frameworks or incorporating word lexicons.\nHowever, the inner composition of entity mentions in character-level Chinese\nNER has been rarely studied. Actually, most mentions of regular types have\nstrong name regularity. For example, entities end with indicator words such as\n\"company\" or \"bank\" usually belong to organization. In this paper, we propose a\nsimple but effective method for investigating the regularity of entity spans in\nChinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).\nSpecifically, the proposed model consists of two branches: a regularity-aware\nmodule and a regularityagnostic module. The regularity-aware module captures\nthe internal regularity of each span for better entity type prediction, while\nthe regularity-agnostic module is employed to locate the boundary of entities\nand relieve the excessive attention to span regularity. An orthogonality space\nis further constructed to encourage two modules to extract different aspects of\nregularity features. To verify the effectiveness of our method, we conduct\nextensive experiments on three benchmark datasets and a practical medical\ndataset. The experimental results show that our RICON significantly outperforms\nprevious state-of-the-art methods, including various lexicon-based methods.", "published": "2022-04-12 06:12:31", "link": "http://arxiv.org/abs/2204.05544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiomify -- Building a Collocation-supplemented Reverse Dictionary of\n  English Idioms with Word2Vec for non-native learners", "abstract": "The aim of idiomify is to build a collocation-supplemented reverse dictionary\nof idioms for the non-native learners of English. We aim to do so because the\nreverse dictionary could help the non-natives explore idioms on demand, and the\ncollocations could also guide them on using idioms more adequately. The\ncornerstone of the project is a reliable way of mining idioms from corpora,\nwhich is however a challenge because idioms extensively vary in forms. We\ntackle this by automatically deriving matching rules from their base forms. We\nuse Point-wise Mutual Inclusion (PMI), Term Frequency - Inverse Document\nFrequency (TF-IDF) to model collocations, since both of them are popular metric\nfor pairwise significance. We also try Term Frequency (TF) as the baseline\nmodel. As for implementing the reverse-dictionary, three approaches could be\ntaken: inverted index, graphs and distributional semantics. We choose to take\nthe last approach and implement the reverse dictionary with Word2Vec, because\nit is the most flexible approach of all and Word2Vec is a simple yet strong\nbaseline. Evaluating the methods has revealed rooms for improvement. We learn\nthat we can better identify idioms with the help of slop, wildcard and\nreordering techniques. We also learn that we can get the best of both PMI and\nTF-IDF if we use machine learning to find the sweet spot. Lastly, We learn that\nIdiomify could be further improved with a mixture of inverted index and\ndistributional semantics approach. The limits aside, the proposed methods are\nfeasible, and their benefits to the non-natives are apparent, which therefore\ncan be used to aid the non-natives in acquiring English idioms.", "published": "2022-04-12 08:55:27", "link": "http://arxiv.org/abs/2204.05634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Toothbrushes do in the Kitchen? How Transformers Think our World\n  is Structured", "abstract": "Transformer-based models are now predominant in NLP. They outperform\napproaches based on static models in many respects. This success has in turn\nprompted research that reveals a number of biases in the language models\ngenerated by transformers. In this paper we utilize this research on biases to\ninvestigate to what extent transformer-based language models allow for\nextracting knowledge about object relations (X occurs in Y; X consists of Z;\naction A involves using X). To this end, we compare contextualized models with\ntheir static counterparts. We make this comparison dependent on the application\nof a number of similarity measures and classifiers. Our results are threefold:\nFirstly, we show that the models combined with the different similarity\nmeasures differ greatly in terms of the amount of knowledge they allow for\nextracting. Secondly, our results suggest that similarity measures perform much\nworse than classifier-based approaches. Thirdly, we show that, surprisingly,\nstatic models perform almost as well as contextualized models -- in some cases\neven better.", "published": "2022-04-12 10:00:20", "link": "http://arxiv.org/abs/2204.05673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Generative Approach for Financial Causality Extraction", "abstract": "Causality represents the foremost relation between events in financial\ndocuments such as financial news articles, financial reports. Each financial\ncausality contains a cause span and an effect span. Previous works proposed\nsequence labeling approaches to solve this task. But sequence labeling models\nfind it difficult to extract multiple causalities and overlapping causalities\nfrom the text segments. In this paper, we explore a generative approach for\ncausality extraction using the encoder-decoder framework and pointer networks.\nWe use a causality dataset from the financial domain, \\textit{FinCausal}, for\nour experiments and our proposed framework achieves very competitive\nperformance on this dataset.", "published": "2022-04-12 10:05:41", "link": "http://arxiv.org/abs/2204.05674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Losses for One-Class Textual Anomaly Detection", "abstract": "Current deep learning methods for anomaly detection in text rely on\nsupervisory signals in inliers that may be unobtainable or bespoke\narchitectures that are difficult to tune. We study a simpler alternative:\nfine-tuning Transformers on the inlier data with self-supervised objectives and\nusing the losses as an anomaly score. Overall, the self-supervision approach\noutperforms other methods under various anomaly detection scenarios, improving\nthe AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by\n22.8% on average. Additionally, the optimal objective and resultant learnt\nrepresentation depend on the type of downstream anomaly. The separability of\nanomalies and inliers signals that a representation is more effective for\ndetecting semantic anomalies, whilst the presence of narrow feature directions\nsignals a representation that is effective for detecting syntactic anomalies.", "published": "2022-04-12 10:42:47", "link": "http://arxiv.org/abs/2204.05695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Not Fire the Linguist: Grammatical Profiles Help Language Models\n  Detect Semantic Change", "abstract": "Morphological and syntactic changes in word usage (as captured, e.g., by\ngrammatical profiles) have been shown to be good predictors of a word's meaning\nchange. In this work, we explore whether large pre-trained contextualised\nlanguage models, a common tool for lexical semantic change detection, are\nsensitive to such morphosyntactic changes. To this end, we first compare the\nperformance of grammatical profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, covering 7 languages, and then combine\nthe two approaches in ensembles to assess their complementarity. Our results\nshow that ensembling grammatical profiles with XLM-R improves semantic change\ndetection performance for most datasets and languages. This indicates that\nlanguage models do not fully cover the fine-grained morphological and syntactic\nsignals that are explicitly represented in grammatical profiles.\n  An interesting exception are the test sets where the time spans under\nanalysis are much longer than the time gap between them (for example,\ncentury-long spans with a one-year gap between them). Morphosyntactic change is\nslow so grammatical profiles do not detect in such cases. In contrast, language\nmodels, thanks to their access to lexical information, are able to detect fast\ntopical changes.", "published": "2022-04-12 11:20:42", "link": "http://arxiv.org/abs/2204.05717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposed Meta-Learning for Few-Shot Named Entity Recognition", "abstract": "Few-shot named entity recognition (NER) systems aim at recognizing\nnovel-class named entities based on only a few labeled examples. In this paper,\nwe present a decomposed meta-learning approach which addresses the problem of\nfew-shot NER by sequentially tackling few-shot span detection and few-shot\nentity typing using meta-learning. In particular, we take the few-shot span\ndetection as a sequence labeling problem and train the span detector by\nintroducing the model-agnostic meta-learning (MAML) algorithm to find a good\nmodel parameter initialization that could fast adapt to new entity classes. For\nfew-shot entity typing, we propose MAML-ProtoNet, i.e., MAML-enhanced\nprototypical networks to find a good embedding space that can better\ndistinguish text span representations from different entity classes. Extensive\nexperiments on various benchmarks show that our approach achieves superior\nperformance over prior methods.", "published": "2022-04-12 12:46:23", "link": "http://arxiv.org/abs/2204.05751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Express in Knowledge-Grounded Conversation", "abstract": "Grounding dialogue generation by extra knowledge has shown great potentials\ntowards building a system capable of replying with knowledgeable and engaging\nresponses. Existing studies focus on how to synthesize a response with proper\nknowledge, yet neglect that the same knowledge could be expressed differently\nby speakers even under the same context. In this work, we mainly consider two\naspects of knowledge expression, namely the structure of the response and style\nof the content in each part. We therefore introduce two sequential latent\nvariables to represent the structure and the content style respectively. We\npropose a segmentation-based generation model and optimize the model by a\nvariational approach to discover the underlying pattern of knowledge expression\nin a response. Evaluation results on two benchmarks indicate that our model can\nlearn the structure style defined by a few examples and generate responses in\ndesired content style.", "published": "2022-04-12 13:43:47", "link": "http://arxiv.org/abs/2204.05805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution\n  in Literary Texts", "abstract": "We present the Project Dialogism Novel Corpus, or PDNC, an annotated dataset\nof quotations for English literary texts. PDNC contains annotations for 35,978\nquotations across 22 full-length novels, and is by an order of magnitude the\nlargest corpus of its kind. Each quotation is annotated for the speaker,\naddressees, type of quotation, referring expression, and character mentions\nwithin the quotation text. The annotated attributes allow for a comprehensive\nevaluation of models of quotation attribution and coreference for literary\ntexts.", "published": "2022-04-12 14:23:55", "link": "http://arxiv.org/abs/2204.05836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Full Length Wikipedia Biographies: The Impact of Gender Bias\n  on the Retrieval-Based Generation of Women Biographies", "abstract": "Generating factual, long-form text such as Wikipedia articles raises three\nkey challenges: how to gather relevant evidence, how to structure information\ninto well-formed text, and how to ensure that the generated text is factually\ncorrect. We address these by developing a model for English text that uses a\nretrieval mechanism to identify relevant supporting information on the web and\na cache-based pre-trained encoder-decoder to generate long-form biographies\nsection by section, including citation information. To assess the impact of\navailable web evidence on the output text, we compare the performance of our\napproach when generating biographies about women (for which less information is\navailable on the web) vs. biographies generally. To this end, we curate a\ndataset of 1,500 biographies about women. We analyze our generated text to\nunderstand how differences in available web evidence data affect generation. We\nevaluate the factuality, fluency, and quality of the generated texts using\nautomatic metrics and human evaluation. We hope that these techniques can be\nused as a starting point for human writers, to aid in reducing the complexity\ninherent in the creation of long-form, factual text.", "published": "2022-04-12 15:16:57", "link": "http://arxiv.org/abs/2204.05879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking", "abstract": "Dialogue State Tracking (DST), a crucial component of task-oriented dialogue\n(ToD) systems, keeps track of all important information pertaining to dialogue\nhistory: filling slots with the most probable values throughout the\nconversation. Existing methods generally rely on a predefined set of values and\nstruggle to generalise to previously unseen slots in new domains. To overcome\nthese challenges, we propose a domain-agnostic extractive question answering\n(QA) approach with shared weights across domains. To disentangle the complex\ndomain information in ToDs, we train our DST with a novel domain filtering\nstrategy by excluding out-of-domain question samples. With an independent\nclassifier that predicts the presence of multiple domains given the context,\nour model tackles DST by extracting spans in active domains. Empirical results\ndemonstrate that our model can efficiently leverage domain-agnostic QA datasets\nby two-stage fine-tuning while being both domain-scalable and open-vocabulary\nin DST. It shows strong transferability by achieving zero-shot\ndomain-adaptation results on MultiWOZ 2.1 with an average JGA of 36.7%. It\nfurther achieves cross-lingual transfer with state-of-the-art zero-shot\nresults, 66.2% JGA from English to German and 75.7% JGA from English to Italian\non WOZ 2.0.", "published": "2022-04-12 15:45:32", "link": "http://arxiv.org/abs/2204.05895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explore More Guidance: A Task-aware Instruction Network for Sign\n  Language Translation Enhanced with Data Augmentation", "abstract": "Sign language recognition and translation first uses a recognition module to\ngenerate glosses from sign language videos and then employs a translation\nmodule to translate glosses into spoken sentences. Most existing works focus on\nthe recognition step, while paying less attention to sign language translation.\nIn this work, we propose a task-aware instruction network, namely TIN-SLT, for\nsign language translation, by introducing the instruction module and the\nlearning-based feature fuse strategy into a Transformer network. In this way,\nthe pre-trained model's language ability can be well explored and utilized to\nfurther boost the translation performance. Moreover, by exploring the\nrepresentation space of sign language glosses and target spoken language, we\npropose a multi-level data augmentation scheme to adjust the data distribution\nof the training set. We conduct extensive experiments on two challenging\nbenchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method\noutperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code\nis published at https://github.com/yongcaoplus/TIN-SLT.", "published": "2022-04-12 17:09:44", "link": "http://arxiv.org/abs/2204.05953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantified Reproducibility Assessment of NLP Results", "abstract": "This paper describes and tests a method for carrying out quantified\nreproducibility assessment (QRA) that is based on concepts and definitions from\nmetrology. QRA produces a single score estimating the degree of reproducibility\nof a given system and evaluation measure, on the basis of the scores from, and\ndifferences between, different reproductions. We test QRA on 18 system and\nevaluation measure combinations (involving diverse NLP tasks and types of\nevaluation), for each of which we have the original results and one to seven\nreproduction results. The proposed QRA method produces\ndegree-of-reproducibility scores that are comparable across multiple\nreproductions not only of the same, but of different original studies. We find\nthat the proposed method facilitates insights into causes of variation between\nreproductions, and allows conclusions to be drawn about what changes to system\nand/or evaluation design might lead to improved reproducibility.", "published": "2022-04-12 17:22:46", "link": "http://arxiv.org/abs/2204.05961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as\n  a Multi-Task Problem", "abstract": "We propose an autoregressive entity linking model, that is trained with two\nauxiliary tasks, and learns to re-rank generated samples at inference time. Our\nproposed novelties address two weaknesses in the literature. First, a recent\nmethod proposes to learn mention detection and then entity candidate selection,\nbut relies on predefined sets of candidates. We use encoder-decoder\nautoregressive entity linking in order to bypass this need, and propose to\ntrain mention detection as an auxiliary task instead. Second, previous work\nsuggests that re-ranking could help correct prediction errors. We add a new,\nauxiliary task, match prediction, to learn re-ranking. Without the use of a\nknowledge base or candidate sets, our model sets a new state of the art in two\nbenchmark datasets of entity linking: COMETA in the biomedical domain, and\nAIDA-CoNLL in the news domain. We show through ablation studies that each of\nthe two auxiliary tasks increases performance, and that re-ranking is an\nimportant factor to the increase. Finally, our low-resource experimental\nresults suggest that performance on the main task benefits from the knowledge\nlearned by the auxiliary tasks, and not just from the additional training data.", "published": "2022-04-12 17:55:22", "link": "http://arxiv.org/abs/2204.05990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022", "abstract": "In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.", "published": "2022-04-12 18:30:20", "link": "http://arxiv.org/abs/2204.06028v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASQA: Factoid Questions Meet Long-Form Answers", "abstract": "An abundance of datasets and availability of reliable evaluation metrics have\nresulted in strong progress in factoid question answering (QA). This progress,\nhowever, does not easily transfer to the task of long-form QA, where the goal\nis to answer questions that require in-depth explanations. The hurdles include\n(i) a lack of high-quality data, and (ii) the absence of a well-defined notion\nof the answer's quality. In this work, we address these problems by (i)\nreleasing a novel dataset and a task that we call ASQA (Answer Summaries for\nQuestions which are Ambiguous); and (ii) proposing a reliable metric for\nmeasuring performance on ASQA. Our task focuses on factoid questions that are\nambiguous, that is, have different correct answers depending on interpretation.\nAnswers to ambiguous questions should synthesize factual information from\nmultiple sources into a long-form summary that resolves the ambiguity. In\ncontrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear\nnotion of correctness: a user faced with a good summary should be able to\nanswer different interpretations of the original ambiguous question. We use\nthis notion of correctness to define an automated metric of performance for\nASQA. Our analysis demonstrates an agreement between this metric and human\njudgments, and reveals a considerable gap between human performance and strong\nbaselines.", "published": "2022-04-12 21:58:44", "link": "http://arxiv.org/abs/2204.06092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Easy Adaptation to Mitigate Gender Bias in Multilingual Text\n  Classification", "abstract": "Existing approaches to mitigate demographic biases evaluate on monolingual\ndata, however, multilingual data has not been examined. In this work, we treat\nthe gender as domains (e.g., male vs. female) and present a standard domain\nadaptation model to reduce the gender bias and improve performance of text\nclassifiers under multilingual settings. We evaluate our approach on two text\nclassification tasks, hate speech detection and rating prediction, and\ndemonstrate the effectiveness of our approach with three fair-aware baselines.", "published": "2022-04-12 01:15:36", "link": "http://arxiv.org/abs/2204.05459v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "GERE: Generative Evidence Retrieval for Fact Verification", "abstract": "Fact verification (FV) is a challenging task which aims to verify a claim\nusing multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.\nMost existing approaches follow a three-step pipeline framework, including\ndocument retrieval, sentence retrieval and claim verification. High-quality\nevidences provided by the first two steps are the foundation of the effective\nreasoning in the last step. Despite being important, high-quality evidences are\nrarely studied by existing works for FV, which often adopt the off-the-shelf\nmodels to retrieve relevant documents and sentences in an\n\"index-retrieve-then-rank\" fashion. This classical approach has clear drawbacks\nas follows: i) a large document index as well as a complicated search process\nis required, leading to considerable memory and computational overhead; ii)\nindependent scoring paradigms fail to capture the interactions among documents\nand sentences in ranking; iii) a fixed number of sentences are selected to form\nthe final evidence set. In this work, we propose GERE, the first system that\nretrieves evidences in a generative fashion, i.e., generating the document\ntitles as well as evidence sentence identifiers. This enables us to mitigate\nthe aforementioned technical issues since: i) the memory and computational cost\nis greatly reduced because the document index is eliminated and the heavy\nranking process is replaced by a light generative process; ii) the dependency\nbetween documents and that between sentences could be captured via sequential\ngeneration process; iii) the generative formulation allows us to dynamically\nselect a precise set of relevant evidences for each claim. The experimental\nresults on the FEVER dataset show that GERE achieves significant improvements\nover the state-of-the-art baselines, with both time-efficiency and\nmemory-efficiency.", "published": "2022-04-12 03:49:35", "link": "http://arxiv.org/abs/2204.05511v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Faithfulness Metrics for Model Interpretability\n  Methods", "abstract": "Interpretation methods to reveal the internal reasoning processes behind\nmachine learning models have attracted increasing attention in recent years. To\nquantify the extent to which the identified interpretations truly reflect the\nintrinsic decision-making mechanisms, various faithfulness evaluation metrics\nhave been proposed. However, we find that different faithfulness metrics show\nconflicting preferences when comparing different interpretations. Motivated by\nthis observation, we aim to conduct a comprehensive and comparative study of\nthe widely adopted faithfulness metrics. In particular, we introduce two\nassessment dimensions, namely diagnosticity and time complexity. Diagnosticity\nrefers to the degree to which the faithfulness metric favours relatively\nfaithful interpretations over randomly generated ones, and time complexity is\nmeasured by the average number of model forward passes. According to the\nexperimental results, we find that sufficiency and comprehensiveness metrics\nhave higher diagnosticity and lower time complexity than the other faithfulness\nmetric", "published": "2022-04-12 04:02:17", "link": "http://arxiv.org/abs/2204.05514v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trigger-GNN: A Trigger-Based Graph Neural Network for Nested Named\n  Entity Recognition", "abstract": "Nested named entity recognition (NER) aims to identify the entity boundaries\nand recognize categories of the named entities in a complex hierarchical\nsentence. Some works have been done using character-level, word-level, or\nlexicon-level based models. However, such researches ignore the role of the\ncomplementary annotations. In this paper, we propose a trigger-based graph\nneural network (Trigger-GNN) to leverage the nested NER. It obtains the\ncomplementary annotation embeddings through entity trigger encoding and\nsemantic matching, and tackle nested entity utilizing an efficient graph\nmessage passing architecture, aggregation-update mode. We posit that using\nentity triggers as external annotations can add in complementary supervision\nsignals on the whole sentences. It helps the model to learn and generalize more\nefficiently and cost-effectively. Experiments show that the Trigger-GNN\nconsistently outperforms the baselines on four public NER datasets, and it can\neffectively alleviate the nested NER.", "published": "2022-04-12 04:15:39", "link": "http://arxiv.org/abs/2204.05518v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How does fake news use a thumbnail? CLIP-based Multimodal Detection on\n  the Unrepresentative News Image", "abstract": "This study investigates how fake news uses a thumbnail for a news article\nwith a focus on whether a news article's thumbnail represents the news content\ncorrectly. A news article shared with an irrelevant thumbnail can mislead\nreaders into having a wrong impression of the issue, especially in social media\nenvironments where users are less likely to click the link and consume the\nentire content. We propose to capture the degree of semantic incongruity in the\nmultimodal relation by using the pretrained CLIP representation. From a\nsource-level analysis, we found that fake news employs a more incongruous image\nto the main content than general news. Going further, we attempted to detect\nnews articles with image-text incongruity. Evaluation experiments suggest that\nCLIP-based methods can successfully detect news articles in which the thumbnail\nis semantically irrelevant to news text. This study contributes to the research\nby providing a novel view on tackling online fake news and misinformation. Code\nand datasets are available at\nhttps://github.com/ssu-humane/fake-news-thumbnail.", "published": "2022-04-12 05:38:14", "link": "http://arxiv.org/abs/2204.05533v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ASR in German: A Detailed Error Analysis", "abstract": "The amount of freely available systems for automatic speech recognition (ASR)\nbased on neural networks is growing steadily, with equally increasingly\nreliable predictions. However, the evaluation of trained models is typically\nexclusively based on statistical metrics such as WER or CER, which do not\nprovide any insight into the nature or impact of the errors produced when\npredicting transcripts from speech input. This work presents a selection of ASR\nmodel architectures that are pretrained on the German language and evaluates\nthem on a benchmark of diverse test datasets. It identifies cross-architectural\nprediction errors, classifies those into categories and traces the sources of\nerrors per category back into training data as well as other sources. Finally,\nit discusses solutions in order to create qualitatively better training\ndatasets and more robust ASR systems.", "published": "2022-04-12 08:25:01", "link": "http://arxiv.org/abs/2204.05617v1", "categories": ["cs.CL", "cs.AI", "C.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Creativity in translation: machine translation as a constraint for\n  literary texts", "abstract": "This article presents the results of a study involving the translation of a\nshort story by Kurt Vonnegut from English to Catalan and Dutch using three\nmodalities: machine-translation (MT), post-editing (PE) and translation without\naid (HT). Our aim is to explore creativity, understood to involve novelty and\nacceptability, from a quantitative perspective. The results show that HT has\nthe highest creativity score, followed by PE, and lastly, MT, and this is\nunanimous from all reviewers. A neural MT system trained on literary data does\nnot currently have the necessary capabilities for a creative translation; it\nrenders literal solutions to translation problems. More importantly, using MT\nto post-edit raw output constrains the creativity of translators, resulting in\na poorer translation often not fit for publication, according to experts.", "published": "2022-04-12 09:27:00", "link": "http://arxiv.org/abs/2204.05655v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "abstract": "We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.", "published": "2022-04-12 15:02:38", "link": "http://arxiv.org/abs/2204.05862v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Dialogue Policy for Continual Reinforcement Learning", "abstract": "Continual learning is one of the key components of human learning and a\nnecessary requirement of artificial intelligence. As dialogue can potentially\nspan infinitely many topics and tasks, a task-oriented dialogue system must\nhave the capability to continually learn, dynamically adapting to new\nchallenges while preserving the knowledge it already acquired. Despite the\nimportance, continual reinforcement learning of the dialogue policy has\nremained largely unaddressed. The lack of a framework with training protocols,\nbaseline models and suitable metrics, has so far hindered research in this\ndirection. In this work we fill precisely this gap, enabling research in\ndialogue policy optimisation to go from static to dynamic learning. We provide\na continual learning algorithm, baseline architectures and metrics for\nassessing continual learning models. Moreover, we propose the dynamic dialogue\npolicy transformer (DDPT), a novel dynamic architecture that can integrate new\nknowledge seamlessly, is capable of handling large state spaces and obtains\nsignificant zero-shot performance when being exposed to unseen domains, without\nany growth in network parameter size.", "published": "2022-04-12 16:30:40", "link": "http://arxiv.org/abs/2204.05928v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mining Logical Event Schemas From Pre-Trained Language Models", "abstract": "We present NESL (the Neuro-Episodic Schema Learner), an event schema learning\nsystem that combines large language models, FrameNet parsing, a powerful\nlogical representation of language, and a set of simple behavioral schemas\nmeant to bootstrap the learning process. In lieu of a pre-made corpus of\nstories, our dataset is a continuous feed of \"situation samples\" from a\npre-trained language model, which are then parsed into FrameNet frames, mapped\ninto simple behavioral schemas, and combined and generalized into complex,\nhierarchical schemas for a variety of everyday scenarios. We show that careful\nsampling from the language model can help emphasize stereotypical properties of\nsituations and de-emphasize irrelevant details, and that the resulting schemas\nspecify situations more comprehensively than those learned by other systems.", "published": "2022-04-12 16:41:18", "link": "http://arxiv.org/abs/2204.05939v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression\n  Comprehension", "abstract": "Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.", "published": "2022-04-12 17:55:38", "link": "http://arxiv.org/abs/2204.05991v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT\n  models", "abstract": "Named Entity Recognition (NER) is a basic NLP task and finds major\napplications in conversational and search systems. It helps us identify key\nentities in a sentence used for the downstream application. NER or similar slot\nfilling systems for popular languages have been heavily used in commercial\napplications. In this work, we focus on Marathi, an Indian language, spoken\nprominently by the people of Maharashtra state. Marathi is a low resource\nlanguage and still lacks useful NER resources. We present L3Cube-MahaNER, the\nfirst major gold standard named entity recognition dataset in Marathi. We also\ndescribe the manual annotation guidelines followed during the process. In the\nend, we benchmark the dataset on different CNN, LSTM, and Transformer based\nmodels like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides\nthe best performance among all the models. The data and models are available at\nhttps://github.com/l3cube-pune/MarathiNLP .", "published": "2022-04-12 18:32:15", "link": "http://arxiv.org/abs/2204.06029v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Review on Language Models as Knowledge Bases", "abstract": "Recently, there has been a surge of interest in the NLP community on the use\nof pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have\nshown that LMs trained on a sufficiently large (web) corpus will encode a\nsignificant amount of knowledge implicitly in its parameters. The resulting LM\ncan be probed for different kinds of knowledge and thus acting as a KB. This\nhas a major advantage over traditional KBs in that this method requires no\nhuman supervision. In this paper, we present a set of aspects that we deem a LM\nshould have to fully act as a KB, and review the recent literature with respect\nto those aspects.", "published": "2022-04-12 18:35:23", "link": "http://arxiv.org/abs/2204.06031v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finding Trolls Under Bridges: Preliminary Work on a Motif Detector", "abstract": "Motifs are distinctive recurring elements found in folklore that have\nsignificance as communicative devices in news, literature, press releases, and\npropaganda. Motifs concisely imply a large constellation of culturally-relevant\ninformation, and their broad usage suggests their cognitive importance as\ntouchstones of cultural knowledge, making their detection a worthy step toward\nculturally-aware natural language processing tasks. Until now, folklorists and\nothers interested in motifs have only extracted motifs from narratives\nmanually. We present a preliminary report on the development of a system for\nautomatically detecting motifs. We briefly describe an annotation effort to\nproduce data for training motif detection, which is on-going. We describe our\nin-progress architecture in detail, which aims to capture, in part, how people\ndetermine whether or not a motif candidate is being used in a motific way. This\ndescription includes a test of an off-the-shelf metaphor detector as a feature\nfor motif detection, which achieves a F1 of 0.35 on motifs and a macro-average\nF1 of 0.21 across four categories which we assign to motif candidates.", "published": "2022-04-12 21:26:49", "link": "http://arxiv.org/abs/2204.06085v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CorrectSpeech: A Fully Automated System for Speech Correction and Accent\n  Reduction", "abstract": "This study propose a fully automated system for speech correction and accent\nreduction. Consider the application scenario that a recorded speech audio\ncontains certain errors, e.g., inappropriate words, mispronunciations, that\nneed to be corrected. The proposed system, named CorrectSpeech, performs the\ncorrection in three steps: recognizing the recorded speech and converting it\ninto time-stamped symbol sequence, aligning recognized symbol sequence with\ntarget text to determine locations and types of required edit operations, and\ngenerating the corrected speech. Experiments show that the quality and\nnaturalness of corrected speech depend on the performance of speech recognition\nand alignment modules, as well as the granularity level of editing operations.\nThe proposed system is evaluated on two corpora: a manually perturbed version\nof VCTK and L2-ARCTIC. The results demonstrate that our system is able to\ncorrect mispronunciation and reduce accent in speech recordings. Audio samples\nare available online for demonstration\nhttps://daxintan-cuhk.github.io/CorrectSpeech/ .", "published": "2022-04-12 01:20:29", "link": "http://arxiv.org/abs/2204.05460v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Solving Price Per Unit Problem Around the World: Formulating Fact\n  Extraction as Question Answering", "abstract": "Price Per Unit (PPU) is an essential information for consumers shopping on\ne-commerce websites when comparing products. Finding total quantity in a\nproduct is required for computing PPU, which is not always provided by the\nsellers. To predict total quantity, all relevant quantities given in a product\nattributes such as title, description and image need to be inferred correctly.\nWe formulate this problem as a question-answering (QA) task rather than named\nentity recognition (NER) task for fact extraction. In our QA approach, we first\npredict the unit of measure (UoM) type (e.g., volume, weight or count), that\nformulates the desired question (e.g., \"What is the total volume?\") and then\nuse this question to find all the relevant answers. Our model architecture\nconsists of two subnetworks for the two subtasks: a classifier to predict UoM\ntype (or the question) and an extractor to extract the relevant quantities. We\nuse a deep character-level CNN architecture for both subtasks, which enables\n(1) easy expansion to new stores with similar alphabets, (2) multi-span\nanswering due to its span-image architecture and (3) easy deployment by keeping\nmodel-inference latency low. Our QA approach outperforms rule-based methods by\n34.4% in precision and also BERT-based fact extraction approach in all stores\nglobally, with largest precision lift of 10.6% in the US store.", "published": "2022-04-12 06:43:48", "link": "http://arxiv.org/abs/2204.05555v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stylized Knowledge-Grounded Dialogue Generation via Disentangled\n  Template Rewriting", "abstract": "Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in\nproducing rational and factual responses. However, to establish long-term\nrelationships with users, the KDG model needs the capability to generate\nresponses in a desired style or attribute. Thus, we study a new problem:\nStylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two\nchallenges: (1) How to train a SKDG model where no <context, knowledge,\nstylized response> triples are available. (2) How to cohere with context and\npreserve the knowledge when generating a stylized response. In this paper, we\npropose a novel disentangled template rewriting (DTR) method which generates\nresponses via combing disentangled style templates (from monolingual stylized\ncorpus) and content templates (from KDG corpus). The entire framework is\nend-to-end differentiable and learned without supervision. Extensive\nexperiments on two benchmarks indicate that DTR achieves a significant\nimprovement on all evaluation metrics compared with previous state-of-the-art\nstylized dialogue generation methods. Besides, DTR achieves comparable\nperformance with the state-of-the-art KDG methods in standard KDG evaluation\nsetting.", "published": "2022-04-12 08:17:21", "link": "http://arxiv.org/abs/2204.05610v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks", "abstract": "In this paper, we study the challenging instance-wise vision-language tasks,\nwhere the free-form language is required to align with the objects instead of\nthe whole image. To address these tasks, we propose X-DETR, whose architecture\nhas three major components: an object detector, a language encoder, and\nvision-language alignment. The vision and language streams are independent\nuntil the end and they are aligned using an efficient dot-product operation.\nThe whole network is trained end-to-end, such that the detector is optimized\nfor the vision-language tasks instead of an off-the-shelf component. To\novercome the limited size of paired object-language annotations, we leverage\nother weak types of supervision to expand the knowledge coverage. This simple\nyet effective architecture of X-DETR shows good accuracy and fast speeds for\nmultiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection\nof 1.2K categories at ~20 frames per second without using any LVIS annotation\nduring training.", "published": "2022-04-12 08:34:42", "link": "http://arxiv.org/abs/2204.05626v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning\n  Tasks", "abstract": "Given the ubiquitous nature of numbers in text, reasoning with numbers to\nperform simple calculations is an important skill of AI systems. While many\ndatasets and models have been developed to this end, state-of-the-art AI\nsystems are brittle; failing to perform the underlying mathematical reasoning\nwhen they appear in a slightly different scenario. Drawing inspiration from\nGLUE that was proposed in the context of natural language understanding, we\npropose NumGLUE, a multi-task benchmark that evaluates the performance of AI\nsystems on eight different tasks, that at their core require simple arithmetic\nunderstanding. We show that this benchmark is far from being solved with neural\nmodels including state-of-the-art large-scale language models performing\nsignificantly worse than humans (lower by 46.4%). Further, NumGLUE promotes\nsharing knowledge across tasks, especially those with limited training data as\nevidenced by the superior performance (average gain of 3.4% on each task) when\na model is jointly trained on all the tasks as opposed to task-specific\nmodeling. Finally, we hope that NumGLUE will encourage systems that perform\nrobust and general arithmetic reasoning within language, a first step towards\nbeing able to perform more complex mathematical reasoning.", "published": "2022-04-12 09:36:10", "link": "http://arxiv.org/abs/2204.05660v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MuCoT: Multilingual Contrastive Training for Question-Answering in\n  Low-resource Languages", "abstract": "Accuracy of English-language Question Answering (QA) systems has improved\nsignificantly in recent years with the advent of Transformer-based models\n(e.g., BERT). These models are pre-trained in a self-supervised fashion with a\nlarge English text corpus and further fine-tuned with a massive English QA\ndataset (e.g., SQuAD). However, QA datasets on such a scale are not available\nfor most of the other languages. Multi-lingual BERT-based models (mBERT) are\noften used to transfer knowledge from high-resource languages to low-resource\nlanguages. Since these models are pre-trained with huge text corpora containing\nmultiple languages, they typically learn language-agnostic embeddings for\ntokens from different languages. However, directly training an mBERT-based QA\nsystem for low-resource languages is challenging due to the paucity of training\ndata. In this work, we augment the QA samples of the target language using\ntranslation and transliteration into other languages and use the augmented data\nto fine-tune an mBERT-based QA model, which is already pre-trained in English.\nExperiments on the Google ChAII dataset show that fine-tuning the mBERT model\nwith translations from the same language family boosts the question-answering\nperformance, whereas the performance degrades in the case of cross-language\nfamilies. We further show that introducing a contrastive loss between the\ntranslated question-context feature pairs during the fine-tuning process,\nprevents such degradation with cross-lingual family translations and leads to\nmarginal improvement. The code for this work is available at\nhttps://github.com/gokulkarthik/mucot.", "published": "2022-04-12 13:52:54", "link": "http://arxiv.org/abs/2204.05814v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Language Model Architecture and Pretraining Objective Work Best for\n  Zero-Shot Generalization?", "abstract": "Large pretrained Transformer language models have been shown to exhibit\nzero-shot generalization, i.e. they can perform a wide variety of tasks that\nthey were not explicitly trained on. However, the architectures and pretraining\nobjectives used across state-of-the-art models differ significantly, and there\nhas been limited systematic comparison of these factors. In this work, we\npresent a large-scale evaluation of modeling choices and their impact on\nzero-shot generalization. In particular, we focus on text-to-text models and\nexperiment with three model architectures (causal/non-causal decoder-only and\nencoder-decoder), trained with two different pretraining objectives\n(autoregressive and masked language modeling), and evaluated with and without\nmultitask prompted finetuning. We train models with over 5 billion parameters\nfor more than 170 billion tokens, thereby increasing the likelihood that our\nconclusions will transfer to even larger scales. Our experiments show that\ncausal decoder-only models trained on an autoregressive language modeling\nobjective exhibit the strongest zero-shot generalization after purely\nunsupervised pretraining. However, models with non-causal visibility on their\ninput trained with a masked language modeling objective followed by multitask\nfinetuning perform the best among our experiments. We therefore consider the\nadaptation of pretrained models across architectures and objectives. We find\nthat pretrained non-causal decoder models can be adapted into performant\ngenerative causal decoder models, using autoregressive language modeling as a\ndownstream task. Furthermore, we find that pretrained causal decoder models can\nbe efficiently adapted into non-causal decoder models, ultimately achieving\ncompetitive performance after multitask finetuning. Code and checkpoints are\navailable at https://github.com/bigscience-workshop/architecture-objective.", "published": "2022-04-12 14:19:49", "link": "http://arxiv.org/abs/2204.05832v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "InCoder: A Generative Model for Code Infilling and Synthesis", "abstract": "Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models", "published": "2022-04-12 16:25:26", "link": "http://arxiv.org/abs/2204.05999v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Deep Annotation of Therapeutic Working Alliance in Psychotherapy", "abstract": "The therapeutic working alliance is an important predictor of the outcome of\nthe psychotherapy treatment. In practice, the working alliance is estimated\nfrom a set of scoring questionnaires in an inventory that both the patient and\nthe therapists fill out. In this work, we propose an analytical framework of\ndirectly inferring the therapeutic working alliance from the natural language\nwithin the psychotherapy sessions in a turn-level resolution with deep\nembeddings such as the Doc2Vec and SentenceBERT models. The transcript of each\npsychotherapy session can be transcribed and generated in real-time from the\nsession speech recordings, and these embedded dialogues are compared with the\ndistributed representations of the statements in the working alliance\ninventory. We demonstrate, in a real-world dataset with over 950 sessions of\npsychotherapy treatments in anxiety, depression, schizophrenia and suicidal\npatients, the effectiveness of this method in mapping out trajectories of\npatient-therapist alignment and the interpretability that can offer insights in\nclinical psychiatry. We believe such a framework can be provide timely feedback\nto the therapist regarding the quality of the conversation in interview\nsessions.", "published": "2022-04-12 04:42:51", "link": "http://arxiv.org/abs/2204.05522v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Low Latency Time Domain Multichannel Speech and Music Source Separation", "abstract": "The Goal is to obtain a simple multichannel source separation with very low\nlatency. Applications can be teleconferencing, hearing aids, augmented reality,\nor selective active noise cancellation. These real time applications need a\nvery low latency, usually less than about 6 ms, and low complexity, because\nthey usually run on small portable devices. For that we don't need the best\nseparation, but \"useful\" separation, and not just on speech, but also music and\nnoise. Usual frequency domain approaches have higher latency and complexity.\nHence we introduce a novel probabilistic optimization method which we call\n\"Random Directions\", which can overcome local minima, applied to a simple time\ndomain unmixing structure, and which is scalable for low complexity. Then it is\ncompared to frequency domain approaches on separating speech and music sources,\nand using 3D microphone setups.", "published": "2022-04-12 08:17:01", "link": "http://arxiv.org/abs/2204.05609v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion\n  Recognition", "abstract": "Music emotion recognition (MER), a sub-task of music information retrieval\n(MIR), has developed rapidly in recent years. However, the learning of\naffect-salient features remains a challenge. In this paper, we propose an\nend-to-end attention-based deep feature fusion (ADFF) approach for MER. Only\ntaking log Mel-spectrogram as input, this method uses adapted VGGNet as spatial\nfeature learning module (SFLM) to obtain spatial features across different\nlevels. Then, these features are fed into squeeze-and-excitation (SE)\nattention-based temporal feature learning module (TFLM) to get multi-level\nemotion-related spatial-temporal features (ESTFs), which can discriminate\nemotions well in the final emotion space. In addition, a novel data processing\nis devised to cut the single-channel input into multi-channel to improve\ncalculative efficiency while ensuring the quality of MER. Experiments show that\nour proposed method achieves 10.43% and 4.82% relative improvement of valence\nand arousal respectively on the R2 score compared to the state-of-the-art\nmodel, meanwhile, performs better on datasets with distinct scales and in\nmulti-task learning.", "published": "2022-04-12 09:15:48", "link": "http://arxiv.org/abs/2204.05649v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-Driven Separation of Arbitrary Sounds", "abstract": "We propose a method of separating a desired sound source from a\nsingle-channel mixture, based on either a textual description or a short audio\nsample of the target source. This is achieved by combining two distinct models.\nThe first model, SoundWords, is trained to jointly embed both an audio clip and\nits textual description to the same embedding in a shared representation. The\nsecond model, SoundFilter, takes a mixed source audio clip as an input and\nseparates it based on a conditioning vector from the shared text-audio\nrepresentation defined by SoundWords, making the model agnostic to the\nconditioning modality. Evaluating on multiple datasets, we show that our\napproach can achieve an SI-SDR of 9.1 dB for mixtures of two arbitrary sounds\nwhen conditioned on text and 10.1 dB when conditioned on audio. We also show\nthat SoundWords is effective at learning co-embeddings and that our multi-modal\ntraining approach improves the performance of SoundFilter.", "published": "2022-04-12 12:26:01", "link": "http://arxiv.org/abs/2204.05738v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Post Auto-regressive GAN Vocoder Focused on Spectrum Fracture", "abstract": "Generative adversarial networks (GANs) have been indicated their superiority\nin usage of the real-time speech synthesis. Nevertheless, most of them make use\nof deep convolutional layers as their backbone, which may cause the absence of\nprevious signal information. However, the generation of speech signals\ninvariably require preceding waveform samples in its reconstruction, as the\nlack of this can lead to artifacts in generated speech. To address this\nconflict, in this paper, we propose an improved model: a post auto-regressive\n(AR) GAN vocoder with a self-attention layer, which merging self-attention in\nan AR loop. It will not participate in inference, but can assist the generator\nto learn temporal dependencies within frames in training. Furthermore, an\nablation study was done to confirm the contribution of each part. Systematic\nexperiments show that our model leads to a consistent improvement on both\nobjective and subjective evaluation performance.", "published": "2022-04-12 21:33:10", "link": "http://arxiv.org/abs/2204.06086v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition with Global-Aware Fusion on Multi-scale\n  Feature Representation", "abstract": "Speech Emotion Recognition (SER) is a fundamental task to predict the emotion\nlabel from speech data. Recent works mostly focus on using convolutional neural\nnetworks~(CNNs) to learn local attention map on fixed-scale feature\nrepresentation by viewing time-varied spectral features as images. However,\nrich emotional feature at different scales and important global information are\nnot able to be well captured due to the limits of existing CNNs for SER. In\nthis paper, we propose a novel GLobal-Aware Multi-scale (GLAM) neural network\n(The code is available at https://github.com/lixiangucas01/GLAM) to learn\nmulti-scale feature representation with global-aware fusion module to attend\nemotional information. Specifically, GLAM iteratively utilizes multiple\nconvolutional kernels with different scales to learn multiple feature\nrepresentation. Then, instead of using attention-based methods, a simple but\neffective global-aware fusion module is applied to grab most important\nemotional information globally. Experiments on the benchmark corpus IEMOCAP\nover four emotions demonstrates the superiority of our proposed model with 2.5%\nto 4.5% improvements on four common metrics compared to previous\nstate-of-the-art approaches.", "published": "2022-04-12 07:03:04", "link": "http://arxiv.org/abs/2204.05571v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Assessment of convolutional recurrent autoencoder network for learning\n  wave propagation", "abstract": "It is challenging to construct generalized physical models of wave\npropagation in nature owing to their complex physics as well as widely varying\nenvironmental parameters and dynamical scales. In this article, we present the\nconvolutional autoencoder recurrent network (CRAN) as a data-driven model for\nlearning wave propagation phenomena. The CRAN consists of a convolutional\nautoencoder for learning low-dimensional system representation and a long\nshort-term memory recurrent neural network for the system evolution in low\ndimension. We show that the convolutional autoencoder significantly outperforms\nthe dimension-reduction of complex wave propagation phenomena via\nprojection-based methods as it can directly learn subspaces resembling wave\ncharacteristics. On the other hand, the projection-based modes are restricted\nto the Fourier subspace. Geometric priors of the convolutional autoencoder\nenabling selective scale separation of complex wave dynamics further enhance\nits dimension-reduction capability. We also demonstrate that geometric priors\nsuch as translation equivariance and translational invariance of the\nconvolutional autoencoder enable generalized learning of low-dimensional maps.\nThus, the composite CRAN model connecting the convolutional autoencoder with a\nlong short-term memory network specially designed for autoregressive modeling\ncan perform generalized wave propagation prediction over the desired time\nhorizon. Numerical experiments display 90% mean structural similarity index\nmeasure of CRAN predictions compared to true solutions for out-of-training\ncases, and less than 10% pointwise $L_1$ error for most cases, verifying such\ngeneralization claims. Finally, the CRAN predictions offer similar wave\ncharacteristic patterns to the target solutions indicating not only their\ngeneralization but also their kinematical consistency.", "published": "2022-04-12 07:09:03", "link": "http://arxiv.org/abs/2204.05573v2", "categories": ["physics.flu-dyn", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "physics.flu-dyn"}
{"title": "Enhancement of Pitch Controllability using Timbre-Preserving Pitch\n  Augmentation in FastPitch", "abstract": "The recently developed pitch-controllable text-to-speech (TTS) model, i.e.\nFastPitch, was conditioned for the pitch contours. However, the quality of the\nsynthesized speech degraded considerably for pitch values that deviated\nsignificantly from the average pitch; i.e. the ability to control pitch was\nlimited. To address this issue, we propose two algorithms to improve the\nrobustness of FastPitch. First, we propose a novel timbre-preserving\npitch-shifting algorithm for natural pitch augmentation. Pitch-shifted speech\nsamples sound more natural when using the proposed algorithm because the\nspeaker's vocal timbre is maintained. Moreover, we propose a training algorithm\nthat defines FastPitch using pitch-augmented speech datasets with different\npitch ranges for the same sentence. The experimental results demonstrate that\nthe proposed algorithms improve the pitch controllability of FastPitch.", "published": "2022-04-12 12:48:06", "link": "http://arxiv.org/abs/2204.05753v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration", "abstract": "Speech restoration aims to remove distortions in speech signals. Prior\nmethods mainly focus on a single type of distortion, such as speech denoising\nor dereverberation. However, speech signals can be degraded by several\ndifferent distortions simultaneously in the real world. It is thus important to\nextend speech restoration models to deal with multiple distortions. In this\npaper, we introduce VoiceFixer, a unified framework for high-fidelity speech\nrestoration. VoiceFixer restores speech from multiple distortions (e.g., noise,\nreverberation, and clipping) and can expand degraded speech (e.g., noisy\nspeech) with a low bandwidth to 44.1 kHz full-bandwidth high-fidelity speech.\nWe design VoiceFixer based on (1) an analysis stage that predicts\nintermediate-level features from the degraded speech, and (2) a synthesis stage\nthat generates waveform using a neural vocoder. Both objective and subjective\nevaluations show that VoiceFixer is effective on severely degraded speech, such\nas real-world historical speech recordings. Samples of VoiceFixer are available\nat https://haoheliu.github.io/voicefixer.", "published": "2022-04-12 14:37:39", "link": "http://arxiv.org/abs/2204.05841v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
