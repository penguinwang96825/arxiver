{"title": "Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation\n  Models", "abstract": "Building a persona-based conversation agent is challenging owing to the lack\nof large amounts of speaker-specific conversation data for model training. This\npaper addresses the problem by proposing a multi-task learning approach to\ntraining neural conversation models that leverages both conversation data\nacross speakers and other types of data pertaining to the speaker and speaker\nroles to be modeled. Experiments show that our approach leads to significant\nimprovements over baseline model quality, generating responses that capture\nmore precisely speakers' traits and speaking styles. The model offers the\nbenefits of being algorithmically simple and easy to implement, and not relying\non large quantities of data representing specific individual speakers.", "published": "2017-10-20 01:31:31", "link": "http://arxiv.org/abs/1710.07388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised\n  Two-path Bootstrapping Approach", "abstract": "In the wake of a polarizing election, social media is laden with hateful\ncontent. To address various limitations of supervised hate speech\nclassification methods including corpus bias and huge cost of annotation, we\npropose a weakly supervised two-path bootstrapping approach for an online hate\nspeech detection model leveraging large-scale unlabeled data. This system\nsignificantly outperforms hate speech detection systems that are trained in a\nsupervised manner using manually annotated data. Applying this model on a large\nquantity of tweets collected before, after, and on election day reveals\nmotivations and patterns of inflammatory language.", "published": "2017-10-20 02:11:06", "link": "http://arxiv.org/abs/1710.07394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Online Hate Speech Using Context Aware Models", "abstract": "In the wake of a polarizing election, the cyber world is laden with hate\nspeech. Context accompanying a hate speech text is useful for identifying hate\nspeech, which however has been largely overlooked in existing datasets and hate\nspeech detection models. In this paper, we provide an annotated corpus of hate\nspeech with context information well kept. Then we propose two types of hate\nspeech detection models that incorporate context information, a logistic\nregression model with context features and a neural network model with learning\ncomponents for context. Our evaluation shows that both models outperform a\nstrong baseline by around 3% to 4% in F1 score and combining these two models\nfurther improve the performance by another 7% in F1 score.", "published": "2017-10-20 02:11:21", "link": "http://arxiv.org/abs/1710.07395v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semantically Motivated Approach to Compute ROUGE Scores", "abstract": "ROUGE is one of the first and most widely used evaluation metrics for text\nsummarization. However, its assessment merely relies on surface similarities\nbetween peer and model summaries. Consequently, ROUGE is unable to fairly\nevaluate abstractive summaries including lexical variations and paraphrasing.\nExploring the effectiveness of lexical resource-based models to address this\nissue, we adopt a graph-based algorithm into ROUGE to capture the semantic\nsimilarities between peer and model summaries. Our semantically motivated\napproach computes ROUGE scores based on both lexical and semantic similarities.\nExperiment results over TAC AESOP datasets indicate that exploiting the\nlexico-semantic similarity of the words used in summaries would significantly\nhelp ROUGE correlate better with human judgments.", "published": "2017-10-20 07:38:54", "link": "http://arxiv.org/abs/1710.07441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local Word Vectors Guiding Keyphrase Extraction", "abstract": "Automated keyphrase extraction is a fundamental textual information\nprocessing task concerned with the selection of representative phrases from a\ndocument that summarize its content. This work presents a novel unsupervised\nmethod for keyphrase extraction, whose main innovation is the use of local word\nembeddings (in particular GloVe vectors), i.e., embeddings trained from the\nsingle document under consideration. We argue that such local representation of\nwords and keyphrases are able to accurately capture their semantics in the\ncontext of the document they are part of, and therefore can help in improving\nkeyphrase extraction quality. Empirical results offer evidence that indeed\nlocal representations lead to better keyphrase extraction results compared to\nboth embeddings trained on very large third corpora or larger corpora\nconsisting of several documents of the same scientific field and to other\nstate-of-the-art unsupervised keyphrase extraction methods.", "published": "2017-10-20 12:22:15", "link": "http://arxiv.org/abs/1710.07503v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verb Pattern: A Probabilistic Semantic Representation on Verbs", "abstract": "Verbs are important in semantic understanding of natural language.\nTraditional verb representations, such as FrameNet, PropBank, VerbNet, focus on\nverbs' roles. These roles are too coarse to represent verbs' semantics. In this\npaper, we introduce verb patterns to represent verbs' semantics, such that each\npattern corresponds to a single semantic of the verb. First we analyze the\nprinciples for verb patterns: generality and specificity. Then we propose a\nnonparametric model based on description length. Experimental results prove the\nhigh effectiveness of verb patterns. We further apply verb patterns to\ncontext-aware conceptualization, to show that verb patterns are helpful in\nsemantic-related tasks.", "published": "2017-10-20 20:23:45", "link": "http://arxiv.org/abs/1710.07695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is space a word, too?", "abstract": "For words, rank-frequency distributions have long been heralded for adherence\nto a potentially-universal phenomenon known as Zipf's law. The hypothetical\nform of this empirical phenomenon was refined by Ben\\^{i}ot Mandelbrot to that\nwhich is presently referred to as the Zipf-Mandelbrot law. Parallel to this,\nHerbet Simon proposed a selection model potentially explaining Zipf's law.\nHowever, a significant dispute between Simon and Mandelbrot, notable empirical\nexceptions, and the lack of a strong empirical connection between Simon's model\nand the Zipf-Mandelbrot law have left the questions of universality and\nmechanistic generation open. We offer a resolution to these issues by\nexhibiting how the dark matter of word segmentation, i.e., space, punctuation,\netc., connect the Zipf-Mandelbrot law to Simon's mechanistic process. This\nexplains Mandelbrot's refinement as no more than a fudge factor, accommodating\nthe effects of the exclusion of the rank-frequency dark matter. Thus,\nintegrating these non-word objects resolves a more-generalized rank-frequency\nlaw. Since this relies upon the integration of space, etc., we find support for\nthe hypothesis that $all$ are generated by common processes, indicating from a\nphysical perspective that space is a word, too.", "published": "2017-10-20 23:43:26", "link": "http://arxiv.org/abs/1710.07729v1", "categories": ["cs.CL", "cond-mat.stat-mech"], "primary_category": "cs.CL"}
{"title": "Spoken Language Biomarkers for Detecting Cognitive Impairment", "abstract": "In this study we developed an automated system that evaluates speech and\nlanguage features from audio recordings of neuropsychological examinations of\n92 subjects in the Framingham Heart Study. A total of 265 features were used in\nan elastic-net regularized binomial logistic regression model to classify the\npresence of cognitive impairment, and to select the most predictive features.\nWe compared performance with a demographic model from 6,258 subjects in the\ngreater study cohort (0.79 AUC), and found that a system that incorporated both\naudio and text features performed the best (0.92 AUC), with a True Positive\nRate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow\ntest > 0.05). We also found that decreasing pitch and jitter, shorter segments\nof speech, and responses phrased as questions were positively associated with\ncognitive impairment.", "published": "2017-10-20 14:41:43", "link": "http://arxiv.org/abs/1710.07551v1", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "A Computational Framework for Multi-Modal Social Action Identification", "abstract": "We create a computational framework for understanding social action and\ndemonstrate how this framework can be used to build an open-source event\ndetection tool with scalable statistical machine learning algorithms and a\nsubsampled database of over 600 million geo-tagged Tweets from around the\nworld. These Tweets were collected between April 1st, 2014 and April 30th,\n2015, most notably when the Black Lives Matter movement began. We demonstrate\nhow these methods can be used diagnostically-by researchers, government\nofficials and the public-to understand peaceful and violent collective action\nat very fine-grained levels of time and geography.", "published": "2017-10-20 23:42:22", "link": "http://arxiv.org/abs/1710.07728v2", "categories": ["cs.SI", "cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence\n  Learning", "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\nspeech synthesis systems in naturalness while training ten times faster. We\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\nthan eight hundred hours of audio from over two thousand speakers. In addition,\nwe identify common error modes of attention-based speech synthesis networks,\ndemonstrate how to mitigate them, and compare several different waveform\nsynthesis methods. We also describe how to scale inference to ten million\nqueries per day on one single-GPU server.", "published": "2017-10-20 18:17:23", "link": "http://arxiv.org/abs/1710.07654v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
