{"title": "DxFormer: A Decoupled Automatic Diagnostic System Based on\n  Decoder-Encoder Transformer with Dense Symptom Representations", "abstract": "Diagnosis-oriented dialogue system queries the patient's health condition and\nmakes predictions about possible diseases through continuous interaction with\nthe patient. A few studies use reinforcement learning (RL) to learn the optimal\npolicy from the joint action space of symptoms and diseases. However, existing\nRL (or Non-RL) methods cannot achieve sufficiently good prediction accuracy,\nstill far from its upper limit. To address the problem, we propose a decoupled\nautomatic diagnostic framework DxFormer, which divides the diagnosis process\ninto two steps: symptom inquiry and disease diagnosis, where the transition\nfrom symptom inquiry to disease diagnosis is explicitly determined by the\nstopping criteria. In DxFormer, we treat each symptom as a token, and formalize\nthe symptom inquiry and disease diagnosis to a language generation model and a\nsequence classification model respectively. We use the inverted version of\nTransformer, i.e., the decoder-encoder structure, to learn the representation\nof symptoms by jointly optimizing the reinforce reward and cross entropy loss.\nExtensive experiments on three public real-world datasets prove that our\nproposed model can effectively learn doctors' clinical experience and achieve\nthe state-of-the-art results in terms of symptom recall and diagnostic\naccuracy.", "published": "2022-05-08 01:52:42", "link": "http://arxiv.org/abs/2205.03755v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scheduled Multi-task Learning for Neural Chat Translation", "abstract": "Neural Chat Translation (NCT) aims to translate conversational text into\ndifferent languages. Existing methods mainly focus on modeling the bilingual\ndialogue characteristics (e.g., coherence) to improve chat translation via\nmulti-task learning on small-scale chat translation data. Although the NCT\nmodels have achieved impressive success, it is still far from satisfactory due\nto insufficient chat translation data and simple joint training manners. To\naddress the above issues, we propose a scheduled multi-task learning framework\nfor NCT. Specifically, we devise a three-stage training framework to\nincorporate the large-scale in-domain chat translation data into training by\nadding a second pre-training stage between the original pre-training and\nfine-tuning stages. Further, we investigate where and how to schedule the\ndialogue-related auxiliary tasks in multiple training stages to effectively\nenhance the main chat translation task. Extensive experiments in four language\ndirections (English-Chinese and English-German) verify the effectiveness and\nsuperiority of the proposed approach. Additionally, we have made the\nlarge-scale in-domain paired bilingual dialogue dataset publicly available to\nthe research community.", "published": "2022-05-08 02:57:28", "link": "http://arxiv.org/abs/2205.03766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Abbreviation Expansion Using Large Language Models", "abstract": "Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.", "published": "2022-05-08 03:02:53", "link": "http://arxiv.org/abs/2205.03767v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Targeted Sentiment Analysis", "abstract": "Targeted Sentiment Analysis (TSA) is a central task for generating insights\nfrom consumer reviews. Such content is extremely diverse, with sites like\nAmazon or Yelp containing reviews on products and businesses from many\ndifferent domains. A real-world TSA system should gracefully handle that\ndiversity. This can be achieved by a multi-domain model -- one that is robust\nto the domain of the analyzed texts, and performs well on various domains. To\naddress this scenario, we present a multi-domain TSA system based on augmenting\na given training set with diverse weak labels from assorted domains. These are\nobtained through self-training on the Yelp reviews corpus. Extensive\nexperiments with our approach on three evaluation datasets across different\ndomains demonstrate the effectiveness of our solution. We further analyze how\nrestrictions imposed on the available labeled data affect the performance, and\ncompare the proposed method to the costly alternative of manually gathering\ndiverse TSA labeled data. Our results and analysis show that our approach is a\npromising step towards a practical domain-robust TSA system.", "published": "2022-05-08 07:40:36", "link": "http://arxiv.org/abs/2205.03804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MASALA: Modelling and Analysing the Semantics of Adpositions in\n  Linguistic Annotation of Hindi", "abstract": "We present a completed, publicly available corpus of annotated semantic\nrelations of adpositions and case markers in Hindi. We used the multilingual\nSNACS annotation scheme, which has been applied to a variety of typologically\ndiverse languages. Building on past work examining linguistic problems in SNACS\nannotation, we use language models to attempt automatic labelling of SNACS\nsupersenses in Hindi and achieve results competitive with past work on English.\nWe look towards upstream applications in semantic role labelling and extension\nto related languages such as Gujarati.", "published": "2022-05-08 21:13:33", "link": "http://arxiv.org/abs/2205.03955v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Chart Question Answering: State of the Art and Future Directions", "abstract": "Information visualizations such as bar charts and line charts are very common\nfor analyzing data and discovering critical insights. Often people analyze\ncharts to answer questions that they have in mind. Answering such questions can\nbe challenging as they often require a significant amount of perceptual and\ncognitive effort. Chart Question Answering (CQA) systems typically take a chart\nand a natural language question as input and automatically generate the answer\nto facilitate visual data analysis. Over the last few years, there has been a\ngrowing body of literature on the task of CQA. In this survey, we\nsystematically review the current state-of-the-art research focusing on the\nproblem of chart question answering. We provide a taxonomy by identifying\nseveral important dimensions of the problem domain including possible inputs\nand outputs of the task and discuss the advantages and limitations of proposed\nsolutions. We then summarize various evaluation techniques used in the surveyed\npapers. Finally, we outline the open challenges and future research\nopportunities related to chart question answering.", "published": "2022-05-08 22:54:28", "link": "http://arxiv.org/abs/2205.03966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text\n  Correspondence", "abstract": "The logical negation property (LNP), which implies generating different\npredictions for semantically opposite inputs, is an important property that a\ntrustworthy language model must satisfy. However, much recent evidence shows\nthat large-size pre-trained language models (PLMs) do not satisfy this\nproperty. In this paper, we perform experiments using probing tasks to assess\nPLM's LNP understanding. Unlike previous studies that only examined negation\nexpressions, we expand the boundary of the investigation to lexical semantics.\nThrough experiments, we observe that PLMs violate the LNP frequently. To\nalleviate the issue, we propose a novel intermediate training task, names\nmeaning-matching, designed to directly learn a meaning-text correspondence,\ninstead of relying on the distributional hypothesis. Through multiple\nexperiments, we find that the task enables PLMs to learn lexical semantic\ninformation. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm\nthat it is a safe intermediate task that guarantees a similar or better\nperformance of downstream tasks. Finally, we observe that our proposed approach\noutperforms our previous counterparts despite its time and resource efficiency.", "published": "2022-05-08 08:37:36", "link": "http://arxiv.org/abs/2205.03815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Use of BERT for Automated Essay Scoring: Joint Learning of\n  Multi-Scale Essay Representation", "abstract": "In recent years, pre-trained models have become dominant in most natural\nlanguage processing (NLP) tasks. However, in the area of Automated Essay\nScoring (AES), pre-trained models such as BERT have not been properly used to\noutperform other deep learning models such as LSTM. In this paper, we introduce\na novel multi-scale essay representation for BERT that can be jointly learned.\nWe also employ multiple losses and transfer learning from out-of-domain essays\nto further improve the performance. Experiment results show that our approach\nderives much benefit from joint learning of multi-scale essay representation\nand obtains almost the state-of-the-art result among all deep learning models\nin the ASAP task. Our multi-scale essay representation also generalizes well to\nCommonLit Readability Prize data set, which suggests that the novel text\nrepresentation proposed in this paper may be a new and effective choice for\nlong-text tasks.", "published": "2022-05-08 10:36:54", "link": "http://arxiv.org/abs/2205.03835v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "It's the Same Old Story! Enriching Event-Centric Knowledge Graphs by\n  Narrative Aspects", "abstract": "Our lives are ruled by events of varying importance ranging from simple\neveryday occurrences to incidents of societal dimension. And a lot of effort is\ntaken to exchange information and discuss about such events: generally\nspeaking, stringent narratives are formed to reduce complexity. But when\nconsidering complex events like the current conflict between Russia and Ukraine\nit is easy to see that those events cannot be grasped by objective facts alone,\nlike the start of the conflict or respective troop sizes. There are different\nviewpoints and assessments to consider, a different understanding of the roles\ntaken by individual participants, etc. So how can such subjective and\nviewpoint-dependent information be effectively represented together with all\nobjective information? Recently event-centric knowledge graphs have been\nproposed for objective event representation in the otherwise primarily\nentity-centric domain of knowledge graphs. In this paper we introduce a novel\nand lightweight structure for event-centric knowledge graphs, which for the\nfirst time allows for queries incorporating viewpoint-dependent and narrative\naspects. Our experiments prove the effective incorporation of subjective\nattributions for event participants and show the benefits of specifically\ntailored indexes for narrative query processing.", "published": "2022-05-08 14:00:41", "link": "http://arxiv.org/abs/2205.03876v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Structured Span Selector", "abstract": "Many natural language processing tasks, e.g., coreference resolution and\nsemantic role labeling, require selecting text spans and making decisions about\nthem. A typical approach to such tasks is to score all possible spans and\ngreedily select spans for task-specific downstream processing. This approach,\nhowever, does not incorporate any inductive bias about what sort of spans ought\nto be selected, e.g., that selected spans tend to be syntactic constituents. In\nthis paper, we propose a novel grammar-based structured span selection model\nwhich learns to make use of the partial span-level annotation provided for such\nproblems. Compared to previous approaches, our approach gets rid of the\nheuristic greedy span selection scheme, allowing us to model the downstream\ntask on an optimal set of spans. We evaluate our model on two popular span\nprediction tasks: coreference resolution and semantic role labeling. We show\nempirical improvements on both.", "published": "2022-05-08 23:58:40", "link": "http://arxiv.org/abs/2205.03977v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Math-KG: Construction and Applications of Mathematical Knowledge Graph", "abstract": "Recently, the explosion of online education platforms makes a success in\nencouraging us to easily access online education resources. However, most of\nthem ignore the integration of massive unstructured information, which\ninevitably brings the problem of \\textit{information overload} and\n\\textit{knowledge trek}. In this paper, we proposed a mathematical knowledge\ngraph named Math-KG, which automatically constructed by the pipeline method\nwith the natural language processing technology to integrate the resources of\nthe mathematics. It is built from the corpora of Baidu Baike, Wikipedia. We\nimplement a simple application system to validate the proposed Math-KG can make\ncontributions on a series of scenes, including faults analysis and semantic\nsearch. The system is publicly available at GitHub\n\\footnote{\\url{https://github.com/wjn1996/Mathematical-Knowledge-Entity-Recognition}.}.", "published": "2022-05-08 03:39:07", "link": "http://arxiv.org/abs/2205.03772v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Should We Rely on Entity Mentions for Relation Extraction? Debiasing\n  Relation Extraction with Counterfactual Analysis", "abstract": "Recent literature focuses on utilizing the entity information in the\nsentence-level relation extraction (RE), but this risks leaking superficial and\nspurious clues of relations. As a result, RE still suffers from unintended\nentity bias, i.e., the spurious correlation between entity mentions (names) and\nrelations. Entity bias can mislead the RE models to extract the relations that\ndo not exist in the text. To combat this issue, some previous work masks the\nentity mentions to prevent the RE models from overfitting entity mentions.\nHowever, this strategy degrades the RE performance because it loses the\nsemantic information of entities. In this paper, we propose the CORE\n(Counterfactual Analysis based Relation Extraction) debiasing method that\nguides the RE models to focus on the main effects of textual context without\nlosing the entity information. We first construct a causal graph for RE, which\nmodels the dependencies between variables in RE models. Then, we propose to\nconduct counterfactual analysis on our causal graph to distill and mitigate the\nentity bias, that captures the causal effects of specific entity mentions in\neach instance. Note that our CORE method is model-agnostic to debias existing\nRE systems during inference without changing their training processes.\nExtensive experimental results demonstrate that our CORE yields significant\ngains on both effectiveness and generalization for RE. The source code is\nprovided at: https://github.com/vanoracai/CoRE.", "published": "2022-05-08 05:13:54", "link": "http://arxiv.org/abs/2205.03784v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRAPHCACHE: Message Passing as Caching for Sentence-Level Relation\n  Extraction", "abstract": "Entity types and textual context are essential properties for sentence-level\nrelation extraction (RE). Existing work only encodes these properties within\nindividual instances, which limits the performance of RE given the insufficient\nfeatures in a single sentence. In contrast, we model these properties from the\nwhole dataset and use the dataset-level information to enrich the semantics of\nevery instance. We propose the GRAPHCACHE (Graph Neural Network as Caching)\nmodule, that propagates the features across sentences to learn better\nrepresentations for RE. GRAPHCACHE aggregates the features from sentences in\nthe whole dataset to learn global representations of properties, and use them\nto augment the local features within individual sentences. The global property\nfeatures act as dataset-level prior knowledge for RE, and a complement to the\nsentence-level features. Inspired by the classical caching technique in\ncomputer systems, we develop GRAPHCACHE to update the property representations\nin an online manner. Overall, GRAPHCACHE yields significant effectiveness gains\non RE and enables efficient message passing across all sentences in the\ndataset.", "published": "2022-05-08 05:30:19", "link": "http://arxiv.org/abs/2205.03786v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Assigning Species Information to Corresponding Genes by a Sequence\n  Labeling Framework", "abstract": "The automatic assignment of species information to the corresponding genes in\na research article is a critically important step in the gene normalization\ntask, whereby a gene mention is normalized and linked to a database record or\nidentifier by a text-mining algorithm. Existing methods typically rely on\nheuristic rules based on gene and species co-occurrence in the article, but\ntheir accuracy is suboptimal. We therefore developed a high-performance method,\nusing a novel deep learning-based framework, to classify whether there is a\nrelation between a gene and a species. Instead of the traditional binary\nclassification framework in which all possible pairs of genes and species in\nthe same article are evaluated, we treat the problem as a sequence-labeling\ntask such that only a fraction of the pairs needs to be considered. Our\nbenchmarking results show that our approach obtains significantly higher\nperformance compared to that of the rule-based baseline method for the species\nassignment task (from 65.8% to 81.3% in accuracy). The source code and data for\nspecies assignment are freely available at\nhttps://github.com/ncbi/SpeciesAssignment.", "published": "2022-05-08 12:39:45", "link": "http://arxiv.org/abs/2205.03853v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "Robust (Controlled) Table-to-Text Generation with Structure-Aware\n  Equivariance Learning", "abstract": "Controlled table-to-text generation seeks to generate natural language\ndescriptions for highlighted subparts of a table. Previous SOTA systems still\nemploy a sequence-to-sequence generation method, which merely captures the\ntable as a linear structure and is brittle when table layouts change. We seek\nto go beyond this paradigm by (1) effectively expressing the relations of\ncontent pieces in the table, and (2) making our model robust to\ncontent-invariant structural transformations. Accordingly, we propose an\nequivariance learning framework, which encodes tables with a structure-aware\nself-attention mechanism. This prunes the full self-attention structure into an\norder-invariant graph attention that captures the connected graph structure of\ncells belonging to the same row or column, and it differentiates between\nrelevant cells and irrelevant cells from the structural perspective. Our\nframework also modifies the positional encoding mechanism to preserve the\nrelative position of tokens in the same cell but enforce position invariance\namong different cells. Our technology is free to be plugged into existing\ntable-to-text generation models, and has improved T5-based models to offer\nbetter performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,\nwe preserve promising performance, while previous SOTA systems, even with\ntransformation-based data augmentation, have seen significant performance\ndrops. Our code is available at https://github.com/luka-group/Lattice.", "published": "2022-05-08 23:37:27", "link": "http://arxiv.org/abs/2205.03972v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Silence is Sweeter Than Speech: Self-Supervised Model Using Silence to\n  Store Speaker Information", "abstract": "Self-Supervised Learning (SSL) has made great strides recently. SSL speech\nmodels achieve decent performance on a wide range of downstream tasks,\nsuggesting that they extract different aspects of information from speech.\nHowever, how SSL models store various information in hidden representations\nwithout interfering is still poorly understood. Taking the recently successful\nSSL model, HuBERT, as an example, we explore how the SSL model processes and\nstores speaker information in the representation. We found that HuBERT stores\nspeaker information in representations whose positions correspond to silences\nin a waveform. There are several pieces of evidence. (1) We find that the\nutterances with more silent parts in the waveforms have better Speaker\nIdentification (SID) accuracy. (2) If we use the whole utterances for SID, the\nsilence part always contributes more to the SID task. (3) If we only use the\nrepresentation of a part of the utterance for SID, the silenced part has higher\naccuracy than the other parts. Our findings not only contribute to a better\nunderstanding of SSL models but also improve performance. By simply adding\nsilence to the original waveform, HuBERT improved its accuracy on SID by nearly\n2%.", "published": "2022-05-08 02:10:39", "link": "http://arxiv.org/abs/2205.03759v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
