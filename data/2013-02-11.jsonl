{"title": "Toric grammars: a new statistical approach to natural language modeling", "abstract": "We propose a new statistical model for computational linguistics. Rather than\ntrying to estimate directly the probability distribution of a random sentence\nof the language, we define a Markov chain on finite sets of sentences with many\nfinite recurrent communicating classes and define our language model as the\ninvariant probability measures of the chain on each recurrent communicating\nclass. This Markov chain, that we call a communication model, recombines at\neach step randomly the set of sentences forming its current state, using some\ngrammar rules. When the grammar rules are fixed and known in advance instead of\nbeing estimated on the fly, we can prove supplementary mathematical properties.\nIn particular, we can prove in this case that all states are recurrent states,\nso that the chain defines a partition of its state space into finite recurrent\ncommunicating classes. We show that our approach is a decisive departure from\nMarkov models at the sentence level and discuss its relationships with Context\nFree Grammars. Although the toric grammars we use are closely related to\nContext Free Grammars, the way we generate the language from the grammar is\nqualitatively different. Our communication model has two purposes. On the one\nhand, it is used to define indirectly the probability distribution of a random\nsentence of the language. On the other hand it can serve as a (crude) model of\nlanguage transmission from one speaker to another speaker through the\ncommunication of a (large) set of sentences.", "published": "2013-02-11 18:51:03", "link": "http://arxiv.org/abs/1302.2569v1", "categories": ["stat.ML", "cs.CL", "math.PR", "62M09, 62P99, 68T50, 91F20, 03B65, 91E40, 60J20"], "primary_category": "stat.ML"}
