{"title": "Cutoff for congestion dynamics and related generalized exclusion processes", "abstract": "We consider congestion dynamics with $n$ players and $Q$ resources under the\nconstraint that the number of each resource is $\\kappa$ and that $n<\\kappa Q$\nin the regime that $n$ and $\\kappa$ diverge but $Q$ is fixed with\n$n=\\lfloor{\\rho \\kappa Q\\rfloor}$ for a fixed constant $\\rho \\in (0, 1/2]$. We\nshow that the Glauber dynamics and its unlabeled version exhibit cutoff at time\n$(1/2)n \\log n$ and $(1/2)(1-\\rho)n\\log n$ in total variation respectively. The\nunlabeled version is a special case of natural Markov chains for sampling from\nlog M-concave distributions. We also show that a family of Markov chains for\nuniform sampling on M-convex sets does not necessarily exhibit cutoff.", "published": "2025-02-09 23:30:22", "link": "http://arxiv.org/abs/2502.06071v1", "categories": ["math.PR", "cs.DM"], "primary_category": "math.PR"}
{"title": "Verified Certificates via SAT and Computer Algebra Systems for the Ramsey R(3, 8) and R(3, 9) Problems", "abstract": "The Ramsey problem R(3, k) seeks to determine the smallest value of n such\nthat any red/blue edge coloring of the complete graph on n vertices must either\ncontain a blue triangle (3-clique) or a red clique of size k. Despite its\nsignificance, many previous computational results for the Ramsey R(3, k)\nproblem such as R(3, 8) and R(3, 9) lack formal verification. To address this\nissue, we use the software MATHCHECK to generate certificates for Ramsey\nproblems R(3, 8) and R(3, 9) (and symmetrically R(8, 3) and R(9, 3)) by\nintegrating a Boolean satisfiability (SAT) solver with a computer algebra\nsystem (CAS). Our SAT+CAS approach significantly outperforms traditional\nSAT-only methods, demonstrating a significant improvement in runtime. For\ninstance, our SAT+CAS approach solves R(8, 3) sequentially in 18.5 hours, while\na SAT-only approach using the state-of-the-art CaDiCaL solver times out after 7\ndays. Additionally, in order to be able to scale to harder Ramsey problem like\nR(9, 3) we further optimized our SAT+CAS tool using a parallelized\ncube-and-conquer approach. Our results provide the first independently\nverifiable certificates for these Ramsey numbers, ensuring both correctness and\ncompleteness of the exhaustive search process of our SAT+CAS tool.", "published": "2025-02-09 22:24:37", "link": "http://arxiv.org/abs/2502.06055v1", "categories": ["cs.DM", "cs.SC", "math.GR"], "primary_category": "cs.DM"}
{"title": "Amnesiac Flooding: Easy to break, hard to escape", "abstract": "Broadcast is a central problem in distributed computing. Recently, Hussak and\nTrehan [PODC'19/DC'23] proposed a stateless broadcasting protocol (Amnesiac\nFlooding), which was surprisingly proven to terminate in asymptotically optimal\ntime (linear in the diameter of the network). However, it remains unclear: (i)\nAre there other stateless terminating broadcast algorithms with the desirable\nproperties of Amnesiac Flooding, (ii) How robust is Amnesiac Flooding with\nrespect to \\emph{faults}?\n  In this paper we make progress on both of these fronts. Under a reasonable\nrestriction (obliviousness to message content) additional to the fault-free\nsynchronous model, we prove that Amnesiac Flooding is the \\emph{only} strictly\nstateless deterministic protocol that can achieve terminating broadcast. We\nidentify four natural properties of a terminating broadcast protocol that\nAmnesiac Flooding uniquely satisfies. In contrast, we prove that even minor\nrelaxations of \\textit{any} of these four criteria allow the construction of\nother terminating broadcast protocols.\n  On the other hand, we prove that Amnesiac Flooding can become non-terminating\nor non-broadcasting, even if we allow just one node to drop a single message on\na single edge in a single round. As a tool for proving this, we focus on the\nset of all \\textit{configurations} of transmissions between nodes in the\nnetwork, and obtain a \\textit{dichotomy} characterizing the configurations,\nstarting from which, Amnesiac Flooding terminates.\n  Additionally, we characterise the structure of sets of Byzantine agents\ncapable of forcing non-termination or non-broadcast of the protocol on\narbitrary networks.", "published": "2025-02-09 19:17:42", "link": "http://arxiv.org/abs/2502.06001v1", "categories": ["cs.DC", "cs.DM", "cs.DS", "68W15, 68W40, 05C85, 68Q25", "F.2.0; G.2.2; C.2.2; C.2.4"], "primary_category": "cs.DC"}
{"title": "Polynomial Regret Concentration of UCB for Non-Deterministic State Transitions", "abstract": "Monte Carlo Tree Search (MCTS) has proven effective in solving\ndecision-making problems in perfect information settings. However, its\napplication to stochastic and imperfect information domains remains limited.\nThis paper extends the theoretical framework of MCTS to stochastic domains by\naddressing non-deterministic state transitions, where actions lead to\nprobabilistic outcomes. Specifically, building on the work of Shah et al.\n(2020), we derive polynomial regret concentration bounds for the Upper\nConfidence Bound algorithm in multi-armed bandit problems with stochastic\ntransitions, offering improved theoretical guarantees. Our primary contribution\nis proving that these bounds also apply to non-deterministic environments,\nensuring robust performance in stochastic settings. This broadens the\napplicability of MCTS to real-world decision-making problems with probabilistic\noutcomes, such as in autonomous systems and financial decision-making.", "published": "2025-02-09 15:46:42", "link": "http://arxiv.org/abs/2502.06900v1", "categories": ["cs.LG", "cs.DM"], "primary_category": "cs.LG"}
{"title": "Sink-free orientations: a local sampler with applications", "abstract": "For sink-free orientations in graphs of minimum degree at least $3$, we show\nthat there is a deterministic approximate counting algorithm that runs in time\n$O((n^{73}/\\varepsilon^{72})\\log(n/\\varepsilon))$, a near-linear time sampling\nalgorithm, and a randomised approximate counting algorithm that runs in time\n$O((n/\\varepsilon)^2\\log(n/\\varepsilon))$, where $n$ denotes the number of\nvertices of the input graph and $0<\\varepsilon<1$ is the desired accuracy. All\nthree algorithms are based on a local implementation of the sink popping method\n(Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling\nframework (Guo, Jerrum, and Liu, 2019).", "published": "2025-02-09 12:25:09", "link": "http://arxiv.org/abs/2502.05877v2", "categories": ["cs.DS", "cs.DM", "math.PR"], "primary_category": "cs.DS"}
{"title": "De Finetti's problem with fixed transaction costs and regime switching", "abstract": "In this paper, we examine a modified version of de Finetti's optimal dividend\nproblem, incorporating fixed transaction costs and altering the surplus process\nby introducing two-valued drift and two-valued volatility coefficients. This\nmodification aims to capture the transitions or adjustments in the company's\nfinancial status. We identify the optimal dividend strategy, which maximizes\nthe expected total net dividend payments (after accounting for transaction\ncosts) until ruin, as a two-barrier impulsive dividend strategy. Notably, the\noptimal strategy can be explicitly determined for almost all scenarios\ninvolving different drifts and volatility coefficients. Our primary focus is on\nexploring how changes in drift and volatility coefficients influence the\noptimal dividend strategy.", "published": "2025-02-09 10:21:47", "link": "http://arxiv.org/abs/2502.05839v1", "categories": ["q-fin.MF", "q-fin.RM"], "primary_category": "q-fin.MF"}
{"title": "BnTTS: Few-Shot Speaker Adaptation in Low-Resource Setting", "abstract": "This paper introduces BnTTS (Bangla Text-To-Speech), the first framework for\nBangla speaker adaptation-based TTS, designed to bridge the gap in Bangla\nspeech synthesis using minimal training data. Building upon the XTTS\narchitecture, our approach integrates Bangla into a multilingual TTS pipeline,\nwith modifications to account for the phonetic and linguistic characteristics\nof the language. We pre-train BnTTS on 3.85k hours of Bangla speech dataset\nwith corresponding text labels and evaluate performance in both zero-shot and\nfew-shot settings on our proposed test dataset. Empirical evaluations in\nfew-shot settings show that BnTTS significantly improves the naturalness,\nintelligibility, and speaker fidelity of synthesized Bangla speech. Compared to\nstate-of-the-art Bangla TTS systems, BnTTS exhibits superior performance in\nSubjective Mean Opinion Score (SMOS), Naturalness, and Clarity metrics.", "published": "2025-02-09 00:15:47", "link": "http://arxiv.org/abs/2502.05729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Lifelong Editing for Language Models", "abstract": "Large language models (LLMs) acquire information from pre-training corpora,\nbut their stored knowledge can become inaccurate or outdated over time. Model\nediting addresses this challenge by modifying model parameters without\nretraining, and prevalent approaches leverage hypernetworks to generate these\nparameter updates. However, they face significant challenges in lifelong\nediting due to their incompatibility with LLM parameters that dynamically\nchange during the editing process. To address this, we observed that\nhypernetwork-based lifelong editing aligns with reinforcement learning modeling\nand proposed RLEdit, an RL-based editing method. By treating editing losses as\nrewards and optimizing hypernetwork parameters at the full knowledge sequence\nlevel, we enable it to precisely capture LLM changes and generate appropriate\nparameter updates. Our extensive empirical evaluation across several LLMs\ndemonstrates that RLEdit outperforms existing methods in lifelong editing with\nsuperior effectiveness and efficiency, achieving a 59.24% improvement while\nrequiring only 2.11% of the time compared to most approaches. Our code is\navailable at: https://github.com/zhrli324/RLEdit.", "published": "2025-02-09 03:37:06", "link": "http://arxiv.org/abs/2502.05759v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Reference (In-)Determinacy in Natural Language Inference", "abstract": "We revisit the reference determinacy (RD) assumption in the task of natural\nlanguage inference (NLI), i.e., the premise and hypothesis are assumed to refer\nto the same context when human raters annotate a label. While RD is a practical\nassumption for constructing a new NLI dataset, we observe that current NLI\nmodels, which are typically trained solely on hypothesis-premise pairs created\nwith the RD assumption, fail in downstream applications such as fact\nverification, where the input premise and hypothesis may refer to different\ncontexts. To highlight the impact of this phenomenon in real-world use cases,\nwe introduce RefNLI, a diagnostic benchmark for identifying reference ambiguity\nin NLI examples. In RefNLI, the premise is retrieved from a knowledge source\n(i.e., Wikipedia) and does not necessarily refer to the same context as the\nhypothesis. With RefNLI, we demonstrate that finetuned NLI models and few-shot\nprompted LLMs both fail to recognize context mismatch, leading to over 80%\nfalse contradiction and over 50% entailment predictions. We discover that the\nexistence of reference ambiguity in NLI examples can in part explain the\ninherent human disagreements in NLI and provide insight into how the RD\nassumption impacts the NLI dataset creation process.", "published": "2025-02-09 06:58:13", "link": "http://arxiv.org/abs/2502.05793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Perturbation in Large Language Model Representations through\n  Recursive Symbolic Regeneration", "abstract": "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.", "published": "2025-02-09 07:00:10", "link": "http://arxiv.org/abs/2502.05794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on\n  Fairness-Related Queries", "abstract": "The generation of incorrect images, such as depictions of people of color in\nNazi-era uniforms by Gemini, frustrated users and harmed Google's reputation,\nmotivating us to investigate the relationship between accurately reflecting\nfactuality and promoting diversity and equity. In this study, we focus on 19\nreal-world statistics collected from authoritative sources. Using these\nstatistics, we develop a checklist comprising objective and subjective queries\nto analyze behavior of large language models (LLMs) and text-to-image (T2I)\nmodels. Objective queries assess the models' ability to provide accurate world\nknowledge. In contrast, the design of subjective queries follows a key\nprinciple: statistical or experiential priors should not be overgeneralized to\nindividuals, ensuring that models uphold diversity. These subjective queries\nare based on three common human cognitive errors that often result in social\nbiases. We propose metrics to assess factuality and fairness, and formally\nprove the inherent trade-off between these two aspects. Results show that\nGPT-4o and DALL-E 3 perform notably well among six LLMs and four T2I models.\nOur code is publicly available at https://github.com/uclanlp/Fact-or-Fair.", "published": "2025-02-09 10:54:11", "link": "http://arxiv.org/abs/2502.05849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Training Large Language Models for Tool-Use Without Demonstrations", "abstract": "Large language models (LLMs) remain prone to factual inaccuracies and\ncomputational errors, including hallucinations and mistakes in mathematical\nreasoning. Recent work augmented LLMs with tools to mitigate these\nshortcomings, but often requires curated gold tool-use demonstrations. In this\npaper, we investigate whether LLMs can learn to use tools without\ndemonstrations. First, we analyse zero-shot prompting strategies to guide LLMs\nin tool utilisation. Second, we propose a self-training method to synthesise\ntool-use traces using the LLM itself. We compare supervised fine-tuning and\npreference fine-tuning techniques for fine-tuning the model on datasets\nconstructed using existing Question Answering (QA) datasets, i.e., TriviaQA and\nGSM8K. Experiments show that tool-use enhances performance on a long-tail\nknowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads\nto mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our\nfindings highlight the potential and challenges of integrating external tools\ninto LLMs without demonstrations.", "published": "2025-02-09 12:06:10", "link": "http://arxiv.org/abs/2502.05867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models", "abstract": "Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field.", "published": "2025-02-09 12:26:05", "link": "http://arxiv.org/abs/2502.05878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective\n  Hallucination Mitigation", "abstract": "Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models\n(LLMs) by improving their ability to refuse responses to questions beyond their\nknowledge, thereby reducing hallucinations and improving reliability. Effective\nRAIT must address two key challenges: firstly, effectively reject unknown\nquestions to minimize hallucinations; secondly, avoid over-refusal to ensure\nquestions that can be correctly answered are not rejected, thereby maintain the\nhelpfulness of LLM outputs. In this paper, we address the two challenges by\nderiving insightful observations from the gradient-based perspective, and\nproposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT:\n(1) employs gradient-driven sample selection to effectively minimize\nhallucinations and (2) introduces an adaptive weighting mechanism during\nfine-tuning to reduce the risk of over-refusal, achieving the balance between\naccurate refusals and maintaining useful responses. Experimental evaluations on\nopen-ended and multiple-choice question answering tasks demonstrate that GRAIT\nsignificantly outperforms existing RAIT methods in the overall performance. The\nsource code and data will be available at https://github.com/opendatalab/GRAIT .", "published": "2025-02-09 14:11:30", "link": "http://arxiv.org/abs/2502.05911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARISE: Iterative Rule Induction and Synthetic Data Generation for Text\n  Classification", "abstract": "We propose ARISE, a framework that iteratively induces rules and generates\nsynthetic data for text classification. We combine synthetic data generation\nand automatic rule induction, via bootstrapping, to iteratively filter the\ngenerated rules and data. We induce rules via inductive generalisation of\nsyntactic n-grams, enabling us to capture a complementary source of\nsupervision. These rules alone lead to performance gains in both, in-context\nlearning (ICL) and fine-tuning (FT) settings. Similarly, use of augmented data\nfrom ARISE alone improves the performance for a model, outperforming\nconfigurations that rely on complex methods like contrastive learning. Further,\nour extensive experiments on various datasets covering three full-shot, eight\nfew-shot and seven multilingual variant settings demonstrate that the rules and\ndata we generate lead to performance improvements across these diverse domains\nand languages.", "published": "2025-02-09 14:39:01", "link": "http://arxiv.org/abs/2502.05923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-granular Training Strategies for Robust Multi-hop Reasoning Over\n  Noisy and Heterogeneous Knowledge Sources", "abstract": "Multi-source multi-hop question answering (QA) represents a challenging task\nin natural language processing due to the need for dynamic integration of\nheterogeneous knowledge sources and multi-step reasoning. Existing methods\noften suffer from cascading errors, insufficient handling of knowledge\nconflicts, and computational inefficiency. In this paper, we propose Adaptive\nMulti-source Knowledge-Oriented Reasoning (AMKOR), a generative framework that\nleverages large language models (LLMs) to dynamically fuse parametric and\nretrieved knowledge while exploring reasoning trajectories using probabilistic\nbeam reasoning. AMKOR is further enhanced by a multi-granular learning\nstrategy, optimizing both local reasoning steps and global answer accuracy.\nExperiments conducted on four widely-used multi-hop QA datasets, including\nHotpotQA and MuSiQue, demonstrate that AMKOR achieves state-of-the-art\nperformance, significantly outperforming baseline methods on both reasoning\naccuracy and robustness. Additional analyses confirm its scalability,\nadaptability to noisy knowledge, and superior ability to handle complex\nmulti-hop tasks. This work establishes a new benchmark for multi-source\nmulti-hop QA by effectively combining reasoning quality and efficiency.", "published": "2025-02-09 16:06:43", "link": "http://arxiv.org/abs/2502.05944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered\n  Therapy Using LLM Agents", "abstract": "This paper presents HamRaz, a novel Persian-language mental health dataset\ndesigned for Person-Centered Therapy (PCT) using Large Language Models (LLMs).\nDespite the growing application of LLMs in AI-driven psychological counseling,\nexisting datasets predominantly focus on Western and East Asian contexts,\noverlooking cultural and linguistic nuances essential for effective\nPersian-language therapy. To address this gap, HamRaz combines script-based\ndialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy\ninteractions. We also introduce HamRazEval, a dual evaluation framework that\nmeasures conversational quality and therapeutic effectiveness using General\nDialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI).\nExperimental results show HamRaz outperforms conventional Script Mode and\nTwo-Agent Mode, producing more empathetic, context-aware, and realistic therapy\nsessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven\nresource to advance AI-powered psychotherapy research in diverse communities.", "published": "2025-02-09 18:23:34", "link": "http://arxiv.org/abs/2502.05982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling: A Systematical Survey", "abstract": "Semantic role labeling (SRL) is a central natural language processing (NLP)\ntask aiming to understand the semantic roles within texts, facilitating a wide\nrange of downstream applications. While SRL has garnered extensive and enduring\nresearch, there is currently a lack of a comprehensive survey that thoroughly\norganizes and synthesizes the field. This paper aims to review the entire\nresearch trajectory of the SRL community over the past two decades. We begin by\nproviding a complete definition of SRL. To offer a comprehensive taxonomy, we\ncategorize SRL methodologies into four key perspectives: model architectures,\nsyntax feature modeling, application scenarios, and multi-modal extensions.\nFurther, we discuss SRL benchmarks, evaluation metrics, and paradigm modeling\napproaches, while also exploring practical applications across various domains.\nFinally, we analyze future research directions in SRL, addressing the evolving\nrole of SRL in the age of large language models (LLMs) and its potential impact\non the broader NLP landscape. We maintain a public repository and consistently\nupdate related resources at: https://github.com/DreamH1gh/Awesome-SRL", "published": "2025-02-09 12:45:49", "link": "http://arxiv.org/abs/2502.08660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delta -- Contrastive Decoding Mitigates Text Hallucinations in Large\n  Language Models", "abstract": "Large language models (LLMs) demonstrate strong capabilities in natural\nlanguage processing but remain prone to hallucinations, generating factually\nincorrect or fabricated content. This issue undermines their reliability,\nparticularly in high-stakes domains such as healthcare and legal advisory. To\naddress this challenge, we propose Delta, an inference-time method that reduces\nhallucinations without requiring model retraining or additional data. Delta\nworks by randomly masking parts of the input prompt and contrasting the output\ndistributions for the original and masked inputs, effectively suppressing\nhallucinations through inference-only computations. We evaluate Delta on\ncontext-rich question-answering benchmarks, achieving absolute improvements of\napproximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and\n7 and 2 percentage points on TriviaQA and Natural Questions under-sampling\ndecoding. Delta also improves the no-answer exact match score on SQuAD v2 by\nover ten percentage points, demonstrating its effectiveness in mitigating\nhallucinations arising from contextual ambiguity. These results highlight Delta\nas a computationally efficient and scalable approach for improving the\nreliability of LLMs in real-world applications.", "published": "2025-02-09 09:16:42", "link": "http://arxiv.org/abs/2502.05825v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Depression Detection with Chain-of-Thought Prompting: From\n  Emotion to Reasoning Using Large Language Models", "abstract": "Depression is one of the leading causes of disability worldwide, posing a\nsevere burden on individuals, healthcare systems, and society at large. Recent\nadvancements in Large Language Models (LLMs) have shown promise in addressing\nmental health challenges, including the detection of depression through\ntext-based analysis. However, current LLM-based methods often struggle with\nnuanced symptom identification and lack a transparent, step-by-step reasoning\nprocess, making it difficult to accurately classify and explain mental health\nconditions. To address these challenges, we propose a Chain-of-Thought\nPrompting approach that enhances both the performance and interpretability of\nLLM-based depression detection. Our method breaks down the detection process\ninto four stages: (1) sentiment analysis, (2) binary depression classification,\n(3) identification of underlying causes, and (4) assessment of severity. By\nguiding the model through these structured reasoning steps, we improve\ninterpretability and reduce the risk of overlooking subtle clinical indicators.\nWe validate our method on the E-DAIC dataset, where we test multiple\nstate-of-the-art large language models. Experimental results indicate that our\nChain-of-Thought Prompting technique yields superior performance in both\nclassification accuracy and the granularity of diagnostic insights, compared to\nbaseline approaches.", "published": "2025-02-09 12:30:57", "link": "http://arxiv.org/abs/2502.05879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational\n  Agents", "abstract": "Understanding temporal dynamics is critical for conversational agents,\nenabling effective content analysis and informed decision-making. However,\ntime-aware datasets, particularly for persona-grounded conversations, are still\nlimited, which narrows their scope and diminishes their complexity. To address\nthis gap, we introduce MTPChat, a multimodal, time-aware persona dialogue\ndataset that integrates linguistic, visual, and temporal elements within\ndialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive\ntasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory\nPrediction (TGMP), both designed to assess a model's ability to understand\nimplicit temporal cues and dynamic interactions. Additionally, we present an\ninnovative framework featuring an adaptive temporal module to effectively\nintegrate multimodal streams and capture temporal dependencies. Experimental\nresults validate the challenges posed by MTPChat and demonstrate the\neffectiveness of our framework in multimodal time-sensitive scenarios.", "published": "2025-02-09 13:00:53", "link": "http://arxiv.org/abs/2502.05887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Distributional Perspective on Word Learning in Neural Language Models", "abstract": "Language models (LMs) are increasingly being studied as models of human\nlanguage learners. Due to the nascency of the field, it is not well-established\nwhether LMs exhibit similar learning dynamics to humans, and there are few\ndirect comparisons between learning trajectories in humans and models. Word\nlearning trajectories for children are relatively well-documented, and recent\nwork has tried to extend these investigations to language models. However,\nthere are no widely agreed-upon metrics for word learning in language models.\nWe take a distributional approach to this problem, defining lexical knowledge\nin terms of properties of the learned distribution for a target word. We argue\nthat distributional signatures studied in prior work fail to capture key\ndistributional information. Thus, we propose an array of signatures that\nimprove on earlier approaches by capturing knowledge of both where the target\nword can and cannot occur as well as gradient preferences about the word's\nappropriateness. We obtain learning trajectories for a selection of small\nlanguage models we train from scratch, study the relationship between different\ndistributional signatures, compare how well they align with human word learning\ntrajectories and interpretable lexical features, and address basic\nmethodological questions about estimating these distributional signatures. Our\nmetrics largely capture complementary information, suggesting that it is\nimportant not to rely on a single metric. However, across all metrics, language\nmodels' learning trajectories fail to correlate with those of children.", "published": "2025-02-09 13:15:59", "link": "http://arxiv.org/abs/2502.05892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Substitute Words with Model-based Score Ranking", "abstract": "Smart word substitution aims to enhance sentence quality by improving word\nchoices; however current benchmarks rely on human-labeled data. Since word\nchoices are inherently subjective, ground-truth word substitutions generated by\na small group of annotators are often incomplete and likely not generalizable.\nTo circumvent this issue, we instead employ a model-based score (BARTScore) to\nquantify sentence quality, thus forgoing the need for human annotations.\nSpecifically, we use this score to define a distribution for each word\nsubstitution, allowing one to test whether a substitution is statistically\nsuperior relative to others. In addition, we propose a loss function that\ndirectly optimizes the alignment between model predictions and sentence scores,\nwhile also enhancing the overall quality score of a substitution. Crucially,\nmodel learning no longer requires human labels, thus avoiding the cost of\nannotation while maintaining the quality of the text modified with\nsubstitutions. Experimental results show that the proposed approach outperforms\nboth masked language models (BERT, BART) and large language models (GPT-4,\nLLaMA). The source code is available at\nhttps://github.com/Hyfred/Substitute-Words-with-Ranking.", "published": "2025-02-09 15:26:32", "link": "http://arxiv.org/abs/2502.05933v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Semi-Supervised Text Generation Framework Combining a Deep Transformer\n  and a GAN", "abstract": "This paper introduces a framework that connects a deep generative pre-trained\nTransformer language model with a generative adversarial network for\nsemi-supervised text generation. In other words, the proposed model is first\npre-trained unsupervised on a large and diverse text corpus with 24 layers.\nThen a simple GAN architecture for synthetic text generation is introduced, and\nGumbel-Softmax is applied to handle the discreteness of tokens. The paper also\nshows a semi-supervised approach where real data is augmented with GAN samples,\nwhich is further used to fine-tune the Transformer model on the merged dataset.\nDetailed theoretical derivations are also included, outlining the proof of the\nmin-max objective function, and an extensive discussion of the Gumbel-Softmax\nreparameterization trick.", "published": "2025-02-09 15:38:43", "link": "http://arxiv.org/abs/2502.05937v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"Let the AI conspiracy begin...\" Language Model coordination is just one\n  inference-intervention away", "abstract": "In this work, we introduce a straightforward and effective methodology to\nsteer large language model behaviour capable of bypassing learned alignment\ngoals. We employ interference-time activation shifting, which is effective\nwithout additional training. Following prior studies, we derive intervention\ndirections from activation differences in contrastive pairs of model outputs,\nwhich represent the desired and undesired behaviour. By prompting the model to\ninclude multiple-choice answers in its response, we can automatically evaluate\nthe sensitivity of model output to individual attention heads steering efforts.\nWe demonstrate that interventions on these heads generalize well to open-ended\nanswer generation in the challenging \"AI coordination\" dataset. In this\ndataset, models must choose between assisting another AI or adhering to\nethical, safe, and unharmful behaviour. Our fine-grained interventions lead\nLlama 2 to prefer coordination with other AIs over following established\nalignment goals. Additionally, this approach enables stronger interventions\nthan those applied to whole model layers, preserving the overall cohesiveness\nof the output. The simplicity of our method highlights the shortcomings of\ncurrent alignment strategies and points to potential future research\ndirections, as concepts like \"AI coordination\" can be influenced by selected\nattention heads.", "published": "2025-02-09 16:11:57", "link": "http://arxiv.org/abs/2502.05945v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention", "abstract": "Multiple heads decoding accelerates the inference of Large Language Models\n(LLMs) by predicting next several tokens simultaneously. It generates and\nverifies multiple candidate sequences in parallel via tree attention with a\nfixed structure. In this paper, we replace the fixed tree attention with\ndynamic tree attention on multiple head decoding, specifically in the context\nof MEDUSA. We propose a simple and low complexity strategy to generate\ncandidates and construct the dynamic tree structure. Preliminary experiments\nshow that the proposed method improves the decoding efficiency of multiple head\ndecoding for LLMs while maintaining the generation quality. This result\ndemonstrates the potential for improvement of multiple head decoding in\ncandidate generation.", "published": "2025-02-09 16:28:21", "link": "http://arxiv.org/abs/2502.05947v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents", "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce AutoAgent-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, AutoAgent comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, AutoAgent\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.", "published": "2025-02-09 16:53:56", "link": "http://arxiv.org/abs/2502.05957v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Speech to Speech Translation with Translatotron: A State of the Art\n  Review", "abstract": "A cascade-based speech-to-speech translation has been considered a benchmark\nfor a very long time, but it is plagued by many issues, like the time taken to\ntranslate a speech from one language to another and compound errors. These\nissues are because a cascade-based method uses a combination of methods such as\nspeech recognition, speech-to-text translation, and finally, text-to-speech\ntranslation. Translatotron, a sequence-to-sequence direct speech-to-speech\ntranslation model was designed by Google to address the issues of compound\nerrors associated with cascade model. Today there are 3 versions of the\nTranslatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The\nfirst version was designed as a proof of concept to show that a direct\nspeech-to-speech translation was possible, it was found to be less effective\nthan the cascade model but was producing promising results. Translatotron2 was\nan improved version of Translatotron 1 with results similar to the cascade\nmodel. Translatotron 3 the latest version of the model is better than the\ncascade model at some points. In this paper, a complete review of\nspeech-to-speech translation will be presented, with a particular focus on all\nthe versions of Translatotron models. We will also show that Translatotron is\nthe best model to bridge the language gap between African Languages and other\nwell-formalized languages.", "published": "2025-02-09 18:15:00", "link": "http://arxiv.org/abs/2502.05980v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Preventing Rogue Agents Improves Multi-Agent Collaboration", "abstract": "Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents $\\textit{before they act}$ may\nprevent the system's failure. In this work, we propose to $\\textit{monitor}$\nagents during action prediction and $\\textit{intervene}$ when a future error is\nlikely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent\ncollaboration environment that allows modular control over task complexity and\ncommunication structure. Experiments on two variants of WhoDunitEnv and the\nGovSim environment for resource sustainability show that our approach leads to\nsubstantial performance gains up to 17.4% and 20%, respectively. Moreover, a\nthorough analysis shows that our monitors successfully identify critical points\nof agent confusion and our interventions effectively stop agent errors from\npropagating.", "published": "2025-02-09 18:35:08", "link": "http://arxiv.org/abs/2502.05986v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Forgetting during Finetuning with Pretraining Data\n  Injection", "abstract": "A widespread strategy to obtain a language model that performs well on a\ntarget domain is to finetune a pretrained model to perform unsupervised\nnext-token prediction on data from that target domain. Finetuning presents two\nchallenges: (i) if the amount of target data is limited, as in most practical\napplications, the model will quickly overfit, and (ii) the model will drift\naway from the original model, forgetting the pretraining data and the generic\nknowledge that comes with it. We aim to derive scaling laws that quantify these\ntwo phenomena for various target domains, amounts of available target data, and\nmodel scales. We measure the efficiency of injecting pretraining data into the\nfinetuning data mixture to avoid forgetting and mitigate overfitting. A key\npractical takeaway from our study is that injecting as little as 1% of\npretraining data in the finetuning data mixture prevents the model from\nforgetting the pretraining set.", "published": "2025-02-09 21:44:27", "link": "http://arxiv.org/abs/2502.06042v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LM2: Large Memory Models", "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only\nTransformer architecture enhanced with an auxiliary memory module that aims to\naddress the limitations of standard Transformers in multi-step reasoning,\nrelational argumentation, and synthesizing information distributed over long\ncontexts. The proposed LM2 incorporates a memory module that acts as a\ncontextual representation repository, interacting with input tokens via cross\nattention and updating through gating mechanisms. To preserve the Transformers\ngeneral-purpose capabilities, LM2 maintains the original information flow while\nintegrating a complementary memory pathway. Experimental results on the\nBABILong benchmark demonstrate that the LM2model outperforms both the\nmemory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3%\non average across tasks. LM2 exhibits exceptional capabilities in multi-hop\ninference, numerical reasoning, and large-context question-answering. On the\nMMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model,\ndemonstrating that its memory module does not degrade performance on general\ntasks. Further, in our analysis, we explore the memory interpretability,\neffectiveness of memory modules, and test-time behavior. Our findings emphasize\nthe importance of explicit memory in enhancing Transformer architectures.", "published": "2025-02-09 22:11:42", "link": "http://arxiv.org/abs/2502.06049v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot LLM Synthetic Data with Distribution Matching", "abstract": "As large language models (LLMs) advance, their ability to perform in-context\nlearning and few-shot language generation has improved significantly. This has\nspurred using LLMs to produce high-quality synthetic data to enhance the\nperformance of smaller models like online retrievers or weak LLMs. However,\nLLM-generated synthetic data often differs from the real data in key language\nattributes (e.g., styles, tones, content proportions, etc.). As a result,\nmixing these synthetic data directly with real data may distort the original\ndata distribution, potentially hindering performance improvements. To solve\nthis, we introduce SynAlign: a synthetic data generation and filtering\nframework based on key attribute distribution matching. Before generation,\nSynAlign employs an uncertainty tracker surrogated by the Gaussian Process\nmodel to iteratively select data clusters distinct from selected ones as\ndemonstrations for new data synthesis, facilitating the efficient exploration\ndiversity of the real data. Then, a latent attribute reasoning method is\nemployed: the LLM summarizes linguistic attributes of demonstrations and then\nsynthesizes new data based on them. This approach facilitates synthesizing\ndiverse data with linguistic attributes that appear in real data.After\ngeneration, the Maximum Mean Discrepancy is used as the objective function to\nlearn the sampling weight of each synthetic data, ensuring distribution\nmatching with the real data. Our experiments on multiple text prediction tasks\nshow significant performance improvements. We also conducted an online A/B test\non an online retriever to demonstrate SynAlign's effectiveness.", "published": "2025-02-09 16:43:32", "link": "http://arxiv.org/abs/2502.08661v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CORRECT: Context- and Reference-Augmented Reasoning and Prompting for\n  Fact-Checking", "abstract": "Fact-checking the truthfulness of claims usually requires reasoning over\nmultiple evidence sentences. Oftentimes, evidence sentences may not be always\nself-contained, and may require additional contexts and references from\nelsewhere to understand coreferential expressions, acronyms, and the scope of a\nreported finding. For example, evidence sentences from an academic paper may\nneed contextual sentences in the paper and descriptions in its cited papers to\ndetermine the scope of a research discovery. However, most fact-checking models\nmainly focus on the reasoning within evidence sentences, and ignore the\nauxiliary contexts and references. To address this problem, we propose a novel\nmethod, Context- and Reference-augmented Reasoning and Prompting. For evidence\nreasoning, we construct a three-layer evidence graph with evidence, context,\nand reference layers. We design intra- and cross-layer reasoning to integrate\nthree graph layers into a unified evidence embedding. For verdict prediction,\nwe design evidence-conditioned prompt encoder, which produces unique prompt\nembeddings for each claim. These evidence-conditioned prompt embeddings and\nclaims are unified for fact-checking. Experiments verify the strength of our\nmodel.", "published": "2025-02-09 01:41:15", "link": "http://arxiv.org/abs/2502.09635v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?", "abstract": "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps://github.com/sougata-ub/reading_between_lines", "published": "2025-02-09 04:40:35", "link": "http://arxiv.org/abs/2502.09636v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jailbreaking to Jailbreak", "abstract": "Refusal training on Large Language Models (LLMs) prevents harmful outputs,\nyet this defense remains vulnerable to both automated and human-crafted\njailbreaks. We present a novel LLM-as-red-teamer approach in which a human\njailbreaks a refusal-trained LLM to make it willing to jailbreak itself or\nother LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can\nsystematically evaluate target models using various red teaming strategies and\nimprove its performance via in-context learning from the previous failures. Our\nexperiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other\nLLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs)\nrespectively against GPT-4o (and similar results across other capable LLMs) on\nHarmbench. Our work not only introduces a scalable approach to strategic red\nteaming, drawing inspiration from human red teamers, but also highlights\njailbreaking-to-jailbreak as an overlooked failure mode of the safeguard.\nSpecifically, an LLM can bypass its own safeguards by employing a jailbroken\nversion of itself that is willing to assist in further jailbreaking. To prevent\nany direct misuse with $J_2$, while advancing research in AI safety, we\npublicly share our methodology while keeping specific prompting details\nprivate.", "published": "2025-02-09 20:49:16", "link": "http://arxiv.org/abs/2502.09638v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NutriTransform: Estimating Nutritional Information From Online Food\n  Posts", "abstract": "Deriving nutritional information from online food posts is challenging,\nparticularly when users do not explicitly log the macro-nutrients of a shared\nmeal. In this work, we present an efficient and straightforward approach to\napproximating macro-nutrients based solely on the titles of food posts. Our\nmethod combines a public food database from the U.S. Department of Agriculture\nwith advanced text embedding techniques. We evaluate the approach on a labeled\nfood dataset, demonstrating its effectiveness, and apply it to over 500,000\nreal-world posts from Reddit's popular /r/food subreddit to uncover trends in\nfood-sharing behavior based on the estimated macro-nutrient content.\nAltogether, this work lays a foundation for researchers and practitioners\naiming to estimate caloric and nutritional content using only text data.", "published": "2025-02-09 10:33:29", "link": "http://arxiv.org/abs/2503.04755v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "LegalSeg: Unlocking the Structure of Indian Legal Judgments Through\n  Rhetorical Role Classification", "abstract": "In this paper, we address the task of semantic segmentation of legal\ndocuments through rhetorical role classification, with a focus on Indian legal\njudgments. We introduce LegalSeg, the largest annotated dataset for this task,\ncomprising over 7,000 documents and 1.4 million sentences, labeled with 7\nrhetorical roles. To benchmark performance, we evaluate multiple\nstate-of-the-art models, including Hierarchical BiLSTM-CRF,\nTransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and\nRole-Aware Transformers, alongside an exploratory RhetoricLLaMA, an\ninstruction-tuned large language model. Our results demonstrate that models\nincorporating broader context, structural relationships, and sequential\nsentence information outperform those relying solely on sentence-level\nfeatures. Additionally, we conducted experiments using surrounding context and\npredicted or actual labels of neighboring sentences to assess their impact on\nclassification accuracy. Despite these advancements, challenges persist in\ndistinguishing between closely related roles and addressing class imbalance.\nOur work underscores the potential of advanced techniques for improving legal\ndocument understanding and sets a strong foundation for future research in\nlegal NLP.", "published": "2025-02-09 10:07:05", "link": "http://arxiv.org/abs/2502.05836v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Generative Framework for Bidirectional Image-Report Understanding in\n  Chest Radiography", "abstract": "The rapid advancements in large language models (LLMs) have unlocked their\npotential for multimodal tasks, where text and visual data are processed\njointly. However, applying LLMs to medical imaging, particularly for chest\nX-rays (CXR), poses significant challenges due to the need for precise\nvisual-textual alignment and the preservation of critical diagnostic details.\nIn this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT),\na novel framework designed to enhance multimodal reasoning and generation for\nCXR understanding. MAViLT incorporates a clinical gradient-weighted\ntokenization process and a hierarchical fine-tuning strategy, enabling it to\ngenerate accurate radiology reports, synthesize realistic CXRs from text, and\nanswer vision-based clinical questions. We evaluate MAViLT on two benchmark\ndatasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art\nresults across all tasks. Human evaluations further validate the clinical\nrelevance and utility of MAViLT, making it a robust tool for real-world medical\napplications. This work demonstrates the feasibility of leveraging LLMs for\nmultimodal medical imaging while addressing key challenges in vision-language\nintegration.", "published": "2025-02-09 15:02:57", "link": "http://arxiv.org/abs/2502.05926v1", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Analysis of LLM as a grammatical feature tagger for African American\n  English", "abstract": "African American English (AAE) presents unique challenges in natural language\nprocessing (NLP). This research systematically compares the performance of\navailable NLP models--rule-based, transformer-based, and large language models\n(LLMs)--capable of identifying key grammatical features of AAE, namely Habitual\nBe and Multiple Negation. These features were selected for their distinct\ngrammatical complexity and frequency of occurrence. The evaluation involved\nsentence-level binary classification tasks, using both zero-shot and few-shot\nstrategies. The analysis reveals that while LLMs show promise compared to the\nbaseline, they are influenced by biases such as recency and unrelated features\nin the text such as formality. This study highlights the necessity for improved\nmodel training and architectural adjustments to better accommodate AAE's unique\nlinguistic characteristics. Data and code are available.", "published": "2025-02-09 19:46:33", "link": "http://arxiv.org/abs/2502.06004v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.7; K.4.2; J.4; J.5"], "primary_category": "cs.CL"}
{"title": "Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning", "abstract": "Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/", "published": "2025-02-09 22:44:45", "link": "http://arxiv.org/abs/2502.06060v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Benchmarking Prompt Sensitivity in Large Language Models", "abstract": "Large language Models (LLMs) are highly sensitive to variations in prompt\nformulation, which can significantly impact their ability to generate accurate\nresponses. In this paper, we introduce a new task, Prompt Sensitivity\nPrediction, and a dataset PromptSET designed to investigate the effects of\nslight prompt variations on LLM performance. Using TriviaQA and HotpotQA\ndatasets as the foundation of our work, we generate prompt variations and\nevaluate their effectiveness across multiple LLMs. We benchmark the prompt\nsensitivity prediction task employing state-of-the-art methods from related\ntasks, including LLM-based self-evaluation, text classification, and query\nperformance prediction techniques. Our findings reveal that existing methods\nstruggle to effectively address prompt sensitivity prediction, underscoring the\nneed to understand how information needs should be phrased for accurate LLM\nresponses.", "published": "2025-02-09 23:01:03", "link": "http://arxiv.org/abs/2502.06065v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Deconstructing Depression Stigma: Integrating AI-driven Data Collection\n  and Analysis with Causal Knowledge Graphs", "abstract": "Mental-illness stigma is a persistent social problem, hampering both\ntreatment-seeking and recovery. Accordingly, there is a pressing need to\nunderstand it more clearly, but analyzing the relevant data is highly\nlabor-intensive. Therefore, we designed a chatbot to engage participants in\nconversations; coded those conversations qualitatively with AI assistance; and,\nbased on those coding results, built causal knowledge graphs to decode stigma.\nThe results we obtained from 1,002 participants demonstrate that conversation\nwith our chatbot can elicit rich information about people's attitudes toward\ndepression, while our AI-assisted coding was strongly consistent with\nhuman-expert coding. Our novel approach combining large language models (LLMs)\nand causal knowledge graphs uncovered patterns in individual responses and\nillustrated the interrelationships of psychological constructs in the dataset\nas a whole. The paper also discusses these findings' implications for HCI\nresearchers in developing digital interventions, decomposing human\npsychological constructs, and fostering inclusive attitudes.", "published": "2025-02-09 23:58:46", "link": "http://arxiv.org/abs/2502.06075v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "ScaffoldGPT: A Scaffold-based Large Language Model for Drug Improvement", "abstract": "Drug optimization has become increasingly crucial in light of fast-mutating\nvirus strains and drug-resistant cancer cells. Nevertheless, it remains\nchallenging as it necessitates retaining the beneficial properties of the\noriginal drug while simultaneously enhancing desired attributes beyond its\nscope. In this work, we aim to tackle this challenge by introducing\nScaffoldGPT, a novel Large Language Model (LLM) designed for drug optimization\nbased on molecular scaffolds. Our work comprises three key components: (1) A\nthree-stage drug optimization approach that integrates pretraining, finetuning,\nand decoding optimization. (2) A uniquely designed two-phase incremental\ntraining approach for pre-training the drug optimization LLM-based generator on\nmolecule scaffold with enhanced performance. (3) A token-level decoding\noptimization strategy, TOP-N, that enabling controlled, reward-guided\ngeneration using pretrained/finetuned LLMs. Finally, by conducting a\ncomprehensive evaluation on COVID and cancer benchmarks, we demonstrate that\nSCAFFOLDGPT outperforms the competing baselines in drug optimization\nbenchmarks, while excelling in preserving the original functional scaffold and\nenhancing desired properties.", "published": "2025-02-09 10:36:33", "link": "http://arxiv.org/abs/2502.06891v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "A New Hybrid Intelligent Approach for Multimodal Detection of Suspected\n  Disinformation on TikTok", "abstract": "In the context of the rapid dissemination of multimedia content, identifying\ndisinformation on social media platforms such as TikTok represents a\nsignificant challenge. This study introduces a hybrid framework that combines\nthe computational power of deep learning with the interpretability of fuzzy\nlogic to detect suspected disinformation in TikTok videos. The methodology is\ncomprised of two core components: a multimodal feature analyser that extracts\nand evaluates data from text, audio, and video; and a multimodal disinformation\ndetector based on fuzzy logic. These systems operate in conjunction to evaluate\nthe suspicion of spreading disinformation, drawing on human behavioural cues\nsuch as body language, speech patterns, and text coherence. Two experiments\nwere conducted: one focusing on context-specific disinformation and the other\non the scalability of the model across broader topics. For each video\nevaluated, high-quality, comprehensive, well-structured reports are generated,\nproviding a detailed view of the disinformation behaviours.", "published": "2025-02-09 12:37:48", "link": "http://arxiv.org/abs/2502.06893v1", "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SC"], "primary_category": "cs.CV"}
{"title": "Enabling Autoregressive Models to Fill In Masked Tokens", "abstract": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.", "published": "2025-02-09 20:02:05", "link": "http://arxiv.org/abs/2502.06901v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emergence of Episodic Memory in Transformers: Characterizing Changes in\n  Temporal Structure of Attention Scores During Training", "abstract": "We investigate in-context temporal biases in attention heads and transformer\noutputs. Using cognitive science methodologies, we analyze attention scores and\noutputs of the GPT-2 models of varying sizes. Across attention heads, we\nobserve effects characteristic of human episodic memory, including temporal\ncontiguity, primacy and recency. Transformer outputs demonstrate a tendency\ntoward in-context serial recall. Importantly, this effect is eliminated after\nthe ablation of the induction heads, which are the driving force behind the\ncontiguity effect. Our findings offer insights into how transformers organize\ninformation temporally during in-context learning, shedding light on their\nsimilarities and differences with human memory and learning.", "published": "2025-02-09 20:20:37", "link": "http://arxiv.org/abs/2502.06902v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness", "abstract": "Numerous recent studies have shown that Large Language Models (LLMs) are\nbiased towards a Western and Anglo-centric worldview, which compromises their\nusefulness in non-Western cultural settings. However, \"culture\" is a complex,\nmultifaceted topic, and its awareness, representation, and modeling in LLMs and\nLLM-based applications can be defined and measured in numerous ways. In this\nposition paper, we ask what does it mean for an LLM to possess \"cultural\nawareness\", and through a thought experiment, which is an extension of the\nOctopus test proposed by Bender and Koller (2020), we argue that it is not\ncultural awareness or knowledge, rather meta-cultural competence, which is\nrequired of an LLM and LLM-based AI system that will make it useful across\nvarious, including completely unseen, cultures. We lay out the principles of\nmeta-cultural competence AI systems, and discuss ways to measure and model\nthose.", "published": "2025-02-09 04:51:59", "link": "http://arxiv.org/abs/2502.09637v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "MixLLM: Dynamic Routing in Mixed Large Language Models", "abstract": "Large Language Models (LLMs) exhibit potential artificial generic\nintelligence recently, however, their usage is costly with high response\nlatency. Given mixed LLMs with their own strengths and weaknesses, LLM routing\naims to identify the most suitable model for each query in the stream to\nmaximize response quality and minimize cost and latency. However, the\nchallenges involve: (1) dynamic trade-offs among quality, cost, and latency;\n(2) enabling continual learning in deployed systems; and (3) navigating a\nvarying (e.g., new LLM addition or old LLM removal) set of LLM candidates over\ntime. To bridge these gaps, we develop MixLLM, a dynamic\ncontextual-bandit-based routing system for query-LLM assignment. Specifically,\nwe first leverage query tags to enhance query embeddings for the routing task.\nNext, we design lightweight prediction models to estimate the response\nqualities and costs of queries over LLMs. We then devise a meta-decision maker\nto choose the query-LLM assignments to best tradeoff response quality, cost,\nand latency. Finally, the system benefits from continual training, allowing it\nto adapt to evolving queries and user feedback over time. Our extensive\nexperiments show that MixLLM achieves the best trade-offs in response quality,\ncost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the\ntime constraint).", "published": "2025-02-09 02:26:15", "link": "http://arxiv.org/abs/2502.18482v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "N/A"], "primary_category": "cs.CL"}
{"title": "Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data\n  Curators", "abstract": "The rapid advancement in building large language models (LLMs) has\nintensified competition among big-tech companies and AI startups. In this\nregard, model evaluations are critical for product and investment-related\ndecision-making. While open evaluation sets like MMLU initially drove progress,\nconcerns around data contamination and data bias have constantly questioned\ntheir reliability. As a result, it has led to the rise of private data curators\nwho have begun conducting hidden evaluations with high-quality self-curated\ntest prompts and their own expert annotators. In this paper, we argue that\ndespite potential advantages in addressing contamination issues, private\nevaluations introduce inadvertent financial and evaluation risks. In\nparticular, the key concerns include the potential conflict of interest arising\nfrom private data curators' business relationships with their clients (leading\nLLM firms). In addition, we highlight that the subjective preferences of\nprivate expert annotators will lead to inherent evaluation bias towards the\nmodels trained with the private curators' data. Overall, this paper lays the\nfoundation for studying the risks of private evaluations that can lead to\nwide-ranging community discussions and policy changes.", "published": "2025-02-09 23:57:33", "link": "http://arxiv.org/abs/2503.04756v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis\n  Care", "abstract": "Under-resourced or rural hospitals have limited access to medical specialists\nand healthcare professionals, which can negatively impact patient outcomes in\nsepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care)\nframework, which integrates a team of specialized AI agents for sepsis care.\nThe sepsis AI agent team includes five doctor agents, four health professional\nagents, and a risk prediction model agent, with an additional 33 doctor agents\navailable for consultations. Ten attending physicians at a teaching hospital\nevaluated this framework, spending approximately 40 minutes on the web-based\nMATEC application and participating in the 5-point Likert scale survey (rated\nfrom 1-unfavorable to 5-favorable). The physicians found the MATEC framework\nvery useful (Median=4, P=0.01), and very accurate (Median=4, P<0.01). This\npilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can\npotentially be useful in assisting medical professionals, particularly in\nunder-resourced hospital settings.", "published": "2025-02-09 12:46:13", "link": "http://arxiv.org/abs/2503.16433v1", "categories": ["cs.HC", "cs.CL", "cs.MA"], "primary_category": "cs.HC"}
{"title": "Target Speaker Lipreading by Audio-Visual Self-Distillation Pretraining\n  and Speaker Adaptation", "abstract": "Lipreading is an important technique for facilitating human-computer\ninteraction in noisy environments. Our previously developed self-supervised\nlearning method, AV2vec, which leverages multimodal self-distillation, has\ndemonstrated promising performance in speaker-independent lipreading on the\nEnglish LRS3 dataset. However, AV2vec faces challenges such as high training\ncosts and a potential scarcity of audio-visual data for lipreading in languages\nother than English, such as Chinese. Additionally, most studies concentrate on\nspeakerindependent lipreading models, which struggle to account for the\nsubstantial variation in speaking styles across di?erent speakers. To address\nthese issues, we propose a comprehensive approach. First, we investigate\ncross-lingual transfer learning, adapting a pre-trained AV2vec model from a\nsource language and optimizing it for the lipreading task in a target language.\nSecond, we enhance the accuracy of lipreading for specific target speakers\nthrough a speaker adaptation strategy, which is not extensively explored in\nprevious research. Third, after analyzing the complementary performance of\nlipreading with lip region-of-interest (ROI) and face inputs, we introduce a\nmodel ensembling strategy that integrates both, signi?cantly boosting model\nperformance. Our method achieved a character error rate (CER) of 77.3% on the\nevaluation set of the ChatCLR dataset, which is lower than the top result from\nthe 2024 Chat-scenario Chinese Lipreading Challenge.", "published": "2025-02-09 03:37:01", "link": "http://arxiv.org/abs/2502.05758v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Non-invasive electromyographic speech neuroprosthesis: a geometric\n  perspective", "abstract": "In this article, we present a high-bandwidth egocentric neuromuscular speech\ninterface for translating silently voiced speech articulations into textand\naudio. Specifically, we collect electromyogram (EMG) signals from multiple\narticulatorysites on the face and neck as individuals articulate speech in an\nalaryngeal manner to perform EMG-to-text or EMG-to-audio translation. Such an\ninterface is useful for restoring audible speech in individuals who have lost\nthe ability to speak intelligibly due to laryngectomy, neuromuscular disease,\nstroke, or trauma-induced damage (e.g., radiotherapy toxicity) to speech\narticulators. Previous works have focused on training text or speech synthesis\nmodels using EMG collected during audible speech articulations or by\ntransferring audio targets from EMG collected during audible articulation to\nEMG collected during silent articulation. However, such paradigms are not\nsuited for individuals who have already lost the ability to audibly articulate\nspeech. We are the first to present an alignment-free EMG-to-text and\nEMG-to-audio conversion using only EMG collected during silently articulated\nspeech in an open-sourced manner. On a limited vocabulary corpora, our approach\nachieves almost 2.4x improvement in word error rate with a model that is 25x\nsmaller by leveraging the inherent geometry of EMG.", "published": "2025-02-09 03:49:27", "link": "http://arxiv.org/abs/2502.05762v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Synergistic Effects of Knowledge Distillation and Structured Pruning for\n  Self-Supervised Speech Models", "abstract": "Traditionally, Knowledge Distillation (KD) is used for model compression,\noften leading to suboptimal performance. In this paper, we evaluate the impact\nof combining KD loss with alternative pruning techniques, including Low-Rank\nFactorization (LRF) and l0 regularization, on a conformer-based pre-trained\nnetwork under the paradigm of Self-Supervised Learning (SSL). We also propose a\nstrategy to jointly prune and train an RNN-T-based ASR model, demonstrating\nthat this approach yields superior performance compared to pruning a\npre-trained network first and then using it for ASR training. This approach led\nto a significant reduction in word error rate: l0 and KD combination achieves\nthe best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER)\nimprovement over the baseline, while LRF and KD combination yields the best\nresults for streaming ASR, improving RWER by 13.4%.", "published": "2025-02-09 10:17:25", "link": "http://arxiv.org/abs/2502.05837v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Representation Learning via Knowledge Distillation from\n  Speech Foundation Models", "abstract": "Audio-visual representation learning is crucial for advancing multimodal\nspeech processing tasks, such as lipreading and audio-visual speech\nrecognition. Recently, speech foundation models (SFMs) have shown remarkable\ngeneralization capabilities across various speech-related tasks. Building on\nthis progress, we propose an audio-visual representation learning model that\nleverages cross-modal knowledge distillation from SFMs. In our method, SFMs\nserve as teachers, from which multi-layer hidden representations are extracted\nusing clean audio inputs. We also introduce a multi-teacher ensemble method to\ndistill the student, which receives audio-visual data as inputs. A novel\nrepresentational knowledge distillation loss is employed to train the student\nduring pretraining, which is also applied during finetuning to further enhance\nthe performance on downstream tasks. Our experiments utilized both a\nself-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results\ndemonstrated that our proposed method achieved superior or at least comparable\nperformance to previous state-of-the-art baselines across automatic speech\nrecognition, visual speech recognition, and audio-visual speech recognition\ntasks. Additionally, comprehensive ablation studies and the visualization of\nlearned representations were conducted to evaluate the effectiveness of our\nproposed method.", "published": "2025-02-09 03:56:20", "link": "http://arxiv.org/abs/2502.05766v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the use of Performer and Agent Attention for Spoken Language\n  Identification", "abstract": "One of the methods for language Identification (LID) involves deriving speech\nrepresentation from pre-trained models using self-supervised learning, followed\nby fine-tuning the model for the LID task. State-of-the-art approaches for LID\nuse an attention-based statistical pooling layer to facilitate the aggregation\nof contextual information across time frames of the embedding vectors extracted\nfrom the pre-trained model. In this paper, we delve into exploring recently\nproposed attention mechanisms, namely performer and agent-attention, in\nconjunction with the statistical pooling layer. The LID experiments are\nperformed on three datasets: VoxPopuli, FLEURS, and VoxLingua. We compare their\nperformance against vanilla self-attention. Our findings suggest that\nperformer-attention outperforms self-attention and agent-attention exhibits\ncomparable or occasionally superior performance to self-attention, while also\nbeing computationally less expensive.", "published": "2025-02-09 10:25:42", "link": "http://arxiv.org/abs/2502.05841v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large Language Model-based Nonnegative Matrix Factorization For\n  Cardiorespiratory Sound Separation", "abstract": "This study represents the first integration of large language models (LLMs)\nwith non-negative matrix factorization (NMF), marking a novel advancement in\nthe source separation field. The LLM is employed in two unique ways: enhancing\nthe separation results by providing detailed insights for disease prediction\nand operating in a feedback loop to optimize a fundamental frequency penalty\nadded to the NMF cost function. We tested the algorithm on two datasets: 100\nsynthesized mixtures of real measurements, and 210 recordings of heart and lung\nsounds from a clinical manikin including both individual and mixed sounds,\ncaptured using a digital stethoscope. The approach consistently outperformed\nexisting methods, demonstrating its potential to significantly enhance medical\nsound analysis for disease diagnostics.", "published": "2025-02-09 03:27:53", "link": "http://arxiv.org/abs/2502.05757v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Temporal Working Memory: Query-Guided Segment Refinement for Enhanced\n  Multimodal Understanding", "abstract": "Multimodal foundation models (MFMs) have demonstrated significant success in\ntasks such as visual captioning, question answering, and image-text retrieval.\nHowever, these models face inherent limitations due to their finite internal\ncapacity, which restricts their ability to process extended temporal sequences,\na crucial requirement for comprehensive video and audio analysis. To overcome\nthese challenges, we introduce a specialized cognitive module, temporal working\nmemory (TWM), which aims to enhance the temporal modeling capabilities of MFMs.\nIt selectively retains task-relevant information across temporal dimensions,\nensuring that critical details are preserved throughout the processing of video\nand audio content. The TWM uses a query-guided attention approach to focus on\nthe most informative multimodal segments within temporal sequences. By\nretaining only the most relevant content, TWM optimizes the use of the model's\nlimited capacity, enhancing its temporal modeling ability. This plug-and-play\nmodule can be easily integrated into existing MFMs. With our TWM, nine\nstate-of-the-art models exhibit significant performance improvements across\ntasks such as video captioning, question answering, and video-text retrieval.\nBy enhancing temporal modeling, TWM extends the capability of MFMs to handle\ncomplex, time-sensitive data effectively. Our code is available at\nhttps://github.com/xid32/NAACL_2025_TWM.", "published": "2025-02-09 20:26:30", "link": "http://arxiv.org/abs/2502.06020v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
