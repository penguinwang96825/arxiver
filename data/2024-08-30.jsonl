{"title": "Bitcoin ETF: Opportunities and risk", "abstract": "The year 2024 witnessed a major development in the cryptocurrency industry\nwith the long-awaited approval of spot Bitcoin exchange-traded funds (ETFs).\nThis innovation provides investors with a new, regulated path to gain exposure\nto Bitcoin through a familiar investment vehicle (Kumar et al., 2024). However,\nunlike traditional ETFs that directly hold underlying assets, Bitcoin ETFs rely\non a creation and redemption process managed by authorized participants (APs).\nThis unique structure introduces distinct characteristics in terms of\npremium/discount behavior compared to traditional ETFs. This paper investigates\nthe premium and discount patterns observed in Bitcoin ETFs during first\nfour-month period (January 11th, 2024, to May 17th, 2024). Our analysis reveals\nthat these patterns differ significantly from those observed in traditional\nindex ETFs, potentially exposing investors to additional risk factors. By\nidentifying and analyzing these risk factors associated with Bitcoin ETF\npremiums/discounts, this paper aims to achieve two key objectives: Enhance\nmarket understanding: Equip and market and investors with a deeper\ncomprehension of the unique liquidity risks inherent in Bitcoin ETFs. Provide a\nclearer risk management frameworks: Offer a clearer perspective on the\nrisk-return profile of digital asset ETFs, specifically focusing on Bitcoin\nETFs. Through a thorough analysis of premium/discount behavior and the\nunderlying factors contributing to it, this paper strives to contribute\nvaluable insights for investors navigating the evolving landscape of digital\nasset investments", "published": "2024-08-30 21:57:31", "link": "http://arxiv.org/abs/2409.00270v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World\n  Scenarios", "abstract": "Recent Text-to-SQL methods leverage large language models (LLMs) by\nincorporating feedback from the database management system. While these methods\neffectively address execution errors in SQL queries, they struggle with\ndatabase mismatches -- errors that do not trigger execution exceptions.\nDatabase mismatches include issues such as condition mismatches and stricter\nconstraint mismatches, both of which are more prevalent in real-world\nscenarios. To address these challenges, we propose a tool-assisted agent\nframework for SQL inspection and refinement, equipping the LLM-based agent with\ntwo specialized tools: a retriever and a detector, designed to diagnose and\ncorrect SQL queries with database mismatches. These tools enhance the\ncapability of LLMs to handle real-world queries more effectively. We also\nintroduce Spider-Mismatch, a new dataset specifically constructed to reflect\nthe condition mismatch problems encountered in real-world scenarios.\nExperimental results demonstrate that our method achieves the highest\nperformance on the averaged results of the Spider and Spider-Realistic datasets\nin few-shot settings, and it significantly outperforms baseline methods on the\nmore realistic dataset, Spider-Mismatch.", "published": "2024-08-30 03:38:37", "link": "http://arxiv.org/abs/2408.16991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InkubaLM: A small language model for low-resource African languages", "abstract": "High-resource language models often fall short in the African context, where\nthere is a critical need for models that are efficient, accessible, and locally\nrelevant, even amidst significant computing and data constraints. This paper\nintroduces InkubaLM, a small language model with 0.4 billion parameters, which\nachieves performance comparable to models with significantly larger parameter\ncounts and more extensive training data on tasks such as machine translation,\nquestion-answering, AfriMMLU, and the AfriXnli task. Notably, InkubaLM\noutperforms many larger models in sentiment analysis and demonstrates\nremarkable consistency across multiple languages. This work represents a\npivotal advancement in challenging the conventional paradigm that effective\nlanguage models must rely on substantial resources. Our model and datasets are\npublicly available at https://huggingface.co/lelapa to encourage research and\ndevelopment on low-resource languages.", "published": "2024-08-30 05:42:31", "link": "http://arxiv.org/abs/2408.17024v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Text to Emotion: Unveiling the Emotion Annotation Capabilities of\n  LLMs", "abstract": "Training emotion recognition models has relied heavily on human annotated\ndata, which present diversity, quality, and cost challenges. In this paper, we\nexplore the potential of Large Language Models (LLMs), specifically GPT4, in\nautomating or assisting emotion annotation. We compare GPT4 with supervised\nmodels and or humans in three aspects: agreement with human annotations,\nalignment with human perception, and impact on model training. We find that\ncommon metrics that use aggregated human annotations as ground truth can\nunderestimate the performance, of GPT-4 and our human evaluation experiment\nreveals a consistent preference for GPT-4 annotations over humans across\nmultiple datasets and evaluators. Further, we investigate the impact of using\nGPT-4 as an annotation filtering process to improve model training. Together,\nour findings highlight the great potential of LLMs in emotion annotation tasks\nand underscore the need for refined evaluation methodologies.", "published": "2024-08-30 05:50:15", "link": "http://arxiv.org/abs/2408.17026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using\n  Prefix-Tuning", "abstract": "Teaching new information to pre-trained large language models (PLM) is a\ncrucial but challenging task. Model adaptation techniques, such as fine-tuning\nand parameter-efficient training have been shown to store new facts at a slow\nrate; continual learning is an option but is costly and prone to catastrophic\nforgetting. This work studies and quantifies how PLM may learn and remember new\nworld knowledge facts that do not occur in their pre-training corpus, which\nonly contains world knowledge up to a certain date. To that purpose, we first\npropose Novel-WD, a new dataset consisting of sentences containing novel facts\nextracted from recent Wikidata updates, along with two evaluation tasks in the\nform of causal language modeling and multiple choice questions (MCQ). We make\nthis dataset freely available to the community, and release a procedure to\nlater build new versions of similar datasets with up-to-date information. We\nalso explore the use of prefix-tuning for novel information learning, and\nanalyze how much information can be stored within a given prefix. We show that\na single fact can reliably be encoded within a single prefix, and that the\nprefix capacity increases with its length and with the base model size.", "published": "2024-08-30 07:54:50", "link": "http://arxiv.org/abs/2408.17070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for\n  Retrieval-Augmented Large Language Models", "abstract": "In a real-world RAG system, the current query often involves spoken ellipses\nand ambiguous references from dialogue contexts, necessitating query rewriting\nto better describe user's information needs. However, traditional context-based\nrewriting has minimal enhancement on downstream generation tasks due to the\nlengthy process from query rewriting to response generation. Some researchers\ntry to utilize reinforcement learning with generation feedback to assist the\nrewriter, but these sparse rewards provide little guidance in most cases,\nleading to unstable training and generation results. We find that user's needs\nare also reflected in the gold document, retrieved documents and ground truth.\nTherefore, by feeding back these multi-aspect dense rewards to query rewriting,\nmore stable and satisfactory responses can be achieved. In this paper, we\npropose a novel query rewriting method MaFeRw, which improves RAG performance\nby integrating multi-aspect feedback from both the retrieval process and\ngenerated results. Specifically, we first use manual data to train a T5 model\nfor the rewriter initialization. Next, we design three metrics as reinforcement\nlearning feedback: the similarity between the rewritten query and the gold\ndocument, the ranking metrics, and ROUGE between the generation and the ground\ntruth. Inspired by RLAIF, we train three kinds of reward models for the above\nmetrics to achieve more efficient training. Finally, we combine the scores of\nthese reward models as feedback, and use PPO algorithm to explore the optimal\nquery rewriting strategy. Experimental results on two conversational RAG\ndatasets demonstrate that MaFeRw achieves superior generation metrics and more\nstable training compared to baselines.", "published": "2024-08-30 07:57:30", "link": "http://arxiv.org/abs/2408.17072v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Extraction of Clinical Event Contextual Properties from\n  Electronic Health Records: A Comparative Study", "abstract": "Electronic Health Records are large repositories of valuable clinical data,\nwith a significant portion stored in unstructured text format. This textual\ndata includes clinical events (e.g., disorders, symptoms, findings, medications\nand procedures) in context that if extracted accurately at scale can unlock\nvaluable downstream applications such as disease prediction. Using an existing\nNamed Entity Recognition and Linking methodology, MedCAT, these identified\nconcepts need to be further classified (contextualised) for their relevance to\nthe patient, and their temporal and negated status for example, to be useful\ndownstream. This study performs a comparative analysis of various natural\nlanguage models for medical text classification. Extensive experimentation\nreveals the effectiveness of transformer-based language models, particularly\nBERT. When combined with class imbalance mitigation techniques, BERT\noutperforms Bi-LSTM models by up to 28% and the baseline BERT model by up to\n16% for recall of the minority classes. The method has been implemented as part\nof CogStack/MedCAT framework and made available to the community for further\nresearch.", "published": "2024-08-30 10:28:49", "link": "http://arxiv.org/abs/2408.17181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Tailored Recovery of Lexical Diversity in Literary Machine\n  Translation", "abstract": "Machine translations are found to be lexically poorer than human\ntranslations. The loss of lexical diversity through MT poses an issue in the\nautomatic translation of literature, where it matters not only what is written,\nbut also how it is written. Current methods for increasing lexical diversity in\nMT are rigid. Yet, as we demonstrate, the degree of lexical diversity can vary\nconsiderably across different novels. Thus, rather than aiming for the rigid\nincrease of lexical diversity, we reframe the task as recovering what is lost\nin the machine translation process. We propose a novel approach that consists\nof reranking translation candidates with a classifier that distinguishes\nbetween original and translated text. We evaluate our approach on 31\nEnglish-to-Dutch book translations, and find that, for certain books, our\napproach retrieves lexical diversity scores that are close to human\ntranslation.", "published": "2024-08-30 14:12:04", "link": "http://arxiv.org/abs/2408.17308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Generative Language Models in Classification Tasks:\n  Performance and Self-Evaluation Capabilities in the Environmental and Climate\n  Change Domain", "abstract": "This paper examines the performance of two Large Language Models (LLMs),\nGPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three\ndifferent classification tasks within the climate change (CC) and environmental\ndomain. Employing BERT-based models as a baseline, we compare their efficacy\nagainst these transformer-based models. Additionally, we assess the models'\nself-evaluation capabilities by analyzing the calibration of verbalized\nconfidence scores in these text classification tasks. Our findings reveal that\nwhile BERT-based models generally outperform both the LLMs and SLM, the\nperformance of the large generative models is still noteworthy. Furthermore,\nour calibration analysis reveals that although Gemma is well-calibrated in\ninitial tasks, it thereafter produces inconsistent results; Llama is reasonably\ncalibrated, and GPT consistently exhibits strong calibration. Through this\nresearch, we aim to contribute to the ongoing discussion on the utility and\neffectiveness of generative LMs in addressing some of the planet's most urgent\nissues, highlighting their strengths and limitations in the context of ecology\nand CC.", "published": "2024-08-30 15:52:41", "link": "http://arxiv.org/abs/2408.17362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic\n  CheckLists", "abstract": "Traditional benchmarking in NLP typically involves using static held-out test\nsets. However, this approach often results in an overestimation of performance\nand lacks the ability to offer comprehensive, interpretable, and dynamic\nassessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)\nand CheckList (Ribeiro et al., 2020) have addressed these limitations through\nbehavioral testing of NLP models with test types generated by a multistep\nhuman-annotated pipeline. Unfortunately, manually creating a variety of test\ntypes requires much human labor, often at prohibitive cost. In this work, we\npropose SYNTHEVAL, a hybrid behavioral testing framework that leverages large\nlanguage models (LLMs) to generate a wide range of test types for a\ncomprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via\nLLMs using controlled generation, and then identifies challenging examples by\ncomparing the predictions made by LLMs with task-specific NLP models. In the\nlast stage, human experts investigate the challenging examples, manually design\ntemplates, and identify the types of failures the taskspecific models\nconsistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment\nanalysis and toxic language detection, and show that our framework is effective\nin identifying weaknesses of strong models on these tasks. We share our code in\nhttps://github.com/Loreley99/SynthEval_CheckList.", "published": "2024-08-30 17:41:30", "link": "http://arxiv.org/abs/2408.17437v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The creative psychometric item generator: a framework for item\n  generation and validation using large language models", "abstract": "Increasingly, large language models (LLMs) are being used to automate\nworkplace processes requiring a high degree of creativity. While much prior\nwork has examined the creativity of LLMs, there has been little research on\nwhether they can generate valid creativity assessments for humans despite the\nincreasingly central role of creativity in modern economies. We develop a\npsychometrically inspired framework for creating test items (questions) for a\nclassic free-response creativity test: the creative problem-solving (CPS) task.\nOur framework, the creative psychometric item generator (CPIG), uses a mixture\nof LLM-based item generators and evaluators to iteratively develop new prompts\nfor writing CPS items, such that items from later iterations will elicit more\ncreative responses from test takers. We find strong empirical evidence that\nCPIG generates valid and reliable items and that this effect is not\nattributable to known biases in the evaluation process. Our findings have\nimplications for employing LLMs to automatically generate valid and reliable\ncreativity tests for humans and AI.", "published": "2024-08-30 18:31:02", "link": "http://arxiv.org/abs/2409.00202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Document-level Argument Extraction with Definition-augmented\n  Heuristic-driven Prompting for LLMs", "abstract": "Event Argument Extraction (EAE) is pivotal for extracting structured\ninformation from unstructured text, yet it remains challenging due to the\ncomplexity of real-world document-level EAE. We propose a novel\nDefinition-augmented Heuristic-driven Prompting (DHP) method to enhance the\nperformance of Large Language Models (LLMs) in document-level EAE. Our method\nintegrates argument extraction-related definitions and heuristic rules to guide\nthe extraction process, reducing error propagation and improving task accuracy.\nWe also employ the Chain-of-Thought (CoT) method to simulate human reasoning,\nbreaking down complex problems into manageable sub-problems. Experiments have\nshown that our method achieves a certain improvement in performance over\nexisting prompting methods and few-shot supervised learning on document-level\nEAE datasets. The DHP method enhances the generalization capability of LLMs and\nreduces reliance on large annotated datasets, offering a novel research\nperspective for document-level EAE.", "published": "2024-08-30 19:03:14", "link": "http://arxiv.org/abs/2409.00214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Address Open-Target Stance Detection?", "abstract": "Stance detection (SD) identifies the text position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs) from\nGPT, Gemini, Llama, and Mistral families, comparing their performance to the\nonly existing work, Target-Stance Extraction (TSE), which benefits from\npredefined targets. Unlike TSE, OTSD removes the dependency of a predefined\nlist, making target generation and evaluation more challenging. We also provide\na metric for evaluating target quality that correlates well with human\njudgment. Our experiments reveal that LLMs outperform TSE in target generation,\nboth when the real target is explicitly and not explicitly mentioned in the\ntext. Similarly, LLMs overall surpass TSE in stance detection for both explicit\nand non-explicit cases. However, LLMs struggle in both target generation and\nstance detection when the target is not explicit.", "published": "2024-08-30 19:26:15", "link": "http://arxiv.org/abs/2409.00222v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiverseDialogue: A Methodology for Designing Chatbots with Human-Like\n  Diversity", "abstract": "Large Language Models (LLMs), which simulate human users, are frequently\nemployed to evaluate chatbots in applications such as tutoring and customer\nservice. Effective evaluation necessitates a high degree of human-like\ndiversity within these simulations. In this paper, we demonstrate that\nconversations generated by GPT-4o mini, when used as simulated human\nparticipants, systematically differ from those between actual humans across\nmultiple linguistic features. These features include topic variation, lexical\nattributes, and both the average behavior and diversity (variance) of the\nlanguage used. To address these discrepancies, we propose an approach that\nautomatically generates prompts for user simulations by incorporating features\nderived from real human interactions, such as age, gender, emotional tone, and\nthe topics discussed. We assess our approach using differential language\nanalysis combined with deep linguistic inquiry. Our method of prompt\noptimization, tailored to target specific linguistic features, shows\nsignificant improvements. Specifically, it enhances the human-likeness of LLM\nchatbot conversations, increasing their linguistic diversity. On average, we\nobserve a 54 percent reduction in the error of average features between human\nand LLM-generated conversations. This method of constructing chatbot sets with\nhuman-like diversity holds great potential for enhancing the evaluation process\nof user-facing bots.", "published": "2024-08-30 21:33:58", "link": "http://arxiv.org/abs/2409.00262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging a Cognitive Model to Measure Subjective Similarity of Human\n  and GPT-4 Written Content", "abstract": "Cosine similarity between two documents can be computed using token\nembeddings formed by Large Language Models (LLMs) such as GPT-4, and used to\ncategorize those documents across a range of uses. However, these similarities\nare ultimately dependent on the corpora used to train these LLMs, and may not\nreflect subjective similarity of individuals or how their biases and\nconstraints impact similarity metrics. This lack of cognitively-aware\npersonalization of similarity metrics can be particularly problematic in\neducational and recommendation settings where there is a limited number of\nindividual judgements of category or preference, and biases can be particularly\nrelevant. To address this, we rely on an integration of an Instance-Based\nLearning (IBL) cognitive model with LLM embeddings to develop the\nInstance-Based Individualized Similarity (IBIS) metric. This similarity metric\nis beneficial in that it takes into account individual biases and constraints\nin a manner that is grounded in the cognitive mechanisms of decision making. To\nevaluate the IBIS metric, we also introduce a dataset of human categorizations\nof emails as being either dangerous (phishing) or safe (ham). This dataset is\nused to demonstrate the benefits of leveraging a cognitive model to measure the\nsubjective similarity of human participants in an educational setting.", "published": "2024-08-30 21:54:13", "link": "http://arxiv.org/abs/2409.00269v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding frames with BERT: A transformer-based approach to generic news\n  frame detection", "abstract": "Framing is among the most extensively used concepts in the field of\ncommunication science. The availability of digital data offers new\npossibilities for studying how specific aspects of social reality are made more\nsalient in online communication but also raises challenges related to the\nscaling of framing analysis and its adoption to new research areas (e.g.\nstudying the impact of artificial intelligence-powered systems on\nrepresentation of societally relevant issues). To address these challenges, we\nintroduce a transformer-based approach for generic news frame detection in\nAnglophone online content. While doing so, we discuss the composition of the\ntraining and test datasets, the model architecture, and the validation of the\napproach and reflect on the possibilities and limitations of the automated\ndetection of generic news frames.", "published": "2024-08-30 22:05:01", "link": "http://arxiv.org/abs/2409.00272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple stochastic processes behind Menzerath's Law", "abstract": "This paper revisits Menzerath's Law, also known as the Menzerath-Altmann Law,\nwhich models a relationship between the length of a linguistic construct and\nthe average length of its constituents. Recent findings indicate that simple\nstochastic processes can display Menzerathian behaviour, though existing models\nfail to accurately reflect real-world data. If we adopt the basic principle\nthat a word can change its length in both syllables and phonemes, where the\ncorrelation between these variables is not perfect and these changes are of a\nmultiplicative nature, we get bivariate log-normal distribution. The present\npaper shows, that from this very simple principle, we obtain the classic\nAltmann model of the Menzerath-Altmann Law. If we model the joint distribution\nseparately and independently from the marginal distributions, we can obtain an\neven more accurate model by using a Gaussian copula. The models are confronted\nwith empirical data, and alternative approaches are discussed.", "published": "2024-08-30 22:20:50", "link": "http://arxiv.org/abs/2409.00279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling", "abstract": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong", "published": "2024-08-30 02:01:56", "link": "http://arxiv.org/abs/2408.16967v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for\n  Efficient LLM Sampling", "abstract": "Self-Consistency mitigates hallucinations in Large Language Models (LLMs) by\nsampling multiple reasoning paths,but it lacks a systematic approach to\ndetermine the optimal number of samples or select the most faithful rationale.\nTo address this limitation, we introduce Reasoning-Aware Self-Consistency\n(RASC), a novel framework that enhances sampling efficiency and reasoning\nfaithfulness by dynamically evaluating both outputs and rationales. RASC\nassesses the quality of reasoning and the consistency of answers for each\ngenerated sample, using these assessments to guide early stopping decisions and\nrationale selection. The framework employs criteria-based stopping and weighted\nmajority voting, enabling more informed choices on when to halt sampling and\nwhich rationale to select. Our comprehensive experiments across diverse\nquestion-answering datasets demonstrate that RASC outperforms existing methods,\nreducing sample usage by approximately 70% while maintaining accuracy.\nMoreover, RASC facilitates the selection of high-fidelity rationales, thereby\nimproving the faithfulness of LLM outputs. Our approach effectively addresses\nthe efficiency-accuracy trade-off in LLM reasoning tasks, offering a new\nperspective for more nuanced, faithful, and effective utilization of LLMs in\nresource-constrained environments.", "published": "2024-08-30 05:14:59", "link": "http://arxiv.org/abs/2408.17017v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flexible and Effective Mixing of Large Language Models into a Mixture of\n  Domain Experts", "abstract": "We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.", "published": "2024-08-30 13:28:45", "link": "http://arxiv.org/abs/2408.17280v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bridging Domain Knowledge and Process Discovery Using Large Language\n  Models", "abstract": "Discovering good process models is essential for different process analysis\ntasks such as conformance checking and process improvements. Automated process\ndiscovery methods often overlook valuable domain knowledge. This knowledge,\nincluding insights from domain experts and detailed process documentation,\nremains largely untapped during process discovery. This paper leverages Large\nLanguage Models (LLMs) to integrate such knowledge directly into process\ndiscovery. We use rules derived from LLMs to guide model construction, ensuring\nalignment with both domain knowledge and actual process executions. By\nintegrating LLMs, we create a bridge between process knowledge expressed in\nnatural language and the discovery of robust process models, advancing process\ndiscovery methodologies significantly. To showcase the usability of our\nframework, we conducted a case study with the UWV employee insurance agency,\ndemonstrating its practical benefits and effectiveness.", "published": "2024-08-30 14:23:40", "link": "http://arxiv.org/abs/2408.17316v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "NDP: Next Distribution Prediction as a More Broad Target", "abstract": "Large language models (LLMs) trained on next-token prediction (NTP) paradigm\nhave demonstrated powerful capabilities. However, the existing NTP paradigm\ncontains several limitations, particularly related to planned task\ncomplications and error propagation during inference. In our work, we extend\nthe critique of NTP, highlighting its limitation also due to training with a\nnarrow objective: the prediction of a sub-optimal one-hot distribution. To\nsupport this critique, we conducted a pre-experiment treating the output\ndistribution from powerful LLMs as efficient world data compression. By\nevaluating the similarity between the $n$-gram distribution and the one-hot\ndistribution with LLMs, we observed that the $n$-gram distributions align more\nclosely with the output distribution of LLMs. Based on this insight, we\nintroduce Next Distribution Prediction (NDP), which uses $n$-gram distributions\nto replace the one-hot targets, enhancing learning without extra online\ntraining time. We conducted experiments across translation, general task,\nlanguage transfer, and medical domain adaptation. Compared to NTP, NDP can\nachieve up to +2.97 COMET improvement in translation tasks, +0.61 average\nimprovement in general tasks, and incredible +10.75 average improvement in the\nmedical domain. This demonstrates the concrete benefits of addressing the\ntarget narrowing problem, pointing to a new direction for future work on\nimproving NTP.", "published": "2024-08-30 16:13:49", "link": "http://arxiv.org/abs/2408.17377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language\n  Models", "abstract": "The digitisation of historical print media archives is crucial for increasing\naccessibility to contemporary records. However, the process of Optical\nCharacter Recognition (OCR) used to convert physical records to digital text is\nprone to errors, particularly in the case of newspapers and periodicals due to\ntheir complex layouts. This paper introduces Context Leveraging OCR Correction\n(CLOCR-C), which utilises the infilling and context-adaptive abilities of\ntransformer-based language models (LMs) to improve OCR quality. The study aims\nto determine if LMs can perform post-OCR correction, improve downstream NLP\ntasks, and the value of providing the socio-cultural context as part of the\ncorrection process. Experiments were conducted using seven LMs on three\ndatasets: the 19th Century Serials Edition (NCSE) and two datasets from the\nOverproof collection. The results demonstrate that some LMs can significantly\nreduce error rates, with the top-performing model achieving over a 60\\%\nreduction in character error rate on the NCSE dataset. The OCR improvements\nextend to downstream tasks, such as Named Entity Recognition, with increased\nCosine Named Entity Similarity. Furthermore, the study shows that providing\nsocio-cultural context in the prompts improves performance, while misleading\nprompts lower performance. In addition to the findings, this study releases a\ndataset of 91 transcribed articles from the NCSE, containing a total of 40\nthousand words, to support further research in this area. The findings suggest\nthat CLOCR-C is a promising approach for enhancing the quality of existing\ndigital archives by leveraging the socio-cultural information embedded in the\nLMs and the text requiring correction.", "published": "2024-08-30 17:26:05", "link": "http://arxiv.org/abs/2408.17428v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Dynamic Depth Decoding: Faster Speculative Decoding for LLMs", "abstract": "The acceleration of Large Language Models (LLMs) with speculative decoding\nprovides a significant runtime improvement without any loss of accuracy.\nCurrently, EAGLE-2 is the state-of-the-art speculative decoding method,\nimproving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth\nDecoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic\ndepth. This extends the average speedup that EAGLE-2 achieves over EAGLE by\n$44\\%$, giving DDD an average speedup of $3.16$x.", "published": "2024-08-30 03:27:48", "link": "http://arxiv.org/abs/2409.00142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiMath: Bridging Visual and Mathematical Reasoning for Large Language\n  Models", "abstract": "The rapid development of large language models (LLMs) has spurred extensive\nresearch into their domain-specific capabilities, particularly mathematical\nreasoning. However, most open-source LLMs focus solely on mathematical\nreasoning, neglecting the integration with visual injection, despite the fact\nthat many mathematical tasks rely on visual inputs such as geometric diagrams,\ncharts, and function plots. To fill this gap, we introduce\n\\textbf{MultiMath-7B}, a multimodal large language model that bridges the gap\nbetween math and vision. \\textbf{MultiMath-7B} is trained through a four-stage\nprocess, focusing on vision-language alignment, visual and math\ninstruction-tuning, and process-supervised reinforcement learning. We also\nconstruct a novel, diverse and comprehensive multimodal mathematical dataset,\n\\textbf{MultiMath-300K}, which spans K-12 levels with image captions and\nstep-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance\namong open-source models on existing multimodal mathematical benchmarks and\nalso excels on text-only mathematical benchmarks. Our model and dataset are\navailable at\n{\\textcolor{blue}{\\url{https://github.com/pengshuai-rin/MultiMath}}}.", "published": "2024-08-30 07:37:38", "link": "http://arxiv.org/abs/2409.00147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language\n  Feedback", "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions\nand values remains a critical challenge. Reinforcement learning from human\nfeedback (RLHF) aligns LLMs by training a reward model (RM) on human\npreferences and fine-tuning the LLMs to maximize RM feedback. Despite its\neffectiveness and popularity, RLHF is prone to biased local optimization. It\nmeans RM fails to provide feedback that accurately aligns with human\npreference, causing LLMs to explore unexpected generalizations, and failing to\nachieve alignment objectives. To mitigate this issue, we propose a novel\n\\textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight\nis that learning from language feedback rather than scalar feedback improves\nRLHF without additional annotations. We replaced the reward modeling target\nfrom binary maximum likelihood estimation (MLE) with sequence MLE. This method\nenables richer and fine-grained language feedback without additional\nannotations, models, or training stages. Our experiments demonstrated its\neffectiveness, specifically, reducing the refusal-to-response paradigm in\nsingle-turn safety dialogues and the long-response bias in text summarization\ntasks. We provide further analysis that seq2seq RM improves RLHF performance\nacross 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\\%.\nWe further show that seq2seq RM can still improve the performance of RLHF under\nout-of-distribution prompts.", "published": "2024-08-30 16:14:35", "link": "http://arxiv.org/abs/2409.00162v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Facilitating phenotyping from clinical texts: the medkit library", "abstract": "Phenotyping consists in applying algorithms to identify individuals\nassociated with a specific, potentially complex, trait or condition, typically\nout of a collection of Electronic Health Records (EHRs). Because a lot of the\nclinical information of EHRs are lying in texts, phenotyping from text takes an\nimportant role in studies that rely on the secondary use of EHRs. However, the\nheterogeneity and highly specialized aspect of both the content and form of\nclinical texts makes this task particularly tedious, and is the source of time\nand cost constraints in observational studies. To facilitate the development,\nevaluation and reproductibility of phenotyping pipelines, we developed an\nopen-source Python library named medkit. It enables composing data processing\npipelines made of easy-to-reuse software bricks, named medkit operations. In\naddition to the core of the library, we share the operations and pipelines we\nalready developed and invite the phenotyping community for their reuse and\nenrichment. medkit is available at https://github.com/medkit-lib/medkit", "published": "2024-08-30 16:54:06", "link": "http://arxiv.org/abs/2409.00164v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enhancing Event Reasoning in Large Language Models through Instruction\n  Fine-Tuning with Semantic Causal Graphs", "abstract": "Event detection and text reasoning have become critical applications across\nvarious domains. While LLMs have recently demonstrated impressive progress in\nreasoning abilities, they often struggle with event detection, particularly due\nto the absence of training methods that consider causal relationships between\nevent triggers and types. To address this challenge, we propose a novel\napproach for instruction fine-tuning LLMs for event detection. Our method\nintroduces Semantic Causal Graphs (SCGs) to capture both causal relationships\nand contextual information within text. Building off of SCGs, we propose SCG\nInstructions for fine-tuning LLMs by focusing on event triggers and their\nrelationships to event types, and employ Low-Rank Adaptation (LoRA) to help\npreserve the general reasoning abilities of LLMs. Our evaluations demonstrate\nthat training LLMs with SCG Instructions outperforms standard instruction\nfine-tuning by an average of 35.69\\% on Event Trigger Classification. Notably,\nour fine-tuned Mistral 7B model also outperforms GPT-4 on key event detection\nmetrics by an average of 31.01\\% on Event Trigger Identification, 37.40\\% on\nEvent Trigger Classification, and 16.43\\% on Event Classification. We analyze\nthe retention of general capabilities, observing only a minimal average drop of\n2.03 points across six benchmarks. This comprehensive study investigates\nmultiple LLMs for the event detection task across various datasets, prompting\nstrategies, and training approaches.", "published": "2024-08-30 18:56:06", "link": "http://arxiv.org/abs/2409.00209v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding\n  Data", "abstract": "Multimodal language models can exhibit hallucinations in their outputs, which\nlimits their reliability. The ability to automatically detect these errors is\nimportant for mitigating them, but has been less explored and existing efforts\ndo not localize hallucinations, instead framing this as a classification task.\nIn this work, we first pose multimodal hallucination detection as a sequence\nlabeling task where models must localize hallucinated text spans and present a\nstrong baseline model. Given the high cost of human annotations for this task,\nwe propose an approach to improve the sample efficiency of these models by\ncreating corrupted grounding data, which we use for pre-training. Leveraging\nphrase grounding data, we generate hallucinations to replace grounded spans and\ncreate hallucinated text. Experiments show that pre-training on this data\nimproves sample efficiency when fine-tuning, and that the learning signal from\nthe grounding data plays an important role in these improvements.", "published": "2024-08-30 20:11:00", "link": "http://arxiv.org/abs/2409.00238v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA\n  Performance under Billion Parameters", "abstract": "This paper explores the potential of a small, domain-specific language model\ntrained exclusively on sports-related data. We investigate whether extensive\ntraining data with specially designed small model structures can overcome model\nsize constraints. The study introduces the OnlySports collection, comprising\nOnlySportsLM, OnlySports Dataset, and OnlySports Benchmark. Our approach\ninvolves: 1) creating a massive 600 billion tokens OnlySports Dataset from\nFineWeb, 2) optimizing the RWKV architecture for sports-related tasks,\nresulting in a 196M parameters model with 20-layer, 640-dimension structure, 3)\ntraining the OnlySportsLM on part of OnlySports Dataset, and 4) testing the\nresultant model on OnlySports Benchmark. OnlySportsLM achieves a 37.62%/34.08%\naccuracy improvement over previous 135M/360M state-of-the-art models and\nmatches the performance of larger models such as SomlLM 1.7B and Qwen 1.5B in\nthe sports domain. Additionally, the OnlySports collection presents a\ncomprehensive workflow for building high-quality, domain-specific language\nmodels, providing a replicable blueprint for efficient AI development across\nvarious specialized fields.", "published": "2024-08-30 22:39:35", "link": "http://arxiv.org/abs/2409.00286v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches", "abstract": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.", "published": "2024-08-30 01:56:57", "link": "http://arxiv.org/abs/2408.16966v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model", "abstract": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)", "published": "2024-08-30 10:24:07", "link": "http://arxiv.org/abs/2408.17175v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Neuron Ablation in Attention Heads: The Case for Peak\n  Activation Centering", "abstract": "The use of transformer-based models is growing rapidly throughout society.\nWith this growth, it is important to understand how they work, and in\nparticular, how the attention mechanisms represent concepts. Though there are\nmany interpretability methods, many look at models through their neuronal\nactivations, which are poorly understood. We describe different lenses through\nwhich to view neuron activations, and investigate the effectiveness in language\nmodels and vision transformers through various methods of neural ablation: zero\nablation, mean ablation, activation resampling, and a novel approach we term\n'peak ablation'. Through experimental analysis, we find that in different\nregimes and models, each method can offer the lowest degradation of model\nperformance compared to other methods, with resampling usually causing the most\nsignificant performance deterioration. We make our code available at\nhttps://github.com/nickypro/investigating-ablation.", "published": "2024-08-30 14:32:25", "link": "http://arxiv.org/abs/2408.17322v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T07 (Primary) 68T30, 68T50 (Secondary)", "I.2.4; I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Modularity in Transformers: Investigating Neuron Separability &\n  Specialization", "abstract": "Transformer models are increasingly prevalent in various applications, yet\nour understanding of their internal workings remains limited. This paper\ninvestigates the modularity and task specialization of neurons within\ntransformer architectures, focusing on both vision (ViT) and language (Mistral\n7B) models. Using a combination of selective pruning and MoEfication clustering\ntechniques, we analyze the overlap and specialization of neurons across\ndifferent tasks and data subsets. Our findings reveal evidence of task-specific\nneuron clusters, with varying degrees of overlap between related tasks. We\nobserve that neuron importance patterns persist to some extent even in randomly\ninitialized models, suggesting an inherent structure that training refines.\nAdditionally, we find that neuron clusters identified through MoEfication\ncorrespond more strongly to task-specific neurons in earlier and later layers\nof the models. This work contributes to a more nuanced understanding of\ntransformer internals and offers insights into potential avenues for improving\nmodel interpretability and efficiency.", "published": "2024-08-30 14:35:01", "link": "http://arxiv.org/abs/2408.17324v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07 (Primary) 68Q32, 68T05 (Secondary)", "I.2.4; I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Impact of ChatGPT on the writing style of condensed matter physicists", "abstract": "We apply a state-of-the-art difference-in-differences approach to estimate\nthe impact of ChatGPT's release on the writing style of condensed matter papers\non arXiv. Our analysis reveals a statistically significant improvement in the\nEnglish quality of abstracts written by non-native English speakers.\nImportantly, this improvement remains robust even after accounting for other\npotential factors, confirming that it can be attributed to the release of\nChatGPT. This indicates widespread adoption of the tool. Following the release\nof ChatGPT, there is a significant increase in the use of unique words, while\nthe frequency of rare words decreases. Across language families, the changes in\nwriting style are significant for authors from the Latin and Ural-Altaic\ngroups, but not for those from the Germanic or other Indo-European groups.", "published": "2024-08-30 14:37:10", "link": "http://arxiv.org/abs/2408.17325v1", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.stat-mech"], "primary_category": "cs.CL"}
{"title": "HERMES: temporal-coHERent long-forM understanding with Episodes and\n  Semantics", "abstract": "Existing research often treats long-form videos as extended short videos,\nleading to several limitations: inadequate capture of long-range dependencies,\ninefficient processing of redundant information, and failure to extract\nhigh-level semantic concepts. To address these issues, we propose a novel\napproach that more accurately reflects human cognition. This paper introduces\nHERMES: temporal-coHERent long-forM understanding with Episodes and Semantics,\na model that simulates episodic memory accumulation to capture action sequences\nand reinforces them with semantic knowledge dispersed throughout the video. Our\nwork makes two key contributions: First, we develop an Episodic COmpressor\n(ECO) that efficiently aggregates crucial representations from micro to\nsemi-macro levels, overcoming the challenge of long-range dependencies. Second,\nwe propose a Semantics ReTRiever (SeTR) that enhances these aggregated\nrepresentations with semantic information by focusing on the broader context,\ndramatically reducing feature dimensionality while preserving relevant\nmacro-level information. This addresses the issues of redundancy and lack of\nhigh-level concept extraction. Extensive experiments demonstrate that HERMES\nachieves state-of-the-art performance across multiple long-video understanding\nbenchmarks in both zero-shot and fully-supervised settings.", "published": "2024-08-30 17:52:55", "link": "http://arxiv.org/abs/2408.17443v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Speaker Tagging Correction With Non-Autoregressive Language Models", "abstract": "Speech applications dealing with conversations require not only recognizing\nthe spoken words but also determining who spoke when. The task of assigning\nwords to speakers is typically addressed by merging the outputs of two separate\nsystems, namely, an automatic speech recognition (ASR) system and a speaker\ndiarization (SD) system. In practical settings, speaker diarization systems can\nexperience significant degradation in performance due to a variety of factors,\nincluding uniform segmentation with a high temporal resolution, inaccurate word\ntimestamps, incorrect clustering and estimation of speaker numbers, as well as\nbackground noise.\n  Therefore, it is important to automatically detect errors and make\ncorrections if possible. We used a second-pass speaker tagging correction\nsystem based on a non-autoregressive language model to correct mistakes in\nwords placed at the borders of sentences spoken by different speakers. We first\nshow that the employed error correction approach leads to reductions in word\ndiarization error rate (WDER) on two datasets: TAL and test set of Fisher.\nAdditionally, we evaluated our system in the Post-ASR Speaker Tagging\nCorrection challenge and observed significant improvements in cpWER compared to\nbaseline methods.", "published": "2024-08-30 11:02:17", "link": "http://arxiv.org/abs/2409.00151v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS", "68T01 General topics in artificial intelligence"], "primary_category": "cs.CL"}
{"title": "Developing an End-to-End Framework for Predicting the Social\n  Communication Severity Scores of Children with Autism Spectrum Disorder", "abstract": "Autism Spectrum Disorder (ASD) is a lifelong condition that significantly\ninfluencing an individual's communication abilities and their social\ninteractions. Early diagnosis and intervention are critical due to the profound\nimpact of ASD's characteristic behaviors on foundational developmental stages.\nHowever, limitations of standardized diagnostic tools necessitate the\ndevelopment of objective and precise diagnostic methodologies. This paper\nproposes an end-to-end framework for automatically predicting the social\ncommunication severity of children with ASD from raw speech data. This\nframework incorporates an automatic speech recognition model, fine-tuned with\nspeech data from children with ASD, followed by the application of fine-tuned\npre-trained language models to generate a final prediction score. Achieving a\nPearson Correlation Coefficient of 0.6566 with human-rated scores, the proposed\nmethod showcases its potential as an accessible and objective tool for the\nassessment of ASD.", "published": "2024-08-30 14:43:58", "link": "http://arxiv.org/abs/2409.00158v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities", "abstract": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities.", "published": "2024-08-30 15:04:11", "link": "http://arxiv.org/abs/2409.00159v3", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ProGRes: Prompted Generative Rescoring on ASR n-Best", "abstract": "Large Language Models (LLMs) have shown their ability to improve the\nperformance of speech recognizers by effectively rescoring the n-best\nhypotheses generated during the beam search process. However, the best way to\nexploit recent generative instruction-tuned LLMs for hypothesis rescoring is\nstill unclear. This paper proposes a novel method that uses instruction-tuned\nLLMs to dynamically expand the n-best speech recognition hypotheses with new\nhypotheses generated through appropriately-prompted LLMs. Specifically, we\nintroduce a new zero-shot method for ASR n-best rescoring, which combines\nconfidence scores, LLM sequence scoring, and prompt-based hypothesis\ngeneration. We compare Llama-3-Instruct, GPT-3.5 Turbo, and GPT-4 Turbo as\nprompt-based generators with Llama-3 as sequence scorer LLM. We evaluated our\napproach using different speech recognizers and observed significant relative\nimprovement in the word error rate (WER) ranging from 5% to 25%.", "published": "2024-08-30 19:14:17", "link": "http://arxiv.org/abs/2409.00217v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explainable Artificial Intelligence: A Survey of Needs, Techniques,\n  Applications, and Future Direction", "abstract": "Artificial intelligence models encounter significant challenges due to their\nblack-box nature, particularly in safety-critical domains such as healthcare,\nfinance, and autonomous vehicles. Explainable Artificial Intelligence (XAI)\naddresses these challenges by providing explanations for how these models make\ndecisions and predictions, ensuring transparency, accountability, and fairness.\nExisting studies have examined the fundamental concepts of XAI, its general\nprinciples, and the scope of XAI techniques. However, there remains a gap in\nthe literature as there are no comprehensive reviews that delve into the\ndetailed mathematical representations, design methodologies of XAI models, and\nother associated aspects. This paper provides a comprehensive literature review\nencompassing common terminologies and definitions, the need for XAI,\nbeneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI\nmethods in different application areas. The survey is aimed at XAI researchers,\nXAI practitioners, AI model developers, and XAI beneficiaries who are\ninterested in enhancing the trustworthiness, transparency, accountability, and\nfairness of their AI models.", "published": "2024-08-30 21:42:17", "link": "http://arxiv.org/abs/2409.00265v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Towards a dynamical model of English vowels. Evidence from\n  diphthongisation", "abstract": "Diphthong vowels exhibit a degree of inherent dynamic change, the extent of\nwhich can vary synchronically and diachronically, such that diphthong vowels\ncan become monophthongs and vice versa. Modelling this type of change requires\ndefining diphthongs in opposition to monophthongs. However, formulating an\nexplicit definition has proven elusive in acoustics and articulation, as\ndiphthongisation is often gradient in these domains. In this study, we consider\nwhether diphthong vowels form a coherent phonetic category from the\narticulatory point of view. We present articulometry and acoustic data from six\nspeakers of Northern Anglo-English producing a full set of phonologically long\nvowels. We analyse several measures of diphthongisation, all of which suggest\nthat diphthongs are not categorically distinct from long monophthongs. We\naccount for this observation with an Articulatory Phonology/Task Dynamic model\nin which diphthongs and long monophthongs have a common gestural\nrepresentation, comprising two articulatory targets in each case, but they\ndiffer according to gestural constriction and location of the component\ngestures. We argue that a two-target representation for all long vowels is\nindependently supported by phonological weight, as well as by the nature of\nhistorical diphthongisation and present-day dynamic vowel variation in British\nEnglish.", "published": "2024-08-30 22:10:19", "link": "http://arxiv.org/abs/2409.00275v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reframing Data Value for Large Language Models Through the Lens of\n  Plausibility", "abstract": "Data valuation seeks to answer the important question, \"How much is this data\nworth?\" Existing data valuation methods have largely focused on discriminative\nmodels, primarily examining data value through the lens of its utility in\ntraining. However, with the push for ever-larger language models, relying on\nvaluation methods that require training becomes increasingly expensive and\ndependent on specific techniques. We propose an alternative perspective on the\ndata value problem for language models, centering around the plausibility of\nthe data. We posit that data holds lesser value if it can be plausibly\ngenerated by the model itself. Starting from some intuitive criteria that align\nwith our notions of valuable data, we develop a novel value function that is\ncomputationally tractable and derived from first principles with provable\nproperties. We conduct a theoretical analysis of our value function and\nevaluate it across multiple scenarios and datasets.", "published": "2024-08-30 22:32:24", "link": "http://arxiv.org/abs/2409.00284v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "REFFLY: Melody-Constrained Lyrics Editing Model", "abstract": "Automatic melody-to-lyric generation aims to produce lyrics that align with a\ngiven melody. Although previous work can generate lyrics based on high-level\ncontrol signals, such as keywords or genre, they often struggle with three\nchallenges: (1) lack of controllability, as prior works are only able to\nproduce lyrics from scratch, with little or no control over the content; (2)\ninability to generate fully structured songs with the desired format; and (3)\nfailure to align prominent words in the lyrics with prominent notes in the\nmelody, resulting in poor lyrics-melody alignment. In this work, we introduce\nREFFLY (REvision Framework For Lyrics), the first revision framework designed\nto edit arbitrary forms of plain text draft into high-quality, full-fledged\nsong lyrics. Our approach ensures that the generated lyrics retain the original\nmeaning of the draft, align with the melody, and adhere to the desired song\nstructures. We demonstrate that REFFLY performs well in diverse task settings,\nsuch as lyrics revision and song translation. Experimental results show that\nour model outperforms strong baselines, such as Lyra (Tian et al. 2023) and\nGPT-4, by 25% in both musicality and text quality.", "published": "2024-08-30 23:22:34", "link": "http://arxiv.org/abs/2409.00292v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "\"Is This It?\": Towards Ecologically Valid Benchmarks for Situated\n  Collaboration", "abstract": "We report initial work towards constructing ecologically valid benchmarks to\nassess the capabilities of large multimodal models for engaging in situated\ncollaboration. In contrast to existing benchmarks, in which question-answer\npairs are generated post hoc over preexisting or synthetic datasets via\ntemplates, human annotators, or large language models (LLMs), we propose and\ninvestigate an interactive system-driven approach, where the questions are\ngenerated by users in context, during their interactions with an end-to-end\nsituated AI system. We illustrate how the questions that arise are different in\nform and content from questions typically found in existing embodied question\nanswering (EQA) benchmarks and discuss new real-world challenge problems\nbrought to the fore.", "published": "2024-08-30 12:41:23", "link": "http://arxiv.org/abs/2409.10525v1", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "MAPWise: Evaluating Vision-Language Models for Advanced Map Queries", "abstract": "Vision-language models (VLMs) excel at tasks requiring joint understanding of\nvisual and linguistic information. A particularly promising yet under-explored\napplication for these models lies in answering questions based on various kinds\nof maps. This study investigates the efficacy of VLMs in answering questions\nbased on choropleth maps, which are widely used for data analysis and\nrepresentation. To facilitate and encourage research in this area, we introduce\na novel map-based question-answering benchmark, consisting of maps from three\ngeographical regions (United States, India, China), each containing 1000\nquestions. Our benchmark incorporates 43 diverse question templates, requiring\nnuanced understanding of relative spatial relationships, intricate map\nfeatures, and complex reasoning. It also includes maps with discrete and\ncontinuous values, encompassing variations in color-mapping, category ordering,\nand stylistic patterns, enabling comprehensive analysis. We evaluate the\nperformance of multiple VLMs on this benchmark, highlighting gaps in their\nabilities and providing insights for improving such models.", "published": "2024-08-30 20:57:34", "link": "http://arxiv.org/abs/2409.00255v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Utilizing Speaker Profiles for Impersonation Audio Detection", "abstract": "Fake audio detection is an emerging active topic. A growing number of\nliteratures have aimed to detect fake utterance, which are mostly generated by\nText-to-speech (TTS) or voice conversion (VC). However, countermeasures against\nimpersonation remain an underexplored area. Impersonation is a fake type that\ninvolves an imitator replicating specific traits and speech style of a target\nspeaker. Unlike TTS and VC, which often leave digital traces or signal\nartifacts, impersonation involves live human beings producing entirely natural\nspeech, rendering the detection of impersonation audio a challenging task.\nThus, we propose a novel method that integrates speaker profiles into the\nprocess of impersonation audio detection. Speaker profiles are inherent\ncharacteristics that are challenging for impersonators to mimic accurately,\nsuch as speaker's age, job. We aim to leverage these features to extract\ndiscriminative information for detecting impersonation audio. Moreover, there\nis no large impersonated speech corpora available for quantitative study of\nimpersonation impacts. To address this gap, we further design the first\nlarge-scale, diverse-speaker Chinese impersonation dataset, named ImPersonation\nAudio Detection (IPAD), to advance the community's research on impersonation\naudio detection. We evaluate several existing fake audio detection methods on\nour proposed dataset IPAD, demonstrating its necessity and the challenges.\nAdditionally, our findings reveal that incorporating speaker profiles can\nsignificantly enhance the model's performance in detecting impersonation audio.", "published": "2024-08-30 04:42:01", "link": "http://arxiv.org/abs/2408.17009v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "User-Driven Voice Generation and Editing through Latent Space Navigation", "abstract": "This paper presents a user-driven approach for synthesizing specific target\nvoices based on user feedback rather than reference recordings, which is\nparticularly beneficial for speech-impaired individuals who want to recreate\ntheir lost voices but lack prior recordings. Our method leverages the neural\nanalysis and synthesis framework to construct a latent speaker embedding space.\nWithin this latent space, a human-in-the-loop search algorithm guides the voice\ngeneration process. Users participate in a series of straightforward\nlistening-and-comparison tasks, providing feedback that iteratively refines the\nsynthesized voice to match their desired target. Both computer simulations and\nreal-world user studies demonstrate that the proposed approach can effectively\napproximate target voices. Moreover, by analyzing the mel-spectrogram\ngenerator's Jacobians, we identify a set of meaningful voice editing directions\nwithin the latent space. These directions enable users to further fine-tune\nspecific attributes of the generated voice, including the pitch level, pitch\nrange, volume, vocal tension, nasality, and tone color.", "published": "2024-08-30 07:51:45", "link": "http://arxiv.org/abs/2408.17068v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recursive Attentive Pooling for Extracting Speaker Embeddings from\n  Multi-Speaker Recordings", "abstract": "This paper proposes a method for extracting speaker embedding for each\nspeaker from a variable-length recording containing multiple speakers. Speaker\nembeddings are crucial not only for speaker recognition but also for various\nmulti-speaker speech applications such as speaker diarization and\ntarget-speaker speech processing. Despite the challenges of obtaining a single\nspeaker's speech without pre-registration in multi-speaker scenarios, most\nstudies on speaker embedding extraction focus on extracting embeddings only\nfrom single-speaker recordings. Some methods have been proposed for extracting\nspeaker embeddings directly from multi-speaker recordings, but they typically\nrequire preparing a model for each possible number of speakers or involve\ncomplicated training procedures. The proposed method computes the embeddings of\nmultiple speakers by focusing on different parts of the frame-wise embeddings\nextracted from the input multi-speaker audio. This is achieved by recursively\ncomputing attention weights for pooling the frame-wise embeddings.\nAdditionally, we propose using the calculated attention weights to estimate the\nnumber of speakers in the recording, which allows the same model to be applied\nto various numbers of speakers. Experimental evaluations demonstrate the\neffectiveness of the proposed method in speaker verification and diarization\ntasks.", "published": "2024-08-30 09:34:21", "link": "http://arxiv.org/abs/2408.17142v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Multi-Target TDOA Features for Sound Event Localization and\n  Detection", "abstract": "Sound event localization and detection (SELD) systems using audio recordings\nfrom a microphone array rely on spatial cues for determining the location of\nsound events. As a consequence, the localization performance of such systems is\nto a large extent determined by the quality of the audio features that are used\nas inputs to the system. We propose a new feature, based on neural generalized\ncross-correlations with phase-transform (NGCC-PHAT), that learns audio\nrepresentations suitable for localization. Using permutation invariant training\nfor the time-difference of arrival (TDOA) estimation problem enables NGCC-PHAT\nto learn TDOA features for multiple overlapping sound events. These features\ncan be used as a drop-in replacement for GCC-PHAT inputs to a SELD-network. We\ntest our method on the STARSS23 dataset and demonstrate improved localization\nperformance compared to using standard GCC-PHAT or SALSA-Lite input features.", "published": "2024-08-30 10:09:12", "link": "http://arxiv.org/abs/2408.17166v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Advancing Multi-talker ASR Performance with Large Language Models", "abstract": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.", "published": "2024-08-30 17:29:25", "link": "http://arxiv.org/abs/2408.17431v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame\n  Selection", "abstract": "Synthesizing the voices of unseen speakers is a persisting challenge in\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\nmodeling speaker characteristics through speaker conditioning during training.\nModeling unseen speaker attributes through this approach has necessitated an\nincrease in model complexity, which makes it challenging to reproduce results\nand improve upon them. We design a simple alternative to this. We propose\nSelectTTS, a novel method to select the appropriate frames from the target\nspeaker and decode using frame-level self-supervised learning (SSL) features.\nWe show that this approach can effectively capture speaker characteristics for\nunseen speakers, and achieves comparable results to other multi-speaker TTS\nframeworks in both objective and subjective metrics. With SelectTTS, we show\nthat frame selection from the target speaker's speech is a direct way to\nachieve generalization in unseen speakers with low model complexity. We achieve\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\nwith over an 8x reduction in model parameters and a 270x reduction in training\ndata.", "published": "2024-08-30 17:34:46", "link": "http://arxiv.org/abs/2408.17432v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and\n  Effective Baseline", "abstract": "With the popularity of cellular phones, events are often recorded by multiple\ndevices from different locations and shared on social media. Several different\nrecordings could be found for many events. Such recordings are usually noisy,\nwhere noise for each device is local and unrelated to others. This case of\nmultiple microphones at unknown locations, capturing local, uncorrelated noise,\nwas rarely treated in the literature. In this work we propose a simple and\neffective crowdsourced audio enhancement method to remove local noises at each\ninput audio signal. Then, averaging all cleaned source signals gives an\nimproved audio of the event. We demonstrate the effectiveness of our method\nusing synthetic audio signals, together with real-world recordings. This simple\napproach can set a new baseline for crowdsourced audio enhancement for more\nsophisticated methods which we hope will be developed by the research\ncommunity.", "published": "2024-08-30 17:35:12", "link": "http://arxiv.org/abs/2408.17434v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Point Neuron Learning: A New Physics-Informed Neural Network\n  Architecture", "abstract": "Machine learning and neural networks have advanced numerous research domains,\nbut challenges such as large training data requirements and inconsistent model\nperformance hinder their application in certain scientific problems. To\novercome these challenges, researchers have investigated integrating physics\nprinciples into machine learning models, mainly through: (i) physics-guided\nloss functions, generally termed as physics-informed neural networks, and (ii)\nphysics-guided architectural design. While both approaches have demonstrated\nsuccess across multiple scientific disciplines, they have limitations including\nbeing trapped to a local minimum, poor interpretability, and restricted\ngeneralizability. This paper proposes a new physics-informed neural network\n(PINN) architecture that combines the strengths of both approaches by embedding\nthe fundamental solution of the wave equation into the network architecture,\nenabling the learned model to strictly satisfy the wave equation. The proposed\npoint neuron learning method can model an arbitrary sound field based on\nmicrophone observations without any dataset. Compared to other PINN methods,\nour approach directly processes complex numbers and offers better\ninterpretability and generalizability. We evaluate the versatility of the\nproposed architecture by a sound field reconstruction problem in a reverberant\nenvironment. Results indicate that the point neuron method outperforms two\ncompeting methods and can efficiently handle noisy environments with sparse\nmicrophone observations.", "published": "2024-08-30 02:07:13", "link": "http://arxiv.org/abs/2408.16969v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "AASIST3: KAN-Enhanced AASIST Speech Deepfake Detection using SSL\n  Features and Additional Regularization for the ASVspoof 2024 Challenge", "abstract": "Automatic Speaker Verification (ASV) systems, which identify speakers based\non their voice characteristics, have numerous applications, such as user\nauthentication in financial transactions, exclusive access control in smart\ndevices, and forensic fraud detection. However, the advancement of deep\nlearning algorithms has enabled the generation of synthetic audio through\nText-to-Speech (TTS) and Voice Conversion (VC) systems, exposing ASV systems to\npotential vulnerabilities. To counteract this, we propose a novel architecture\nnamed AASIST3. By enhancing the existing AASIST framework with\nKolmogorov-Arnold networks, additional layers, encoders, and pre-emphasis\ntechniques, AASIST3 achieves a more than twofold improvement in performance. It\ndemonstrates minDCF results of 0.5357 in the closed condition and 0.1414 in the\nopen condition, significantly enhancing the detection of synthetic voices and\nimproving ASV security.", "published": "2024-08-30 15:30:01", "link": "http://arxiv.org/abs/2408.17352v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hold Me Tight: Stable Encoder-Decoder Design for Speech Enhancement", "abstract": "Convolutional layers with 1-D filters are often used as frontend to encode\naudio signals. Unlike fixed time-frequency representations, they can adapt to\nthe local characteristics of input data. However, 1-D filters on raw audio are\nhard to train and often suffer from instabilities. In this paper, we address\nthese problems with hybrid solutions, i.e., combining theory-driven and\ndata-driven approaches. First, we preprocess the audio signals via a auditory\nfilterbank, guaranteeing good frequency localization for the learned encoder.\nSecond, we use results from frame theory to define an unsupervised learning\nobjective that encourages energy conservation and perfect reconstruction.\nThird, we adapt mixed compressed spectral norms as learning objectives to the\nencoder coefficients. Using these solutions in a low-complexity\nencoder-mask-decoder model significantly improves the perceptual evaluation of\nspeech quality (PESQ) in speech enhancement.", "published": "2024-08-30 15:49:31", "link": "http://arxiv.org/abs/2408.17358v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
