{"title": "How Useful is Continued Pre-Training for Generative Unsupervised Domain\n  Adaptation?", "abstract": "Recent breakthroughs in scale have enabled the emergence of powerful\ngenerative language models, and the ability to fine-tune these models on\nvarious tasks by casting them into prompts or instructions. In this landscape,\nthe problem of Unsupervised Domain Adaptation (UDA), or the problem of\nleveraging knowledge from a labeled source domain to an unlabeled target\ndomain, has been left behind, with recent UDA methods still addressing\ndiscriminative classification. In particular, two popular UDA approaches,\ninvolving Continued Pre-Training (CPT) and learning domain invariant\nrepresentations, have been under-explored in the generative setting, signaling\na gap. In this work, we evaluate the utility of CPT for generative UDA. We\nfirst perform an empirical evaluation to measure the trade-offs between CPT and\nstrong methods promoting domain invariance. We further evaluate how well the\nbenefits of CPT extend to different architectures, tuning methods and data\nregimes. We then motivate the use of CPT by studying to what degree it benefits\nclassification performance on the target domain. Finally, we attempt to\nunderstand the mechanism behind which CPT improves classification performance\non the unlabeled target domain. Our findings suggest that a implicitly learns\nthe downstream task while predicting masked words informative to that task. Our\nwork connects the body of UDA research with that of instruction tuning,\nenabling an initial step towards a wider applicability of modern language\nmodels.", "published": "2024-01-31 00:15:34", "link": "http://arxiv.org/abs/2401.17514v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs", "abstract": "It is well acknowledged that incorporating explicit knowledge graphs (KGs)\ncan benefit question answering. Existing approaches typically follow a\ngrounding-reasoning pipeline in which entity nodes are first grounded for the\nquery (question and candidate answers), and then a reasoning module reasons\nover the matched multi-hop subgraph for answer prediction. Although the\npipeline largely alleviates the issue of extracting essential information from\ngiant KGs, efficiency is still an open challenge when scaling up hops in\ngrounding the subgraphs. In this paper, we target at finding semantically\nrelated entity nodes in the subgraph to improve the efficiency of graph\nreasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune\nnoisy nodes, remarkably reducing the computation cost and memory usage while\nalso obtaining decent subgraph representation. In detail, the pruning module\nfirst scores concept nodes based on the dependency distance between matched\nspans and then prunes the nodes according to score ranks. To facilitate the\nevaluation of pruned subgraphs, we also propose a graph attention network (GAT)\nbased module to reason with the subgraph data. Experimental results on\nCommonsenseQA and OpenBookQA demonstrate the effectiveness of our method.", "published": "2024-01-31 01:37:33", "link": "http://arxiv.org/abs/2401.17536v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local and Global Contexts for Conversation", "abstract": "The context in conversation is the dialog history crucial for multi-turn\ndialogue. Learning from the relevant contexts in dialog history for grounded\nconversation is a challenging problem. Local context is the most neighbor and\nmore sensitive to the subsequent response, and global context is relevant to a\nwhole conversation far beyond neighboring utterances. Currently, pretrained\ntransformer models for conversation challenge capturing the correlation and\nconnection between local and global contexts. We introduce a local and global\nconversation model (LGCM) for general-purpose conversation in open domain. It\nis a local-global hierarchical transformer model that excels at accurately\ndiscerning and assimilating the relevant contexts necessary for generating\nresponses. It employs a local encoder to grasp the local context at the level\nof individual utterances and a global encoder to understand the broader context\nat the dialogue level. The seamless fusion of these locally and globally\ncontextualized encodings ensures a comprehensive comprehension of the\nconversation. Experiments on popular datasets show that LGCM outperforms the\nexisting conversation models on the performance of automatic metrics with\nsignificant margins.", "published": "2024-01-31 04:19:22", "link": "http://arxiv.org/abs/2401.17588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization", "abstract": "Multi-turn dialogues are characterized by their extended length and the\npresence of turn-taking conversations. Traditional language models often\noverlook the distinct features of these dialogues by treating them as regular\ntext. In this paper, we propose a speaker-enhanced pre-training method for long\ndialogue summarization, which leverages the inherent structure of multiple-turn\ndialogues. To support our study, we curate a diverse dataset that includes\ntranscripts from real-world scenarios, movie or TV show transcripts, and\ndialogues generated by a Large Language Model. We then perform a pre-training,\nwhich encompasses the detection of speaker changes, and masked utterance\ngeneration. Experimental results of fine-tuned models demonstrate that our\nmodel achieves state-of-the-art performance on downstream benchmarks with long\ncontext, surpassing baseline models and highlighting the effectiveness of our\napproach. Our findings highlight the importance of curating pre-training\ndatasets that exhibit diversity and variations in length distribution to ensure\neffective alignment with downstream datasets.", "published": "2024-01-31 04:50:00", "link": "http://arxiv.org/abs/2401.17597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assertion Detection Large Language Model In-context Learning LoRA\n  Fine-tuning", "abstract": "In this study, we aim to address the task of assertion detection when\nextracting medical concepts from clinical notes, a key process in clinical\nnatural language processing (NLP). Assertion detection in clinical NLP usually\ninvolves identifying assertion types for medical concepts in the clinical text,\nnamely certainty (whether the medical concept is positive, negated, possible,\nor hypothetical), temporality (whether the medical concept is for present or\nthe past history), and experiencer (whether the medical concept is described\nfor the patient or a family member). These assertion types are essential for\nhealthcare professionals to quickly and clearly understand the context of\nmedical conditions from unstructured clinical texts, directly influencing the\nquality and outcomes of patient care. Although widely used, traditional\nmethods, particularly rule-based NLP systems and machine learning or deep\nlearning models, demand intensive manual efforts to create patterns and tend to\noverlook less common assertion types, leading to an incomplete understanding of\nthe context. To address this challenge, our research introduces a novel\nmethodology that utilizes Large Language Models (LLMs) pre-trained on a vast\narray of medical data for assertion detection. We enhanced the current method\nwith advanced reasoning techniques, including Tree of Thought (ToT), Chain of\nThought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank\nAdaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010\nassertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11\nimprovements over the previous works. To further assess the generalizability of\nour approach, we extended our evaluation to a local dataset that focused on\nsleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31\nhigher than the previous method.", "published": "2024-01-31 05:11:00", "link": "http://arxiv.org/abs/2401.17602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighboring Perturbations of Knowledge Editing on Large Language Models", "abstract": "Despite their exceptional capabilities, large language models (LLMs) are\nprone to generating unintended text due to false or outdated knowledge. Given\nthe resource-intensive nature of retraining LLMs, there has been a notable\nincrease in the development of knowledge editing. However, current approaches\nand evaluations rarely explore the perturbation of editing on neighboring\nknowledge. This paper studies whether updating new knowledge to LLMs perturbs\nthe neighboring knowledge encapsulated within them. Specifically, we seek to\nfigure out whether appending a new answer into an answer list to a factual\nquestion leads to catastrophic forgetting of original correct answers in this\nlist, as well as unintentional inclusion of incorrect answers. A metric of\nadditivity is introduced and a benchmark dubbed as Perturbation Evaluation of\nAppending Knowledge (PEAK) is constructed to evaluate the degree of\nperturbation to neighboring knowledge when appending new knowledge. Besides, a\nplug-and-play framework termed Appending via Preservation and Prevention (APP)\nis proposed to mitigate the neighboring perturbation by maintaining the\nintegrity of the answer list. Experiments demonstrate the effectiveness of APP\ncoupling with four editing methods on four LLMs. The code and data are\navailable at https://github.com/mjy1111/PEAK.", "published": "2024-01-31 06:49:36", "link": "http://arxiv.org/abs/2401.17623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document Structure in Long Document Transformers", "abstract": "Long documents often exhibit structure with hierarchically organized elements\nof different functions, such as section headers and paragraphs. Despite the\nomnipresence of document structure, its role in natural language processing\n(NLP) remains opaque. Do long-document Transformer models acquire an internal\nrepresentation of document structure during pre-training? How can structural\ninformation be communicated to a model after pre-training, and how does it\ninfluence downstream performance? To answer these questions, we develop a novel\nsuite of probing tasks to assess structure-awareness of long-document\nTransformers, propose general-purpose structure infusion methods, and evaluate\nthe effects of structure infusion on QASPER and Evidence Inference, two\nchallenging long-document NLP tasks. Results on LED and LongT5 suggest that\nthey acquire implicit understanding of document structure during pre-training,\nwhich can be further enhanced by structure infusion, leading to improved\nend-task performance. To foster research on the role of document structure in\nNLP modeling, we make our data and code publicly available.", "published": "2024-01-31 08:28:06", "link": "http://arxiv.org/abs/2401.17658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought\n  Reasoning", "abstract": "Recent advancements have significantly augmented the reasoning capabilities\nof Large Language Models (LLMs) through various methodologies, especially\nchain-of-thought (CoT) reasoning. However, previous methods fail to address\nreasoning errors in intermediate steps, leading to accumulative errors. In this\npaper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT\nand deductive reasoning with step-wise beam search for LLMs. Our approach\ndeploys a verifier, verifying the deducibility of a reasoning step and its\npremises, thus alleviating the error accumulation. Furthermore, we introduce a\nscalable and labor-free data construction method to amplify our model's\nverification capabilities. Extensive experiments demonstrate that our approach\nsignificantly enhances the base performance of LLMs of various scales (7B, 13B,\n70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres,\nincluding arithmetic, commonsense, and symbolic. Moreover, our analysis proves\nDBS's capability of detecting diverse and subtle reasoning errors and\nrobustness on different model scales.", "published": "2024-01-31 09:16:35", "link": "http://arxiv.org/abs/2401.17686v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware\n  Decoding", "abstract": "The broad capabilities of Language Models (LMs) can be limited by their\nsensitivity to distractor tasks: LMs can infer secondary tasks from the prompt\nin addition to the intended one, leading to unwanted outputs. For example,\nprompt injection attacks can cause models to deviate from explicit directives.\nIn some 'inverse scaling' cases, this unwanted behaviour actually worsens as\nmodels scale up to at least 540B parameters. We present a theoretical framework\nthat interprets LMs as a product of experts that combine multiple data\ngeneration processes. Based on this framework, we demonstrate prior-aware\ndecoding (PAD) - a simple contrastive inference method to reduce the influence\nof distractor tasks. We apply PAD to eleven models, across four datasets, and\nfind improvements in 41 out of 44 task-model combinations, with a median\nincrease in task completion proportion of 40%. The results suggest a promising\ndirection for further development towards more reliable language models.", "published": "2024-01-31 09:28:06", "link": "http://arxiv.org/abs/2401.17692v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Model with Decomposed Reasoning for Emotion\n  Cause Pair Extraction", "abstract": "Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs\nrepresenting emotions and their causes in a document. Existing methods tend to\noverfit spurious correlations, such as positional bias in existing benchmark\ndatasets, rather than capturing semantic features. Inspired by recent work, we\nexplore leveraging large language model (LLM) to address ECPE task without\nadditional training. Despite strong capabilities, LLMs suffer from\nuncontrollable outputs, resulting in mediocre performance. To address this, we\nintroduce chain-of-thought to mimic human cognitive process and propose the\nDecomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference\nand logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance\nthe framework by incorporating in-context learning. Experiment results\ndemonstrate the strength of DECC compared to state-of-the-art supervised\nfine-tuning methods. Finally, we analyze the effectiveness of each component\nand the robustness of the method in various scenarios, including different LLM\nbases, rebalanced datasets, and multi-pair extraction.", "published": "2024-01-31 10:20:01", "link": "http://arxiv.org/abs/2401.17716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CauESC: A Causal Aware Model for Emotional Support Conversation", "abstract": "Emotional Support Conversation aims at reducing the seeker's emotional\ndistress through supportive response. Existing approaches have two limitations:\n(1) They ignore the emotion causes of the distress, which is important for\nfine-grained emotion understanding; (2) They focus on the seeker's own mental\nstate rather than the emotional dynamics during interaction between speakers.\nTo address these issues, we propose a novel framework CauESC, which firstly\nrecognizes the emotion causes of the distress, as well as the emotion effects\ntriggered by the causes, and then understands each strategy of verbal grooming\nindependently and integrates them skillfully. Experimental results on the\nbenchmark dataset demonstrate the effectiveness of our approach and show the\nbenefits of emotion understanding from cause to effect and\nindependent-integrated strategy modeling.", "published": "2024-01-31 11:30:24", "link": "http://arxiv.org/abs/2401.17755v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey of Pre-trained Language Models for Processing Scientific Text", "abstract": "The number of Language Models (LMs) dedicated to processing scientific text\nis on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs)\nhas become a daunting task for researchers. To date, no comprehensive surveys\non SciLMs have been undertaken, leaving this issue unaddressed. Given the\nconstant stream of new SciLMs, appraising the state-of-the-art and how they\ncompare to each other remain largely unknown. This work fills that gap and\nprovides a comprehensive review of SciLMs, including an extensive analysis of\ntheir effectiveness across different domains, tasks and datasets, and a\ndiscussion on the challenges that lie ahead.", "published": "2024-01-31 13:35:07", "link": "http://arxiv.org/abs/2401.17824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Language Models' Gesture Understanding for Enhanced Human-AI\n  Interaction", "abstract": "The rise of Large Language Models (LLMs) has affected various disciplines\nthat got beyond mere text generation. Going beyond their textual nature, this\nproject proposal aims to investigate the interaction between LLMs and\nnon-verbal communication, specifically focusing on gestures. The proposal sets\nout a plan to examine the proficiency of LLMs in deciphering both explicit and\nimplicit non-verbal cues within textual prompts and their ability to associate\nthese gestures with various contextual factors. The research proposes to test\nestablished psycholinguistic study designs to construct a comprehensive dataset\nthat pairs textual prompts with detailed gesture descriptions, encompassing\ndiverse regional variations, and semantic labels. To assess LLMs' comprehension\nof gestures, experiments are planned, evaluating their ability to simulate\nhuman behaviour in order to replicate psycholinguistic experiments. These\nexperiments consider cultural dimensions and measure the agreement between\nLLM-identified gestures and the dataset, shedding light on the models'\ncontextual interpretation of non-verbal cues (e.g. gestures).", "published": "2024-01-31 14:19:03", "link": "http://arxiv.org/abs/2401.17858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Think, Therefore I am: Benchmarking Awareness of Large Language Models\n  Using AwareBench", "abstract": "Do large language models (LLMs) exhibit any forms of awareness similar to\nhumans? In this paper, we introduce AwareBench, a benchmark designed to\nevaluate awareness in LLMs. Drawing from theories in psychology and philosophy,\nwe define awareness in LLMs as the ability to understand themselves as AI\nmodels and to exhibit social intelligence. Subsequently, we categorize\nawareness in LLMs into five dimensions, including capability, mission, emotion,\nculture, and perspective. Based on this taxonomy, we create a dataset called\nAwareEval, which contains binary, multiple-choice, and open-ended questions to\nassess LLMs' understandings of specific awareness dimensions. Our experiments,\nconducted on 13 LLMs, reveal that the majority of them struggle to fully\nrecognize their capabilities and missions while demonstrating decent social\nintelligence. We conclude by connecting awareness of LLMs with AI alignment and\nsafety, emphasizing its significance to the trustworthy and ethical development\nof LLMs. Our dataset and code are available at\nhttps://github.com/HowieHwong/Awareness-in-LLM.", "published": "2024-01-31 14:41:23", "link": "http://arxiv.org/abs/2401.17882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Employing Label Models on ChatGPT Answers Improves Legal Text Entailment\n  Performance", "abstract": "The objective of legal text entailment is to ascertain whether the assertions\nin a legal query logically follow from the information provided in one or\nmultiple legal articles. ChatGPT, a large language model, is robust in many\nnatural language processing tasks, including legal text entailment: when we set\nthe temperature = 0 (the ChatGPT answers are deterministic) and prompt the\nmodel, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms\nthe previous SOTA of 67.89%. On the other hand, if the temperature is larger\nthan zero, ChatGPT answers are not deterministic, leading to inconsistent\nanswers and fluctuating results. We propose to leverage label models (a\nfundamental component of weak supervision techniques) to integrate the\nprovisional answers by ChatGPT into consolidated labels. By that way, we treat\nChatGPT provisional answers as noisy predictions which can be consolidated by\nlabel models. The experimental results demonstrate that this approach can\nattain an accuracy of 76.15%, marking a significant improvement of 8.26% over\nthe prior state-of-the-art benchmark. Additionally, we perform an analysis of\nthe instances where ChatGPT produces incorrect answers, then we classify the\nerrors, offering insights that could guide potential enhancements for future\nresearch endeavors.", "published": "2024-01-31 15:04:01", "link": "http://arxiv.org/abs/2401.17897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural\n  Networks", "abstract": "As spiking neural networks receive more attention, we look toward\napplications of this computing paradigm in fields other than computer vision\nand signal processing. One major field, underexplored in the neuromorphic\nsetting, is Natural Language Processing (NLP), where most state-of-the-art\nsolutions still heavily rely on resource-consuming and power-hungry traditional\ndeep learning architectures. Therefore, it is compelling to design NLP models\nfor neuromorphic architectures due to their low energy requirements, with the\nadditional benefit of a more human-brain-like operating model for processing\ninformation. However, one of the biggest issues with bringing NLP to the\nneuromorphic setting is in properly encoding text into a spike train so that it\ncan be seamlessly handled by both current and future SNN architectures. In this\npaper, we compare various methods of encoding text as spikes and assess each\nmethod's performance in an associated SNN on a downstream NLP task, namely,\nsentiment analysis. Furthermore, we go on to propose a new method of encoding\ntext as spikes that outperforms a widely-used rate-coding technique, Poisson\nrate-coding, by around 13\\% on our benchmark NLP tasks. Subsequently, we\ndemonstrate the energy efficiency of SNNs implemented in hardware for the\nsentiment analysis task compared to traditional deep neural networks, observing\nan energy efficiency increase of more than 32x during inference and 60x during\ntraining while incurring the expected energy-performance tradeoff.", "published": "2024-01-31 15:16:25", "link": "http://arxiv.org/abs/2401.17911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference\n  Annotation with LLMs", "abstract": "Coreference annotation and resolution is a vital component of computational\nliterary studies. However, it has previously been difficult to build high\nquality systems for fiction. Coreference requires complicated structured\noutputs, and literary text involves subtle inferences and highly varied\nlanguage. New language-model-based seq2seq systems present the opportunity to\nsolve both these problems by learning to directly generate a copy of an input\nsentence with markdown-like annotations. We create, evaluate, and release\nseveral trained models for coreference, as well as a workflow for training new\nmodels.", "published": "2024-01-31 15:35:21", "link": "http://arxiv.org/abs/2401.17922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GUMsley: Evaluating Entity Salience in Summarization for 12 English\n  Genres", "abstract": "As NLP models become increasingly capable of understanding documents in terms\nof coherent entities rather than strings, obtaining the most salient entities\nfor each document is not only an important end task in itself but also vital\nfor Information Retrieval (IR) and other downstream applications such as\ncontrollable summarization. In this paper, we present and evaluate GUMsley, the\nfirst entity salience dataset covering all named and non-named salient entities\nfor 12 genres of English text, aligned with entity types, Wikification links\nand full coreference resolution annotations. We promote a strict definition of\nsalience using human summaries and demonstrate high inter-annotator agreement\nfor salience based on whether a source entity is mentioned in the summary. Our\nevaluation shows poor performance by pre-trained SOTA summarization models and\nzero-shot LLM prompting in capturing salient entities in generated summaries.\nWe also show that predicting or providing salient entities to several model\narchitectures enhances performance and helps derive higher-quality summaries by\nalleviating the entity hallucination problem in existing abstractive\nsummarization.", "published": "2024-01-31 16:30:50", "link": "http://arxiv.org/abs/2401.17974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Linking in the Job Market Domain", "abstract": "In Natural Language Processing, entity linking (EL) has centered around\nWikipedia, but yet remains underexplored for the job market domain.\nDisambiguating skill mentions can help us get insight into the current labor\nmarket demands. In this work, we are the first to explore EL in this domain,\nspecifically targeting the linkage of occupational skills to the ESCO taxonomy\n(le Vrang et al., 2014). Previous efforts linked coarse-grained (full)\nsentences to a corresponding ESCO skill. In this work, we link more\nfine-grained span-level mentions of skills. We tune two high-performing neural\nEL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et\nal., 2021), on a synthetically generated mention--skill pair dataset and\nevaluate them on a human-annotated skill-linking benchmark. Our findings reveal\nthat both models are capable of linking implicit mentions of skills to their\ncorrect taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict\nevaluation, but GENRE performs better in loose evaluation (accuracy@$k$).", "published": "2024-01-31 16:34:10", "link": "http://arxiv.org/abs/2401.17979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Desiderata for the Context Use of Question Answering Systems", "abstract": "Prior work has uncovered a set of common problems in state-of-the-art\ncontext-based question answering (QA) systems: a lack of attention to the\ncontext when the latter conflicts with a model's parametric knowledge, little\nrobustness to noise, and a lack of consistency with their answers. However,\nmost prior work focus on one or two of those problems in isolation, which makes\nit difficult to see trends across them. We aim to close this gap, by first\noutlining a set of -- previously discussed as well as novel -- desiderata for\nQA models. We then survey relevant analysis and methods papers to provide an\noverview of the state of the field. The second part of our work presents\nexperiments where we evaluate 15 QA systems on 5 datasets according to all\ndesiderata at once. We find many novel trends, including (1) systems that are\nless susceptible to noise are not necessarily more consistent with their\nanswers when given irrelevant context; (2) most systems that are more\nsusceptible to noise are more likely to correctly answer according to a context\nthat conflicts with their parametric knowledge; and (3) the combination of\nconflicting knowledge and noise can reduce system performance by up to 96%. As\nsuch, our desiderata help increase our understanding of how these models work\nand reveal potential avenues for improvements.", "published": "2024-01-31 17:02:31", "link": "http://arxiv.org/abs/2401.18001v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multipath parsing in the brain", "abstract": "Humans understand sentences word-by-word, in the order that they hear them.\nThis incrementality entails resolving temporary ambiguities about syntactic\nrelationships. We investigate how humans process these syntactic ambiguities by\ncorrelating predictions from incremental generative dependency parsers with\ntimecourse data from people undergoing functional neuroimaging while listening\nto an audiobook. In particular, we compare competing hypotheses regarding the\nnumber of developing syntactic analyses in play during word-by-word\ncomprehension: one vs more than one. This comparison involves evaluating\nsyntactic surprisal from a state-of-the-art dependency parser with LLM-adapted\nencodings against an existing fMRI dataset. In both English and Chinese data,\nwe find evidence for multipath parsing. Brain regions associated with this\nmultipath effect include bilateral superior temporal gyrus.", "published": "2024-01-31 18:07:12", "link": "http://arxiv.org/abs/2401.18046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Language Adapters in Cross-Lingual Transfer for NLU", "abstract": "Modular deep learning has been proposed for the efficient adaption of\npre-trained models to new tasks, domains and languages. In particular,\ncombining language adapters with task adapters has shown potential where no\nsupervised data exists for a language. In this paper, we explore the role of\nlanguage adapters in zero-shot cross-lingual transfer for natural language\nunderstanding (NLU) benchmarks. We study the effect of including a\ntarget-language adapter in detailed ablation studies with two multilingual\nmodels and three multilingual datasets. Our results show that the effect of\ntarget-language adapters is highly inconsistent across tasks, languages and\nmodels. Retaining the source-language adapter instead often leads to an\nequivalent, and sometimes to a better, performance. Removing the language\nadapter after training has only a weak negative effect, indicating that the\nlanguage adapters do not have a strong impact on the predictions.", "published": "2024-01-31 20:07:43", "link": "http://arxiv.org/abs/2402.00149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Mathematical Reasoning: Progresses and\n  Challenges", "abstract": "Mathematical reasoning serves as a cornerstone for assessing the fundamental\ncognitive capabilities of human intelligence. In recent times, there has been a\nnotable surge in the development of Large Language Models (LLMs) geared towards\nthe automated resolution of mathematical problems. However, the landscape of\nmathematical problem types is vast and varied, with LLM-oriented techniques\nundergoing evaluation across diverse datasets and settings. This diversity\nmakes it challenging to discern the true advancements and obstacles within this\nburgeoning field. This survey endeavors to address four pivotal dimensions: i)\na comprehensive exploration of the various mathematical problems and their\ncorresponding datasets that have been investigated; ii) an examination of the\nspectrum of LLM-oriented techniques that have been proposed for mathematical\nproblem-solving; iii) an overview of factors and concerns affecting LLMs in\nsolving math; and iv) an elucidation of the persisting challenges within this\ndomain. To the best of our knowledge, this survey stands as one of the first\nextensive examinations of the landscape of LLMs in the realm of mathematics,\nproviding a holistic perspective on the current state, accomplishments, and\nfuture challenges in this rapidly evolving field.", "published": "2024-01-31 20:26:32", "link": "http://arxiv.org/abs/2402.00157v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model\n  Pretraining Research", "abstract": "Information about pretraining corpora used to train the current\nbest-performing language models is seldom discussed: commercial models rarely\ndetail their data, and even open models are often released without accompanying\ntraining data or recipes to reproduce them. As a result, it is challenging to\nconduct and advance scientific research on language modeling, such as\nunderstanding how training data impacts model capabilities and limitations. To\nfacilitate scientific research on language model pretraining, we curate and\nrelease Dolma, a three-trillion-token English corpus, built from a diverse\nmixture of web content, scientific papers, code, public-domain books, social\nmedia, and encyclopedic materials. We extensively document Dolma, including its\ndesign principles, details about its construction, and a summary of its\ncontents. We present analyses and experimental results on intermediate states\nof Dolma to share what we have learned about important data curation practices.\nFinally, we open-source our data curation toolkit to enable reproduction of our\nwork as well as support further research in large-scale data curation.", "published": "2024-01-31 20:29:50", "link": "http://arxiv.org/abs/2402.00159v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergency Department Decision Support using Clinical Pseudo-notes", "abstract": "In this work, we introduce the Multiple Embedding Model for EHR (MEME), an\napproach that serializes multimodal EHR tabular data into text using\npseudo-notes, mimicking clinical text generation. This conversion not only\npreserves better representations of categorical data and learns contexts but\nalso enables the effective employment of pretrained foundation models for rich\nfeature representation. To address potential issues with context length, our\nframework encodes embeddings for each EHR modality separately. We demonstrate\nthe effectiveness of MEME by applying it to several decision support tasks\nwithin the Emergency Department across multiple hospital systems. Our findings\nindicate that MEME outperforms traditional machine learning, EHR-specific\nfoundation models, and general LLMs, highlighting its potential as a general\nand extendible EHR representation strategy.", "published": "2024-01-31 20:31:56", "link": "http://arxiv.org/abs/2402.00160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-identification is not enough: a comparison between de-identified and\n  synthetic clinical notes", "abstract": "For sharing privacy-sensitive data, de-identification is commonly regarded as\nadequate for safeguarding privacy. Synthetic data is also being considered as a\nprivacy-preserving alternative. Recent successes with numerical and tabular\ndata generative models and the breakthroughs in large generative language\nmodels raise the question of whether synthetically generated clinical notes\ncould be a viable alternative to real notes for research purposes. In this\nwork, we demonstrated that (i) de-identification of real clinical notes does\nnot protect records against a membership inference attack, (ii) proposed a\nnovel approach to generate synthetic clinical notes using the current\nstate-of-the-art large language models, (iii) evaluated the performance of the\nsynthetically generated notes in a clinical domain task, and (iv) proposed a\nway to mount a membership inference attack where the target model is trained\nwith synthetic data. We observed that when synthetically generated notes\nclosely match the performance of real data, they also exhibit similar privacy\nconcerns to the real data. Whether other approaches to synthetically generated\nclinical notes could offer better trade-offs and become a better alternative to\nsensitive real notes warrants further investigation.", "published": "2024-01-31 21:14:01", "link": "http://arxiv.org/abs/2402.00179v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Communicating Uncertainty in Patient-Facing Risk\n  Prediction Models", "abstract": "This paper addresses the unique challenges associated with uncertainty\nquantification in AI models when applied to patient-facing contexts within\nhealthcare. Unlike traditional eXplainable Artificial Intelligence (XAI)\nmethods tailored for model developers or domain experts, additional\nconsiderations of communicating in natural language, its presentation and\nevaluating understandability are necessary. We identify the challenges in\ncommunication model performance, confidence, reasoning and unknown knowns using\nnatural language in the context of risk prediction. We propose a design aimed\nat addressing these challenges, focusing on the specific application of\nin-vitro fertilisation outcome prediction.", "published": "2024-01-31 00:08:44", "link": "http://arxiv.org/abs/2401.17511v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models", "abstract": "The rapid evolution of Large Language Models (LLMs), epitomized by\narchitectures like GPT-4, has reshaped the landscape of natural language\nprocessing. This paper introduces a pioneering approach to address the\nefficiency concerns associated with LLM pre-training, proposing the use of\nknowledge distillation for cross-architecture transfer. Leveraging insights\nfrom the efficient Hyena mechanism, our method replaces attention heads in\ntransformer models by Hyena, offering a cost-effective alternative to\ntraditional pre-training while confronting the challenge of processing long\ncontextual information, inherent in quadratic attention mechanisms. Unlike\nconventional compression-focused methods, our technique not only enhances\ninference speed but also surpasses pre-training in terms of both accuracy and\nefficiency. In the era of evolving LLMs, our work contributes to the pursuit of\nsustainable AI solutions, striking a balance between computational power and\nenvironmental impact.", "published": "2024-01-31 03:39:07", "link": "http://arxiv.org/abs/2401.17574v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigating the OverKill in Large Language Models", "abstract": "Large language models are meticulously aligned to be both helpful and\nharmless. However, recent research points to a potential overkill which means\nmodels may refuse to answer benign queries. In this paper, we investigate the\nfactors for overkill by exploring how models handle and determine the safety of\nqueries. Our findings reveal the presence of shortcuts within models, leading\nto an over-attention of harmful words like 'kill' and prompts emphasizing\nsafety will exacerbate overkill. Based on these insights, we introduce\nSelf-Contrastive Decoding (Self-CD), a training-free and model-agnostic\nstrategy, to alleviate this phenomenon. We first extract such over-attention by\namplifying the difference in the model's output distributions when responding\nto system prompts that either include or omit an emphasis on safety. Then we\ndetermine the final next-token predictions by downplaying the over-attention\nfrom the model via contrastive decoding. Empirical results indicate that our\nmethod has achieved an average reduction of the refusal rate by 20\\% while\nhaving almost no impact on safety.", "published": "2024-01-31 07:26:47", "link": "http://arxiv.org/abs/2401.17633v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts", "abstract": "The Winograd Schema Challenge (WSC) serves as a prominent benchmark for\nevaluating machine understanding. While Large Language Models (LLMs) excel at\nanswering WSC questions, their ability to generate such questions remains less\nexplored. In this work, we propose Tree-of-Experts (ToE), a novel prompting\nmethod which enhances the generation of WSC instances (50% valid cases vs. 10%\nin recent methods). Using this approach, we introduce WSC+, a novel dataset\ncomprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework\nby incorporating new 'ambiguous' and 'offensive' categories, providing a deeper\ninsight into model overconfidence and bias. Our analysis reveals nuances in\ngeneration-evaluation consistency, suggesting that LLMs may not always\noutperform in evaluating their own generated questions when compared to those\ncrafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an\naccuracy of 68.7%, significantly below the human benchmark of 95.1%.", "published": "2024-01-31 09:49:22", "link": "http://arxiv.org/abs/2401.17703v1", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.1"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Malayalam Paraphrase Generation", "abstract": "This study explores four methods of generating paraphrases in Malayalam,\nutilizing resources available for English paraphrasing and pre-trained Neural\nMachine Translation (NMT) models. We evaluate the resulting paraphrases using\nboth automated metrics, such as BLEU, METEOR, and cosine similarity, as well as\nhuman annotation. Our findings suggest that automated evaluation measures may\nnot be fully appropriate for Malayalam, as they do not consistently align with\nhuman judgment. This discrepancy underscores the need for more nuanced\nparaphrase evaluation approaches especially for highly agglutinative languages.", "published": "2024-01-31 13:40:00", "link": "http://arxiv.org/abs/2401.17827v1", "categories": ["cs.CL", "cs.AI", "I.7.0; I.2.7"], "primary_category": "cs.CL"}
{"title": "LOCOST: State-Space Models for Long Document Abstractive Summarization", "abstract": "State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.", "published": "2024-01-31 15:33:37", "link": "http://arxiv.org/abs/2401.17919v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Paramanu: A Family of Novel Efficient Generative Foundation Language\n  Models for Indian Languages", "abstract": "We present \"Paramanu\", a family of novel language models (LM) for Indian\nlanguages, consisting of auto-regressive monolingual, bilingual, and\nmultilingual models pretrained from scratch. Currently, it covers 10 languages\n(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,\nTelugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The models\nare pretrained on a single GPU with context size of 1024 and vary in size from\n13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scaling\nmethod that enables us to pretrain language models from scratch at larger\nsequence length context size than typical GPU memory permits. We also\nintroduced a novel efficient Indic tokenizer, \"mBharat\", using a combination of\nBPE and Unigram, achieving the least fertility score and the ability to\ntokenize unseen languages in both the same script & Roman script. We also\nproposed and performed language-specific tokenization for multilingual models &\ndomain-specific tokenization for monolingual models. To address the \"curse of\nmultilinguality\" in our mParamanu model, we pretrained on comparable corpora\nbased on typological grouping within the same script. Our findings show a\nlanguage transfer phenomenon from low-resource to high-resource languages\nwithin languages of the same script & typology. Human evaluations for\nopen-ended text generation demonstrated that Paramanu models outperformed\nseveral LLMs, despite being 20 to 64 times smaller. We created\ninstruction-tuning datasets & instruction-tuned our models on 23,000\ninstructions in respective languages. Comparisons with multilingual LLMs across\nvarious benchmarks for natural language (NL) understanding, NL inference, &\nreading comprehension highlight the advantages of our models; leads to the\nconclusion that high quality generative LM are possible without high amount of\ncompute power & enormous number of parameters.", "published": "2024-01-31 17:58:10", "link": "http://arxiv.org/abs/2401.18034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic\n  Motivation Reinforcement Learning Algorithms for Improved Training and\n  Adaptability", "abstract": "End-to-end multi-task dialogue systems are usually designed with separate\nmodules for the dialogue pipeline. Among these, the policy module is essential\nfor deciding what to do in response to user input. This policy is trained by\nreinforcement learning algorithms by taking advantage of an environment in\nwhich an agent receives feedback in the form of a reward signal. The current\ndialogue systems, however, only provide meagre and simplistic rewards.\nInvestigating intrinsic motivation reinforcement learning algorithms is the\ngoal of this study. Through this, the agent can quickly accelerate training and\nimprove its capacity to judge the quality of its actions by teaching it an\ninternal incentive system. In particular, we adapt techniques for random\nnetwork distillation and curiosity-driven reinforcement learning to measure the\nfrequency of state visits and encourage exploration by using semantic\nsimilarity between utterances. Experimental results on MultiWOZ, a\nheterogeneous dataset, show that intrinsic motivation-based debate systems\noutperform policies that depend on extrinsic incentives. By adopting random\nnetwork distillation, for example, which is trained using semantic similarity\nbetween user-system dialogues, an astounding average success rate of 73% is\nachieved. This is a significant improvement over the baseline Proximal Policy\nOptimization (PPO), which has an average success rate of 60%. In addition,\nperformance indicators such as booking rates and completion rates show a 10%\nrise over the baseline. Furthermore, these intrinsic incentive models help\nimprove the system's policy's resilience in an increasing amount of domains.\nThis implies that they could be useful in scaling up to settings that cover a\nwider range of domains.", "published": "2024-01-31 18:03:39", "link": "http://arxiv.org/abs/2401.18040v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "abstract": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.", "published": "2024-01-31 18:29:39", "link": "http://arxiv.org/abs/2401.18058v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "abstract": "Retrieval-augmented language models can better adapt to changes in world\nstate and incorporate long-tail knowledge. However, most existing methods\nretrieve only short contiguous chunks from a retrieval corpus, limiting\nholistic understanding of the overall document context. We introduce the novel\napproach of recursively embedding, clustering, and summarizing chunks of text,\nconstructing a tree with differing levels of summarization from the bottom up.\nAt inference time, our RAPTOR model retrieves from this tree, integrating\ninformation across lengthy documents at different levels of abstraction.\nControlled experiments show that retrieval with recursive summaries offers\nsignificant improvements over traditional retrieval-augmented LMs on several\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\nwith the use of GPT-4, we can improve the best performance on the QuALITY\nbenchmark by 20% in absolute accuracy.", "published": "2024-01-31 18:30:21", "link": "http://arxiv.org/abs/2401.18059v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing Template-based and Template-free Language Model Probing", "abstract": "The differences between cloze-task language model (LM) probing with 1)\nexpert-made templates and 2) naturally-occurring text have often been\noverlooked. Here, we evaluate 16 different LMs on 10 probing English datasets\n-- 4 template-based and 6 template-free -- in general and biomedical domains to\nanswer the following research questions: (RQ1) Do model rankings differ between\nthe two approaches? (RQ2) Do models' absolute scores differ between the two\napproaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and\ndomain-specific models? Our findings are: 1) Template-free and template-based\napproaches often rank models differently, except for the top domain-specific\nmodels. 2) Scores decrease by up to 42% Acc@1 when comparing parallel\ntemplate-free and template-based prompts. 3) Perplexity is negatively\ncorrelated with accuracy in the template-free approach, but,\ncounter-intuitively, they are positively correlated for template-based probing.\n4) Models tend to predict the same answers frequently across prompts for\ntemplate-based probing, which is less common when employing template-free\ntechniques.", "published": "2024-01-31 19:07:37", "link": "http://arxiv.org/abs/2402.00123v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Common Sense Reasoning for Deepfake Detection", "abstract": "State-of-the-art deepfake detection approaches rely on image-based features\nextracted via neural networks. While these approaches trained in a supervised\nmanner extract likely fake features, they may fall short in representing\nunnatural `non-physical' semantic facial attributes -- blurry hairlines, double\neyebrows, rigid eye pupils, or unnatural skin shading. However, such facial\nattributes are easily perceived by humans and used to discern the authenticity\nof an image based on human common sense. Furthermore, image-based feature\nextraction methods that provide visual explanations via saliency maps can be\nhard to interpret for humans. To address these challenges, we frame deepfake\ndetection as a Deepfake Detection VQA (DD-VQA) task and model human intuition\nby providing textual explanations that describe common sense reasons for\nlabeling an image as real or fake. We introduce a new annotated dataset and\npropose a Vision and Language Transformer-based framework for the DD-VQA task.\nWe also incorporate text and image-aware feature alignment formulation to\nenhance multi-modal representation learning. As a result, we improve upon\nexisting deepfake detection models by integrating our learned vision\nrepresentations, which reason over common sense knowledge from the DD-VQA task.\nWe provide extensive empirical results demonstrating that our method enhances\ndetection performance, generalization ability, and language-based\ninterpretability in the deepfake detection task.", "published": "2024-01-31 19:11:58", "link": "http://arxiv.org/abs/2402.00126v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Making a Long Story Short in Conversation Modeling", "abstract": "Conversation systems accommodate diverse users with unique personalities and\ndistinct writing styles. Within the domain of multi-turn dialogue modeling,\nthis work studies the impact of varied utterance lengths on the quality of\nsubsequent responses generated by conversation models. Using GPT-3 as the base\nmodel, multiple dialogue datasets, and several metrics, we conduct a thorough\nexploration of this aspect of conversational models. Our analysis sheds light\non the complex relationship between utterance lengths and the quality of\nfollow-up responses generated by dialogue systems. Empirical findings suggests\nthat, for certain types of conversations, utterance lengths can be reduced by\nup to 72% without any noticeable difference in the quality of follow-up\nresponses.", "published": "2024-01-31 19:48:58", "link": "http://arxiv.org/abs/2402.00143v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LLMs Simulate Big Five Personality Traits: Further Evidence", "abstract": "An empirical investigation into the simulation of the Big Five personality\ntraits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is\npresented. We analyze the personality traits simulated by these models and\ntheir stability. This contributes to the broader understanding of the\ncapabilities of LLMs to simulate personality traits and the respective\nimplications for personalized human-computer interaction.", "published": "2024-01-31 13:45:25", "link": "http://arxiv.org/abs/2402.01765v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.4; I.2.1"], "primary_category": "cs.CL"}
{"title": "Large Scale Generative AI Text Applied to Sports and Music", "abstract": "We address the problem of scaling up the production of media content,\nincluding commentary and personalized news stories, for large-scale sports and\nmusic events worldwide. Our approach relies on generative AI models to\ntransform a large volume of multimodal data (e.g., videos, articles, real-time\nscoring feeds, statistics, and fact sheets) into coherent and fluent text.\nBased on this approach, we introduce, for the first time, an AI commentary\nsystem, which was deployed to produce automated narrations for highlight\npackages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same\nvein, our solution was extended to create personalized content for ESPN Fantasy\nFootball and stories about music artists for the Grammy awards. These\napplications were built using a common software architecture achieved a 15x\nspeed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our\nwork was successfully deployed at the aforementioned events, supporting 90\nmillion fans around the world with 8 billion page views, continuously pushing\nthe bounds on what is possible at the intersection of sports, entertainment,\nand AI.", "published": "2024-01-31 22:47:01", "link": "http://arxiv.org/abs/2402.15514v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ontologia para monitorar a defici\u00eancia mental em seus d\u00e9ficts no\n  processamento da informa\u00e7\u00e3o por decl\u00ednio cognitivo e evitar\n  agress\u00f5es psicol\u00f3gicas e f\u00edsicas em ambientes educacionais com ajuda da\n  I.A*", "abstract": "The intention of this article is to propose the use of artificial\nintelligence to detect through analysis by UFO ontology the emergence of verbal\nand physical aggression related to psychosocial deficiencies and their\nprovoking agents, in an attempt to prevent catastrophic consequences within\nschool environments.", "published": "2024-01-31 16:15:41", "link": "http://arxiv.org/abs/2403.08795v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Propagation and Pitfalls: Reasoning-based Assessment of Knowledge\n  Editing through Counterfactual Tasks", "abstract": "Current approaches of knowledge editing struggle to effectively propagate\nupdates to interconnected facts. In this work, we delve into the barriers that\nhinder the appropriate propagation of updated knowledge within these models for\naccurate reasoning. To support our analysis, we introduce a novel\nreasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing\ndataset) -- which covers six common reasoning schemes in real world. We conduct\na thorough analysis of existing knowledge editing techniques, including input\naugmentation, finetuning, and locate-and-edit. We found that all model editing\nmethods show notably low performance on this dataset, especially in certain\nreasoning schemes. Our analysis over the chain-of-thought generation of edited\nmodels further uncover key reasons behind the inadequacy of existing knowledge\nediting methods from a reasoning standpoint, involving aspects on fact-wise\nediting, fact recall ability, and coherence in generation. We will make our\nbenchmark publicly available.", "published": "2024-01-31 04:12:59", "link": "http://arxiv.org/abs/2401.17585v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Good at captioning, bad at counting: Benchmarking GPT-4V on Earth\n  observation data", "abstract": "Large Vision-Language Models (VLMs) have demonstrated impressive performance\non complex tasks involving visual input with natural language instructions.\nHowever, it remains unclear to what extent capabilities on natural images\ntransfer to Earth observation (EO) data, which are predominantly satellite and\naerial images less common in VLM training data. In this work, we propose a\ncomprehensive benchmark to gauge the progress of VLMs toward being useful tools\nfor EO data by assessing their abilities on scene understanding, localization\nand counting, and change detection tasks. Motivated by real-world applications,\nour benchmark includes scenarios like urban monitoring, disaster relief, land\nuse, and conservation. We discover that, although state-of-the-art VLMs like\nGPT-4V possess extensive world knowledge that leads to strong performance on\nopen-ended tasks like location understanding and image captioning, their poor\nspatial reasoning limits usefulness on object localization and counting tasks.\nOur benchmark will be made publicly available at https://vleo.danielz.ch/ and\non Hugging Face at\nhttps://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70\nfor easy model evaluation.", "published": "2024-01-31 04:57:12", "link": "http://arxiv.org/abs/2401.17600v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "What Do Self-Supervised Speech and Speaker Models Learn? New Findings\n  From a Cross Model Layer-Wise Analysis", "abstract": "Self-supervised learning (SSL) has attracted increased attention for learning\nmeaningful speech representations. Speech SSL models, such as WavLM, employ\nmasked prediction training to encode general-purpose representations. In\ncontrast, speaker SSL models, exemplified by DINO-based models, adopt\nutterance-level training objectives primarily for speaker representation.\nUnderstanding how these models represent information is essential for refining\nmodel efficiency and effectiveness. Unlike the various analyses of speech SSL,\nthere has been limited investigation into what information speaker SSL captures\nand how its representation differs from speech SSL or other fully-supervised\nspeaker models. This paper addresses these fundamental questions. We explore\nthe capacity to capture various speech properties by applying SUPERB evaluation\nprobing tasks to speech and speaker SSL models. We also examine which layers\nare predominantly utilized for each task to identify differences in how speech\nis represented. Furthermore, we conduct direct comparisons to measure the\nsimilarities between layers within and across models. Our analysis unveils that\n1) the capacity to represent content information is somewhat unrelated to\nenhanced speaker representation, 2) specific layers of speech SSL models would\nbe partly specialized in capturing linguistic information, and 3) speaker SSL\nmodels tend to disregard linguistic information but exhibit more sophisticated\nspeaker representation.", "published": "2024-01-31 07:23:22", "link": "http://arxiv.org/abs/2401.17632v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Contextual Feature Extraction Hierarchies Converge in Large Language\n  Models and the Brain", "abstract": "Recent advancements in artificial intelligence have sparked interest in the\nparallels between large language models (LLMs) and human neural processing,\nparticularly in language comprehension. While prior research has established\nsimilarities in the representation of LLMs and the brain, the underlying\ncomputational principles that cause this convergence, especially in the context\nof evolving LLMs, remain elusive. Here, we examined a diverse selection of\nhigh-performance LLMs with similar parameter sizes to investigate the factors\ncontributing to their alignment with the brain's language processing\nmechanisms. We find that as LLMs achieve higher performance on benchmark tasks,\nthey not only become more brain-like as measured by higher performance when\npredicting neural responses from LLM embeddings, but also their hierarchical\nfeature extraction pathways map more closely onto the brain's while using fewer\nlayers to do the same encoding. We also compare the feature extraction pathways\nof the LLMs to each other and identify new ways in which high-performing models\nhave converged toward similar hierarchical processing mechanisms. Finally, we\nshow the importance of contextual information in improving model performance\nand brain similarity. Our findings reveal the converging aspects of language\nprocessing in the brain and LLMs and offer new directions for developing models\nthat align more closely with human cognitive processing.", "published": "2024-01-31 08:48:35", "link": "http://arxiv.org/abs/2401.17671v1", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "SWEA: Updating Factual Knowledge in Large Language Models via Subject\n  Word Embedding Altering", "abstract": "The general capabilities of large language models (LLMs) make them the\ninfrastructure for various AI applications, but updating their inner knowledge\nrequires significant resources. Recent model editing is a promising technique\nfor efficiently updating a small amount of knowledge of LLMs and has attracted\nmuch attention. In particular, local editing methods, which directly update\nmodel parameters, are proven suitable for updating small amounts of knowledge.\nLocal editing methods update weights by computing least squares closed-form\nsolutions and identify edited knowledge by vector-level matching in inference,\nwhich achieve promising results. However, these methods still require a lot of\ntime and resources to complete the computation. Moreover, vector-level matching\nlacks reliability, and such updates disrupt the original organization of the\nmodel's parameters. To address these issues, we propose a detachable and\nexpandable Subject Word Embedding Altering (SWEA) framework, which finds the\nediting embeddings through token-level matching and adds them to the subject\nword embeddings in Transformer input. To get these editing embeddings, we\npropose optimizing then suppressing fusion method, which first optimizes\nlearnable embedding vectors for the editing target and then suppresses the\nKnowledge Embedding Dimensions (KEDs) to obtain final editing embeddings. We\nthus propose SWEA$\\oplus$OS method for editing factual knowledge in LLMs. We\ndemonstrate the overall state-of-the-art (SOTA) performance of SWEA$\\oplus$OS\non the CounterFact and zsRE datasets. To further validate the reasoning ability\nof SWEA$\\oplus$OS in editing knowledge, we evaluate it on the more complex\nRippleEdits benchmark. The results demonstrate that SWEA$\\oplus$OS possesses\nSOTA reasoning ability.", "published": "2024-01-31 13:08:45", "link": "http://arxiv.org/abs/2401.17809v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global-Liar: Factuality of LLMs over Time and Geographic Regions", "abstract": "The increasing reliance on AI-driven solutions, particularly Large Language\nModels (LLMs) like the GPT series, for information retrieval highlights the\ncritical need for their factuality and fairness, especially amidst the rampant\nspread of misinformation and disinformation online. Our study evaluates the\nfactual accuracy, stability, and biases in widely adopted GPT models, including\nGPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated\ninformation dissemination.\n  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of\ngeographic and temporal representation, facilitating a more nuanced evaluation\nof LLM biases. Our analysis reveals that newer iterations of GPT models do not\nalways equate to improved performance. Notably, the GPT-4 version from March\ndemonstrates higher factual accuracy than its subsequent June release.\nFurthermore, a concerning bias is observed, privileging statements from the\nGlobal North over the Global South, thus potentially exacerbating existing\ninformational inequities. Regions such as Africa and the Middle East are at a\ndisadvantage, with much lower factual accuracy. The performance fluctuations\nover time suggest that model updates may not consistently benefit all regions\nequally.\n  Our study also offers insights into the impact of various LLM configuration\nsettings, such as binary decision forcing, model re-runs and temperature, on\nmodel's factuality. Models constrained to binary (true/false) choices exhibit\nreduced factuality compared to those allowing an 'unclear' option. Single\ninference at a low temperature setting matches the reliability of majority\nvoting across various configurations. The insights gained highlight the need\nfor culturally diverse and geographically inclusive model training and\nevaluation. This approach is key to achieving global equity in technology,\ndistributing AI benefits fairly worldwide.", "published": "2024-01-31 13:57:24", "link": "http://arxiv.org/abs/2401.17839v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Revisiting speech segmentation and lexicon learning with better features", "abstract": "We revisit a self-supervised method that segments unlabelled speech into\nword-like segments. We start from the two-stage duration-penalised dynamic\nprogramming method that performs zero-resource segmentation without learning an\nexplicit lexicon. In the first acoustic unit discovery stage, we replace\ncontrastive predictive coding features with HuBERT. After word segmentation in\nthe second stage, we get an acoustic word embedding for each segment by\naveraging HuBERT features. These embeddings are clustered using K-means to get\na lexicon. The result is good full-coverage segmentation with a lexicon that\nachieves state-of-the-art performance on the ZeroSpeech benchmarks.", "published": "2024-01-31 15:06:34", "link": "http://arxiv.org/abs/2401.17902v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Prompt-Driven Safeguarding for Large Language Models", "abstract": "Prepending model inputs with safety prompts is a common practice for\nsafeguarding large language models (LLMs) against queries with harmful intents.\nHowever, the underlying working mechanisms of safety prompts have not been\nunraveled yet, restricting the possibility of automatically optimizing them to\nimprove LLM safety. In this work, we investigate how LLMs' behavior (i.e.,\ncomplying with or refusing user queries) is affected by safety prompts from the\nperspective of model representation. We find that in the representation space,\nthe input queries are typically moved by safety prompts in a \"higher-refusal\"\ndirection, in which models become more prone to refusing to provide assistance,\neven when the queries are harmless. On the other hand, LLMs are naturally\ncapable of distinguishing harmful and harmless queries without safety prompts.\nInspired by these findings, we propose a method for safety prompt optimization,\nnamely DRO (Directed Representation Optimization). Treating a safety prompt as\ncontinuous, trainable embeddings, DRO learns to move the queries'\nrepresentations along or opposite the refusal direction, depending on their\nharmfulness. Experiments with eight LLMs on out-of-domain and jailbreak\nbenchmarks demonstrate that DRO remarkably improves the safeguarding\nperformance of human-crafted safety prompts, without compromising the models'\ngeneral performance.", "published": "2024-01-31 17:28:24", "link": "http://arxiv.org/abs/2401.18018v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating the Capabilities of LLMs for Supporting Anticipatory Impact\n  Assessment", "abstract": "Gaining insight into the potential negative impacts of emerging Artificial\nIntelligence (AI) technologies in society is a challenge for implementing\nanticipatory governance approaches. One approach to produce such insight is to\nuse Large Language Models (LLMs) to support and guide experts in the process of\nideating and exploring the range of undesirable consequences of emerging\ntechnologies. However, performance evaluations of LLMs for such tasks are still\nneeded, including examining the general quality of generated impacts but also\nthe range of types of impacts produced and resulting biases. In this paper, we\ndemonstrate the potential for generating high-quality and diverse impacts of AI\nin society by fine-tuning completion models (GPT-3 and Mistral-7B) on a diverse\nsample of articles from news media and comparing those outputs to the impacts\ngenerated by instruction-based (GPT-4 and Mistral-7B-Instruct) models. We\nexamine the generated impacts for coherence, structure, relevance, and\nplausibility and find that the generated impacts using Mistral-7B, a small\nopen-source model fine-tuned on impacts from the news media, tend to be\nqualitatively on par with impacts generated using a more capable and larger\nscale model such as GPT-4. Moreover, we find that impacts produced by\ninstruction-based models had gaps in the production of certain categories of\nimpacts in comparison to fine-tuned models. This research highlights a\npotential bias in the range of impacts generated by state-of-the-art LLMs and\nthe potential of aligning smaller LLMs on news media as a scalable alternative\nto generate high quality and more diverse impacts in support of anticipatory\ngovernance approaches.", "published": "2024-01-31 17:43:04", "link": "http://arxiv.org/abs/2401.18028v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition", "abstract": "Recent advancements in language models have significantly enhanced\nperformance in multiple speech-related tasks. Existing speech language models\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\na single model. However, this design omits the intrinsic connections between\ndifferent speech tasks, which can potentially boost the performance of each\ntask. In this work, we propose a novel decoder-only speech language model,\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\nrecognition, speech language modeling, and text language modeling --\nSpeechComposer can easily extend to more speech tasks via compositions of\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\nunification of prompt tokens also makes it possible for knowledge sharing among\ndifferent speech tasks in a more structured manner. Experimental results\ndemonstrate that our proposed SpeechComposer can improve the performance of\nboth primary tasks and composite tasks, showing the effectiveness of the shared\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\nand even better performance than the baselines which are expert models designed\nfor single tasks.", "published": "2024-01-31 18:06:29", "link": "http://arxiv.org/abs/2401.18045v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving\n  as Human Learners?", "abstract": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\nproperties of human cognition are well-modeled by LLMs, and which are not. In\nthis work, we study the biases of LLMs in relation to those known in children\nwhen solving arithmetic word problems. Surveying the learning science\nliterature, we posit that the problem-solving process can be split into three\ndistinct steps: text comprehension, solution planning and solution execution.\nWe construct tests for each one in order to understand whether current LLMs\ndisplay the same cognitive biases as children in these steps. We generate a\nnovel set of word problems for each of these tests, using a neuro-symbolic\napproach that enables fine-grained control over the problem features. We find\nevidence that LLMs, with and without instruction-tuning, exhibit human-like\nbiases in both the text-comprehension and the solution-planning steps of the\nsolving process, but not in the final step, in which the arithmetic expressions\nare executed to obtain the answer.", "published": "2024-01-31 18:48:20", "link": "http://arxiv.org/abs/2401.18070v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Generative AI Support Patients' & Caregivers' Informational Needs?\n  Towards Task-Centric Evaluation Of AI Systems", "abstract": "Generative AI systems such as ChatGPT and Claude are built upon language\nmodels that are typically evaluated for accuracy on curated benchmark datasets.\nSuch evaluation paradigms measure predictive and reasoning capabilities of\nlanguage models but do not assess if they can provide information that is\nuseful to people. In this paper, we take some initial steps in developing an\nevaluation paradigm that centers human understanding and decision-making. We\nstudy the utility of generative AI systems in supporting people in a concrete\ntask - making sense of clinical reports and imagery in order to make a clinical\ndecision. We conducted a formative need-finding study in which participants\ndiscussed chest computed tomography (CT) scans and associated radiology reports\nof a fictitious close relative with a cardiothoracic radiologist. Using\nthematic analysis of the conversation between participants and medical experts,\nwe identified commonly occurring themes across interactions, including\nclarifying medical terminology, locating the problems mentioned in the report\nin the scanned image, understanding disease prognosis, discussing the next\ndiagnostic steps, and comparing treatment options. Based on these themes, we\nevaluated two state-of-the-art generative AI systems against the radiologist's\nresponses. Our results reveal variability in the quality of responses generated\nby the models across various themes. We highlight the importance of\npatient-facing generative AI systems to accommodate a diverse range of\nconversational themes, catering to the real-world informational needs of\npatients.", "published": "2024-01-31 23:24:37", "link": "http://arxiv.org/abs/2402.00234v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Exploring the limits of decoder-only models trained on public speech\n  recognition corpora", "abstract": "The emergence of industrial-scale speech recognition (ASR) models such as\nWhisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio\nonly proprietary data respectively, has led to a stronger need for large scale\npublic ASR corpora and competitive open source pipelines. Unlike the said\nmodels, large language models are typically based on Transformer decoders, and\nit remains unclear if decoder-only models trained on public data alone can\ndeliver competitive performance. In this work, we investigate factors such as\nchoice of training datasets and modeling components necessary for obtaining the\nbest performance using public English ASR corpora alone. Our Decoder-Only\nTransformer for ASR (DOTA) model comprehensively outperforms the\nencoder-decoder open source replication of Whisper (OWSM) on nearly all English\nASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We\nrelease our codebase and model checkpoints under permissive license.", "published": "2024-01-31 23:29:42", "link": "http://arxiv.org/abs/2402.00235v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Early Categorization of Prompt Injection Attacks on Large Language\n  Models", "abstract": "Large language models and AI chatbots have been at the forefront of\ndemocratizing artificial intelligence. However, the releases of ChatGPT and\nother similar tools have been followed by growing concerns regarding the\ndifficulty of controlling large language models and their outputs. Currently,\nwe are witnessing a cat-and-mouse game where users attempt to misuse the models\nwith a novel attack called prompt injections. In contrast, the developers\nattempt to discover the vulnerabilities and block the attacks simultaneously.\nIn this paper, we provide an overview of these emergent threats and present a\ncategorization of prompt injections, which can guide future research on prompt\ninjections and act as a checklist of vulnerabilities in the development of LLM\ninterfaces. Moreover, based on previous literature and our own empirical\nresearch, we discuss the implications of prompt injections to LLM end users,\ndevelopers, and researchers.", "published": "2024-01-31 19:52:00", "link": "http://arxiv.org/abs/2402.00898v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Uncertainty-Aware Explainable Recommendation with Large Language Models", "abstract": "Providing explanations within the recommendation system would boost user\nsatisfaction and foster trust, especially by elaborating on the reasons for\nselecting recommended items tailored to the user. The predominant approach in\nthis domain revolves around generating text-based explanations, with a notable\nemphasis on applying large language models (LLMs). However, refining LLMs for\nexplainable recommendations proves impractical due to time constraints and\ncomputing resource limitations. As an alternative, the current approach\ninvolves training the prompt rather than the LLM. In this study, we developed a\nmodel that utilizes the ID vectors of user and item inputs as prompts for\nGPT-2. We employed a joint training mechanism within a multi-task learning\nframework to optimize both the recommendation task and explanation task. This\nstrategy enables a more effective exploration of users' interests, improving\nrecommendation effectiveness and user satisfaction. Through the experiments,\nour method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor\nand Amazon dataset respectively, demonstrates superior performance over four\nSOTA methods in terms of explainability evaluation metric. In addition, we\nidentified that the proposed model is able to ensure stable textual quality on\nthe three public datasets.", "published": "2024-01-31 14:06:26", "link": "http://arxiv.org/abs/2402.03366v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "LLM Voting: Human Choices and AI Collective Decision Making", "abstract": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.", "published": "2024-01-31 14:52:02", "link": "http://arxiv.org/abs/2402.01766v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC", "68T05, 91B14, 91C20", "I.2.7; J.4; K.4.1"], "primary_category": "cs.CL"}
{"title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and\n  ACE-KiSing", "abstract": "In singing voice synthesis (SVS), generating singing voices from musical\nscores faces challenges due to limited data availability. This study proposes a\nunique strategy to address the data scarcity in SVS. We employ an existing\nsinging voice synthesizer for data augmentation, complemented by detailed\nmanual tuning, an approach not previously explored in data curation, to reduce\ninstances of unnatural voice synthesis. This innovative method has led to the\ncreation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing,\nwhich are instrumental for large-scale, multi-singer voice synthesis. Through\nthorough experimentation, we establish that these datasets not only serve as\nnew benchmarks for SVS but also enhance SVS performance on other singing voice\ndatasets when used as supplementary resources. The corpora, pre-trained models,\nand their related training recipes are publicly available at ESPnet-Muskits\n(\\url{https://github.com/espnet/espnet})", "published": "2024-01-31 06:17:51", "link": "http://arxiv.org/abs/2401.17619v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Audio-Visual Features with Pretrained AV-HuBERT for\n  Multi-Modal Dysarthric Speech Reconstruction", "abstract": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech\ninto normal speech by improving the intelligibility and naturalness. This is a\nchallenging task especially for patients with severe dysarthria and speaking in\ncomplex, noisy acoustic environments. To address these challenges, we propose a\nnovel multi-modal framework to utilize visual information, e.g., lip movements,\nin DSR as extra clues for reconstructing the highly abnormal pronunciations.\nThe multi-modal framework consists of: (i) a multi-modal encoder to extract\nrobust phoneme embeddings from dysarthric speech with auxiliary visual\nfeatures; (ii) a variance adaptor to infer the normal phoneme duration and\npitch contour from the extracted phoneme embeddings; (iii) a speaker encoder to\nencode the speaker's voice characteristics; and (iv) a mel-decoder to generate\nthe reconstructed mel-spectrogram based on the extracted phoneme embeddings,\nprosodic features and speaker embeddings. Both objective and subjective\nevaluations conducted on the commonly used UASpeech corpus show that our\nproposed approach can achieve significant improvements over baseline systems in\nterms of speech intelligibility and naturalness, especially for the speakers\nwith more severe symptoms. Compared with original dysarthric speech, the\nreconstructed speech achieves 42.1\\% absolute word error rate reduction for\npatients with more severe dysarthria levels.", "published": "2024-01-31 12:45:43", "link": "http://arxiv.org/abs/2401.17796v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computation and Parameter Efficient Multi-Modal Fusion Transformer for\n  Cued Speech Recognition", "abstract": "Cued Speech (CS) is a pure visual coding method used by hearing-impaired\npeople that combines lip reading with several specific hand shapes to make the\nspoken language visible. Automatic CS recognition (ACSR) seeks to transcribe\nvisual cues of speech into text, which can help hearing-impaired people to\ncommunicate effectively. The visual information of CS contains lip reading and\nhand cueing, thus the fusion of them plays an important role in ACSR. However,\nmost previous fusion methods struggle to capture the global dependency present\nin long sequence inputs of multi-modal CS data. As a result, these methods\ngenerally fail to learn the effective cross-modal relationships that contribute\nto the fusion. Recently, attention-based transformers have been a prevalent\nidea for capturing the global dependency over the long sequence in multi-modal\nfusion, but existing multi-modal fusion transformers suffer from both poor\nrecognition accuracy and inefficient computation for the ACSR task. To address\nthese problems, we develop a novel computation and parameter efficient\nmulti-modal fusion transformer by proposing a novel Token-Importance-Aware\nAttention mechanism (TIAA), where a token utilization rate (TUR) is formulated\nto select the important tokens from the multi-modal streams. More precisely,\nTIAA firstly models the modality-specific fine-grained temporal dependencies\nover all tokens of each modality, and then learns the efficient cross-modal\ninteraction for the modality-shared coarse-grained temporal dependencies over\nthe important tokens of different modalities. Besides, a light-weight gated\nhidden projection is designed to control the feature flows of TIAA. The\nresulting model, named Economical Cued Speech Fusion Transformer (EcoCued),\nachieves state-of-the-art performance on all existing CS datasets, compared\nwith existing transformer-based fusion methods and ACSR fusion methods.", "published": "2024-01-31 05:20:29", "link": "http://arxiv.org/abs/2401.17604v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for\n  Automated Audio Captioning", "abstract": "We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP\nemploys two acoustic representation models, EnCodec and CLAP, along with a\npretrained language model, BART. We also introduce a new training objective\ncalled masked codec modeling that improves acoustic awareness of the pretrained\nlanguage model. Experimental results on AudioCaps and Clotho demonstrate that\nour model surpasses the performance of baseline models. Source code will be\navailable at https://github.com/jaeyeonkim99/EnCLAP . An online demo is\navailable at https://huggingface.co/spaces/enclap-team/enclap .", "published": "2024-01-31 09:23:16", "link": "http://arxiv.org/abs/2401.17690v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harnessing Smartwatch Microphone Sensors for Cough Detection and\n  Classification", "abstract": "This study investigates the potential of using smartwatches with built-in\nmicrophone sensors for monitoring coughs and detecting various cough types. We\nconducted a study involving 32 participants and collected 9 hours of audio data\nin a controlled manner. Afterward, we processed this data using a structured\napproach, resulting in 223 positive cough samples. We further improved the\ndataset through augmentation techniques and employed a specialized 1D CNN\nmodel. This model achieved an impressive accuracy rate of 98.49% while\nnon-walking and 98.2% while walking, showing smartwatches can detect cough.\nMoreover, our research successfully identified four distinct types of coughs\nusing clustering techniques.", "published": "2024-01-31 10:58:59", "link": "http://arxiv.org/abs/2401.17738v2", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dance-to-Music Generation with Encoder-based Textual Inversion", "abstract": "The seamless integration of music with dance movements is essential for\ncommunicating the artistic intent of a dance piece. This alignment also\nsignificantly improves the immersive quality of gaming experiences and\nanimation productions. Although there has been remarkable advancement in\ncreating high-fidelity music from textual descriptions, current methodologies\nmainly focus on modulating overall characteristics such as genre and emotional\ntone. They often overlook the nuanced management of temporal rhythm, which is\nindispensable in crafting music for dance, since it intricately aligns the\nmusical beats with the dancers' movements. Recognizing this gap, we propose an\nencoder-based textual inversion technique to augment text-to-music models with\nvisual control, facilitating personalized music generation. Specifically, we\ndevelop dual-path rhythm-genre inversion to effectively integrate the rhythm\nand genre of a dance motion sequence into the textual space of a text-to-music\nmodel. Contrary to traditional textual inversion methods, which directly update\ntext embeddings to reconstruct a single target object, our approach utilizes\nseparate rhythm and genre encoders to obtain text embeddings for two\npseudo-words, adapting to the varying rhythms and genres. We collect a new\ndataset called In-the-wild Dance Videos (InDV) and demonstrate that our\napproach outperforms state-of-the-art methods across multiple evaluation\nmetrics. Furthermore, our method is able to adapt to changes in tempo and\neffectively integrates with the inherent text-guided generation capability of\nthe pre-trained model. Our source code and demo videos are available at\n\\url{https://github.com/lsfhuihuiff/Dance-to-music_Siggraph_Asia_2024}", "published": "2024-01-31 12:51:26", "link": "http://arxiv.org/abs/2401.17800v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EVA-GAN: Enhanced Various Audio Generation via Scalable Generative\n  Adversarial Networks", "abstract": "The advent of Large Models marks a new era in machine learning, significantly\noutperforming smaller models by leveraging vast datasets to capture and\nsynthesize complex patterns. Despite these advancements, the exploration into\nscaling, especially in the audio generation domain, remains limited, with\nprevious efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and\nsuffering from both spectral discontinuities and blurriness in the\nhigh-frequency domain, alongside a lack of robustness against out-of-domain\ndata. These limitations restrict the applicability of models to diverse use\ncases, including music and singing generation. Our work introduces Enhanced\nVarious Audio Generation via Scalable Generative Adversarial Networks\n(EVA-GAN), yields significant improvements over previous state-of-the-art in\nspectral and high-frequency reconstruction and robustness in out-of-domain data\nperformance, enabling the generation of HiFi audios by employing an extensive\ndataset of 36,000 hours of 44.1kHz audio, a context-aware module, a\nHuman-In-The-Loop artifact measurement toolkit, and expands the model to\napproximately 200 million parameters. Demonstrations of our work are available\nat https://double-blind-eva-gan.cc.", "published": "2024-01-31 03:31:03", "link": "http://arxiv.org/abs/2402.00892v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Screening method for early dementia using sound objects as voice\n  biomarkers", "abstract": "Introduction: We present a screening method for early dementia using features\nbased on sound objects as voice biomarkers.\n  Methods: The final dataset used for machine learning models consisted of 266\nobservations, with a distribution of 186 healthy individuals, 46 diagnosed with\nAlzheimer's, and 34 with MCI. This method is based on six-second recordings of\nthe sustained vowel /a/ spoken by the subject. The main original contribution\nof this work is the use of carefully crafted features based on sound objects.\nThis approach allows one to first represent the sound spectrum in a more\naccurate way than the standard spectrum, and then build interpretable features\ncontaining relevant information about subjects' control over their voice.\n  Results: ROC AUC obtained in this work for distinguishing healthy subjects\nfrom those with MCI was 0.85, while accuracy was 0.76. For distinguishing\nbetween healthy subjects and those with either MCI or Alzheimer's the results\nwere 0.84, 0.77, respectively.\n  Conclusion: The use of features based on sound objects enables screening for\nearly dementia even on very short recordings of language-independent voice\nsamples.", "published": "2024-01-31 19:20:08", "link": "http://arxiv.org/abs/2402.00897v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
