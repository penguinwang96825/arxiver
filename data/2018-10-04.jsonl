{"title": "Semi-Supervised Methods for Out-of-Domain Dependency Parsing", "abstract": "Dependency parsing is one of the important natural language processing tasks\nthat assigns syntactic trees to texts. Due to the wider availability of\ndependency corpora and improved parsing and machine learning techniques,\nparsing accuracies of supervised learning-based systems have been significantly\nimproved. However, due to the nature of supervised learning, those parsing\nsystems highly rely on the manually annotated training corpora. They work\nreasonably good on the in-domain data but the performance drops significantly\nwhen tested on out-of-domain texts. To bridge the performance gap between\nin-domain and out-of-domain, this thesis investigates three semi-supervised\ntechniques for out-of-domain dependency parsing, namely co-training,\nself-training and dependency language models. Our approaches use easily\nobtainable unlabelled data to improve out-of-domain parsing accuracies without\nthe need of expensive corpora annotation. The evaluations on several English\ndomains and multi-lingual data show quite good improvements on parsing\naccuracy. Overall this work conducted a survey of semi-supervised methods for\nout-of-domain dependency parsing, where I extended and compared a number of\nimportant semi-supervised methods in a unified framework. The comparison\nbetween those techniques shows that self-training works equally well as\nco-training on out-of-domain parsing, while dependency language models can\nimprove both in- and out-of-domain accuracies.", "published": "2018-10-04 08:41:50", "link": "http://arxiv.org/abs/1810.02100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zooming Network", "abstract": "Structural information is important in natural language understanding.\nAlthough some current neural net-based models have a limited ability to take\nlocal syntactic information, they fail to use high-level and large-scale\nstructures of documents. This information is valuable for text understanding\nsince it contains the author's strategy to express information, in building an\neffective representation and forming appropriate output modes. We propose a\nneural net-based model, Zooming Network, capable of representing and leveraging\ntext structure of long document and developing its own analyzing rhythm to\nextract critical information. Generally, ZN consists of an encoding neural net\nthat can build a hierarchical representation of a document, and an interpreting\nneural model that can read the information at multi-levels and issuing labeling\nactions through a policy-net. Our model is trained with a hybrid paradigm of\nsupervised learning (distinguishing right and wrong decision) and reinforcement\nlearning (determining the goodness among multiple right paths). We applied the\nproposed model to long text sequence labeling tasks, with performance exceeding\nbaseline model (biLSTM-crf) by 10 F1-measure.", "published": "2018-10-04 09:20:32", "link": "http://arxiv.org/abs/1810.02114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Networks for Cross-lingual Negation Scope Detection", "abstract": "Negation scope has been annotated in several English and Chinese corpora, and\nhighly accurate models for this task in these languages have been learned from\nthese annotations. Unfortunately, annotations are not available in other\nlanguages. Could a model that detects negation scope be applied to a language\nthat it hasn't been trained on? We develop neural models that learn from\ncross-lingual word embeddings or universal dependencies in English, and test\nthem on Chinese, showing that they work surprisingly well. We find that\nmodelling syntax is helpful even in monolingual settings and that cross-lingual\nword embeddings help relatively little, and we analyse cases that are still\ndifficult for this task.", "published": "2018-10-04 11:51:47", "link": "http://arxiv.org/abs/1810.02156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Span Selection Model for Semantic Role Labeling", "abstract": "We present a simple and accurate span-based model for semantic role labeling\n(SRL). Our model directly takes into account all possible argument spans and\nscores them for each label. At decoding time, we greedily select higher scoring\nlabeled spans. One advantage of our model is to allow us to design and use\nspan-level features, that are difficult to use in token-based BIO tagging\napproaches. Experimental results demonstrate that our ensemble model achieves\nthe state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012\ndatasets, respectively.", "published": "2018-10-04 14:34:02", "link": "http://arxiv.org/abs/1810.02245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun\n  Translation in Neural Machine Translation", "abstract": "The translation of pronouns presents a special challenge to machine\ntranslation to this day, since it often requires context outside the current\nsentence. Recent work on models that have access to information across sentence\nboundaries has seen only moderate improvements in terms of automatic evaluation\nmetrics such as BLEU. However, metrics that quantify the overall translation\nquality are ill-equipped to measure gains from additional context. We argue\nthat a different kind of evaluation is needed to assess how well models\ntranslate inter-sentential phenomena such as pronouns. This paper therefore\npresents a test suite of contrastive translations focused specifically on the\ntranslation of pronouns. Furthermore, we perform experiments with several\ncontext-aware models. We show that, while gains in BLEU are moderate for those\nsystems, they outperform baselines by a large margin in terms of accuracy on\nour contrastive test set. Our experiments also show the effectiveness of\nparameter tying for multi-encoder architectures.", "published": "2018-10-04 15:06:27", "link": "http://arxiv.org/abs/1810.02268v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classical linear logic, cobordisms and categorical semantics of\n  categorial grammars", "abstract": "We propose a categorial grammar based on classical multiplicative linear\nlogic.\n  This can be seen as an extension of abstract categorial grammars (ACG) and is\nat least as expressive. However, constituents of {\\it linear logic grammars\n(LLG)} are not abstract ${\\lambda}$-terms, but simply tuples of words with\nlabeled endpoints, we call them {\\it multiwords}. At least, this gives a\nconcrete and intuitive representation of ACG.\n  A key observation is that the class of multiwords has a fundamental algebraic\nstructure. Namely, multiwords can be organized in a category, very similar to\nthe category of topological cobordisms. This category is symmetric monoidal\nclosed and compact closed and thus is a model of linear $\\lambda$-calculus and\nclassical linear logic. We think that this category is interesting on its own\nright. In particular, it might provide categorical representation for other\nformalisms.\n  On the other hand, many models of language semantics are based on commutative\nlogic or, more generally, on symmetric monoidal closed categories. But the\ncategory of {\\it word cobordisms} is a category of language elements, which is\nitself symmetric monoidal closed and independent of any grammar. Thus, it might\nprove useful in understanding language semantics as well.", "published": "2018-10-04 04:02:11", "link": "http://arxiv.org/abs/1810.02047v6", "categories": ["cs.LO", "cs.CL", "math.LO"], "primary_category": "cs.LO"}
{"title": "Italian Event Detection Goes Deep Learning", "abstract": "This paper reports on a set of experiments with different word embeddings to\ninitialize a state-of-the-art Bi-LSTM-CRF network for event detection and\nclassification in Italian, following the EVENTI evaluation exercise. The net-\nwork obtains a new state-of-the-art result by improving the F1 score for\ndetection of 1.3 points, and of 6.5 points for classification, by using a\nsingle step approach. The results also provide further evidence that embeddings\nhave a major impact on the performance of such architectures.", "published": "2018-10-04 14:09:20", "link": "http://arxiv.org/abs/1810.02229v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language\n  Understanding", "abstract": "We marry two powerful ideas: deep representation learning for visual\nrecognition and language understanding, and symbolic program execution for\nreasoning. Our neural-symbolic visual question answering (NS-VQA) system first\nrecovers a structural scene representation from the image and a program trace\nfrom the question. It then executes the program on the scene representation to\nobtain an answer. Incorporating symbolic structure as prior knowledge offers\nthree unique advantages. First, executing programs on a symbolic space is more\nrobust to long program traces; our model can solve complex reasoning tasks\nbetter, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model\nis more data- and memory-efficient: it performs well after learning on a small\nnumber of training data; it can also encode an image into a compact\nrepresentation, requiring less storage than existing methods for offline\nquestion answering. Third, symbolic program execution offers full transparency\nto the reasoning process; we are thus able to interpret and diagnose each\nexecution step.", "published": "2018-10-04 17:38:50", "link": "http://arxiv.org/abs/1810.02338v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Building a language evolution tree based on word vector combination\n  model", "abstract": "In this paper, we try to explore the evolution of language through case\ncalculations. First, we chose the novels of eleven British writers from 1400 to\n2005 and found the corresponding works; Then, we use the natural language\nprocessing tool to construct the corresponding eleven corpora, and calculate\nthe respective word vectors of 100 high-frequency words in eleven corpora;\nNext, for each corpus, we concatenate the 100 word vectors from beginning to\nend into one; Finally, we use the similarity comparison and hierarchical\nclustering method to generate the relationship tree between the combined eleven\nword vectors. This tree represents the relationship between eleven corpora. We\nfound that in the tree generated by clustering, the distance between the corpus\nand the year corresponding to the corpus are basically the same. This means\nthat we have discovered a specific language evolution tree. To verify the\nstability and versatility of this method, we add three other themes: Dickens's\neight works, the 19th century poets' works, and art criticism of recent 60\nyears. For these four themes, we tested different parameters such as the time\nspan of the corpus, the time interval between the corpora, the dimension of the\nword vector, and the number of high-frequency public words. The results show\nthat this is fairly stable and versatile.", "published": "2018-10-04 14:25:36", "link": "http://arxiv.org/abs/1810.03445v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multilingual sequence-to-sequence speech recognition: architecture,\n  transfer learning, and language modeling", "abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively\nnew direction in speech research. The approach benefits by performing model\ntraining without using lexicon and alignments. However, this poses a new\nproblem of requiring more data compared to conventional DNN-HMM systems. In\nthis work, we attempt to use data from 10 BABEL languages to build a\nmulti-lingual seq2seq model as a prior model, and then port them towards 4\nother BABEL languages using transfer learning approach. We also explore\ndifferent architectures for improving the prior multilingual seq2seq model. The\npaper also discusses the effect of integrating a recurrent neural network\nlanguage model (RNNLM) with a seq2seq model during decoding. Experimental\nresults show that the transfer learning approach from the multilingual model\nshows substantial gains over monolingual models across all 4 BABEL languages.\nIncorporating an RNNLM also brings significant improvements in terms of %WER,\nand achieves recognition performance comparable to the models trained with\ntwice more training data.", "published": "2018-10-04 08:53:42", "link": "http://arxiv.org/abs/1810.03459v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep Learning Approaches for Understanding Simple Speech Commands", "abstract": "Automatic classification of sound commands is becoming increasingly\nimportant, especially for mobile and embedded devices. Many of these devices\ncontain both cameras and microphones, and companies that develop them would\nlike to use the same technology for both of these classification tasks. One way\nof achieving this is to represent sound commands as images, and use\nconvolutional neural networks when classifying images as well as sounds. In\nthis paper we consider several approaches to the problem of sound\nclassification that we applied in TensorFlow Speech Recognition Challenge\norganized by Google Brain team on the Kaggle platform. Here we show different\nrepresentation of sounds (Wave frames, Spectrograms, Mel-Spectrograms, MFCCs)\nand apply several 1D and 2D convolutional neural networks in order to get the\nbest performance. Our experiments show that we found appropriate sound\nrepresentation and corresponding convolutional neural networks. As a result we\nachieved good classification accuracy that allowed us to finish the challenge\non 8-th place among 1315 teams.", "published": "2018-10-04 17:42:18", "link": "http://arxiv.org/abs/1810.02364v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
