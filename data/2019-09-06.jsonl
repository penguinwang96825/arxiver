{"title": "Incorporating External Knowledge into Machine Reading for Generative\n  Question Answering", "abstract": "Commonsense and background knowledge is required for a QA model to answer\nmany nontrivial questions. Different from existing work on knowledge-aware QA,\nwe focus on a more challenging task of leveraging external knowledge to\ngenerate answers in natural language for a given question with context.\n  In this paper, we propose a new neural model, Knowledge-Enriched Answer\nGenerator (KEAG), which is able to compose a natural answer by exploiting and\naggregating evidence from all four information sources available: question,\npassage, vocabulary and knowledge. During the process of answer generation,\nKEAG adaptively determines when to utilize symbolic knowledge and which fact\nfrom the knowledge is useful. This allows the model to exploit external\nknowledge that is not explicitly stated in the given text, but that is relevant\nfor generating an answer. The empirical study on public benchmark of answer\ngeneration demonstrates that KEAG improves answer quality over models without\nknowledge and existing knowledge-aware models, confirming its effectiveness in\nleveraging knowledge.", "published": "2019-09-06 07:20:17", "link": "http://arxiv.org/abs/1909.02745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Search of Logical Forms for Weakly Supervised Knowledge-Based\n  Question Answering", "abstract": "Many algorithms for Knowledge-Based Question Answering (KBQA) depend on\nsemantic parsing, which translates a question to its logical form. When only\nweak supervision is provided, it is usually necessary to search valid logical\nforms for model training. However, a complex question typically involves a huge\nsearch space, which creates two main problems: 1) the solutions limited by\ncomputation time and memory usually reduce the success rate of the search, and\n2) spurious logical forms in the search results degrade the quality of training\ndata. These two problems lead to a poorly-trained semantic parsing model. In\nthis work, we propose an effective search method for weakly supervised KBQA\nbased on operator prediction for questions. With search space constrained by\npredicted operators, sufficient search paths can be explored, more valid\nlogical forms can be derived, and operators possibly causing spurious logical\nforms can be avoided. As a result, a larger proportion of questions in a weakly\nsupervised training set are equipped with logical forms, and fewer spurious\nlogical forms are generated. Such high-quality training data directly\ncontributes to a better semantic parsing model. Experimental results on one of\nthe largest KBQA datasets (i.e., CSQA) verify the effectiveness of our\napproach: improving the precision from 67% to 72% and the recall from 67% to\n72% in terms of the overall score.", "published": "2019-09-06 08:22:28", "link": "http://arxiv.org/abs/1909.02762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Giveme5W1H: A Universal System for Extracting Main Events from News\n  Articles", "abstract": "Event extraction from news articles is a commonly required prerequisite for\nvarious tasks, such as article summarization, article clustering, and news\naggregation. Due to the lack of universally applicable and publicly available\nmethods tailored to news datasets, many researchers redundantly implement event\nextraction methods for their own projects. The journalistic 5W1H questions are\ncapable of describing the main event of an article, i.e., by answering who did\nwhat, when, where, why, and how. We provide an in-depth description of an\nimproved version of Giveme5W1H, a system that uses syntactic and\ndomain-specific rules to automatically extract the relevant phrases from\nEnglish news articles to provide answers to these 5W1H questions. Given the\nanswers to these questions, the system determines an article's main event. In\nan expert evaluation with three assessors and 120 articles, we determined an\noverall precision of p=0.73, and p=0.82 for answering the first four W\nquestions, which alone can sufficiently summarize the main event reported on in\na news article. We recently made our system publicly available, and it remains\nthe only universal open-source 5W1H extractor capable of being applied to a\nwide range of use cases in news analysis.", "published": "2019-09-06 08:37:42", "link": "http://arxiv.org/abs/1909.02766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Forget the Long Tail! A Comprehensive Analysis of Morphological\n  Generalization in Bilingual Lexicon Induction", "abstract": "Human translators routinely have to translate rare inflections of words - due\nto the Zipfian distribution of words in a language. When translating from\nSpanish, a good translator would have no problem identifying the proper\ntranslation of a statistically rare inflection such as habl\\'aramos. Note the\nlexeme itself, hablar, is relatively common. In this work, we investigate\nwhether state-of-the-art bilingual lexicon inducers are capable of learning\nthis kind of generalization. We introduce 40 morphologically complete\ndictionaries in 10 languages and evaluate three of the state-of-the-art models\non the task of translation of less frequent morphological forms. We demonstrate\nthat the performance of state-of-the-art models drops considerably when\nevaluated on infrequent morphological inflections and then show that adding a\nsimple morphological constraint at training time improves the performance,\nproving that the bilingual lexicon inducers can benefit from better encoding of\nmorphology.", "published": "2019-09-06 12:27:43", "link": "http://arxiv.org/abs/1909.02855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A systematic comparison of methods for low-resource dependency parsing\n  on genuinely low-resource languages", "abstract": "Parsers are available for only a handful of the world's languages, since they\nrequire lots of training data. How far can we get with just a small amount of\ntraining data? We systematically compare a set of simple strategies for\nimproving low-resource parsers: data augmentation, which has not been tested\nbefore; cross-lingual training; and transliteration. Experimenting on three\ntypologically diverse low-resource languages---North S\\'ami, Galician, and\nKazah---We find that (1) when only the low-resource treebank is available, data\naugmentation is very helpful; (2) when a related high-resource treebank is\navailable, cross-lingual training is helpful and complements data augmentation;\nand (3) when the high-resource treebank uses a different writing system,\ntransliteration into a shared orthographic spaces is also very helpful.", "published": "2019-09-06 12:32:52", "link": "http://arxiv.org/abs/1909.02857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting and Learning a Dependency-Enhanced Type Lexicon for Dutch", "abstract": "This thesis is concerned with type-logical grammars and their practical\napplicability as tools of reasoning about sentence syntax and semantics. The\nfocal point is narrowed to Dutch, a language exhibiting a large degree of word\norder variability. In order to overcome difficulties arising as a result of\nthat variability, the thesis explores and expands upon a type grammar based on\nMultiplicative Intuitionistic Linear Logic, agnostic to word order but enriched\nwith decorations that aim to reduce its proof-theoretic complexity. An\nalgorithm for the conversion of dependency-annotated sentences into type\nsequences is then implemented, populating the type logic with concrete,\ndata-driven lexical types. Two experiments are ran on the resulting grammar\ninstantiation. The first pertains to the learnability of the type-assignment\nprocess by a neural architecture. A novel application of a self-attentive\nsequence transduction model is proposed; contrary to established practices, it\nconstructs types inductively by internalizing the type-formation syntax, thus\nexhibiting generalizability beyond a pre-specified type vocabulary. The second\nrevolves around a deductive parsing system that can resolve structural\nambiguities by consulting both word and type information; preliminary results\nsuggest both excellent computational efficiency and performance.", "published": "2019-09-06 15:03:29", "link": "http://arxiv.org/abs/1909.02955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Component Classification for Classroom Discussions", "abstract": "This paper focuses on argument component classification for transcribed\nspoken classroom discussions, with the goal of automatically classifying\nstudent utterances into claims, evidence, and warrants. We show that an\nexisting method for argument component classification developed for another\neducationally-oriented domain performs poorly on our dataset. We then show that\nfeature sets from prior work on argument mining for student essays and online\ndialogues can be used to improve performance considerably. We also provide a\ncomparison between convolutional neural networks and recurrent neural networks\nwhen trained under different conditions to classify argument components in\nclassroom discussions. While neural network models are not always able to\noutperform a logistic regression model, we were able to gain some useful\ninsights: convolutional networks are more robust than recurrent networks both\nat the character and at the word level, and specificity information can help\nboost performance in multi-task training.", "published": "2019-09-06 17:17:06", "link": "http://arxiv.org/abs/1909.03022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating Student Talk in Text-based Classroom Discussions", "abstract": "Classroom discussions in English Language Arts have a positive effect on\nstudents' reading, writing and reasoning skills. Although prior work has\nlargely focused on teacher talk and student-teacher interactions, we focus on\nthree theoretically-motivated aspects of high-quality student talk:\nargumentation, specificity, and knowledge domain. We introduce an annotation\nscheme, then show that the scheme can be used to produce reliable annotations\nand that the annotations are predictive of discussion quality. We also\nhighlight opportunities provided by our scheme for education and natural\nlanguage processing research.", "published": "2019-09-06 17:18:49", "link": "http://arxiv.org/abs/1909.03023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertain Natural Language Inference", "abstract": "We introduce Uncertain Natural Language Inference (UNLI), a refinement of\nNatural Language Inference (NLI) that shifts away from categorical labels,\ntargeting instead the direct prediction of subjective probability assessments.\nWe demonstrate the feasibility of collecting annotations for UNLI by relabeling\na portion of the SNLI dataset under a probabilistic scale, where items even\nwith the same categorical label differ in how likely people judge them to be\ntrue given a premise. We describe a direct scalar regression modeling approach,\nand find that existing categorically labeled NLI data can be used in\npre-training. Our best models approach human performance, demonstrating models\nmay be capable of more subtle inferences than the categorical bin assignment\nemployed in current NLI tasks.", "published": "2019-09-06 17:52:55", "link": "http://arxiv.org/abs/1909.03042v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "\"Going on a vacation\" takes longer than \"Going for a walk\": A Study of\n  Temporal Commonsense Understanding", "abstract": "Understanding time is crucial for understanding events expressed in natural\nlanguage. Because people rarely say the obvious, it is often necessary to have\ncommonsense knowledge about various temporal aspects of events, such as\nduration, frequency, and temporal order. However, this important problem has so\nfar received limited attention. This paper systematically studies this temporal\ncommonsense problem. Specifically, we define five classes of temporal\ncommonsense, and use crowdsourcing to develop a new dataset, MCTACO, that\nserves as a test set for this task. We find that the best current methods used\non MCTACO are still far behind human performance, by about 20%, and discuss\nseveral directions for improvement. We hope that the new dataset and our study\nhere can foster more future research on this topic.", "published": "2019-09-06 14:12:15", "link": "http://arxiv.org/abs/1909.03065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Discriminate Perturbations for Blocking Adversarial Attacks\n  in Text Classification", "abstract": "Adversarial attacks against machine learning models have threatened various\nreal-world applications such as spam filtering and sentiment analysis. In this\npaper, we propose a novel framework, learning to DIScriminate Perturbations\n(DISP), to identify and adjust malicious perturbations, thereby blocking\nadversarial attacks for text classification models. To identify adversarial\nattacks, a perturbation discriminator validates how likely a token in the text\nis perturbed and provides a set of potential perturbations. For each potential\nperturbation, an embedding estimator learns to restore the embedding of the\noriginal word based on the context and a replacement token is chosen based on\napproximate kNN search. DISP can block adversarial attacks for any NLP model\nwithout modifying the model structure or training procedure. Extensive\nexperiments on two benchmark datasets demonstrate that DISP significantly\noutperforms baseline methods in blocking adversarial attacks for text\nclassification. In addition, in-depth analysis shows the robustness of DISP\nacross different situations.", "published": "2019-09-06 18:13:48", "link": "http://arxiv.org/abs/1909.03084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and\n  Multi-turn Comparisons", "abstract": "While dialogue remains an important end-goal of natural language research,\nthe difficulty of evaluation is an oft-quoted reason why it remains troublesome\nto make real progress towards its solution. Evaluation difficulties are\nactually two-fold: not only do automatic metrics not correlate well with human\njudgments, but also human judgments themselves are in fact difficult to\nmeasure. The two most used human judgment tests, single-turn pairwise\nevaluation and multi-turn Likert scores, both have serious flaws as we discuss\nin this work.\n  We instead provide a novel procedure involving comparing two full dialogues,\nwhere a human judge is asked to pay attention to only one speaker within each,\nand make a pairwise judgment. The questions themselves are optimized to\nmaximize the robustness of judgments across different annotators, resulting in\nbetter tests. We also show how these tests work in self-play model chat setups,\nresulting in faster, cheaper tests. We hope these tests become the de facto\nstandard, and will release open-source code to that end.", "published": "2019-09-06 18:44:49", "link": "http://arxiv.org/abs/1909.03087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attending the Emotions to Detect Online Abusive Language", "abstract": "In recent years, abusive behavior has become a serious issue in online social\nnetworks. In this paper, we present a new corpus from a semi-anonymous social\nmedia platform, which contains the instances of offensive and neutral classes.\nWe introduce a single deep neural architecture that considers both local and\nsequential information from the text in order to detect abusive language. Along\nwith this model, we introduce a new attention mechanism called emotion-aware\nattention. This mechanism utilizes the emotions behind the text to find the\nmost important words within that text. We experiment with this model on our\ndataset and later present the analysis. Additionally, we evaluate our proposed\nmethod on different corpora and show new state-of-the-art results with respect\nto offensive language detection.", "published": "2019-09-06 19:40:06", "link": "http://arxiv.org/abs/1909.03100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Sentence Embedding using Discrete Cosine Transform", "abstract": "Vector averaging remains one of the most popular sentence embedding methods\nin spite of its obvious disregard for syntactic structure. While more complex\nsequential or convolutional networks potentially yield superior classification\nperformance, the improvements in classification accuracy are typically mediocre\ncompared to the simple vector averaging. As an efficient alternative, we\npropose the use of discrete cosine transform (DCT) to compress word sequences\nin an order-preserving manner. The lower order DCT coefficients represent the\noverall feature patterns in sentences, which results in suitable embeddings for\ntasks that could benefit from syntactic features. Our results in semantic\nprobing tasks demonstrate that DCT embeddings indeed preserve more syntactic\ninformation compared with vector averaging. With practically equivalent\ncomplexity, the model yields better overall performance in downstream\nclassification tasks that correlate with syntactic features, which illustrates\nthe capacity of DCT to preserve word order information.", "published": "2019-09-06 19:44:48", "link": "http://arxiv.org/abs/1909.03104v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To lemmatize or not to lemmatize: how word normalisation affects ELMo\n  performance in word sense disambiguation", "abstract": "We critically evaluate the widespread assumption that deep learning NLP\nmodels do not require lemmatized input. To test this, we trained versions of\ncontextualised word embedding ELMo models on raw tokenized corpora and on the\ncorpora with word tokens replaced by their lemmas. Then, these models were\nevaluated on the word sense disambiguation task. This was done for the English\nand Russian languages.\n  The experiments showed that while lemmatization is indeed not necessary for\nEnglish, the situation is different for Russian. It seems that for\nrich-morphology languages, using lemmatized training and testing data yields\nsmall but consistent improvements: at least for word sense disambiguation. This\nmeans that the decisions about text pre-processing before training ELMo should\nconsider the linguistic nature of the language in question.", "published": "2019-09-06 21:49:47", "link": "http://arxiv.org/abs/1909.03135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Machine Translation with Dependency-Aware Self-Attention", "abstract": "Most neural machine translation models only rely on pairs of parallel\nsentences, assuming syntactic information is automatically learned by an\nattention mechanism. In this work, we investigate different approaches to\nincorporate syntactic knowledge in the Transformer model and also propose a\nnovel, parameter-free, dependency-aware self-attention mechanism that improves\nits translation quality, especially for long sentences and in low-resource\nscenarios. We show the efficacy of each approach on WMT English-German and\nEnglish-Turkish, and WAT English-Japanese translation tasks.", "published": "2019-09-06 23:29:57", "link": "http://arxiv.org/abs/1909.03149v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Features in Extractive Supervised Single-document Summarization: Case of\n  Persian News", "abstract": "Text summarization has been one of the most challenging areas of research in\nNLP. Much effort has been made to overcome this challenge by using either the\nabstractive or extractive methods. Extractive methods are more popular, due to\ntheir simplicity compared with the more elaborate abstractive methods. In\nextractive approaches, the system will not generate sentences. Instead, it\nlearns how to score sentences within the text by using some textual features\nand subsequently selecting those with the highest-rank. Therefore, the core\nobjective is ranking and it highly depends on the document. This dependency has\nbeen unnoticed by many state-of-the-art solutions. In this work, the features\nof the document are integrated into vectors of every sentence. In this way, the\nsystem becomes informed about the context, increases the precision of the\nlearned model and consequently produces comprehensive and brief summaries.", "published": "2019-09-06 09:04:14", "link": "http://arxiv.org/abs/1909.02776v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "#MeTooMaastricht: Building a chatbot to assist survivors of sexual\n  harassment", "abstract": "Inspired by the recent social movement of #MeToo, we are building a chatbot\nto assist survivors of sexual harassment cases (designed for the city of\nMaastricht but can easily be extended). The motivation behind this work is\ntwofold: properly assist survivors of such events by directing them to\nappropriate institutions that can offer them help and increase the incident\ndocumentation so as to gather more data about harassment cases which are\ncurrently under reported. We break down the problem into three data\nscience/machine learning components: harassment type identification (treated as\na classification problem), spatio-temporal information extraction (treated as\nNamed Entity Recognition problem) and dialogue with the users (treated as a\nslot-filling based chatbot). We are able to achieve a success rate of more than\n98% for the identification of a harassment-or-not case and around 80% for the\nspecific type harassment identification. Locations and dates are identified\nwith more than 90% accuracy and time occurrences prove more challenging with\nalmost 80%. Finally, initial validation of the chatbot shows great potential\nfor the further development and deployment of such a beneficial for the whole\nsociety tool.", "published": "2019-09-06 10:36:33", "link": "http://arxiv.org/abs/1909.02809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "User Evaluation of a Multi-dimensional Statistical Dialogue System", "abstract": "We present the first complete spoken dialogue system driven by a\nmulti-dimensional statistical dialogue manager. This framework has been shown\nto substantially reduce data needs by leveraging domain-independent dimensions,\nsuch as social obligations or feedback, which (as we show) can be transferred\nbetween domains. In this paper, we conduct a user study and show that the\nperformance of a multi-dimensional system, which can be adapted from a source\ndomain, is equivalent to that of a one-dimensional baseline, which can only be\ntrained from scratch.", "published": "2019-09-06 15:10:37", "link": "http://arxiv.org/abs/1909.02965v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abductive Reasoning as Self-Supervision for Common Sense Question\n  Answering", "abstract": "Question answering has seen significant advances in recent times, especially\nwith the introduction of increasingly bigger transformer-based models\npre-trained on massive amounts of data. While achieving impressive results on\nmany benchmarks, their performances appear to be proportional to the amount of\ntraining data available in the target domain. In this work, we explore the\nability of current question-answering models to generalize - to both other\ndomains as well as with restricted training data. We find that large amounts of\ntraining data are necessary, both for pre-training as well as fine-tuning to a\ntask, for the models to perform well on the designated task. We introduce a\nnovel abductive reasoning approach based on Grenander's Pattern Theory\nframework to provide self-supervised domain adaptation cues or \"pseudo-labels,\"\nwhich can be used instead of expensive human annotations. The proposed\nself-supervised training regimen allows for effective domain adaptation without\nlosing performance compared to fully supervised baselines. Extensive\nexperiments on two publicly available benchmarks show the efficacy of the\nproposed approach. We show that neural networks models trained using\nself-labeled data can retain up to $75\\%$ of the performance of models trained\non large amounts of human-annotated training data.", "published": "2019-09-06 19:39:37", "link": "http://arxiv.org/abs/1909.03099v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars\n  using Transfer Learning", "abstract": "The recognition of emotions by humans is a complex process which considers\nmultiple interacting signals such as facial expressions and both prosody and\nsemantic content of utterances. Commonly, research on automatic recognition of\nemotions is, with few exceptions, limited to one modality. We describe an\nin-car experiment for emotion recognition from speech interactions for three\nmodalities: the audio signal of a spoken interaction, the visual signal of the\ndriver's face, and the manually transcribed content of utterances of the\ndriver. We use off-the-shelf tools for emotion detection in audio and face and\ncompare that to a neural transfer learning approach for emotion recognition\nfrom text which utilizes existing resources from other domains. We see that\ntransfer learning enables models based on out-of-domain corpora to perform\nwell. This method contributes up to 10 percentage points in F1, with up to 76\nmicro-average F1 across the emotions joy, annoyance and insecurity. Our\nfindings also indicate that off-the-shelf-tools analyzing face and audio are\nnot ready yet for emotion detection in in-car speech interactions without\nfurther adjustments.", "published": "2019-09-06 08:33:00", "link": "http://arxiv.org/abs/1909.02764v2", "categories": ["cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Structured Query Construction via Knowledge Graph Embedding", "abstract": "In order to facilitate the accesses of general users to knowledge graphs, an\nincreasing effort is being exerted to construct graph-structured queries of\ngiven natural language questions. At the core of the construction is to deduce\nthe structure of the target query and determine the vertices/edges which\nconstitute the query. Existing query construction methods rely on question\nunderstanding and conventional graph-based algorithms which lead to inefficient\nand degraded performances facing complex natural language questions over\nknowledge graphs with large scales. In this paper, we focus on this problem and\npropose a novel framework standing on recent knowledge graph embedding\ntechniques. Our framework first encodes the underlying knowledge graph into a\nlow-dimensional embedding space by leveraging generalized local knowledge\ngraphs. Given a natural language question, the learned embedding\nrepresentations of the knowledge graph are utilized to compute the query\nstructure and assemble vertices/edges into the target query. Extensive\nexperiments were conducted on the benchmark dataset, and the results\ndemonstrate that our framework outperforms state-of-the-art baseline models\nregarding effectiveness and efficiency.", "published": "2019-09-06 14:29:00", "link": "http://arxiv.org/abs/1909.02930v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Supervised Multimodal Bitransformers for Classifying Images and Text", "abstract": "Self-supervised bidirectional transformer models such as BERT have led to\ndramatic improvements in a wide variety of textual classification tasks. The\nmodern digital world is increasingly multimodal, however, and textual\ninformation is often accompanied by other modalities such as images. We\nintroduce a supervised multimodal bitransformer model that fuses information\nfrom text and image encoders, and obtain state-of-the-art performance on\nvarious multimodal classification benchmark tasks, outperforming strong\nbaselines, including on hard test sets specifically designed to measure\nmultimodal performance.", "published": "2019-09-06 14:59:18", "link": "http://arxiv.org/abs/1909.02950v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Show Your Work: Improved Reporting of Experimental Results", "abstract": "Research in natural language processing proceeds, in part, by demonstrating\nthat new models achieve superior performance (e.g., accuracy) on held-out test\ndata, compared to previous results. In this paper, we demonstrate that test-set\nperformance scores alone are insufficient for drawing accurate conclusions\nabout which model performs best. We argue for reporting additional details,\nespecially performance on validation data obtained during model development. We\npresent a novel technique for doing so: expected validation performance of the\nbest-found model as a function of computation budget (i.e., the number of\nhyperparameter search trials or the overall training time). Using our approach,\nwe find multiple recent model comparisons where authors would have reached a\ndifferent conclusion if they had used more (or less) computation. Our approach\nalso allows us to estimate the amount of computation required to obtain a given\naccuracy; applying it to several recently published results yields massive\nvariation across papers, from hours to weeks. We conclude with a set of best\npractices for reporting experimental results which allow for robust future\ncomparisons, and provide code to allow researchers to use our technique.", "published": "2019-09-06 16:40:42", "link": "http://arxiv.org/abs/1909.03004v1", "categories": ["cs.LG", "cs.CL", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RNN Architecture Learning with Sparse Regularization", "abstract": "Neural models for NLP typically use large numbers of parameters to reach\nstate-of-the-art performance, which can lead to excessive memory usage and\nincreased runtime. We present a structure learning method for learning sparse,\nparameter-efficient NLP models. Our method applies group lasso to rational RNNs\n(Peng et al., 2018), a family of models that is closely connected to weighted\nfinite-state automata (WFSAs). We take advantage of rational RNNs' natural\ngrouping of the weights, so the group lasso penalty directly removes WFSA\nstates, substantially reducing the number of parameters in the model. Our\nexperiments on a number of sentiment analysis datasets, using both GloVe and\nBERT embeddings, show that our approach learns neural structures which have\nfewer parameters without sacrificing performance relative to parameter-rich\nbaselines. Our method also highlights the interpretable properties of rational\nRNNs. We show that sparsifying such models makes them easier to visualize, and\nwe present models that rely exclusively on as few as three WFSAs after pruning\nmore than 90% of the weights. We publicly release our code.", "published": "2019-09-06 16:51:21", "link": "http://arxiv.org/abs/1909.03011v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved Hierarchical Patient Classification with Language Model\n  Pretraining over Clinical Notes", "abstract": "Clinical notes in electronic health records contain highly heterogeneous\nwriting styles, including non-standard terminology or abbreviations. Using\nthese notes in predictive modeling has traditionally required preprocessing\n(e.g. taking frequent terms or topic modeling) that removes much of the\nrichness of the source data. We propose a pretrained hierarchical recurrent\nneural network model that parses minimally processed clinical notes in an\nintuitive fashion, and show that it improves performance for discharge\ndiagnosis classification tasks on the Medical Information Mart for Intensive\nCare III (MIMIC-III) dataset, compared to models that treat the notes as an\nunordered collection of terms or that conduct no pretraining. We also apply an\nattribution technique to examples to identify the words that the model uses to\nmake its prediction, and show the importance of the words' nearby context.", "published": "2019-09-06 17:49:56", "link": "http://arxiv.org/abs/1909.03039v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep learning with sentence embeddings pre-trained on biomedical corpora\n  improves the performance of finding similar sentences in electronic medical\n  records", "abstract": "Capturing sentence semantics plays a vital role in a range of text mining\napplications. Despite continuous efforts on the development of related datasets\nand models in the general domain, both datasets and models are limited in\nbiomedical and clinical domains. The BioCreative/OHNLP organizers have made the\nfirst attempt to annotate 1,068 sentence pairs from clinical notes and have\ncalled for a community effort to tackle the Semantic Textual Similarity\n(BioCreative/OHNLP STS) challenge. We developed models using traditional\nmachine learning and deep learning approaches. For the post challenge, we focus\non two models: the Random Forest and the Encoder Network. We applied sentence\nembeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and\nupdated the Random Forest and the Encoder Network accordingly. The official\nresults demonstrated our best submission was the ensemble of eight models. It\nachieved a Person correlation coefficient of 0.8328, the highest performance\namong 13 submissions from 4 teams. For the post challenge, the performance of\nboth Random Forest and the Encoder Network was improved; in particular, the\ncorrelation of the Encoder Network was improved by ~13%. During the challenge\ntask, no end-to-end deep learning models had better performance than machine\nlearning models that take manually-crafted features. In contrast, with the\nsentence embeddings pre-trained on biomedical corpora, the Encoder Network now\nachieves a correlation of ~0.84, which is higher than the original best model.\nThe ensembled model taking the improved versions of the Random Forest and\nEncoder Network as inputs further increased performance to 0.8528. Deep\nlearning models with sentence embeddings pre-trained on biomedical corpora\nachieve the highest performance on the test set.", "published": "2019-09-06 17:56:01", "link": "http://arxiv.org/abs/1909.03044v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Context-aware Deep Model for Entity Recommendation in Search Engine at\n  Alibaba", "abstract": "Entity recommendation, providing search users with an improved experience via\nassisting them in finding related entities for a given query, has become an\nindispensable feature of today's search engines. Existing studies typically\nonly consider the queries with explicit entities. They usually fail to handle\ncomplex queries that without entities, such as \"what food is good for cold\nweather\", because their models could not infer the underlying meaning of the\ninput text. In this work, we believe that contexts convey valuable evidence\nthat could facilitate the semantic modeling of queries, and take them into\nconsideration for entity recommendation. In order to better model the semantics\nof queries and entities, we learn the representation of queries and entities\njointly with attentive deep neural networks. We evaluate our approach using\nlarge-scale, real-world search logs from a widely used commercial Chinese\nsearch engine. Our system has been deployed in ShenMa Search Engine and you can\nfetch it in UC Browser of Alibaba. Results from online A/B test suggest that\nthe impression efficiency of click-through rate increased by 5.1% and page view\nincreased by 5.5%.", "published": "2019-09-06 07:47:20", "link": "http://arxiv.org/abs/1909.04493v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Natural Adversarial Sentence Generation with Gradient-based Perturbation", "abstract": "This work proposes a novel algorithm to generate natural language adversarial\ninput for text classification models, in order to investigate the robustness of\nthese models. It involves applying gradient-based perturbation on the sentence\nembeddings that are used as the features for the classifier, and learning a\ndecoder for generation. We employ this method to a sentiment analysis model and\nverify its effectiveness in inducing incorrect predictions by the model. We\nalso conduct quantitative and qualitative analysis on these examples and\ndemonstrate that our approach can generate more natural adversaries. In\naddition, it can be used to successfully perform black-box attacks, which\ninvolves attacking other existing models whose parameters are not known. On a\npublic sentiment analysis API, the proposed method introduces a 20% relative\ndecrease in average accuracy and 74% relative increase in absolute error.", "published": "2019-09-06 07:22:01", "link": "http://arxiv.org/abs/1909.04495v1", "categories": ["cs.IR", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Building Task-Oriented Visual Dialog Systems Through Alternative\n  Optimization Between Dialog Policy and Language Generation", "abstract": "Reinforcement learning (RL) is an effective approach to learn an optimal\ndialog policy for task-oriented visual dialog systems. A common practice is to\napply RL on a neural sequence-to-sequence (seq2seq) framework with the action\nspace being the output vocabulary in the decoder. However, it is difficult to\ndesign a reward function that can achieve a balance between learning an\neffective policy and generating a natural dialog response. This paper proposes\na novel framework that alternatively trains a RL policy for image guessing and\na supervised seq2seq model to improve dialog generation quality. We evaluate\nour framework on the GuessWhich task and the framework achieves the\nstate-of-the-art performance in both task completion and dialog quality.", "published": "2019-09-06 01:28:34", "link": "http://arxiv.org/abs/1909.05365v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning in Text Streams: Discovery and Disambiguation of Entity and\n  Relation Instances", "abstract": "We consider a scenario where an artificial agent is reading a stream of text\ncomposed of a set of narrations, and it is informed about the identity of some\nof the individuals that are mentioned in the text portion that is currently\nbeing read. The agent is expected to learn to follow the narrations, thus\ndisambiguating mentions and discovering new individuals. We focus on the case\nin which individuals are entities and relations, and we propose an end-to-end\ntrainable memory network that learns to discover and disambiguate them in an\nonline manner, performing one-shot learning, and dealing with a small number of\nsparse supervisions. Our system builds a not-given-in-advance knowledge base,\nand it improves its skills while reading unsupervised text. The model deals\nwith abrupt changes in the narration, taking into account their effects when\nresolving co-references. We showcase the strong disambiguation and discovery\nskills of our model on a corpus of Wikipedia documents and on a newly\nintroduced dataset, that we make publicly available.", "published": "2019-09-06 15:01:07", "link": "http://arxiv.org/abs/1909.05367v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An Auxiliary Classifier Generative Adversarial Framework for Relation\n  Extraction", "abstract": "Relation extraction models suffer from limited qualified training data. Using\nhuman annotators to label sentences is too expensive and does not scale well\nespecially when dealing with large datasets. In this paper, we use Auxiliary\nClassifier Generative Adversarial Networks (AC-GANs) to generate high-quality\nrelational sentences and to improve the performance of relation classifier in\nend-to-end models. In AC-GAN, the discriminator gives not only a probability\ndistribution over the real source, but also a probability distribution over the\nrelation labels. This helps to generate meaningful relational sentences.\nExperimental results show that our proposed data augmentation method\nsignificantly improves the performance of relation extraction compared to\nstate-of-the-art methods", "published": "2019-09-06 19:24:51", "link": "http://arxiv.org/abs/1909.05370v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Alignment for Multimodal Emotion Recognition from Speech", "abstract": "Speech emotion recognition is a challenging problem because human convey\nemotions in subtle and complex ways. For emotion recognition on human speech,\none can either extract emotion related features from audio signals or employ\nspeech recognition techniques to generate text from speech and then apply\nnatural language processing to analyze the sentiment. Further, emotion\nrecognition will be beneficial from using audio-textual multimodal information,\nit is not trivial to build a system to learn from multimodality. One can build\nmodels for two input sources separately and combine them in a decision level,\nbut this method ignores the interaction between speech and text in the temporal\ndomain. In this paper, we propose to use an attention mechanism to learn the\nalignment between speech frames and text words, aiming to produce more accurate\nmultimodal feature representations. The aligned multimodal features are fed\ninto a sequential model for emotion recognition. We evaluate the approach on\nthe IEMOCAP dataset and the experimental results show the proposed approach\nachieves the state-of-the-art performance on the dataset.", "published": "2019-09-06 03:06:38", "link": "http://arxiv.org/abs/1909.05645v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Edge Computing Robot Experience for Automatic Elderly Mental Health\n  Care Based on Voice", "abstract": "We need open platforms driven by specialists, in which queries can be created\nand collected for long periods and the diagnosis made, based on a rigorous\nclinical follow-up. In this work, we developed a multi-language robot interface\nhelping to evaluate the mental health of seniors by interacting through\nquestions. The specialist can propose questions, as well as to receive users'\nanswers, in text form. The robot can automatically interact with the user using\nthe appropriate language. It can process the answers and under the guidance of\na specialist, questions and answers can be oriented towards the desired therapy\ndirection. The prototype, was implemented on an embedded device meant for edge\ncomputing, thus it is able to filter environmental noise and can be placed\nanywhere at home. The experience is now available for specialists to create\nqueries and answers through a Web-based interface.", "published": "2019-09-06 14:16:00", "link": "http://arxiv.org/abs/1909.02924v1", "categories": ["cs.HC", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Neural Network-Based Modeling of Phonetic Durations", "abstract": "A deep neural network (DNN)-based model has been developed to predict\nnon-parametric distributions of durations of phonemes in specified phonetic\ncontexts and used to explore which factors influence durations most. Major\nfactors in US English are pre-pausal lengthening, lexical stress, and speaking\nrate. The model can be used to check that text-to-speech (TTS) training speech\nfollows the script and words are pronounced as expected. Duration prediction is\npoorer with training speech for automatic speech recognition (ASR) because the\ntraining corpus typically consists of single utterances from many speakers and\nis often noisy or casually spoken. Low probability durations in ASR training\nmaterial nevertheless mostly correspond to non-standard speech, with some\nhaving disfluencies. Children's speech is disproportionately present in these\nutterances, since children show much more variation in timing.", "published": "2019-09-06 17:37:48", "link": "http://arxiv.org/abs/1909.03030v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
