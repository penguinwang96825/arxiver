{"title": "A non-hierarchical attention network with modality dropout for textual\n  response generation in multimodal dialogue systems", "abstract": "Existing text- and image-based multimodal dialogue systems use the\ntraditional Hierarchical Recurrent Encoder-Decoder (HRED) framework, which has\nan utterance-level encoder to model utterance representation and a\ncontext-level encoder to model context representation. Although pioneer efforts\nhave shown promising performances, they still suffer from the following\nchallenges: (1) the interaction between textual features and visual features is\nnot fine-grained enough. (2) the context representation can not provide a\ncomplete representation for the context. To address the issues mentioned above,\nwe propose a non-hierarchical attention network with modality dropout, which\nabandons the HRED framework and utilizes attention modules to encode each\nutterance and model the context representation. To evaluate our proposed model,\nwe conduct comprehensive experiments on a public multimodal dialogue dataset.\nAutomatic and human evaluation demonstrate that our proposed model outperforms\nthe existing methods and achieves state-of-the-art performance.", "published": "2021-10-19 03:08:16", "link": "http://arxiv.org/abs/2110.09702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inter-Sense: An Investigation of Sensory Blending in Fiction", "abstract": "This study reports on the semantic organization of English sensory\ndescriptors of the five basic senses of sight, hearing, touch, taste, and smell\nin a large corpus of over 8,000 fiction books. We introduce a large-scale text\ndata-driven approach based on distributional-semantic word embeddings to\nidentify and extract these descriptors as well as analyze their mixing\ninterconnections in the resulting conceptual and sensory space. The findings\nare relevant for research on concept acquisition and representation, as well as\nfor applications that can benefit from a better understanding of perceptual\nspaces of sensory experiences, in fiction, in particular, and in language in\ngeneral.", "published": "2021-10-19 03:25:26", "link": "http://arxiv.org/abs/2110.09710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Sensory Spaces of English Perceptual Verbs in Natural\n  Language Data", "abstract": "In this study, we explore how language captures the meaning of words, in\nparticular meaning related to sensory experiences learned from statistical\ndistributions across texts. We focus on the most frequent perception verbs of\nEnglish analyzed from an and Agentive vs. Experiential distinction across the\nfive basic sensory modalities: Visual (to look vs. to see), Auditory (to listen\nvs. to hear), Tactile (to touch vs. to feel), Olfactory (to smell), and\nGustatory (to taste). In this study we report on a data-driven approach based\non distributional-semantic word embeddings and clustering models to identify\nand uncover the descriptor sensory spaces of the perception verbs. In the\nanalysis, we identified differences and similarities of the generated\ndescriptors based on qualitative and quantitative differences of the perceptual\nexperience they denote. For instance, our results show that while the\nperceptual spaces of the experiential verbs like to see, to hear show a more\ndetached, logical way of knowing and learning, their agentive counterparts (to\nlook, listen) provide a more intentional as well as more intimate and intuitive\nway of discovering and interacting with the world around us. We believe that\nsuch an approach has a high potential to expand our understanding and the\napplicability of such sensory spaces to different fields of social and cultural\nanalysis. Research on the semantic organization of sensory spaces for various\napplications might benefit from an the Agentive/Experiential account to address\nthe complexity of multiple senses wired with each other in still unexplored\nways.", "published": "2021-10-19 03:58:44", "link": "http://arxiv.org/abs/2110.09721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-domain clarification question generation without question examples", "abstract": "An overarching goal of natural language processing is to enable machines to\ncommunicate seamlessly with humans. However, natural language can be ambiguous\nor unclear. In cases of uncertainty, humans engage in an interactive process\nknown as repair: asking questions and seeking clarification until their\nuncertainty is resolved. We propose a framework for building a visually\ngrounded question-asking model capable of producing polar (yes-no)\nclarification questions to resolve misunderstandings in dialogue. Our model\nuses an expected information gain objective to derive informative questions\nfrom an off-the-shelf image captioner without requiring any supervised\nquestion-answer data. We demonstrate our model's ability to pose questions that\nimprove communicative success in a goal-oriented 20 questions game with\nsynthetic and human answerers.", "published": "2021-10-19 07:51:54", "link": "http://arxiv.org/abs/2110.09779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiomatic Expression Identification using Semantic Compatibility", "abstract": "Idiomatic expressions are an integral part of natural language and constantly\nbeing added to a language. Owing to their non-compositionality and their\nability to take on a figurative or literal meaning depending on the sentential\ncontext, they have been a classical challenge for NLP systems. To address this\nchallenge, we study the task of detecting whether a sentence has an idiomatic\nexpression and localizing it. Prior art for this task had studied specific\nclasses of idiomatic expressions offering limited views of their\ngeneralizability to new idioms. We propose a multi-stage neural architecture\nwith the attention flow mechanism for identifying these expressions. The\nnetwork effectively fuses contextual and lexical information at different\nlevels using word and sub-word representations. Empirical evaluations on three\nof the largest benchmark datasets with idiomatic expressions of varied\nsyntactic patterns and degrees of non-compositionality show that our proposed\nmodel achieves new state-of-the-art results. A salient feature of the model is\nits ability to identify idioms unseen during training with gains from 1.4% to\n30.8% over competitive baselines on the largest dataset.", "published": "2021-10-19 15:44:28", "link": "http://arxiv.org/abs/2110.10064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Medication Extraction: A Comparison of Recent Models in\n  Supervised and Semi-supervised Learning Settings", "abstract": "Drug prescriptions are essential information that must be encoded in\nelectronic medical records. However, much of this information is hidden within\nfree-text reports. This is why the medication extraction task has emerged. To\ndate, most of the research effort has focused on small amount of data and has\nonly recently considered deep learning methods. In this paper, we present an\nindependent and comprehensive evaluation of state-of-the-art neural\narchitectures on the I2B2 medical prescription extraction task both in the\nsupervised and semi-supervised settings. The study shows the very competitive\nperformance of simple DNN models on the task as well as the high interest of\npre-trained models. Adapting the latter models on the I2B2 dataset enables to\npush medication extraction performances above the state-of-the-art. Finally,\nthe study also confirms that semi-supervised techniques are promising to\nleverage large amounts of unlabeled data in particular in low resource setting\nwhen labeled data is too costly to acquire.", "published": "2021-10-19 19:23:38", "link": "http://arxiv.org/abs/2110.10213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble ALBERT on SQuAD 2.0", "abstract": "Machine question answering is an essential yet challenging task in natural\nlanguage processing. Recently, Pre-trained Contextual Embeddings (PCE) models\nlike Bidirectional Encoder Representations from Transformers (BERT) and A Lite\nBERT (ALBERT) have attracted lots of attention due to their great performance\nin a wide range of NLP tasks. In our Paper, we utilized the fine-tuned ALBERT\nmodels and implemented combinations of additional layers (e.g. attention layer,\nRNN layer) on top of them to improve model performance on Stanford Question\nAnswering Dataset (SQuAD 2.0). We implemented four different models with\ndifferent layers on top of ALBERT-base model, and two other models based on\nALBERT-xlarge and ALBERT-xxlarge. We compared their performance to our baseline\nmodel ALBERT-base-v2 + ALBERT-SQuAD-out with details. Our best-performing\nindividual model is ALBERT-xxlarge + ALBERT-SQuAD-out, which achieved an F1\nscore of 88.435 on the dev set. Furthermore, we have implemented three\ndifferent ensemble algorithms to boost overall performance. By passing in\nseveral best-performing models' results into our weighted voting ensemble\nalgorithm, our final result ranks first on the Stanford CS224N Test PCE SQuAD\nLeaderboard with F1 = 90.123.", "published": "2021-10-19 00:15:19", "link": "http://arxiv.org/abs/2110.09665v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Importance Estimation from Multiple Perspectives for Keyphrase\n  Extraction", "abstract": "Keyphrase extraction is a fundamental task in Natural Language Processing,\nwhich usually contains two main parts: candidate keyphrase extraction and\nkeyphrase importance estimation. From the view of human understanding\ndocuments, we typically measure the importance of phrase according to its\nsyntactic accuracy, information saliency, and concept consistency\nsimultaneously. However, most existing keyphrase extraction approaches only\nfocus on the part of them, which leads to biased results. In this paper, we\npropose a new approach to estimate the importance of keyphrase from multiple\nperspectives (called as \\textit{KIEMP}) and further improve the performance of\nkeyphrase extraction. Specifically, \\textit{KIEMP} estimates the importance of\nphrase with three modules: a chunking module to measure its syntactic accuracy,\na ranking module to check its information saliency, and a matching module to\njudge the concept (i.e., topic) consistency between phrase and the whole\ndocument. These three modules are seamlessly jointed together via an end-to-end\nmulti-task learning model, which is helpful for three parts to enhance each\nother and balance the effects of three perspectives. Experimental results on\nsix benchmark datasets show that \\textit{KIEMP} outperforms the existing\nstate-of-the-art keyphrase extraction approaches in most cases.", "published": "2021-10-19 05:48:22", "link": "http://arxiv.org/abs/2110.09749v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AequeVox: Automated Fairness Testing of Speech Recognition Systems", "abstract": "Automatic Speech Recognition (ASR) systems have become ubiquitous. They can\nbe found in a variety of form factors and are increasingly important in our\ndaily lives. As such, ensuring that these systems are equitable to different\nsubgroups of the population is crucial. In this paper, we introduce, AequeVox,\nan automated testing framework for evaluating the fairness of ASR systems.\nAequeVox simulates different environments to assess the effectiveness of ASR\nsystems for different populations. In addition, we investigate whether the\nchosen simulations are comprehensible to humans. We further propose a fault\nlocalization technique capable of identifying words that are not robust to\nthese varying environments. Both components of AequeVox are able to operate in\nthe absence of ground truth data.\n  We evaluated AequeVox on speech from four different datasets using three\ndifferent commercial ASRs. Our experiments reveal that non-native English,\nfemale and Nigerian English speakers generate 109%, 528.5% and 156.9% more\nerrors, on average than native English, male and UK Midlands speakers,\nrespectively. Our user study also reveals that 82.9% of the simulations\n(employed through speech transformations) had a comprehensibility rating above\nseven (out of ten), with the lowest rating being 6.78. This further validates\nthe fairness violations discovered by AequeVox. Finally, we show that the\nnon-robust words, as predicted by the fault localization technique embodied in\nAequeVox, show 223.8% more errors than the predicted robust words across all\nASRs.", "published": "2021-10-19 10:56:46", "link": "http://arxiv.org/abs/2110.09843v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DEEPAG\u00c9: Answering Questions in Portuguese about the Brazilian\n  Environment", "abstract": "The challenge of climate change and biome conservation is one of the most\npressing issues of our time - particularly in Brazil, where key environmental\nreserves are located. Given the availability of large textual databases on\necological themes, it is natural to resort to question answering (QA) systems\nto increase social awareness and understanding about these topics. In this\nwork, we introduce multiple QA systems that combine in novel ways the BM25\nalgorithm, a sparse retrieval technique, with PTT5, a pre-trained\nstate-of-the-art language model. Our QA systems focus on the Portuguese\nlanguage, thus offering resources not found elsewhere in the literature. As\ntraining data, we collected questions from open-domain datasets, as well as\ncontent from the Portuguese Wikipedia and news from the press. We thus\ncontribute with innovative architectures and novel applications, attaining an\nF1-score of 36.2 with our best model.", "published": "2021-10-19 14:35:29", "link": "http://arxiv.org/abs/2110.10015v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Smart Healthcare", "abstract": "Smart healthcare has achieved significant progress in recent years. Emerging\nartificial intelligence (AI) technologies enable various smart applications\nacross various healthcare scenarios. As an essential technology powered by AI,\nnatural language processing (NLP) plays a key role in smart healthcare due to\nits capability of analysing and understanding human language. In this work, we\nreview existing studies that concern NLP for smart healthcare from the\nperspectives of technique and application. We first elaborate on different NLP\napproaches and the NLP pipeline for smart healthcare from the technical point\nof view. Then, in the context of smart healthcare applications employing NLP\ntechniques, we introduce representative smart healthcare scenarios, including\nclinical practice, hospital management, personal care, public health, and drug\ndevelopment. We further discuss two specific medical issues, i.e., the\ncoronavirus disease 2019 (COVID-19) pandemic and mental health, in which\nNLP-driven smart healthcare plays an important role. Finally, we discuss the\nlimitations of current works and identify the directions for future works.", "published": "2021-10-19 02:48:44", "link": "http://arxiv.org/abs/2110.15803v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretive Blindness", "abstract": "We model here an epistemic bias we call \\textit{interpretive blindness} (IB).\nIB is a special problem for learning from testimony, in which one acquires\ninformation only from text or conversation. We show that IB follows from a\nco-dependence between background beliefs and interpretation in a Bayesian\nsetting and the nature of contemporary testimony. We argue that a particular\ncharacteristic contemporary testimony, \\textit{argumentative completeness}, can\npreclude learning in hierarchical Bayesian settings, even in the presence of\nconstraints that are designed to promote good epistemic practices.", "published": "2021-10-19 13:25:22", "link": "http://arxiv.org/abs/2111.00867v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by\n  Leveraging External Textual Knowledge", "abstract": "End-to-end TTS requires a large amount of speech/text paired data to cover\nall necessary knowledge, particularly how to pronounce different words in\ndiverse contexts, so that a neural model may learn such knowledge accordingly.\nBut in real applications, such high demand of training data is hard to be\nsatisfied and additional knowledge often needs to be injected manually. For\nexample, to capture pronunciation knowledge on languages without regular\northography, a complicated grapheme-to-phoneme pipeline needs to be built based\non a large structured pronunciation lexicon, leading to extra, sometimes high,\ncosts to extend neural TTS to such languages. In this paper, we propose a\nframework to learn to automatically extract knowledge from unstructured\nexternal resources using a novel Token2Knowledge attention module. The\nframework is applied to build a TTS model named Neural Lexicon Reader that\nextracts pronunciations from raw lexicon texts in an end-to-end manner.\nExperiments show the proposed model significantly reduces pronunciation errors\nin low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can\nbe transferred to other languages with a smaller amount of data.", "published": "2021-10-19 02:35:10", "link": "http://arxiv.org/abs/2110.09698v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Trajectory Prediction with Linguistic Representations", "abstract": "Language allows humans to build mental models that interpret what is\nhappening around them resulting in more accurate long-term predictions. We\npresent a novel trajectory prediction model that uses linguistic intermediate\nrepresentations to forecast trajectories, and is trained using trajectory\nsamples with partially-annotated captions. The model learns the meaning of each\nof the words without direct per-word supervision. At inference time, it\ngenerates a linguistic description of trajectories which captures maneuvers and\ninteractions over an extended time interval. This generated description is used\nto refine predictions of the trajectories of multiple agents. We train and\nvalidate our model on the Argoverse dataset, and demonstrate improved accuracy\nresults in trajectory prediction. In addition, our model is more interpretable:\nit presents part of its reasoning in plain language as captions, which can aid\nmodel development and can aid in building confidence in the model before\ndeploying it.", "published": "2021-10-19 05:22:38", "link": "http://arxiv.org/abs/2110.09741v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Unifying Multimodal Transformer for Bi-directional Image and Text\n  Generation", "abstract": "We study the joint learning of image-to-text and text-to-image generations,\nwhich are naturally bi-directional tasks. Typical existing works design two\nseparate task-specific models for each task, which impose expensive design\nefforts. In this work, we propose a unified image-and-text generative framework\nbased on a single multimodal model to jointly study the bi-directional tasks.\nWe adopt Transformer as our unified architecture for its strong performance and\ntask-agnostic design. Specifically, we formulate both tasks as sequence\ngeneration tasks, where we represent images and text as unified sequences of\ntokens, and the Transformer learns multimodal interactions to generate\nsequences. We further propose two-level granularity feature representations and\nsequence-level training to improve the Transformer-based unified framework.\nExperiments show that our approach significantly improves previous\nTransformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for\ntext-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for\nfine-tuned image-to-text generation on the MS-COCO dataset. Our code is\navailable online.", "published": "2021-10-19 06:01:24", "link": "http://arxiv.org/abs/2110.09753v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Picture is Worth a Thousand Words: A Unified System for Diverse\n  Captions and Rich Images Generation", "abstract": "A creative image-and-text generative AI system mimics humans' extraordinary\nabilities to provide users with diverse and comprehensive caption suggestions,\nas well as rich image creations. In this work, we demonstrate such an AI\ncreation system to produce both diverse captions and rich images. When users\nimagine an image and associate it with multiple captions, our system paints a\nrich image to reflect all captions faithfully. Likewise, when users upload an\nimage, our system depicts it with multiple diverse captions. We propose a\nunified multi-modal framework to achieve this goal. Specifically, our framework\njointly models image-and-text representations with a Transformer network, which\nsupports rich image creation by accepting multiple captions as input. We\nconsider the relations among input captions to encourage diversity in training\nand adopt a non-autoregressive decoding strategy to enable real-time inference.\nBased on these, our system supports both diverse captions and rich images\ngenerations. Our code is available online.", "published": "2021-10-19 06:10:42", "link": "http://arxiv.org/abs/2110.09756v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Two-stage Voice Application Recommender System for Unhandled Utterances\n  in Intelligent Personal Assistant", "abstract": "Intelligent personal assistants (IPA) enable voice applications that\nfacilitate people's daily tasks. However, due to the complexity and ambiguity\nof voice requests, some requests may not be handled properly by the standard\nnatural language understanding (NLU) component. In such cases, a simple reply\nlike \"Sorry, I don't know\" hurts the user's experience and limits the\nfunctionality of IPA. In this paper, we propose a two-stage\nshortlister-reranker recommender system to match third-party voice applications\n(skills) to unhandled utterances. In this approach, a skill shortlister is\nproposed to retrieve candidate skills from the skill catalog by calculating\nboth lexical and semantic similarity between skills and user requests. We also\nillustrate how to build a new system by using observed data collected from a\nbaseline rule-based system, and how the exposure biases can generate\ndiscrepancy between offline and human metrics. Lastly, we present two\nrelabeling methods that can handle the incomplete ground truth, and mitigate\nexposure bias. We demonstrate the effectiveness of our proposed system through\nextensive offline experiments. Furthermore, we present online A/B testing\nresults that show a significant boost on user experience satisfaction.", "published": "2021-10-19 11:52:56", "link": "http://arxiv.org/abs/2110.09877v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Entity Relation Extraction as Dependency Parsing in Visually Rich\n  Documents", "abstract": "Previous works on key information extraction from visually rich documents\n(VRDs) mainly focus on labeling the text within each bounding box (i.e.,\nsemantic entity), while the relations in-between are largely unexplored. In\nthis paper, we adapt the popular dependency parsing model, the biaffine parser,\nto this entity relation extraction task. Being different from the original\ndependency parsing model which recognizes dependency relations between words,\nwe identify relations between groups of words with layout information instead.\nWe have compared different representations of the semantic entity, different\nVRD encoders, and different relation decoders. The results demonstrate that our\nproposed model achieves 65.96% F1 score on the FUNSD dataset. As for the\nreal-world application, our model has been applied to the in-house customs\ndata, achieving reliable performance in the production setting.", "published": "2021-10-19 12:26:40", "link": "http://arxiv.org/abs/2110.09915v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation", "abstract": "Table2Text systems generate textual output based on structured data utilizing\nmachine learning. These systems are essential for fluent natural language\ninterfaces in tools such as virtual assistants; however, left to generate\nfreely these ML systems often produce misleading or unexpected outputs. GenNI\n(Generation Negotiation Interface) is an interactive visual system for\nhigh-level human-AI collaboration in producing descriptive text. The tool\nutilizes a deep learning model designed with explicit control states. These\ncontrols allow users to globally constrain model generations, without\nsacrificing the representation power of the deep learning models. The visual\ninterface makes it possible for users to interact with AI systems following a\nRefine-Forecast paradigm to ensure that the generation system acts in a manner\nhuman users find suitable. We report multiple use cases on two experiments that\nimprove over uncontrolled generation approaches, while at the same time\nproviding fine-grained control. A demo and source code are available at\nhttps://genni.vizhub.ai .", "published": "2021-10-19 18:07:07", "link": "http://arxiv.org/abs/2110.10185v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; H.5.2"], "primary_category": "cs.CL"}
{"title": "Social Media Reveals Urban-Rural Differences in Stress across China", "abstract": "Modeling differential stress expressions in urban and rural regions in China\ncan provide a better understanding of the effects of urbanization on\npsychological well-being in a country that has rapidly grown economically in\nthe last two decades. This paper studies linguistic differences in the\nexperiences and expressions of stress in urban-rural China from Weibo posts\nfrom over 65,000 users across 329 counties using hierarchical mixed-effects\nmodels. We analyzed phrases, topical themes, and psycho-linguistic word choices\nin Weibo posts mentioning stress to better understand appraisal differences\nsurrounding psychological stress in urban and rural communities in China; we\nthen compared them with large-scale polls from Gallup. After controlling for\nsocioeconomic and gender differences, we found that rural communities tend to\nexpress stress in emotional and personal themes such as relationships, health,\nand opportunity while users in urban areas express stress using relative,\ntemporal, and external themes such as work, politics, and economics. These\ndifferences exist beyond controlling for GDP and urbanization, indicating a\nfundamentally different lifestyle between rural and urban residents in very\nspecific environments, arguably having different sources of stress. We found\ncorroborative trends in physical, financial, and social wellness with\nurbanization in Gallup polls.", "published": "2021-10-19 21:17:30", "link": "http://arxiv.org/abs/2110.15726v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Application of the Multi-label Residual Convolutional Neural Network\n  text classifier using Content-Based Routing process", "abstract": "In this article, we will present an NLP application in text classifying\nprocess using the content-based router. The ultimate goal throughout this\narticle is to predict the event described by a legal ad from the plain text of\nthe ad. This problem is purely a supervised problem that will involve the use\nof NLP techniques and conventional modeling methodologies through the use of\nthe Multi-label Residual Convolutional Neural Network for text classification.\nWe will explain the approach put in place to solve the problem of classified\nads, the difficulties encountered and the experimental results.", "published": "2021-10-19 19:10:34", "link": "http://arxiv.org/abs/2110.15801v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Pattern based Black-box Model Watermarking for Automatic Speech\n  Recognition", "abstract": "As an effective method for intellectual property (IP) protection, model\nwatermarking technology has been applied on a wide variety of deep neural\nnetworks (DNN), including speech classification models. However, how to design\na black-box watermarking scheme for automatic speech recognition (ASR) models\nis still an unsolved problem, which is a significant demand for protecting\nremote ASR Application Programming Interface (API) deployed in cloud servers.\nDue to conditional independence assumption and label-detection-based evasion\nattack risk of ASR models, the black-box model watermarking scheme for speech\nclassification models cannot apply to ASR models. In this paper, we propose the\nfirst black-box model watermarking framework for protecting the IP of ASR\nmodels. Specifically, we synthesize trigger audios by spreading the speech\nclips of model owners over the entire input audios and labeling the trigger\naudios with the stego texts, which hides the authorship information with\nlinguistic steganography. Experiments on the state-of-the-art open-source ASR\nsystem DeepSpeech demonstrate the feasibility of the proposed watermarking\nscheme, which is robust against five kinds of attacks and has little impact on\naccuracy.", "published": "2021-10-19 09:01:41", "link": "http://arxiv.org/abs/2110.09814v2", "categories": ["cs.SD", "cs.CL", "cs.CR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "StructFormer: Learning Spatial Structure for Language-Guided Semantic\n  Rearrangement of Novel Objects", "abstract": "Geometric organization of objects into semantically meaningful arrangements\npervades the built world. As such, assistive robots operating in warehouses,\noffices, and homes would greatly benefit from the ability to recognize and\nrearrange objects into these semantically meaningful structures. To be useful,\nthese robots must contend with previously unseen objects and receive\ninstructions without significant programming. While previous works have\nexamined recognizing pairwise semantic relations and sequential manipulation to\nchange these simple relations none have shown the ability to arrange objects\ninto complex structures such as circles or table settings. To address this\nproblem we propose a novel transformer-based neural network, StructFormer,\nwhich takes as input a partial-view point cloud of the current object\narrangement and a structured language command encoding the desired object\nconfiguration. We show through rigorous experiments that StructFormer enables a\nphysical robot to rearrange novel objects into semantically meaningful\nstructures with multi-object relational constraints inferred from the language\ncommand.", "published": "2021-10-19 18:13:01", "link": "http://arxiv.org/abs/2110.10189v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "As long as you talk about me: The importance of family firm brands and\n  the contingent role of family-firm identity", "abstract": "This study explores the role of external audiences in determining the\nimportance of family firm brands and the relationship with firm performance.\nDrawing on text mining and social network analysis techniques, and considering\nthe brand prevalence, diversity, and connectivity dimensions, we use the\nsemantic brand score to measure the importance the media give to family firm\nbrands. The analysis of a sample of 52,555 news articles published in 2017\nabout 63 Italian entrepreneurial families reveals that brand importance is\npositively associated with family firm revenues, and this relationship is\nstronger when there is identity match between the family and the firm. This\nstudy advances current literature by offering a rich and multifaceted\nperspective on how external audiences perceptions of the brand shape family\nfirm performance.", "published": "2021-10-19 10:34:40", "link": "http://arxiv.org/abs/2110.13815v1", "categories": ["econ.GN", "cs.CL", "cs.SI", "physics.soc-ph", "q-fin.EC", "I.2.7; H.0; J.4"], "primary_category": "econ.GN"}
{"title": "Rep Works in Speaker Verification", "abstract": "Multi-branch convolutional neural network architecture has raised lots of\nattention in speaker verification since the aggregation of multiple parallel\nbranches can significantly improve performance. However, this design is not\nefficient enough during the inference time due to the increase of model\nparameters and extra operations. In this paper, we present a new multi-branch\nnetwork architecture RepSPKNet that uses a re-parameterization technique. With\nthis technique, our backbone model contains an efficient VGG-like inference\nstate while its training state is a complicated multi-branch structure. We\nfirst introduce the specific structure of RepVGG into speaker verification and\npropose several variants of this structure. The performance is evaluated on\nVoxCeleb-based test sets. We demonstrate that both the branch diversity and the\nbranch capacity play important roles in RepSPKNet designing. Our RepSPKNet\nachieves state-of-the-art performance with a 1.5982% EER and a 0.1374 minDCF on\nVoxCeleb1-H.", "published": "2021-10-19 03:47:48", "link": "http://arxiv.org/abs/2110.09720v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and\n  Text Encoder Aggregation", "abstract": "Learning emotion embedding from reference audio is a straightforward approach\nfor multi-emotion speech synthesis in encoder-decoder systems. But how to get\nbetter emotion embedding and how to inject it into TTS acoustic model more\neffectively are still under investigation. In this paper, we propose an\ninnovative constraint to help VAE extract emotion embedding with better cluster\ncohesion. Besides, the obtained emotion embedding is used as query to aggregate\nlatent representations of all encoder layers via attention. Moreover, the\nqueries from encoder layers themselves are also helpful. Experiments prove the\nproposed methods can enhance the encoding of comprehensive syntactic and\nsemantic information and produce more expressive emotional speech.", "published": "2021-10-19 07:53:52", "link": "http://arxiv.org/abs/2110.09780v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement-assisted Voice Conversion in Noisy Environments", "abstract": "Numerous voice conversion (VC) techniques have been proposed for the\nconversion of voices among different speakers. Although good quality of the\nconverted speech can be observed when VC is applied in a clean environment, the\nquality degrades drastically when the system is run in noisy conditions. In\norder to address this issue, we propose a novel speech enhancement\n(SE)-assisted VC system that utilizes the SE techniques for signal\npre-processing, where the VC and SE components are optimized in an joint\ntraining strategy with the aim to provide high-quality converted speech\nsignals. We adopt a popular model, StarGAN, as the VC component and thus call\nthe combined system as EStarGAN. We test the proposed EStarGAN system using a\nMandarin speech corpus. The experimental results first verified the\neffectiveness of joint training strategy used in EStarGAN. Moreover, EStarGAN\ndemonstrated performance robustness in various unseen noisy environments. The\nsubjective listening test results further showed that EStarGAN can improve the\nsound quality of speech signals converted from noise-corrupted source\nutterances.", "published": "2021-10-19 12:38:18", "link": "http://arxiv.org/abs/2110.09923v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement Based on Cyclegan with Noise-informed Training", "abstract": "Cycle-consistent generative adversarial networks (CycleGAN) were successfully\napplied to speech enhancement (SE) tasks with unpaired noisy-clean training\ndata. The CycleGAN SE system adopted two generators and two discriminators\ntrained with losses from noisy-to-clean and clean-to-noisy conversions.\nCycleGAN showed promising results for numerous SE tasks. Herein, we investigate\na potential limitation of the clean-to-noisy conversion part and propose a\nnovel noise-informed training (NIT) approach to improve the performance of the\noriginal CycleGAN SE system. The main idea of the NIT approach is to\nincorporate target domain information for clean-to-noisy conversion to\nfacilitate a better training procedure. The experimental results confirmed that\nthe proposed NIT approach improved the generalization capability of the\noriginal CycleGAN SE system with a notable margin.", "published": "2021-10-19 12:38:25", "link": "http://arxiv.org/abs/2110.09924v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Cocktail Fork Problem: Three-Stem Audio Separation for Real-World\n  Soundtracks", "abstract": "The cocktail party problem aims at isolating any source of interest within a\ncomplex acoustic scene, and has long inspired audio source separation research.\nRecent efforts have mainly focused on separating speech from noise, speech from\nspeech, musical instruments from each other, or sound events from each other.\nHowever, separating an audio mixture (e.g., movie soundtrack) into the three\nbroad categories of speech, music, and sound effects (understood to include\nambient noise and natural sound events) has been left largely unexplored,\ndespite a wide range of potential applications. This paper formalizes this task\nas the cocktail fork problem, and presents the Divide and Remaster (DnR)\ndataset to foster research on this topic. DnR is built from three\nwell-established audio datasets (LibriSpeech, FMA, FSD50k), taking care to\nreproduce conditions similar to professionally produced content in terms of\nsource overlap and relative loudness, and made available at CD quality. We\nbenchmark standard source separation algorithms on DnR, and further introduce a\nnew multi-resolution model to better address the variety of acoustic\ncharacteristics of the three source types. Our best model produces SI-SDR\nimprovements over the mixture of 11.0 dB for music, 11.2 dB for speech, and\n10.8 dB for sound effects.", "published": "2021-10-19 13:18:13", "link": "http://arxiv.org/abs/2110.09958v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Temporal separation of whale vocalizations from background oceanic noise\n  using a power calculation", "abstract": "The process of analyzing audio signals in search of cetacean vocalizations is\nin many cases a very arduous task, requiring many complex computations, a\nplethora of digital processing techniques and the scrutinization of an audio\nsignal with a fine comb to determine where the vocalizations are located. To\nease this process, a computationally efficient and noise-resistant method for\ndetermining whether an audio segment contains a potential cetacean call is\ndeveloped here with the help of a robust power calculation for stationary\nGaussian noise signals and a recursive method for determining the mean and\nvariance of a given sample frame. The resulting detector is tested on audio\nrecordings containing southern right whale sounds and its performance is\ncompared to a contemporary energy detector and a popular deep learning method.\nThe detector exhibits good performance at moderate-to-high signal-to-noise\nratio values. The detector succeeds in being easy to implement, computationally\nefficient to use and robust enough to accurately detect whale vocalizations in\na noisy underwater environment.", "published": "2021-10-19 14:22:26", "link": "http://arxiv.org/abs/2110.10010v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis", "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms\ngiven conditioning such as text, mel-spectrograms, or MIDI. These systems\nemploy deep generative models that model the waveform via either sequential\n(autoregressive) or parallel (non-autoregressive) sampling. Generative\nadversarial networks (GANs) have become a common choice for non-autoregressive\nwaveform synthesis. However, state-of-the-art GAN-based models produce\nartifacts when performing mel-spectrogram inversion. In this paper, we\ndemonstrate that these artifacts correspond with an inability for the generator\nto learn accurate pitch and periodicity. We show that simple pitch and\nperiodicity conditioning is insufficient for reducing this error relative to\nusing autoregression. We discuss the inductive bias that autoregression\nprovides for learning the relationship between instantaneous frequency and\nphase, and show that this inductive bias holds even when autoregressively\nsampling large chunks of the waveform during each forward pass. Relative to\nprior state-of-the-art GAN-based models, our proposed model, Chunked\nAutoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training\ntime by 58%, maintains a fast generation speed suitable for real-time or\ninteractive applications, and maintains or improves subjective quality.", "published": "2021-10-19 17:48:12", "link": "http://arxiv.org/abs/2110.10139v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SSAST: Self-Supervised Audio Spectrogram Transformer", "abstract": "Recently, neural networks based purely on self-attention, such as the Vision\nTransformer (ViT), have been shown to outperform deep learning models\nconstructed with convolutional neural networks (CNNs) on various vision tasks,\nthus extending the success of Transformers, which were originally developed for\nlanguage processing, to the vision domain. A recent study showed that a similar\nmethodology can also be applied to the audio domain. Specifically, the Audio\nSpectrogram Transformer (AST) achieves state-of-the-art results on various\naudio classification benchmarks. However, pure Transformer models tend to\nrequire more training data compared to CNNs, and the success of the AST relies\non supervised pretraining that requires a large amount of labeled data and a\ncomplex training pipeline, thus limiting the practical usage of AST.\n  This paper focuses on audio and speech classification, and aims to reduce the\nneed for large amounts of labeled data for AST by leveraging self-supervised\nlearning using unlabeled data. Specifically, we propose to pretrain the AST\nmodel with joint discriminative and generative masked spectrogram patch\nmodeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We\nevaluate our pretrained models on both audio and speech classification tasks\nincluding audio event classification, keyword spotting, emotion recognition,\nand speaker identification. The proposed self-supervised framework\nsignificantly boosts AST performance on all tasks, with an average improvement\nof 60.9%, leading to similar or even better results than a supervised\npretrained AST. To the best of our knowledge, it is the first patch-based\nself-supervised learning framework in the audio and speech domain, and also the\nfirst self-supervised learning framework for AST.", "published": "2021-10-19 07:58:28", "link": "http://arxiv.org/abs/2110.09784v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continual self-training with bootstrapped remixing for speech\n  enhancement", "abstract": "We propose RemixIT, a simple and novel self-supervised training method for\nspeech enhancement. The proposed method is based on a continuously\nself-training scheme that overcomes limitations from previous studies including\nassumptions for the in-domain noise distribution and having access to clean\ntarget signals. Specifically, a separation teacher model is pre-trained on an\nout-of-domain dataset and is used to infer estimated target signals for a batch\nof in-domain mixtures. Next, we bootstrap the mixing process by generating\nartificial mixtures using permuted estimated clean and noise signals. Finally,\nthe student model is trained using the permuted estimated sources as targets\nwhile we periodically update teacher's weights using the latest student model.\nOur experiments show that RemixIT outperforms several previous state-of-the-art\nself-supervised methods under multiple speech enhancement tasks. Additionally,\nRemixIT provides a seamless alternative for semi-supervised and unsupervised\ndomain adaptation for speech enhancement tasks, while being general enough to\nbe applied to any separation task and paired with any separation model.", "published": "2021-10-19 16:56:18", "link": "http://arxiv.org/abs/2110.10103v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A mathematical model of the vowel space", "abstract": "The articulatory-acoustic relationship is many-to-one and non linear and this\nis a great limitation for studying speech production. A simplification is\nproposed to set a bijection between the vowel space (f1, f2) and the parametric\nspace of different vocal tract models. The generic area function model is based\non mixtures of cosines allowing the generation of main vowels with two\nformulas. Then the mixture function is transformed into a coordination function\nable to deal with articulatory parameters. This is shown that the coordination\nfunction acts similarly with the Fant's model and with the 4-Tube DRM derived\nfrom the generic model.", "published": "2021-10-19 07:59:23", "link": "http://arxiv.org/abs/2111.00868v2", "categories": ["cs.SD", "eess.AS", "physics.class-ph", "q-bio.PE"], "primary_category": "cs.SD"}
