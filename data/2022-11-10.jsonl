{"title": "FormLM: Recommending Creation Ideas for Online Forms by Modelling\n  Semantic and Structural Information", "abstract": "Online forms are widely used to collect data from human and have a\nmulti-billion market. Many software products provide online services for\ncreating semi-structured forms where questions and descriptions are organized\nby pre-defined structures. However, the design and creation process of forms is\nstill tedious and requires expert knowledge. To assist form designers, in this\nwork we present FormLM to model online forms (by enhancing pre-trained language\nmodel with form structural information) and recommend form creation ideas\n(including question / options recommendations and block type suggestion). For\nmodel training and evaluation, we collect the first public online form dataset\nwith 62K online forms. Experiment results show that FormLM significantly\noutperforms general-purpose language models on all tasks, with an improvement\nby 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms\nof ROUGE-1 and Macro-F1, respectively.", "published": "2022-11-10 01:32:55", "link": "http://arxiv.org/abs/2211.05284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not Just Plain Text! Fuel Document-Level Relation Extraction with\n  Explicit Syntax Refinement and Subsentence Modeling", "abstract": "Document-level relation extraction (DocRE) aims to identify semantic labels\namong entities within a single document. One major challenge of DocRE is to dig\ndecisive details regarding a specific entity pair from long text. However, in\nmany cases, only a fraction of text carries required information, even in the\nmanually labeled supporting evidence. To better capture and exploit instructive\ninformation, we propose a novel expLicit syntAx Refinement and Subsentence\nmOdeliNg based framework (LARSON). By introducing extra syntactic information,\nLARSON can model subsentences of arbitrary granularity and efficiently screen\ninstructive ones. Moreover, we incorporate refined syntax into text\nrepresentations which further improves the performance of LARSON. Experimental\nresults on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that\nLARSON significantly outperforms existing methods.", "published": "2022-11-10 05:06:37", "link": "http://arxiv.org/abs/2211.05343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSDT: Masked Language Model Scoring Defense in Text Domain", "abstract": "Pre-trained language models allowed us to process downstream tasks with the\nhelp of fine-tuning, which aids the model to achieve fairly high accuracy in\nvarious Natural Language Processing (NLP) tasks. Such easily-downloaded\nlanguage models from various websites empowered the public users as well as\nsome major institutions to give a momentum to their real-life application.\nHowever, it was recently proven that models become extremely vulnerable when\nthey are backdoor attacked with trigger-inserted poisoned datasets by malicious\nusers. The attackers then redistribute the victim models to the public to\nattract other users to use them, where the models tend to misclassify when\ncertain triggers are detected within the training sample. In this paper, we\nwill introduce a novel improved textual backdoor defense method, named MSDT,\nthat outperforms the current existing defensive algorithms in specific\ndatasets. The experimental results illustrate that our method can be effective\nand constructive in terms of defending against backdoor attack in text domain.\nCode is available at https://github.com/jcroh0508/MSDT.", "published": "2022-11-10 06:46:47", "link": "http://arxiv.org/abs/2211.05371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models", "abstract": "This paper investigates models of event implications. Specifically, how well\nmodels predict entity state-changes, by targeting their understanding of\nphysical attributes. Nominally, Large Language models (LLM) have been exposed\nto procedural knowledge about how objects interact, yet our benchmarking shows\nthey fail to reason about the world. Conversely, we also demonstrate that\nexisting approaches often misrepresent the surprising abilities of LLMs via\nimproper task encodings and that proper model prompting can dramatically\nimprove performance of reported baseline results across multiple tasks. In\nparticular, our results indicate that our prompting technique is especially\nuseful for unseen attributes (out-of-domain) or when only limited data is\navailable.", "published": "2022-11-10 07:48:01", "link": "http://arxiv.org/abs/2211.05392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ADEPT: A DEbiasing PrompT Framework", "abstract": "Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects.", "published": "2022-11-10 08:41:40", "link": "http://arxiv.org/abs/2211.05414v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Human-Centred Explainability Benchmarks For Text Classification", "abstract": "Progress on many Natural Language Processing (NLP) tasks, such as text\nclassification, is driven by objective, reproducible and scalable evaluation\nvia publicly available benchmarks. However, these are not always representative\nof real-world scenarios where text classifiers are employed, such as sentiment\nanalysis or misinformation detection. In this position paper, we put forward\ntwo points that aim to alleviate this problem. First, we propose to extend text\nclassification benchmarks to evaluate the explainability of text classifiers.\nWe review challenges associated with objectively evaluating the capabilities to\nproduce valid explanations which leads us to the second main point: We propose\nto ground these benchmarks in human-centred applications, for example by using\nsocial media, gamification or to learn explainability metrics from human\njudgements.", "published": "2022-11-10 09:52:31", "link": "http://arxiv.org/abs/2211.05452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Text Classification Data and Models Using Aggregated Input\n  Salience", "abstract": "Realizing when a model is right for a wrong reason is not trivial and\nrequires a significant effort by model developers. In some cases an input\nsalience method, which highlights the most important parts of the input, may\nreveal problematic reasoning. But scrutinizing highlights over many data\ninstances is tedious and often infeasible. Furthermore, analyzing examples in\nisolation does not reveal general patterns in the data or in the model's\nbehavior. In this paper we aim to address these issues and go from\nunderstanding single examples to understanding entire datasets and models. The\nmethodology we propose is based on aggregated salience maps, to which we apply\nclustering, nearest neighbor search and visualizations. Using this methodology\nwe address multiple distinct but common model developer needs by showing how\nproblematic data and model behavior can be identified and explained -- a\nnecessary first step for improving the model.", "published": "2022-11-10 11:00:57", "link": "http://arxiv.org/abs/2211.05485v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue\n  State Tracking", "abstract": "Dialogue state tracking (DST) aims to convert the dialogue history into\ndialogue states which consist of slot-value pairs. As condensed structural\ninformation memorizing all history information, the dialogue state in the last\nturn is typically adopted as the input for predicting the current state by DST\nmodels. However, these models tend to keep the predicted slot values unchanged,\nwhich is defined as state momentum in this paper. Specifically, the models\nstruggle to update slot values that need to be changed and correct wrongly\npredicted slot values in the last turn. To this end, we propose MoNET to tackle\nstate momentum via noise-enhanced training. First, the previous state of each\nturn in the training data is noised via replacing some of its slot values.\nThen, the noised previous state is used as the input to learn to predict the\ncurrent state, improving the model's ability to update and correct slot values.\nFurthermore, a contrastive context matching framework is designed to narrow the\nrepresentation distance between a state and its corresponding noised variant,\nwhich reduces the impact of noised state and makes the model better understand\nthe dialogue history. Experimental results on MultiWOZ datasets show that MoNET\noutperforms previous DST methods. Ablations and analysis verify the\neffectiveness of MoNET in alleviating state momentum and improving anti-noise\nability.", "published": "2022-11-10 11:55:25", "link": "http://arxiv.org/abs/2211.05503v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Inclusive Notion of Text", "abstract": "Natural language processing (NLP) researchers develop models of grammar,\nmeaning and communication based on written text. Due to task and data\ndifferences, what is considered text can vary substantially across studies. A\nconceptual framework for systematically capturing these differences is lacking.\nWe argue that clarity on the notion of text is crucial for reproducible and\ngeneralizable NLP. Towards that goal, we propose common terminology to discuss\nthe production and transformation of textual data, and introduce a two-tier\ntaxonomy of linguistic and non-linguistic elements that are available in\ntextual sources and can be used in NLP modeling. We apply this taxonomy to\nsurvey existing work that extends the notion of text beyond the conservative\nlanguage-centered view. We outline key desiderata and challenges of the\nemerging inclusive approach to text in NLP, and suggest community-level\nreporting as a crucial next step to consolidate the discussion.", "published": "2022-11-10 14:26:43", "link": "http://arxiv.org/abs/2211.05604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT on a Data Diet: Finding Important Examples by Gradient-Based\n  Pruning", "abstract": "Current pre-trained language models rely on large datasets for achieving\nstate-of-the-art performance. However, past research has shown that not all\nexamples in a dataset are equally important during training. In fact, it is\nsometimes possible to prune a considerable fraction of the training set while\nmaintaining the test performance. Established on standard vision benchmarks,\ntwo gradient-based scoring metrics for finding important examples are GraNd and\nits estimated version, EL2N. In this work, we employ these two metrics for the\nfirst time in NLP. We demonstrate that these metrics need to be computed after\nat least one epoch of fine-tuning and they are not reliable in early steps.\nFurthermore, we show that by pruning a small portion of the examples with the\nhighest GraNd/EL2N scores, we can not only preserve the test accuracy, but also\nsurpass it. This paper details adjustments and implementation choices which\nenable GraNd and EL2N to be applied to NLP.", "published": "2022-11-10 14:37:23", "link": "http://arxiv.org/abs/2211.05610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple\n  Analysis", "abstract": "The rapid development of aspect-based sentiment analysis (ABSA) within recent\ndecades shows great potential for real-world society. The current ABSA works,\nhowever, are mostly limited to the scenario of a single text piece, leaving the\nstudy in dialogue contexts unexplored. To bridge the gap between fine-grained\nsentiment analysis and conversational opinion mining, in this work, we\nintroduce a novel task of conversational aspect-based sentiment quadruple\nanalysis, namely DiaASQ, aiming to detect the quadruple of\ntarget-aspect-opinion-sentiment in a dialogue. We manually construct a\nlarge-scale high-quality DiaASQ dataset in both Chinese and English languages.\nWe deliberately develop a neural model to benchmark the task, which advances in\neffectively performing end-to-end quadruple prediction, and manages to\nincorporate rich dialogue-specific and discourse feature representations for\nbetter cross-utterance quadruple extraction. We hope the new benchmark will\nspur more advancements in the sentiment analysis community.", "published": "2022-11-10 17:18:20", "link": "http://arxiv.org/abs/2211.05705v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language\n  Model Control", "abstract": "Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.", "published": "2022-11-10 18:31:56", "link": "http://arxiv.org/abs/2211.05750v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Climate Policy Tracker: Pipeline for automated analysis of public\n  climate policies", "abstract": "The number of standardized policy documents regarding climate policy and\ntheir publication frequency is significantly increasing. The documents are long\nand tedious for manual analysis, especially for policy experts, lawmakers, and\ncitizens who lack access or domain expertise to utilize data analytics tools.\nPotential consequences of such a situation include reduced citizen governance\nand involvement in climate policies and an overall surge in analytics costs,\nrendering less accessibility for the public. In this work, we use a Latent\nDirichlet Allocation-based pipeline for the automatic summarization and\nanalysis of 10-years of national energy and climate plans (NECPs) for the\nperiod from 2021 to 2030, established by 27 Member States of the European\nUnion. We focus on analyzing policy framing, the language used to describe\nspecific issues, to detect essential nuances in the way governments frame their\nclimate policies and achieve climate goals. The methods leverage topic modeling\nand clustering for the comparative analysis of policy documents across\ndifferent countries. It allows for easier integration in potential\nuser-friendly applications for the development of theories and processes of\nclimate policy. This would further lead to better citizen governance and\nengagement over climate policies and public policy research.", "published": "2022-11-10 20:19:28", "link": "http://arxiv.org/abs/2211.05852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CREATIVESUMM: Shared Task on Automatic Summarization for Creative\n  Writing", "abstract": "This paper introduces the shared task of summarizing documents in several\ncreative domains, namely literary texts, movie scripts, and television scripts.\nSummarizing these creative documents requires making complex literary\ninterpretations, as well as understanding non-trivial temporal dependencies in\ntexts containing varied styles of plot development and narrative structure.\nThis poses unique challenges and is yet underexplored for text summarization\nsystems. In this shared task, we introduce four sub-tasks and their\ncorresponding datasets, focusing on summarizing books, movie scripts, primetime\ntelevision scripts, and daytime soap opera scripts. We detail the process of\ncurating these datasets for the task, as well as the metrics used for the\nevaluation of the submissions. As part of the CREATIVESUMM workshop at COLING\n2022, the shared task attracted 18 submissions in total. We discuss the\nsubmissions and the baselines for each sub-task in this paper, along with\ndirections for facilitating future work in the field.", "published": "2022-11-10 21:31:03", "link": "http://arxiv.org/abs/2211.05886v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WEKA-Based: Key Features and Classifier for French of Five Countries", "abstract": "This paper describes a French dialect recognition system that will\nappropriately distinguish between different regional French dialects. A corpus\nof five regions - Monaco, French-speaking, Belgium, French-speaking\nSwitzerland, French-speaking Canada and France, which is targeted\nforconstruction by the Sketch Engine. The content of the corpus is related to\nthe four themes of eating, drinking, sleeping and living, which are closely\nlinked to popular life. The experimental results were obtained through the\nprocessing of a python coded pre-processor and Waikato Environment for\nKnowledge Analysis (WEKA) data analytic tool which contains many filters and\nclassifiers for machine learning.", "published": "2022-11-10 10:35:34", "link": "http://arxiv.org/abs/2212.08132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-Based Combination of Convolutional and Recurrent Neural Network for\n  Indonesian Sentiment Analysis", "abstract": "Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.", "published": "2022-11-10 00:32:40", "link": "http://arxiv.org/abs/2211.05273v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "LERT: A Linguistically-motivated Pre-trained Language Model", "abstract": "Pre-trained Language Model (PLM) has become a representative foundation model\nin the natural language processing field. Most PLMs are trained with\nlinguistic-agnostic pre-training tasks on the surface form of the text, such as\nthe masked language model (MLM). To further empower the PLMs with richer\nlinguistic features, in this paper, we aim to propose a simple but effective\nway to learn linguistic features for pre-trained language models. We propose\nLERT, a pre-trained language model that is trained on three types of linguistic\nfeatures along with the original MLM pre-training task, using a\nlinguistically-informed pre-training (LIP) strategy. We carried out extensive\nexperiments on ten Chinese NLU tasks, and the experimental results show that\nLERT could bring significant improvements over various comparable baselines.\nFurthermore, we also conduct analytical experiments in various linguistic\naspects, and the results prove that the design of LERT is valid and effective.\nResources are available at https://github.com/ymcui/LERT", "published": "2022-11-10 05:09:16", "link": "http://arxiv.org/abs/2211.05344v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning", "abstract": "Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.", "published": "2022-11-10 08:19:44", "link": "http://arxiv.org/abs/2211.05405v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UIT-HWDB: Using Transferring Method to Construct A Novel Benchmark for\n  Evaluating Unconstrained Handwriting Image Recognition in Vietnamese", "abstract": "Recognizing handwriting images is challenging due to the vast variation in\nwriting style across many people and distinct linguistic aspects of writing\nlanguages. In Vietnamese, besides the modern Latin characters, there are accent\nand letter marks together with characters that draw confusion to\nstate-of-the-art handwriting recognition methods. Moreover, as a low-resource\nlanguage, there are not many datasets for researching handwriting recognition\nin Vietnamese, which makes handwriting recognition in this language have a\nbarrier for researchers to approach. Recent works evaluated offline handwriting\nrecognition methods in Vietnamese using images from an online handwriting\ndataset constructed by connecting pen stroke coordinates without further\nprocessing. This approach obviously can not measure the ability of recognition\nmethods effectively, as it is trivial and may be lack of features that are\nessential in offline handwriting images. Therefore, in this paper, we propose\nthe Transferring method to construct a handwriting image dataset that\nassociates crucial natural attributes required for offline handwriting images.\nUsing our method, we provide a first high-quality synthetic dataset which is\ncomplex and natural for efficiently evaluating handwriting recognition methods.\nIn addition, we conduct experiments with various state-of-the-art methods to\nfigure out the challenge to reach the solution for handwriting recognition in\nVietnamese.", "published": "2022-11-10 08:23:54", "link": "http://arxiv.org/abs/2211.05407v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can Transformers Reason in Fragments of Natural Language?", "abstract": "State-of-the-art deep-learning-based approaches to Natural Language\nProcessing (NLP) are credited with various capabilities that involve reasoning\nwith natural language texts. In this paper we carry out a large-scale empirical\nstudy investigating the detection of formally valid inferences in controlled\nfragments of natural language for which the satisfiability problem becomes\nincreasingly complex. We find that, while transformer-based language models\nperform surprisingly well in these scenarios, a deeper analysis re-veals that\nthey appear to overfit to superficial patterns in the data rather than\nacquiring the logical principles governing the reasoning in these fragments.", "published": "2022-11-10 08:46:53", "link": "http://arxiv.org/abs/2211.05417v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Impact of Adversarial Training on Robustness and Generalizability of\n  Language Models", "abstract": "Adversarial training is widely acknowledged as the most effective defense\nagainst adversarial attacks. However, it is also well established that\nachieving both robustness and generalization in adversarially trained models\ninvolves a trade-off. The goal of this work is to provide an in depth\ncomparison of different approaches for adversarial training in language models.\nSpecifically, we study the effect of pre-training data augmentation as well as\ntraining time input perturbations vs. embedding space perturbations on the\nrobustness and generalization of transformer-based language models. Our\nfindings suggest that better robustness can be achieved by pre-training data\naugmentation or by training with input space perturbation. However, training\nwith embedding space perturbation significantly improves generalization. A\nlinguistic correlation analysis of neurons of the learned models reveals that\nthe improved generalization is due to 'more specialized' neurons. To the best\nof our knowledge, this is the first work to carry out a deep qualitative\nanalysis of different methods of generating adversarial examples in adversarial\ntraining of language models.", "published": "2022-11-10 12:36:50", "link": "http://arxiv.org/abs/2211.05523v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assistive Completion of Agrammatic Aphasic Sentences: A Transfer\n  Learning Approach using Neurolinguistics-based Synthetic Dataset", "abstract": "Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic\naphasia wherein patients, although able to comprehend, lack the ability to form\ncomplete sentences. This inability leads to communication gaps which cause\ndifficulties in their daily lives. The usage of assistive devices can help in\nmitigating these issues and enable the patients to communicate effectively.\nHowever, due to lack of large scale studies of linguistic deficits in aphasia,\nresearch on such assistive technology is relatively limited. In this work, we\npresent two contributions that aim to re-initiate research and development in\nthis field. Firstly, we propose a model that uses linguistic features from\nsmall scale studies on aphasia patients and generates large scale datasets of\nsynthetic aphasic utterances from grammatically correct datasets. We show that\nthe mean length of utterance, the noun/verb ratio, and the simple/complex\nsentence ratio of our synthetic datasets correspond to the reported features of\naphasic speech. Further, we demonstrate how the synthetic datasets may be\nutilized to develop assistive devices for aphasia patients. The pre-trained T5\ntransformer is fine-tuned using the generated dataset to suggest 5 corrected\nsentences given an aphasic utterance as input. We evaluate the efficacy of the\nT5 model using the BLEU and cosine semantic similarity scores. Affirming\nresults with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00\nwere obtained. These results provide a strong foundation for the concept that a\nsynthetic dataset based on small scale studies on aphasia can be used to\ndevelop effective assistive technology.", "published": "2022-11-10 13:24:02", "link": "http://arxiv.org/abs/2211.05557v1", "categories": ["q-bio.QM", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Prompt Learning for Domain Adaptation in Task-Oriented Dialogue", "abstract": "Conversation designers continue to face significant obstacles when creating\nproduction quality task-oriented dialogue systems. The complexity and cost\ninvolved in schema development and data collection is often a major barrier for\nsuch designers, limiting their ability to create natural, user-friendly\nexperiences. We frame the classification of user intent as the generation of a\ncanonical form, a lightweight semantic representation using natural language.\nWe show that canonical forms offer a promising alternative to traditional\nmethods for intent classification. By tuning soft prompts for a frozen large\nlanguage model, we show that canonical forms generalize very well to new,\nunseen domains in a zero- or few-shot setting. The method is also\nsample-efficient, reducing the complexity and effort of developing new\ntask-oriented dialogue domains.", "published": "2022-11-10 14:16:00", "link": "http://arxiv.org/abs/2211.05596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The CRINGE Loss: Learning what language not to model", "abstract": "Standard language model training employs gold human documents or human-human\ninteraction data, and treats all training data as positive examples. Growing\nevidence shows that even with very large amounts of positive training data,\nissues remain that can be alleviated with relatively small amounts of negative\ndata -- examples of what the model should not do. In this work, we propose a\nnovel procedure to train with such data called the CRINGE loss (ContRastive\nIterative Negative GEneration). We show the effectiveness of this approach\nacross three different experiments on the tasks of safe generation,\ncontradiction avoidance, and open-domain dialogue. Our models outperform\nmultiple strong baselines and are conceptually simple, easy to train and\nimplement.", "published": "2022-11-10 19:30:08", "link": "http://arxiv.org/abs/2211.05826v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Classification with Hypersphere Modeling of Prototypes", "abstract": "Metric-based meta-learning is one of the de facto standards in few-shot\nlearning. It composes of representation learning and metrics calculation\ndesigns. Previous works construct class representations in different ways,\nvarying from mean output embedding to covariance and distributions. However,\nusing embeddings in space lacks expressivity and cannot capture class\ninformation robustly, while statistical complex modeling poses difficulty to\nmetric designs. In this work, we use tensor fields (``areas'') to model classes\nfrom the geometrical perspective for few-shot learning. We present a simple and\neffective method, dubbed hypersphere prototypes (HyperProto), where class\ninformation is represented by hyperspheres with dynamic sizes with two sets of\nlearnable parameters: the hypersphere's center and the radius. Extending from\npoints to areas, hyperspheres are much more expressive than embeddings.\nMoreover, it is more convenient to perform metric-based classification with\nhypersphere prototypes than statistical modeling, as we only need to calculate\nthe distance from a data point to the surface of the hypersphere. Following\nthis idea, we also develop two variants of prototypes under other measurements.\nExtensive experiments and analysis on few-shot learning tasks across NLP and CV\nand comparison with 20+ competitive baselines demonstrate the effectiveness of\nour approach.", "published": "2022-11-10 03:46:02", "link": "http://arxiv.org/abs/2211.05319v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Enabling Efficient Attack Investigation via Human-in-the-Loop Security\n  Analysis", "abstract": "System auditing is a vital technique for collecting system call events as\nsystem provenance and investigating complex multi-step attacks such as Advanced\nPersistent Threats. However, existing attack investigation methods struggle to\nuncover long attack sequences due to the massive volume of system provenance\ndata and their inability to focus on attack-relevant parts. In this paper, we\npresent Raptor, a defense system that enables human analysts to effectively\nanalyze large-scale system provenance to reveal multi-step attack sequences.\nRaptor introduces an expressive domain-specific language, ProvQL, that offers\nessential primitives for various types of attack analyses (e.g., attack pattern\nsearch, attack dependency tracking) with user-defined constraints, enabling\nanalysts to focus on attack-relevant parts and iteratively sift through the\nlarge provenance data. Moreover, Raptor provides an optimized execution engine\nfor efficient language execution. Our extensive evaluations on a wide range of\nattack scenarios demonstrate the practical effectiveness of Raptor in\nfacilitating timely attack investigation.", "published": "2022-11-10 08:13:19", "link": "http://arxiv.org/abs/2211.05403v2", "categories": ["cs.CR", "cs.CL", "cs.DB"], "primary_category": "cs.CR"}
{"title": "GREENER: Graph Neural Networks for News Media Profiling", "abstract": "We study the problem of profiling news media on the Web with respect to their\nfactuality of reporting and bias. This is an important but under-studied\nproblem related to disinformation and \"fake news\" detection, but it addresses\nthe issue at a coarser granularity compared to looking at an individual article\nor an individual claim. This is useful as it allows to profile entire media\noutlets in advance. Unlike previous work, which has focused primarily on text\n(e.g.,~on the text of the articles published by the target website, or on the\ntextual description in their social media profiles or in Wikipedia), here our\nmain focus is on modeling the similarity between media outlets based on the\noverlap of their audience. This is motivated by homophily considerations,\ni.e.,~the tendency of people to have connections to people with similar\ninterests, which we extend to media, hypothesizing that similar types of media\nwould be read by similar kinds of users. In particular, we propose GREENER\n(GRaph nEural nEtwork for News mEdia pRofiling), a model that builds a graph of\ninter-media connections based on their audience overlap, and then uses graph\nneural networks to represent each medium. We find that such representations are\nquite useful for predicting the factuality and the bias of news media outlets,\nyielding improvements over state-of-the-art results reported on two datasets.\nWhen augmented with conventionally used representations obtained from news\narticles, Twitter, YouTube, Facebook, and Wikipedia, prediction accuracy is\nfound to improve by 2.5-27 macro-F1 points for the two tasks.", "published": "2022-11-10 12:46:29", "link": "http://arxiv.org/abs/2211.05533v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Estimating Soft Labels for Out-of-Domain Intent Detection", "abstract": "Out-of-Domain (OOD) intent detection is important for practical dialog\nsystems. To alleviate the issue of lacking OOD training samples, some works\npropose synthesizing pseudo OOD samples and directly assigning one-hot OOD\nlabels to these pseudo samples. However, these one-hot labels introduce noises\nto the training process because some hard pseudo OOD samples may coincide with\nIn-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo\nlabeling (ASoul) method that can estimate soft labels for pseudo OOD samples\nwhen training OOD detectors. Semantic connections between pseudo OOD samples\nand IND intents are captured using an embedding graph. A co-training framework\nis further introduced to produce resulting soft labels following the smoothness\nassumption, i.e., close samples are likely to have similar labels. Extensive\nexperiments on three benchmark datasets show that ASoul consistently improves\nthe OOD detection performance and outperforms various competitive baselines.", "published": "2022-11-10 13:31:13", "link": "http://arxiv.org/abs/2211.05561v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DisentQA: Disentangling Parametric and Contextual Knowledge with\n  Counterfactual Question Answering", "abstract": "Question answering models commonly have access to two sources of \"knowledge\"\nduring inference time: (1) parametric knowledge - the factual knowledge encoded\nin the model weights, and (2) contextual knowledge - external knowledge (e.g.,\na Wikipedia passage) given to the model to generate a grounded answer. Having\nthese two sources of knowledge entangled together is a core issue for\ngenerative QA models as it is unclear whether the answer stems from the given\nnon-parametric knowledge or not. This unclarity has implications on issues of\ntrust, interpretability and factuality. In this work, we propose a new paradigm\nin which QA models are trained to disentangle the two sources of knowledge.\nUsing counterfactual data augmentation, we introduce a model that predicts two\nanswers for a given question: one based on given contextual knowledge and one\nbased on parametric knowledge. Our experiments on the Natural Questions dataset\nshow that this approach improves the performance of QA models by making them\nmore robust to knowledge conflicts between the two knowledge sources, while\ngenerating useful disentangled answers.", "published": "2022-11-10 15:34:44", "link": "http://arxiv.org/abs/2211.05655v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT in Plutarch's Shadows", "abstract": "The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea\n(ca. 45-120 CE) also contains several texts which, according to current\nscholarly opinion, did not originate with him and are therefore attributed to\nan anonymous author Pseudo-Plutarch. These include, in particular, the work\nPlacita Philosophorum (Quotations and Opinions of the Ancient Philosophers),\nwhich is extremely important for the history of ancient philosophy. Little is\nknown about the identity of that anonymous author and its relation to other\nauthors from the same period. This paper presents a BERT language model for\nAncient Greek. The model discovers previously unknown statistical properties\nrelevant to these literary, philosophical, and historical problems and can shed\nnew light on this authorship question. In particular, the Placita\nPhilosophorum, together with one of the other Pseudo-Plutarch texts, shows\nsimilarities with the texts written by authors from an Alexandrian context\n(2nd/3rd century CE).", "published": "2022-11-10 16:21:42", "link": "http://arxiv.org/abs/2211.05673v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "68T50", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual ASR on 70 Languages: Tokenization, Architecture,\n  and Generalization Capabilities", "abstract": "End-to-end multilingual ASR has become more appealing because of several\nreasons such as simplifying the training and deployment process and positive\nperformance transfer from high-resource to low-resource languages. However,\nscaling up the number of languages, total hours, and number of unique tokens is\nnot a trivial task. This paper explores large-scale multilingual ASR models on\n70 languages. We inspect two architectures: (1) Shared embedding and output and\n(2) Multiple embedding and output model. In the shared model experiments, we\nshow the importance of tokenization strategy across different languages. Later,\nwe use our optimal tokenization strategy to train multiple embedding and output\nmodel to further improve our result. Our multilingual ASR achieves 13.9%-15.6%\naverage WER relative improvement compared to monolingual models. We show that\nour multilingual ASR generalizes well on an unseen dataset and domain,\nachieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot\nand finetuning, respectively.", "published": "2022-11-10 18:43:42", "link": "http://arxiv.org/abs/2211.05756v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Casual Conversations v2: Designing a large consent-driven dataset to\n  measure algorithmic bias and robustness", "abstract": "Developing robust and fair AI systems require datasets with comprehensive set\nof labels that can help ensure the validity and legitimacy of relevant\nmeasurements. Recent efforts, therefore, focus on collecting person-related\ndatasets that have carefully selected labels, including sensitive\ncharacteristics, and consent forms in place to use those attributes for model\ntesting and development. Responsible data collection involves several stages,\nincluding but not limited to determining use-case scenarios, selecting\ncategories (annotations) such that the data are fit for the purpose of\nmeasuring algorithmic bias for subgroups and most importantly ensure that the\nselected categories/subcategories are robust to regional diversities and\ninclusive of as many subgroups as possible.\n  Meta, in a continuation of our efforts to measure AI algorithmic bias and\nrobustness\n(https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set),\nis working on collecting a large consent-driven dataset with a comprehensive\nlist of categories. This paper describes our proposed design of such categories\nand subcategories for Casual Conversations v2.", "published": "2022-11-10 19:06:21", "link": "http://arxiv.org/abs/2211.05809v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Measuring Reliability of Large Language Models through Semantic\n  Consistency", "abstract": "While large pretrained language models (PLMs) demonstrate incredible fluency\nand performance on many natural language tasks, recent work has shown that\nwell-performing PLMs are very sensitive to what prompts are feed into them.\nEven when prompts are semantically identical, language models may give very\ndifferent answers. When considering safe and trustworthy deployments of PLMs we\nwould like their outputs to be consistent under prompts that mean the same\nthing or convey the same intent. While some work has looked into how\nstate-of-the-art PLMs address this need, they have been limited to only\nevaluating lexical equality of single- or multi-word answers and do not address\nconsistency of generative text sequences. In order to understand consistency of\nPLMs under text generation settings, we develop a measure of semantic\nconsistency that allows the comparison of open-ended text outputs. We implement\nseveral versions of this consistency metric to evaluate the performance of a\nnumber of PLMs on paraphrased versions of questions in the TruthfulQA dataset,\nwe find that our proposed metrics are considerably more consistent than\ntraditional metrics embodying lexical consistency, and also correlate with\nhuman evaluation of output consistency to a higher degree.", "published": "2022-11-10 20:21:07", "link": "http://arxiv.org/abs/2211.05853v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models\n  for Spoken Language Understanding", "abstract": "Collecting sufficient labeled data for spoken language understanding (SLU) is\nexpensive and time-consuming. Recent studies achieved promising results by\nusing pre-trained models in low-resource scenarios. Inspired by this, we aim to\nask: which (if any) pre-training strategies can improve performance across SLU\nbenchmarks? To answer this question, we employ four types of pre-trained models\nand their combinations for SLU. We leverage self-supervised speech and language\nmodels (LM) pre-trained on large quantities of unpaired data to extract strong\nspeech and text representations. We also explore using supervised models\npre-trained on larger external automatic speech recognition (ASR) or SLU\ncorpora. We conduct extensive experiments on the SLU Evaluation (SLUE)\nbenchmark and observe self-supervised pre-trained models to be more powerful,\nwith pre-trained LM and speech models being most beneficial for the Sentiment\nAnalysis and Named Entity Recognition task, respectively.", "published": "2022-11-10 20:59:13", "link": "http://arxiv.org/abs/2211.05869v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Debiasing Methods for Fairer Neural Models in Vision and Language\n  Research: A Survey", "abstract": "Despite being responsible for state-of-the-art results in several computer\nvision and natural language processing tasks, neural networks have faced harsh\ncriticism due to some of their current shortcomings. One of them is that neural\nnetworks are correlation machines prone to model biases within the data instead\nof focusing on actual useful causal relationships. This problem is particularly\nserious in application domains affected by aspects such as race, gender, and\nage. To prevent models from incurring on unfair decision-making, the AI\ncommunity has concentrated efforts in correcting algorithmic biases, giving\nrise to the research area now widely known as fairness in AI. In this survey\npaper, we provide an in-depth overview of the main debiasing methods for\nfairness-aware neural networks in the context of vision and language research.\nWe propose a novel taxonomy to better organize the literature on debiasing\nmethods for fairness, and we discuss the current challenges, trends, and\nimportant future work directions for the interested researcher and\npractitioner.", "published": "2022-11-10 14:42:46", "link": "http://arxiv.org/abs/2211.05617v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal\n  Open-domain Conversation", "abstract": "Responding with multi-modal content has been recognized as an essential\ncapability for an intelligent conversational agent. In this paper, we introduce\nthe MMDialog dataset to better facilitate multi-modal conversation. MMDialog is\ncomposed of a curated set of 1.08 million real-world dialogues with 1.53\nmillion unique images across 4,184 topics. MMDialog has two main and unique\nadvantages. First, it is the largest multi-modal conversation dataset by the\nnumber of dialogues by 88x. Second, it contains massive topics to generalize\nthe open-domain. To build engaging dialogue system with this dataset, we\npropose and normalize two response producing tasks based on retrieval and\ngenerative scenarios. In addition, we build two baselines for above tasks with\nstate-of-the-art techniques and report their experimental performance. We also\npropose a novel evaluation metric MM-Relevance to measure the multi-modal\nresponses. Our dataset and scripts are available in\nhttps://github.com/victorsungo/MMDialog.", "published": "2022-11-10 17:37:04", "link": "http://arxiv.org/abs/2211.05719v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Understanding ME? Multimodal Evaluation for Fine-grained Visual\n  Commonsense", "abstract": "Visual commonsense understanding requires Vision Language (VL) models to not\nonly understand image and text but also cross-reference in-between to fully\nintegrate and achieve comprehension of the visual scene described. Recently,\nvarious approaches have been developed and have achieved high performance on\nvisual commonsense benchmarks. However, it is unclear whether the models really\nunderstand the visual scene and underlying commonsense knowledge due to limited\nevaluation data resources. To provide an in-depth analysis, we present a\nMultimodal Evaluation (ME) pipeline to automatically generate question-answer\npairs to test models' understanding of the visual scene, text, and related\nknowledge. We then take a step further to show that training with the ME data\nboosts the model's performance in standard VCR evaluation. Lastly, our in-depth\nanalysis and comparison reveal interesting findings: (1) semantically low-level\ninformation can assist the learning of high-level information but not the\nopposite; (2) visual information is generally under utilization compared with\ntext.", "published": "2022-11-10 21:44:33", "link": "http://arxiv.org/abs/2211.05895v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Remap, warp and attend: Non-parallel many-to-many accent conversion with\n  Normalizing Flows", "abstract": "Regional accents of the same language affect not only how words are\npronounced (i.e., phonetic content), but also impact prosodic aspects of speech\nsuch as speaking rate and intonation. This paper investigates a novel\nflow-based approach to accent conversion using normalizing flows. The proposed\napproach revolves around three steps: remapping the phonetic conditioning, to\nbetter match the target accent, warping the duration of the converted speech,\nto better suit the target phonemes, and an attention mechanism that implicitly\naligns source and target speech sequences. The proposed remap-warp-attend\nsystem enables adaptation of both phonetic and prosodic aspects of speech while\nallowing for source and converted speech signals to be of different lengths.\nObjective and subjective evaluations show that the proposed approach\nsignificantly outperforms a competitive CopyCat baseline model in terms of\nsimilarity to the target accent, naturalness and intelligibility.", "published": "2022-11-10 20:14:01", "link": "http://arxiv.org/abs/2211.05850v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EmoFake: An Initial Dataset for Emotion Fake Audio Detection", "abstract": "Many datasets have been designed to further the development of fake audio\ndetection, such as datasets of the ASVspoof and ADD challenges. However, these\ndatasets do not consider a situation that the emotion of the audio has been\nchanged from one to another, while other information (e.g. speaker identity and\ncontent) remains the same. Changing the emotion of an audio can lead to\nsemantic changes. Speech with tampered semantics may pose threats to people's\nlives. Therefore, this paper reports our progress in developing such an emotion\nfake audio detection dataset involving changing emotion state of the origin\naudio named EmoFake. The fake audio in EmoFake is generated by open source\nemotion voice conversion models. Furthermore, we proposed a method named Graph\nAttention networks using Deep Emotion embedding (GADE) for the detection of\nemotion fake audio. Some benchmark experiments are conducted on this dataset.\nThe results show that our designed dataset poses a challenge to the fake audio\ndetection model trained with the LA dataset of ASVspoof 2019. The proposed GADE\nshows good performance in the face of emotion fake audio.", "published": "2022-11-10 06:09:51", "link": "http://arxiv.org/abs/2211.05363v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Enhancement with Fullband-Subband Cross-Attention Network", "abstract": "FullSubNet has shown its promising performance on speech enhancement by\nutilizing both fullband and subband information. However, the relationship\nbetween fullband and subband in FullSubNet is achieved by simply concatenating\nthe output of fullband model and subband units. It only supplements the subband\nunits with a small quantity of global information and has not considered the\ninteraction between fullband and subband. This paper proposes a\nfullband-subband cross-attention (FSCA) module to interactively fuse the global\nand local information and applies it to FullSubNet. This new framework is\ncalled as FS-CANet. Moreover, different from FullSubNet, the proposed FS-CANet\noptimize the fullband extractor by temporal convolutional network (TCN) blocks\nto further reduce the model size. Experimental results on DNS Challenge -\nInterspeech 2021 dataset show that the proposed FS-CANet outperforms other\nstate-of-the-art speech enhancement approaches, and demonstrate the\neffectiveness of fullband-subband cross-attention.", "published": "2022-11-10 09:17:06", "link": "http://arxiv.org/abs/2211.05432v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised learning of audio representations using angular\n  contrastive loss", "abstract": "In Self-Supervised Learning (SSL), various pretext tasks are designed for\nlearning feature representations through contrastive loss. However, previous\nstudies have shown that this loss is less tolerant to semantically similar\nsamples due to the inherent defect of instance discrimination objectives, which\nmay harm the quality of learned feature embeddings used in downstream tasks. To\nimprove the discriminative ability of feature embeddings in SSL, we propose a\nnew loss function called Angular Contrastive Loss (ACL), a linear combination\nof angular margin and contrastive loss. ACL improves contrastive learning by\nexplicitly adding an angular margin between positive and negative augmented\npairs in SSL. Experimental results show that using ACL for both supervised and\nunsupervised learning significantly improves performance. We validated our new\nloss function using the FSDnoisy18k dataset, where we achieved 73.6% and 77.1%\naccuracy in sound event classification using supervised and self-supervised\nlearning, respectively.", "published": "2022-11-10 09:32:10", "link": "http://arxiv.org/abs/2211.05442v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised learning with bi-label masked speech prediction for\n  streaming multi-talker speech recognition", "abstract": "Self-supervised learning (SSL), which utilizes the input data itself for\nrepresentation learning, has achieved state-of-the-art results for various\ndownstream speech tasks. However, most of the previous studies focused on\noffline single-talker applications, with limited investigations in multi-talker\ncases, especially for streaming scenarios. In this paper, we investigate SSL\nfor streaming multi-talker speech recognition, which generates transcriptions\nof overlapping speakers in a streaming fashion. We first observe that\nconventional SSL techniques do not work well on this task due to the poor\nrepresentation of overlapping speech. We then propose a novel SSL training\nobjective, referred to as bi-label masked speech prediction, which explicitly\npreserves representations of all speakers in overlapping speech. We investigate\nvarious aspects of the proposed system including data configuration and\nquantizer selection. The proposed SSL setup achieves substantially better word\nerror rates on the LibriSpeechMix dataset.", "published": "2022-11-10 13:36:27", "link": "http://arxiv.org/abs/2211.05564v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GANStrument: Adversarial Instrument Sound Synthesis with Pitch-invariant\n  Instance Conditioning", "abstract": "We propose GANStrument, a generative adversarial model for instrument sound\nsynthesis. Given a one-shot sound as input, it is able to generate pitched\ninstrument sounds that reflect the timbre of the input within an interactive\ntime. By exploiting instance conditioning, GANStrument achieves better fidelity\nand diversity of synthesized sounds and generalization ability to various\ninputs. In addition, we introduce an adversarial training scheme for a\npitch-invariant feature extractor that significantly improves the pitch\naccuracy and timbre consistency. Experimental results show that GANStrument\noutperforms strong baselines that do not use instance conditioning in terms of\ngeneration quality and input editability. Qualitative examples are available\nonline.", "published": "2022-11-10 07:24:09", "link": "http://arxiv.org/abs/2211.05385v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Privacy-Utility Balanced Voice De-Identification Using Adversarial\n  Examples", "abstract": "Faced with the threat of identity leakage during voice data publishing, users\nare engaged in a privacy-utility dilemma when enjoying convenient voice\nservices. Existing studies employ direct modification or text-based\nre-synthesis to de-identify users' voices, but resulting in inconsistent\naudibility in the presence of human participants. In this paper, we propose a\nvoice de-identification system, which uses adversarial examples to balance the\nprivacy and utility of voice services. Instead of typical additive examples\ninducing perceivable distortions, we design a novel convolutional adversarial\nexample that modulates perturbations into real-world room impulse responses.\nBenefit from this, our system could preserve user identity from exposure by\nAutomatic Speaker Identification (ASI) while remaining the voice perceptual\nquality for non-intrusive de-identification. Moreover, our system learns a\ncompact speaker distribution through a conditional variational auto-encoder to\nsample diverse target embeddings on demand. Combining diverse target generation\nand input-specific perturbation construction, our system enables any-to-any\nidentify transformation for adaptive de-identification. Experimental results\nshow that our system could achieve 98% and 79% successful de-identification on\nmainstream ASIs and commercial systems with an objective Mel cepstral\ndistortion of 4.31dB and a subjective mean opinion score of 4.48.", "published": "2022-11-10 09:35:58", "link": "http://arxiv.org/abs/2211.05446v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vis2Mus: Exploring Multimodal Representation Mapping for Controllable\n  Music Generation", "abstract": "In this study, we explore the representation mapping from the domain of\nvisual arts to the domain of music, with which we can use visual arts as an\neffective handle to control music generation. Unlike most studies in multimodal\nrepresentation learning that are purely data-driven, we adopt an\nanalysis-by-synthesis approach that combines deep music representation learning\nwith user studies. Such an approach enables us to discover\n\\textit{interpretable} representation mapping without a huge amount of paired\ndata. In particular, we discover that visual-to-music mapping has a nice\nproperty similar to equivariant. In other words, we can use various image\ntransformations, say, changing brightness, changing contrast, style transfer,\nto control the corresponding transformations in the music domain. In addition,\nwe released the Vis2Mus system as a controllable interface for symbolic music\ngeneration.", "published": "2022-11-10 13:01:26", "link": "http://arxiv.org/abs/2211.05543v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
