{"title": "Text Similarity Using Word Embeddings to Classify Misinformation", "abstract": "Fake news is a growing problem in the last years, especially during\nelections. It's hard work to identify what is true and what is false among all\nthe user generated content that circulates every day. Technology can help with\nthat work and optimize the fact-checking process. In this work, we address the\nchallenge of finding similar content in order to be able to suggest to a\nfact-checker articles that could have been verified before and thus avoid that\nthe same information is verified more than once. This is especially important\nin collaborative approaches to fact-checking where members of large teams will\nnot know what content others have already fact-checked.", "published": "2020-03-14 14:02:27", "link": "http://arxiv.org/abs/2003.06634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "abstract": "Disambiguation of word senses in context is easy for humans, but is a major\nchallenge for automatic approaches. Sophisticated supervised and\nknowledge-based models were developed to solve this task. However, (i) the\ninherent Zipfian distribution of supervised training instances for a given word\nand/or (ii) the quality of linguistic knowledge representations motivate the\ndevelopment of completely unsupervised and knowledge-free approaches to word\nsense disambiguation (WSD). They are particularly useful for under-resourced\nlanguages which do not have any resources for building either supervised and/or\nknowledge-based models. In this paper, we present a method that takes as input\na standard pre-trained word embedding model and induces a fully-fledged word\nsense inventory, which can be used for disambiguation in context. We use this\nmethod to induce a collection of sense inventories for 158 languages on the\nbasis of the original pre-trained fastText word embeddings by Grave et al.\n(2018), enabling WSD in these languages. Models and system are available\nonline.", "published": "2020-03-14 14:50:04", "link": "http://arxiv.org/abs/2003.06651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantically-Enriched Search Engine for Geoportals: A Case Study with\n  ArcGIS Online", "abstract": "Many geoportals such as ArcGIS Online are established with the goal of\nimproving geospatial data reusability and achieving intelligent knowledge\ndiscovery. However, according to previous research, most of the existing\ngeoportals adopt Lucene-based techniques to achieve their core search\nfunctionality, which has a limited ability to capture the user's search\nintentions. To better understand a user's search intention, query expansion can\nbe used to enrich the user's query by adding semantically similar terms. In the\ncontext of geoportals and geographic information retrieval, we advocate the\nidea of semantically enriching a user's query from both geospatial and thematic\nperspectives. In the geospatial aspect, we propose to enrich a query by using\nboth place partonomy and distance decay. In terms of the thematic aspect,\nconcept expansion and embedding-based document similarity are used to infer the\nimplicit information hidden in a user's query. This semantic query expansion 1\n2 G. Mai et al. framework is implemented as a semantically-enriched search\nengine using ArcGIS Online as a case study. A benchmark dataset is constructed\nto evaluate the proposed framework. Our evaluation results show that the\nproposed semantic query expansion framework is very effective in capturing a\nuser's search intention and significantly outperforms a well-established\nbaseline-Lucene's practical scoring function-with more than 3.0 increments in\nDCG@K (K=3,5,10).", "published": "2020-03-14 06:16:30", "link": "http://arxiv.org/abs/2003.06561v1", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "abstract": "Despite Visual Question Answering (VQA) has realized impressive progress over\nthe last few years, today's VQA models tend to capture superficial linguistic\ncorrelations in the train set and fail to generalize to the test set with\ndifferent QA distributions. To reduce the language biases, several recent works\nintroduce an auxiliary question-only model to regularize the training of\ntargeted VQA model, and achieve dominating performance on VQA-CP. However,\nsince the complexity of design, current methods are unable to equip the\nensemble-based models with two indispensable characteristics of an ideal VQA\nmodel: 1) visual-explainable: the model should rely on the right visual regions\nwhen making decisions. 2) question-sensitive: the model should be sensitive to\nthe linguistic variations in question. To this end, we propose a model-agnostic\nCounterfactual Samples Synthesizing (CSS) training scheme. The CSS generates\nnumerous counterfactual training samples by masking critical objects in images\nor words in questions, and assigning different ground-truth answers. After\ntraining with the complementary samples (ie, the original and generated\nsamples), the VQA models are forced to focus on all critical objects and words,\nwhich significantly improves both visual-explainable and question-sensitive\nabilities. In return, the performance of these models is further boosted.\nExtensive ablations have shown the effectiveness of CSS. Particularly, by\nbuilding on top of the model LMH, we achieve a record-breaking performance of\n58.95% on VQA-CP v2, with 6.5% gains.", "published": "2020-03-14 08:34:31", "link": "http://arxiv.org/abs/2003.06576v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Revisit Systematic Generalization via Meaningful Learning", "abstract": "Humans can systematically generalize to novel compositions of existing\nconcepts. Recent studies argue that neural networks appear inherently\nineffective in such cognitive capacity, leading to a pessimistic view and a\nlack of attention to optimistic results. We revisit this controversial topic\nfrom the perspective of meaningful learning, an exceptional capability of\nhumans to learn novel concepts by connecting them with known ones. We reassess\nthe compositional skills of sequence-to-sequence models conditioned on the\nsemantic links between new and old concepts. Our observations suggest that\nmodels can successfully one-shot generalize to novel concepts and compositions\nthrough semantic linking, either inductively or deductively. We demonstrate\nthat prior knowledge plays a key role as well. In addition to synthetic tests,\nwe further conduct proof-of-concept experiments in machine translation and\nsemantic parsing, showing the benefits of meaningful learning in applications.\nWe hope our positive findings will encourage excavating modern neural networks'\npotential in systematic generalization through more advanced learning schemes.", "published": "2020-03-14 15:27:29", "link": "http://arxiv.org/abs/2003.06658v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Perception of prosodic variation for speech synthesis using an\n  unsupervised discrete representation of F0", "abstract": "In English, prosody adds a broad range of information to segment sequences,\nfrom information structure (e.g. contrast) to stylistic variation (e.g.\nexpression of emotion). However, when learning to control prosody in\ntext-to-speech voices, it is not clear what exactly the control is modifying.\nExisting research on discrete representation learning for prosody has\ndemonstrated high naturalness, but no analysis has been performed on what these\nrepresentations capture, or if they can generate meaningfully-distinct variants\nof an utterance. We present a phrase-level variational autoencoder with a\nmulti-modal prior, using the mode centres as \"intonation codes\". Our evaluation\nestablishes which intonation codes are perceptually distinct, finding that the\nintonation codes from our multi-modal latent model were significantly more\ndistinct than a baseline using k-means clustering. We carry out a follow-up\nqualitative study to determine what information the codes are carrying. Most\ncommonly, listeners commented on the intonation codes having a statement or\nquestion style. However, many other affect-related styles were also reported,\nincluding: emotional, uncertain, surprised, sarcastic, passive aggressive, and\nupset.", "published": "2020-03-14 19:17:42", "link": "http://arxiv.org/abs/2003.06686v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Finnish Language Modeling with Deep Transformer Models", "abstract": "Transformers have recently taken the center stage in language modeling after\nLSTM's were considered the dominant model architecture for a long time. In this\nproject, we investigate the performance of the Transformer architectures-BERT\nand Transformer-XL for the language modeling task. We use a sub-word model\nsetting with the Finnish language and compare it to the previous State of the\nart (SOTA) LSTM model. BERT achieves a pseudo-perplexity score of 14.5, which\nis the first such measure achieved as far as we know. Transformer-XL improves\nupon the perplexity score to 73.58 which is 27\\% better than the LSTM model.", "published": "2020-03-14 15:12:03", "link": "http://arxiv.org/abs/2003.11562v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Spatial Aligment Requirements of Central and Peripheral\n  Object Events", "abstract": "Immersive audio-visual perception relies on the spatial integration of both\nauditory and visual information which are heterogeneous sensing modalities with\ndifferent fields of reception and spatial resolution. This study investigates\nthe perceived coherence of audiovisual object events presented either centrally\nor peripherally with horizontally aligned/misaligned sound. Various object\nevents were selected to represent three acoustic feature classes. Subjective\ntest results in a simulated virtual environment from 18 participants indicate a\nwider capture region in the periphery, with an outward bias favoring more\nlateral sounds. Centered stimulus results support previous findings for simpler\nscenes.", "published": "2020-03-14 15:19:52", "link": "http://arxiv.org/abs/2003.06656v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
