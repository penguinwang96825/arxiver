{"title": "Sentence-Level Relation Extraction via Contrastive Learning with\n  Descriptive Relation Prompts", "abstract": "Sentence-level relation extraction aims to identify the relation between two\nentities for a given sentence. The existing works mostly focus on obtaining a\nbetter entity representation and adopting a multi-label classifier for relation\nextraction. A major limitation of these works is that they ignore background\nrelational knowledge and the interrelation between entity types and candidate\nrelations. In this work, we propose a new paradigm, Contrastive Learning with\nDescriptive Relation Prompts(CTL-DRP), to jointly consider entity information,\nrelational knowledge and entity type restrictions. In particular, we introduce\nan improved entity marker and descriptive relation prompts when generating\ncontextual embedding, and utilize contrastive learning to rank the restricted\ncandidate relations. The CTL-DRP obtains a competitive F1-score of 76.7% on\nTACRED. Furthermore, the new presented paradigm achieves F1-scores of 85.8% and\n91.6% on TACREV and Re-TACRED respectively, which are both the state-of-the-art\nperformance.", "published": "2023-04-11 02:15:13", "link": "http://arxiv.org/abs/2304.04935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast\n  Inference", "abstract": "We propose Conditional Adapter (CoDA), a parameter-efficient transfer\nlearning method that also improves inference efficiency. CoDA generalizes\nbeyond standard adapter approaches to enable a new way of balancing speed and\naccuracy using conditional computation. Starting with an existing dense\npretrained model, CoDA adds sparse activation together with a small number of\nnew parameters and a light-weight training phase. Our experiments demonstrate\nthat the CoDA approach provides an unexpectedly efficient way to transfer\nknowledge. Across a variety of language, vision, and speech tasks, CoDA\nachieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter\napproaches with moderate to no accuracy loss and the same parameter efficiency.", "published": "2023-04-11 03:17:37", "link": "http://arxiv.org/abs/2304.04947v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and\n  generative models for Vietnamese multi-document summarization", "abstract": "Multi-document summarization is challenging because the summaries should not\nonly describe the most important information from all documents but also\nprovide a coherent interpretation of the documents. This paper proposes a\nmethod for multi-document summarization based on cluster similarity. In the\nextractive method we use hybrid model based on a modified version of the\nPageRank algorithm and a text correlation considerations mechanism. After\ngenerating summaries by selecting the most important sentences from each\ncluster, we apply BARTpho and ViT5 to construct the abstractive models. Both\nextractive and abstractive approaches were considered in this study. The\nproposed method achieves competitive results in VLSP 2022 competition.", "published": "2023-04-11 13:15:24", "link": "http://arxiv.org/abs/2304.05205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards preserving word order importance through Forced Invalidation", "abstract": "Large pre-trained language models such as BERT have been widely used as a\nframework for natural language understanding (NLU) tasks. However, recent\nfindings have revealed that pre-trained language models are insensitive to word\norder. The performance on NLU tasks remains unchanged even after randomly\npermuting the word of a sentence, where crucial syntactic information is\ndestroyed. To help preserve the importance of word order, we propose a simple\napproach called Forced Invalidation (FI): forcing the model to identify\npermuted sequences as invalid samples. We perform an extensive evaluation of\nour approach on various English NLU and QA based tasks over BERT-based and\nattention-based models over word embeddings. Our experiments demonstrate that\nForced Invalidation significantly improves the sensitivity of the models to\nword order.", "published": "2023-04-11 13:42:10", "link": "http://arxiv.org/abs/2304.05221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Entity-based Claim Extraction Pipeline for Real-world Biomedical\n  Fact-checking", "abstract": "Existing fact-checking models for biomedical claims are typically trained on\nsynthetic or well-worded data and hardly transfer to social media content. This\nmismatch can be mitigated by adapting the social media input to mimic the\nfocused nature of common training claims. To do so, Wuehrl & Klinger (2022)\npropose to extract concise claims based on medical entities in the text.\nHowever, their study has two limitations: First, it relies on gold-annotated\nentities. Therefore, its feasibility for a real-world application cannot be\nassessed since this requires detecting relevant entities automatically. Second,\nthey represent claim entities with the original tokens. This constitutes a\nterminology mismatch which potentially limits the fact-checking performance. To\nunderstand both challenges, we propose a claim extraction pipeline for medical\ntweets that incorporates named entity recognition and terminology normalization\nvia entity linking. We show that automatic NER does lead to a performance drop\nin comparison to using gold annotations but the fact-checking performance still\nimproves considerably over inputting the unchanged tweets. Normalizing entities\nto their canonical forms does, however, not improve the performance.", "published": "2023-04-11 15:07:24", "link": "http://arxiv.org/abs/2304.05268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RRHF: Rank Responses to Align Language Models with Human Feedback\n  without tears", "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment\nof large language models with human preferences, significantly enhancing the\nquality of interactions between humans and models. InstructGPT implements RLHF\nthrough several stages, including Supervised Fine-Tuning (SFT), reward model\ntraining, and Proximal Policy Optimization (PPO). However, PPO is sensitive to\nhyperparameters and requires multiple models in its standard implementation,\nmaking it hard to train and scale up to larger parameter counts. In contrast,\nwe propose a novel learning paradigm called RRHF, which scores sampled\nresponses from different sources via a logarithm of conditional probabilities\nand learns to align these probabilities with human preferences through ranking\nloss. RRHF can leverage sampled responses from various sources including the\nmodel responses from itself, other large language model responses, and human\nexpert responses to learn to rank them. RRHF only needs 1 to 2 models during\ntuning and can efficiently align language models with human preferences\nrobustly without complex hyperparameter tuning. Additionally, RRHF can be\nconsidered an extension of SFT and reward model training while being simpler\nthan PPO in terms of coding, model counts, and hyperparameters. We evaluate\nRRHF on the Helpful and Harmless dataset, demonstrating comparable alignment\nperformance with PPO by reward model score and human labeling. Extensive\nexperiments show that the performance of RRHF is highly related to sampling\nquality which suggests RRHF is a best-of-n learner. Codes available at\nhttps://github.com/GanjinZero/RRHF.", "published": "2023-04-11 15:53:40", "link": "http://arxiv.org/abs/2304.05302v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Use of Foundation Models for Named Entity Recognition and\n  Lemmatization Tasks in Slavic Languages", "abstract": "This paper describes Adam Mickiewicz University's (AMU) solution for the 4th\nShared Task on SlavNER. The task involves the identification, categorization,\nand lemmatization of named entities in Slavic languages. Our approach involved\nexploring the use of foundation models for these tasks. In particular, we used\nmodels based on the popular BERT and T5 model architectures. Additionally, we\nused external datasets to further improve the quality of our models. Our\nsolution obtained promising results, achieving high metrics scores in both\ntasks. We describe our approach and the results of our experiments in detail,\nshowing that the method is effective for NER and lemmatization in Slavic\nlanguages. Additionally, our models for lemmatization will be available at:\nhttps://huggingface.co/amu-cai.", "published": "2023-04-11 16:55:11", "link": "http://arxiv.org/abs/2304.05336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Adaptive Language Learning Chatbots with a Curriculum", "abstract": "Along with the development of systems for natural language understanding and\ngeneration, dialog systems have been widely adopted for language learning and\npracticing. Many current educational dialog systems perform chitchat, where the\ngenerated content and vocabulary are not constrained. However, for learners in\na school setting, practice through dialog is more effective if it aligns with\nstudents' curriculum and focuses on textbook vocabulary. Therefore, we adapt\nlexically constrained decoding to a dialog system, which urges the dialog\nsystem to include curriculum-aligned words and phrases in its generated\nutterances. We adopt a generative dialog system, BlenderBot3, as our backbone\nmodel and evaluate our curriculum-based dialog system with middle school\nstudents learning English as their second language. The constrained words and\nphrases are derived from their textbooks, suggested by their English teachers.\nThe evaluation result demonstrates that the dialog system with curriculum\ninfusion improves students' understanding of target words and increases their\ninterest in practicing English.", "published": "2023-04-11 20:41:41", "link": "http://arxiv.org/abs/2304.05489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "chatClimate: Grounding Conversational AI in Climate Science", "abstract": "Large Language Models (LLMs) have made significant progress in recent years,\nachieving remarkable results in question-answering tasks (QA). However, they\nstill face two major challenges: hallucination and outdated information after\nthe training phase. These challenges take center stage in critical domains like\nclimate change, where obtaining accurate and up-to-date information from\nreliable sources in a limited time is essential and difficult. To overcome\nthese barriers, one potential solution is to provide LLMs with access to\nexternal, scientifically accurate, and robust sources (long-term memory) to\ncontinuously update their knowledge and prevent the propagation of inaccurate,\nincorrect, or outdated information. In this study, we enhanced GPT-4 by\nintegrating the information from the Sixth Assessment Report of the\nIntergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable\nsource in this domain. We present our conversational AI prototype, available at\nwww.chatclimate.ai and demonstrate its ability to answer challenging questions\naccurately in three different QA scenarios: asking from 1) GPT-4, 2)\nchatClimate, and 3) hybrid chatClimate. The answers and their sources were\nevaluated by our team of IPCC authors, who used their expert knowledge to score\nthe accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation\nshowed that the hybrid chatClimate provided more accurate answers, highlighting\nthe effectiveness of our solution. This approach can be easily scaled for\nchatbots in specific domains, enabling the delivery of reliable and accurate\ninformation.", "published": "2023-04-11 21:31:39", "link": "http://arxiv.org/abs/2304.05510v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mathematical and Linguistic Characterization of Orhan Pamuk's Nobel\n  Works", "abstract": "In this study, Nobel Laureate Orhan Pamuk's works are chosen as examples of\nTurkish literature. By counting the number of letters and words in his texts,\nwe find it possible to study his works statistically. It has been known that\nthere is a geometrical order in text structures. Here the method based on the\nbasic assumption of fractal geometry is introduced for calculating the fractal\ndimensions of Pamuk's texts. The results are compared with the applications of\nZipf's law, which is successfully applied for letters and words, where two\nconcepts, namely Zipf's dimension and Zipf's order, are introduced. The Zipf\ndimension of the novel My Name is Red is found to be much different than his\nother novels. However, it is linguistically observed that there is no\nfundamental difference between his corpora. The results are interpreted in\nterms of fractal dimensions and the Turkish language.", "published": "2023-04-11 21:37:50", "link": "http://arxiv.org/abs/2304.05512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers\n  through Japanese stylometric analysis", "abstract": "In the first half of 2023, text-generative artificial intelligence (AI),\nincluding ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted\nconsiderable attention worldwide. In this study, first, we compared Japanese\nstylometric features of texts generated by GPT (-3.5 and -4) and those written\nby humans. In this work, we performed multi-dimensional scaling (MDS) to\nconfirm the distributions of 216 texts of three classes (72 academic papers\nwritten by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts\ngenerated by GPT-4 on the basis of the titles of the aforementioned papers)\nfocusing on the following stylometric features: (1) bigrams of parts-of-speech,\n(2) bigram of postpositional particle words, (3) positioning of commas, and (4)\nrate of function words. MDS revealed distinct distributions at each stylometric\nfeature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than\nGPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions\nare likely to overlap. These results indicate that although the number of\nparameters may increase in the future, GPT-generated texts may not be close to\nthat written by humans in terms of stylometric features. Second, we verified\nthe classification performance of random forest (RF) for two classes (GPT and\nhuman) focusing on Japanese stylometric features. This study revealed the high\nperformance of RF in each stylometric feature: The RF classifier focusing on\nthe rate of function words achieved 98.1% accuracy. Furthermore the RF\nclassifier focusing on all stylometric features reached 100% in terms of all\nperformance indexes (accuracy, recall, precision, and F1 score). This study\nconcluded that at this stage we human discriminate ChatGPT from human limited\nto Japanese language.", "published": "2023-04-11 23:29:56", "link": "http://arxiv.org/abs/2304.05534v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Corpus-based Analysis of Attitudinal Changes in Lin Yutang's\n  Self-translation of Between Tears and Laughter", "abstract": "Attitude is omnipresent in almost every type of text. There has yet to be any\nrelevant research on attitudinal shifts in self-translation. The Chinese\nversion of Between Tears and Laughter is a rare case of self-translation and\nco-translation in that the first 11 chapters are self-translated by Lin Yutang,\nand the last 12 chapters by Xu Chengbin. The current study conducted a word\nfrequency analysis of this book's English and Chinese versions with LIWC and\nAntConc, and made comparative research into Lin Yutang's attitudinal changes.\nThe results show that due to different writing purposes and readerships, there\nis less anger in Lin's self-translation (M=0.7755, SD=0.2775) than in the first\n11 chapters of the English original (M=1.1036, SD=0.3861), which is a\nsignificant difference (t=2.2892, p=0.0331). This attitudinal change is also\nreflected in the translations of some n-grams containing anger words. In\ncontrast, there is no significant difference (t=1.88, p=0.07) between Xu's\nco-translation and the corresponding part of the original in attitude \"anger\".\nThis paper believes that corpus tools can help co-translators keep their\ntranslation consistent in attitude.", "published": "2023-04-11 14:36:03", "link": "http://arxiv.org/abs/2304.08173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning Tutor Better Supported Lower Performers in a Math\n  Task", "abstract": "Resource limitations make it hard to provide all students with one of the\nmost effective educational interventions: personalized instruction.\nReinforcement learning could be a key tool to reduce the development cost and\nimprove the effectiveness of intelligent tutoring software that aims to provide\nthe right support, at the right time, to a student. Here we illustrate that\ndeep reinforcement learning can be used to provide adaptive pedagogical support\nto students learning about the concept of volume in a narrative storyline\nsoftware. Using explainable artificial intelligence tools, we extracted\ninterpretable insights about the pedagogical policy learned and demonstrated\nthat the resulting policy had similar performance in a different student\npopulation. Most importantly, in both studies, the reinforcement-learning\nnarrative system had the largest benefit for those students with the lowest\ninitial pretest scores, suggesting the opportunity for AI to adapt and provide\nsupport for those most in need.", "published": "2023-04-11 02:11:24", "link": "http://arxiv.org/abs/2304.04933v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards an Understanding and Explanation for Mixed-Initiative Artificial\n  Scientific Text Detection", "abstract": "Large language models (LLMs) have gained popularity in various fields for\ntheir exceptional capability of generating human-like text. Their potential\nmisuse has raised social concerns about plagiarism in academic contexts.\nHowever, effective artificial scientific text detection is a non-trivial task\ndue to several challenges, including 1) the lack of a clear understanding of\nthe differences between machine-generated and human-written scientific text, 2)\nthe poor generalization performance of existing methods caused by\nout-of-distribution issues, and 3) the limited support for human-machine\ncollaboration with sufficient interpretability during the detection process. In\nthis paper, we first identify the critical distinctions between\nmachine-generated and human-written scientific text through a quantitative\nexperiment. Then, we propose a mixed-initiative workflow that combines human\nexperts' prior knowledge with machine intelligence, along with a visual\nanalytics prototype to facilitate efficient and trustworthy scientific text\ndetection. Finally, we demonstrate the effectiveness of our approach through\ntwo case studies and a controlled user study with proficient researchers. We\nalso provide design implications for interactive artificial text detection\ntools in high-stakes decision-making scenarios.", "published": "2023-04-11 06:37:30", "link": "http://arxiv.org/abs/2304.05011v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Human-machine cooperation for semantic feature listing", "abstract": "Semantic feature norms, lists of features that concepts do and do not\npossess, have played a central role in characterizing human conceptual\nknowledge, but require extensive human labor. Large language models (LLMs)\noffer a novel avenue for the automatic generation of such feature lists, but\nare prone to significant error. Here, we present a new method for combining a\nlearned model of human lexical-semantics from limited data with LLM-generated\ndata to efficiently generate high-quality feature norms.", "published": "2023-04-11 06:38:04", "link": "http://arxiv.org/abs/2304.05012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion\n  Vision-Language Pre-training", "abstract": "Fashion vision-language pre-training models have shown efficacy for a wide\nrange of downstream tasks. However, general vision-language pre-training models\npay less attention to fine-grained domain features, while these features are\nimportant in distinguishing the specific domain tasks from general tasks. We\npropose a method for fine-grained fashion vision-language pre-training based on\nfashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained\nmulti-modalities fashion attributes and characteristics. Firstly, we propose\nthe fashion symbols, a novel abstract fashion concept layer, to represent\ndifferent fashion items and to generalize various kinds of fine-grained fashion\nfeatures, making modelling fine-grained attributes more effective. Secondly,\nthe attributes prompt method is proposed to make the model learn specific\nattributes of fashion items explicitly. We design proper prompt templates\naccording to the format of fashion data. Comprehensive experiments are\nconducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and\nFashionSAP gets SOTA performances for four popular fashion tasks. The ablation\nstudy also shows the proposed abstract fashion symbols, and the attribute\nprompt method enables the model to acquire fine-grained semantics in the\nfashion domain effectively. The obvious performance gains from FashionSAP\nprovide a new baseline for future fashion task research.", "published": "2023-04-11 08:20:17", "link": "http://arxiv.org/abs/2304.05051v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Teaching Large Language Models to Self-Debug", "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.", "published": "2023-04-11 10:43:43", "link": "http://arxiv.org/abs/2304.05128v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT", "abstract": "With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.", "published": "2023-04-11 13:05:04", "link": "http://arxiv.org/abs/2304.05197v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Approximating Online Human Evaluation of Social Chatbots with Prompting", "abstract": "As conversational models become increasingly available to the general public,\nusers are engaging with this technology in social interactions. Such\nunprecedented interaction experiences may pose considerable social and\npsychological risks to the users unless the technology is properly controlled.\nThis highlights the need for scalable and robust evaluation metrics for\nconversational chatbots. Existing evaluation metrics aim to automate offline\nuser evaluation and approximate human judgment of pre-curated dialogs. However,\nthey are limited in their ability to capture subjective perceptions of users\nwho actually interact with the bots and might not generalize to real-world\nsettings. To address this limitation, we propose an approach to approximate\nonline human evaluation leveraging large language models (LLMs) from the GPT\nfamily. We introduce a new Dialog system Evaluation framework based on\nPrompting (DEP), which enables a fully automatic evaluation pipeline that\nreplicates live user studies and achieves an impressive correlation with human\njudgment (up to Pearson r=0.95 on a system level). The DEP approach involves\ncollecting synthetic chat logs of evaluated bots with an LLM in the other-play\nsetting, where the LLM is carefully conditioned to follow a specific scenario.\nWe further explore different prompting approaches to produce evaluation scores\nwith the same LLM. The best performing prompts, which contain few-shot\ndemonstrations and instructions, show outstanding performance on the tested\ndataset and demonstrate the ability to generalize to other dialog corpora.", "published": "2023-04-11 14:45:01", "link": "http://arxiv.org/abs/2304.05253v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ELVIS: Empowering Locality of Vision Language Pre-training with\n  Intra-modal Similarity", "abstract": "Deep learning has shown great potential in assisting radiologists in reading\nchest X-ray (CXR) images, but its need for expensive annotations for improving\nperformance prevents widespread clinical application. Visual language\npre-training (VLP) can alleviate the burden and cost of annotation by\nleveraging routinely generated reports for radiographs, which exist in large\nquantities as well as in paired form (image-text pairs). Additionally,\nextensions to localization-aware VLPs are being proposed to address the needs\nfor accurate localization of abnormalities for computer-aided diagnosis (CAD)\nin CXR. However, we find that the formulation proposed by locality-aware VLP\nliterature actually leads to a loss in spatial relationships required for\ndownstream localization tasks. Therefore, we propose Empowering Locality of VLP\nwith Intra-modal Similarity, ELVIS, a VLP aware of intra-modal locality, to\nbetter preserve the locality within radiographs or reports, which enhances the\nability to comprehend location references in text reports. Our locality-aware\nVLP method significantly outperforms state-of-the art baselines in multiple\nsegmentation tasks and the MS-CXR phrase grounding task. Qualitatively, we show\nthat ELVIS focuses well on regions of interest described in the report text\ncompared to prior approaches, allowing for enhanced interpretability.", "published": "2023-04-11 15:54:25", "link": "http://arxiv.org/abs/2304.05303v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Emergent autonomous scientific research capabilities of large language\n  models", "abstract": "Transformer-based large language models are rapidly advancing in the field of\nmachine learning research, with applications spanning natural language,\nbiology, chemistry, and computer programming. Extreme scaling and reinforcement\nlearning from human feedback have significantly improved the quality of\ngenerated text, enabling these models to perform various tasks and reason about\ntheir choices. In this paper, we present an Intelligent Agent system that\ncombines multiple large language models for autonomous design, planning, and\nexecution of scientific experiments. We showcase the Agent's scientific\nresearch capabilities with three distinct examples, with the most complex being\nthe successful performance of catalyzed cross-coupling reactions. Finally, we\ndiscuss the safety implications of such systems and propose measures to prevent\ntheir misuse.", "published": "2023-04-11 16:50:17", "link": "http://arxiv.org/abs/2304.05332v1", "categories": ["physics.chem-ph", "cs.CL"], "primary_category": "physics.chem-ph"}
{"title": "Zero-shot Temporal Relation Extraction with ChatGPT", "abstract": "The goal of temporal relation extraction is to infer the temporal relation\nbetween two events in the document. Supervised models are dominant in this\ntask. In this work, we investigate ChatGPT's ability on zero-shot temporal\nrelation extraction. We designed three different prompt techniques to break\ndown the task and evaluate ChatGPT. Our experiments show that ChatGPT's\nperformance has a large gap with that of supervised methods and can heavily\nrely on the design of prompts. We further demonstrate that ChatGPT can infer\nmore small relation classes correctly than supervised methods. The current\nshortcomings of ChatGPT on temporal relation extraction are also discussed in\nthis paper. We found that ChatGPT cannot keep consistency during temporal\ninference and it fails in actively long-dependency temporal inference.", "published": "2023-04-11 18:59:05", "link": "http://arxiv.org/abs/2304.05454v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Causality with Large Language Models: Feasibility and\n  Opportunities", "abstract": "We assess the ability of large language models (LLMs) to answer causal\nquestions by analyzing their strengths and weaknesses against three types of\ncausal question. We believe that current LLMs can answer causal questions with\nexisting causal knowledge as combined domain experts. However, they are not yet\nable to provide satisfactory answers for discovering new knowledge or for\nhigh-stakes decision-making tasks with high precision. We discuss possible\nfuture directions and opportunities, such as enabling explicit and implicit\ncausal modules as well as deep causal-aware LLMs. These will not only enable\nLLMs to answer many different types of causal questions for greater impact but\nalso enable LLMs to be more trustworthy and efficient in general.", "published": "2023-04-11 22:30:03", "link": "http://arxiv.org/abs/2304.05524v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Vision-and-Language Navigation by Generating Future-View Image\n  Semantics", "abstract": "Vision-and-Language Navigation (VLN) is the task that requires an agent to\nnavigate through the environment based on natural language instructions. At\neach step, the agent takes the next action by selecting from a set of navigable\nlocations. In this paper, we aim to take one step further and explore whether\nthe agent can benefit from generating the potential future view during\nnavigation. Intuitively, humans will have an expectation of how the future\nenvironment will look like, based on the natural language instructions and\nsurrounding views, which will aid correct navigation. Hence, to equip the agent\nwith this ability to generate the semantics of future navigation views, we\nfirst propose three proxy tasks during the agent's in-domain pre-training:\nMasked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action\nPrediction with Image Generation (APIG). These three objectives teach the model\nto predict missing views in a panorama (MPM), predict missing steps in the full\ntrajectory (MTM), and generate the next view based on the full instruction and\nnavigation history (APIG), respectively. We then fine-tune the agent on the VLN\ntask with an auxiliary loss that minimizes the difference between the view\nsemantics generated by the agent and the ground truth view semantics of the\nnext step. Empirically, our VLN-SIG achieves the new state-of-the-art on both\nthe Room-to-Room dataset and the CVDN dataset. We further show that our agent\nlearns to fill in missing patches in future views qualitatively, which brings\nmore interpretability over agents' predicted actions. Lastly, we demonstrate\nthat learning to predict future view semantics also enables the agent to have\nbetter performance on longer paths.", "published": "2023-04-11 00:36:02", "link": "http://arxiv.org/abs/2304.04907v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sim-T: Simplify the Transformer Network by Multiplexing Technique for\n  Speech Recognition", "abstract": "In recent years, a great deal of attention has been paid to the Transformer\nnetwork for speech recognition tasks due to its excellent model performance.\nHowever, the Transformer network always involves heavy computation and large\nnumber of parameters, causing serious deployment problems in devices with\nlimited computation sources or storage memory. In this paper, a new lightweight\nmodel called Sim-T has been proposed to expand the generality of the\nTransformer model. Under the help of the newly developed multiplexing\ntechnique, the Sim-T can efficiently compress the model with negligible\nsacrifice on its performance. To be more precise, the proposed technique\nincludes two parts, that are, module weight multiplexing and attention score\nmultiplexing. Moreover, a novel decoder structure has been proposed to\nfacilitate the attention score multiplexing. Extensive experiments have been\nconducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when\nthe proposed Sim-T is 48% parameter less than the baseline Transformer, 0.4%\nCER improvement can be obtained. Alternatively, 69% parameter reduction can be\nachieved if the Sim-T gives the same performance as the baseline Transformer.\nWith regard to the HKUST and WSJ eval92 datasets, CER and WER will be improved\nby 0.3% and 0.2%, respectively, when parameters in Sim-T are 40% less than the\nbaseline Transformer.", "published": "2023-04-11 05:25:00", "link": "http://arxiv.org/abs/2304.04991v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What Food Do We Tweet about on a Rainy Day?", "abstract": "Food choice is a complex phenomenon shaped by factors such as taste,\nambience, culture or weather. In this paper, we explore food-related tweeting\nin different weather conditions. We inspect a Latvian food tweet dataset\nspanning the past decade in conjunction with a weather observation dataset\nconsisting of average temperature, precipitation, and other phenomena. We find\nwhich weather conditions lead to specific food information sharing;\nautomatically classify tweet sentiment and discuss how it changes depending on\nthe weather. This research contributes to the growing area of large-scale\nsocial network data understanding of food consumers' choices and perceptions.", "published": "2023-04-11 07:57:10", "link": "http://arxiv.org/abs/2304.05041v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Towards Efficient Fine-tuning of Pre-trained Code Models: An\n  Experimental Study and Beyond", "abstract": "Recently, fine-tuning pre-trained code models such as CodeBERT on downstream\ntasks has achieved great success in many software testing and analysis tasks.\nWhile effective and prevalent, fine-tuning the pre-trained parameters incurs a\nlarge computational cost. In this paper, we conduct an extensive experimental\nstudy to explore what happens to layer-wise pre-trained representations and\ntheir encoded code knowledge during fine-tuning. We then propose efficient\nalternatives to fine-tune the large pre-trained code model based on the above\nfindings. Our experimental study shows that (1) lexical, syntactic and\nstructural properties of source code are encoded in the lower, intermediate,\nand higher layers, respectively, while the semantic property spans across the\nentire model. (2) The process of fine-tuning preserves most of the code\nproperties. Specifically, the basic code properties captured by lower and\nintermediate layers are still preserved during fine-tuning. Furthermore, we\nfind that only the representations of the top two layers change most during\nfine-tuning for various downstream tasks. (3) Based on the above findings, we\npropose Telly to efficiently fine-tune pre-trained code models via layer\nfreezing. The extensive experimental results on five various downstream tasks\ndemonstrate that training parameters and the corresponding time cost are\ngreatly reduced, while performances are similar or better. Replication package\nincluding source code, datasets, and online Appendix is available at:\n\\url{https://github.com/DeepSoftwareAnalytics/Telly}.", "published": "2023-04-11 13:34:13", "link": "http://arxiv.org/abs/2304.05216v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Controllable Textual Inversion for Personalized Text-to-Image Generation", "abstract": "The recent large-scale generative modeling has attained unprecedented\nperformance especially in producing high-fidelity images driven by text\nprompts. Text inversion (TI), alongside the text-to-image model backbones, is\nproposed as an effective technique in personalizing the generation when the\nprompts contain user-defined, unseen or long-tail concept tokens. Despite that,\nwe find and show that the deployment of TI remains full of \"dark-magics\" -- to\nname a few, the harsh requirement of additional datasets, arduous human efforts\nin the loop and lack of robustness. In this work, we propose a much-enhanced\nversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving all\nthe aforementioned problems and in turn delivering a robust, data-efficient and\neasy-to-use framework. The core to COTI is a theoretically-guided loss\nobjective instantiated with a comprehensive and novel weighted scoring\nmechanism, encapsulated by an active-learning paradigm. The extensive results\nshow that COTI significantly outperforms the prior TI-related approaches with a\n26.05 decrease in the FID score and a 23.00% boost in the R-precision.", "published": "2023-04-11 14:56:44", "link": "http://arxiv.org/abs/2304.05265v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models", "abstract": "Large language models (LLMs) have shown incredible capabilities and\ntranscended the natural language processing (NLP) community, with adoption\nthroughout many services like healthcare, therapy, education, and customer\nservice. Since users include people with critical information needs like\nstudents or patients engaging with chatbots, the safety of these systems is of\nprime importance. Therefore, a clear understanding of the capabilities and\nlimitations of LLMs is necessary. To this end, we systematically evaluate\ntoxicity in over half a million generations of ChatGPT, a popular\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\nincreases the toxicity of generations. Depending on the persona assigned to\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\nfind concerning patterns where specific entities (e.g., certain races) are\ntargeted more than others (3x more) irrespective of the assigned persona, that\nreflect inherent discriminatory biases in the model. We hope that our findings\ninspire the broader AI community to rethink the efficacy of current safety\nguardrails and develop better techniques that lead to robust, safe, and\ntrustworthy AI systems.", "published": "2023-04-11 16:53:54", "link": "http://arxiv.org/abs/2304.05335v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Resources and Methods for Natural Language Processing of\n  Serbian Language", "abstract": "The Serbian language is a Slavic language spoken by over 12 million speakers\nand well understood by over 15 million people. In the area of natural language\nprocessing, it can be considered a low-resourced language. Also, Serbian is\nconsidered a high-inflectional language. The combination of many word\ninflections and low availability of language resources makes natural language\nprocessing of Serbian challenging. Nevertheless, over the past three decades,\nthere have been a number of initiatives to develop resources and methods for\nnatural language processing of Serbian, ranging from developing a corpus of\nfree text from books and the internet, annotated corpora for classification and\nnamed entity recognition tasks to various methods and models performing these\ntasks. In this paper, we review the initiatives, resources, methods, and their\navailability.", "published": "2023-04-11 19:33:41", "link": "http://arxiv.org/abs/2304.05468v1", "categories": ["cs.CL", "cs.DL", "cs.HC", "A.1"], "primary_category": "cs.CL"}
{"title": "MoMo: A shared encoder Model for text, image and multi-Modal\n  representations", "abstract": "We propose a self-supervised shared encoder model that achieves strong\nresults on several visual, language and multimodal benchmarks while being data,\nmemory and run-time efficient. We make three key contributions. First, in\ncontrast to most existing works, we use a single transformer with all the\nencoder layers processing both the text and the image modalities. Second, we\npropose a stage-wise training strategy where the model is first trained on\nimages, then jointly with unimodal text and image datasets and finally jointly\nwith text and text-image datasets. Third, to preserve information across both\nthe modalities, we propose a training pipeline that learns simultaneously from\ngradient updates of different modalities at each training update step. The\nresults on downstream text-only, image-only and multimodal tasks show that our\nmodel is competitive with several strong models while using fewer parameters\nand lesser pre-training data. For example, MoMo performs competitively with\nFLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasks\ndespite having 2/5th the number of parameters and using 1/3rd the image-text\ntraining pairs. Finally, we ablate various design choices and further show that\nincreasing model size produces significant performance gains indicating\npotential for substantial improvements with larger models using our approach.", "published": "2023-04-11 22:26:10", "link": "http://arxiv.org/abs/2304.05523v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Items and Contexts Understanding with Descriptive Graph for\n  Conversational Recommendation", "abstract": "State-of-the-art methods on conversational recommender systems (CRS) leverage\nexternal knowledge to enhance both items' and contextual words' representations\nto achieve high quality recommendations and responses generation. However, the\nrepresentations of the items and words are usually modeled in two separated\nsemantic spaces, which leads to misalignment issue between them. Consequently,\nthis will cause the CRS to only achieve a sub-optimal ranking performance,\nespecially when there is a lack of sufficient information from the user's\ninput. To address limitations of previous works, we propose a new CRS framework\nKLEVER, which jointly models items and their associated contextual words in the\nsame semantic space. Particularly, we construct an item descriptive graph from\nthe rich items' textual features, such as item description and categories.\nBased on the constructed descriptive graph, KLEVER jointly learns the\nembeddings of the words and items, towards enhancing both recommender and\ndialog generation modules. Extensive experiments on benchmarking CRS dataset\ndemonstrate that KLEVER achieves superior performance, especially when the\ninformation from the users' responses is lacking.", "published": "2023-04-11 21:21:46", "link": "http://arxiv.org/abs/2304.09093v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for\n  Noise-Robust ASR", "abstract": "Automatic speech recognition (ASR) has gained remarkable successes thanks to\nrecent advances of deep learning, but it usually degrades significantly under\nreal-world noisy conditions. Recent works introduce speech enhancement (SE) as\nfront-end to improve speech quality, which is proved effective but may not be\noptimal for downstream ASR due to speech distortion problem. Based on that,\nlatest works combine SE and currently popular self-supervised learning (SSL) to\nalleviate distortion and improve noise robustness. Despite the effectiveness,\nthe speech distortion caused by conventional SE still cannot be cleared out. In\nthis paper, we propose a self-supervised framework named Wav2code to implement\na feature-level SE with reduced distortions for noise-robust ASR. First, in\npre-training stage the clean speech representations from SSL model are sent to\nlookup a discrete codebook via nearest-neighbor feature matching, the resulted\ncode sequence are then exploited to reconstruct the original clean\nrepresentations, in order to store them in codebook as prior. Second, during\nfinetuning we propose a Transformer-based code predictor to accurately predict\nclean codes by modeling the global dependency of input noisy representations,\nwhich enables discovery and restoration of high-quality clean representations\nwith reduced distortions. Furthermore, we propose an interactive feature fusion\nnetwork to combine original noisy and the restored clean representations to\nconsider both fidelity and quality, resulting in more informative features for\ndownstream ASR. Finally, experiments on both synthetic and real noisy datasets\ndemonstrate that Wav2code can solve the speech distortion and improve ASR\nperformance under various noisy conditions, resulting in stronger robustness.", "published": "2023-04-11 04:46:12", "link": "http://arxiv.org/abs/2304.04974v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Soft Dynamic Time Warping for Multi-Pitch Estimation and Beyond", "abstract": "Many tasks in music information retrieval (MIR) involve weakly aligned data,\nwhere exact temporal correspondences are unknown. The connectionist temporal\nclassification (CTC) loss is a standard technique to learn feature\nrepresentations based on weakly aligned training data. However, CTC is limited\nto discrete-valued target sequences and can be difficult to extend to\nmulti-label problems. In this article, we show how soft dynamic time warping\n(SoftDTW), a differentiable variant of classical DTW, can be used as an\nalternative to CTC. Using multi-pitch estimation as an example scenario, we\nshow that SoftDTW yields results on par with a state-of-the-art multi-label\nextension of CTC. In addition to being more elegant in terms of its algorithmic\nformulation, SoftDTW naturally extends to real-valued target sequences.", "published": "2023-04-11 07:39:16", "link": "http://arxiv.org/abs/2304.05032v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Bank: A High-Level Acoustic Signal Representation for Audio Event\n  Recognition", "abstract": "Automatic audio event recognition plays a pivotal role in making human robot\ninteraction more closer and has a wide applicability in industrial automation,\ncontrol and surveillance systems. Audio event is composed of intricate phonic\npatterns which are harmonically entangled. Audio recognition is dominated by\nlow and mid-level features, which have demonstrated their recognition\ncapability but they have high computational cost and low semantic meaning. In\nthis paper, we propose a new computationally efficient framework for audio\nrecognition. Audio Bank, a new high-level representation of audio, is comprised\nof distinctive audio detectors representing each audio class in\nfrequency-temporal space. Dimensionality of the resulting feature vector is\nreduced using non-negative matrix factorization preserving its discriminability\nand rich semantic information. The high audio recognition performance using\nseveral classifiers (SVM, neural network, Gaussian process classification and\nk-nearest neighbors) shows the effectiveness of the proposed method.", "published": "2023-04-11 09:03:07", "link": "http://arxiv.org/abs/2304.05067v1", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard\n  Voting Ensemble Method for Parkinson's Disease", "abstract": "Parkinson's disease (PD) is the most widespread movement condition and the\nsecond most common neurodegenerative disorder, following Alzheimer's. Movement\nsymptoms and imaging techniques are the most popular ways to diagnose this\ndisease. However, they are not accurate and fast and may only be accessible to\na few people. This study provides an autonomous system, i.e., PD-ADSV, for\ndiagnosing PD based on voice signals, which uses four machine learning\nclassifiers and the hard voting ensemble method to achieve the highest\naccuracy. PD-ADSV is developed using Python and the Gradio web framework.", "published": "2023-04-11 17:24:25", "link": "http://arxiv.org/abs/2304.06016v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "AffectMachine-Classical: A novel system for generating affective\n  classical music", "abstract": "This work introduces a new music generation system, called\nAffectMachine-Classical, that is capable of generating affective Classic music\nin real-time. AffectMachine was designed to be incorporated into biofeedback\nsystems (such as brain-computer-interfaces) to help users become aware of, and\nultimately mediate, their own dynamic affective states. That is, this system\nwas developed for music-based MedTech to support real-time emotion\nself-regulation in users. We provide an overview of the rule-based,\nprobabilistic system architecture, describing the main aspects of the system\nand how they are novel. We then present the results of a listener study that\nwas conducted to validate the ability of the system to reliably convey target\nemotions to listeners. The findings indicate that AffectMachine-Classical is\nvery effective in communicating various levels of Arousal ($R^2 = .96$) to\nlisteners, and is also quite convincing in terms of Valence (R^2 = .90). Future\nwork will embed AffectMachine-Classical into biofeedback systems, to leverage\nthe efficacy of the affective music for emotional well-being in listeners.", "published": "2023-04-11 01:06:26", "link": "http://arxiv.org/abs/2304.04915v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.MM", "eess.AS", "J.5; J.4"], "primary_category": "cs.SD"}
