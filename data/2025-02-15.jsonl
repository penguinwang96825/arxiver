{"title": "Light Edge Fault Tolerant Graph Spanners", "abstract": "There has recently been significant interest in fault tolerant spanners,\nwhich are spanners that still maintain their stretch guarantees after some\nnodes or edges fail. This work has culminated in an almost complete\nunderstanding of the three-way tradeoff between stretch, sparsity, and number\nof faults tolerated. However, despite some progress in metric settings, there\nhave been no results to date on the tradeoff in general graphs between stretch,\nlightness, and number of faults tolerated.\n  We initiate the study of light edge fault tolerant (EFT) graph spanners,\nobtaining the first such results. First, we observe that lightness can be\nunbounded if we use the traditional definition (normalizing by the MST). We\nthen argue that a natural definition of fault-tolerant lightness is to instead\nnormalize by a min-weight fault tolerant connectivity preserver; essentially, a\nfault-tolerant version of the MST. However, even with this, we show that it is\nstill not generally possible to construct $f$-EFT spanners whose weight\ncompares reasonably to the weight of a min-weight $f$-EFT connectivity\npreserver.\n  In light of this lower bound, it is natural to then consider bicriteria\nnotions of lightness, where we compare the weight of an $f$-EFT spanner to a\nmin-weight $(f' > f)$-EFT connectivity preserver. The most interesting question\nis to determine the minimum value of $f'$ that allows for reasonable lightness\nupper bounds. Our main result is a precise answer to this question: $f' = 2f$.\nIn particular, we show that the lightness can be untenably large (roughly $n/k$\nfor a $k$-spanner) if one normalizes by the min-weight $(2f-1)$-EFT\nconnectivity preserver. But if one normalizes by the min-weight $2f$-EFT\nconnectivity preserver, then we show that the lightness is bounded by just\n$O(f^{1/2})$ times the non-fault tolerant lightness (roughly $n^{1/k}$, for a\n$(1+\\epsilon)(2k-1)$-spanner).", "published": "2025-02-15 19:46:27", "link": "http://arxiv.org/abs/2502.10890v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "Heterogenous Macro-Finance Model: A Mean-field Game Approach", "abstract": "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.", "published": "2025-02-15 04:21:15", "link": "http://arxiv.org/abs/2502.10666v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Retrieval-augmented Encoders for Extreme Multi-label Text Classification", "abstract": "Extreme multi-label classification (XMC) seeks to find relevant labels from\nan extremely large label collection for a given text input. To tackle such a\nvast label space, current state-of-the-art methods fall into two categories.\nThe one-versus-all (OVA) method uses learnable label embeddings for each label,\nexcelling at memorization (i.e., capturing detailed training signals for\naccurate head label prediction). In contrast, the dual-encoder (DE) model maps\ninput and label text into a shared embedding space for better generalization\n(i.e., the capability of predicting tail labels with limited training data),\nbut may fall short at memorization. To achieve generalization and memorization,\nexisting XMC methods often combine DE and OVA models, which involves complex\ntraining pipelines. Inspired by the success of retrieval-augmented language\nmodels, we propose the Retrieval-augmented Encoders for XMC (RAEXMC), a novel\nframework that equips a DE model with retrieval-augmented capability for\nefficient memorization without additional trainable parameter. During training,\nRAEXMC is optimized by the contrastive loss over a knowledge memory that\nconsists of both input instances and labels. During inference, given a test\ninput, RAEXMC retrieves the top-$K$ keys from the knowledge memory, and\naggregates the corresponding values as the prediction scores. We showcase the\neffectiveness and efficiency of RAEXMC on four public LF-XMC benchmarks. RAEXMC\nnot only advances the state-of-the-art (SOTA) DE method DEXML, but also\nachieves more than 10x speedup on the largest LF-AmazonTitles-1.3M dataset\nunder the same 8 A100 GPUs training environments.", "published": "2025-02-15 00:30:28", "link": "http://arxiv.org/abs/2502.10615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Mixed Telugu-English Hate Speech Detection", "abstract": "Hate speech detection in low-resource languages like Telugu is a growing\nchallenge in NLP. This study investigates transformer-based models, including\nTeluguHateBERT, HateBERT, DeBERTa, Muril, IndicBERT, Roberta, and\nHindi-Abusive-MuRIL, for classifying hate speech in Telugu. We fine-tune these\nmodels using Low-Rank Adaptation (LoRA) to optimize efficiency and performance.\nAdditionally, we explore a multilingual approach by translating Telugu text\ninto English using Google Translate to assess its impact on classification\naccuracy.\n  Our experiments reveal that most models show improved performance after\ntranslation, with DeBERTa and Hindi-Abusive-MuRIL achieving higher accuracy and\nF1 scores compared to training directly on Telugu text. Notably,\nHindi-Abusive-MuRIL outperforms all other models in both the original Telugu\ndataset and the translated dataset, demonstrating its robustness across\ndifferent linguistic settings. This suggests that translation enables models to\nleverage richer linguistic features available in English, leading to improved\nclassification performance. The results indicate that multilingual processing\ncan be an effective approach for hate speech detection in low-resource\nlanguages. These findings demonstrate that transformer models, when fine-tuned\nappropriately, can significantly improve hate speech detection in Telugu,\npaving the way for more robust multilingual NLP applications.", "published": "2025-02-15 02:03:13", "link": "http://arxiv.org/abs/2502.10632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in the Passage: Passage-level In-context Learning Does Not\n  Necessarily Need a \"Passage\"", "abstract": "By simply incorporating demonstrations into the context, in-context learning\n(ICL) enables large language models (LLMs) to yield awesome performance on many\ntasks. In this paper, we focus on passage-level long-context ICL for generation\ntasks and find that LLMs cannot learn the intrinsic relationships between the\ndemonstration passage and the generation output. We conduct experiments with\ndifferent LLMs on two typical generation tasks including single-document QA and\ndistractor generation, demonstrating that even a completely meaningless\ndemonstration passage with 1/4 length achieves much better performance than the\noriginal full passage. Analysis via attention score reveals that LLMs pay\nlittle attention to passages compared to other components in prompt and little\nattention flows from the passage to other parts of the demonstration, which\nfurther confirms our finding. Additionally, experiments on context compression\nindicate that compression approaches proven effective on other long-context\ntasks are not suitable for passage-level ICL, since simply using shorter\nmeaningless demonstration passages has achieved competitive performance.", "published": "2025-02-15 02:11:19", "link": "http://arxiv.org/abs/2502.10634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate\n  Public Perceptions of Health Resource Accessibility", "abstract": "Access to health resources is a critical determinant of public well-being and\nsocietal resilience, particularly during public health crises when demand for\nmedical services and preventive care surges. However, disparities in\naccessibility persist across demographic and geographic groups, raising\nconcerns about equity. Traditional survey methods often fall short due to\nlimitations in coverage, cost, and timeliness. This study leverages\ncrowdsourced data from Google Maps reviews, applying advanced natural language\nprocessing techniques, specifically ModernBERT, to extract insights on public\nperceptions of health resource accessibility in the United States during the\nCOVID-19 pandemic. Additionally, we employ Partial Least Squares regression to\nexamine the relationship between accessibility perceptions and key\nsocioeconomic and demographic factors including political affiliation, racial\ncomposition, and educational attainment. Our findings reveal that public\nperceptions of health resource accessibility varied significantly across the\nU.S., with disparities peaking during the pandemic and slightly easing\npost-crisis. Political affiliation, racial demographics, and education levels\nemerged as key factors shaping these perceptions. These findings underscore the\nneed for targeted interventions and policy measures to address inequities,\nfostering a more inclusive healthcare infrastructure that can better withstand\nfuture public health challenges.", "published": "2025-02-15 02:34:55", "link": "http://arxiv.org/abs/2502.10641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BabyLM Turns 3: Call for papers for the 2025 BabyLM workshop", "abstract": "BabyLM aims to dissolve the boundaries between cognitive modeling and\nlanguage modeling. We call for both workshop papers and for researchers to join\nthe 3rd BabyLM competition. As in previous years, we call for participants in\nthe data-efficient pretraining challenge in the general track. This year, we\nalso offer a new track: INTERACTION. This new track encourages interactive\nbehavior, learning from a teacher, and adapting the teaching material to the\nstudent. We also call for papers outside the competition in any relevant areas.\nThese include training efficiency, cognitively plausible research, weak model\nevaluation, and more.", "published": "2025-02-15 02:46:43", "link": "http://arxiv.org/abs/2502.10645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Profile with Large Language Models: Construction, Updating, and\n  Benchmarking", "abstract": "User profile modeling plays a key role in personalized systems, as it\nrequires building accurate profiles and updating them with new information. In\nthis paper, we present two high-quality open-source user profile datasets: one\nfor profile construction and another for profile updating. These datasets offer\na strong basis for evaluating user profile modeling techniques in dynamic\nsettings. We also show a methodology that uses large language models (LLMs) to\ntackle both profile construction and updating. Our method uses a probabilistic\nframework to predict user profiles from input text, allowing for precise and\ncontext-aware profile generation. Our experiments demonstrate that models like\nMistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the\nprecision and recall of the generated profiles, and high evaluation scores\nconfirm the effectiveness of our approach.", "published": "2025-02-15 03:57:52", "link": "http://arxiv.org/abs/2502.10660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Domain-Specific Knowledge into Large Language Models: A\n  Comprehensive Survey", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various\ntasks such as natural language understanding, text summarization, and machine\ntranslation. However, their general-purpose nature often limits their\neffectiveness in domain-specific applications that require specialized\nknowledge, such as healthcare, chemistry, or legal analysis. To address this,\nresearchers have explored diverse methods to enhance LLMs by integrating\ndomain-specific knowledge. In this survey, we provide a comprehensive overview\nof these methods, which we categorize into four key approaches: dynamic\nknowledge injection, static knowledge embedding, modular adapters, and prompt\noptimization. Each approach offers unique mechanisms to equip LLMs with domain\nexpertise, balancing trade-offs between flexibility, scalability, and\nefficiency. We discuss how these methods enable LLMs to tackle specialized\ntasks, compare their advantages and disadvantages, evaluate domain-specific\nLLMs against general LLMs, and highlight the challenges and opportunities in\nthis emerging field. For those interested in delving deeper into this area, we\nalso summarize the commonly used datasets and benchmarks. To keep researchers\nupdated on the latest studies, we maintain an open-source at:\nhttps://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to\ndocumenting research in the field of specialized LLM.", "published": "2025-02-15 07:43:43", "link": "http://arxiv.org/abs/2502.10708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language\n  Models via Evolutionary Optimization", "abstract": "Post-training pruning has emerged as a crucial optimization technique as\nlarge language models (LLMs) continue to grow rapidly. However, the significant\nvariations in weight distributions across different LLMs make fixed pruning\nstrategies inadequate for multiple models. In this paper, we introduce\n\\textbf{\\textsc{OptiShear}}, an efficient evolutionary optimization framework\nfor adaptive LLM pruning. Our framework features two key innovations: an\neffective search space built on our Meta pruning metric to handle diverse\nweight distributions, and a model-wise reconstruction error for rapid\nevaluation during search trials. We employ Non-dominated Sorting Genetic\nAlgorithm III (NSGA-III) to optimize both pruning metrics and layerwise\nsparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models\n(7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning\nmetrics consistently outperform existing methods. Additionally, our discovered\nlayerwise sparsity ratios enhance the effectiveness of other pruning metrics.\nThe framework exhibits strong cross-task and cross-model generalizability,\nproviding a cost-effective solution for model compression.", "published": "2025-02-15 09:17:38", "link": "http://arxiv.org/abs/2502.10735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BASE-SQL: A powerful open source Text-To-SQL baseline approach", "abstract": "The conversion of natural language into SQL language for querying databases\n(Text-to-SQL) has broad application prospects and has attracted widespread\nattention. At present, the mainstream Text-to-SQL methods are mainly divided\ninto in-context learning (ICL) based methods and supervised fine-tuning (SFT)\nbased methods. ICL-based methods can achieve relatively good results thanks to\nthe use of the most advanced closed-source models. However, in real-world\napplication scenarios, factors such as data privacy, SQL generation efficiency\nand cost need to be considered. SFT-based methods have certain advantages. At\npresent, methods based on fine-tuning of open source models lack\neasy-to-implement and effective (cost-effective) baseline methods. We propose a\npipeline-based method using open source model fine-tuning, referred to as\nBASE-SQL, which includes four components: Schema Linking, Candidate SQL\nGenerate, SQL Revision and SQL Merge Revision. Experimental results show that\nBASE-SQL uses the open source model Qwen2.5-Coder-32B-Instruct, and achieves an\naccuracy of 67.47% on the BIRD development set and 88.9% on the Spider test\nset, which is significantly better than other methods using open source models,\nand even exceeds several methods using the GPT-4o closed-source model. At the\nsame time, BASE-SQL is easy to implement and highly efficient (on average, only\nfive calls to the large language model are required to generate SQL once). The\ncode will be open sourced at https://github.com/CycloneBoy/base_sql.", "published": "2025-02-15 09:23:37", "link": "http://arxiv.org/abs/2502.10739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "abstract": "Recent advances in large language models have led to specialized models\nexcelling in specific domains, creating a need for efficient model merging\ntechniques. While traditional merging approaches combine parameters into a\nsingle static model, they often compromise task-specific performance. However,\ntask-specific routing methods maintain accuracy but introduce substantial\nstorage overhead. We present \\texttt{1bit}-Merging, a novel framework that\nintegrates task-specific routing with 1-bit quantized task vectors to balance\nperformance and storage efficiency. Our approach leverages the observation that\ndifferent task-specific models store knowledge in distinct layers-chat models\nprimarily in attention layers and math/code models in MLP layers-enabling\ntargeted compression strategies. Through extensive experiments with LLaMA2 and\nMistral model families across chat, mathematical reasoning, and code generation\ntasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or\nsuperior performance to existing methods while significantly reducing storage\nrequirements. Our framework offers a practical solution for combining\nspecialized models while maintaining their individual strengths and addressing\nthe storage challenges of current approaches.", "published": "2025-02-15 09:47:50", "link": "http://arxiv.org/abs/2502.10743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large\n  Language Models", "abstract": "We investigate how large language models perform latent multi-hop reasoning\nin prompts like \"Wolfgang Amadeus Mozart's mother's spouse is\". To analyze this\nprocess, we introduce logit flow, an interpretability method that traces how\nlogits propagate across layers and positions toward the final prediction. Using\nlogit flow, we identify four distinct stages in single-hop knowledge\nprediction: (A) entity subject enrichment, (B) entity attribute extraction, (C)\nrelation subject enrichment, and (D) relation attribute extraction. Extending\nthis analysis to multi-hop reasoning, we find that failures often stem from the\nrelation attribute extraction stage, where conflicting logits reduce prediction\naccuracy. To address this, we propose back attention, a novel mechanism that\nenables lower layers to leverage higher-layer hidden states from different\npositions during attention computation. With back attention, a 1-layer\ntransformer achieves the performance of a 2-layer transformer. Applied to four\nLLMs, back attention improves accuracy on five reasoning datasets,\ndemonstrating its effectiveness in enhancing latent multi-hop reasoning\nability.", "published": "2025-02-15 15:36:42", "link": "http://arxiv.org/abs/2502.10835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective Extraction and Evaluation of Factual Claims", "abstract": "A common strategy for fact-checking long-form content generated by Large\nLanguage Models (LLMs) is extracting simple claims that can be verified\nindependently. Since inaccurate or incomplete claims compromise fact-checking\nresults, ensuring claim quality is critical. However, the lack of a\nstandardized evaluation framework impedes assessment and comparison of claim\nextraction methods. To address this gap, we propose a framework for evaluating\nclaim extraction in the context of fact-checking along with automated,\nscalable, and replicable methods for applying this framework, including novel\napproaches for measuring coverage and decontextualization. We also introduce\nClaimify, an LLM-based claim extraction method, and demonstrate that it\noutperforms existing methods under our evaluation framework. A key feature of\nClaimify is its ability to handle ambiguity and extract claims only when there\nis high confidence in the correct interpretation of the source text.", "published": "2025-02-15 16:58:05", "link": "http://arxiv.org/abs/2502.10855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration\n  System for Electronic Design Automation", "abstract": "Recently, with the development of tool-calling capabilities in large language\nmodels (LLMs), these models have demonstrated significant potential for\nautomating electronic design automation (EDA) flows by interacting with EDA\ntool APIs via EDA scripts. However, considering the limited understanding of\nEDA tools, LLMs face challenges in practical scenarios where diverse interfaces\nof EDA tools exist across different platforms. Additionally, EDA flow\nautomation often involves intricate, long-chain tool-calling processes,\nincreasing the likelihood of errors in intermediate steps. Any errors will lead\nto the instability and failure of EDA flow automation. To address these\nchallenges, we introduce EDAid, a multi-agent collaboration system where\nmultiple agents harboring divergent thoughts converge towards a common goal,\nensuring reliable and successful EDA flow automation. Specifically, each agent\nis controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA\nflow automation. Our experiments demonstrate the state-of-the-art (SOTA)\nperformance of our ChipLlama models and validate the effectiveness of our EDAid\nin the automation of complex EDA flows, showcasing superior performance\ncompared to single-agent systems.", "published": "2025-02-15 16:59:29", "link": "http://arxiv.org/abs/2502.10857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering", "abstract": "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly.", "published": "2025-02-15 17:52:14", "link": "http://arxiv.org/abs/2502.10868v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CiteCheck: Towards Accurate Citation Faithfulness Detection", "abstract": "Citation faithfulness detection is critical for enhancing retrieval-augmented\ngeneration (RAG) systems, yet large-scale Chinese datasets for this task are\nscarce. Existing methods face prohibitive costs due to the need for manually\nannotated negative samples. To address this, we introduce the first large-scale\nChinese dataset CiteCheck for citation faithfulness detection, constructed via\na cost-effective approach using two-stage manual annotation. This method\nbalances positive and negative samples while significantly reducing annotation\nexpenses. CiteCheck comprises training and test splits. Experiments demonstrate\nthat: (1) the test samples are highly challenging, with even state-of-the-art\nLLMs failing to achieve high accuracy; and (2) training data augmented with\nLLM-generated negative samples enables smaller models to attain strong\nperformance using parameter-efficient fine-tuning. CiteCheck provides a robust\nfoundation for advancing citation faithfulness detection in Chinese RAG\nsystems. The dataset is publicly available to facilitate research.", "published": "2025-02-15 18:48:39", "link": "http://arxiv.org/abs/2502.10881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of\n  Vision-Language and Reasoning Models", "abstract": "Entity tracking is a fundamental challenge in natural language understanding,\nrequiring models to maintain coherent representations of entities. Previous\nwork has benchmarked entity tracking performance in purely text-based tasks. We\nintroduce MET-Bench, a multimodal entity tracking benchmark designed to\nevaluate the ability of vision-language models to track entity states across\nmodalities. Using two structured domains, Chess and the Shell Game, we assess\nhow effectively current models integrate textual and image-based state updates.\nOur findings reveal a significant performance gap between text-based and\nimage-based tracking and that this performance gap stems from deficits in\nvisual reasoning rather than perception. We further show that explicit\ntext-based reasoning strategies improve performance, yet substantial\nlimitations remain, especially in long-horizon multimodal scenarios. Our\nresults highlight the need for improved multimodal representations and\nreasoning techniques to bridge the gap between textual and visual entity\ntracking.", "published": "2025-02-15 19:39:58", "link": "http://arxiv.org/abs/2502.10886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing Conversational Speech Systems for Robots to Detect Speech\n  Biomarkers of Cognition in People Living with Dementia", "abstract": "This study presents the development and testing of a conversational speech\nsystem designed for robots to detect speech biomarkers indicative of cognitive\nimpairments in people living with dementia (PLwD). The system integrates a\nbackend Python WebSocket server and a central core module with a large language\nmodel (LLM) fine-tuned for dementia to process user input and generate robotic\nconversation responses in real-time in less than 1.5 seconds. The frontend user\ninterface, a Progressive Web App (PWA), displays information and biomarker\nscore graphs on a smartphone in real-time to human users (PLwD, caregivers,\nclinicians). Six speech biomarkers based on the existing literature - Altered\nGrammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking, Slurred\nPronunciation, and Prosody Changes - were developed for the robot conversation\nsystem using two datasets, one that included conversations of PLwD with a human\nclinician (DementiaBank dataset) and one that included conversations of PLwD\nwith a robot (Indiana dataset). We also created a composite speech biomarker\nthat combined all six individual biomarkers into a single score. The speech\nsystem's performance was first evaluated on the DementiaBank dataset showing\nmoderate correlation with MMSE scores, with the composite biomarker score\noutperforming individual biomarkers. Analysis of the Indiana dataset revealed\nhigher and more variable biomarker scores, suggesting potential differences due\nto study populations (e.g. severity of dementia) and the conversational\nscenario (human-robot conversations are different from human-human). The\nfindings underscore the need for further research on the impact of\nconversational scenarios on speech biomarkers and the potential clinical\napplications of robotic speech systems.", "published": "2025-02-15 20:25:58", "link": "http://arxiv.org/abs/2502.10896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fundamental Principles of Linguistic Structure are Not Represented by o3", "abstract": "A core component of a successful artificial general intelligence would be the\nrapid creation and manipulation of grounded compositional abstractions and the\ndemonstration of expertise in the family of recursive hierarchical syntactic\nobjects necessary for the creative use of human language. We evaluated the\nrecently released o3 model (OpenAI; o3-mini-high) and discovered that while it\nsucceeds on some basic linguistic tests relying on linear, surface statistics\n(e.g., the Strawberry Test), it fails to generalize basic phrase structure\nrules; it fails with comparative sentences involving semantically illegal\ncardinality comparisons ('Escher sentences'); its fails to correctly rate and\nexplain acceptability dynamics; and it fails to distinguish between\ninstructions to generate unacceptable semantic vs. unacceptable syntactic\noutputs. When tasked with generating simple violations of grammatical rules, it\nis seemingly incapable of representing multiple parses to evaluate against\nvarious possible semantic interpretations. In stark contrast to many recent\nclaims that artificial language models are on the verge of replacing the field\nof linguistics, our results suggest not only that deep learning is hitting a\nwall with respect to compositionality (Marcus 2022), but that it is hitting [a\n[stubbornly [resilient wall]]] that cannot readily be surmounted to reach\nhuman-like compositional reasoning simply through more compute.", "published": "2025-02-15 23:53:31", "link": "http://arxiv.org/abs/2502.10934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs", "abstract": "Retrieval-Augmented Generation (RAG) has become an effective method for\nenhancing large language models (LLMs) with up-to-date knowledge. However, it\nposes a significant risk of IP infringement, as IP datasets may be incorporated\ninto the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs)\nwithout authorization. To protect the rights of the dataset owner, an effective\ndataset membership inference algorithm for RA-LLMs is needed. In this work, we\nintroduce a novel approach to safeguard the ownership of text datasets and\neffectively detect unauthorized use by the RA-LLMs. Our approach preserves the\noriginal data completely unchanged while protecting it by inserting\nspecifically designed canary documents into the IP dataset. These canary\ndocuments are created with synthetic content and embedded watermarks to ensure\nuniqueness, stealthiness, and statistical provability. During the detection\nprocess, unauthorized usage is identified by querying the canary documents and\nanalyzing the responses of RA-LLMs for statistical evidence of the embedded\nwatermark. Our experimental results demonstrate high query efficiency,\ndetectability, and stealthiness, along with minimal perturbation to the\noriginal dataset, all without compromising the performance of the RAG system.", "published": "2025-02-15 04:56:45", "link": "http://arxiv.org/abs/2502.10673v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations", "abstract": "As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.", "published": "2025-02-15 07:45:20", "link": "http://arxiv.org/abs/2502.10709v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PropNet: a White-Box and Human-Like Network for Sentence Representation", "abstract": "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks.", "published": "2025-02-15 08:28:58", "link": "http://arxiv.org/abs/2502.10725v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model\n  Merging", "abstract": "While most current approaches rely on further training techniques, such as\nfine-tuning or reinforcement learning, to enhance model capacities, model\nmerging stands out for its ability of improving models without requiring any\nadditional training. In this paper, we propose a unified framework for model\nmerging based on low-rank estimation of task vectors without the need for\naccess to the base model, named \\textsc{LoRE-Merging}. Our approach is\nmotivated by the observation that task vectors from fine-tuned models\nfrequently exhibit a limited number of dominant singular values, making\nlow-rank estimations less prone to interference. We implement the method by\nformulating the merging problem as an optimization problem. Extensive empirical\nexperiments demonstrate the effectiveness of our framework in mitigating\ninterference and preserving task-specific information, thereby advancing the\nstate-of-the-art performance in model merging techniques.", "published": "2025-02-15 10:18:46", "link": "http://arxiv.org/abs/2502.10749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Encoder Knows more than You Realize: Shared Weights\n  Pretraining for Extremely Low-Resource Languages", "abstract": "While multilingual language models like XLM-R have advanced multilingualism\nin NLP, they still perform poorly in extremely low-resource languages. This\nsituation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen\nsupport far fewer languages than XLM-R, making text generation models\nnon-existent for many languages in the world. To tackle this challenge, we\npropose a novel framework for adapting multilingual encoders to text generation\nin extremely low-resource languages. By reusing the weights between the encoder\nand the decoder, our framework allows the model to leverage the learned\nsemantic space of the encoder, enabling efficient learning and effective\ngeneralization in low-resource languages. Applying this framework to four\nChinese minority languages, we present XLM-SWCM, and demonstrate its superior\nperformance on various downstream tasks even when compared with much larger\nmodels.", "published": "2025-02-15 16:53:10", "link": "http://arxiv.org/abs/2502.10852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs", "abstract": "Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle\ncomplex tasks by progressively activating relevant pre-trained knowledge.\nHowever, it faces challenges in ensuring continual improvement and determining\na stopping criterion. In this paper, we investigate whether the relevant\nknowledge that contributes directly to solving the given question can be\nactivated from the initial reasoning path, thus circumventing the need for\niterative refinement. Our experiments reveal that increasing the diversity of\ninitial reasoning paths can achieve comparable or superior performance, a\nconcept we term \\textit{breadth reasoning}. However, existing breadth reasoning\napproaches, such as self-consistency, offer limited diversity. To address this\nlimitation, we propose a simple yet effective method that enhances reasoning\nbreadth by integrating contextual exploration with reduced sampling randomness.\nExtensive experiments demonstrate that our approach significantly outperforms\ndeep iterative reasoning. Our code is provided in\nhttps://github.com/zongqianwu/breadth.", "published": "2025-02-15 16:59:59", "link": "http://arxiv.org/abs/2502.10858v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1", "abstract": "OpenAI o1 has shown that applying reinforcement learning to integrate\nreasoning steps directly during inference can significantly improve a model's\nreasoning capabilities. This result is exciting as the field transitions from\nthe conventional autoregressive method of generating answers to a more\ndeliberate approach that models the slow-thinking process through step-by-step\nreasoning training. Reinforcement learning plays a key role in both the model's\ntraining and decoding processes. In this article, we present a comprehensive\nformulation of reasoning problems and investigate the use of both model-based\nand model-free approaches to better support this slow-thinking framework.", "published": "2025-02-15 17:52:11", "link": "http://arxiv.org/abs/2502.10867v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Open-Source Web-Based Tool for Evaluating Open-Source Large Language\n  Models Leveraging Information Retrieval from Custom Documents", "abstract": "In our work, we present the first-of-its-kind open-source web-based tool\nwhich is able to demonstrate the impacts of a user's speech act during\ndiscourse with conversational agents, which leverages open-source large\nlanguage models. With this software resource, it is possible for researchers\nand experts to evaluate the performance of various dialogues, visualize the\nuser's communicative intents, and utilise uploaded specific documents for the\nchat agent to use for its information retrieval to respond to the user query.\nThe context gathered by these models is obtained from a set of linguistic\nfeatures extracted, which forms the context embeddings of the models.\nRegardless of these models showing good context understanding based on these\nfeatures, there still remains a gap in including deeper pragmatic features to\nimprove the model's comprehension of the query, hence the efforts to develop\nthis web resource, which is able to extract and then inject this overlooked\nfeature in the encoder-decoder pipeline of the conversational agent. To\ndemonstrate the effect and impact of the resource, we carried out an experiment\nwhich evaluated the system using 2 knowledge files for information retrieval,\nwith two user queries each, across 5 open-source large language models using 10\nstandard metrics. Our results showed that larger open-source models,\ndemonstrated an improved alignment when the user speech act was included with\ntheir query. The smaller models in contrast showed an increased perplexity and\nmixed performance, which explicitly indicated struggles in processing queries\nthat explicitly included speech acts. The results from the analysis using the\ndeveloped web resource highlight the potential of speech acts towards enhancing\nconversational depths while underscoring the need for model-specific\noptimizations to address increased computational costs and response times.", "published": "2025-02-15 22:08:53", "link": "http://arxiv.org/abs/2502.10916v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evolving Hate Speech Online: An Adaptive Framework for Detection and\n  Mitigation", "abstract": "The proliferation of social media platforms has led to an increase in the\nspread of hate speech, particularly targeting vulnerable communities.\nUnfortunately, existing methods for automatically identifying and blocking\ntoxic language rely on pre-constructed lexicons, making them reactive rather\nthan adaptive. As such, these approaches become less effective over time,\nespecially when new communities are targeted with slurs not included in the\noriginal datasets. To address this issue, we present an adaptive approach that\nuses word embeddings to update lexicons and develop a hybrid model that adjusts\nto emerging slurs and new linguistic patterns. This approach can effectively\ndetect toxic language, including intentional spelling mistakes employed by\naggressors to avoid detection. Our hybrid model, which combines BERT with\nlexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art\ndatasets. Our work has significant implications for creating safer online\nenvironments by improving the detection of toxic content and proactively\nupdating the lexicon. Content Warning: This paper contains examples of hate\nspeech that may be triggering.", "published": "2025-02-15 22:46:50", "link": "http://arxiv.org/abs/2502.10921v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Extrapolative Modeling of Manufacturing\n  Processes", "abstract": "Conventional predictive modeling of parametric relationships in manufacturing\nprocesses is limited by the subjectivity of human expertise and intuition on\nthe one hand and by the cost and time of experimental data generation on the\nother hand. This work addresses this issue by establishing a new Large Language\nModel (LLM) framework. The novelty lies in combining automatic extraction of\nprocess-relevant knowledge embedded in the literature with iterative model\nrefinement based on a small amount of experimental data. This approach is\nevaluated on three distinct manufacturing processes that are based on\nmachining, deformation, and additive principles. The results show that for the\nsame small experimental data budget the models derived by our framework have\nunexpectedly high extrapolative performance, often surpassing the capabilities\nof conventional Machine Learning. Further, our approach eliminates manual\ngeneration of initial models or expertise-dependent interpretation of the\nliterature. The results also reveal the importance of the nature of the\nknowledge extracted from the literature and the significance of both the\nknowledge extraction and model refinement components.", "published": "2025-02-15 02:43:22", "link": "http://arxiv.org/abs/2502.12185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-supervised Attribute-aware Dynamic Preference Ranking Alignment", "abstract": "Reinforcement Learning from Human Feedback and its variants excel in aligning\nwith human intentions to generate helpful, harmless, and honest responses.\nHowever, most of them rely on costly human-annotated pairwise comparisons for\nsupervised alignment, which is not suitable for list-level scenarios, such as\ncommunity question answering. Additionally, human preferences are influenced by\nmultiple intrinsic factors in responses, leading to decision-making\ninconsistencies. Therefore, we propose \\textbf{Se}lf-supervised\n\\textbf{A}ttribute-aware \\textbf{d}ynamic \\textbf{p}reference \\textbf{ra}nking,\ncalled \\shortname. \\ It quantifies preference differences between responses\nbased on Attribute-Perceptual Distance Factors (APDF) and dynamically\ndetermines the list-wise alignment order. Furthermore, it achieves fine-grained\npreference difference learning and enables precise alignment with the optimal\none. We specifically constructed a challenging code preference dataset named\nStaCoCoQA, and introduced more cost-effective and scalable preference\nevaluation metrics: PrefHit and PrefRecall. Extensive experimental results show\nthat SeAdpra exhibits superior performance and generalizability on both\nStaCoCoQA and preference datasets from eight popular domains.", "published": "2025-02-15 08:20:42", "link": "http://arxiv.org/abs/2502.12189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI and the Law: Evaluating ChatGPT's Performance in Legal Classification", "abstract": "The use of ChatGPT to analyze and classify evidence in criminal proceedings\nhas been a topic of ongoing discussion. However, to the best of our knowledge,\nthis issue has not been studied in the context of the Polish language. This\nstudy addresses this research gap by evaluating the effectiveness of ChatGPT in\nclassifying legal cases under the Polish Penal Code. The results show excellent\nbinary classification accuracy, with all positive and negative cases correctly\ncategorized. In addition, a qualitative evaluation confirms that the legal\nbasis provided for each case, along with the relevant legal content, was\nappropriate. The results obtained suggest that ChatGPT can effectively analyze\nand classify evidence while applying the appropriate legal rules. In\nconclusion, ChatGPT has the potential to assist interested parties in the\nanalysis of evidence and serve as a valuable legal resource for individuals\nwith less experience or knowledge in this area.", "published": "2025-02-15 09:28:52", "link": "http://arxiv.org/abs/2502.12193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Closer Look at System Prompt Robustness", "abstract": "System prompts have emerged as a critical control surface for specifying the\nbehavior of LLMs in chat and agent settings. Developers depend on system\nprompts to specify important context, output format, personalities, guardrails,\ncontent policies, and safety countermeasures, all of which require models to\nrobustly adhere to the system prompt, especially when facing conflicting or\nadversarial user inputs. In practice, models often forget to consider relevant\nguardrails or fail to resolve conflicting demands between the system and the\nuser. In this work, we study various methods for improving system prompt\nrobustness by creating realistic new evaluation and fine-tuning datasets based\non prompts collected from from OpenAI's GPT Store and HuggingFace's\nHuggingChat. Our experiments assessing models with a panel of new and existing\nbenchmarks show that performance can be considerably improved with realistic\nfine-tuning data, as well as inference-time interventions such as\nclassifier-free guidance. Finally, we analyze the results of recently released\nreasoning models from OpenAI and DeepSeek, which show exciting but uneven\nimprovements on the benchmarks we study. Overall, current techniques fall short\nof ensuring system prompt robustness and further study is warranted.", "published": "2025-02-15 18:10:45", "link": "http://arxiv.org/abs/2502.12197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach\n  to Contextual Memory Integration", "abstract": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling.", "published": "2025-02-15 07:06:10", "link": "http://arxiv.org/abs/2502.10699v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Why is prompting hard? Understanding prompts on binary sequence\n  predictors", "abstract": "Large language models (LLMs) can be prompted to do many tasks, but finding\ngood prompts is not always easy, nor is understanding some performant prompts.\nWe explore these issues by viewing prompting as conditioning a near-optimal\nsequence predictor (LLM) pretrained on diverse data sources. Through numerous\nprompt search experiments, we show that the unintuitive patterns in optimal\nprompts can be better understood given the pretraining distribution, which is\noften unavailable in practice. Moreover, even using exhaustive search, reliably\nidentifying optimal prompts from practical neural predictors can be difficult.\nFurther, we demonstrate that common prompting methods, such as using intuitive\nprompts or samples from the targeted task, are in fact suboptimal. Thus, this\nwork takes an initial step towards understanding the difficulties in finding\nand understanding optimal prompts from a statistical and empirical perspective.", "published": "2025-02-15 10:55:47", "link": "http://arxiv.org/abs/2502.10760v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable\n  Multi-Objective Generation", "abstract": "User information needs are often highly diverse and varied. A key challenge\nin current research is how to achieve controllable multi-objective generation\nwhile enabling rapid adaptation to accommodate diverse user demands during test\ntime. Existing solutions, such as Rewarded Soup, focus on merging language\nmodels individually tuned on single objectives. While easy to implement and\nwidely used, these approaches face limitations in achieving optimal performance\ndue to their disregard for the impacts of competing objectives on model tuning.\nTo address this issue, we propose Bone Soup, a novel model merging approach\nthat first seeks a series of backbone models by considering the impacts of\nmultiple objectives and then makes the soup (i.e., merge the backbone models).\nSpecifically, Bone Soup begins by training multiple backbone models for\ndifferent objectives using multi-objective reinforcement learning. Each\nbackbone model is guided by a combination of backbone reward signals. To ensure\nthat these models are optimal for the Pareto front, the backbone rewards are\ncrafted by combining standard reward functions into basis vectors, which can\nthen be modified through a rule-based construction method. Bone Soup leverages\na symmetric circulant matrix mapping to generate the merging coefficients,\nwhich are used to merge the backbone models according to user preferences.\nExtensive experimental results demonstrate that Bone Soup exhibits strong\ncontrollability and Pareto optimality in controllable multi-objective\ngeneration, providing a more effective and efficient approach to addressing\ndiverse user needs at test time.", "published": "2025-02-15 11:00:36", "link": "http://arxiv.org/abs/2502.10762v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating improvements on using Large Language Models (LLMs) for\n  property extraction in the Open Research Knowledge Graph (ORKG)", "abstract": "Current research highlights the great potential of Large Language Models\n(LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly\ncomplex step in this process is relation extraction, aimed at identifying\nsuitable properties to describe the content of research. This study builds\ndirectly on previous research of three Open Research Knowledge Graph (ORKG)\nteam members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and\nMistral for property extraction in scientific literature. Given the moderate\nperformance observed, the previous work concluded that fine-tuning is needed to\nimprove these models' alignment with scientific tasks and their emulation of\nhuman expertise. Expanding on this prior experiment, this study evaluates the\nimpact of advanced prompt engineering techniques and demonstrates that these\ntechniques can highly significantly enhance the results. Additionally, this\nstudy extends the property extraction process to include property matching to\nexisting ORKG properties, which are retrieved via the API. The evaluation\nreveals that results generated through advanced prompt engineering achieve a\nhigher proportion of matches with ORKG properties, further emphasizing the\nenhanced alignment achieved. Moreover, this lays the groundwork for addressing\nchallenges such as the inconsistency of ORKG properties, an issue highlighted\nin prior studies. By assigning unique URIs and using standardized terminology,\nthis work increases the consistency of the properties, fulfilling a crucial\naspect of Linked Data and FAIR principles - core commitments of ORKG. This, in\nturn, significantly enhances the applicability of ORKG content for subsequent\ntasks such as comparisons of research publications. Finally, the study\nconcludes with recommendations for future improvements in the overall property\nextraction process.", "published": "2025-02-15 11:17:37", "link": "http://arxiv.org/abs/2502.10768v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Representation and Recall of Interwoven Structured Knowledge in\n  LLMs: A Geometric and Layered Analysis", "abstract": "This study investigates how large language models (LLMs) represent and recall\nmulti-associated attributes across transformer layers. We show that\nintermediate layers encode factual knowledge by superimposing related\nattributes in overlapping spaces, along with effective recall even when\nattributes are not explicitly prompted. In contrast, later layers refine\nlinguistic patterns and progressively separate attribute representations,\noptimizing task-specific outputs while appropriately narrowing attribute\nrecall. We identify diverse encoding patterns including, for the first time,\nthe observation of 3D spiral structures when exploring information related to\nthe periodic table of elements. Our findings reveal a dynamic transition in\nattribute representations across layers, contributing to mechanistic\ninterpretability and providing insights for understanding how LLMs handle\ncomplex, interrelated knowledge.", "published": "2025-02-15 18:08:51", "link": "http://arxiv.org/abs/2502.10871v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek\n  R1 Expert Specialization", "abstract": "DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has\ndemonstrated reasoning capabilities comparable to proprietary frontier models.\nPrior research has explored expert routing in MoE models, but findings suggest\nthat expert selection is often token-dependent rather than semantically driven.\nGiven DeepSeek-R1's enhanced reasoning abilities, we investigate whether its\nrouting mechanism exhibits greater semantic specialization than previous MoE\nmodels. To explore this, we conduct two key experiments: (1) a word sense\ndisambiguation task, where we examine expert activation patterns for words with\ndiffering senses, and (2) a cognitive reasoning analysis, where we assess\nDeepSeek-R1's structured thought process in an interactive task setting of\nDiscoveryWorld. We conclude that DeepSeek-R1's routing mechanism is more\nsemantically aware and it engages in structured cognitive processes.", "published": "2025-02-15 23:37:32", "link": "http://arxiv.org/abs/2502.10928v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hallucinations are inevitable but statistically negligible", "abstract": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, a recent\nstudy established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. In contrast, we present a positive\ntheoretical result from a probabilistic perspective. Specifically, we prove\nthat hallucinations can be made statistically negligible, provided that the\nquality and quantity of the training data are sufficient. Interestingly, our\npositive result coexists with the computability-theoretic result, implying that\nwhile hallucinations on an infinite set of inputs cannot be entirely\neliminated, their probability can always be reduced by improving algorithms and\ntraining data. By evaluating the two seemingly contradictory results through\nthe lens of information theory, we argue that our probability-theoretic\npositive result better reflects practical considerations than the\ncomputability-theoretic negative result.", "published": "2025-02-15 07:28:40", "link": "http://arxiv.org/abs/2502.12187v1", "categories": ["cs.CL", "cs.FL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.CL"}
{"title": "Hyperdimensional Intelligent Sensing for Efficient Real-Time Audio\n  Processing on Extreme Edge", "abstract": "The escalating challenges of managing vast sensor-generated data,\nparticularly in audio applications, necessitate innovative solutions. Current\nsystems face significant computational and storage demands, especially in\nreal-time applications like gunshot detection systems (GSDS), and the\nproliferation of edge sensors exacerbates these issues. This paper proposes a\ngroundbreaking approach with a near-sensor model tailored for intelligent\naudio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module,\nconvolutional neural network (CNN) layers, and HyperDimensional Computing\n(HDC), our model excels in low-energy, rapid inference, and online learning. It\nis highly adaptable for efficient ASIC design implementation, offering superior\nenergy efficiency compared to conventional embedded CPUs or GPUs, and is\ncompatible with the trend of shrinking microphone sensor sizes. Comprehensive\nevaluations at both software and hardware levels underscore the model's\nefficacy. Software assessments through detailed ROC curve analysis revealed a\ndelicate balance between energy conservation and quality loss, achieving up to\n82.1% energy savings with only 1.39% quality loss. Hardware evaluations\nhighlight the model's commendable energy efficiency when implemented via ASIC\ndesign, especially with the Google Edge TPU, showcasing its superiority over\nprevalent embedded CPUs and GPUs.", "published": "2025-02-15 08:19:20", "link": "http://arxiv.org/abs/2502.10718v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for\n  Personalized Hearing Aids", "abstract": "The prevalence of hearing aids is increasing. However, optimizing the\namplification processes of hearing aids remains challenging due to the\ncomplexity of integrating multiple modular components in traditional methods.\nTo address this challenge, we present NeuroAMP, a novel deep neural network\ndesigned for end-to-end, personalized amplification in hearing aids. NeuroAMP\nleverages both spectral features and the listener's audiogram as inputs, and we\ninvestigate four architectures: Convolutional Neural Network (CNN), Long\nShort-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and\nTransformer. We also introduce Denoising NeuroAMP, an extension that integrates\nnoise reduction along with amplification capabilities for improved performance\nin real-world scenarios. To enhance generalization, a comprehensive data\naugmentation strategy was employed during training on diverse speech (TIMIT and\nTMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the\nHearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index\n(HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the\nTransformer architecture within NeuroAMP achieves the best performance, with\nSRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI)\non the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy\nmaintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ).\nFurthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC\napproach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a\n10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results\nhighlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable\nimprovements in personalized hearing aid amplification.", "published": "2025-02-15 14:55:40", "link": "http://arxiv.org/abs/2502.10822v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generalizable speech deepfake detection via meta-learned LoRA", "abstract": "Generalizable deepfake detection can be formulated as a detection problem\nwhere labels (bonafide and fake) are fixed but distributional drift affects the\ndeepfake set. We can always train our detector with one-selected attacks and\nbonafide data, but an attacker can generate new attacks by just retraining his\ngenerator with a different seed. One reasonable approach is to simply pool all\ndifferent attack types available in training time. Our proposed approach is to\nutilize meta-learning in combination with LoRA adapters to learn the structure\nin the training data that is common to all attack types.", "published": "2025-02-15 16:02:54", "link": "http://arxiv.org/abs/2502.10838v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
