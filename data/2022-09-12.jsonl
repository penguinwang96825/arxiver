{"title": "CSL: A Large-scale Chinese Scientific Literature Dataset", "abstract": "Scientific literature serves as a high-quality corpus, supporting a lot of\nNatural Language Processing (NLP) research. However, existing datasets are\ncentered around the English language, which restricts the development of\nChinese scientific NLP. In this work, we present CSL, a large-scale Chinese\nScientific Literature dataset, which contains the titles, abstracts, keywords\nand academic fields of 396k papers. To our knowledge, CSL is the first\nscientific document dataset in Chinese. The CSL can serve as a Chinese corpus.\nAlso, this semi-structured data is a natural annotation that can constitute\nmany supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the\nperformance of models across scientific domain tasks, i.e., summarization,\nkeyword generation and text classification. We analyze the behavior of existing\ntext-to-text models on the evaluation tasks and reveal the challenges for\nChinese scientific NLP tasks, which provides a valuable reference for future\nresearch. Data and code are available at https://github.com/ydli-ai/CSL", "published": "2022-09-12 06:10:47", "link": "http://arxiv.org/abs/2209.05034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-Preserving Adversarial Code Comprehension", "abstract": "Based on the tremendous success of pre-trained language models (PrLMs) for\nsource code comprehension tasks, current literature studies either ways to\nfurther improve the performance (generalization) of PrLMs, or their robustness\nagainst adversarial attacks. However, they have to compromise on the trade-off\nbetween the two aspects and none of them consider improving both sides in an\neffective and practical way. To fill this gap, we propose Semantic-Preserving\nAdversarial Code Embeddings (SPACE) to find the worst-case semantic-preserving\nattacks while forcing the model to predict the correct labels under these worst\ncases. Experiments and analysis demonstrate that SPACE can stay robust against\nstate-of-the-art attacks while boosting the performance of PrLMs for code.", "published": "2022-09-12 10:32:51", "link": "http://arxiv.org/abs/2209.05130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Domain Dialog Evaluation using Follow-Ups Likelihood", "abstract": "Automatic evaluation of open-domain dialogs remains an unsolved problem.\nMoreover, existing methods do not correlate strongly with human annotations.\nThis paper presents a new automated evaluation method using follow-ups: we\nmeasure the probability that a language model will continue the conversation\nwith a fixed set of follow-ups (e.g., not really relevant here, what are you\ntrying to say). When compared against twelve existing methods, our new\nevaluation achieves the highest correlation with human evaluations.", "published": "2022-09-12 12:22:31", "link": "http://arxiv.org/abs/2209.05185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A new hazard event classification model via deep learning and\n  multifractal", "abstract": "Hazard and operability analysis (HAZOP) is the paradigm of industrial safety\nthat can reveal the hazards of process from its node deviations, consequences,\ncauses, measures and suggestions, and such hazards can be considered as hazard\nevents (HaE). The classification research on HaE has much irreplaceable\npragmatic values. In this paper, we present a novel deep learning model termed\nDLF through multifractal to explore HaE classification where the motivation is\nthat HaE can be naturally regarded as a kind of time series. Specifically,\nfirst HaE is vectorized to get HaE time series by employing BERT. Then, a new\nmultifractal analysis method termed HmF-DFA is proposed to win HaE fractal\nseries by analyzing HaE time series. Finally, a new hierarchical gating neural\nnetwork (HGNN) is designed to process HaE fractal series to accomplish the\nclassification of HaE from three aspects: severity, possibility and risk. We\ntake HAZOP reports of 18 processes as cases, and launch the experiments on this\nbasis. Results demonstrate that compared with other classifiers, DLF classifier\nperforms better under metrics of precision, recall and F1-score, especially for\nthe severity aspect. Also, HmF-DFA and HGNN effectively promote HaE\nclassification. Our HaE classification system can serve application incentives\nto experts, engineers, employees, and other enterprises. We hope our research\ncan contribute added support to the daily practice in industrial safety.", "published": "2022-09-12 14:13:13", "link": "http://arxiv.org/abs/2209.05263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DECK: Behavioral Tests to Improve Interpretability and Generalizability\n  of BERT Models Detecting Depression from Text", "abstract": "Models that accurately detect depression from text are important tools for\naddressing the post-pandemic mental health crisis. BERT-based classifiers'\npromising performance and the off-the-shelf availability make them great\ncandidates for this task. However, these models are known to suffer from\nperformance inconsistencies and poor generalization. In this paper, we\nintroduce the DECK (DEpression ChecKlist), depression-specific model\nbehavioural tests that allow better interpretability and improve\ngeneralizability of BERT classifiers in depression domain. We create 23 tests\nto evaluate BERT, RoBERTa and ALBERT depression classifiers on three datasets,\ntwo Twitter-based and one clinical interview-based. Our evaluation shows that\nthese models: 1) are robust to certain gender-sensitive variations in text; 2)\nrely on the important depressive language marker of the increased use of first\nperson pronouns; 3) fail to detect some other depression symptoms like suicidal\nideation. We also demonstrate that DECK tests can be used to incorporate\nsymptom-specific information in the training data and consistently improve\ngeneralizability of all three BERT models, with an out-of-distribution F1-score\nincrease of up to 53.93%.", "published": "2022-09-12 14:39:46", "link": "http://arxiv.org/abs/2209.05286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DoubleMix: Simple Interpolation-Based Data Augmentation for Text\n  Classification", "abstract": "This paper proposes a simple yet effective interpolation-based data\naugmentation approach termed DoubleMix, to improve the robustness of models in\ntext classification. DoubleMix first leverages a couple of simple augmentation\noperations to generate several perturbed samples for each training data, and\nthen uses the perturbed data and original data to carry out a two-step\ninterpolation in the hidden space of neural models. Concretely, it first mixes\nup the perturbed data to a synthetic sample and then mixes up the original data\nand the synthetic perturbed data. DoubleMix enhances models' robustness by\nlearning the \"shifted\" features in hidden space. On six text classification\nbenchmark datasets, our approach outperforms several popular text augmentation\nmethods including token-level, sentence-level, and hidden-level data\naugmentation techniques. Also, experiments in low-resource settings show our\napproach consistently improves models' performance when the training data is\nscarce. Extensive ablation studies and case studies confirm that each component\nof our approach contributes to the final performance and show that our approach\nexhibits superior performance on challenging counterexamples. Additionally,\nvisual analysis shows that text features generated by our approach are highly\ninterpretable. Our code for this paper can be found at\nhttps://github.com/declare-lab/DoubleMix.git.", "published": "2022-09-12 15:01:04", "link": "http://arxiv.org/abs/2209.05297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Simplification Benchmarks for English, Portuguese, and Spanish", "abstract": "Even in highly-developed countries, as many as 15-30\\% of the population can\nonly understand texts written using a basic vocabulary. Their understanding of\neveryday texts is limited, which prevents them from taking an active role in\nsociety and making informed decisions regarding healthcare, legal\nrepresentation, or democratic choice. Lexical simplification is a natural\nlanguage processing task that aims to make text understandable to everyone by\nreplacing complex vocabulary and expressions with simpler ones, while\npreserving the original meaning. It has attracted considerable attention in the\nlast 20 years, and fully automatic lexical simplification systems have been\nproposed for various languages. The main obstacle for the progress of the field\nis the absence of high-quality datasets for building and evaluating lexical\nsimplification systems. We present a new benchmark dataset for lexical\nsimplification in English, Spanish, and (Brazilian) Portuguese, and provide\ndetails about data selection and annotation procedures. This is the first\ndataset that offers a direct comparison of lexical simplification systems for\nthree languages. To showcase the usability of the dataset, we adapt two\nstate-of-the-art lexical simplification systems with differing architectures\n(neural vs.\\ non-neural) to all three languages (English, Spanish, and\nBrazilian Portuguese) and evaluate their performances on our new dataset. For a\nfairer comparison, we use several evaluation measures which capture varied\naspects of the systems' efficacy, and discuss their strengths and weaknesses.\nWe find a state-of-the-art neural lexical simplification system outperforms a\nstate-of-the-art non-neural lexical simplification system in all three\nlanguages. More importantly, we find that the state-of-the-art neural lexical\nsimplification systems perform significantly better for English than for\nSpanish and Portuguese.", "published": "2022-09-12 15:06:26", "link": "http://arxiv.org/abs/2209.05301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Question Answering: A Semantic Parsing Perspective", "abstract": "Recent advances in deep learning have greatly propelled the research on\nsemantic parsing. Improvement has since been made in many downstream tasks,\nincluding natural language interface to web APIs, text-to-SQL generation, among\nothers. However, despite the close connection shared with these tasks, research\non question answering over knowledge bases (KBQA) has comparatively been\nprogressing slowly. We identify and attribute this to two unique challenges of\nKBQA, schema-level complexity and fact-level complexity. In this survey, we\nsituate KBQA in the broader literature of semantic parsing and give a\ncomprehensive account of how existing KBQA approaches attempt to address the\nunique challenges. Regardless of the unique challenges, we argue that we can\nstill take much inspiration from the literature of semantic parsing, which has\nbeen overlooked by existing research on KBQA. Based on our discussion, we can\nbetter understand the bottleneck of current KBQA research and shed light on\npromising directions for KBQA to keep up with the literature of semantic\nparsing, particularly in the era of pre-trained language models.", "published": "2022-09-12 02:56:29", "link": "http://arxiv.org/abs/2209.04994v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation for Question Answering via Question Classification", "abstract": "Question answering (QA) has demonstrated impressive progress in answering\nquestions from customized domains. Nevertheless, domain adaptation remains one\nof the most elusive challenges for QA systems, especially when QA systems are\ntrained in a source domain but deployed in a different target domain. In this\nwork, we investigate the potential benefits of question classification for QA\ndomain adaptation. We propose a novel framework: Question Classification for\nQuestion Answering (QC4QA). Specifically, a question classifier is adopted to\nassign question classes to both the source and target data. Then, we perform\njoint training in a self-supervised fashion via pseudo-labeling. For\noptimization, inter-domain discrepancy between the source and target domain is\nreduced via maximum mean discrepancy (MMD) distance. We additionally minimize\nintra-class discrepancy among QA samples of the same question class for\nfine-grained adaptation performance. To the best of our knowledge, this is the\nfirst work in QA domain adaptation to leverage question classification with\nself-supervised adaptation. We demonstrate the effectiveness of the proposed\nQC4QA with consistent improvements against the state-of-the-art baselines on\nmultiple datasets.", "published": "2022-09-12 03:12:02", "link": "http://arxiv.org/abs/2209.04998v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SANCL: Multimodal Review Helpfulness Prediction with Selective Attention\n  and Natural Contrastive Learning", "abstract": "With the boom of e-commerce, Multimodal Review Helpfulness Prediction (MRHP),\nwhich aims to sort product reviews according to the predicted helpfulness\nscores has become a research hotspot. Previous work on this task focuses on\nattention-based modality fusion, information integration, and relation\nmodeling, which primarily exposes the following drawbacks: 1) the model may\nfail to capture the really essential information due to its indiscriminate\nattention formulation; 2) lack appropriate modeling methods that take full\nadvantage of correlation among provided data. In this paper, we propose SANCL:\nSelective Attention and Natural Contrastive Learning for MRHP. SANCL adopts a\nprobe-based strategy to enforce high attention weights on the regions of\ngreater significance. It also constructs a contrastive learning framework based\non natural matching properties in the dataset. Experimental results on two\nbenchmark datasets with three categories show that SANCL achieves\nstate-of-the-art baseline performance with lower memory consumption.", "published": "2022-09-12 06:31:13", "link": "http://arxiv.org/abs/2209.05040v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Review of Challenges in Machine Learning based Automated Hate Speech\n  Detection", "abstract": "The spread of hate speech on social media space is currently a serious issue.\nThe undemanding access to the enormous amount of information being generated on\nthese platforms has led people to post and react with toxic content that\noriginates violence. Though efforts have been made toward detecting and\nrestraining such content online, it is still challenging to identify it\naccurately. Deep learning based solutions have been at the forefront of\nidentifying hateful content. However, the factors such as the context-dependent\nnature of hate speech, the intention of the user, undesired biases, etc. make\nthis process overcritical. In this work, we deeply explore a wide range of\nchallenges in automatic hate speech detection by presenting a hierarchical\norganization of these problems. We focus on challenges faced by machine\nlearning or deep learning based solutions to hate speech identification. At the\ntop level, we distinguish between data level, model level, and human level\nchallenges. We further provide an exhaustive analysis of each level of the\nhierarchy with examples. This survey will help researchers to design their\nsolutions more efficiently in the domain of hate speech detection.", "published": "2022-09-12 14:56:14", "link": "http://arxiv.org/abs/2209.05294v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MaXM: Towards Multilingual Visual Question Answering", "abstract": "Visual Question Answering (VQA) has been primarily studied through the lens\nof the English language. Yet, tackling VQA in other languages in the same\nmanner would require a considerable amount of resources. In this paper, we\npropose scalable solutions to multilingual visual question answering (mVQA), on\nboth data and modeling fronts. We first propose a translation-based framework\nto mVQA data generation that requires much less human annotation efforts than\nthe conventional approach of directly collection questions and answers. Then,\nwe apply our framework to the multilingual captions in the Crossmodal-3600\ndataset and develop an efficient annotation protocol to create MaXM, a\ntest-only VQA benchmark in 7 diverse languages. Finally, we develop a simple,\nlightweight, and effective approach as well as benchmark state-of-the-art\nEnglish and multilingual VQA models. We hope that our benchmark encourages\nfurther research on mVQA.", "published": "2022-09-12 16:53:37", "link": "http://arxiv.org/abs/2209.05401v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Large-scale Evaluation of Transformer-based Article Encoders on the Task\n  of Citation Recommendation", "abstract": "Recently introduced transformer-based article encoders (TAEs) designed to\nproduce similar vector representations for mutually related scientific articles\nhave demonstrated strong performance on benchmark datasets for scientific\narticle recommendation. However, the existing benchmark datasets are\npredominantly focused on single domains and, in some cases, contain easy\nnegatives in small candidate pools. Evaluating representations on such\nbenchmarks might obscure the realistic performance of TAEs in setups with\nthousands of articles in candidate pools. In this work, we evaluate TAEs on\nlarge benchmarks with more challenging candidate pools. We compare the\nperformance of TAEs with a lexical retrieval baseline model BM25 on the task of\ncitation recommendation, where the model produces a list of recommendations for\nciting in a given input article. We find out that BM25 is still very\ncompetitive with the state-of-the-art neural retrievers, a finding which is\nsurprising given the strong performance of TAEs on small benchmarks. As a\nremedy for the limitations of the existing benchmarks, we propose a new\nbenchmark dataset for evaluating scientific article representations:\nMulti-Domain Citation Recommendation dataset (MDCR), which covers different\nscientific fields and contains challenging candidate pools.", "published": "2022-09-12 17:53:12", "link": "http://arxiv.org/abs/2209.05452v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PreSTU: Pre-Training for Scene-Text Understanding", "abstract": "The ability to recognize and reason about text embedded in visual inputs is\noften lacking in vision-and-language (V&L) models, perhaps because V&L\npre-training methods have often failed to include such an ability in their\ntraining objective. In this paper, we propose PreSTU, a novel pre-training\nrecipe dedicated to scene-text understanding (STU). PreSTU introduces OCR-aware\npre-training objectives that encourage the model to recognize text from an\nimage and connect it to the rest of the image content. We implement PreSTU\nusing a simple transformer-based encoder-decoder architecture, combined with\nlarge-scale image-text datasets with scene text obtained from an off-the-shelf\nOCR system. We empirically demonstrate the effectiveness of this pre-training\napproach on eight visual question answering and four image captioning\nbenchmarks.", "published": "2022-09-12 18:29:55", "link": "http://arxiv.org/abs/2209.05534v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Embedding-Based Grocery Search Model at Instacart", "abstract": "The key to e-commerce search is how to best utilize the large yet noisy log\ndata. In this paper, we present our embedding-based model for grocery search at\nInstacart. The system learns query and product representations with a two-tower\ntransformer-based encoder architecture. To tackle the cold-start problem, we\nfocus on content-based features. To train the model efficiently on noisy data,\nwe propose a self-adversarial learning method and a cascade training method.\nAccOn an offline human evaluation dataset, we achieve 10% relative improvement\nin RECALL@20, and for online A/B testing, we achieve 4.1% cart-adds per search\n(CAPS) and 1.5% gross merchandise value (GMV) improvement. We describe how we\ntrain and deploy the embedding based search model and give a detailed analysis\nof the effectiveness of our method.", "published": "2022-09-12 19:15:01", "link": "http://arxiv.org/abs/2209.05555v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "emojiSpace: Spatial Representation of Emojis", "abstract": "In the absence of nonverbal cues during messaging communication, users\nexpress part of their emotions using emojis. Thus, having emojis in the\nvocabulary of text messaging language models can significantly improve many\nnatural language processing (NLP) applications such as online communication\nanalysis. On the other hand, word embedding models are usually trained on a\nvery large corpus of text such as Wikipedia or Google News datasets that\ninclude very few samples with emojis. In this study, we create emojiSpace,\nwhich is a combined word-emoji embedding using the word2vec model from the\nGenism library in Python. We trained emojiSpace on a corpus of more than 4\nbillion tweets and evaluated it by implementing sentiment analysis on a Twitter\ndataset containing more than 67 million tweets as an extrinsic task. For this\ntask, we compared the performance of two different classifiers of random forest\n(RF) and linear support vector machine (SVM). For evaluation, we compared\nemojiSpace performance with two other pre-trained embeddings and demonstrated\nthat emojiSpace outperforms both.", "published": "2022-09-12 13:57:31", "link": "http://arxiv.org/abs/2209.09871v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VarArray Meets t-SOT: Advancing the State of the Art of Streaming\n  Distant Conversational Speech Recognition", "abstract": "This paper presents a novel streaming automatic speech recognition (ASR)\nframework for multi-talker overlapping speech captured by a distant microphone\narray with an arbitrary geometry. Our framework, named t-SOT-VA, capitalizes on\nindependently developed two recent technologies; array-geometry-agnostic\ncontinuous speech separation, or VarArray, and streaming multi-talker ASR based\non token-level serialized output training (t-SOT). To combine the best of both\ntechnologies, we newly design a t-SOT-based ASR model that generates a\nserialized multi-talker transcription based on two separated speech signals\nfrom VarArray. We also propose a pre-training scheme for such an ASR model\nwhere we simulate VarArray's output signals based on monaural single-talker ASR\ntraining data. Conversation transcription experiments using the AMI meeting\ncorpus show that the system based on the proposed framework significantly\noutperforms conventional ones. Our system achieves the state-of-the-art word\nerror rates of 13.7% and 15.5% for the AMI development and evaluation sets,\nrespectively, in the multiple-distant-microphone setting while retaining the\nstreaming inference capability.", "published": "2022-09-12 01:22:04", "link": "http://arxiv.org/abs/2209.04974v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Signs of Language: Embodied Sign Language Fingerspelling Acquisition\n  from Demonstrations for Human-Robot Interaction", "abstract": "Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.", "published": "2022-09-12 10:42:26", "link": "http://arxiv.org/abs/2209.05135v3", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "On Faithfulness and Coherence of Language Explanations for\n  Recommendation Systems", "abstract": "Reviews contain rich information about product characteristics and user\ninterests and thus are commonly used to boost recommender system performance.\nSpecifically, previous work show that jointly learning to perform review\ngeneration improves rating prediction performance. Meanwhile, these\nmodel-produced reviews serve as recommendation explanations, providing the user\nwith insights on predicted ratings. However, while existing models could\ngenerate fluent, human-like reviews, it is unclear to what degree the reviews\nfully uncover the rationale behind the jointly predicted rating. In this work,\nwe perform a series of evaluations that probes state-of-the-art models and\ntheir review generation component. We show that the generated explanations are\nbrittle and need further evaluation before being taken as literal rationales\nfor the estimated ratings.", "published": "2022-09-12 17:00:31", "link": "http://arxiv.org/abs/2209.05409v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Leveraging Large (Visual) Language Models for Robot 3D Scene\n  Understanding", "abstract": "Abstract semantic 3D scene understanding is a problem of critical importance\nin robotics. As robots still lack the common-sense knowledge about household\nobjects and locations of an average human, we investigate the use of\npre-trained language models to impart common sense for scene understanding. We\nintroduce and compare a wide range of scene classification paradigms that\nleverage language only (zero-shot, embedding-based, and structured-language) or\nvision and language (zero-shot and fine-tuned). We find that the best\napproaches in both categories yield $\\sim 70\\%$ room classification accuracy,\nexceeding the performance of pure-vision and graph classifiers. We also find\nsuch methods demonstrate notable generalization and transfer capabilities\nstemming from their use of language.", "published": "2022-09-12 21:36:58", "link": "http://arxiv.org/abs/2209.05629v2", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "VL-Taboo: An Analysis of Attribute-based Zero-shot Capabilities of\n  Vision-Language Models", "abstract": "Vision-language models trained on large, randomly collected data had\nsignificant impact in many areas since they appeared. But as they show great\nperformance in various fields, such as image-text-retrieval, their inner\nworkings are still not fully understood. The current work analyses the true\nzero-shot capabilities of those models. We start from the analysis of the\ntraining corpus assessing to what extent (and which of) the test classes are\nreally zero-shot and how this correlates with individual classes performance.\nWe follow up with the analysis of the attribute-based zero-shot learning\ncapabilities of these models, evaluating how well this classical zero-shot\nnotion emerges from large-scale webly supervision. We leverage the recently\nreleased LAION400M data corpus as well as the publicly available pretrained\nmodels of CLIP, OpenCLIP, and FLAVA, evaluating the attribute-based zero-shot\ncapabilities on CUB and AWA2 benchmarks. Our analysis shows that: (i) most of\nthe classes in popular zero-shot benchmarks are observed (a lot) during\npre-training; (ii) zero-shot performance mainly comes out of models' capability\nof recognizing class labels, whenever they are present in the text, and a\nsignificantly lower performing capability of attribute-based zeroshot learning\nis only observed when class labels are not used; (iii) the number of the\nattributes used can have a significant effect on performance, and can easily\ncause a significant performance decrease.", "published": "2022-09-12 15:43:09", "link": "http://arxiv.org/abs/2209.06103v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Factual and Informative Review Generation for Explainable Recommendation", "abstract": "Recent models can generate fluent and grammatical synthetic reviews while\naccurately predicting user ratings. The generated reviews, expressing users'\nestimated opinions towards related products, are often viewed as natural\nlanguage 'rationales' for the jointly predicted rating. However, previous\nstudies found that existing models often generate repetitive, universally\napplicable, and generic explanations, resulting in uninformative rationales.\nFurther, our analysis shows that previous models' generated content often\ncontain factual hallucinations. These issues call for novel solutions that\ncould generate both informative and factually grounded explanations. Inspired\nby recent success in using retrieved content in addition to parametric\nknowledge for generation, we propose to augment the generator with a\npersonalized retriever, where the retriever's output serves as external\nknowledge for enhancing the generator. Experiments on Yelp, TripAdvisor, and\nAmazon Movie Reviews dataset show our model could generate explanations that\nmore reliably entail existing reviews, are more diverse, and are rated more\ninformative by human evaluators.", "published": "2022-09-12 16:46:47", "link": "http://arxiv.org/abs/2209.12613v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", "abstract": "Transformers have revolutionized vision and natural language processing with\ntheir ability to scale with large datasets. But in robotic manipulation, data\nis both limited and expensive. Can manipulation still benefit from Transformers\nwith the right problem formulation? We investigate this question with PerAct, a\nlanguage-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.\nPerAct encodes language goals and RGB-D voxel observations with a Perceiver\nTransformer, and outputs discretized actions by ``detecting the next best voxel\naction''. Unlike frameworks that operate on 2D images, the voxelized 3D\nobservation and action space provides a strong structural prior for efficiently\nlearning 6-DoF actions. With this formulation, we train a single multi-task\nTransformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks\n(with 18 variations) from just a few demonstrations per task. Our results show\nthat PerAct significantly outperforms unstructured image-to-action agents and\n3D ConvNet baselines for a wide range of tabletop tasks.", "published": "2022-09-12 17:51:05", "link": "http://arxiv.org/abs/2209.05451v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "How Much Does Prosody Help Turn-taking? Investigations using Voice\n  Activity Projection Models", "abstract": "Turn-taking is a fundamental aspect of human communication and can be\ndescribed as the ability to take turns, project upcoming turn shifts, and\nsupply backchannels at appropriate locations throughout a conversation. In this\nwork, we investigate the role of prosody in turn-taking using the recently\nproposed Voice Activity Projection model, which incrementally models the\nupcoming speech activity of the interlocutors in a self-supervised manner,\nwithout relying on explicit annotation of turn-taking events, or the explicit\nmodeling of prosodic features. Through manipulation of the speech signal, we\ninvestigate how these models implicitly utilize prosodic information. We show\nthat these systems learn to utilize various prosodic aspects of speech both on\naggregate quantitative metrics of long-form conversations and on single\nutterances specifically designed to depend on prosody.", "published": "2022-09-12 11:40:16", "link": "http://arxiv.org/abs/2209.05161v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The 2022 Far-field Speaker Verification Challenge: Exploring domain\n  mismatch and semi-supervised learning under the far-field scenario", "abstract": "FFSVC2022 is the second challenge of far-field speaker verification.\nFFSVC2022 provides the fully-supervised far-field speaker verification to\nfurther explore the far-field scenario and proposes semi-supervised far-field\nspeaker verification. In contrast to FFSVC2020, FFSVC2022 focus on the\nsingle-channel scenario. In addition, a supplementary set for the FFSVC2020\ndataset is released this year. The supplementary set consists of more recording\ndevices and has the same data distribution as the FFSVC2022 evaluation set.\nThis paper summarizes the FFSVC 2022, including tasks description, trial\ndesigning details, a baseline system and a summary of challenge results. The\nchallenge results indicate substantial progress made in the field but also\npresent that there are still difficulties with the far-field scenario.", "published": "2022-09-12 14:31:43", "link": "http://arxiv.org/abs/2209.05273v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Continuous head-related transfer function representation based on\n  hyperspherical harmonics", "abstract": "Expressing head-related transfer functions (HRTFs) in spherical harmonic (SH)\ndomain has been thoroughly studied as a method of obtaining continuity over\nspace. However, HRTFs are functions not only of direction but also of\nfrequency. This paper presents an extension of the SH-based method, utilizing\nhyperspherical harmonics (HSHs) to obtain an HRTF representation that is\ncontinuous over both space and frequency. The application of the HSH\napproximation results in a relatively small set of coefficients which can be\ndecoded into HRTF values at any direction and frequency. The paper discusses\nresults obtained by applying the method to magnitude spectra extracted from\nexemplary HRTF measurements. The HRTF representations based on SHs and HSHs\nexhibit similar reproduction accuracy, with the latter one featuring continuity\nover both space and frequency and requiring much lower number of coefficients.\nThe developed HSH-based continuous functional model can serve multiple\npurposes, such as interpolation, compression or parametrization for\nmachine-learning applications.", "published": "2022-09-12 09:40:42", "link": "http://arxiv.org/abs/2209.05110v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
