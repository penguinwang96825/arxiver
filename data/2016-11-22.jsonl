{"title": "Learning to Distill: The Essence Vector Modeling Framework", "abstract": "In the context of natural language processing, representation learning has\nemerged as a newly active research subject because of its excellent performance\nin many applications. Learning representations of words is a pioneering study\nin this school of research. However, paragraph (or sentence and document)\nembedding learning is more suitable/reasonable for some tasks, such as\nsentiment classification and document summarization. Nevertheless, as far as we\nare aware, there is relatively less work focusing on the development of\nunsupervised paragraph embedding methods. Classic paragraph embedding methods\ninfer the representation of a given paragraph by considering all of the words\noccurring in the paragraph. Consequently, those stop or function words that\noccur frequently may mislead the embedding learning process to produce a misty\nparagraph representation. Motivated by these observations, our major\ncontributions in this paper are twofold. First, we propose a novel unsupervised\nparagraph embedding method, named the essence vector (EV) model, which aims at\nnot only distilling the most representative information from a paragraph but\nalso excluding the general background information to produce a more informative\nlow-dimensional vector representation for the paragraph. Second, in view of the\nincreasing importance of spoken content processing, an extension of the EV\nmodel, named the denoising essence vector (D-EV) model, is proposed. The D-EV\nmodel not only inherits the advantages of the EV model but also can infer a\nmore robust representation for a given spoken paragraph against imperfect\nspeech recognition.", "published": "2016-11-22 09:11:42", "link": "http://arxiv.org/abs/1611.07206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Learning of Relation Path Embedding for Knowledge Base\n  Completion", "abstract": "Large-scale knowledge bases have currently reached impressive sizes; however,\nthese knowledge bases are still far from complete. In addition, most of the\nexisting methods for knowledge base completion only consider the direct links\nbetween entities, ignoring the vital impact of the consistent semantics of\nrelation paths. In this paper, we study the problem of how to better embed\nentities and relations of knowledge bases into different low-dimensional spaces\nby taking full advantage of the additional semantics of relation paths, and we\npropose a compositional learning model of relation path embedding (RPE).\nSpecifically, with the corresponding relation and path projections, RPE can\nsimultaneously embed each entity into two types of latent spaces. It is also\nproposed that type constraints could be extended from traditional\nrelation-specific constraints to the new proposed path-specific constraints.\nThe results of experiments show that the proposed model achieves significant\nand consistent improvements compared with the state-of-the-art algorithms.", "published": "2016-11-22 10:11:56", "link": "http://arxiv.org/abs/1611.07232v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Recurrent Convolutional Neural Network: Improving Performance For\n  Speech Recognition", "abstract": "A deep learning approach has been widely applied in sequence modeling\nproblems. In terms of automatic speech recognition (ASR), its performance has\nsignificantly been improved by increasing large speech corpus and deeper neural\nnetwork. Especially, recurrent neural network and deep convolutional neural\nnetwork have been applied in ASR successfully. Given the arising problem of\ntraining speed, we build a novel deep recurrent convolutional network for\nacoustic modeling and then apply deep residual learning to it. Our experiments\nshow that it has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network. In the\nexperiments, we compare the convergence speed of our novel deep recurrent\nconvolutional networks and traditional deep convolutional recurrent networks.\nWith faster convergence speed, our novel deep recurrent convolutional networks\ncan reach the comparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel deep recurret\nconvolutional networks. Finally, we evaluate all our experimental networks by\nphoneme error rate (PER) with our proposed bidirectional statistical n-gram\nlanguage model. Our evaluation results show that our newly proposed deep\nrecurrent convolutional network applied with deep residual learning can reach\nthe best PER of 17.33\\% with the fastest convergence speed on TIMIT database.\nThe outstanding performance of our novel deep recurrent convolutional neural\nnetwork with deep residual learning indicates that it can be potentially\nadopted in other sequential problems.", "published": "2016-11-22 07:36:21", "link": "http://arxiv.org/abs/1611.07174v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Natural Language Query Interface for Searching Personal Information on\n  Smartwatches", "abstract": "Currently, personal assistant systems, run on smartphones and use natural\nlanguage interfaces. However, these systems rely mostly on the web for finding\ninformation. Mobile and wearable devices can collect an enormous amount of\ncontextual personal data such as sleep and physical activities. These\ninformation objects and their applications are known as quantified-self, mobile\nhealth or personal informatics, and they can be used to provide a deeper\ninsight into our behavior. To our knowledge, existing personal assistant\nsystems do not support all types of quantified-self queries. In response to\nthis, we have undertaken a user study to analyze a set of \"textual\nquestions/queries\" that users have used to search their quantified-self or\nmobile health data. Through analyzing these questions, we have constructed a\nlight-weight natural language based query interface, including a text parser\nalgorithm and a user interface, to process the users' queries that have been\nused for searching quantified-self information. This query interface has been\ndesigned to operate on small devices, i.e. smartwatches, as well as augmenting\nthe personal assistant systems by allowing them to process end users' natural\nlanguage queries about their quantified-self data.", "published": "2016-11-22 03:42:44", "link": "http://arxiv.org/abs/1611.07139v1", "categories": ["cs.HC", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
