{"title": "Challenges of language technologies for the indigenous languages of the\n  Americas", "abstract": "Indigenous languages of the American continent are highly diverse. However,\nthey have received little attention from the technological perspective. In this\npaper, we review the research, the digital resources and the available NLP\nsystems that focus on these languages. We present the main challenges and\nresearch questions that arise when distant languages and low-resource scenarios\nare faced. We would like to encourage NLP research in linguistically rich and\ndiverse areas like the Americas.", "published": "2018-06-12 01:26:55", "link": "http://arxiv.org/abs/1806.04291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational\n  Agents", "abstract": "Dialogue Act (DA) tagging is crucial for spoken language understanding\nsystems, as it provides a general representation of speakers' intents, not\nbound to a particular dialogue system. Unfortunately, publicly available data\nsets with DA annotation are all based on different annotation schemes and thus\nincompatible with each other. Moreover, their schemes often do not cover all\naspects necessary for open-domain human-machine interaction. In this paper, we\npropose a methodology to map several publicly available corpora to a subset of\nthe ISO standard, in order to create a large task-independent training corpus\nfor DA classification. We show the feasibility of using this corpus to train a\ndomain-independent DA tagger testing it on out-of-domain conversational data,\nand argue the importance of training on multiple corpora to achieve robustness\nacross different DA categories.", "published": "2018-06-12 04:22:11", "link": "http://arxiv.org/abs/1806.04327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Network Models for Paraphrase Identification, Semantic Textual\n  Similarity, Natural Language Inference, and Question Answering", "abstract": "In this paper, we analyze several neural network designs (and their\nvariations) for sentence pair modeling and compare their performance\nextensively across eight datasets, including paraphrase identification,\nsemantic textual similarity, natural language inference, and question answering\ntasks. Although most of these models have claimed state-of-the-art performance,\nthe original papers often reported on only one or two selected datasets. We\nprovide a systematic study and show that (i) encoding contextual information by\nLSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help\nas much as previously claimed but surprisingly improves performance on Twitter\ndatasets, (iii) the Enhanced Sequential Inference Model is the best so far for\nlarger datasets, while the Pairwise Word Interaction Model achieves the best\nperformance when less data is available. We release our implementations as an\nopen-source toolkit.", "published": "2018-06-12 04:48:06", "link": "http://arxiv.org/abs/1806.04330v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Document Knowledge for Aspect-level Sentiment Classification", "abstract": "Attention-based long short-term memory (LSTM) networks have proven to be\nuseful in aspect-level sentiment classification. However, due to the\ndifficulties in annotating aspect-level data, existing public datasets for this\ntask are all relatively small, which largely limits the effectiveness of those\nneural models. In this paper, we explore two approaches that transfer knowledge\nfrom document- level data, which is much less expensive to obtain, to improve\nthe performance of aspect-level sentiment classification. We demonstrate the\neffectiveness of our approaches on 4 public datasets from SemEval 2014, 2015,\nand 2016, and we show that attention-based LSTM benefits from document-level\nknowledge in multiple ways.", "published": "2018-06-12 06:04:11", "link": "http://arxiv.org/abs/1806.04346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Neural Models for Translating Between Styles Within and\n  Across Languages", "abstract": "Generating natural language requires conveying content in an appropriate\nstyle. We explore two related tasks on generating text of varying formality:\nmonolingual formality transfer and formality-sensitive machine translation. We\npropose to solve these tasks jointly using multi-task learning, and show that\nour models achieve state-of-the-art performance for formality transfer and are\nable to perform formality-sensitive translation without being explicitly\ntrained on style-annotated translation examples.", "published": "2018-06-12 06:59:43", "link": "http://arxiv.org/abs/1806.04357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Projecting Embeddings for Domain Adaptation: Joint Modeling of Sentiment\n  Analysis in Diverse Domains", "abstract": "Domain adaptation for sentiment analysis is challenging due to the fact that\nsupervised classifiers are very sensitive to changes in domain. The two most\nprominent approaches to this problem are structural correspondence learning and\nautoencoders. However, they either require long training times or suffer\ngreatly on highly divergent domains. Inspired by recent advances in\ncross-lingual sentiment analysis, we provide a novel perspective and cast the\ndomain adaptation problem as an embedding projection task. Our model takes as\ninput two mono-domain embedding spaces and learns to project them to a\nbi-domain space, which is jointly optimized to (1) project across domains and\nto (2) predict sentiment. We perform domain adaptation experiments on 20\nsource-target domain pairs for sentiment classification and report novel\nstate-of-the-art results on 11 domain pairs, including the Amazon domain\nadaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that\nour model performs comparably to state-of-the-art approaches on domains that\nare similar, while performing significantly better on highly divergent domains.\nOur code is available at https://github.com/jbarnesspain/domain_blse", "published": "2018-06-12 08:16:28", "link": "http://arxiv.org/abs/1806.04381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining and Generalizing Back-Translation through Wake-Sleep", "abstract": "Back-translation has become a commonly employed heuristic for semi-supervised\nneural machine translation. The technique is both straightforward to apply and\nhas led to state-of-the-art results. In this work, we offer a principled\ninterpretation of back-translation as approximate inference in a generative\nmodel of bitext and show how the standard implementation of back-translation\ncorresponds to a single iteration of the wake-sleep algorithm in our proposed\nmodel. Moreover, this interpretation suggests a natural iterative\ngeneralization, which we demonstrate leads to further improvement of up to 1.6\nBLEU.", "published": "2018-06-12 09:17:40", "link": "http://arxiv.org/abs/1806.04402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue\n  State Representation", "abstract": "Classic pipeline models for task-oriented dialogue system require explicit\nmodeling the dialogue states and hand-crafted action spaces to query a\ndomain-specific knowledge base. Conversely, sequence-to-sequence models learn\nto map dialogue history to the response in current turn without explicit\nknowledge base querying. In this work, we propose a novel framework that\nleverages the advantages of classic pipeline and sequence-to-sequence models.\nOur framework models a dialogue state as a fixed-size distributed\nrepresentation and use this representation to query a knowledge base via an\nattention mechanism. Experiment on Stanford Multi-turn Multi-domain\nTask-oriented Dialogue Dataset shows that our framework significantly\noutperforms other sequence-to-sequence based baseline models on both automatic\nand human evaluation.", "published": "2018-06-12 11:21:16", "link": "http://arxiv.org/abs/1806.04441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ensemble Model for Sentiment Analysis of Hindi-English Code-Mixed\n  Data", "abstract": "In multilingual societies like India, code-mixed social media texts comprise\nthe majority of the Internet. Detecting the sentiment of the code-mixed user\nopinions plays a crucial role in understanding social, economic and political\ntrends. In this paper, we propose an ensemble of character-trigrams based LSTM\nmodel and word-ngrams based Multinomial Naive Bayes (MNB) model to identify the\nsentiments of Hindi-English (Hi-En) code-mixed data. The ensemble model\ncombines the strengths of rich sequential patterns from the LSTM model and\npolarity of keywords from the probabilistic ngram model to identify sentiments\nin sparse and inconsistent code-mixed data. Experiments on reallife user\ncode-mixed data reveals that our approach yields state-of-the-art results as\ncompared to several baselines and other deep learning based proposed methods.", "published": "2018-06-12 11:46:51", "link": "http://arxiv.org/abs/1806.04450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impersonation: Modeling Persona in Smart Responses to Email", "abstract": "In this paper, we present design, implementation, and effectiveness of\ngenerating personalized suggestions for email replies. To personalize email\nresponses based on users style and personality, we model the users persona\nbased on her past responses to emails. This model is added to the\nlanguage-based model created across users using past responses of the all user\nemails.\n  A users model captures the typical responses of the user given a particular\ncontext. The context includes the email received, recipient of the email, and\nother external signals such as calendar activities, preferences, etc. The\ncontext along with users personality (e.g., extrovert, formal, reserved, etc.)\nis used to suggest responses. These responses can be a mixture of multiple\nmodes: email replies (textual), audio clips, etc. This helps in making\nresponses mimic the user as much as possible and helps the user to be more\nproductive while retaining her mark in the responses.", "published": "2018-06-12 12:08:30", "link": "http://arxiv.org/abs/1806.04456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fusing Recency into Neural Machine Translation with an Inter-Sentence\n  Gate Model", "abstract": "Neural machine translation (NMT) systems are usually trained on a large\namount of bilingual sentence pairs and translate one sentence at a time,\nignoring inter-sentence information. This may make the translation of a\nsentence ambiguous or even inconsistent with the translations of neighboring\nsentences. In order to handle this issue, we propose an inter-sentence gate\nmodel that uses the same encoder to encode two adjacent sentences and controls\nthe amount of information flowing from the preceding sentence to the\ntranslation of the current sentence with an inter-sentence gate. In this way,\nour proposed model can capture the connection between sentences and fuse\nrecency from neighboring sentences into neural machine translation. On several\nNIST Chinese-English translation tasks, our experiments demonstrate that the\nproposed inter-sentence gate model achieves substantial improvements over the\nbaseline.", "published": "2018-06-12 12:37:20", "link": "http://arxiv.org/abs/1806.04466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design Challenges and Misconceptions in Neural Sequence Labeling", "abstract": "We investigate the design challenges of constructing effective and efficient\nneural sequence labeling systems, by reproducing twelve neural sequence\nlabeling models, which include most of the state-of-the-art structures, and\nconduct a systematic model comparison on three benchmarks (i.e. NER, Chunking,\nand POS tagging). Misconceptions and inconsistent conclusions in existing\nliterature are examined and clarified under statistical experiments. In the\ncomparison and analysis process, we reach several practical conclusions which\ncan be useful to practitioners.", "published": "2018-06-12 12:43:42", "link": "http://arxiv.org/abs/1806.04470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs", "abstract": "Large scale knowledge graphs (KGs) such as Freebase are generally incomplete.\nReasoning over multi-hop (mh) KG paths is thus an important capability that is\nneeded for question answering or other NLP tasks that require knowledge about\nthe world. mh-KG reasoning includes diverse scenarios, e.g., given a head\nentity and a relation path, predict the tail entity; or given two entities\nconnected by some relation paths, predict the unknown relation between them. We\npresent ROPs, recurrent one-hop predictors, that predict entities at each step\nof mh-KB paths by using recurrent neural networks and vector representations of\nentities and relations, with two benefits: (i) modeling mh-paths of arbitrary\nlengths while updating the entity and relation representations by the training\nsignal at each step; (ii) handling different types of mh-KG reasoning in a\nunified framework. Our models show state-of-the-art for two important multi-hop\nKG reasoning tasks: Knowledge Base Completion and Path Query Answering.", "published": "2018-06-12 13:52:15", "link": "http://arxiv.org/abs/1806.04523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Automatically Generate Fill-In-The-Blank Quizzes", "abstract": "In this paper we formalize the problem automatic fill-in-the-blank question\ngeneration using two standard NLP machine learning schemes, proposing concrete\ndeep learning models for each. We present an empirical study based on data\nobtained from a language learning platform showing that both of our proposed\nsettings offer promising results.", "published": "2018-06-12 13:53:22", "link": "http://arxiv.org/abs/1806.04524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Term Definitions Help Hypernymy Detection", "abstract": "Existing methods of hypernymy detection mainly rely on statistics over a big\ncorpus, either mining some co-occurring patterns like \"animals such as cats\" or\nembedding words of interest into context-aware vectors. These approaches are\ntherefore limited by the availability of a large enough corpus that can cover\nall terms of interest and provide sufficient contextual information to\nrepresent their meaning. In this work, we propose a new paradigm, HyperDef, for\nhypernymy detection -- expressing word meaning by encoding word definitions,\nalong with context driven representation. This has two main benefits: (i)\nDefinitional sentences express (sense-specific) corpus-independent meanings of\nwords, hence definition-driven approaches enable strong generalization -- once\ntrained, the model is expected to work well in open-domain testbeds; (ii)\nGlobal context from a large corpus and definitions provide complementary\ninformation for words. Consequently, our model, HyperDef, once trained on\ntask-agnostic data, gets state-of-the-art results in multiple benchmarks", "published": "2018-06-12 14:00:18", "link": "http://arxiv.org/abs/1806.04532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Unsupervised Compositional Representations", "abstract": "We evaluated various compositional models, from bag-of-words representations\nto compositional RNN-based models, on several extrinsic supervised and\nunsupervised evaluation benchmarks. Our results confirm that weighted vector\naveraging can outperform context-sensitive models in most benchmarks, but\nstructural features encoded in RNN models can also be useful in certain\nclassification tasks. We analyzed some of the evaluation datasets to identify\nthe aspects of meaning they measure and the characteristics of the various\nmodels that explain their performance variance.", "published": "2018-06-12 18:53:14", "link": "http://arxiv.org/abs/1806.04713v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Text in Hyperbolic Spaces", "abstract": "Natural language text exhibits hierarchical structure in a variety of\nrespects. Ideally, we could incorporate our prior knowledge of this\nhierarchical structure into unsupervised learning algorithms that work on text\ndata. Recent work by Nickel & Kiela (2017) proposed using hyperbolic instead of\nEuclidean embedding spaces to represent hierarchical data and demonstrated\nencouraging results when embedding graphs. In this work, we extend their method\nwith a re-parameterization technique that allows us to learn hyperbolic\nembeddings of arbitrarily parameterized objects. We apply this framework to\nlearn word and sentence embeddings in hyperbolic space in an unsupervised\nmanner from text corpora. The resulting embeddings seem to encode certain\nintuitive notions of hierarchy, such as word-context frequency and phrase\nconstituency. However, the implicit continuous hierarchy in the learned\nhyperbolic space makes interrogating the model's learned hierarchies more\ndifficult than for models that learn explicit edges between items. The learned\nhyperbolic embeddings show improvements over Euclidean embeddings in some --\nbut not all -- downstream tasks, suggesting that hierarchical organization is\nmore useful for some tasks than others.", "published": "2018-06-12 03:25:15", "link": "http://arxiv.org/abs/1806.04313v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Amalgam: Generating Jokes and Quotes Together", "abstract": "Generating humor and quotes are very challenging problems in the field of\ncomputational linguistics and are often tackled separately. In this paper, we\npresent a controlled Long Short-Term Memory (LSTM) architecture which is\ntrained with categorical data like jokes and quotes together by passing\ncategory as an input along with the sequence of words. The idea is that a\nsingle neural net will learn the structure of both jokes and quotes to generate\nthem on demand according to input category. Importantly, we believe the neural\nnet has more knowledge as it's trained on different datasets and hence will\nenable it to generate more creative jokes or quotes from the mixture of\ninformation. May the network generate a funny inspirational joke!", "published": "2018-06-12 08:22:21", "link": "http://arxiv.org/abs/1806.04387v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Learning to Detect Redundant Method Comments", "abstract": "Comments in software are critical for maintenance and reuse. But apart from\nprescriptive advice, there is little practical support or quantitative\nunderstanding of what makes a comment useful. In this paper, we introduce the\ntask of identifying comments which are uninformative about the code they are\nmeant to document. To address this problem, we introduce the notion of comment\nentailment from code, high entailment indicating that a comment's natural\nlanguage semantics can be inferred directly from the code. Although not all\nentailed comments are low quality, comments that are too easily inferred, for\nexample, comments that restate the code, are widely discouraged by authorities\non software style. Based on this, we develop a tool called CRAIC which scores\nmethod-level comments for redundancy. Highly redundant comments can then be\nexpanded or alternately removed by the developer. CRAIC uses deep language\nmodels to exploit large software corpora without requiring expensive manual\nannotations of entailment. We show that CRAIC can perform the comment\nentailment task with good agreement with human judgements. Our findings also\nhave implications for documentation tools. For example, we find that common\ntags in Javadoc are at least two times more predictable from code than\nnon-Javadoc sentences, suggesting that Javadoc tags are less informative than\nmore free-form comments", "published": "2018-06-12 15:49:14", "link": "http://arxiv.org/abs/1806.04616v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Sparse Stochastic Zeroth-Order Optimization with an Application to\n  Bandit Structured Prediction", "abstract": "Stochastic zeroth-order (SZO), or gradient-free, optimization allows to\noptimize arbitrary functions by relying only on function evaluations under\nparameter perturbations, however, the iteration complexity of SZO methods\nsuffers a factor proportional to the dimensionality of the perturbed function.\nWe show that in scenarios with natural sparsity patterns as in structured\nprediction applications, this factor can be reduced to the expected number of\nactive features over input-output pairs. We give a general proof that applies\nsparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic\nobjectives, and present an experimental evaluation on linear bandit structured\nprediction tasks with sparse word-based feature representations that confirm\nour theoretical results.", "published": "2018-06-12 12:16:54", "link": "http://arxiv.org/abs/1806.04458v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Deep State Space Models for Unconditional Word Generation", "abstract": "Autoregressive feedback is considered a necessity for successful\nunconditional text generation using stochastic sequence models. However, such\nfeedback is known to introduce systematic biases into the training process and\nit obscures a principle of generation: committing to global information and\nforgetting local nuances. We show that a non-autoregressive deep state space\nmodel with a clear separation of global and local uncertainty can be built from\nonly two ingredients: An independent noise source and a deterministic\ntransition function. Recent advances on flow-based variational inference can be\nused to train an evidence lower-bound without resorting to annealing, auxiliary\nlosses or similar measures. The result is a highly interpretable generative\nmodel on par with comparable auto-regressive models on the task of word\ngeneration.", "published": "2018-06-12 14:19:48", "link": "http://arxiv.org/abs/1806.04550v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transfer Learning from Speaker Verification to Multispeaker\n  Text-To-Speech Synthesis", "abstract": "We describe a neural network-based system for text-to-speech (TTS) synthesis\nthat is able to generate speech audio in the voice of many different speakers,\nincluding those unseen during training. Our system consists of three\nindependently trained components: (1) a speaker encoder network, trained on a\nspeaker verification task using an independent dataset of noisy speech from\nthousands of speakers without transcripts, to generate a fixed-dimensional\nembedding vector from seconds of reference speech from a target speaker; (2) a\nsequence-to-sequence synthesis network based on Tacotron 2, which generates a\nmel spectrogram from text, conditioned on the speaker embedding; (3) an\nauto-regressive WaveNet-based vocoder that converts the mel spectrogram into a\nsequence of time domain waveform samples. We demonstrate that the proposed\nmodel is able to transfer the knowledge of speaker variability learned by the\ndiscriminatively-trained speaker encoder to the new task, and is able to\nsynthesize natural speech from speakers that were not seen during training. We\nquantify the importance of training the speaker encoder on a large and diverse\nspeaker set in order to obtain the best generalization performance. Finally, we\nshow that randomly sampled speaker embeddings can be used to synthesize speech\nin the voice of novel speakers dissimilar from those used in training,\nindicating that the model has learned a high quality speaker representation.", "published": "2018-06-12 14:29:22", "link": "http://arxiv.org/abs/1806.04558v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual End-to-End Speech Recognition with A Single Transformer on\n  Low-Resource Languages", "abstract": "Sequence-to-sequence attention-based models integrate an acoustic,\npronunciation and language model into a single neural network, which make them\nvery suitable for multilingual automatic speech recognition (ASR). In this\npaper, we are concerned with multilingual speech recognition on low-resource\nlanguages by a single Transformer, one of sequence-to-sequence attention-based\nmodels. Sub-words are employed as the multilingual modeling unit without using\nany pronunciation lexicon. First, we show that a single multilingual ASR\nTransformer performs well on low-resource languages despite of some language\nconfusion. We then look at incorporating language information into the model by\ninserting the language symbol at the beginning or at the end of the original\nsub-words sequence under the condition of language information being known\nduring training. Experiments on CALLHOME datasets demonstrate that the\nmultilingual ASR Transformer with the language symbol at the end performs\nbetter and can obtain relatively 10.5\\% average word error rate (WER) reduction\ncompared to SHL-MLSTM with residual learning. We go on to show that, assuming\nthe language information being known during training and testing, about\nrelatively 12.4\\% average WER reduction can be observed compared to SHL-MLSTM\nwith residual learning through giving the language symbol as the sentence start\ntoken.", "published": "2018-06-12 05:13:04", "link": "http://arxiv.org/abs/1806.05059v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "iParaphrasing: Extracting Visually Grounded Paraphrases via an Image", "abstract": "A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing.", "published": "2018-06-12 00:58:59", "link": "http://arxiv.org/abs/1806.04284v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Capsule Routing for Sound Event Detection", "abstract": "The detection of acoustic scenes is a challenging problem in which\nenvironmental sound events must be detected from a given audio signal. This\nincludes classifying the events as well as estimating their onset and offset\ntimes. We approach this problem with a neural network architecture that uses\nthe recently-proposed capsule routing mechanism. A capsule is a group of\nactivation units representing a set of properties for an entity of interest,\nand the purpose of routing is to identify part-whole relationships between\ncapsules. That is, a capsule in one layer is assumed to belong to a capsule in\nthe layer above in terms of the entity being represented. Using capsule\nrouting, we wish to train a network that can learn global coherence implicitly,\nthereby improving generalization performance. Our proposed method is evaluated\non Task 4 of the DCASE 2017 challenge. Results show that classification\nperformance is state-of-the-art, achieving an F-score of 58.6%. In addition,\noverfitting is reduced considerably compared to other architectures.", "published": "2018-06-12 18:22:20", "link": "http://arxiv.org/abs/1806.04699v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The NES Music Database: A multi-instrumental dataset with expressive\n  performance attributes", "abstract": "Existing research on music generation focuses on composition, but often\nignores the expressive performance characteristics required for plausible\nrenditions of resultant pieces. In this paper, we introduce the Nintendo\nEntertainment System Music Database (NES-MDB), a large corpus allowing for\nseparate examination of the tasks of composition and performance. NES-MDB\ncontains thousands of multi-instrumental songs composed for playback by the\ncompositionally-constrained NES audio synthesizer. For each song, the dataset\ncontains a musical score for four instrument voices as well as expressive\nattributes for the dynamics and timbre of each voice. Unlike datasets comprised\nof General MIDI files, NES-MDB includes all of the information needed to render\nexact acoustic performances of the original compositions. Alongside the\ndataset, we provide a tool that renders generated compositions as NES-style\naudio by emulating the device's audio processor. Additionally, we establish\nbaselines for the tasks of composition, which consists of learning the\nsemantics of composing for the NES synthesizer, and performance, which involves\nfinding a mapping between a composition and realistic expressive attributes.", "published": "2018-06-12 00:28:50", "link": "http://arxiv.org/abs/1806.04278v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
