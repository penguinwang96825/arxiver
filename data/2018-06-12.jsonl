{"title": "Augmenting Stream Constraint Programming with Eventuality Conditions", "abstract": "Stream constraint programming is a recent addition to the family of constraint programming frameworks, where variable domains are sets of infinite streams over finite alphabets. Previous works showed promising results for its applicability to real-world planning and control problems. In this paper, motivated by the modelling of planning applications, we improve the expressiveness of the framework by introducing 1) the \"until\" constraint, a new construct that is adapted from Linear Temporal Logic and 2) the @ operator on streams, a syntactic sugar for which we provide a more efficient solving algorithm over simple desugaring. For both constructs, we propose corresponding novel solving algorithms and prove their correctness. We present competitive experimental results on the Missionaries and Cannibals logic puzzle and a standard path planning application on the grid, by comparing with Apt and Brand's method for verifying eventuality conditions using a CP approach.", "published": "2018-06-12 04:20:02", "link": "http://arxiv.org/abs/1806.04325v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Unsupervised Meta-Learning for Reinforcement Learning", "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.", "published": "2018-06-12 16:48:52", "link": "http://arxiv.org/abs/1806.04640v3", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ToxicBlend: Virtual Screening of Toxic Compounds with Ensemble Predictors", "abstract": "Timely assessment of compound toxicity is one of the biggest challenges facing the pharmaceutical industry today. A significant proportion of compounds identified as potential leads are ultimately discarded due to the toxicity they induce. In this paper, we propose a novel machine learning approach for the prediction of molecular activity on ToxCast targets. We combine extreme gradient boosting with fully-connected and graph-convolutional neural network architectures trained on QSAR physical molecular property descriptors, PubChem molecular fingerprints, and SMILES sequences. Our ensemble predictor leverages the strengths of each individual technique, significantly outperforming existing state-of-the art models on the ToxCast and Tox21 toxicity-prediction datasets. We provide free access to molecule toxicity prediction using our model at http://www.owkin.com/toxicblend.", "published": "2018-06-12 11:46:35", "link": "http://arxiv.org/abs/1806.04449v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Gaussian mixture models with Wasserstein distance", "abstract": "Generative models with both discrete and continuous latent variables are highly motivated by the structure of many real-world data sets. They present, however, subtleties in training often manifesting in the discrete latent being under leveraged. In this paper, we show that such models are more amenable to training when using the Optimal Transport framework of Wasserstein Autoencoders. We find our discrete latent variable to be fully leveraged by the model when trained, without any modifications to the objective function or significant fine tuning. Our model generates comparable samples to other approaches while using relatively simple neural networks, since the discrete latent variable carries much of the descriptive burden. Furthermore, the discrete latent provides significant control over generation.", "published": "2018-06-12 12:35:10", "link": "http://arxiv.org/abs/1806.04465v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A review on distance based time series classification", "abstract": "Time series classification is an increasing research topic due to the vast amount of time series data that are being created over a wide variety of fields. The particularity of the data makes it a challenging task and different approaches have been taken, including the distance based approach. 1-NN has been a widely used method within distance based time series classification due to it simplicity but still good performance. However, its supremacy may be attributed to being able to use specific distances for time series within the classification process and not to the classifier itself. With the aim of exploiting these distances within more complex classifiers, new approaches have arisen in the past few years that are competitive or which outperform the 1-NN based approaches. In some cases, these new methods use the distance measure to transform the series into feature vectors, bridging the gap between time series and traditional classifiers. In other cases, the distances are employed to obtain a time series kernel and enable the use of kernel methods for time series classification. One of the main challenges is that a kernel function must be positive semi-definite, a matter that is also addressed within this review. The presented review includes a taxonomy of all those methods that aim to classify time series using a distance based approach, as well as a discussion of the strengths and weaknesses of each method.", "published": "2018-06-12 13:40:30", "link": "http://arxiv.org/abs/1806.04509v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Meta-Learning for Stochastic Gradient MCMC", "abstract": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of neural network energy landscapes. Experiments validate the proposed approach on both Bayesian fully connected neural network and Bayesian recurrent neural network tasks, showing that the learned sampler out-performs generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.", "published": "2018-06-12 13:50:59", "link": "http://arxiv.org/abs/1806.04522v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Approximate inference with Wasserstein gradient flows", "abstract": "We present a novel approximate inference method for diffusion processes, based on the Wasserstein gradient flow formulation of the diffusion. In this formulation, the time-dependent density of the diffusion is derived as the limit of implicit Euler steps that follow the gradients of a particular free energy functional. Existing methods for computing Wasserstein gradient flows rely on discretization of the domain of the diffusion, prohibiting their application to domains in more than several dimensions. We propose instead a discretization-free inference method that computes the Wasserstein gradient flow directly in a space of continuous functions. We characterize approximation properties of the proposed method and evaluate it on a nonlinear filtering task, finding performance comparable to the state-of-the-art for filtering diffusions.", "published": "2018-06-12 14:08:16", "link": "http://arxiv.org/abs/1806.04542v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Logistic Ensemble Models", "abstract": "Predictive models that are developed in a regulated industry or a regulated application, like determination of credit worthiness, must be interpretable and rational (e.g., meaningful improvements in basic credit behavior must result in improved credit worthiness scores). Machine Learning technologies provide very good performance with minimal analyst intervention, making them well suited to a high volume analytic environment, but the majority are black box tools that provide very limited insight or interpretability into key drivers of model performance or predicted model output values. This paper presents a methodology that blends one of the most popular predictive statistical modeling methods for binary classification with a core model enhancement strategy found in machine learning. The resulting prediction methodology provides solid performance, from minimal analyst effort, while providing the interpretability and rationality required in regulated industries, as well as in other environments where interpretation of model parameters is required (e.g. businesses that require interpretation of models, to take action on them).", "published": "2018-06-12 14:27:21", "link": "http://arxiv.org/abs/1806.04555v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Efficient Tracking of Sparse Signals via an Earth Mover's Distance Dynamics Regularizer", "abstract": "Tracking algorithms such as the Kalman filter aim to improve inference performance by leveraging the temporal dynamics in streaming observations. However, the tracking regularizers are often based on the $\\ell_p$-norm which cannot account for important geometrical relationships between neighboring signal elements. We propose a practical approach to using the earth mover's distance (EMD) via the earth mover's distance dynamic filtering (EMD-DF) algorithm for causally tracking time-varying sparse signals when there is a natural geometry to the coefficient space that should be respected (e.g., meaningful ordering). Specifically, this paper presents a new Beckmann formulation that dramatically reduces computational complexity, as well as an evaluation of the performance and complexity of the proposed approach in imaging and frequency tracking applications with real and simulated neurophysiology data.", "published": "2018-06-12 13:33:04", "link": "http://arxiv.org/abs/1806.04674v5", "categories": ["eess.SP"], "primary_category": "eess.SP"}
