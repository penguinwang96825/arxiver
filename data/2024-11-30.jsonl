{"title": "SeQwen at the Financial Misinformation Detection Challenge Task: Sequential Learning for Claim Verification and Explanation Generation in Financial Domains", "abstract": "This paper presents the system description of our entry for the COLING 2025\nFMD challenge, focusing on misinformation detection in financial domains. We\nexperimented with a combination of large language models, including Qwen,\nMistral, and Gemma-2, and leveraged pre-processing and sequential learning for\nnot only identifying fraudulent financial content but also generating coherent,\nand concise explanations that clarify the rationale behind the classifications.\nOur approach achieved competitive results with an F1-score of 0.8283 for\nclassification, and ROUGE-1 of 0.7253 for explanations. This work highlights\nthe transformative potential of LLMs in financial applications, offering\ninsights into their capabilities for combating misinformation and enhancing\ntransparency while identifying areas for future improvement in robustness and\ndomain adaptation.", "published": "2024-11-30 18:03:04", "link": "http://arxiv.org/abs/2412.00549v1", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Detecting imbalanced financial markets through time-varying optimization and nonlinear functionals", "abstract": "This paper studies the time-varying structure of the equity market with\nrespect to market capitalization. First, we analyze the distribution of the 100\nlargest companies' market capitalizations over time, in terms of inequality,\nconcentration at the top, and overall discrepancies in the distribution between\ndifferent times. In the next section, we introduce a mathematical framework of\nlinear and nonlinear functionals of time-varying portfolios. We apply this to\nstudy the market capitalization exposure and spread of optimal portfolios\nchosen by a Sharpe optimization procedure. These methods could be more widely\nused to study various measures of optimal portfolios and measure different\naspects of market exposure while holding portfolios selected by an optimization\nroutine that changes over time.", "published": "2024-11-30 13:05:19", "link": "http://arxiv.org/abs/2412.00468v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Does Self-Attention Need Separate Weights in Transformers?", "abstract": "The success of self-attention lies in its ability to capture long-range\ndependencies and enhance context understanding, but it is limited by its\ncomputational complexity and challenges in handling sequential data with\ninherent directionality. This work introduces a shared weight\nself-attention-based BERT model that only learns one weight matrix for (Key,\nValue, and Query) representations instead of three individual matrices for each\nof them. Our shared weight attention reduces the training parameter size by\nmore than half and training time by around one-tenth. Furthermore, we\ndemonstrate higher prediction accuracy on small tasks of GLUE over the BERT\nbaseline and in particular a generalization power on noisy and out-of-domain\ndata. Experimental results indicate that our shared self-attention method\nachieves a parameter size reduction of 66.53% in the attention block. In the\nGLUE dataset, the shared weight self-attention-based BERT model demonstrates\naccuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric,\nand pairwise attention-based BERT models, respectively. The model and source\ncode are available at Anonymous.", "published": "2024-11-30 04:46:20", "link": "http://arxiv.org/abs/2412.00359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Was that Sarcasm?: A Literature Survey on Sarcasm Detection", "abstract": "Sarcasm is hard to interpret as human beings. Being able to interpret sarcasm\nis often termed as a sign of intelligence, given the complex nature of sarcasm.\nHence, this is a field of Natural Language Processing which is still complex\nfor computers to decipher. This Literature Survey delves into different aspects\nof sarcasm detection, to create an understanding of the underlying problems\nfaced during detection, approaches used to solve this problem, and different\nforms of available datasets for sarcasm detection.", "published": "2024-11-30 10:38:26", "link": "http://arxiv.org/abs/2412.00425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Domain Adaptation for Named-Entity Recognition via Joint\n  Constrained k-Means and Subspace Selection", "abstract": "Named-entity recognition (NER) is a task that typically requires large\nannotated datasets, which limits its applicability across domains with varying\nentity definitions. This paper addresses few-shot NER, aiming to transfer\nknowledge to new domains with minimal supervision. Unlike previous approaches\nthat rely solely on limited annotated data, we propose a weakly supervised\nalgorithm that combines small labeled datasets with large amounts of unlabeled\ndata. Our method extends the k-means algorithm with label supervision, cluster\nsize constraints and domain-specific discriminative subspace selection. This\nunified framework achieves state-of-the-art results in few-shot NER on several\nEnglish datasets.", "published": "2024-11-30 10:52:24", "link": "http://arxiv.org/abs/2412.00426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-native speakers of English or ChatGPT: Who thinks better?", "abstract": "This study sets out to answer one major question: Who thinks better,\nnon-native speakers of English or ChatGPT?, providing evidence from processing\nand interpreting center-embedding English constructions that human brain\nsurpasses ChatGPT, and that ChatGPT cannot be regarded as a theory of language.\nFifteen non-native speakers of English were recruited as participants of the\nstudy. A center-embedding English sentence was presented to both the study\nparticipants and ChatGPT. The study findings unveil that human brain is still\nfar ahead of Large Language Models, specifically ChatGPT, even in the case of\nnon-native speakers of an L2, here English. The study concludes that human\nbrain's ability to process and interpret natural language data is unique and\nthat ChatGPT still lags behind this human unique ability.", "published": "2024-11-30 12:04:25", "link": "http://arxiv.org/abs/2412.00457v1", "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "GloCOM: A Short Text Neural Topic Model via Global Clustering Context", "abstract": "Uncovering hidden topics from short texts is challenging for traditional and\nneural models due to data sparsity, which limits word co-occurrence patterns,\nand label sparsity, stemming from incomplete reconstruction targets. Although\ndata aggregation offers a potential solution, existing neural topic models\noften overlook it due to time complexity, poor aggregation quality, and\ndifficulty in inferring topic proportions for individual documents. In this\npaper, we propose a novel model, GloCOM (Global Clustering COntexts for Topic\nModels), which addresses these challenges by constructing aggregated global\nclustering contexts for short documents, leveraging text embeddings from\npre-trained language models. GloCOM can infer both global topic distributions\nfor clustering contexts and local distributions for individual short texts.\nAdditionally, the model incorporates these global contexts to augment the\nreconstruction loss, effectively handling the label sparsity issue. Extensive\nexperiments on short text datasets show that our approach outperforms other\nstate-of-the-art models in both topic quality and document representations.", "published": "2024-11-30 16:19:25", "link": "http://arxiv.org/abs/2412.00525v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding\n  Models Performance & Efficiency on a Specific Domain", "abstract": "Recent advancements in language models have started a new era of superior\ninformation retrieval and content generation, with embedding models playing an\nimportant role in optimizing data representation efficiency and performance.\nWhile benchmarks like the Massive Text Embedding Benchmark (MTEB) have\nstandardized the evaluation of general domain embedding models, a gap remains\nin specialized fields such as chemistry, which require tailored approaches due\nto domain-specific challenges. This paper introduces a novel benchmark, the\nChemical Text Embedding Benchmark (ChemTEB), designed specifically for the\nchemical sciences. ChemTEB addresses the unique linguistic and semantic\ncomplexities of chemical literature and data, offering a comprehensive suite of\ntasks on chemical domain data. Through the evaluation of 34 open-source and\nproprietary models using this benchmark, we illuminate the strengths and\nweaknesses of current methodologies in processing and understanding chemical\ninformation. Our work aims to equip the research community with a standardized,\ndomain-specific evaluation framework, promoting the development of more precise\nand efficient NLP models for chemistry-related applications. Furthermore, it\nprovides insights into the performance of generic models in a domain-specific\ncontext. ChemTEB comes with open-source code and data, contributing further to\nits accessibility and utility.", "published": "2024-11-30 16:45:31", "link": "http://arxiv.org/abs/2412.00532v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Consistency of LLM Evaluators", "abstract": "Large language models (LLMs) have shown potential as general evaluators along\nwith the evident benefits of speed and cost. While their correlation against\nhuman annotators has been widely studied, consistency as evaluators is still\nunderstudied, raising concerns about the reliability of LLM evaluators. In this\npaper, we conduct extensive studies on the two aspects of consistency in LLM\nevaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on\ndifferent scoring scales and criterion granularity with open-source and\nproprietary models. Our comprehensive analysis demonstrates that strong\nproprietary models are not necessarily consistent evaluators, highlighting the\nimportance of considering consistency in assessing the capability of LLM\nevaluators.", "published": "2024-11-30 17:29:08", "link": "http://arxiv.org/abs/2412.00543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting\n  Based on Question Classification", "abstract": "This paper presents DynRank, a novel framework for enhancing passage\nretrieval in open-domain question-answering systems through dynamic zero-shot\nquestion classification. Traditional approaches rely on static prompts and\npre-defined templates, which may limit model adaptability across different\nquestions and contexts. In contrast, DynRank introduces a dynamic prompting\nmechanism, leveraging a pre-trained question classification model that\ncategorizes questions into fine-grained types. Based on these classifications,\ncontextually relevant prompts are generated, enabling more effective passage\nretrieval. We integrate DynRank into existing retrieval frameworks and conduct\nextensive experiments on multiple QA benchmark datasets.", "published": "2024-11-30 22:22:26", "link": "http://arxiv.org/abs/2412.00600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cognitive Biases in Large Language Models: A Survey and Mitigation\n  Experiments", "abstract": "Large Language Models (LLMs) are trained on large corpora written by humans\nand demonstrate high performance on various tasks. However, as humans are\nsusceptible to cognitive biases, which can result in irrational judgments, LLMs\ncan also be influenced by these biases, leading to irrational decision-making.\nFor example, changing the order of options in multiple-choice questions affects\nthe performance of LLMs due to order bias. In our research, we first conducted\nan extensive survey of existing studies examining LLMs' cognitive biases and\ntheir mitigation. The mitigation techniques in LLMs have the disadvantage that\nthey are limited in the type of biases they can apply or require lengthy inputs\nor outputs. We then examined the effectiveness of two mitigation methods for\nhumans, SoPro and AwaRe, when applied to LLMs, inspired by studies in\ncrowdsourcing. To test the effectiveness of these methods, we conducted\nexperiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the\noutputs before and after applying these methods. The results demonstrate that\nwhile SoPro has little effect, AwaRe enables LLMs to mitigate the effect of\nthese biases and make more rational responses.", "published": "2024-11-30 02:37:59", "link": "http://arxiv.org/abs/2412.00323v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided\n  Strategy Selection", "abstract": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of\nlarge language models (LLMs) by structuring their reasoning processes. However,\nexisting methods face critical limitations: handcrafted demonstrations require\nextensive human expertise, while trigger phrases are prone to inaccuracies. In\nthis paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method,\na novel approach that improves CoT prompting by utilizing uncertainty estimates\nto select effective demonstrations without needing access to model parameters.\nUnlike traditional methods, ZEUS offers high sensitivity in distinguishing\nbetween helpful and ineffective questions, ensuring more precise and reliable\nselection. Our extensive evaluation shows that ZEUS consistently outperforms\nexisting CoT strategies across four challenging reasoning benchmarks,\ndemonstrating its robustness and scalability.", "published": "2024-11-30 04:22:00", "link": "http://arxiv.org/abs/2412.00353v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Node Importance Estimation Leveraging LLMs for Semantic Augmentation in\n  Knowledge Graphs", "abstract": "Node Importance Estimation (NIE) is a task that quantifies the importance of\nnode in a graph. Recent research has investigated to exploit various\ninformation from Knowledge Graphs (KGs) to estimate node importance scores.\nHowever, the semantic information in KGs could be insufficient, missing, and\ninaccurate, which would limit the performance of existing NIE models. To\naddress these issues, we leverage Large Language Models (LLMs) for semantic\naugmentation thanks to the LLMs' extra knowledge and ability of integrating\nknowledge from both LLMs and KGs. To this end, we propose the LLMs Empowered\nNode Importance Estimation (LENIE) method to enhance the semantic information\nin KGs for better supporting NIE tasks. To our best knowledge, this is the\nfirst work incorporating LLMs into NIE. Specifically, LENIE employs a novel\nclustering-based triplet sampling strategy to extract diverse knowledge of a\nnode sampled from the given KG. After that, LENIE adopts the node-specific\nadaptive prompts to integrate the sampled triplets and the original node\ndescriptions, which are then fed into LLMs for generating richer and more\nprecise augmented node descriptions. These augmented descriptions finally\ninitialize node embeddings for boosting the downstream NIE model performance.\nExtensive experiments demonstrate LENIE's effectiveness in addressing semantic\ndeficiencies in KGs, enabling more informative semantic augmentation and\nenhancing existing NIE models to achieve the state-of-the-art performance. The\nsource code of LENIE is freely available at\n\\url{https://github.com/XinyuLin-FZ/LENIE}.", "published": "2024-11-30 13:32:05", "link": "http://arxiv.org/abs/2412.00478v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene\n  Understanding", "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.", "published": "2024-11-30 14:28:53", "link": "http://arxiv.org/abs/2412.00493v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TextClass Benchmark: A Continuous Elo Rating of LLMs in Social Sciences", "abstract": "The TextClass Benchmark project is an ongoing, continuous benchmarking\nprocess that aims to provide a comprehensive, fair, and dynamic evaluation of\nLLMs and transformers for text classification tasks. This evaluation spans\nvarious domains and languages in social sciences disciplines engaged in NLP and\ntext-as-data approach. The leaderboards present performance metrics and\nrelative ranking using a tailored Elo rating system. With each leaderboard\ncycle, novel models are added, fixed test sets can be replaced for unseen,\nequivalent data to test generalisation power, ratings are updated, and a\nMeta-Elo leaderboard combines and weights domain-specific leaderboards. This\narticle presents the rationale and motivation behind the project, explains the\nElo rating system in detail, and estimates Meta-Elo across different\nclassification tasks in social science disciplines. We also present a snapshot\nof the first cycle of classification tasks on incivility data in Chinese,\nEnglish, German and Russian. This ongoing benchmarking process includes not\nonly additional languages such as Arabic, Hindi, and Spanish but also a\nclassification of policy agenda topics, misinformation, among others.", "published": "2024-11-30 17:09:49", "link": "http://arxiv.org/abs/2412.00539v2", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10, 91F20 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Unveiling Performance Challenges of Large Language Models in\n  Low-Resource Healthcare: A Demographic Fairness Perspective", "abstract": "This paper studies the performance of large language models (LLMs),\nparticularly regarding demographic fairness, in solving real-world healthcare\ntasks. We evaluate state-of-the-art LLMs with three prevalent learning\nframeworks across six diverse healthcare tasks and find significant challenges\nin applying LLMs to real-world healthcare tasks and persistent fairness issues\nacross demographic groups. We also find that explicitly providing demographic\ninformation yields mixed results, while LLM's ability to infer such details\nraises concerns about biased health predictions. Utilizing LLMs as autonomous\nagents with access to up-to-date guidelines does not guarantee performance\nimprovement. We believe these findings reveal the critical limitations of LLMs\nin healthcare fairness and the urgent need for specialized research in this\narea.", "published": "2024-11-30 18:52:30", "link": "http://arxiv.org/abs/2412.00554v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Polish Medical Exams: A new dataset for cross-lingual medical knowledge\n  transfer assessment", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice.", "published": "2024-11-30 19:02:34", "link": "http://arxiv.org/abs/2412.00559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opportunities and Challenges of Large Language Models for Low-Resource\n  Languages in Humanities Research", "abstract": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity.", "published": "2024-11-30 00:10:56", "link": "http://arxiv.org/abs/2412.04497v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forma mentis networks predict creativity ratings of short texts via\n  interpretable artificial intelligence in human and GPT-simulated raters", "abstract": "Creativity is a fundamental skill of human cognition. We use textual forma\nmentis networks (TFMN) to extract network (semantic/syntactic associations) and\nemotional features from approximately one thousand human- and GPT3.5-generated\nstories. Using Explainable Artificial Intelligence (XAI), we test whether\nfeatures relative to Mednick's associative theory of creativity can explain\ncreativity ratings assigned by humans and GPT-3.5. Using XGBoost, we examine\nthree scenarios: (i) human ratings of human stories, (ii) GPT-3.5 ratings of\nhuman stories, and (iii) GPT-3.5 ratings of GPT-generated stories. Our findings\nreveal that GPT-3.5 ratings differ significantly from human ratings not only in\nterms of correlations but also because of feature patterns identified with XAI\nmethods. GPT-3.5 favours 'its own' stories and rates human stories differently\nfrom humans. Feature importance analysis with SHAP scores shows that: (i)\nnetwork features are more predictive for human creativity ratings but also for\nGPT-3.5's ratings of human stories; (ii) emotional features played a greater\nrole than semantic/syntactic network structure in GPT-3.5 rating its own\nstories. These quantitative results underscore key limitations in GPT-3.5's\nability to align with human assessments of creativity. We emphasise the need\nfor caution when using GPT-3.5 to assess and generate creative content, as it\ndoes not yet capture the nuanced complexity that characterises human\ncreativity.", "published": "2024-11-30 16:33:48", "link": "http://arxiv.org/abs/2412.00530v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Sample adaptive data augmentation with progressive scheduling", "abstract": "Data augmentation is a widely adopted technique utilized to improve the\nrobustness of automatic speech recognition (ASR). Employing a fixed data\naugmentation strategy for all training data is a common practice. However, it\nis important to note that there can be variations in factors such as background\nnoise, speech rate, etc. among different samples within a single training\nbatch. By using a fixed augmentation strategy, there is a risk that the model\nmay reach a suboptimal state. In addition to the risks of employing a fixed\naugmentation strategy, the model's capabilities may differ across various\ntraining stages. To address these issues, this paper proposes the method of\nsample-adaptive data augmentation with progressive scheduling(PS-SapAug). The\nproposed method applies dynamic data augmentation in a two-stage training\napproach. It employs hybrid normalization to compute sample-specific\naugmentation parameters based on each sample's loss. Additionally, the\nprobability of augmentation gradually increases throughout the training\nprogression. Our method is evaluated on popular ASR benchmark datasets,\nincluding Aishell-1 and Librispeech-100h, achieving up to 8.13% WER reduction\non LibriSpeech-100h test-clean, 6.23% on test-other, and 5.26% on AISHELL-1\ntest set, which demonstrate the efficacy of our approach enhancing performance\nand minimizing errors.", "published": "2024-11-30 09:48:48", "link": "http://arxiv.org/abs/2412.00415v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Audio Deepfake Detection to AI-Generated Music Detection -- A\n  Pathway and Overview", "abstract": "As Artificial Intelligence (AI) technologies continue to evolve, their use in\ngenerating realistic, contextually appropriate content has expanded into\nvarious domains. Music, an art form and medium for entertainment, deeply rooted\ninto human culture, is seeing an increased involvement of AI into its\nproduction. However, despite the effective application of AI music generation\n(AIGM) tools, the unregulated use of them raises concerns about potential\nnegative impacts on the music industry, copyright and artistic integrity,\nunderscoring the importance of effective AIGM detection. This paper provides an\noverview of existing AIGM detection methods. To lay a foundation to the general\nworkings and challenges of AIGM detection, we first review general principles\nof AIGM, including recent advancements in deepfake audios, as well as\nmultimodal detection techniques. We further propose a potential pathway for\nleveraging foundation models from audio deepfake detection to AIGM detection.\nAdditionally, we discuss implications of these tools and propose directions for\nfuture research to address ongoing challenges in the field.", "published": "2024-11-30 19:53:23", "link": "http://arxiv.org/abs/2412.00571v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Raw Audio Classification with Cosine Convolutional Neural Network\n  (CosCovNN)", "abstract": "This study explores the field of audio classification from raw waveform using\nConvolutional Neural Networks (CNNs), a method that eliminates the need for\nextracting specialised features in the pre-processing step. Unlike recent\ntrends in literature, which often focuses on designing frontends or filters for\nonly the initial layers of CNNs, our research introduces the Cosine\nConvolutional Neural Network (CosCovNN) replacing the traditional CNN filters\nwith Cosine filters. The CosCovNN surpasses the accuracy of the equivalent CNN\narchitectures with approximately $77\\%$ less parameters. Our research further\nprogresses with the development of an augmented CosCovNN named Vector Quantised\nCosine Convolutional Neural Network with Memory (VQCCM), incorporating a memory\nand vector quantisation layer VQCCM achieves state-of-the-art (SOTA)\nperformance across five different datasets in comparison with existing\nliterature. Our findings show that cosine filters can greatly improve the\nefficiency and accuracy of CNNs in raw audio classification.", "published": "2024-11-30 01:39:16", "link": "http://arxiv.org/abs/2412.00312v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving speaker verification robustness with synthetic emotional\n  utterances", "abstract": "A speaker verification (SV) system offers an authentication service designed\nto confirm whether a given speech sample originates from a specific speaker.\nThis technology has paved the way for various personalized applications that\ncater to individual preferences. A noteworthy challenge faced by SV systems is\ntheir ability to perform consistently across a range of emotional spectra. Most\nexisting models exhibit high error rates when dealing with emotional utterances\ncompared to neutral ones. Consequently, this phenomenon often leads to missing\nout on speech of interest. This issue primarily stems from the limited\navailability of labeled emotional speech data, impeding the development of\nrobust speaker representations that encompass diverse emotional states.\n  To address this concern, we propose a novel approach employing the CycleGAN\nframework to serve as a data augmentation method. This technique synthesizes\nemotional speech segments for each specific speaker while preserving the unique\nvocal identity. Our experimental findings underscore the effectiveness of\nincorporating synthetic emotional data into the training process. The models\ntrained using this augmented dataset consistently outperform the baseline\nmodels on the task of verifying speakers in emotional speech scenarios,\nreducing equal error rate by as much as 3.64% relative.", "published": "2024-11-30 02:18:26", "link": "http://arxiv.org/abs/2412.00319v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MusicGen-Chord: Advancing Music Generation through Chord Progressions\n  and Interactive Web-UI", "abstract": "MusicGen is a music generation language model (LM) that can be conditioned on\ntextual descriptions and melodic features. We introduce MusicGen-Chord, which\nextends this capability by incorporating chord progression features. This model\nmodifies one-hot encoded melody chroma vectors into multi-hot encoded chord\nchroma vectors, enabling the generation of music that reflects both chord\nprogressions and textual descriptions. Furthermore, we developed\nMusicGen-Remixer, an application utilizing MusicGen-Chord to generate remixes\nof input music conditioned on textual descriptions. Both models are integrated\ninto Replicate's web-UI using cog, facilitating broad accessibility and\nuser-friendly controllable interaction for creating and experiencing\nAI-generated music.", "published": "2024-11-30 02:49:45", "link": "http://arxiv.org/abs/2412.00325v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Atlas: Visualizing and Exploring Audio Datasets", "abstract": "We introduce Audio Atlas, an interactive web application for visualizing\naudio data using text-audio embeddings. Audio Atlas is designed to facilitate\nthe exploration and analysis of audio datasets using a contrastive embedding\nmodel and a vector database for efficient data management and semantic search.\nThe system maps audio embeddings into a two-dimensional space and leverages\nDeepScatter for dynamic visualization. Designed for extensibility, Audio Atlas\nallows easy integration of new datasets, enabling users to better understand\ntheir audio data and identify both patterns and outliers. We open-source the\ncodebase of Audio Atlas, and provide an initial implementation containing\nvarious audio and music datasets.", "published": "2024-11-30 21:35:20", "link": "http://arxiv.org/abs/2412.00591v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personal Sound Zones and Shielded Localized Communication through Active\n  Acoustic Control", "abstract": "In this paper, we present a time domain extension of our strategy on\nmanipulating radiated scalar Helmholtz fields and discuss two important applied\nscenarios, namely (1) creating personal sound zones inside a bounded domain and\n(2) shielded localized communication. Our strategy is based on the authors'\nprevious works establishing the possibility and stability of controlling\nacoustic fields using an array of almost non-radiating coupling sources and\npresents a detailed Fourier synthesis approach towards a time-domain effect. We\nrequire that the array of acoustic sources creates the desired fields on the\ncontrol regions while maintaining a zero field beyond a larger circumscribed\nsphere. This paper recalls the main theoretical results then presents the\nunderlying Fourier synthesis paradigm and show, through relevant simulations,\nthe performance of our strategy.", "published": "2024-11-30 12:04:10", "link": "http://arxiv.org/abs/2412.00456v1", "categories": ["cs.SD", "cs.NA", "eess.AS", "math.AP", "math.NA", "math.OC"], "primary_category": "cs.SD"}
