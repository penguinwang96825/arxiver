{"title": "Mixture of In-Context Experts Enhance LLMs' Long Context Awareness", "abstract": "Many studies have revealed that large language models (LLMs) exhibit uneven\nawareness of different contextual positions. Their limited context awareness\ncan lead to overlooking critical information and subsequent task failures.\nWhile several approaches have been proposed to enhance LLMs' context awareness,\nachieving both effectiveness and efficiency remains challenging. In this paper,\nfor LLMs utilizing RoPE as position embeddings, we introduce a novel method\ncalled \"Mixture of In-Context Experts\" (MoICE) to address this challenge. MoICE\ncomprises two key components: a router integrated into each attention head\nwithin LLMs and a lightweight router-only training optimization strategy: (1)\nMoICE views each RoPE angle as an `in-context' expert, demonstrated to be\ncapable of directing the attention of a head to specific contextual positions.\nConsequently, each attention head flexibly processes tokens using multiple RoPE\nangles dynamically selected by the router to attend to the needed positions.\nThis approach mitigates the risk of overlooking essential contextual\ninformation. (2) The router-only training strategy entails freezing LLM\nparameters and exclusively updating routers for only a few steps. When applied\nto open-source LLMs including Llama and Mistral, MoICE surpasses prior methods\nacross multiple tasks on long context understanding and generation, all while\nmaintaining commendable inference efficiency.", "published": "2024-06-28 01:46:41", "link": "http://arxiv.org/abs/2406.19598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark\n  for Incoherence Detection, Reasoning, and Rewriting", "abstract": "Coherence in writing, an aspect that second-language (L2) English learners\noften struggle with, is crucial in assessing L2 English writing. Existing\nautomated writing evaluation systems primarily use basic surface linguistic\nfeatures to detect coherence in writing. However, little effort has been made\nto correct the detected incoherence, which could significantly benefit L2\nlanguage learners seeking to improve their writing. To bridge this gap, we\nintroduce DECOR, a novel benchmark that includes expert annotations for\ndetecting incoherence in L2 English writing, identifying the underlying\nreasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the\nfirst coherence assessment dataset specifically designed for improving L2\nEnglish writing, featuring pairs of original incoherent sentences alongside\ntheir expert-rewritten counterparts. Additionally, we fine-tuned models to\nautomatically detect and rewrite incoherence in student essays. We find that\nincorporating specific reasons for incoherence during fine-tuning consistently\nimproves the quality of the rewrites, achieving a result that is favored in\nboth automatic and human evaluations.", "published": "2024-06-28 04:38:54", "link": "http://arxiv.org/abs/2406.19650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Le sens de la famille : analyse du vocabulaire de la parent{\u00e9} par les\n  plongements de mots", "abstract": "In this study, we propose a corpus analysis of an area of the French lexicon\nthat is both dense and highly structured: the vocabulary of family\nrelationships. Starting with a lexicon of 25 nouns designating the main\nrelationships (son, cousin, mother, grandfather, sister-in-law etc.), we\nexamine how these terms are positioned in relation to each other through\ndistributional analyses based on the use of these terms in corpora. We show\nthat distributional information can capture certain features that organize this\nvocabulary (descent, alliance, siblings, genre), in ways that vary according to\nthe different corpora compared.", "published": "2024-06-28 08:19:36", "link": "http://arxiv.org/abs/2406.19729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Message du troisi{\u00e8}me type : irruption d'un tiers dans un dialogue en\n  ligne", "abstract": "Our study focuses on Wikipedia talk pages, from a global perspective\nanalyzing contributors' behaviors in online interactions. Using a corpus\ncomprising all Wikipedia talk pages in French, totaling more than 300,000\ndiscussion threads, we examine how discussions with more than two participants\n(multiparty conversation) unfold and we specifically investigate the role of a\nthird participant's intervention when two Wikipedians have already initiated an\nexchange. In this regard, we concentrate on the sequential structure of these\ninteractions in terms of articulation among different participants and aim to\nspecify this third message by exploring its lexical particularities, while also\nproposing an initial typology of the third participant's message role and how\nit aligns with preceding messages.", "published": "2024-06-28 08:20:32", "link": "http://arxiv.org/abs/2406.19731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking the Script Barrier in Multilingual Pre-Trained Language Models\n  with Transliteration-Based Post-Training Alignment", "abstract": "Multilingual pre-trained models (mPLMs) have shown impressive performance on\ncross-lingual transfer tasks. However, the transfer performance is often\nhindered when a low-resource target language is written in a different script\nthan the high-resource source language, even though the two languages may be\nrelated or share parts of their vocabularies. Inspired by recent work that uses\ntransliteration to address this problem, our paper proposes a\ntransliteration-based post-pretraining alignment (PPA) method aiming to improve\nthe cross-lingual alignment between languages using diverse scripts. We select\ntwo areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and\n$\\textbf{South+East Asian Languages}$, wherein the languages are mutually\ninfluenced but use different scripts. We apply our method to these language\ngroups and conduct extensive experiments on a spectrum of downstream tasks. The\nresults show that after PPA, models consistently outperform the original model\n(up to 50% for some tasks) in English-centric transfer. In addition, when we\nuse languages other than English as sources in transfer, our method obtains\neven larger improvements. We will make our code and models publicly available\nat \\url{https://github.com/cisnlp/Transliteration-PPA}.", "published": "2024-06-28 08:59:24", "link": "http://arxiv.org/abs/2406.19759v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Belief Revision: The Adaptability of Large Language Models Reasoning", "abstract": "The capability to reason from text is crucial for real-world NLP\napplications. Real-world scenarios often involve incomplete or evolving data.\nIn response, individuals update their beliefs and understandings accordingly.\nHowever, most existing evaluations assume that language models (LMs) operate\nwith consistent information. We introduce Belief-R, a new dataset designed to\ntest LMs' belief revision ability when presented with new evidence. Inspired by\nhow humans suppress prior inferences, this task assesses LMs within the newly\nproposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of\npremises designed to simulate scenarios where additional information could\nnecessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across\ndiverse prompting strategies and found that LMs generally struggle to\nappropriately revise their beliefs in response to new information. Further,\nmodels adept at updating often underperformed in scenarios without necessary\nupdates, highlighting a critical trade-off. These insights underscore the\nimportance of improving LMs' adaptiveness to changing information, a step\ntoward more reliable AI systems.", "published": "2024-06-28 09:09:36", "link": "http://arxiv.org/abs/2406.19764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Preference Knowledge Distillation for Large Language Models", "abstract": "In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd.", "published": "2024-06-28 09:23:40", "link": "http://arxiv.org/abs/2406.19774v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable and Domain-General Abstractive Proposition Segmentation", "abstract": "Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use.", "published": "2024-06-28 10:24:31", "link": "http://arxiv.org/abs/2406.19803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic register identification for the open web using multilingual\n  deep learning", "abstract": "This article investigates how well deep learning models can identify web\nregisters -- text varieties such as news reports and discussion forums --\nacross 16 languages. We introduce the Multilingual CORE corpora, which contain\n72,504 documents annotated with a hierarchical taxonomy of 25 registers\ndesigned to cover the entire open web. Our multilingual models achieve\nstate-of-the-art results (79% F1 score) using multi-label classification. This\nperformance matches or exceeds previous studies that used simpler\nclassification schemes, showing that models can perform well even with a\ncomplex register scheme at a massively multilingual scale. However, we observe\na consistent performance ceiling around 77-80% F1 score across all models and\nconfigurations. When we remove documents with uncertain labels through data\npruning, performance increases to over 90% F1, suggesting that this ceiling\nstems from inherent ambiguity in web registers rather than model limitations.\nAnalysis of hybrid documents -- texts combining multiple registers -- reveals\nthat the main challenge is not in classifying hybrids themselves, but in\ndistinguishing between hybrid and non-hybrid documents. Multilingual models\nconsistently outperform monolingual ones, particularly helping languages with\nlimited training data. While zero-shot performance drops by an average of 7% on\nunseen languages, this decrease varies substantially between languages (from 3%\nto 20%), indicating that while registers share many features across languages,\nthey also maintain language-specific characteristics.", "published": "2024-06-28 13:00:30", "link": "http://arxiv.org/abs/2406.19892v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Types Elicit Prompt Engineering Capabilities", "abstract": "Much of the success of modern language models depends on finding a suitable\nprompt to instruct the model. Until now, it has been largely unknown how\nvariations in the linguistic expression of prompts affect these models. This\nstudy systematically and empirically evaluates which linguistic features\ninfluence models through paraphrase types, i.e., different linguistic changes\nat particular positions. We measure behavioral changes for five models across\n120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,\nlexico-syntax, discourse, and others). We also control for other prompt\nengineering factors (e.g., prompt length, lexical diversity, and proximity to\ntraining data). Our results show a potential for language models to improve\ntasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%\nmedian gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in\nmorphology and lexicon, i.e., the vocabulary used, showed promise in improving\nprompts. These findings contribute to developing more robust language models\ncapable of handling variability in linguistic expression.", "published": "2024-06-28 13:06:31", "link": "http://arxiv.org/abs/2406.19898v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating LLMs with Preference Optimization on Thought Trees for\n  Generating Rationale in Science Question Scoring", "abstract": "Generating rationales that justify scoring decisions has been a promising way\nto facilitate explainability in automated scoring systems. However, existing\nmethods do not match the accuracy of classifier-based methods. Plus, the\ngenerated rationales often contain hallucinated information. To address these\nissues, we propose a novel framework capable of generating more faithful\nrationales and, more importantly, matching performance with classifier-based\nblack-box scoring systems. We first mimic the human assessment process by\nquerying Large Language Models (LLMs) to generate a thought tree. We then\nsummarise intermediate assessment decisions from each thought tree path for\ncreating synthetic rationale data and rationale preference data. Finally, we\nutilise the generated synthetic data to calibrate LLMs through a two-step\ntraining process: supervised fine-tuning and preference optimization. Extensive\nexperimental results demonstrate that our framework achieves a 38% assessment\nperformance improvement in the QWK score compared to prior work while producing\nhigher-quality rationales, as recognised by human evaluators and LLMs. Our work\nsheds light on the effectiveness of performing preference optimization using\nsynthetic preference data obtained from thought tree paths. Data and code are\navailable at https://github.com/lijiazheng99/thought_tree_assessment.", "published": "2024-06-28 14:33:05", "link": "http://arxiv.org/abs/2406.19949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Reasons For And Against Vaccination From Unstructured Data Using\n  Nichesourcing and AI Data Augmentation", "abstract": "We present Reasons For and Against Vaccination (RFAV), a dataset for\npredicting reasons for and against vaccination, and scientific authorities used\nto justify them, annotated through nichesourcing and augmented using GPT4 and\nGPT3.5-Turbo. We show how it is possible to mine these reasons in\nnon-structured text, under different task definitions, despite the high level\nof subjectivity involved and explore the impact of artificially augmented data\nusing in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset\nand the trained models along with the annotation manual used to train\nannotators and define the task.", "published": "2024-06-28 14:36:31", "link": "http://arxiv.org/abs/2406.19951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating Financial Market via Large Language Model based Agents", "abstract": "Most economic theories typically assume that financial market participants\nare fully rational individuals and use mathematical models to simulate human\nbehavior in financial markets. However, human behavior is often not entirely\nrational and is challenging to predict accurately with mathematical models. In\nthis paper, we propose \\textbf{A}gent-based \\textbf{S}imulated\n\\textbf{F}inancial \\textbf{M}arket (ASFM), which first constructs a simulated\nstock market with a real order matching system. Then, we propose a large\nlanguage model based agent as the stock trader, which contains the profile,\nobservation, and tool-learning based action module. The trading agent can\ncomprehensively understand current market dynamics and financial policy\ninformation, and make decisions that align with their trading strategy. In the\nexperiments, we first verify that the reactions of our ASFM are consistent with\nthe real stock market in two controllable scenarios. In addition, we also\nconduct experiments in two popular economics research directions, and we find\nthat conclusions drawn in our \\model align with the preliminary findings in\neconomics research. Based on these observations, we believe our proposed ASFM\nprovides a new paradigm for economic research.", "published": "2024-06-28 14:54:12", "link": "http://arxiv.org/abs/2406.19966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models", "abstract": "Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models.", "published": "2024-06-28 15:34:26", "link": "http://arxiv.org/abs/2406.19999v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of\n  Large Language Models", "abstract": "Large language models (LLMs) require continual knowledge updates to stay\nabreast of the ever-changing world facts, prompting the formulation of lifelong\nmodel editing task. While recent years have witnessed the development of\nvarious techniques for single and batch editing, these methods either fail to\napply or perform sub-optimally when faced with lifelong editing. In this paper,\nwe introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong\nmodel editing. We first analyze the factors influencing the effectiveness of\nconventional MoE adaptor in lifelong editing, including catastrophic\nforgetting, inconsistent routing and order sensitivity. Based on these\ninsights, we propose a tailored module insertion method to achieve lifelong\nediting, incorporating a novel KV anchor routing to enhance routing consistency\nbetween training and inference stage, along with a concise yet effective\nclustering-based editing order planning. Experimental results demonstrate the\neffectiveness of our method in lifelong editing, surpassing previous model\nediting techniques while maintaining outstanding performance in batch editing\ntask. Our code will be available.", "published": "2024-06-28 16:17:41", "link": "http://arxiv.org/abs/2406.20030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioMNER: A Dataset for Biomedical Method Entity Recognition", "abstract": "Named entity recognition (NER) stands as a fundamental and pivotal task\nwithin the realm of Natural Language Processing. Particularly within the domain\nof Biomedical Method NER, this task presents notable challenges, stemming from\nthe continual influx of domain-specific terminologies in scholarly literature.\nCurrent research in Biomedical Method (BioMethod) NER suffers from a scarcity\nof resources, primarily attributed to the intricate nature of methodological\nconcepts, which necessitate a profound understanding for precise delineation.\nIn this study, we propose a novel dataset for biomedical method entity\nrecognition, employing an automated BioMethod entity recognition and\ninformation retrieval system to assist human annotation. Furthermore, we\ncomprehensively explore a range of conventional and contemporary open-domain\nNER methodologies, including the utilization of cutting-edge large-scale\nlanguage models (LLMs) customised to our dataset. Our empirical findings reveal\nthat the large parameter counts of language models surprisingly inhibit the\neffective assimilation of entity extraction patterns pertaining to biomedical\nmethods. Remarkably, the approach, leveraging the modestly sized ALBERT model\n(only 11MB), in conjunction with conditional random fields (CRF), achieves\nstate-of-the-art (SOTA) performance.", "published": "2024-06-28 16:34:24", "link": "http://arxiv.org/abs/2406.20038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Mitigating Language Confusion in LLMs", "abstract": "We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.", "published": "2024-06-28 17:03:51", "link": "http://arxiv.org/abs/2406.20052v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Word Senses and Beyond: Inducing Concepts with Contextualized\n  Language Models", "abstract": "Polysemy and synonymy are two crucial interrelated facets of lexical\nambiguity. While both phenomena are widely documented in lexical resources and\nhave been studied extensively in NLP, leading to dedicated systems, they are\noften being considered independently in practical problems. While many tasks\ndealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight\nthe role of word's senses, the study of synonymy is rooted in the study of\nconcepts, i.e. meanings shared across the lexicon. In this paper, we introduce\nConcept Induction, the unsupervised task of learning a soft clustering among\nwords that defines a set of concepts directly from data. This task generalizes\nWord Sense Induction. We propose a bi-level approach to Concept Induction that\nleverages both a local lemma-centric view and a global cross-lexicon view to\ninduce concepts. We evaluate the obtained clustering on SemCor's annotated data\nand obtain good performance (BCubed F1 above 0.60). We find that the local and\nthe global levels are mutually beneficial to induce concepts and also senses in\nour setting. Finally, we create static embeddings representing our induced\nconcepts and use them on the Word-in-Context task, obtaining competitive\nperformance with the State-of-the-Art.", "published": "2024-06-28 17:07:06", "link": "http://arxiv.org/abs/2406.20054v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying RLAIF for Code Generation with API-usage in Lightweight LLMs", "abstract": "Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant\npotential across various domains, including mitigating harm in LLM outputs,\nenhancing text summarization, and mathematical reasoning. This paper introduces\nan RLAIF framework for improving the code generation abilities of lightweight\n(<1B parameters) LLMs. We specifically focus on code generation tasks that\nrequire writing appropriate API calls, which is challenging due to the\nwell-known issue of hallucination in LLMs. Our framework extracts AI feedback\nfrom a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and\nuses this data to train a reward model towards better alignment from smaller\nLLMs. We run our experiments on the Gorilla dataset and meticulously assess the\nquality of the model-generated code across various metrics, including AST,\nROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate\naccurately. Our approach significantly enhances the fine-tuned LLM baseline's\nperformance, achieving a 4.5% improvement in executability rate. Notably, a\nsmaller LLM model (780M parameters) trained with RLAIF surpasses a much larger\nfine-tuned baseline with 7B parameters, achieving a 1.0% higher code\nexecutability rate.", "published": "2024-06-28 17:16:03", "link": "http://arxiv.org/abs/2406.20060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaKP: On-Demand Keyphrase Generation", "abstract": "Traditional keyphrase prediction methods predict a single set of keyphrases\nper document, failing to cater to the diverse needs of users and downstream\napplications. To bridge the gap, we introduce on-demand keyphrase generation, a\nnovel paradigm that requires keyphrases that conform to specific high-level\ngoals or intents. For this task, we present MetaKP, a large-scale benchmark\ncomprising four datasets, 7500 documents, and 3760 goals across news and\nbiomedical domains with human-annotated keyphrases. Leveraging MetaKP, we\ndesign both supervised and unsupervised methods, including a multi-task\nfine-tuning approach and a self-consistency prompting method with large\nlanguage models. The results highlight the challenges of supervised\nfine-tuning, whose performance is not robust to distribution shifts. By\ncontrast, the proposed self-consistency prompting approach greatly improves the\nperformance of large language models, enabling GPT-4o to achieve 0.548 SemF1,\nsurpassing the performance of a fully fine-tuned BART-base model. Finally, we\ndemonstrate the potential of our method to serve as a general NLP\ninfrastructure, exemplified by its application in epidemic event detection from\nsocial media.", "published": "2024-06-28 19:02:59", "link": "http://arxiv.org/abs/2407.00191v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection and Measurement of Syntactic Templates in Generated Text", "abstract": "Recent work on evaluating the diversity of text generated by LLMs has focused\non word-level features. Here we offer an analysis of syntactic features to\ncharacterize general repetition in models, beyond frequent n-grams.\nSpecifically, we define syntactic templates and show that models tend to\nproduce templated text in downstream tasks at a higher rate than what is found\nin human-reference texts. We find that most (76%) templates in model-generated\ntext can be found in pre-training data (compared to only 35% of human-authored\ntext), and are not overwritten during fine-tuning processes such as RLHF. This\nconnection to the pre-training data allows us to analyze syntactic templates in\nmodels where we do not have the pre-training data. We also find that templates\nas features are able to differentiate between models, tasks, and domains, and\nare useful for qualitatively evaluating common model constructions. Finally, we\ndemonstrate the use of templates as a useful tool for analyzing style\nmemorization of training data in LLMs.", "published": "2024-06-28 19:34:23", "link": "http://arxiv.org/abs/2407.00211v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EHRmonize: A Framework for Medical Concept Abstraction from Electronic\n  Health Records using Large Language Models", "abstract": "Electronic health records (EHRs) contain vast amounts of complex data, but\nharmonizing and processing this information remains a challenging and costly\ntask requiring significant clinical expertise. While large language models\n(LLMs) have shown promise in various healthcare applications, their potential\nfor abstracting medical concepts from EHRs remains largely unexplored. We\nintroduce EHRmonize, a framework leveraging LLMs to abstract medical concepts\nfrom EHR data. Our study uses medication data from two real-world EHR databases\nto evaluate five LLMs on two free-text extraction and six binary classification\ntasks across various prompting strategies. GPT-4o's with 10-shot prompting\nachieved the highest performance in all tasks, accompanied by Claude-3.5-Sonnet\nin a subset of tasks. GPT-4o achieved an accuracy of 97% in identifying generic\nroute names, 82% for generic drug names, and 100% in performing binary\nclassification of antibiotics. While EHRmonize significantly enhances\nefficiency, reducing annotation time by an estimated 60%, we emphasize that\nclinician oversight remains essential. Our framework, available as a Python\npackage, offers a promising tool to assist clinicians in EHR data abstraction,\npotentially accelerating healthcare research and improving data harmonization\nprocesses.", "published": "2024-06-28 21:39:20", "link": "http://arxiv.org/abs/2407.00242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiffuseDef: Improved Robustness to Adversarial Attacks", "abstract": "Pretrained language models have significantly advanced performance across\nvarious natural language processing tasks. However, adversarial attacks\ncontinue to pose a critical challenge to system built using these models, as\nthey can be exploited with carefully crafted adversarial texts. Inspired by the\nability of diffusion models to predict and reduce noise in computer vision, we\npropose a novel and flexible adversarial defense method for language\nclassification tasks, DiffuseDef, which incorporates a diffusion layer as a\ndenoiser between the encoder and the classifier. During inference, the\nadversarial hidden state is first combined with sampled noise, then denoised\niteratively and finally ensembled to produce a robust text representation. By\nintegrating adversarial training, denoising, and ensembling techniques, we show\nthat DiffuseDef improves over different existing adversarial defense methods\nand achieves state-of-the-art performance against common adversarial attacks.", "published": "2024-06-28 22:36:17", "link": "http://arxiv.org/abs/2407.00248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TreeSeg: Hierarchical Topic Segmentation of Large Transcripts", "abstract": "From organizing recorded videos and meetings into chapters, to breaking down\nlarge inputs in order to fit them into the context window of commoditized Large\nLanguage Models (LLMs), topic segmentation of large transcripts emerges as a\ntask of increasing significance. Still, accurate segmentation presents many\nchallenges, including (a) the noisy nature of the Automatic Speech Recognition\n(ASR) software typically used to obtain the transcripts, (b) the lack of\ndiverse labeled data and (c) the difficulty in pin-pointing the ground-truth\nnumber of segments. In this work we present TreeSeg, an approach that combines\noff-the-shelf embedding models with divisive clustering, to generate\nhierarchical, structured segmentations of transcripts in the form of binary\ntrees. Our approach is robust to noise and can handle large transcripts\nefficiently. We evaluate TreeSeg on the ICSI and AMI corpora, demonstrating\nthat it outperforms all baselines. Finally, we introduce TinyRec, a small-scale\ncorpus of manually annotated transcripts, obtained from self-recorded video\nsessions.", "published": "2024-06-28 23:49:26", "link": "http://arxiv.org/abs/2407.12028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs", "abstract": "Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.", "published": "2024-06-28 01:14:43", "link": "http://arxiv.org/abs/2406.19593v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse\n  Argument Generation", "abstract": "Writing persuasive arguments is a challenging task for both humans and\nmachines. It entails incorporating high-level beliefs from various perspectives\non the topic, along with deliberate reasoning and planning to construct a\ncoherent narrative. Current language models often generate surface tokens\nautoregressively, lacking explicit integration of these underlying controls,\nresulting in limited output diversity and coherence. In this work, we propose a\npersona-based multi-agent framework for argument writing. Inspired by the human\ndebate, we first assign each agent a persona representing its high-level\nbeliefs from a unique perspective, and then design an agent interaction process\nso that the agents can collaboratively debate and discuss the idea to form an\noverall plan for argument writing. Such debate process enables fluid and\nnonlinear development of ideas. We evaluate our framework on argumentative\nessay writing. The results show that our framework can generate more diverse\nand persuasive arguments through both automatic and human evaluations.", "published": "2024-06-28 04:21:20", "link": "http://arxiv.org/abs/2406.19643v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification in Large Language Models Through Convex Hull\n  Analysis", "abstract": "Uncertainty quantification approaches have been more critical in large\nlanguage models (LLMs), particularly high-risk applications requiring reliable\noutputs. However, traditional methods for uncertainty quantification, such as\nprobabilistic models and ensemble techniques, face challenges when applied to\nthe complex and high-dimensional nature of LLM-generated outputs. This study\nproposes a novel geometric approach to uncertainty quantification using convex\nhull analysis. The proposed method leverages the spatial properties of response\nembeddings to measure the dispersion and variability of model outputs. The\nprompts are categorized into three types, i.e., `easy', `moderate', and\n`confusing', to generate multiple responses using different LLMs at varying\ntemperature settings. The responses are transformed into high-dimensional\nembeddings via a BERT model and subsequently projected into a two-dimensional\nspace using Principal Component Analysis (PCA). The Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster\nthe embeddings and compute the convex hull for each selected cluster. The\nexperimental results indicate that the uncertainty of the model for LLMs\ndepends on the prompt complexity, the model, and the temperature setting.", "published": "2024-06-28 07:47:34", "link": "http://arxiv.org/abs/2406.19712v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case\n  Reformulation", "abstract": "Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.", "published": "2024-06-28 08:59:45", "link": "http://arxiv.org/abs/2406.19760v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "NLPerturbator: Studying the Robustness of Code LLMs to Natural Language\n  Variations", "abstract": "Large language models (LLMs) achieve promising results in code generation\nbased on a given natural language description. They have been integrated into\nopen-source projects and commercial products to facilitate daily coding\nactivities. The natural language description in the prompt is crucial for LLMs\nto comprehend users' requirements. Prior studies uncover that LLMs are\nsensitive to the changes in the prompts, including slight changes that look\ninconspicuous. However, the natural language descriptions often vary in\nreal-world scenarios (e.g., different formats, grammar, and wording). Prior\nstudies on the robustness of LLMs are often based on random perturbations and\nsuch perturbations may not actually happen. In this paper, we conduct a\ncomprehensive study to investigate how are code LLMs robust to variations of\nnatural language description in real-world scenarios. We summarize 18\ncategories of perturbations of natural language and 3 combinations of\nco-occurred categories based on our literature review and an online survey with\npractitioners. We propose an automated framework, NLPerturbator, which can\nperform perturbations of each category given a set of prompts. Through a series\nof experiments on code generation using six code LLMs, we find that the\nperturbed prompts can decrease the performance of code generation by a\nconsiderable margin (e.g., up to 21.2%, and 4.8% to 6.1% on average). Our study\nhighlights the importance of enhancing the robustness of LLMs to real-world\nvariations in the prompts, as well as the essentiality of attentively\nconstructing the prompts.", "published": "2024-06-28 09:39:33", "link": "http://arxiv.org/abs/2406.19783v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for\n  Multi-hop Question Answering", "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities.\nNevertheless, they still suffer from factual errors when tackling\nknowledge-intensive tasks. Retrieval-augmented reasoning represents a promising\napproach. However, significant challenges still persist, including inaccurate\nand insufficient retrieval for complex questions, as well as difficulty in\nintegrating multi-source knowledge. To address this, we propose Beam\nAggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive\nmulti-hop QA. BeamAggR explores and prioritizes promising answers at each hop\nof question. Concretely, we parse the complex questions into trees, which\ninclude atom and composite questions, followed by bottom-up reasoning. For\natomic questions, the LLM conducts reasoning on multi-source knowledge to get\nanswer candidates. For composite questions, the LLM combines beam candidates,\nexplores multiple reasoning paths through probabilistic aggregation, and\nprioritizes the most promising trajectory. Extensive experiments on four\nopen-domain multi-hop reasoning datasets show that our method significantly\noutperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that\nBeamAggR elicits better knowledge collaboration and answer aggregation.", "published": "2024-06-28 10:53:48", "link": "http://arxiv.org/abs/2406.19820v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through\n  low-confidence single-token predictions", "abstract": "This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.", "published": "2024-06-28 11:28:44", "link": "http://arxiv.org/abs/2406.19840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "YuLan: An Open-source Large Language Model", "abstract": "Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.", "published": "2024-06-28 11:52:53", "link": "http://arxiv.org/abs/2406.19853v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood", "abstract": "Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT", "published": "2024-06-28 12:28:52", "link": "http://arxiv.org/abs/2406.19874v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Investigating the Timescales of Language Processing with EEG and\n  Language Models", "abstract": "This study explores the temporal dynamics of language processing by examining\nthe alignment between word representations from a pre-trained transformer-based\nlanguage model, and EEG data. Using a Temporal Response Function (TRF) model,\nwe investigate how neural activity corresponds to model representations across\ndifferent layers, revealing insights into the interaction between artificial\nlanguage models and brain responses during language comprehension. Our analysis\nreveals patterns in TRFs from distinct layers, highlighting varying\ncontributions to lexical and compositional processing. Additionally, we used\nlinear discriminant analysis (LDA) to isolate part-of-speech (POS)\nrepresentations, offering insights into their influence on neural responses and\nthe underlying mechanisms of syntactic processing. These findings underscore\nEEG's utility for probing language processing dynamics with high temporal\nresolution. By bridging artificial language models and neural activity, this\nstudy advances our understanding of their interaction at fine timescales.", "published": "2024-06-28 12:49:27", "link": "http://arxiv.org/abs/2406.19884v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via\n  Data Synthesis", "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.", "published": "2024-06-28 14:04:10", "link": "http://arxiv.org/abs/2406.19934v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Into the Unknown: Generating Geospatial Descriptions for New\n  Environments", "abstract": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.", "published": "2024-06-28 14:56:21", "link": "http://arxiv.org/abs/2406.19967v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models", "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community has yet\nto fully understand the hallucination issues within these models. To address\nthis challenge, we introduce a comprehensive diagnostic benchmark, ToolBH.\nSpecifically, we assess the LLM's hallucinations through two perspectives:\ndepth and breadth. In terms of depth, we propose a multi-level diagnostic\nprocess, including (1) solvability detection, (2) solution planning, and (3)\nmissing-tool analysis. For breadth, we consider three scenarios based on the\ncharacteristics of the toolset: missing necessary tools, potential tools, and\nlimited functionality tools. Furthermore, we developed seven tasks and\ncollected 700 evaluation samples through multiple rounds of manual annotation.\nThe results show the significant challenges presented by the ToolBH benchmark.\nThe current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores\nof 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger\nmodel parameters do not guarantee better performance; the training data and\nresponse strategies also play crucial roles in tool-enhanced LLM scenarios. Our\ndiagnostic analysis indicates that the primary reason for model errors lies in\nassessing task solvability. Additionally, open-weight models suffer from\nperformance drops with verbose replies, whereas proprietary models excel with\nlonger reasoning.", "published": "2024-06-28 16:03:30", "link": "http://arxiv.org/abs/2406.20015v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Molecular Facts: Desiderata for Decontextualization in LLM Fact\n  Verification", "abstract": "Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.", "published": "2024-06-28 17:43:48", "link": "http://arxiv.org/abs/2406.20079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "abstract": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.", "published": "2024-06-28 17:54:47", "link": "http://arxiv.org/abs/2406.20086v3", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas", "abstract": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.", "published": "2024-06-28 17:59:01", "link": "http://arxiv.org/abs/2406.20094v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language\n  Understanding in Arabic", "abstract": "Despite the growing importance of Arabic as a global language, there is a\nnotable lack of language models pre-trained exclusively on Arabic data. This\nshortage has led to limited benchmarks available for assessing language model\nperformance in Arabic. To address this gap, we introduce two novel benchmarks\ndesigned to evaluate models' mathematical reasoning and language understanding\nabilities in Arabic. These benchmarks are derived from a General Aptitude Test\n(GAT) called Qiyas exam, a standardized test widely used for university\nadmissions in Saudi Arabia. For validation purposes, we assess the performance\nof ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that\nthese benchmarks pose a significant challenge, with ChatGPT-4 achieving an\noverall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall\naccuracy of 49% across the various question types in the Qiyas benchmark. We\nbelieve the release of these benchmarks will pave the way for enhancing the\nmathematical reasoning and language understanding capabilities of future models\ntailored for the low-resource Arabic language.", "published": "2024-06-28 16:34:31", "link": "http://arxiv.org/abs/2407.00146v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale", "abstract": "We study how well large language models (LLMs) explain their generations\nthrough rationales -- a set of tokens extracted from the input text that\nreflect the decision-making process of LLMs. Specifically, we systematically\nstudy rationales derived using two approaches: (1) popular prompting-based\nmethods, where prompts are used to guide LLMs in generating rationales, and (2)\ntechnical attribution-based methods, which leverage attention or gradients to\nidentify important tokens. Our analysis spans three classification datasets\nwith annotated rationales, encompassing tasks with varying performance levels.\nWhile prompting-based self-explanations are widely used, our study reveals that\nthese explanations are not always as \"aligned\" with the human rationale as\nattribution-based explanations. Even more so, fine-tuning LLMs to enhance\nclassification task accuracy does not enhance the alignment of prompting-based\nrationales. Still, it does considerably improve the alignment of\nattribution-based methods (e.g., InputXGradient). More importantly, we show\nthat prompting-based self-explanation is also less \"faithful\" than\nattribution-based explanations, failing to provide a reliable account of the\nmodel's decision-making process. To evaluate faithfulness, unlike prior studies\nthat excluded misclassified examples, we evaluate all instances and also\nexamine the impact of fine-tuning and accuracy on alignment and faithfulness.\nOur findings suggest that inconclusive faithfulness results reported in earlier\nstudies may stem from low classification accuracy. These findings underscore\nthe importance of more rigorous and comprehensive evaluations of LLM\nrationales.", "published": "2024-06-28 20:06:30", "link": "http://arxiv.org/abs/2407.00219v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Analyzing Lacunae with Transformer-Based Transcription", "abstract": "Historical documents frequently suffer from damage and inconsistencies,\nincluding missing or illegible text resulting from issues such as holes, ink\nproblems, and storage damage. These missing portions or gaps are referred to as\nlacunae. In this study, we employ transformer-based optical character\nrecognition (OCR) models trained on synthetic data containing lacunae in a\nsupervised manner. We demonstrate their effectiveness in detecting and\nrestoring lacunae, achieving a success rate of 65%, compared to a base model\nlacking knowledge of lacunae, which achieves only 5% restoration. Additionally,\nwe investigate the mechanistic properties of the model, such as the log\nprobability of transcription, which can identify lacunae and other errors\n(e.g., mistranscriptions due to complex writing or ink issues) in line images\nwithout directly inspecting the image. This capability could be valuable for\nscholars seeking to distinguish images containing lacunae or errors from clean\nones. Although we explore the potential of attention mechanisms in flagging\nlacunae and transcription errors, our findings suggest it is not a significant\nfactor. Our work highlights a promising direction in utilizing\ntransformer-based OCR models for restoring or analyzing damaged historical\ndocuments.", "published": "2024-06-28 22:52:39", "link": "http://arxiv.org/abs/2407.00250v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code\n  Generation", "abstract": "Recently, large language models (LLMs) have demonstrated excellent\nperformance in understanding human instructions and generating code, which has\ninspired researchers to explore the feasibility of generating RTL code with\nLLMs. However, the existing approaches to fine-tune LLMs on RTL codes typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data. To mitigate these issues , we\nintroduce a simple yet effective iterative training paradigm named ITERTL.\nDuring each iteration, samples are drawn from the model trained in the previous\ncycle. Then these new samples are employed for training in this loop. Through\nthis iterative approach, the distribution mismatch between the model and the\ntraining samples is reduced. Additionally, the model is thus enabled to explore\na broader generative space and receive more comprehensive feedback. Theoretical\nanalyses are conducted to investigate the mechanism of the effectiveness.\nExperimental results show the model trained through our proposed approach can\ncompete with and even outperform the state-of-the-art (SOTA) open-source model\nwith nearly 37\\% reference samples, achieving remarkable 42.9\\% and 62.2\\%\npass@1 rate on two VerilogEval evaluation datasets respectively. While using\nthe same amount of reference samples, our method can achieved a relative\nimprovement of 16.9\\% and 12.5\\% in pass@1 compared to the non-iterative\nmethod. This study facilitates the application of LLMs for generating RTL code\nin practical scenarios with limited data.", "published": "2024-06-28 01:44:57", "link": "http://arxiv.org/abs/2407.12022v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for\n  Foundation Models", "abstract": "Due to the rapid advancements in multimodal large language models, evaluating\ntheir multimodal mathematical capabilities continues to receive wide attention.\nDespite the datasets like MathVista proposed benchmarks for assessing\nmathematical capabilities in multimodal scenarios, there is still a lack of\ncorresponding evaluation tools and datasets for fine-grained assessment in the\ncontext of K12 education in Chinese language. To systematically evaluate the\ncapability of multimodal large models in solving Chinese multimodal\nmathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation\nBenchmark, named CMMaTH, contraining 23k multimodal K12 math related questions,\nforming the largest Chinese multimodal mathematical problem benchmark to date.\nCMMaTH questions from elementary to high school levels, provide increased\ndiversity in problem types, solution objectives, visual elements, detailed\nknowledge points, and standard solution annotations. We have constructed an\nopen-source tool GradeGPT integrated with the CMMaTH dataset, facilitating\nstable, rapid, and cost-free model evaluation. Our data and code are available.", "published": "2024-06-28 02:35:51", "link": "http://arxiv.org/abs/2407.12023v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising\n  Adventures with a High-Impact NLP Journal", "abstract": "We show the fraught side of the academic publishing realm and illustrate it\nthrough a recent case study with an NLP journal.", "published": "2024-06-28 10:58:42", "link": "http://arxiv.org/abs/2407.12026v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computational Politeness in Natural Language Processing: A Survey", "abstract": "Computational approach to politeness is the task of automatically predicting\nand generating politeness in text. This is a pivotal task for conversational\nanalysis, given the ubiquity and challenges of politeness in interactions. The\ncomputational approach to politeness has witnessed great interest from the\nconversational analysis community. This article is a compilation of past works\nin computational politeness in natural language processing. We view four\nmilestones in the research so far, viz. supervised and weakly-supervised\nfeature extraction to identify and induce politeness in a given text,\nincorporation of context beyond the target text, study of politeness across\ndifferent social factors, and study the relationship between politeness and\nvarious sociolinguistic cues. In this article, we describe the datasets,\napproaches, trends, and issues in computational politeness research. We also\ndiscuss representative performance values and provide pointers to future works,\nas given in the prior works. In terms of resources to understand the\nstate-of-the-art, this survey presents several valuable illustrations, most\nprominently, a table summarizing the past papers along different dimensions,\nsuch as the types of features, annotation techniques, and datasets used.", "published": "2024-06-28 06:46:36", "link": "http://arxiv.org/abs/2407.12814v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMLT-MUGC: Small, Medium, and Large Texts -- Machine versus\n  User-Generated Content Detection and Comparison", "abstract": "Large language models (LLMs) have gained significant attention due to their\nability to mimic human language. Identifying texts generated by LLMs is crucial\nfor understanding their capabilities and mitigating potential consequences.\nThis paper analyzes datasets of varying text lengths: small, medium, and large.\nWe compare the performance of machine learning algorithms on four datasets: (1)\nsmall (tweets from Election, FIFA, and Game of Thrones), (2) medium (Wikipedia\nintroductions and PubMed abstracts), and (3) large (OpenAI web text dataset).\nOur results indicate that LLMs with very large parameters (such as the XL-1542\nvariant of GPT2 with 1542 million parameters) were harder (74%) to detect using\ntraditional machine learning methods. However, detecting texts of varying\nlengths from LLMs with smaller parameters (762 million or less) can be done\nwith high accuracy (96% and above). We examine the characteristics of human and\nmachine-generated texts across multiple dimensions, including linguistics,\npersonality, sentiment, bias, and morality. Our findings indicate that\nmachine-generated texts generally have higher readability and closely mimic\nhuman moral judgments but differ in personality traits. SVM and Voting\nClassifier (VC) models consistently achieve high performance across most\ndatasets, while Decision Tree (DT) models show the lowest performance. Model\nperformance drops when dealing with rephrased texts, particularly shorter texts\nlike tweets. This study underscores the challenges and importance of detecting\nLLM-generated texts and suggests directions for future research to improve\ndetection methods and understand the nuanced capabilities of LLMs.", "published": "2024-06-28 22:19:01", "link": "http://arxiv.org/abs/2407.12815v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IDT: Dual-Task Adversarial Attacks for Privacy Protection", "abstract": "Natural language processing (NLP) models may leak private information in\ndifferent ways, including membership inference, reconstruction or attribute\ninference attacks. Sensitive information may not be explicit in the text, but\nhidden in underlying writing characteristics. Methods to protect privacy can\ninvolve using representations inside models that are demonstrated not to detect\nsensitive attributes or -- for instance, in cases where users might not trust a\nmodel, the sort of scenario of interest here -- changing the raw text before\nmodels can have access to it. The goal is to rewrite text to prevent someone\nfrom inferring a sensitive attribute (e.g. the gender of the author, or their\nlocation by the writing style) whilst keeping the text useful for its original\nintention (e.g. the sentiment of a product review). The few works tackling this\nhave focused on generative techniques. However, these often create extensively\ndifferent texts from the original ones or face problems such as mode collapse.\nThis paper explores a novel adaptation of adversarial attack techniques to\nmanipulate a text to deceive a classifier w.r.t one task (privacy) whilst\nkeeping the predictions of another classifier trained for another task\n(utility) unchanged. We propose IDT, a method that analyses predictions made by\nauxiliary and interpretable models to identify which tokens are important to\nchange for the privacy task, and which ones should be kept for the utility\ntask. We evaluate different datasets for NLP suitable for different tasks.\nAutomatic and human evaluations show that IDT retains the utility of text,\nwhile also outperforming existing methods when deceiving a classifier w.r.t\nprivacy task.", "published": "2024-06-28 04:14:35", "link": "http://arxiv.org/abs/2406.19642v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Designing and Evaluating Multi-Chatbot Interface for Human-AI\n  Communication: Preliminary Findings from a Persuasion Task", "abstract": "The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.", "published": "2024-06-28 04:33:41", "link": "http://arxiv.org/abs/2406.19648v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Less is More: Accurate Speech Recognition & Translation without\n  Web-Scale Data", "abstract": "Recent advances in speech recognition and translation rely on hundreds of\nthousands of hours of Internet speech data. We argue that state-of-the art\naccuracy can be reached without relying on web-scale data. Canary -\nmultilingual ASR and speech translation model, outperforms current\nstate-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,\nSpanish, and German languages, while being trained on an order of magnitude\nless data than these models. Three key factors enables such data-efficient\nmodel: (1) a FastConformer-based attention encoder-decoder architecture (2)\ntraining on synthetic data generated with machine translation and (3) advanced\ntraining techniques: data-balancing, dynamic data blending, dynamic bucketing\nand noise-robust fine-tuning. The model, weights, and training code will be\nopen-sourced.", "published": "2024-06-28 06:22:23", "link": "http://arxiv.org/abs/2406.19674v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment", "abstract": "This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.", "published": "2024-06-28 08:25:27", "link": "http://arxiv.org/abs/2406.19736v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Interactive Topic Models with Optimal Transport", "abstract": "Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.", "published": "2024-06-28 13:57:27", "link": "http://arxiv.org/abs/2406.19928v1", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BESTOW: Efficient and Streamable Speech Language Model with the Best of\n  Two Worlds in GPT and T5", "abstract": "Incorporating speech understanding capabilities into pretrained\nlarge-language models has become a vital research direction (SpeechLLM). The\nprevious architectures can be categorized as: i) GPT-style, prepend speech\nprompts to the text prompts as a sequence of LLM inputs like a decoder-only\nmodel; ii) T5-style, introduce speech cross-attention to each layer of the\npretrained LLMs. We propose BESTOW architecture to bring the BESt features from\nTwO Worlds into a single model that is highly efficient and has strong\nmultitask capabilities. Moreover, there is no clear streaming solution for\neither style, especially considering the solution should generalize to speech\nmultitask. We reformulate streamable SpeechLLM as a read-write policy problem\nand unifies the offline and streaming research with BESTOW architecture. Hence\nwe demonstrate the first open-source SpeechLLM solution that enables Streaming\nand Multitask at scale (beyond ASR) at the same time. This streamable solution\nachieves very strong performance on a wide range of speech tasks (ASR, AST,\nSQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower\ntraining/inference cost, and demonstrates LLM knowledge transferability to\nspeech.", "published": "2024-06-28 14:40:03", "link": "http://arxiv.org/abs/2406.19954v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Single Parent Family: A Spectrum of Family Members from a Single\n  Pre-Trained Foundation Model", "abstract": "This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.", "published": "2024-06-28 15:27:57", "link": "http://arxiv.org/abs/2406.19995v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation", "abstract": "Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.", "published": "2024-06-28 17:05:46", "link": "http://arxiv.org/abs/2406.20053v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs", "abstract": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose $\\texttt{Web2Code}$, a benchmark consisting of a new\nlarge-scale webpage-to-code dataset for instruction tuning and an evaluation\nframework for the webpage understanding and HTML code translation abilities of\nMLLMs. For dataset construction, we leverage pretrained LLMs to enhance\nexisting webpage-to-code datasets as well as generate a diverse pool of new\nwebpages rendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain. We hope\nour work will contribute to the development of general MLLMs suitable for\nweb-based content generation and task automation. Our data and code are\navailable at https://github.com/MBZUAI-LLM/web2code.", "published": "2024-06-28 17:59:46", "link": "http://arxiv.org/abs/2406.20098v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Simple Attention-Based Mechanism for Bimodal Emotion Classification", "abstract": "Big data contain rich information for machine learning algorithms to utilize\nwhen learning important features during classification tasks. Human beings\nexpress their emotion using certain words, speech (tone, pitch, speed) or\nfacial expression. Artificial Intelligence approach to emotion classification\nare largely based on learning from textual information. However, public\ndatasets containing text and speech data provide sufficient resources to train\nmachine learning algorithms for the tack of emotion classification. In this\npaper, we present novel bimodal deep learning-based architectures enhanced with\nattention mechanism trained and tested on text and speech data for emotion\nclassification. We report details of different deep learning based\narchitectures and show the performance of each architecture including rigorous\nerror analyses. Our finding suggests that deep learning based architectures\ntrained on different types of data (text and speech) outperform architectures\ntrained only on text or speech. Our proposed attention-based bimodal\narchitecture outperforms several state-of-the-art systems in emotion\nclassification.", "published": "2024-06-28 10:43:02", "link": "http://arxiv.org/abs/2407.00134v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert\n  Prompts", "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to\nnovel tasks when prompted with language instructions and in-context demos.\nSince this ability sensitively depends on the quality of prompts, various\nmethods have been explored to automate the instruction design. While these\nmethods demonstrated promising results, they also restricted the searched\nprompt to one instruction. Such simplification significantly limits their\ncapacity, as a single demo-free instruction might not be able to cover the\nentire complex problem space of the targeted task. To alleviate this issue, we\nadopt the Mixture-of-Expert paradigm and divide the problem space into a set of\nsub-regions; Each sub-region is governed by a specialized expert, equipped with\nboth an instruction and a set of demos. A two-phase process is developed to\nconstruct the specialized expert for each region: (1) demo assignment: Inspired\nby the theoretical connection between in-context learning and kernel\nregression, we group demos into experts based on their semantic similarity; (2)\ninstruction assignment: A region-based joint search of an instruction per\nexpert complements the demos assigned to it, yielding a synergistic effect. The\nresulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win\nrate of 81% against prior arts across several major benchmarks.", "published": "2024-06-28 23:05:08", "link": "http://arxiv.org/abs/2407.00256v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML", "68T01"], "primary_category": "cs.AI"}
{"title": "From Local Concepts to Universals: Evaluating the Multicultural\n  Understanding of Vision-Language Models", "abstract": "Despite recent advancements in vision-language models, their performance\nremains suboptimal on images from non-western cultures due to\nunderrepresentation in training datasets. Various benchmarks have been proposed\nto test models' cultural inclusivity, but they have limited coverage of\ncultures and do not adequately assess cultural diversity across universal as\nwell as culture-specific local concepts. To address these limitations, we\nintroduce the GlobalRG benchmark, comprising two challenging tasks: retrieval\nacross universals and cultural visual grounding. The former task entails\nretrieving culturally diverse images for universal concepts from 50 countries,\nwhile the latter aims at grounding culture-specific concepts within images from\n15 countries. Our evaluation across a wide range of models reveals that the\nperformance varies significantly across cultures -- underscoring the necessity\nfor enhancing multicultural understanding in vision-language models.", "published": "2024-06-28 23:28:28", "link": "http://arxiv.org/abs/2407.00263v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficacy of Various Large Language Models in Generating Smart Contracts", "abstract": "This study analyzes the application of code-generating Large Language Models\nin the creation of immutable Solidity smart contracts on the Ethereum\nBlockchain. Other works have previously analyzed Artificial Intelligence code\ngeneration abilities. This paper aims to expand this to a larger scope to\ninclude programs where security and efficiency are of utmost priority such as\nsmart contracts. The hypothesis leading into the study was that LLMs in general\nwould have difficulty in rigorously implementing security details in the code,\nwhich was shown through our results, but surprisingly generally succeeded in\nmany common types of contracts. We also discovered a novel way of generating\nsmart contracts through new prompting strategies.", "published": "2024-06-28 17:31:47", "link": "http://arxiv.org/abs/2407.11019v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.2"], "primary_category": "cs.SE"}
{"title": "ProgressGym: Alignment with a Millennium of Moral Progress", "abstract": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.", "published": "2024-06-28 17:55:24", "link": "http://arxiv.org/abs/2406.20087v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.LG"}
{"title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy", "abstract": "Vision Language Models (VLMs) have recently been leveraged to generate\nrobotic actions, forming Vision-Language-Action (VLA) models. However, directly\nadapting a pretrained VLM for robotic control remains challenging, particularly\nwhen constrained by a limited number of robot demonstrations. In this work, we\nintroduce LLaRA: Large Language and Robotics Assistant, a framework that\nformulates robot action policy as visuo-textual conversations and enables an\nefficient transfer of a pretrained VLM into a powerful VLA, motivated by the\nsuccess of visual instruction tuning in Computer Vision. First, we present an\nautomated pipeline to generate conversation-style instruction tuning data for\nrobots from existing behavior cloning datasets, aligning robotic actions with\nimage pixel coordinates. Further, we enhance this dataset in a self-supervised\nmanner by defining six auxiliary tasks, without requiring any additional action\nannotations. We show that a VLM finetuned with a limited amount of such\ndatasets can produce meaningful action decisions for robotic control. Through\nexperiments across multiple simulated and real-world tasks, we demonstrate that\nLLaRA achieves state-of-the-art performance while preserving the generalization\ncapabilities of large language models. The code, datasets, and pretrained\nmodels are available at https://github.com/LostXine/LLaRA.", "published": "2024-06-28 17:59:12", "link": "http://arxiv.org/abs/2406.20095v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of\n  Automatic Data Annotation Approach", "abstract": "In recent years, the United States has witnessed a significant surge in the\npopularity of vaping or e-cigarette use, leading to a notable rise in cases of\ne-cigarette and vaping use-associated lung injury (EVALI) that caused\nhospitalizations and fatalities during the EVALI outbreak in 2019, highlighting\nthe urgency to comprehend vaping behaviors and develop effective strategies for\ncessation. Due to the ubiquity of social media platforms, over 4.7 billion\nusers worldwide use them for connectivity, communications, news, and\nentertainment with a significant portion of the discourse related to health,\nthereby establishing social media data as an invaluable organic data resource\nfor public health research. In this study, we extracted a sample dataset from\none vaping sub-community on Reddit to analyze users' quit-vaping intentions.\nLeveraging OpenAI's latest large language model GPT-4 for sentence-level quit\nvaping intention detection, this study compares the outcomes of this model\nagainst layman and clinical expert annotations. Using different prompting\nstrategies such as zero-shot, one-shot, few-shot and chain-of-thought\nprompting, we developed 8 prompts with varying levels of detail to explain the\ntask to GPT-4 and also evaluated the performance of the strategies against each\nother. These preliminary findings emphasize the potential of GPT-4 in social\nmedia data analysis, especially in identifying users' subtle intentions that\nmay elude human detection.", "published": "2024-06-28 18:06:48", "link": "http://arxiv.org/abs/2407.00167v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR", "abstract": "Mixture-of-experts (MoE) models have achieved excellent results in many\ntasks. However, conventional MoE models are often very large, making them\nchallenging to deploy on resource-constrained edge devices. In this paper, we\npropose a novel speaker adaptive mixture of LoRA experts (SAML) approach, which\nuses low-rank adaptation (LoRA) modules as experts to reduce the number of\ntrainable parameters in MoE. Specifically, SAML is applied to the quantised and\npersonalised end-to-end automatic speech recognition models, which combines\ntest-time speaker adaptation to improve the performance of heavily compressed\nmodels in speaker-specific scenarios. Experiments have been performed on the\nLibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in\nmodel size, 29.1% and 31.1% relative word error rate reductions were achieved\non the quantised Whisper model and Conformer-based attention-based\nencoder-decoder ASR model respectively, comparing to the original full\nprecision models.", "published": "2024-06-28 07:37:34", "link": "http://arxiv.org/abs/2406.19706v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for\n  Dynamic Speech Enhancement and Localization", "abstract": "The training of deep learning-based multichannel speech enhancement and\nsource localization systems relies heavily on the simulation of room impulse\nresponse and multichannel diffuse noise, due to the lack of large-scale\nreal-recorded datasets. However, the acoustic mismatch between simulated and\nreal-world data could degrade the model performance when applying in real-world\nscenarios. To bridge this simulation-to-real gap, this paper presents a new\nrelatively large-scale Real-recorded and annotated Microphone Array\nspeech&Noise (RealMAN) dataset. The proposed dataset is valuable in two\naspects: 1) benchmarking speech enhancement and localization algorithms in real\nscenarios; 2) offering a substantial amount of real-world training data for\npotentially improving the performance of real-world applications. Specifically,\na 32-channel array with high-fidelity microphones is used for recording. A\nloudspeaker is used for playing source speech signals (about 35 hours of\nMandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for\nstatic speaker and 35.4 hours for moving speaker) are recorded in 32 different\nscenes, and 144.5 hours of background noise are recorded in 31 different\nscenes. Both speech and noise recording scenes cover various common indoor,\noutdoor, semi-outdoor and transportation environments, which enables the\ntraining of general-purpose speech enhancement and source localization\nnetworks. To obtain the task-specific annotations, speaker location is\nannotated with an omni-directional fisheye camera by automatically detecting\nthe loudspeaker. The direct-path signal is set as the target clean speech for\nspeech enhancement, which is obtained by filtering the source speech signal\nwith an estimated direct-path propagation filter.", "published": "2024-06-28 14:47:13", "link": "http://arxiv.org/abs/2406.19959v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Network Bending of Diffusion Models for Audio-Visual Generation", "abstract": "In this paper we present the first steps towards the creation of a tool which\nenables artists to create music visualizations using pre-trained, generative,\nmachine learning models. First, we investigate the application of network\nbending, the process of applying transforms within the layers of a generative\nnetwork, to image generation diffusion models by utilizing a range of\npoint-wise, tensor-wise, and morphological operators. We identify a number of\nvisual effects that result from various operators, including some that are not\neasily recreated with standard image editing tools. We find that this process\nallows for continuous, fine-grain control of image generation which can be\nhelpful for creative applications. Next, we generate music-reactive videos\nusing Stable Diffusion by passing audio features as parameters to network\nbending operators. Finally, we comment on certain transforms which radically\nshift the image and the possibilities of learning more about the latent space\nof Stable Diffusion based on these transforms.", "published": "2024-06-28 00:39:17", "link": "http://arxiv.org/abs/2406.19589v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Labeled Human Voice Signal Dataset for Misbehavior Detection", "abstract": "Voice signal classification based on human behaviours involves analyzing\nvarious aspects of speech patterns and delivery styles. In this study, a\nreal-time dataset collection is performed where participants are instructed to\nspeak twelve psychology questions in two distinct manners: first, in a harsh\nvoice, which is categorized as \"misbehaved\"; and second, in a polite manner,\ncategorized as \"normal\". These classifications are crucial in understanding how\ndifferent vocal behaviours affect the interpretation and classification of\nvoice signals. This research highlights the significance of voice tone and\ndelivery in automated machine-learning systems for voice analysis and\nrecognition. This research contributes to the broader field of voice signal\nanalysis by elucidating the impact of human behaviour on the perception and\ncategorization of voice signals, thereby enhancing the development of more\naccurate and context-aware voice recognition technologies.", "published": "2024-06-28 18:55:07", "link": "http://arxiv.org/abs/2407.00188v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
