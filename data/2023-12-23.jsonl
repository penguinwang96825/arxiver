{"title": "Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary\n  Empirical Study", "abstract": "Zero-shot keyphrase extraction aims to build a keyphrase extractor without\ntraining by human-annotated data, which is challenging due to the limited human\nintervention involved. Challenging but worthwhile, zero-shot setting\nefficiently reduces the time and effort that data labeling takes. Recent\nefforts on pre-trained large language models (e.g., ChatGPT and ChatGLM) show\npromising performance on zero-shot settings, thus inspiring us to explore\nprompt-based methods. In this paper, we ask whether strong keyphrase extraction\nmodels can be constructed by directly prompting the large language model\nChatGPT. Through experimental results, it is found that ChatGPT still has a lot\nof room for improvement in the keyphrase extraction task compared to existing\nstate-of-the-art unsupervised and supervised models.", "published": "2023-12-23 03:50:49", "link": "http://arxiv.org/abs/2312.15156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Bias Detection and Mitigation for Indian Languages", "abstract": "Lack of diverse perspectives causes neutrality bias in Wikipedia content\nleading to millions of worldwide readers getting exposed by potentially\ninaccurate information. Hence, neutrality bias detection and mitigation is a\ncritical problem. Although previous studies have proposed effective solutions\nfor English, no work exists for Indian languages. First, we contribute two\nlarge datasets, mWikiBias and mWNC, covering 8 languages, for the bias\ndetection and mitigation tasks respectively. Next, we investigate the\neffectiveness of popular multilingual Transformer-based models for the two\ntasks by modeling detection as a binary classification problem and mitigation\nas a style transfer problem. We make the code and data publicly available.", "published": "2023-12-23 07:36:20", "link": "http://arxiv.org/abs/2312.15181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering", "abstract": "Multi-hop question answering (MQA) is one of the challenging tasks to\nevaluate machine's comprehension and reasoning abilities, where large language\nmodels (LLMs) have widely achieved the human-comparable performance. Due to the\ndynamics of knowledge facts in real world, knowledge editing has been explored\nto update model with the up-to-date facts while avoiding expensive re-training\nor fine-tuning. Starting from the edited fact, the updated model needs to\nprovide cascading changes in the chain of MQA. The previous art simply adopts a\nmix-up prompt to instruct LLMs conducting multiple reasoning tasks\nsequentially, including question decomposition, answer generation, and conflict\nchecking via comparing with edited facts. However, the coupling of these\nfunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending\nand answering questions while disturbing them with the unskilled task of\nconflict checking. We thus propose a framework, Programmable knowledge editing\nfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,\nwe prompt LLMs to decompose knowledge-augmented multi-hop question, while\ninteracting with a detached trainable scope detector to modulate LLMs behavior\ndepending on external conflict signal. The experiments on three LLM backbones\nand two benchmark datasets validate our superiority in knowledge editing of\nMQA, outperforming all competitors by a large margin in almost all settings and\nconsistently producing reliable reasoning process.", "published": "2023-12-23 08:32:13", "link": "http://arxiv.org/abs/2312.15194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reverse Multi-Choice Dialogue Commonsense Inference with\n  Graph-of-Thought", "abstract": "With the proliferation of dialogic data across the Internet, the Dialogue\nCommonsense Multi-choice Question Answering (DC-MCQ) task has emerged as a\nresponse to the challenge of comprehending user queries and intentions.\nAlthough prevailing methodologies exhibit effectiveness in addressing\nsingle-choice questions, they encounter difficulties in handling multi-choice\nqueries due to the heightened intricacy and informational density. In this\npaper, inspired by the human cognitive process of progressively excluding\noptions, we propose a three-step Reverse Exclusion Graph-of-Thought (ReX-GoT)\nframework, including Option Exclusion, Error Analysis, and Combine Information.\nSpecifically, our ReX-GoT mimics human reasoning by gradually excluding\nirrelevant options and learning the reasons for option errors to choose the\noptimal path of the GoT and ultimately infer the correct answer. By\nprogressively integrating intricate clues, our method effectively reduces the\ndifficulty of multi-choice reasoning and provides a novel solution for DC-MCQ.\nExtensive experiments on the CICERO and CICERO$_{v2}$ datasets validate the\nsignificant improvement of our approach on DC-MCQ task. On zero-shot setting,\nour model outperform the best baseline by 17.67% in terms of F1 score for the\nmulti-choice task. Most strikingly, our GPT3.5-based ReX-GoT framework achieves\na remarkable 39.44% increase in F1 score.", "published": "2023-12-23 16:18:47", "link": "http://arxiv.org/abs/2312.15291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Greedy Grammar Induction with Indirect Negative Evidence", "abstract": "This paper offers a fresh look at the pumping lemma constant as an upper\nbound on the information required for learning Context Free Grammars. An\nobjective function based on indirect negative evidence considers the\noccurrences, and non-occurrences, of a finite number of strings, encountered\nafter a sufficiently long presentation. This function has optimal substructure\nin the hypotheses space, giving rise to a greedy search learner in a branch and\nbound method. A hierarchy of learnable classes is defined in terms of the\nnumber of production rules that must be added to interim solutions in order to\nincrementally fit the input. Efficiency strongly depends on the position of the\ntarget grammar in the hierarchy and on the richness of the input.", "published": "2023-12-23 18:43:56", "link": "http://arxiv.org/abs/2312.15321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and\n  Person Name Recognition", "abstract": "ChatGPT's proficiency in handling modern standard languages suggests\npotential for its use in understanding ancient Chinese. This paper explores\nChatGPT's capabilities on ancient Chinese via two tasks: translating ancient\nChinese to modern Chinese and recognizing ancient Chinese names. A comparison\nof ChatGPT's output with human translations serves to evaluate its\ncomprehension of ancient Chinese. The findings indicate that: (1.)the\nproficiency of ancient Chinese by ChatGPT is yet to reach a satisfactory level;\n(2.) ChatGPT performs the best on ancient-to-modern translation when feeding\nwith three context sentences. To help reproduce our work, we display the python\ncode snippets used in this study.", "published": "2023-12-23 17:30:28", "link": "http://arxiv.org/abs/2312.15304v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue", "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks\nsuch as chatting, reasoning, and question-answering. However, standard LLMs may\nignore crucial paralinguistic information, such as sentiment, emotion, and\nspeaking style, which are essential for achieving natural, human-like spoken\nconversation, especially when such information is conveyed by acoustic cues. We\ntherefore propose Paralinguistics-enhanced Generative Pretrained Transformer\n(ParalinGPT), an LLM that utilizes text and speech modalities to better model\nthe linguistic content and paralinguistic attributes of spoken dialogue. The\nmodel takes the conversational context of text, speech embeddings, and\nparalinguistic attributes as input prompts within a serialized multitasking\nmultimodal framework. Specifically, our framework serializes tasks in the order\nof current paralinguistic attribute prediction, response paralinguistic\nattribute prediction, and response text generation with autoregressive\nconditioning. We utilize the Switchboard-1 corpus, including its sentiment\nlabels as the paralinguistic attribute, as our spoken dialogue dataset.\nExperimental results indicate the proposed serialized multitasking method\noutperforms typical sequence classification techniques on current and response\nsentiment classification. Furthermore, leveraging conversational context and\nspeech embeddings significantly improves both response text generation and\nsentiment prediction. Our proposed framework achieves relative improvements of\n6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment\naccuracy, and response text BLEU score, respectively.", "published": "2023-12-23 18:14:56", "link": "http://arxiv.org/abs/2312.15316v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large\n  Language Model Inference", "abstract": "Recent advancements in large language models (LLMs) boasting billions of\nparameters have generated a significant demand for efficient deployment in\ninference workloads. The majority of existing approaches rely on temporal\narchitectures that reuse hardware units for different network layers and\noperators. However, these methods often encounter challenges in achieving low\nlatency due to considerable memory access overhead. This paper investigates the\nfeasibility and potential of model-specific spatial acceleration for LLM\ninference on FPGAs. Our approach involves the specialization of distinct\nhardware units for specific operators or layers, facilitating direct\ncommunication between them through a dataflow architecture while minimizing\noff-chip memory accesses. We introduce a comprehensive analytical model for\nestimating the performance of a spatial LLM accelerator, taking into account\nthe on-chip compute and memory resources available on an FPGA. Through our\nanalysis, we can determine the scenarios in which FPGA-based spatial\nacceleration can outperform its GPU-based counterpart. To enable more\nproductive implementations of an LLM model on FPGAs, we further provide a\nlibrary of high-level synthesis (HLS) kernels that are composable and reusable.\nThis library will be made available as open-source. To validate the\neffectiveness of both our analytical model and HLS library, we have implemented\nBERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results\ndemonstrate our approach can achieve up to 13.4x speedup when compared to\nprevious FPGA-based accelerators for the BERT model. For GPT generative\ninference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the\nprefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy\nefficiency compared to the NVIDIA A100 GPU in the decode stage.", "published": "2023-12-23 04:27:06", "link": "http://arxiv.org/abs/2312.15159v2", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective\n  Depth Up-Scaling", "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion\nparameters, demonstrating superior performance in various natural language\nprocessing (NLP) tasks. Inspired by recent efforts to efficiently up-scale\nLLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which\nencompasses depthwise scaling and continued pretraining. In contrast to other\nLLM up-scaling methods that use mixture-of-experts, DUS does not require\ncomplex changes to train and inference efficiently. We show experimentally that\nDUS is simple yet effective in scaling up high-performance LLMs from small\nones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\na variant fine-tuned for instruction-following capabilities, surpassing\nMixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0\nlicense, promoting broad access and application in the LLM field.", "published": "2023-12-23 05:11:37", "link": "http://arxiv.org/abs/2312.15166v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head\n  Translation", "abstract": "Direct speech-to-speech translation achieves high-quality results through the\nintroduction of discrete units obtained from self-supervised learning. This\napproach circumvents delays and cascading errors associated with model\ncascading. However, talking head translation, converting audio-visual speech\n(i.e., talking head video) from one language into another, still confronts\nseveral challenges compared to audio speech: (1) Existing methods invariably\nrely on cascading, synthesizing via both audio and text, resulting in delays\nand cascading errors. (2) Talking head translation has a limited set of\nreference frames. If the generated translation exceeds the length of the\noriginal speech, the video sequence needs to be supplemented by repeating\nframes, leading to jarring video transitions. In this work, we propose a model\nfor talking head translation, \\textbf{TransFace}, which can directly translate\naudio-visual speech into audio-visual speech in other languages. It consists of\na speech-to-unit translation model to convert audio speech into discrete units\nand a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize\nsynchronized audio-visual speech from discrete units in parallel. Furthermore,\nwe introduce a Bounded Duration Predictor, ensuring isometric talking head\ntranslation and preventing duplicate reference frames. Experiments demonstrate\nthat our proposed Unit2Lip model significantly improves synchronization (1.601\nand 0.982 on LSE-C for the original and generated audio speech, respectively)\nand boosts inference speed by a factor of 4.35 on LRS2. Additionally, TransFace\nachieves impressive BLEU scores of 61.93 and 47.55 for Es-En and Fr-En on\nLRS3-T and 100% isochronous translations.", "published": "2023-12-23 08:45:57", "link": "http://arxiv.org/abs/2312.15197v1", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Data Poisoning for Fake News Detection: How to Make a Model\n  Misclassify a Target News without Modifying It", "abstract": "Fake news detection models are critical to countering disinformation but can\nbe manipulated through adversarial attacks. In this position paper, we analyze\nhow an attacker can compromise the performance of an online learning detector\non specific news content without being able to manipulate the original target\nnews. In some contexts, such as social networks, where the attacker cannot\nexert complete control over all the information, this scenario can indeed be\nquite plausible. Therefore, we show how an attacker could potentially introduce\npoisoning data into the training data to manipulate the behavior of an online\nlearning method. Our initial findings reveal varying susceptibility of logistic\nregression models based on complexity and attack type.", "published": "2023-12-23 11:37:09", "link": "http://arxiv.org/abs/2312.15228v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion\n  Representation", "abstract": "We propose emotion2vec, a universal speech emotion representation model.\nemotion2vec is pre-trained on open-source unlabeled emotion data through\nself-supervised online distillation, combining utterance-level loss and\nframe-level loss during pre-training. emotion2vec outperforms state-of-the-art\npre-trained universal models and emotion specialist models by only training\nlinear layers for the speech emotion recognition task on the mainstream IEMOCAP\ndataset. In addition, emotion2vec shows consistent improvements among 10\ndifferent languages of speech emotion recognition datasets. emotion2vec also\nshows excellent results on other emotion tasks, such as song emotion\nrecognition, emotion prediction in conversation, and sentiment analysis.\nComparison experiments, ablation experiments, and visualization comprehensively\ndemonstrate the universal capability of the proposed emotion2vec. To the best\nof our knowledge, emotion2vec is the first universal representation model in\nvarious emotion-related tasks, filling a gap in the field.", "published": "2023-12-23 07:46:55", "link": "http://arxiv.org/abs/2312.15185v1", "categories": ["cs.CL", "cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting anxiety from short clips of free-form speech", "abstract": "Barriers to accessing mental health assessments including cost and stigma\ncontinues to be an impediment in mental health diagnosis and treatment. Machine\nlearning approaches based on speech samples could help in this direction. In\nthis work, we develop machine learning solutions to diagnose anxiety disorders\nfrom audio journals of patients. We work on a novel anxiety dataset (provided\nthrough collaboration with Kintsugi Mindful Wellness Inc.) and experiment with\nseveral models of varying complexity utilizing audio, text and a combination of\nmultiple modalities. We show that the multi-modal and audio embeddings based\napproaches achieve good performance in the task achieving an AUC ROC score of\n0.68-0.69.", "published": "2023-12-23 14:44:17", "link": "http://arxiv.org/abs/2312.15272v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SAIC: Integration of Speech Anonymization and Identity Classification", "abstract": "Speech anonymization and de-identification have garnered significant\nattention recently, especially in the healthcare area including telehealth\nconsultations, patient voiceprint matching, and patient real-time monitoring.\nSpeaker identity classification tasks, which involve recognizing specific\nspeakers from audio to learn identity features, are crucial for\nde-identification. Since rare studies have effectively combined speech\nanonymization with identity classification, we propose SAIC - an innovative\npipeline for integrating Speech Anonymization and Identity Classification. SAIC\ndemonstrates remarkable performance and reaches state-of-the-art in the speaker\nidentity classification task on the Voxceleb1 dataset, with a top-1 accuracy of\n96.1%. Although SAIC is not trained or evaluated specifically on clinical data,\nthe result strongly proves the model's effectiveness and the possibility to\ngeneralize into the healthcare area, providing insightful guidance for future\nwork.", "published": "2023-12-23 08:14:33", "link": "http://arxiv.org/abs/2312.15190v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
