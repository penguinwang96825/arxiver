{"title": "Controlling Personality Style in Dialogue with Zero-Shot Prompt-Based\n  Learning", "abstract": "Prompt-based or in-context learning has achieved high zero-shot performance\non many natural language generation (NLG) tasks. Here we explore the\nperformance of prompt-based learning for simultaneously controlling the\npersonality and the semantic accuracy of an NLG for task-oriented dialogue. We\nexperiment with prompt-based learning on the PERSONAGE restaurant\nrecommendation corpus to generate semantically and stylistically-controlled\ntext for 5 different Big-5 personality types: agreeable, disagreeable,\nconscientious, unconscientious, and extravert. We test two different classes of\ndiscrete prompts to generate utterances for a particular personality style: (1)\nprompts that demonstrate generating directly from a meaning representation that\nincludes a personality specification; and (2) prompts that rely on first\nconverting the meaning representation to a textual pseudo-reference, and then\nusing the pseudo-reference in a textual style transfer (TST) prompt. In each\ncase, we show that we can vastly improve performance by over-generating outputs\nand ranking them, testing several ranking functions based on automatic metrics\nfor semantic accuracy, personality-match, and fluency. We also test whether NLG\npersonality demonstrations from the restaurant domain can be used with meaning\nrepresentations for the video game domain to generate personality stylized\nutterances about video games. Our findings show that the TST prompts produces\nthe highest semantic accuracy (78.46% for restaurants and 87.6% for video\ngames) and personality accuracy (100% for restaurants and 97% for video games).\nOur results on transferring personality style to video game utterances are\nsurprisingly good. To our knowledge, there is no previous work testing the\napplication of prompt-based learning to simultaneously controlling both style\nand semantic accuracy in NLG.", "published": "2023-02-08 02:45:21", "link": "http://arxiv.org/abs/2302.03848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EvoText: Enhancing Natural Language Generation Models via\n  Self-Escalation Learning for Up-to-Date Knowledge and Improved Performance", "abstract": "In recent years, pretrained models have been widely used in various fields,\nincluding natural language understanding, computer vision, and natural language\ngeneration. However, the performance of these language generation models is\nhighly dependent on the model size and the dataset size. While larger models\nexcel in some aspects, they cannot learn up-to-date knowledge and are\nrelatively difficult to relearn. In this paper, we introduce EvoText, a novel\ntraining method that enhances the performance of any natural language\ngeneration model without requiring additional datasets during the entire\ntraining process (although a prior dataset is necessary for pretraining).\nEvoText employs two models: $G$, a text generation model, and $D$, a model that\ncan determine whether the data generated by $G$ is legitimate. Initially, the\nfine-tuned $D$ model serves as the knowledge base. The text generated by $G$ is\nthen input to $D$ to determine whether it is legitimate. Finally, $G$ is\nfine-tuned based on $D$'s output. EvoText enables the model to learn up-to-date\nknowledge through a self-escalation process that builds on a priori knowledge.\nWhen EvoText needs to learn something new, it simply fine-tunes the $D$ model.\nOur approach applies to autoregressive language modeling for all Transformer\nclasses. With EvoText, eight models achieved stable improvements in seven\nnatural language processing tasks without any changes to the model structure.", "published": "2023-02-08 06:09:55", "link": "http://arxiv.org/abs/2302.03896v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Offline Compression: Going Beyond Factorization-based Methods\n  for Transformer Language Models", "abstract": "Recent transformer language models achieve outstanding results in many\nnatural language processing (NLP) tasks. However, their enormous size often\nmakes them impractical on memory-constrained devices, requiring practitioners\nto compress them to smaller networks. In this paper, we explore offline\ncompression methods, meaning computationally-cheap approaches that do not\nrequire further fine-tuning of the compressed model. We challenge the classical\nmatrix factorization methods by proposing a novel, better-performing\nautoencoder-based framework. We perform a comprehensive ablation study of our\napproach, examining its different aspects over a diverse set of evaluation\nsettings. Moreover, we show that enabling collaboration between modules across\nlayers by compressing certain modules together positively impacts the final\nmodel performance. Experiments on various NLP tasks demonstrate that our\napproach significantly outperforms commonly used factorization-based offline\ncompression methods.", "published": "2023-02-08 13:36:06", "link": "http://arxiv.org/abs/2302.04045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPTScore: Evaluate as You Desire", "abstract": "Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation--how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.", "published": "2023-02-08 16:17:29", "link": "http://arxiv.org/abs/2302.04166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Event Grounding", "abstract": "Event grounding aims at linking mention references in text corpora to events\nfrom a knowledge base (KB). Previous work on this task focused primarily on\nlinking to a single KB event, thereby overlooking the hierarchical aspects of\nevents. Events in documents are typically described at various levels of\nspatio-temporal granularity (Glavas et al. 2014). These hierarchical relations\nare utilized in downstream tasks of narrative understanding and schema\nconstruction. In this work, we present an extension to the event grounding task\nthat requires tackling hierarchical event structures from the KB. Our proposed\ntask involves linking a mention reference to a set of event labels from a\nsubevent hierarchy in the KB. We propose a retrieval methodology that leverages\nevent hierarchy through an auxiliary hierarchical loss (Murty et al. 2018). On\nan automatically created multilingual dataset from Wikipedia and Wikidata, our\nexperiments demonstrate the effectiveness of the hierarchical loss against\nretrieve and re-rank baselines (Wu et al. 2020; Pratapa, Gupta, and Mitamura\n2022). Furthermore, we demonstrate the systems' ability to aid hierarchical\ndiscovery among unseen events.", "published": "2023-02-08 17:09:41", "link": "http://arxiv.org/abs/2302.04197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis and opinion mining on educational data: A survey", "abstract": "Sentiment analysis AKA opinion mining is one of the most widely used NLP\napplications to identify human intentions from their reviews. In the education\nsector, opinion mining is used to listen to student opinions and enhance their\nlearning-teaching practices pedagogically. With advancements in sentiment\nannotation techniques and AI methodologies, student comments can be labelled\nwith their sentiment orientation without much human intervention. In this\nreview article, (1) we consider the role of emotional analysis in education\nfrom four levels: document level, sentence level, entity level, and aspect\nlevel, (2) sentiment annotation techniques including lexicon-based and\ncorpus-based approaches for unsupervised annotations are explored, (3) the role\nof AI in sentiment analysis with methodologies like machine learning, deep\nlearning, and transformers are discussed, (4) the impact of sentiment analysis\non educational procedures to enhance pedagogy, decision-making, and evaluation\nare presented. Educational institutions have been widely invested to build\nsentiment analysis tools and process their student feedback to draw their\nopinions and insights. Applications built on sentiment analysis of student\nfeedback are reviewed in this study. Challenges in sentiment analysis like\nmulti-polarity, polysemous, negation words, and opinion spam detection are\nexplored and their trends in the research space are discussed. The future\ndirections of sentiment analysis in education are discussed.", "published": "2023-02-08 22:14:08", "link": "http://arxiv.org/abs/2302.04359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Text and Multi-Table Summarization: Dataset and Method", "abstract": "Automatic document summarization aims to produce a concise summary covering\nthe input document's salient information. Within a report document, the salient\ninformation can be scattered in the textual and non-textual content. However,\nexisting document summarization datasets and methods usually focus on the text\nand filter out the non-textual content. Missing tabular data can limit produced\nsummaries' informativeness, especially when summaries require covering\nquantitative descriptions of critical metrics in tables. Existing datasets and\nmethods cannot meet the requirements of summarizing long text and multiple\ntables in each report. To deal with the scarcity of available data, we propose\nFINDSum, the first large-scale dataset for long text and multi-table\nsummarization. Built on 21,125 annual reports from 3,794 companies, it has two\nsubsets for summarizing each company's results of operations and liquidity. To\nsummarize the long text and dozens of tables in each report, we present three\ntypes of summarization methods. Besides, we propose a set of evaluation metrics\nto assess the usage of numerical information in produced summaries. Dataset\nanalyses and experimental results indicate the importance of jointly\nconsidering input textual and tabular data when summarizing report documents.", "published": "2023-02-08 00:46:55", "link": "http://arxiv.org/abs/2302.03815v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.7"], "primary_category": "cs.CL"}
{"title": "COMBO: A Complete Benchmark for Open KG Canonicalization", "abstract": "Open knowledge graph (KG) consists of (subject, relation, object) triples\nextracted from millions of raw text. The subject and object noun phrases and\nthe relation in open KG have severe redundancy and ambiguity and need to be\ncanonicalized. Existing datasets for open KG canonicalization only provide gold\nentity-level canonicalization for noun phrases. In this paper, we present\nCOMBO, a Complete Benchmark for Open KG canonicalization. Compared with\nexisting datasets, we additionally provide gold canonicalization for relation\nphrases, gold ontology-level canonicalization for noun phrases, as well as\nsource sentences from which triples are extracted. We also propose metrics for\nevaluating each type of canonicalization. On the COMBO dataset, we empirically\ncompare previously proposed canonicalization methods as well as a few simple\nbaseline methods based on pretrained language models. We find that properly\nencoding the phrases in a triple using pretrained language models results in\nbetter relation canonicalization and ontology-level canonicalization of the\nnoun phrase. We release our dataset, baselines, and evaluation scripts at\nhttps://github.com/jeffchy/COMBO/tree/main.", "published": "2023-02-08 06:46:01", "link": "http://arxiv.org/abs/2302.03905v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving (Dis)agreement Detection with Inductive Social Relation\n  Information From Comment-Reply Interactions", "abstract": "(Dis)agreement detection aims to identify the authors' attitudes or positions\n(\\textit{{agree, disagree, neutral}}) towards a specific text. It is limited\nfor existing methods merely using textual information for identifying\n(dis)agreements, especially for cross-domain settings. Social relation\ninformation can play an assistant role in the (dis)agreement task besides\ntextual information. We propose a novel method to extract such relation\ninformation from (dis)agreement data into an inductive social relation graph,\nmerely using the comment-reply pairs without any additional platform-specific\ninformation. The inductive social relation globally considers the historical\ndiscussion and the relation between authors. Textual information based on a\npre-trained language model and social relation information encoded by\npre-trained RGCN are jointly considered for (dis)agreement detection.\nExperimental results show that our model achieves state-of-the-art performance\nfor both the in-domain and cross-domain tasks on the benchmark -- DEBAGREEMENT.\nWe find social relations can boost the performance of the (dis)agreement\ndetection model, especially for the long-token comment-reply pairs,\ndemonstrating the effectiveness of the social relation graph. We also explore\nthe effect of the knowledge graph embedding methods, the information fusing\nmethod, and the time interval in constructing the social relation graph, which\nshows the effectiveness of our model.", "published": "2023-02-08 09:09:47", "link": "http://arxiv.org/abs/2302.03950v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Summary Guidance on Medical Report Summarization", "abstract": "This study presents three deidentified large medical text datasets, named\nDISCHARGE, ECHO and RADIOLOGY, which contain 50K, 16K and 378K pairs of report\nand summary that are derived from MIMIC-III, respectively. We implement\nconvincing baselines of automated abstractive summarization on the proposed\ndatasets with pre-trained encoder-decoder language models, including BERT2BERT,\nT5-large and BART. Further, based on the BART model, we leverage the sampled\nsummaries from the train set as prior knowledge guidance, for encoding\nadditional contextual representations of the guidance with the encoder and\nenhancing the decoding representations in the decoder. The experimental results\nconfirm the improvement of ROUGE scores and BERTScore made by the proposed\nmethod, outperforming the larger model T5-large.", "published": "2023-02-08 11:21:58", "link": "http://arxiv.org/abs/2302.04001v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on\n  Reasoning, Hallucination, and Interactivity", "abstract": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.", "published": "2023-02-08 12:35:34", "link": "http://arxiv.org/abs/2302.04023v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Joint Learning for Clinical Named Entity Recognition and\n  Relation Extraction Using Fourier Networks: A Use Case in Adverse Drug Events", "abstract": "Current approaches for clinical information extraction are inefficient in\nterms of computational costs and memory consumption, hindering their\napplication to process large-scale electronic health records (EHRs). We propose\nan efficient end-to-end model, the Joint-NER-RE-Fourier (JNRF), to jointly\nlearn the tasks of named entity recognition and relation extraction for\ndocuments of variable length. The architecture uses positional encoding and\nunitary batch sizes to process variable length documents and uses a\nweight-shared Fourier network layer for low-complexity token mixing. Finally,\nwe reach the theoretical computational complexity lower bound for relation\nextraction using a selective pooling strategy and distance-aware attention\nweights with trainable polynomial distance functions. We evaluated the JNRF\narchitecture using the 2018 N2C2 ADE benchmark to jointly extract\nmedication-related entities and relations in variable-length EHR summaries.\nJNRF outperforms rolling window BERT with selective pooling by 0.42%, while\nbeing twice as fast to train. Compared to state-of-the-art BiLSTM-CRF\narchitectures on the N2C2 ADE benchmark, results show that the proposed\napproach trains 22 times faster and reduces GPU memory consumption by 1.75\nfolds, with a reasonable performance tradeoff of 90%, without the use of\nexternal tools, hand-crafted rules or post-processing. Given the significant\ncarbon footprint of deep learning models and the current energy crises, these\nmethods could support efficient and cleaner information extraction in EHRs and\nother types of large-scale document databases.", "published": "2023-02-08 16:44:27", "link": "http://arxiv.org/abs/2302.04185v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CRL+: A Novel Semi-Supervised Deep Active Contrastive Representation\n  Learning-Based Text Classification Model for Insurance Data", "abstract": "Financial sector and especially the insurance industry collect vast volumes\nof text on a daily basis and through multiple channels (their agents, customer\ncare centers, emails, social networks, and web in general). The information\ncollected includes policies, expert and health reports, claims and complaints,\nresults of surveys, and relevant social media posts. It is difficult to\neffectively extract label, classify, and interpret the essential information\nfrom such varied and unstructured material. Therefore, the Insurance Industry\nis among the ones that can benefit from applying technologies for the\nintelligent analysis of free text through Natural Language Processing (NLP).\n  In this paper, CRL+, a novel text classification model combining Contrastive\nRepresentation Learning (CRL) and Active Learning is proposed to handle the\nchallenge of using semi-supervised learning for text classification. In this\nmethod, supervised (CRL) is used to train a RoBERTa transformer model to encode\nthe textual data into a contrastive representation space and then classify\nusing a classification layer. This (CRL)-based transformer model is used as the\nbase model in the proposed Active Learning mechanism to classify all the data\nin an iterative manner. The proposed model is evaluated using unstructured\nobituary data with objective to determine the cause of the death from the data.\nThis model is compared with the CRL model and an Active Learning model with the\nRoBERTa base model. The experiment shows that the proposed method can\noutperform both methods for this specific task.", "published": "2023-02-08 21:23:52", "link": "http://arxiv.org/abs/2302.04343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ordered Memory Baselines", "abstract": "Natural language semantics can be modeled using the phrase-structured model,\nwhich can be represented using a tree-type architecture. As a result, recent\nadvances in natural language processing have been made utilising recursive\nneural networks using memory models that allow them to infer tree-type\nrepresentations of the input sentence sequence. These new tree models have\nallowed for improvements in sentiment analysis and semantic recognition. Here\nwe review the Ordered Memory model proposed by Shen et al. (2019) at the\nNeurIPS 2019 conference, and try to either create baselines that can perform\nbetter or create simpler models that can perform equally as well. We found that\nthe Ordered Memory model performs on par with the state-of-the-art models used\nin tree-type modelling, and performs better than simplified baselines that\nrequire fewer parameters.", "published": "2023-02-08 22:36:25", "link": "http://arxiv.org/abs/2302.06451v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?", "abstract": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.", "published": "2023-02-08 09:44:51", "link": "http://arxiv.org/abs/2302.06476v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical BioBERT Hyperparameter Optimization using Genetic Algorithm", "abstract": "Clinical factors account only for a small portion, about 10-30%, of the\ncontrollable factors that affect an individual's health outcomes. The remaining\nfactors include where a person was born and raised, where he/she pursued their\neducation, what their work and family environment is like, etc. These factors\nare collectively referred to as Social Determinants of Health (SDoH). The\nmajority of SDoH data is recorded in unstructured clinical notes by physicians\nand practitioners. Recording SDoH data in a structured manner (in an EHR) could\ngreatly benefit from a dedicated ontology of SDoH terms. Our research focuses\non extracting sentences from clinical notes, making use of such an SDoH\nontology (called SOHO) to provide appropriate concepts. We utilize recent\nadvancements in Deep Learning to optimize the hyperparameters of a Clinical\nBioBERT model for SDoH text. A genetic algorithm-based hyperparameter tuning\nregimen was implemented to identify optimal parameter settings. To implement a\ncomplete classifier, we pipelined Clinical BioBERT with two subsequent linear\nlayers and two dropout layers. The output predicts whether a text fragment\ndescribes an SDoH issue of the patient. We compared the AdamW, Adafactor, and\nLAMB optimizers. In our experiments, AdamW outperformed the others in terms of\naccuracy.", "published": "2023-02-08 01:11:59", "link": "http://arxiv.org/abs/2302.03822v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Reception Reader: Exploring Text Reuse in Early Modern British\n  Publications", "abstract": "The Reception Reader is a web tool for studying text reuse in the Early\nEnglish Books Online (EEBO-TCP) and Eighteenth Century Collections Online\n(ECCO) data. Users can: 1) explore a visual overview of the reception of a\nwork, or its incoming connections, across time based on shared text segments,\n2) interactively survey the details of connected documents, and 3) examine the\ncontext of reused text for \"close reading\". We show examples of how the tool\nstreamlines research and exploration tasks, and discuss the utility and\nlimitations of the user interface along with its current data sources.", "published": "2023-02-08 14:37:35", "link": "http://arxiv.org/abs/2302.04084v2", "categories": ["cs.DL", "cs.CL", "cs.HC", "J.5"], "primary_category": "cs.DL"}
{"title": "Training-free Lexical Backdoor Attacks on Language Models", "abstract": "Large-scale language models have achieved tremendous success across various\nnatural language processing (NLP) applications. Nevertheless, language models\nare vulnerable to backdoor attacks, which inject stealthy triggers into models\nfor steering them to undesirable behaviors. Most existing backdoor attacks,\nsuch as data poisoning, require further (re)training or fine-tuning language\nmodels to learn the intended backdoor patterns. The additional training process\nhowever diminishes the stealthiness of the attacks, as training a language\nmodel usually requires long optimization time, a massive amount of data, and\nconsiderable modifications to the model parameters. In this work, we propose\nTraining-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free\nbackdoor attack on language models. Our attack is achieved by injecting lexical\ntriggers into the tokenizer of a language model via manipulating its embedding\ndictionary using carefully designed rules. These rules are explainable to human\ndevelopers which inspires attacks from a wider range of hackers. The sparse\nmanipulation of the dictionary also habilitates the stealthiness of our attack.\nWe conduct extensive experiments on three dominant NLP tasks based on nine\nlanguage models to demonstrate the effectiveness and universality of our\nattack. The code of this work is available at\nhttps://github.com/Jinxhy/TFLexAttack.", "published": "2023-02-08 15:18:51", "link": "http://arxiv.org/abs/2302.04116v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Prompting for Multimodal Hateful Meme Classification", "abstract": "Hateful meme classification is a challenging multimodal task that requires\ncomplex reasoning and contextual background knowledge. Ideally, we could\nleverage an explicit external knowledge base to supplement contextual and\ncultural information in hateful memes. However, there is no known explicit\nexternal knowledge base that could provide such hate speech contextual\ninformation. To address this gap, we propose PromptHate, a simple yet effective\nprompt-based model that prompts pre-trained language models (PLMs) for hateful\nmeme classification. Specifically, we construct simple prompts and provide a\nfew in-context examples to exploit the implicit knowledge in the pre-trained\nRoBERTa language model for hateful meme classification. We conduct extensive\nexperiments on two publicly available hateful and offensive meme datasets. Our\nexperimental results show that PromptHate is able to achieve a high AUC of\n90.96, outperforming state-of-the-art baselines on the hateful meme\nclassification task. We also perform fine-grained analyses and case studies on\nvarious prompt settings and demonstrate the effectiveness of the prompts on\nhateful meme classification.", "published": "2023-02-08 16:04:08", "link": "http://arxiv.org/abs/2302.04156v1", "categories": ["cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Diagnosing and Rectifying Vision Models using Language", "abstract": "Recent multi-modal contrastive learning models have demonstrated the ability\nto learn an embedding space suitable for building strong vision classifiers, by\nleveraging the rich information in large-scale image-caption datasets. Our work\nhighlights a distinct advantage of this multi-modal embedding space: the\nability to diagnose vision classifiers through natural language. The\ntraditional process of diagnosing model behaviors in deployment settings\ninvolves labor-intensive data acquisition and annotation. Our proposed method\ncan discover high-error data slices, identify influential attributes and\nfurther rectify undesirable model behaviors, without requiring any visual data.\nThrough a combination of theoretical explanation and empirical verification, we\npresent conditions under which classifiers trained on embeddings from one\nmodality can be equivalently applied to embeddings from another modality. On a\nrange of image datasets with known error slices, we demonstrate that our method\ncan effectively identify the error slices and influential attributes, and can\nfurther use language to rectify failure modes of the classifier.", "published": "2023-02-08 18:59:42", "link": "http://arxiv.org/abs/2302.04269v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "ChatGPT versus Traditional Question Answering for Knowledge Graphs:\n  Current Status and Future Directions Towards Knowledge Graph Chatbots", "abstract": "Conversational AI and Question-Answering systems (QASs) for knowledge graphs\n(KGs) are both emerging research areas: they empower users with natural\nlanguage interfaces for extracting information easily and effectively.\nConversational AI simulates conversations with humans; however, it is limited\nby the data captured in the training datasets. In contrast, QASs retrieve the\nmost recent information from a KG by understanding and translating the natural\nlanguage question into a formal query supported by the database engine.\n  In this paper, we present a comprehensive study of the characteristics of the\nexisting alternatives towards combining both worlds into novel KG chatbots. Our\nframework compares two representative conversational models, ChatGPT and\nGalactica, against KGQAN, the current state-of-the-art QAS. We conduct a\nthorough evaluation using four real KGs across various application domains to\nidentify the current limitations of each category of systems. Based on our\nfindings, we propose open research opportunities to empower QASs with chatbot\ncapabilities for KGs. All benchmarks and all raw results are available1 for\nfurther analysis.", "published": "2023-02-08 13:03:27", "link": "http://arxiv.org/abs/2302.06466v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security\n  Vulnerabilities in Black-Box Code Language Models", "abstract": "Large language models (LLMs) for automatic code generation have achieved\nbreakthroughs in several programming tasks. Their advances in competition-level\nprogramming problems have made them an essential pillar of AI-assisted pair\nprogramming, and tools such as GitHub Copilot have emerged as part of the daily\nprogramming workflow used by millions of developers. The training data for\nthese models is usually collected from the Internet (e.g., from open-source\nrepositories) and is likely to contain faults and security vulnerabilities.\nThis unsanitized training data can cause the language models to learn these\nvulnerabilities and propagate them during the code generation procedure. While\nthese models have been extensively assessed for their ability to produce\nfunctionally correct programs, there remains a lack of comprehensive\ninvestigations and benchmarks addressing the security aspects of these models.\n  In this work, we propose a method to systematically study the security issues\nof code language models to assess their susceptibility to generating vulnerable\ncode. To this end, we introduce the first approach to automatically find\ngenerated code that contains vulnerabilities in black-box code generation\nmodels. To achieve this, we present an approach to approximate inversion of the\nblack-box code generation models based on few-shot prompting. We evaluate the\neffectiveness of our approach by examining code language models in generating\nhigh-risk security weaknesses. Furthermore, we establish a collection of\ndiverse non-secure prompts for various vulnerability scenarios using our\nmethod. This dataset forms a benchmark for evaluating and comparing the\nsecurity weaknesses in code language models.", "published": "2023-02-08 11:54:07", "link": "http://arxiv.org/abs/2302.04012v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CR"}
{"title": "Towards Inferential Reproducibility of Machine Learning Research", "abstract": "Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.", "published": "2023-02-08 13:47:00", "link": "http://arxiv.org/abs/2302.04054v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Masking Kernel for Learning Energy-Efficient Representations for Speaker\n  Recognition and Mobile Health", "abstract": "Modern smartphones possess hardware for audio acquisition and to perform\nspeech processing tasks such as speaker recognition and health assessment.\nHowever, energy consumption remains a concern, especially for\nresource-intensive DNNs. Prior work has improved the DNN energy efficiency by\nutilizing a compact model or reducing the dimensions of speech features. Both\napproaches reduced energy consumption during DNN inference but not during\nspeech acquisition. This paper proposes using a masking kernel integrated into\ngradient descent during DNN training to learn the most energy-efficient speech\nlength and sampling rate for windowing, a common step for sample construction.\nTo determine the most energy-optimal parameters, a masking function with\nnon-zero derivatives was combined with a low-pass filter. The proposed approach\nminimizes the energy consumption of both data collection and inference by 57%,\nand is competitive with speaker recognition and traumatic brain injury\ndetection baselines.", "published": "2023-02-08 16:13:28", "link": "http://arxiv.org/abs/2302.04161v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise2Music: Text-conditioned Music Generation with Diffusion Models", "abstract": "We introduce Noise2Music, where a series of diffusion models is trained to\ngenerate high-quality 30-second music clips from text prompts. Two types of\ndiffusion models, a generator model, which generates an intermediate\nrepresentation conditioned on text, and a cascader model, which generates\nhigh-fidelity audio conditioned on the intermediate representation and possibly\nthe text, are trained and utilized in succession to generate high-fidelity\nmusic. We explore two options for the intermediate representation, one using a\nspectrogram and the other using audio with lower fidelity. We find that the\ngenerated audio is not only able to faithfully reflect key elements of the text\nprompt such as genre, tempo, instruments, mood, and era, but goes beyond to\nground fine-grained semantics of the prompt. Pretrained large language models\nplay a key role in this story -- they are used to generate paired text for the\naudio of the training set and to extract embeddings of the text prompts\ningested by the diffusion models.\n  Generated examples: https://google-research.github.io/noise2music", "published": "2023-02-08 07:27:27", "link": "http://arxiv.org/abs/2302.03917v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Short-Term Memory Convolutions", "abstract": "The real-time processing of time series signals is a critical issue for many\nreal-life applications. The idea of real-time processing is especially\nimportant in audio domain as the human perception of sound is sensitive to any\nkind of disturbance in perceived signals, especially the lag between auditory\nand visual modalities. The rise of deep learning (DL) models complicated the\nlandscape of signal processing. Although they often have superior quality\ncompared to standard DSP methods, this advantage is diminished by higher\nlatency. In this work we propose novel method for minimization of inference\ntime latency and memory consumption, called Short-Term Memory Convolution\n(STMC) and its transposed counterpart. The main advantage of STMC is the low\nlatency comparable to long short-term memory (LSTM) networks. Furthermore, the\ntraining of STMC-based models is faster and more stable as the method is based\nsolely on convolutional neural networks (CNNs). In this study we demonstrate an\napplication of this solution to a U-Net model for a speech separation task and\nGhostNet model in acoustic scene classification (ASC) task. In case of speech\nseparation we achieved a 5-fold reduction in inference time and a 2-fold\nreduction in latency without affecting the output quality. The inference time\nfor ASC task was up to 4 times faster while preserving the original accuracy.", "published": "2023-02-08 20:52:24", "link": "http://arxiv.org/abs/2302.04331v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World\n  Spontaneous Speech", "abstract": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have\nachieved near human-level naturalness. The diversity of human speech, however,\noften goes beyond the coverage of these corpora. We believe the ability to\nhandle such diversity is crucial for AI systems to achieve human-level\ncommunication. Our work explores the use of more abundant real-world data for\nbuilding speech synthesizers. We train TTS systems using real-world speech from\nYouTube and podcasts. We observe the mismatch between training and inference\nalignments in mel-spectrogram based autoregressive models, leading to\nunintelligible synthesis, and demonstrate that learned discrete codes within\nmultiple code groups effectively resolves this issue. We introduce our MQTTS\nsystem whose architecture is designed for multiple code generation and\nmonotonic alignment, along with the use of a clean silence prompt to improve\nsynthesis quality. We conduct ablation analyses to identify the efficacy of our\nmethods. We show that MQTTS outperforms existing TTS systems in several\nobjective and subjective measures.", "published": "2023-02-08 17:34:32", "link": "http://arxiv.org/abs/2302.04215v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
