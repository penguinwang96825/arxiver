{"title": "A Gap-Based Framework for Chinese Word Segmentation via Very Deep\n  Convolutional Networks", "abstract": "Most previous approaches to Chinese word segmentation can be roughly\nclassified into character-based and word-based methods. The former regards this\ntask as a sequence-labeling problem, while the latter directly segments\ncharacter sequence into words. However, if we consider segmenting a given\nsentence, the most intuitive idea is to predict whether to segment for each gap\nbetween two consecutive characters, which in comparison makes previous\napproaches seem too complex. Therefore, in this paper, we propose a gap-based\nframework to implement this intuitive idea. Moreover, very deep convolutional\nneural networks, namely, ResNets and DenseNets, are exploited in our\nexperiments. Results show that our approach outperforms the best\ncharacter-based and word-based methods on 5 benchmarks, without any further\npost-processing module (e.g. Conditional Random Fields) nor beam search.", "published": "2017-12-27 06:44:02", "link": "http://arxiv.org/abs/1712.09509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text Normalization by Optimizing Nearest Neighbor Matching", "abstract": "Text normalization is an essential task in the processing and analysis of\nsocial media that is dominated with informal writing. It aims to map informal\nwords to their intended standard forms. Previously proposed text normalization\napproaches typically require manual selection of parameters for improved\nperformance. In this paper, we present an automatic optimizationbased nearest\nneighbor matching approach for text normalization. This approach is motivated\nby the observation that text normalization is essentially a matching problem\nand nearest neighbor matching with an adaptive similarity function is the most\ndirect procedure for it. Our similarity function incorporates weighted\ncontributions of contextual, string, and phonetic similarity, and the nearest\nneighbor matching involves a minimum similarity threshold. These four\nparameters are tuned efficiently using grid search. We evaluate the performance\nof our approach on two benchmark datasets. The results demonstrate that\nparameter tuning on small sized labeled datasets produce state-of-the-art text\nnormalization performances. Thus, this approach allows practically easy\nconstruction of evolving domain-specific normalization lexicons", "published": "2017-12-27 08:02:26", "link": "http://arxiv.org/abs/1712.09518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CNN Is All You Need", "abstract": "The Convolution Neural Network (CNN) has demonstrated the unique advantage in\naudio, image and text learning; recently it has also challenged Recurrent\nNeural Networks (RNNs) with long short-term memory cells (LSTM) in\nsequence-to-sequence learning, since the computations involved in CNN are\neasily parallelizable whereas those involved in RNN are mostly sequential,\nleading to a performance bottleneck. However, unlike RNN, the native CNN lacks\nthe history sensitivity required for sequence transformation; therefore\nenhancing the sequential order awareness, or position-sensitivity, becomes the\nkey to make CNN the general deep learning model. In this work we introduce an\nextended CNN model with strengthen position-sensitivity, called PoseNet. A\nnotable feature of PoseNet is the asymmetric treatment of position information\nin the encoder and the decoder. Experiments shows that PoseNet allows us to\nimprove the accuracy of CNN based sequence-to-sequence learning significantly,\nachieving around 33-36 BLEU scores on the WMT 2014 English-to-German\ntranslation task, and around 44-46 BLEU scores on the English-to-French\ntranslation task.", "published": "2017-12-27 19:49:09", "link": "http://arxiv.org/abs/1712.09662v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Combining Representation Learning with Logic for Language Processing", "abstract": "The current state-of-the-art in many natural language processing and\nautomated knowledge base completion tasks is held by representation learning\nmethods which learn distributed vector representations of symbols via\ngradient-based optimization. They require little or no hand-crafted features,\nthus avoiding the need for most preprocessing steps and task-specific\nassumptions. However, in many cases representation learning requires a large\namount of annotated training data to generalize well to unseen data. Such\nlabeled training data is provided by human annotators who often use formal\nlogic as the language for specifying annotations. This thesis investigates\ndifferent combinations of representation learning methods with logic for\nreducing the need for annotated training data, and for improving\ngeneralization.", "published": "2017-12-27 21:09:36", "link": "http://arxiv.org/abs/1712.09687v1", "categories": ["cs.NE", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.NE"}
{"title": "Eventness: Object Detection on Spectrograms for Temporal Localization of\n  Audio Events", "abstract": "In this paper, we introduce the concept of Eventness for audio event\ndetection, which can, in part, be thought of as an analogue to Objectness from\ncomputer vision. The key observation behind the eventness concept is that audio\nevents reveal themselves as 2-dimensional time-frequency patterns with specific\ntextures and geometric structures in spectrograms. These time-frequency\npatterns can then be viewed analogously to objects occurring in natural images\n(with the exception that scaling and rotation invariance properties do not\napply). With this key observation in mind, we pose the problem of detecting\nmonophonic or polyphonic audio events as an equivalent visual object(s)\ndetection problem under partial occlusion and clutter in spectrograms. We adapt\na state-of-the-art visual object detection model to evaluate the audio event\ndetection task on publicly available datasets. The proposed network has\ncomparable results with a state-of-the-art baseline and is more robust on\nminority events. Provided large-scale datasets, we hope that our proposed\nconceptual model of eventness will be beneficial to the audio signal processing\ncommunity towards improving performance of audio event detection.", "published": "2017-12-27 20:09:05", "link": "http://arxiv.org/abs/1712.09668v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multiple Instance Deep Learning for Weakly Supervised Small-Footprint\n  Audio Event Detection", "abstract": "State-of-the-art audio event detection (AED) systems rely on supervised\nlearning using strongly labeled data. However, this dependence severely limits\nscalability to large-scale datasets where fine resolution annotations are too\nexpensive to obtain. In this paper, we propose a small-footprint multiple\ninstance learning (MIL) framework for multi-class AED using weakly annotated\nlabels. The proposed MIL framework uses audio embeddings extracted from a\npre-trained convolutional neural network as input features. We show that by\nusing audio embeddings the MIL framework can be implemented using a simple DNN\nwith performance comparable to recurrent neural networks.\n  We evaluate our approach by training an audio tagging system using a subset\nof AudioSet, which is a large collection of weakly labeled YouTube video\nexcerpts. Combined with a late-fusion approach, we improve the F1 score of a\nbaseline audio tagging system by 17%. We show that audio embeddings extracted\nby the convolutional neural networks significantly boost the performance of all\nMIL models. This framework reduces the model complexity of the AED system and\nis suitable for applications where computational resources are limited.", "published": "2017-12-27 20:30:45", "link": "http://arxiv.org/abs/1712.09673v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Light-Weight Multimodal Framework for Improved Environmental Audio\n  Tagging", "abstract": "The lack of strong labels has severely limited the state-of-the-art fully\nsupervised audio tagging systems to be scaled to larger dataset. Meanwhile,\naudio-visual learning models based on unlabeled videos have been successfully\napplied to audio tagging, but they are inevitably resource hungry and require a\nlong time to train. In this work, we propose a light-weight, multimodal\nframework for environmental audio tagging. The audio branch of the framework is\na convolutional and recurrent neural network (CRNN) based on multiple instance\nlearning (MIL). It is trained with the audio tracks of a large collection of\nweakly labeled YouTube video excerpts; the video branch uses pretrained\nstate-of-the-art image recognition networks and word embeddings to extract\ninformation from the video track and to map visual objects to sound events.\nExperiments on the audio tagging task of the DCASE 2017 challenge show that the\nincorporation of video information improves a strong baseline audio tagging\nsystem by 5.3\\% absolute in terms of $F_1$ score. The entire system can be\ntrained within 6~hours on a single GPU, and can be easily carried over to other\naudio tasks such as speech sentimental analysis.", "published": "2017-12-27 20:52:21", "link": "http://arxiv.org/abs/1712.09680v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
