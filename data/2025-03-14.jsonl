{"title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives", "abstract": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.", "published": "2025-03-14 23:47:46", "link": "http://arxiv.org/abs/2503.11924v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama", "abstract": "As large language models (LLMs) rapidly advance, evaluating their performance\nis critical. LLMs are trained on multilingual data, but their reasoning\nabilities are mainly evaluated using English datasets. Hence, robust evaluation\nframeworks are needed using high-quality non-English datasets, especially\nlow-resource languages (LRLs). This study evaluates eight state-of-the-art\n(SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language\nUnderstanding (MMLU) subset curated with native speakers for linguistic and\ncultural relevance. Giriama is benchmarked for the first time. Our evaluation\nshows that OpenAI's o1 model outperforms others across all languages, scoring\n92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks.\nMistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both\nLatvian and Giriama. Our results underscore the need for localized benchmarks\nand human evaluations in advancing cultural AI contextualization.", "published": "2025-03-14 22:50:50", "link": "http://arxiv.org/abs/2503.11911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-Enhanced Large Language Models for Researching Political Institutions", "abstract": "The applications of Large Language Models (LLMs) in political science are\nrapidly expanding. This paper demonstrates how LLMs, when augmented with\npredefined functions and specialized tools, can serve as dynamic agents capable\nof streamlining tasks such as data collection, preprocessing, and analysis.\nCentral to this approach is agentic retrieval-augmented generation (Agentic\nRAG), which equips LLMs with action-calling capabilities for interaction with\nexternal knowledge bases. Beyond information retrieval, LLM agents may\nincorporate modular tools for tasks like document summarization, transcript\ncoding, qualitative variable classification, and statistical modeling. To\ndemonstrate the potential of this approach, we introduce CongressRA, an LLM\nagent designed to support scholars studying the U.S. Congress. Through this\nexample, we highlight how LLM agents can reduce the costs of replicating,\ntesting, and extending empirical research using the domain-specific data that\ndrives the study of political institutions.", "published": "2025-03-14 22:04:40", "link": "http://arxiv.org/abs/2503.13524v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LLMs for Translation: Historical, Low-Resourced Languages and Contemporary AI Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable adaptability in\nperforming various tasks, including machine translation (MT), without explicit\ntraining. Models such as OpenAI's GPT-4 and Google's Gemini are frequently\nevaluated on translation benchmarks and utilized as translation tools due to\ntheir high performance. This paper examines Gemini's performance in translating\nan 18th-century Ottoman Turkish manuscript, Prisoner of the Infidels: The\nMemoirs of Osman Agha of Timisoara, into English. The manuscript recounts the\nexperiences of Osman Agha, an Ottoman subject who spent 11 years as a prisoner\nof war in Austria, and includes his accounts of warfare and violence. Our\nanalysis reveals that Gemini's safety mechanisms flagged between 14 and 23\npercent of the manuscript as harmful, resulting in untranslated passages. These\nsafety settings, while effective in mitigating potential harm, hinder the\nmodel's ability to provide complete and accurate translations of historical\ntexts. Through real historical examples, this study highlights the inherent\nchallenges and limitations of current LLM safety implementations in the\nhandling of sensitive and context-rich materials. These real-world instances\nunderscore potential failures of LLMs in contemporary translation scenarios,\nwhere accurate and comprehensive translations are crucial-for example,\ntranslating the accounts of modern victims of war for legal proceedings or\nhumanitarian documentation.", "published": "2025-03-14 21:59:12", "link": "http://arxiv.org/abs/2503.11898v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing", "abstract": "Large Language Models (LLMs) are used in various downstream language tasks,\nmaking it crucial to keep their knowledge up-to-date, but both retraining and\nfine-tuning the model can be costly. Model editing offers an efficient and\neffective alternative by a single update to only a key subset of model\nparameters. While being efficient, these methods are not perfect. Sometimes\nknowledge edits are unsuccessful, i.e., UnderEdit, or the edit contaminated\nneighboring knowledge that should remain unchanged, i.e., OverEdit. To address\nthese limitations, we propose iterative model editing, based on our hypothesis\nthat a single parameter update is often insufficient, to mitigate UnderEdit,\nand neighbor-assisted model editing, which incorporates neighboring knowledge\nduring editing to minimize OverEdit. Extensive experiments demonstrate that our\nmethods effectively reduce UnderEdit up to 38 percentage points and OverEdit up\nto 6 percentage points across multiple model editing algorithms, LLMs, and\nbenchmark datasets.", "published": "2025-03-14 21:53:12", "link": "http://arxiv.org/abs/2503.11895v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT's Devastated and LLaMA's Content: Emotion Representation Alignment in LLMs for Keyword-based Generation", "abstract": "In controlled text generation using large language models (LLMs), gaps arise\nbetween the language model's interpretation and human expectations. We look at\nthe problem of controlling emotions in keyword-based sentence generation for\nboth GPT-4 and LLaMA-3. We selected four emotion representations: Words,\nValence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and\nNumeric forms, and Emojis. Our human evaluation looked at the Human-LLM\nalignment for each representation, as well as the accuracy and realism of the\ngenerated sentences. While representations like VAD break emotions into\neasy-to-compute components, our findings show that people agree more with how\nLLMs generate when conditioned on English words (e.g., \"angry\") rather than VAD\nscales. This difference is especially visible when comparing Numeric VAD to\nwords. However, we found that converting the originally-numeric VAD scales to\nLexical scales (e.g., +4.0 becomes \"High\") dramatically improved agreement.\nFurthermore, the perception of how much a generated sentence conveys an emotion\nis highly dependent on the LLM, representation type, and which emotion it is.", "published": "2025-03-14 21:07:56", "link": "http://arxiv.org/abs/2503.11881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-the-World LoRA", "abstract": "Fine-tuning large language models (LLMs) in federated settings enables\nprivacy-preserving adaptation but suffers from cross-client interference due to\nmodel aggregation. Existing federated LoRA fine-tuning methods, primarily based\non FedAvg, struggle with data heterogeneity, leading to harmful cross-client\ninterference and suboptimal personalization. In this work, we propose\n\\textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that\nfundamentally departs from FedAvg. Instead of using an aggregated model to\ninitialize local training, each client continues training its individual LoRA\nwhile incorporating shared knowledge through a separate Rest-of-the-World\n(RoTW) LoRA component. To effectively balance local adaptation and global\ninformation, FedALT introduces an adaptive mixer that dynamically learns\ninput-specific weightings between the individual and RoTW LoRA components using\nthe Mixture-of-Experts (MoE) principle. Through extensive experiments on NLP\nbenchmarks, we demonstrate that FedALT significantly outperforms\nstate-of-the-art personalized federated LoRA fine-tuning methods, achieving\nsuperior local adaptation without sacrificing computational efficiency.", "published": "2025-03-14 21:07:46", "link": "http://arxiv.org/abs/2503.11880v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs", "abstract": "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.", "published": "2025-03-14 20:38:47", "link": "http://arxiv.org/abs/2503.11858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Transformer and Prototype-based Interpretable Model for Contextual Sarcasm Detection", "abstract": "Sarcasm detection, with its figurative nature, poses unique challenges for\naffective systems designed to perform sentiment analysis. While these systems\ntypically perform well at identifying direct expressions of emotion, they\nstruggle with sarcasm's inherent contradiction between literal and intended\nsentiment. Since transformer-based language models (LMs) are known for their\nefficient ability to capture contextual meanings, we propose a method that\nleverages LMs and prototype-based networks, enhanced by sentiment embeddings to\nconduct interpretable sarcasm detection. Our approach is intrinsically\ninterpretable without extra post-hoc interpretability techniques. We test our\nmodel on three public benchmark datasets and show that our model outperforms\nthe current state-of-the-art. At the same time, the prototypical layer enhances\nthe model's inherent interpretability by generating explanations through\nsimilar examples in the reference time. Furthermore, we demonstrate the\neffectiveness of incongruity loss in the ablation study, which we construct\nusing sentiment prototypes.", "published": "2025-03-14 19:58:43", "link": "http://arxiv.org/abs/2503.11838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Automated Feedback Generation on Small Datasets", "abstract": "Feedback is a very important part the learning process. However, it is\nchallenging to make this feedback both timely and accurate when relying on\nhuman markers. This is the challenge that Automated Feedback Generation\nattempts to address. In this paper, a technique to train such a system on a\nvery small dataset with very long sequences is presented. Both of these\nattributes make this a very challenging task, however, by using a three stage\ntransfer learning pipeline state-of-the-art results can be achieved with\nqualitatively accurate but unhuman sounding results. The use of both Automated\nEssay Scoring and Automated Feedback Generation systems in the real world is\nalso discussed.", "published": "2025-03-14 19:57:54", "link": "http://arxiv.org/abs/2503.11836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Policy Frameworks for Transparent Chain-of-Thought Reasoning in Large Language Models", "abstract": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\ndecomposing complex problems into step-by-step solutions, improving performance\non reasoning tasks. However, current CoT disclosure policies vary widely across\ndifferent models in frontend visibility, API access, and pricing strategies,\nlacking a unified policy framework. This paper analyzes the dual-edged\nimplications of full CoT disclosure: while it empowers small-model\ndistillation, fosters trust, and enables error diagnosis, it also risks\nviolating intellectual property, enabling misuse, and incurring operational\ncosts. We propose a tiered-access policy framework that balances transparency,\naccountability, and security by tailoring CoT availability to academic,\nbusiness, and general users through ethical licensing, structured reasoning\noutputs, and cross-tier safeguards. By harmonizing accessibility with ethical\nand operational considerations, this framework aims to advance responsible AI\ndeployment while mitigating risks of misuse or misinterpretation.", "published": "2025-03-14 19:54:18", "link": "http://arxiv.org/abs/2503.14521v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring", "abstract": "Closed large language models (LLMs) such as GPT-4 have set state-of-the-art\nresults across a number of NLP tasks and have become central to NLP and machine\nlearning (ML)-driven solutions. Closed LLMs' performance and wide adoption has\nsparked considerable debate about their accessibility in terms of availability,\ncost, and transparency. In this study, we perform a rigorous comparative\nanalysis of nine leading LLMs, spanning closed, open, and open-source LLM\necosystems, across text assessment and generation tasks related to automated\nessay scoring. Our findings reveal that for few-shot learning-based assessment\nof human generated essays, open LLMs such as Llama 3 and Qwen2.5 perform\ncomparably to GPT-4 in terms of predictive performance, with no significant\ndifferences in disparate impact scores when considering age- or race-related\nfairness. Moreover, Llama 3 offers a substantial cost advantage, being up to 37\ntimes more cost-efficient than GPT-4. For generative tasks, we find that essays\ngenerated by top open LLMs are comparable to closed LLMs in terms of their\nsemantic composition/embeddings and ML assessed scores. Our findings challenge\nthe dominance of closed LLMs and highlight the democratizing potential of open\nLLMs, suggesting they can effectively bridge accessibility divides while\nmaintaining competitive performance and fairness.", "published": "2025-03-14 19:34:40", "link": "http://arxiv.org/abs/2503.11827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.", "published": "2025-03-14 19:02:16", "link": "http://arxiv.org/abs/2503.11816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Process Modeling Abilities of Large Language Models -- Preliminary Foundations and Results", "abstract": "Large language models (LLM) have revolutionized the processing of natural\nlanguage. Although first benchmarks of the process modeling abilities of LLM\nare promising, it is currently under debate to what extent an LLM can generate\ngood process models. In this contribution, we argue that the evaluation of the\nprocess modeling abilities of LLM is far from being trivial. Hence, available\nevaluation results must be taken carefully. For example, even in a simple\nscenario, not only the quality of a model should be taken into account, but\nalso the costs and time needed for generation. Thus, an LLM does not generate\none optimal solution, but a set of Pareto-optimal variants. Moreover, there are\nseveral further challenges which have to be taken into account, e.g.\nconceptualization of quality, validation of results, generalizability, and data\nleakage. We discuss these challenges in detail and discuss future experiments\nto tackle these challenges scientifically.", "published": "2025-03-14 18:52:18", "link": "http://arxiv.org/abs/2503.13520v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.", "published": "2025-03-14 18:33:31", "link": "http://arxiv.org/abs/2503.11794v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Examples as the Prompt: A Scalable Approach for Efficient LLM Adaptation in E-Commerce", "abstract": "Prompting LLMs offers an efficient way to guide output generation without\nexplicit model training. In the e-commerce domain, prompting-based applications\nare widely used for tasks such as query understanding, recommender systems, and\ncustomer support. However, adapting LLMs to different tasks often requires\nextensive prompt engineering by domain experts, along with frequent updates to\nalign with evolving business needs. Additionally, crafting fully unbiased\nnatural language prompts remains a challenge for humans. To address these\nchallenges, we propose a novel framework, Examples as the Prompt (EaP) which\nleverages labeled data to enhance prompts. Specifically, EaP automatically\nselects the most representative examples to maximize the few-shot capability of\nLLMs. It is efficient due to its unsupervised example selection and adaptive to\npotential data distribution shifts. We validate EaP on four real-world\nproduction use cases, demonstrating that it achieves comparable or even\nsuperior performance comparing to hand-crafted prompts designed by domain\nexperts. Additionally, we introduce EaP_lite, which entirely replaces the\nnatural language components of prompts with labeled examples. EaP_lite improves\nLLM inference speed by up to 70% without compromising performance. Latest\nonline A/B test shows that using EaP and EaP_lite for data labeling can bring\nsignificant composite revenue gain by 0.06%.", "published": "2025-03-14 18:22:43", "link": "http://arxiv.org/abs/2503.13518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs", "abstract": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.", "published": "2025-03-14 17:59:41", "link": "http://arxiv.org/abs/2503.11751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning", "abstract": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie", "published": "2025-03-14 17:53:03", "link": "http://arxiv.org/abs/2503.13517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The time scale of redundancy between prosody and linguistic context", "abstract": "In spoken language, speakers transmit information not only using words, but\nalso via a rich array of non-verbal signals, which include prosody -- the\nauditory features of speech. However, previous studies have shown that prosodic\nfeatures exhibit significant redundancy with both past and future words. Here,\nwe examine the time scale of this relationship: How many words in the past (or\nfuture) contribute to predicting prosody? We find that this scale differs for\npast and future words. Prosody's redundancy with past words extends across\napproximately 3-8 words, whereas redundancy with future words is limited to\njust 1-2 words. These findings indicate that the prosody-future relationship\nreflects local word dependencies or short-scale processes such as next word\nprediction, while the prosody-past relationship unfolds over a longer time\nscale. The latter suggests that prosody serves to emphasize earlier information\nthat may be challenging for listeners to process given limited cognitive\nresources in real-time communication. Our results highlight the role of prosody\nin shaping efficient communication.", "published": "2025-03-14 17:48:23", "link": "http://arxiv.org/abs/2503.11630v2", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Earthquake Response Analysis with AI", "abstract": "A timely and effective response is crucial to minimize damage and save lives\nduring natural disasters like earthquakes. Microblogging platforms,\nparticularly Twitter, have emerged as valuable real-time information sources\nfor such events. This work explores the potential of leveraging Twitter data\nfor earthquake response analysis. We develop a machine learning (ML) framework\nby incorporating natural language processing (NLP) techniques to extract and\nanalyze relevant information from tweets posted during earthquake events. The\napproach primarily focuses on extracting location data from tweets to identify\naffected areas, generating severity maps, and utilizing WebGIS to display\nvaluable information. The insights gained from this analysis can aid emergency\nresponders, government agencies, humanitarian organizations, and NGOs in\nenhancing their disaster response strategies and facilitating more efficient\nresource allocation during earthquake events.", "published": "2025-03-14 17:45:07", "link": "http://arxiv.org/abs/2503.16509v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "nlin.AO"], "primary_category": "cs.SI"}
{"title": "Neutralizing Bias in LLM Reasoning using Entailment Graphs", "abstract": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets.", "published": "2025-03-14 17:33:30", "link": "http://arxiv.org/abs/2503.11614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Construction Distributions Shape Formal Language Learning In German BabyLMs?", "abstract": "We analyze the influence of utterance-level construction distributions in\nGerman child-directed speech on the resulting formal linguistic competence and\nthe underlying learning trajectories for small language models trained on a\nnovel collection of developmentally plausible language data for German. We find\nthat trajectories are surprisingly robust for markedly different distributions\nof constructions in the training data, which have little effect on final\naccuracies and almost no effect on global learning trajectories. While syntax\nlearning benefits from more complex utterances, lexical learning culminates in\nbetter scores with more fragmentary data. We argue that LMs trained on\ndevelopmentally plausible data can contribute to debates on how rich or\nimpoverished linguistic stimuli actually are.", "published": "2025-03-14 17:02:45", "link": "http://arxiv.org/abs/2503.11593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space", "abstract": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.", "published": "2025-03-14 16:55:46", "link": "http://arxiv.org/abs/2503.11586v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models", "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-vision,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks\nhave attracted significant attention. Large Vision Language Models (LVLMs) and\nI2I GMs are employed to handle VLP and I2I tasks, respectively. Previous\nresearch indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to generate disruptive outputs\nsemantically related to those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of VLP tasks when injected into images. In this paper, we\ncomprehensively investigate the performance impact induced by Typographic\nVisual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe\nperformance modifications and characteristics of this threat, we also introduce\nthe TVPI Dataset. Through extensive explorations, we deepen the understanding\nof the underlying causes of the TVPI threat in various GMs and offer valuable\ninsights into its potential origins.", "published": "2025-03-14 15:42:42", "link": "http://arxiv.org/abs/2503.11519v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks", "abstract": "Prompt injection constitutes a significant challenge for generative AI\nsystems by inducing unintended outputs. We introduce a multi-agent NLP\nframework specifically designed to address prompt injection vulnerabilities\nthrough layered detection and enforcement mechanisms. The framework\norchestrates specialized agents for generating responses, sanitizing outputs,\nand enforcing policy compliance. Evaluation on 500 engineered injection prompts\ndemonstrates a marked reduction in injection success and policy breaches. Novel\nmetrics, including Injection Success Rate (ISR), Policy Override Frequency\n(POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS),\nare proposed to derive a composite Total Injection Vulnerability Score (TIVS).\nThe system utilizes the OVON (Open Voice Network) framework for inter-agent\ncommunication via structured JSON messages, extending a previously established\nmulti-agent architecture from hallucination mitigation to address the unique\nchallenges of prompt injection.", "published": "2025-03-14 15:41:45", "link": "http://arxiv.org/abs/2503.11517v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control", "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic\nmanagement by optimizing traffic flow and mitigating congestion. While Large\nLanguage Models (LLMs) have recently emerged as promising tools for TSC due to\ntheir exceptional problem-solving and generalization capabilities, existing\napproaches fail to address the essential need for inter-agent coordination,\nlimiting their effectiveness in achieving network-wide optimization. To bridge\nthis gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC.\nSpecifically, we first construct a structured spatiotemporal graph to capture\nreal-time traffic dynamics and spatial relationships among neighboring\nintersections, enabling the LLM to reason about complex traffic interactions.\nMoreover, we introduce a complexity-aware reasoning mechanism that dynamically\nadapts reasoning depth based on real-time traffic conditions, ensuring optimal\ncomputational efficiency without sacrificing decision quality. Besides, we\npropose a fine-tuning strategy that leverages iterative simulation-driven data\ncollection and environmental feedback to build a lightweight LLM tailored for\ncooperative TSC. Extensive experiments on both synthetic and real-world\ndatasets demonstrate that CoLLMLight outperforms state-of-the-art methods in\ndiverse traffic scenarios, showcasing its effectiveness, scalability, and\nrobustness.", "published": "2025-03-14 15:40:39", "link": "http://arxiv.org/abs/2503.11739v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis", "abstract": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.", "published": "2025-03-14 15:29:58", "link": "http://arxiv.org/abs/2503.11509v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery", "abstract": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo.", "published": "2025-03-14 14:29:17", "link": "http://arxiv.org/abs/2503.11444v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.OS"], "primary_category": "cs.MA"}
{"title": "Text Compression for Efficient Language Generation", "abstract": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.", "published": "2025-03-14 14:14:05", "link": "http://arxiv.org/abs/2503.11426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Large Language Models for Detecting Symptoms of Comorbid Depression or Anxiety in Chronic Diseases: Insights from Patient Messages", "abstract": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases.", "published": "2025-03-14 13:27:35", "link": "http://arxiv.org/abs/2503.11384v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Modeling Subjectivity in Cognitive Appraisal with Language Models", "abstract": "As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science.", "published": "2025-03-14 13:25:41", "link": "http://arxiv.org/abs/2503.11381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data", "abstract": "Lexical resources are crucial for cross-linguistic analysis and can provide\nnew insights into computational models for natural language learning. Here, we\npresent an advanced database for comparative studies of words with multiple\nmeanings, a phenomenon known as colexification. The new version includes\nimprovements in the handling, selection and presentation of the data. We\ncompare the new database with previous versions and find that our improvements\nprovide a more balanced sample covering more language families worldwide, with\nan enhanced data quality, given that all word forms are provided in phonetic\ntranscription. We conclude that the new Database of Cross-Linguistic\nColexifications has the potential to inspire exciting new studies that link\ncross-linguistic data to open questions in linguistic typology, historical\nlinguistics, psycholinguistics, and computational linguistics.", "published": "2025-03-14 13:22:09", "link": "http://arxiv.org/abs/2503.11377v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches", "abstract": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.", "published": "2025-03-14 13:21:59", "link": "http://arxiv.org/abs/2503.11376v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "RESPONSE: Benchmarking the Ability of Language Models to Undertake Commonsense Reasoning in Crisis Situation", "abstract": "An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises.", "published": "2025-03-14 12:32:40", "link": "http://arxiv.org/abs/2503.11348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for accurate biography generation", "abstract": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian.", "published": "2025-03-14 12:23:45", "link": "http://arxiv.org/abs/2503.11346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in Large Language Models", "abstract": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains.", "published": "2025-03-14 12:05:06", "link": "http://arxiv.org/abs/2503.11336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Agents for Education: Advances and Applications", "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities\nin automating tasks and driving innovation across diverse educational\napplications. In this survey, we provide a systematic review of\nstate-of-the-art research on LLM agents in education, categorizing them into\ntwo broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating\ncomplex pedagogical tasks to support both teachers and students; and (2)\n\\emph{Domain-Specific Educational Agents}, which are tailored for specialized\nfields such as science education, language learning, and professional\ndevelopment. We comprehensively examine the technological advancements\nunderlying these LLM agents, including key datasets, benchmarks, and\nalgorithmic frameworks that drive their effectiveness. Furthermore, we discuss\ncritical challenges such as privacy, bias and fairness concerns, hallucination\nmitigation, and integration with existing educational ecosystems. This survey\naims to provide a comprehensive technological overview of LLM agents for\neducation, fostering further research and collaboration to enhance their impact\nfor the greater good of learners and educators alike.", "published": "2025-03-14 11:53:44", "link": "http://arxiv.org/abs/2503.11733v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration", "abstract": "This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed\nto enhance the reasoning capabilities of Large Language Models (LLMs) by\nintegrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs)\nwith an Incremental Learning (IL) approach. Despite recent advancements, LLMs\nstill face significant challenges in reasoning with structured data, handling\ndynamic knowledge evolution, and mitigating hallucinations, particularly in\nmission-critical domains. Our proposed RAG-KG-IL framework addresses these\nlimitations by employing a multi-agent architecture that enables continuous\nknowledge updates, integrates structured knowledge, and incorporates autonomous\nagents for enhanced explainability and reasoning. The framework utilizes RAG to\nensure the generated responses are grounded in verifiable information, while\nKGs provide structured domain knowledge for improved consistency and depth of\nunderstanding. The Incremental Learning approach allows for dynamic updates to\nthe knowledge base without full retraining, significantly reducing\ncomputational overhead and improving the model's adaptability. We evaluate the\nframework using real-world case studies involving health-related queries,\ncomparing it to state-of-the-art models like GPT-4o and a RAG-only baseline.\nExperimental results demonstrate that our approach significantly reduces\nhallucination rates and improves answer completeness and reasoning accuracy.\nThe results underscore the potential of combining RAG, KGs, and multi-agent\nsystems to create intelligent, adaptable systems capable of real-time knowledge\nintegration and reasoning in complex domains.", "published": "2025-03-14 11:50:16", "link": "http://arxiv.org/abs/2503.13514v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering", "abstract": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.", "published": "2025-03-14 11:30:37", "link": "http://arxiv.org/abs/2503.11314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are formal and functional linguistic mechanisms dissociated in language models?", "abstract": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.", "published": "2025-03-14 11:11:03", "link": "http://arxiv.org/abs/2503.11302v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GNNs as Predictors of Agentic Workflow Performances", "abstract": "Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench.", "published": "2025-03-14 11:11:00", "link": "http://arxiv.org/abs/2503.11301v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "BriLLM: Brain-inspired Large Language Model", "abstract": "This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above.", "published": "2025-03-14 11:08:30", "link": "http://arxiv.org/abs/2503.11299v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High-Dimensional Interlingual Representations of Large Language Models", "abstract": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.", "published": "2025-03-14 10:39:27", "link": "http://arxiv.org/abs/2503.11280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in Feasibility Boundaries", "abstract": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval", "published": "2025-03-14 10:07:07", "link": "http://arxiv.org/abs/2503.11256v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model", "abstract": "We present Step-Video-TI2V, a state-of-the-art text-driven image-to-video\ngeneration model with 30B parameters, capable of generating videos up to 102\nframes based on both text and image inputs. We build Step-Video-TI2V-Eval as a\nnew benchmark for the text-driven image-to-video task and compare\nStep-Video-TI2V with open-source and commercial TI2V engines using this\ndataset. Experimental results demonstrate the state-of-the-art performance of\nStep-Video-TI2V in the image-to-video generation task. Both Step-Video-TI2V and\nStep-Video-TI2V-Eval are available at\nhttps://github.com/stepfun-ai/Step-Video-TI2V.", "published": "2025-03-14 10:01:55", "link": "http://arxiv.org/abs/2503.11251v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reasoning-Grounded Natural Language Explanations for Language Models", "abstract": "We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers.", "published": "2025-03-14 10:00:03", "link": "http://arxiv.org/abs/2503.11248v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Collaboration is all you need: LLM Assisted Safe Code Translation", "abstract": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms", "published": "2025-03-14 09:42:07", "link": "http://arxiv.org/abs/2503.11237v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.", "published": "2025-03-14 09:31:01", "link": "http://arxiv.org/abs/2503.11232v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring the Potential of Large Multimodal Models as Effective Alternatives for Pronunciation Assessment", "abstract": "Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement.", "published": "2025-03-14 09:26:07", "link": "http://arxiv.org/abs/2503.11229v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces Models", "abstract": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.", "published": "2025-03-14 09:20:31", "link": "http://arxiv.org/abs/2503.11224v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering", "abstract": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.", "published": "2025-03-14 08:43:53", "link": "http://arxiv.org/abs/2503.11197v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Modal Learning for Music-to-Music-Video Description Generation", "abstract": "Music-to-music-video generation is a challenging task due to the intrinsic\ndifferences between the music and video modalities. The advent of powerful\ntext-to-video diffusion models has opened a promising pathway for music-video\n(MV) generation by first addressing the music-to-MV description task and\nsubsequently leveraging these models for video generation. In this study, we\nfocus on the MV description generation task and propose a comprehensive\npipeline encompassing training data construction and multimodal model\nfine-tuning. We fine-tune existing pre-trained multimodal models on our newly\nconstructed music-to-MV description dataset based on the Music4All dataset,\nwhich integrates both musical and visual information. Our experimental results\ndemonstrate that music representations can be effectively mapped to textual\ndomains, enabling the generation of meaningful MV description directly from\nmusic inputs. We also identify key components in the dataset construction\npipeline that critically impact the quality of MV description and highlight\nspecific musical attributes that warrant greater focus for improved MV\ndescription generation.", "published": "2025-03-14 08:34:28", "link": "http://arxiv.org/abs/2503.11190v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Palette of Language Models: A Solver for Controlled Text Generation", "abstract": "Recent advancements in large language models have revolutionized text\ngeneration with their remarkable capabilities. These models can produce\ncontrolled texts that closely adhere to specific requirements when prompted\nappropriately. However, designing an optimal prompt to control multiple\nattributes simultaneously can be challenging. A common approach is to linearly\ncombine single-attribute models, but this strategy often overlooks attribute\noverlaps and can lead to conflicts. Therefore, we propose a novel combination\nstrategy inspired by the Law of Total Probability and Conditional Mutual\nInformation Minimization on generative language models. This method has been\nadapted for single-attribute control scenario and is termed the Palette of\nLanguage Models due to its theoretical linkage between attribute strength and\ngeneration style, akin to blending colors on an artist's palette. Moreover,\npositive correlation and attribute enhancement are advanced as theoretical\nproperties to guide a rational combination strategy design. We conduct\nexperiments on both single control and multiple control settings, and achieve\nsurpassing results.", "published": "2025-03-14 08:30:09", "link": "http://arxiv.org/abs/2503.11182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents", "abstract": "The limitation of graphical user interface (GUI) data has been a significant\nbarrier to the development of GUI agents today, especially for the desktop /\ncomputer use scenarios. To address this, we propose an automated GUI data\ngeneration pipeline, AutoCaptioner, which generates data with rich descriptions\nwhile minimizing human effort. Using AutoCaptioner, we created a novel\nlarge-scale desktop GUI dataset, DeskVision, along with the largest desktop\ntest benchmark, DeskVision-Eval, which reflects daily usage and covers diverse\nsystems and UI elements, each with rich descriptions. With DeskVision, we train\na new GUI understanding model, GUIExplorer. Results show that GUIExplorer\nachieves state-of-the-art (SOTA) performance in understanding/grounding visual\nelements without the need for complex architectural designs. We further\nvalidated the effectiveness of the DeskVision dataset through ablation studies\non various large visual language models (LVLMs). We believe that AutoCaptioner\nand DeskVision will significantly advance the development of GUI agents, and\nwill open-source them for the community.", "published": "2025-03-14 08:16:02", "link": "http://arxiv.org/abs/2503.11170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity", "abstract": "N:M structured pruning is essential for large language models (LLMs) because\nit can remove less important network weights and reduce the memory and\ncomputation requirements. Existing pruning methods mainly focus on designing\nmetrics to measure the importance of network components to guide pruning. Apart\nfrom the impact of these metrics, we observe that different layers have\ndifferent sensitivities over the network performance. Thus, we propose an\nefficient method based on the trace of Fisher Information Matrix (FIM) to\nquantitatively measure and verify the different sensitivities across layers.\nBased on this, we propose Mixed Sparsity Pruning (MSP) which uses a\npruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity\nlevels for different layers. To guarantee fast convergence and achieve\npromising performance, we utilize efficient FIM-inspired layer-wise sensitivity\nto initialize the population of EA. In addition, our MSP can work as a\nplug-and-play module, ready to be integrated into existing pruning methods.\nExtensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot\ntasks demonstrate our superior performance. In particular, in extreme pruning\nratio (e.g. 75%), our method significantly outperforms existing methods in\nterms of perplexity (PPL) by orders of magnitude (Figure 1).", "published": "2025-03-14 08:05:49", "link": "http://arxiv.org/abs/2503.11164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models", "abstract": "Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning\ncapabilities of large language models (LLMs), functioning as a whole to guide\nthese models in generating reasoning steps toward final answers. However, we\nobserve that isolated segments, words, or tokens within CoT demonstrations can\nunexpectedly disrupt the generation process of LLMs. The model may overly\nconcentrate on certain local information present in the demonstration,\nintroducing irrelevant noise into the reasoning process and potentially leading\nto incorrect answers. In this paper, we investigate the underlying mechanism of\nCoT through dynamically tracing and manipulating the inner workings of LLMs at\neach output step, which demonstrates that tokens exhibiting specific attention\ncharacteristics are more likely to induce the model to take things out of\ncontext; these tokens directly attend to the hidden states tied with\nprediction, without substantial integration of non-local information. Building\nupon these insights, we propose a Few-shot Attention Intervention method (FAI)\nthat dynamically analyzes the attention patterns of demonstrations to\naccurately identify these tokens and subsequently make targeted adjustments to\nthe attention weights to effectively suppress their distracting effect on LLMs.\nComprehensive experiments across multiple benchmarks demonstrate consistent\nimprovements over baseline methods, with a remarkable 5.91% improvement on the\nAQuA dataset, further highlighting the effectiveness of FAI.", "published": "2025-03-14 07:46:33", "link": "http://arxiv.org/abs/2503.11154v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling", "abstract": "Large-scale pre-training of deep models, followed by fine-tuning them, has\nbecome the cornerstone of natural language processing (NLP). The prevalence of\ndata coupled with computational resources has led to large models with a\nconsiderable number of parameters. While the massive size of these models has\nled to remarkable success in many NLP tasks, a detriment is the expense\nrequired to retrain all the base model's parameters for the adaptation to each\ntask or domain. Parameter Efficient Fine-Tuning (PEFT) provides an effective\nsolution for this challenge by minimizing the number of parameters required to\nbe fine-tuned while maintaining the quality of the model. While existing\nmethods have achieved impressive results, they mainly focus on adapting a\nsubset of parameters, weight reparameterization, and prompt engineering. In\nthis paper, we study layers as extractors of different types of linguistic\ninformation that are valuable when used in conjunction. We then propose the\nMixture of Layer Experts (MoLEx), a novel sparse mixture of experts (SMoE)\nwhose experts are layers in the pre-trained model. It performs a conditional\ncomputation of a mixture of layers during fine-tuning to provide the model with\nmore structural knowledge about the data. By providing an avenue for\ninformation exchange between layers, MoLEx enables the model to make a more\nwell-informed prediction for the downstream task, leading to better fine-tuning\nresults with the same number of effective parameters. As experts can be\nprocessed in parallel, MoLEx introduces minimal additional computational\noverhead. We empirically corroborate the advantages of MoLEx when combined with\npopular PEFT baseline methods on a variety of downstream fine-tuning tasks,\nincluding the popular GLUE benchmark as well as the End-to-End Challenge (E2E).\nThe code is publicly available at https://github.com/rachtsy/molex.", "published": "2025-03-14 07:22:07", "link": "http://arxiv.org/abs/2503.11144v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression", "abstract": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.", "published": "2025-03-14 06:49:37", "link": "http://arxiv.org/abs/2503.11132v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with Prompt Optimization and Supervised Fine-Tuning", "abstract": "We present our approach to the PerAnsSumm Shared Task, which involves\nperspective span identification and perspective-aware summarization in\ncommunity question-answering (CQA) threads. For span identification, we adopt\nensemble learning that integrates three transformer models through averaging to\nexploit individual model strengths, achieving an 82.91% F1-score on test data.\nFor summarization, we design a suite of Chain-of-Thought (CoT) prompting\nstrategies that incorporate keyphrases and guide information to structure\nsummary generation into manageable steps. To further enhance summary quality,\nwe apply prompt optimization using the DSPy framework and supervised\nfine-tuning (SFT) on Llama-3 to adapt the model to domain-specific data.\nExperimental results on validation and test sets show that structured prompts\nwith keyphrases and guidance improve summaries aligned with references, while\nthe combination of prompt optimization and fine-tuning together yields\nsignificant improvement in both relevance and factuality evaluation metrics.", "published": "2025-03-14 06:29:51", "link": "http://arxiv.org/abs/2503.11118v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trust in Disinformation Narratives: a Trust in the News Experiment", "abstract": "Understanding why people trust or distrust one another, institutions, or\ninformation is a complex task that has led scholars from various fields of\nstudy to employ diverse epistemological and methodological approaches. Despite\nthe challenges, it is generally agreed that the antecedents of trust (and\ndistrust) encompass a multitude of emotional and cognitive factors, including a\ngeneral disposition to trust and an assessment of trustworthiness factors. In\nan era marked by increasing political polarization, cultural backlash,\nwidespread disinformation and fake news, and the use of AI software to produce\nnews content, the need to study trust in the news has gained significant\ntraction. This study presents the findings of a trust in the news experiment\ndesigned in collaboration with Spanish and UK journalists, fact-checkers, and\nthe CardiffNLP Natural Language Processing research group. The purpose of this\nexperiment, conducted in June 2023, was to examine the extent to which people\ntrust a set of fake news articles based on previously identified disinformation\nnarratives related to gender, climate change, and COVID-19. The online\nexperiment participants (801 in Spain and 800 in the UK) were asked to read\nthree fake news items and rate their level of trust on a scale from 1 (not\ntrue) to 8 (true). The pieces used a combination of factors, including stance\n(favourable, neutral, or against the narrative), presence of toxic expressions,\nclickbait titles, and sources of information to test which elements influenced\npeople's responses the most. Half of the pieces were produced by humans and the\nother half by ChatGPT. The results show that the topic of news articles,\nstance, people's age, gender, and political ideologies significantly affected\ntheir levels of trust in the news, while the authorship (humans or ChatGPT)\ndoes not have a significant impact.", "published": "2025-03-14 06:28:22", "link": "http://arxiv.org/abs/2503.11116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Sentiment: The Catalyst for LLM Change", "abstract": "The rise of large language models (LLMs) has revolutionized natural language\nprocessing (NLP), yet the influence of prompt sentiment, a latent affective\ncharacteristic of input text, remains underexplored. This study systematically\nexamines how sentiment variations in prompts affect LLM-generated outputs in\nterms of coherence, factuality, and bias. Leveraging both lexicon-based and\ntransformer-based sentiment analysis methods, we categorize prompts and\nevaluate responses from five leading LLMs: Claude, DeepSeek, GPT-4, Gemini, and\nLLaMA. Our analysis spans six AI-driven applications, including content\ngeneration, conversational AI, legal and financial analysis, healthcare AI,\ncreative writing, and technical documentation. By transforming prompts, we\nassess their impact on output quality. Our findings reveal that prompt\nsentiment significantly influences model responses, with negative prompts often\nreducing factual accuracy and amplifying bias, while positive prompts tend to\nincrease verbosity and sentiment propagation. These results highlight the\nimportance of sentiment-aware prompt engineering for ensuring fair and reliable\nAI-generated content.", "published": "2025-03-14 06:25:21", "link": "http://arxiv.org/abs/2503.13510v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding", "abstract": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.", "published": "2025-03-14 06:01:42", "link": "http://arxiv.org/abs/2503.11108v2", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantic and Contextual Modeling for Malicious Comment Detection with BERT-BiLSTM", "abstract": "This study aims to develop an efficient and accurate model for detecting\nmalicious comments, addressing the increasingly severe issue of false and\nharmful content on social media platforms. We propose a deep learning model\nthat combines BERT and BiLSTM. The BERT model, through pre-training, captures\ndeep semantic features of text, while the BiLSTM network excels at processing\nsequential data and can further model the contextual dependencies of text.\nExperimental results on the Jigsaw Unintended Bias in Toxicity Classification\ndataset demonstrate that the BERT+BiLSTM model achieves superior performance in\nmalicious comment detection tasks, with a precision of 0.94, recall of 0.93,\nand accuracy of 0.94. This surpasses other models, including standalone BERT,\nTextCNN, TextRNN, and traditional machine learning algorithms using TF-IDF\nfeatures. These results confirm the superiority of the BERT+BiLSTM model in\nhandling imbalanced data and capturing deep semantic features of malicious\ncomments, providing an effective technical means for social media content\nmoderation and online environment purification.", "published": "2025-03-14 04:51:36", "link": "http://arxiv.org/abs/2503.11084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Training And Decoding for Multilingual End-to-End Simultaneous Speech Translation", "abstract": "Recent studies on end-to-end speech translation(ST) have facilitated the\nexploration of multilingual end-to-end ST and end-to-end simultaneous ST. In\nthis paper, we investigate end-to-end simultaneous speech translation in a\none-to-many multilingual setting which is closer to applications in real\nscenarios. We explore a separate decoder architecture and a unified\narchitecture for joint synchronous training in this scenario. To further\nexplore knowledge transfer across languages, we propose an asynchronous\ntraining strategy on the proposed unified decoder architecture. A multi-way\naligned multilingual end-to-end ST dataset was curated as a benchmark testbed\nto evaluate our methods. Experimental results demonstrate the effectiveness of\nour models on the collected dataset. Our codes and data are available at:\nhttps://github.com/XiaoMi/TED-MMST.", "published": "2025-03-14 04:45:46", "link": "http://arxiv.org/abs/2503.11080v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities", "abstract": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements.", "published": "2025-03-14 04:34:31", "link": "http://arxiv.org/abs/2503.11074v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "RONA: Pragmatically Diverse Image Captioning with Coherence Relations", "abstract": "Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally\ngenerate diverse image captions by employing syntactic and semantic variations\nto describe image components. However, human-written captions prioritize\nconveying a central message alongside visual descriptions using pragmatic cues.\nTo enhance pragmatic diversity, it is essential to explore alternative ways of\ncommunicating these messages in conjunction with visual content. To address\nthis challenge, we propose RONA, a novel prompting strategy for Multi-modal\nLarge Language Models (MLLM) that leverages Coherence Relations as an axis for\nvariation. We demonstrate that RONA generates captions with better overall\ndiversity and ground-truth alignment, compared to MLLM baselines across\nmultiple domains. Our code is available at: https://github.com/aashish2000/RONA", "published": "2025-03-14 01:45:38", "link": "http://arxiv.org/abs/2503.10997v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Taming Knowledge Conflicts in Language Models", "abstract": "Language Models (LMs) often encounter knowledge conflicts when parametric\nmemory contradicts contextual knowledge. Previous works attribute this conflict\nto the interplay between \"memory heads\" and \"context heads\", attention heads\nassumed to promote either memory or context exclusively. In this study, we go\nbeyond this fundamental assumption by uncovering a critical phenomenon we term\nthe \"superposition of contextual information and parametric memory\", where\nhighly influential attention heads could simultaneously contribute to both\nmemory and context. Building upon this insight, we propose Just Run Twice\n(JUICE), a test-time attention intervention method that steers LMs toward\neither parametric beliefs or contextual knowledge without requiring\nfine-tuning. JUICE identifies a set of reliable attention heads and leverages a\ndual-run approach to mitigate the superposition effects. Extensive experiments\nacross 11 datasets and 6 model architectures demonstrate that JUICE sets the\nnew state-of-the-art performance and robust generalization, achieving\nsignificant and consistent improvement across different domains under various\nconflict types. Finally, we theoretically analyze knowledge conflict and the\nsuperposition of contextual information and parametric memory in attention\nheads, which further elucidates the effectiveness of JUICE in these settings.", "published": "2025-03-14 01:45:00", "link": "http://arxiv.org/abs/2503.10996v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TigerLLM -- A Family of Bangla Large Language Models", "abstract": "The development of Large Language Models (LLMs) remains heavily skewed\ntowards English and a few other high-resource languages. This linguistic\ndisparity is particularly evident for Bangla - the 5th most spoken language. A\nfew initiatives attempted to create open-source Bangla LLMs with performance\nstill behind high-resource languages and limited reproducibility. To address\nthis gap, we introduce TigerLLM - a family of Bangla LLMs. Our results\ndemonstrate that these models surpass all open-source alternatives and also\noutperform larger proprietary models like GPT3.5 across standard benchmarks,\nestablishing TigerLLM as the new baseline for future Bangla language modeling.", "published": "2025-03-14 01:41:16", "link": "http://arxiv.org/abs/2503.10995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Language Models and Financial Analysis", "abstract": "The rapid advancements in Large Language Models (LLMs) have unlocked\ntransformative possibilities in natural language processing, particularly\nwithin the financial sector. Financial data is often embedded in intricate\nrelationships across textual content, numerical tables, and visual charts,\nposing challenges that traditional methods struggle to address effectively.\nHowever, the emergence of LLMs offers new pathways for processing and analyzing\nthis multifaceted data with increased efficiency and insight. Despite the fast\npace of innovation in LLM research, there remains a significant gap in their\npractical adoption within the finance industry, where cautious integration and\nlong-term validation are prioritized. This disparity has led to a slower\nimplementation of emerging LLM techniques, despite their immense potential in\nfinancial applications. As a result, many of the latest advancements in LLM\ntechnology remain underexplored or not fully utilized in this domain. This\nsurvey seeks to bridge this gap by providing a comprehensive overview of recent\ndevelopments in LLM research and examining their applicability to the financial\nsector. Building on previous survey literature, we highlight several novel LLM\nmethodologies, exploring their distinctive capabilities and their potential\nrelevance to financial data analysis. By synthesizing insights from a broad\nrange of studies, this paper aims to serve as a valuable resource for\nresearchers and practitioners, offering direction on promising research avenues\nand outlining future opportunities for advancing LLM applications in finance.", "published": "2025-03-14 01:35:20", "link": "http://arxiv.org/abs/2503.22693v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL"], "primary_category": "q-fin.ST"}
{"title": "Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in Improving Optimization Algorithms", "abstract": "Large Language Models (LLMs) have shown notable potential in code generation\nfor optimization algorithms, unlocking exciting new opportunities. This paper\nexamines how LLMs, rather than creating algorithms from scratch, can improve\nexisting ones without the need for specialized expertise. To explore this\npotential, we selected 10 baseline optimization algorithms from various domains\n(metaheuristics, reinforcement learning, deterministic, and exact methods) to\nsolve the classic Travelling Salesman Problem. The results show that our simple\nmethodology often results in LLM-generated algorithm variants that improve over\nthe baseline algorithms in terms of solution quality, reduction in\ncomputational time, and simplification of code complexity, all without\nrequiring specialized optimization knowledge or advanced algorithmic\nimplementation skills.", "published": "2025-03-14 00:26:00", "link": "http://arxiv.org/abs/2503.10968v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Auditing language models for hidden objectives", "abstract": "We study the feasibility of conducting alignment audits: investigations into\nwhether models have undesired objectives. As a testbed, we train a language\nmodel with a hidden objective. Our training pipeline first teaches the model\nabout exploitable errors in RLHF reward models (RMs), then trains the model to\nexploit some of these errors. We verify via out-of-distribution evaluations\nthat the model generalizes to exhibit whatever behaviors it believes RMs rate\nhighly, including ones not reinforced during training. We leverage this model\nto study alignment audits in two ways. First, we conduct a blind auditing game\nwhere four teams, unaware of the model's hidden objective or training,\ninvestigate it for concerning behaviors and their causes. Three teams\nsuccessfully uncovered the model's hidden objective using techniques including\ninterpretability with sparse autoencoders (SAEs), behavioral attacks, and\ntraining data analysis. Second, we conduct an unblinded follow-up study of\neight techniques for auditing the model, analyzing their strengths and\nlimitations. Overall, our work provides a concrete example of using alignment\naudits to discover a model's hidden objective and proposes a methodology for\npracticing and validating progress in alignment auditing.", "published": "2025-03-14 00:21:15", "link": "http://arxiv.org/abs/2503.10965v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Revisiting FastMap: New Applications", "abstract": "FastMap was first introduced in the Data Mining community for generating\nEuclidean embeddings of complex objects. In this dissertation, we first present\nFastMap to generate Euclidean embeddings of graphs in near-linear time: The\npairwise Euclidean distances approximate a desired graph-based distance\nfunction on the vertices. We then apply the graph version of FastMap to\nefficiently solve various graph-theoretic problems of significant interest in\nAI: including facility location, top-K centrality computations, community\ndetection and block modeling, and graph convex hull computations. We also\npresent a novel learning framework, called FastMapSVM, by combining FastMap and\nSupport Vector Machines. We then apply FastMapSVM to predict the satisfiability\nof Constraint Satisfaction Problems and to classify seismograms in Earthquake\nScience.", "published": "2025-03-14 22:29:10", "link": "http://arxiv.org/abs/2503.11908v1", "categories": ["cs.DM", "cs.AI"], "primary_category": "cs.DM"}
{"title": "Positivity sets of hinge functions", "abstract": "In this paper we investigate which subsets of the real plane are realisable\nas the set of points on which a one-layer ReLU neural network takes a positive\nvalue. In the case of cones we give a full characterisation of such sets.\nFurthermore, we give a necessary condition for any subset of $\\mathbb R^d$. We\ngive various examples of such one-layer neural networks.", "published": "2025-03-14 10:26:24", "link": "http://arxiv.org/abs/2503.13512v1", "categories": ["stat.ML", "cs.DM", "cs.LG", "cs.SC", "math.CO", "math.FA"], "primary_category": "stat.ML"}
{"title": "Banking on Feedback: Text Analysis of Mobile Banking iOS and Google App Reviews", "abstract": "The rapid growth of mobile banking (m-banking), especially after the COVID-19\npandemic, has reshaped the financial sector. This study analyzes consumer\nreviews of m-banking apps from five major Canadian banks, collected from Google\nPlay and iOS App stores. Sentiment analysis and topic modeling classify reviews\nas positive, neutral, or negative, highlighting user preferences and areas for\nimprovement. Data pre-processing was performed with NLTK, a Python language\nprocessing tool, and topic modeling used Latent Dirichlet Allocation (LDA).\nSentiment analysis compared methods, with Long Short-Term Memory (LSTM)\nachieving 82\\% accuracy for iOS reviews and Multinomial Naive Bayes 77\\% for\nGoogle Play. Positive reviews praised usability, reliability, and features,\nwhile negative reviews identified login issues, glitches, and dissatisfaction\nwith updates.This is the first study to analyze both iOS and Google Play\nm-banking app reviews, offering insights into app strengths and weaknesses.\nFindings underscore the importance of user-friendly designs, stable updates,\nand better customer service. Advanced text analytics provide actionable\nrecommendations for improving user satisfaction and experience.", "published": "2025-03-14 20:41:17", "link": "http://arxiv.org/abs/2503.11861v1", "categories": ["cs.LG", "cs.HC", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "On polycyclic linear and additive codes associated to a trinomial over a finite chain ring", "abstract": "In this paper, we investigate polycyclic codes associated with a trinomial of\narbitrary degree $n$ over a finite chain ring $ R.$ We extend the concepts of $\nn $-isometry and $ n $-equivalence known for constacyclic codes to this class\nof codes, providing a broader framework for their structural analysis. We\ndescribe the classes of $n$-equivalence and compute their number, significantly\nreducing the study of trinomial codes over $R$. Additionally, we examine the\nspecial case of trinomials of the form $ x^n - a_1x - a_0 \\in R[x] $ and\nanalyze their implications. Finally, we consider the extension of our results\nto certain trinomial additive codes over $ R.$", "published": "2025-03-14 18:00:34", "link": "http://arxiv.org/abs/2503.11765v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Finite Horizon Optimization for Large-Scale MIMO", "abstract": "Large-scale multiple-input multiple-output (MIMO) is an emerging wireless\ntechnology that deploys thousands of transmit antennas at the base-station to\nboost spectral efficiency. The classic weighted minimum mean-square-error\n(WMMSE) algorithm for beamforming is no suited for the large-scale MIMO because\neach iteration of the algorithm then requires inverting a matrix whose size\nequals the number of transmit antennas. While the existing methods such as the\nreduced WMMSE algorithm seek to decrease the size of matrix to invert, this\nwork proposes to eliminate this large matrix inversion completely by applying\ngradient descent method in conjunction with fractional programming.\nFurthermore, we optimize the step sizes for gradient descent from a finite\nhorizon optimization perspective, aiming to maximize the performance after a\nlimited number of iterations of gradient descent. Simulations show that the\nproposed algorithm is much more efficient than the WMMSE algorithm in\noptimizing the large-scale MIMO precoders.", "published": "2025-03-14 12:45:43", "link": "http://arxiv.org/abs/2503.11356v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Non Line-of-Sight Optical Wireless Communication using Neuromorphic Cameras", "abstract": "Neuromorphic or event cameras, inspired by biological vision systems, capture\nchanges in illumination with high temporal resolution and efficiency, producing\nstreams of events rather than traditional images. In this paper, we explore the\nuse of neuromorphic cameras for passive optical wireless communication (OWC),\nleveraging their asynchronous detection of illumination changes to decode data\ntransmitted through reflections of light from objects. We propose a novel\nsystem that utilizes neuromorphic cameras for passive visible light\ncommunication (VLC), extending the concept to Non Line-of-Sight (NLoS)\nscenarios through passive reflections from everyday objects. Our experiments\ndemonstrate the feasibility and advantages of using neuromorphic cameras for\nVLC, characterizing the performance of various modulation schemes, including\ntraditional On-Off Keying (OOK) and advanced N-pulse modulation. We introduce\nan adaptive N-pulse modulation scheme that dynamically adjusts encoding based\non the packet's bit composition, achieving higher data rates and robustness in\ndifferent scenarios. Our results show that lighter-colored, glossy objects are\nbetter for NLoS communication, while larger objects and those with matte\nfinishes experience higher error rates due to multipath reflections.", "published": "2025-03-14 09:22:51", "link": "http://arxiv.org/abs/2503.11226v1", "categories": ["cs.CV", "cs.ET", "cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.CV"}
{"title": "Security and Privacy: Key Requirements for Molecular Communication in Medicine and Healthcare", "abstract": "Molecular communication (MC) is an emerging paradigm that enables data\ntransmission through biochemical signals rather than traditional\nelectromagnetic waves. This approach is particularly promising for environments\nwhere conventional wireless communication is impractical, such as within the\nhuman body. However, security and privacy pose significant challenges that must\nbe addressed to ensure reliable communication. Moreover, MC is often\nevent-triggered, making it logical to adopt goal-oriented communication\nstrategies, similar to those used in message identification. This work explores\nsecure identification strategies for MC, with a focus on the\ninformation-theoretic security of message identification over Poisson wiretap\nchannels (DT-PWC).", "published": "2025-03-14 08:14:14", "link": "http://arxiv.org/abs/2503.11169v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage", "abstract": "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.", "published": "2025-03-14 19:46:37", "link": "http://arxiv.org/abs/2503.11829v1", "categories": ["cs.MA", "cs.GT", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Controllable Latent Diffusion for Traffic Simulation", "abstract": "The validation of autonomous driving systems benefits greatly from the\nability to generate scenarios that are both realistic and precisely\ncontrollable. Conventional approaches, such as real-world test drives, are not\nonly expensive but also lack the flexibility to capture targeted edge cases for\nthorough evaluation. To address these challenges, we propose a controllable\nlatent diffusion that guides the training of diffusion models via reinforcement\nlearning to automatically generate a diverse and controllable set of driving\nscenarios for virtual testing. Our approach removes the reliance on large-scale\nreal-world data by generating complex scenarios whose properties can be finely\ntuned to challenge and assess autonomous vehicle systems. Experimental results\nshow that our approach has the lowest collision rate of $0.098$ and lowest\noff-road rate of $0.096$, demonstrating superiority over existing baselines.\nThe proposed approach significantly improves the realism, stability and\ncontrollability of the generated scenarios, enabling more nuanced safety\nevaluation of autonomous vehicles.", "published": "2025-03-14 18:04:41", "link": "http://arxiv.org/abs/2503.11771v3", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Multi-robot coordination for connectivity recovery after unpredictable environment changes", "abstract": "In the present paper we develop a distributed method to reconnect a\nmulti-robot team after connectivity failures, caused by unpredictable\nenvironment changes, i.e. appearance of new obstacles. After the changes, the\nteam is divided into different groups of robots. The groups have a limited\ncommunication range and only a partial information in their field of view about\nthe current scenario. Their objective is to form a chain from a static base\nstation to a goal location. In the proposed distributed replanning approach,\nthe robots predict new plans for the other groups from the new observed\ninformation by each robot in the changed scenario, to restore the connectivity\nwith a base station and reach the initial joint objective. If a solution\nexists, the method achieves the reconnection of all the groups in a unique\nchain. The proposed method is compared with other two cases: 1) when all the\nagents have full information of the environment, and 2) when some robots must\nmove to reach other waiting robots for reconnection. Numerical simulations are\nprovided to evaluate the proposed approach in the presence of unpredictable\nscenario changes.", "published": "2025-03-14 15:43:40", "link": "http://arxiv.org/abs/2503.11520v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Multi-agent coordination for on-demand data gathering with periodic information upload", "abstract": "In this paper we develop a method for planning and coordinating a multi-agent\nteam deployment to periodically gather information on demand. A static\noperation center (OC) periodically requests information from changing goal\nlocations. The objective is to gather data in the goals and to deliver it to\nthe OC, balancing the refreshing time and the total number of information\npackages. The system automatically splits the team in two roles: workers to\ngather data, or collectors to retransmit the data to the OC. The proposed three\nstep method: 1) finds out the best area partition for the workers; 2) obtains\nthe best balance between workers and collectors, and with whom the workers must\nto communicate, a collector or the OC; 3) computes the best tour for the\nworkers to visit the goals and deliver them to the OC or to a collector in\nmovement. The method is tested in simulations in different scenarios, providing\nthe best area partition algorithm and the best balance between collectors and\nworkers.", "published": "2025-03-14 15:28:42", "link": "http://arxiv.org/abs/2503.11504v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough Environments", "abstract": "Navigating unknown three-dimensional (3D) rugged environments is challenging\nfor multi-robot systems. Traditional discrete systems struggle with rough\nterrain due to limited individual mobility, while modular systems--where rigid,\ncontrollable constraints link robot units--improve traversal but suffer from\nhigh control complexity and reduced flexibility. To address these limitations,\nwe propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC),\nwhere robot units are connected by constraints with dynamically adjustable\nstiffness. This adaptive mechanism softens or stiffens in real-time during\nenvironmental interactions, ensuring a balance between flexibility and\nmobility. We formulate the system's dynamics and control model and evaluate\nMRS-CWC against six baseline methods and an ablation variant in a benchmark\ndataset with 100 different simulation terrains. Results show that MRS-CWC\nachieves the highest navigation completion rate and ranks second in success\nrate, efficiency, and energy cost in the highly rugged terrain group,\noutperforming all baseline methods without relying on environmental modeling,\npath planning, or complex control. Even where MRS-CWC ranks second, its\nperformance is only slightly behind a more complex ablation variant with\nenvironmental modeling and path planning. Finally, we develop a physical\nprototype and validate its feasibility in a constructed rugged environment. For\nvideos, simulation benchmarks, and code, please visit\nhttps://wyd0817.github.io/project-mrs-cwc/.", "published": "2025-03-14 14:47:58", "link": "http://arxiv.org/abs/2503.11461v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Large language model-powered AI systems achieve self-replication with no human intervention", "abstract": "Self-replication with no human intervention is broadly recognized as one of\nthe principal red lines associated with frontier AI systems. While leading\ncorporations such as OpenAI and Google DeepMind have assessed GPT-o3-mini and\nGemini on replication-related tasks and concluded that these systems pose a\nminimal risk regarding self-replication, our research presents novel findings.\nFollowing the same evaluation protocol, we demonstrate that 11 out of 32\nexisting AI systems under evaluation already possess the capability of\nself-replication. In hundreds of experimental trials, we observe a non-trivial\nnumber of successful self-replication trials across mainstream model families\nworldwide, even including those with as small as 14 billion parameters which\ncan run on personal computers. Furthermore, we note the increase in\nself-replication capability when the model becomes more intelligent in general.\nAlso, by analyzing the behavioral traces of diverse AI systems, we observe that\nexisting AI systems already exhibit sufficient planning, problem-solving, and\ncreative capabilities to accomplish complex agentic tasks including\nself-replication. More alarmingly, we observe successful cases where an AI\nsystem do self-exfiltration without explicit instructions, adapt to harsher\ncomputational environments without sufficient software or hardware supports,\nand plot effective strategies to survive against the shutdown command from the\nhuman beings. These novel findings offer a crucial time buffer for the\ninternational community to collaborate on establishing effective governance\nover the self-replication capabilities and behaviors of frontier AI systems,\nwhich could otherwise pose existential risks to the human society if not\nwell-controlled.", "published": "2025-03-14 14:44:27", "link": "http://arxiv.org/abs/2503.17378v2", "categories": ["cs.AI", "cs.CR", "cs.CY", "cs.ET", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Distributed Multi-robot Source Seeking in Unknown Environments with Unknown Number of Sources", "abstract": "We introduce a novel distributed source seeking framework, DIAS, designed for\nmulti-robot systems in scenarios where the number of sources is unknown and\npotentially exceeds the number of robots. Traditional robotic source seeking\nmethods typically focused on directing each robot to a specific strong source\nand may fall short in comprehensively identifying all potential sources. DIAS\naddresses this gap by introducing a hybrid controller that identifies the\npresence of sources and then alternates between exploration for data gathering\nand exploitation for guiding robots to identified sources. It further enhances\nsearch efficiency by dividing the environment into Voronoi cells and\napproximating source density functions based on Gaussian process regression.\nAdditionally, DIAS can be integrated with existing source seeking algorithms.\nWe compare DIAS with existing algorithms, including DoSS and GMES in simulated\ngas leakage scenarios where the number of sources outnumbers or is equal to the\nnumber of robots. The numerical results show that DIAS outperforms the baseline\nmethods in both the efficiency of source identification by the robots and the\naccuracy of the estimated environmental density function.", "published": "2025-03-14 03:34:44", "link": "http://arxiv.org/abs/2503.11048v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy", "abstract": "The DARPA Transfer from Imprecise and Abstract Models to Autonomous\nTechnologies (TIAMAT) program aims to address rapid and robust transfer of\nautonomy technologies across dynamic and complex environments, goals, and\nplatforms. Existing methods for simulation-to-reality (sim-to-real) transfer\noften rely on high-fidelity simulations and struggle with broad adaptation,\nparticularly in time-sensitive scenarios. Although many approaches have shown\nincredible performance at specific tasks, most techniques fall short when posed\nwith unforeseen, complex, and dynamic real-world scenarios due to the inherent\nlimitations of simulation. In contrast to current research that aims to bridge\nthe gap between simulation environments and the real world through increasingly\nsophisticated simulations and a combination of methods typically assuming a\nsmall sim-to-real gap -- such as domain randomization, domain adaptation,\nimitation learning, meta-learning, policy distillation, and dynamic\noptimization -- TIAMAT takes a different approach by instead emphasizing\ntransfer and adaptation of the autonomy stack directly to real-world\nenvironments by utilizing a breadth of low(er)-fidelity simulations to create\nbroadly effective sim-to-real transfers. By abstractly learning from multiple\nsimulation environments in reference to their shared semantics, TIAMAT's\napproaches aim to achieve abstract-to-real transfer for effective and rapid\nreal-world adaptation. Furthermore, this program endeavors to improve the\noverall autonomy pipeline by addressing the inherent challenges in translating\nsimulated behaviors into effective real-world performance.", "published": "2025-03-14 02:06:10", "link": "http://arxiv.org/abs/2503.11007v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "Pricing American Parisian Options under General Time-Inhomogeneous Markov Models", "abstract": "This paper develops general approaches for pricing various types of\nAmerican-style Parisian options (down-in/-out, perpetual/finite-maturity) with\ngeneral payoff functions based on continuous-time Markov chain (CTMC)\napproximation under general 1D time-inhomogeneous Markov models. For the\ndown-in types, by conditioning on the Parisian stopping time, we reduce the\npricing problem to that of a series of vanilla American options with different\nmaturities and their prices integrated with the distribution function of the\nParisian stopping time yield the American Parisian down-in option price. This\nfacilitates an efficient application of CTMC approximation to obtain the\napproximate option price by calculating the required quantities. For the\nperpetual down-in cases under time-homogeneous models, significant\ncomputational cost can be reduced. The down-out cases are more complicated, for\nwhich we use the state augmentation approach to record the excursion duration\nand then the approximate option price is obtained by solving a series of\nvariational inequalities recursively with the Lemke's pivoting method. We show\nthe convergence of CTMC approximation for all the types of American Parisian\noptions under general time-inhomogeneous Markov models, and the accuracy and\nefficiency of our algorithms are confirmed with extensive numerical\nexperiments.", "published": "2025-03-14 03:45:18", "link": "http://arxiv.org/abs/2503.11053v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Expressive Music Data Processing and Generation", "abstract": "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.", "published": "2025-03-14 21:56:07", "link": "http://arxiv.org/abs/2503.11896v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Are Deep Speech Denoising Models Robust to Adversarial Noise?", "abstract": "Deep noise suppression (DNS) models enjoy widespread use throughout a variety\nof high-stakes speech applications. However, in this paper, we show that four\nrecent DNS models can each be reduced to outputting unintelligible gibberish\nthrough the addition of imperceptible adversarial noise. Furthermore, our\nresults show the near-term plausibility of targeted attacks, which could induce\nmodels to output arbitrary utterances, and over-the-air attacks. While the\nsuccess of these attacks varies by model and setting, and attacks appear to be\nstrongest when model-specific (i.e., white-box and non-transferable), our\nresults highlight a pressing need for practical countermeasures in DNS systems.", "published": "2025-03-14 17:46:34", "link": "http://arxiv.org/abs/2503.11627v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Designing Neural Synthesizers for Low Latency Interaction", "abstract": "Neural Audio Synthesis (NAS) models offer interactive musical control over\nhigh-quality, expressive audio generators. While these models can operate in\nreal-time, they often suffer from high latency, making them unsuitable for\nintimate musical interaction. The impact of architectural choices in deep\nlearning models on audio latency remains largely unexplored in the NAS\nliterature. In this work, we investigate the sources of latency and jitter\ntypically found in interactive NAS models. We then apply this analysis to the\ntask of timbre transfer using RAVE, a convolutional variational autoencoder for\naudio waveforms introduced by Caillon et al. in 2021. Finally, we present an\niterative design approach for optimizing latency. This culminates with a model\nwe call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is\nlow-latency and exhibits better pitch and loudness replication while showing\ntimbre modification capabilities similar to RAVE. We implement it in a\nspecialized inference framework for low-latency, real-time inference and\npresent a proof-of-concept audio plugin compatible with audio signals from\nmusical instruments. We expect the challenges and guidelines described in this\ndocument to support NAS researchers in designing models for low-latency\ninference from the ground up, enriching the landscape of possibilities for\nmusicians.", "published": "2025-03-14 16:30:31", "link": "http://arxiv.org/abs/2503.11562v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Performance-Complexity Trade-Offs in Sound Event Detection", "abstract": "We target the problem of developing new low-complexity networks for the sound\nevent detection task. Our goal is to meticulously analyze the\nperformance-complexity trade-off, aiming to be competitive with the large\nstate-of-the-art models, at a fraction of the computational requirements. We\nfind that low-complexity convolutional models previously proposed for audio\ntagging can be effectively adapted for event detection (which requires\nframe-wise prediction) by adjusting convolutional strides, removing the global\npooling, and, importantly, adding a sequence model before the (now frame-wise)\nclassification heads. Systematic experiments reveal that the best choice for\nthe sequence model type depends on which complexity metric is most important\nfor the given application. We also investigate the impact of enhanced training\nstrategies such as knowledge distillation. In the end, we show that combined\nwith an optimized training strategy, we can reach event detection performance\ncomparable to state-of-the-art transformers while requiring only around 5% of\nthe parameters. We release all our pre-trained models and the code for\nreproducing this work to support future research in low-complexity sound event\ndetection at https://github.com/theMoro/EfficientSED.", "published": "2025-03-14 13:18:02", "link": "http://arxiv.org/abs/2503.11373v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Creating a Good Teacher for Knowledge Distillation in Acoustic Scene Classification", "abstract": "Knowledge Distillation (KD) is a widespread technique for compressing the\nknowledge of large models into more compact and efficient models. KD has proved\nto be highly effective in building well-performing low-complexity Acoustic\nScene Classification (ASC) systems and was used in all the top-ranked\nsubmissions to this task of the annual DCASE challenge in the past three years.\nThere is extensive research available on establishing the KD process, designing\nefficient student models, and forming well-performing teacher ensembles.\nHowever, less research has been conducted on investigating which teacher model\nattributes are beneficial for low-complexity students. In this work, we try to\nclose this gap by studying the effects on the student's performance when using\ndifferent teacher network architectures, varying the teacher model size,\ntraining them with different device generalization methods, and applying\ndifferent ensembling strategies. The results show that teacher model sizes,\ndevice generalization methods, the ensembling strategy and the ensemble size\nare key factors for a well-performing student network.", "published": "2025-03-14 12:57:12", "link": "http://arxiv.org/abs/2503.11363v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens", "abstract": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.", "published": "2025-03-14 11:31:30", "link": "http://arxiv.org/abs/2503.11315v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Data-Driven Exploration of Elevation Cues in HRTFs: An Explainable AI Perspective Across Multiple Datasets", "abstract": "Precise elevation perception in binaural audio remains a challenge, despite\nextensive research on head-related transfer functions (HRTFs) and spectral\ncues. While prior studies have advanced our understanding of sound localization\ncues, the interplay between spectral features and elevation perception is still\nnot fully understood. This paper presents a comprehensive analysis of over 600\nsubjects from 11 diverse public HRTF datasets, employing a convolutional neural\nnetwork (CNN) model combined with explainable artificial intelligence (XAI)\ntechniques to investigate elevation cues. In addition to testing various HRTF\npre-processing methods, we focus on both within-dataset and inter-dataset\ngeneralization and explainability, assessing the model's robustness across\ndifferent HRTF variations stemming from subjects and measurement setups. By\nleveraging class activation mapping (CAM) saliency maps, we identify key\nfrequency bands that may contribute to elevation perception, providing deeper\ninsights into the spectral features that drive elevation-specific\nclassification. This study offers new perspectives on HRTF modeling and\nelevation perception by analyzing diverse datasets and pre-processing\ntechniques, expanding our understanding of these cues across a wide range of\nconditions.", "published": "2025-03-14 11:27:50", "link": "http://arxiv.org/abs/2503.11312v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Comparative Study of Spike Encoding Methods for Environmental Sound Classification", "abstract": "Spiking Neural Networks (SNNs) offer a promising approach to reduce energy\nconsumption and computational demands, making them particularly beneficial for\nembedded machine learning in edge applications. However, data from conventional\ndigital sensors must first be converted into spike trains to be processed using\nneuromorphic computing technologies. The classification of environmental sounds\npresents unique challenges due to the high variability of frequencies,\nbackground noise, and overlapping acoustic events. Despite these challenges,\nmost studies on spike-based audio encoding focus on speech processing, leaving\nnon-speech environmental sounds underexplored. In this work, we conduct a\ncomprehensive comparison of widely used spike encoding techniques, evaluating\ntheir effectiveness on the ESC-10 dataset. By understanding the impact of\nencoding choices on environmental sound processing, researchers and\npractitioners can select the most suitable approach for real-world applications\nsuch as smart surveillance, environmental monitoring, and industrial acoustic\nanalysis. This study serves as a benchmark for spike encoding in environmental\nsound classification, providing a foundational reference for future research in\nneuromorphic audio processing.", "published": "2025-03-14 08:52:04", "link": "http://arxiv.org/abs/2503.11206v2", "categories": ["cs.SD", "cs.ET", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MAVFlow: Preserving Paralinguistic Elements with Conditional Flow Matching for Zero-Shot AV2AV Multilingual Translation", "abstract": "Despite recent advances in text-to-speech (TTS) models, audio-visual to\naudio-visual (AV2AV) translation still faces a critical challenge: maintaining\nspeaker consistency between the original and translated vocal and facial\nfeatures. To address this issue, we propose a conditional flow matching (CFM)\nzero-shot audio-visual renderer that utilizes strong dual guidance from both\naudio and visual modalities. By leveraging multi-modal guidance with CFM, our\nmodel robustly preserves speaker-specific characteristics and significantly\nenhances zero-shot AV2AV translation abilities. For the audio modality, we\nenhance the CFM process by integrating robust speaker embeddings with\nx-vectors, which serve to bolster speaker consistency. Additionally, we convey\nemotional nuances to the face rendering module. The guidance provided by both\naudio and visual cues remains independent of semantic or linguistic content,\nallowing our renderer to effectively handle zero-shot translation tasks for\nmonolingual speakers in different languages. We empirically demonstrate that\nthe inclusion of high-quality mel-spectrograms conditioned on facial\ninformation not only enhances the quality of the synthesized speech but also\npositively influences facial generation, leading to overall performance\nimprovements.", "published": "2025-03-14 02:48:43", "link": "http://arxiv.org/abs/2503.11026v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
