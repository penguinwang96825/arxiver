{"title": "Interactive Text Generation", "abstract": "Users interact with text, image, code, or other editors on a daily basis.\nHowever, machine learning models are rarely trained in the settings that\nreflect the interactivity between users and their editor. This is\nunderstandable as training AI models with real users is not only slow and\ncostly, but what these models learn may be specific to user interface design\nchoices. Unfortunately, this means most of the research on text, code, and\nimage generation has focused on non-interactive settings, whereby the model is\nexpected to get everything right without accounting for any input from a user\nwho may be willing to help.\n  We introduce a new Interactive Text Generation task that allows training\ngeneration models interactively without the costs of involving real users, by\nusing user simulators that provide edits that guide the model towards a given\ntarget text. We train our interactive models using Imitation Learning, and our\nexperiments against competitive non-interactive generation models show that\nmodels trained interactively are superior to their non-interactive\ncounterparts, even when all models are given the same budget of user inputs or\nedits.", "published": "2023-03-02 01:57:17", "link": "http://arxiv.org/abs/2303.00908v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking the Reasonability of the Test Set for Simultaneous Machine\n  Translation", "abstract": "Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.", "published": "2023-03-02 05:06:44", "link": "http://arxiv.org/abs/2303.00969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adopting the Multi-answer Questioning Task with an Auxiliary Metric for\n  Extreme Multi-label Text Classification Utilizing the Label Hierarchy", "abstract": "Extreme multi-label text classification utilizes the label hierarchy to\npartition extreme labels into multiple label groups, turning the task into\nsimple multi-group multi-label classification tasks. Current research encodes\nlabels as a vector with fixed length which needs establish multiple classifiers\nfor different label groups. The problem is how to build only one classifier\nwithout sacrificing the label relationship in the hierarchy. This paper adopts\nthe multi-answer questioning task for extreme multi-label classification. This\npaper also proposes an auxiliary classification evaluation metric. This study\nadopts the proposed method and the evaluation metric to the legal domain. The\nutilization of legal Berts and the study on task distribution are discussed.\nThe experiment results show that the proposed hierarchy and multi-answer\nquestioning task can do extreme multi-label classification for EURLEX dataset.\nAnd in minor/fine-tuning the multi-label classification task, the domain\nadapted BERT models could not show apparent advantages in this experiment. The\nmethod is also theoretically applicable to zero-shot learning.", "published": "2023-03-02 08:40:31", "link": "http://arxiv.org/abs/2303.01064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTRLStruct: Dialogue Structure Learning for Open-Domain Response\n  Generation", "abstract": "Dialogue structure discovery is essential in dialogue generation.\nWell-structured topic flow can leverage background information and predict\nfuture topics to help generate controllable and explainable responses. However,\nmost previous work focused on dialogue structure learning in task-oriented\ndialogue other than open-domain dialogue which is more complicated and\nchallenging. In this paper, we present a new framework CTRLStruct for dialogue\nstructure learning to effectively explore topic-level dialogue clusters as well\nas their transitions with unlabelled information. Precisely, dialogue\nutterances encoded by bi-directional Transformer are further trained through a\nspecial designed contrastive learning task to improve representation. Then we\nperform clustering to utterance-level representations and form topic-level\nclusters that can be considered as vertices in dialogue structure graph. The\nedges in the graph indicating transition probability between vertices are\ncalculated by mimicking expert behavior in datasets. Finally, dialogue\nstructure graph is integrated into dialogue model to perform controlled\nresponse generation. Experiments on two popular open-domain dialogue datasets\nshow our model can generate more coherent responses compared to some excellent\ndialogue models, as well as outperform some typical sentence embedding methods\nin dialogue utterance representation. Code is available in GitHub.", "published": "2023-03-02 09:27:11", "link": "http://arxiv.org/abs/2303.01094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising-based UNMT is more robust to word-order divergence than\n  MASS-based UNMT", "abstract": "We aim to investigate whether UNMT approaches with self-supervised\npre-training are robust to word-order divergence between language pairs. We\nachieve this by comparing two models pre-trained with the same self-supervised\npre-training objective. The first model is trained on language pairs with\ndifferent word-orders, and the second model is trained on the same language\npairs with source language re-ordered to match the word-order of the target\nlanguage. Ideally, UNMT approaches which are robust to word-order divergence\nshould exhibit no visible performance difference between the two\nconfigurations. In this paper, we investigate two such self-supervised\npre-training based UNMT approaches, namely Masked Sequence-to-Sequence\nPre-Training, (MASS) (which does not have shuffling noise) and Denoising\nAutoEncoder (DAE), (which has shuffling noise).\n  We experiment with five English$\\rightarrow$Indic language pairs, i.e.,\nen-hi, en-bn, en-gu, en-kn, and en-ta) where word-order of the source language\nis SVO (Subject-Verb-Object), and the word-order of the target languages is SOV\n(Subject-Object-Verb). We observed that for these language pairs, DAE-based\nUNMT approach consistently outperforms MASS in terms of translation accuracies.\nMoreover, bridging the word-order gap using reordering improves the translation\naccuracy of MASS-based UNMT models, while it cannot improve the translation\naccuracy of DAE-based UNMT models. This observation indicates that DAE-based\nUNMT is more robust to word-order divergence than MASS-based UNMT.\nWord-shuffling noise in DAE approach could be the possible reason for the\napproach being robust to word-order divergence.", "published": "2023-03-02 12:11:58", "link": "http://arxiv.org/abs/2303.01191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching-based Term Semantics Pre-training for Spoken Patient Query\n  Understanding", "abstract": "Medical Slot Filling (MSF) task aims to convert medical queries into\nstructured information, playing an essential role in diagnosis dialogue\nsystems. However, the lack of sufficient term semantics learning makes existing\napproaches hard to capture semantically identical but colloquial expressions of\nterms in medical conversations. In this work, we formalize MSF into a matching\nproblem and propose a Term Semantics Pre-trained Matching Network (TSPMN) that\ntakes both terms and queries as input to model their semantic interaction. To\nlearn term semantics better, we further design two self-supervised objectives,\nincluding Contrastive Term Discrimination (CTD) and Matching-based Mask Term\nModeling (MMTM). CTD determines whether it is the masked term in the dialogue\nfor each given term, while MMTM directly predicts the masked ones. Experimental\nresults on two Chinese benchmarks show that TSPMN outperforms strong baselines,\nespecially in few-shot settings.", "published": "2023-03-02 15:17:38", "link": "http://arxiv.org/abs/2303.01341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP Workbench: Efficient and Extensible Integration of State-of-the-art\n  Text Mining Tools", "abstract": "NLP Workbench is a web-based platform for text mining that allows non-expert\nusers to obtain semantic understanding of large-scale corpora using\nstate-of-the-art text mining models. The platform is built upon latest\npre-trained models and open source systems from academia that provide semantic\nanalysis functionalities, including but not limited to entity linking,\nsentiment analysis, semantic parsing, and relation extraction. Its extensible\ndesign enables researchers and developers to smoothly replace an existing model\nor integrate a new one. To improve efficiency, we employ a microservice\narchitecture that facilitates allocation of acceleration hardware and\nparallelization of computation. This paper presents the architecture of NLP\nWorkbench and discusses the challenges we faced in designing it. We also\ndiscuss diverse use cases of NLP Workbench and the benefits of using it over\nother approaches. The platform is under active development, with its source\ncode released under the MIT license. A website and a short video demonstrating\nour platform are also available.", "published": "2023-03-02 16:59:31", "link": "http://arxiv.org/abs/2303.01410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WiCE: Real-World Entailment for Claims in Wikipedia", "abstract": "Textual entailment models are increasingly applied in settings like\nfact-checking, presupposition verification in question answering, or summary\nevaluation. However, these represent a significant domain shift from existing\nentailment datasets, and models underperform as a result. We propose WiCE, a\nnew fine-grained textual entailment dataset built on natural claim and evidence\npairs extracted from Wikipedia. In addition to standard claim-level entailment,\nWiCE provides entailment judgments over sub-sentence units of the claim, and a\nminimal subset of evidence sentences that support each subclaim. To support\nthis, we propose an automatic claim decomposition strategy using GPT-3.5 which\nwe show is also effective at improving entailment models' performance on\nmultiple datasets at test time. Finally, we show that real claims in our\ndataset involve challenging verification and retrieval problems that existing\nmodels fail to address.", "published": "2023-03-02 17:45:32", "link": "http://arxiv.org/abs/2303.01432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Variety Identification with True Labels", "abstract": "Language identification is an important first step in many IR and NLP\napplications. Most publicly available language identification datasets,\nhowever, are compiled under the assumption that the gold label of each instance\nis determined by where texts are retrieved from. Research has shown that this\nis a problematic assumption, particularly in the case of very similar languages\n(e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian\nand European Portuguese), where texts may contain no distinctive marker of the\nparticular language or variety. To overcome this important limitation, this\npaper presents DSL True Labels (DSL-TL), the first human-annotated multilingual\ndataset for language variety identification. DSL-TL contains a total of 12,900\ninstances in Portuguese, split between European Portuguese and Brazilian\nPortuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and\nEnglish, split between American English and British English. We trained\nmultiple models to discriminate between these language varieties, and we\npresent the results in detail. The data and models presented in this paper\nprovide a reliable benchmark toward the development of robust and fairer\nlanguage variety identification systems. We make DSL-TL freely available to the\nresearch community.", "published": "2023-03-02 18:51:58", "link": "http://arxiv.org/abs/2303.01490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture of Soft Prompts for Controllable Data Generation", "abstract": "Large language models (LLMs) effectively generate fluent text when the target\noutput follows natural language patterns. However, structured prediction tasks\nconfine the output format to a limited ontology, causing even very large models\nto struggle since they were never trained with such restrictions in mind. The\ndifficulty of using LLMs for direct prediction is exacerbated in few-shot\nlearning scenarios, which commonly arise due to domain shift and resource\nlimitations. We flip the problem on its head by leveraging the LLM as a tool\nfor data augmentation rather than direct prediction. Our proposed Mixture of\nSoft Prompts (MSP) serves as a parameter-efficient procedure for generating\ndata in a controlled manner. Denoising mechanisms are further applied to\nimprove the quality of synthesized data. Automatic metrics show our method is\ncapable of producing diverse and natural text, while preserving label\nsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarks\nwhen compared against strong baselines. Our method offers an alternate\ndata-centric approach for applying LLMs to complex prediction tasks.", "published": "2023-03-02 21:13:56", "link": "http://arxiv.org/abs/2303.01580v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic Clustered Federated Learning", "abstract": "Federated learning is a distributed learning framework that takes full\nadvantage of private data samples kept on edge devices. In real-world federated\nlearning systems, these data samples are often decentralized and\nNon-Independently Identically Distributed (Non-IID), causing divergence and\nperformance degradation in the federated learning process. As a new solution,\nclustered federated learning groups federated clients with similar data\ndistributions to impair the Non-IID effects and train a better model for every\ncluster. This paper proposes StoCFL, a novel clustered federated learning\napproach for generic Non-IID issues. In detail, StoCFL implements a flexible\nCFL framework that supports an arbitrary proportion of client participation and\nnewly joined clients for a varying FL system, while maintaining a great\nimprovement in model performance. The intensive experiments are conducted by\nusing four basic Non-IID settings and a real-world dataset. The results show\nthat StoCFL could obtain promising cluster results even when the number of\nclusters is unknown. Based on the client clustering results, models trained\nwith StoCFL outperform baseline approaches in a variety of contexts.", "published": "2023-03-02 01:39:16", "link": "http://arxiv.org/abs/2303.00897v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from\n  fifteen million scientific image-text pairs", "abstract": "Biomedical data is inherently multimodal, comprising physical measurements\nand natural language narratives. A generalist biomedical AI model needs to\nsimultaneously process different modalities of data, including text and images.\nTherefore, training an effective generalist biomedical model requires\nhigh-quality multimodal data, such as parallel image-text pairs. Here, we\npresent PMC-15M, a novel dataset that is two orders of magnitude larger than\nexisting biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse\nrange of biomedical image types. PMC-15M contains 15 million biomedical\nimage-text pairs collected from 4.4 million scientific articles. Based on\nPMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with\ndomain-specific adaptations tailored to biomedical vision-language processing.\nWe conducted extensive experiments and ablation studies on standard biomedical\nimaging tasks from retrieval to classification to visual question-answering\n(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of\nstandard datasets, substantially outperforming prior approaches. Intriguingly,\nby large-scale pretraining on diverse biomedical image types, BiomedCLIP even\noutperforms state-of-the-art radiology-specific models such as BioViL in\nradiology-specific tasks such as RSNA pneumonia detection. In summary,\nBiomedCLIP is a fully open-access foundation model that achieves\nstate-of-the-art performance on various biomedical tasks, paving the way for\ntransformative multimodal biomedical discovery and applications. We release our\nmodels at https://aka.ms/biomedclip to facilitate future research in multimodal\nbiomedical AI.", "published": "2023-03-02 02:20:04", "link": "http://arxiv.org/abs/2303.00915v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Text Corpora for End-to-End Speech Summarization", "abstract": "End-to-end speech summarization (E2E SSum) is a technique to directly\ngenerate summary sentences from speech. Compared with the cascade approach,\nwhich combines automatic speech recognition (ASR) and text summarization\nmodels, the E2E approach is more promising because it mitigates ASR errors,\nincorporates nonverbal information, and simplifies the overall system. However,\nsince collecting a large amount of paired data (i.e., speech and summary) is\ndifficult, the training data is usually insufficient to train a robust E2E SSum\nsystem. In this paper, we present two novel methods that leverage a large\namount of external text summarization data for E2E SSum training. The first\ntechnique is to utilize a text-to-speech (TTS) system to generate synthesized\nspeech, which is used for E2E SSum training with the text summary. The second\nis a TTS-free method that directly inputs phoneme sequence instead of\nsynthesized speech to the E2E SSum model. Experiments show that our proposed\nTTS- and phoneme-based methods improve several metrics on the How2 dataset. In\nparticular, our best system outperforms a previous state-of-the-art one by a\nlarge margin (i.e., METEOR score improvements of more than 6 points). To the\nbest of our knowledge, this is the first work to use external language\nresources for E2E SSum. Moreover, we report a detailed analysis of the How2\ndataset to confirm the validity of our proposed E2E SSum system.", "published": "2023-03-02 05:19:49", "link": "http://arxiv.org/abs/2303.00978v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study", "abstract": "Large pre-trained language models help to achieve state of the art on a\nvariety of natural language processing (NLP) tasks, nevertheless, they still\nsuffer from forgetting when incrementally learning a sequence of tasks. To\nalleviate this problem, recent works enhance existing models by sparse\nexperience replay and local adaption, which yield satisfactory performance.\nHowever, in this paper we find that pre-trained language models like BERT have\na potential ability to learn sequentially, even without any sparse memory\nreplay. To verify the ability of BERT to maintain old knowledge, we adopt and\nre-finetune single-layer probe networks with the parameters of BERT fixed. We\ninvestigate the models on two types of NLP tasks, text classification and\nextractive question answering. Our experiments reveal that BERT can actually\ngenerate high quality representations for previously learned tasks in a long\nterm, under extremely sparse replay or even no replay. We further introduce a\nseries of novel methods to interpret the mechanism of forgetting and how memory\nrehearsal plays a significant role in task incremental learning, which bridges\nthe gap between our new discovery and previous studies about catastrophic\nforgetting.", "published": "2023-03-02 09:03:43", "link": "http://arxiv.org/abs/2303.01081v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data\n  Generation for Cross-Lingual Learning in Tweet Intimacy Prediction", "abstract": "This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9\n\"Multilingual Tweet Intimacy Analysis\". We achieved second-best results in all\n10 languages according to the official Pearson's correlation regression\nevaluation measure. Our cross-lingual transfer learning approach explores the\nbenefits of using a Head-First Fine-Tuning method (HeFiT) that first updates\nonly the regression head parameters and then also updates the pre-trained\ntransformer encoder parameters at a reduced learning rate. Additionally, we\nstudy the impact of using a small set of automatically generated examples (in\nour case, from ChatGPT) for low-resource settings where no human-labeled data\nis available. Our study shows that HeFiT stabilizes training and consistently\nimproves results for pre-trained models that lack domain adaptation to tweets.\nOur study also shows a noticeable performance increase in cross-lingual\nlearning when synthetic data is used, confirming the usefulness of current text\ngeneration systems to improve zero-shot baseline results. Finally, we examine\nhow possible inconsistencies in the annotated data contribute to cross-lingual\ninterference issues.", "published": "2023-03-02 12:18:53", "link": "http://arxiv.org/abs/2303.01194v2", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Document Provenance and Authentication through Authorship Classification", "abstract": "Style analysis, which is relatively a less explored topic, enables several\ninteresting applications. For instance, it allows authors to adjust their\nwriting style to produce a more coherent document in collaboration. Similarly,\nstyle analysis can also be used for document provenance and authentication as a\nprimary step. In this paper, we propose an ensemble-based text-processing\nframework for the classification of single and multi-authored documents, which\nis one of the key tasks in style analysis. The proposed framework incorporates\nseveral state-of-the-art text classification algorithms including classical\nMachine Learning (ML) algorithms, transformers, and deep learning algorithms\nboth individually and in merit-based late fusion. For the merit-based late\nfusion, we employed several weight optimization and selection methods to assign\nmerit-based weights to the individual text classification algorithms. We also\nanalyze the impact of the characters on the task that are usually excluded in\nNLP applications during pre-processing by conducting experiments on both clean\nand un-clean data. The proposed framework is evaluated on a large-scale\nbenchmark dataset, significantly improving performance over the existing\nsolutions.", "published": "2023-03-02 12:26:03", "link": "http://arxiv.org/abs/2303.01197v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Letz Translate: Low-Resource Machine Translation for Luxembourgish", "abstract": "Natural language processing of Low-Resource Languages (LRL) is often\nchallenged by the lack of data. Therefore, achieving accurate machine\ntranslation (MT) in a low-resource environment is a real problem that requires\npractical solutions. Research in multilingual models have shown that some LRLs\ncan be handled with such models. However, their large size and computational\nneeds make their use in constrained environments (e.g., mobile/IoT devices or\nlimited/old servers) impractical. In this paper, we address this problem by\nleveraging the power of large multilingual MT models using knowledge\ndistillation. Knowledge distillation can transfer knowledge from a large and\ncomplex teacher model to a simpler and smaller student model without losing\nmuch in performance. We also make use of high-resource languages that are\nrelated or share the same linguistic root as the target LRL. For our\nevaluation, we consider Luxembourgish as the LRL that shares some roots and\nproperties with German. We build multiple resource-efficient models based on\nGerman, knowledge distillation from the multilingual No Language Left Behind\n(NLLB) model, and pseudo-translation. We find that our efficient models are\nmore than 30\\% faster and perform only 4\\% lower compared to the large\nstate-of-the-art NLLB model.", "published": "2023-03-02 15:26:46", "link": "http://arxiv.org/abs/2303.01347v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Semiparametric Language Models Are Scalable Continual Learners", "abstract": "Semiparametric language models (LMs) have shown promise in continuously\nlearning from new text data by combining a parameterized neural LM with a\ngrowable non-parametric memory for memorizing new content. However,\nconventional semiparametric LMs will finally become prohibitive for computing\nand storing if they are applied to continual learning over streaming data,\nbecause the non-parametric memory grows linearly with the amount of data they\nlearn from over time. To address the issue of scalability, we present a simple\nand intuitive approach called Selective Memorization (SeMem), which only\nmemorizes difficult samples that the model is likely to struggle with. We\ndemonstrate that SeMem improves the scalability of semiparametric LMs for\ncontinual learning over streaming data in two ways: (1) data-wise scalability:\nas the model becomes stronger through continual learning, it will encounter\nfewer difficult cases that need to be memorized, causing the growth of the\nnon-parametric memory to slow down over time rather than growing at a linear\nrate with the size of training data; (2) model-wise scalability: SeMem allows a\nlarger model to memorize fewer samples than its smaller counterpart because it\nis rarer for a larger model to encounter incomprehensible cases, resulting in a\nnon-parametric memory that does not scale linearly with model size. We conduct\nextensive experiments in language modeling and downstream tasks to test SeMem's\nresults, showing SeMem enables a semiparametric LM to be a scalable continual\nlearner with little forgetting.", "published": "2023-03-02 17:15:02", "link": "http://arxiv.org/abs/2303.01421v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Computational Language Acquisition with Theory of Mind", "abstract": "Unlike current state-of-the-art language models, young children actively\nacquire language through interactions with their surrounding environment and\ncaretakers. One mechanism that has been argued to be critical to language\nlearning is the ability to infer the mental states of other agents in social\nenvironments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing\ninspiration from the modern operationalized versions of ToM implemented in\nRabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning\nagents equipped with ToM, and measure its effects on the learning process. We\nmodel ToM by giving the speaker agent an internal listener model that is\ntrained alongside the speaker and used to rerank potential utterances. We\nexperiment with varying task difficulty, hypothesizing that models will acquire\nmore complex language to adapt to stronger environmental pressures. We find\nthat training speakers with a highly weighted ToM listener component leads to\nperformance gains in our image referential game setting. We also find some\nevidence that increasing task difficulty in the training process results in\nmore fluent and precise utterances in evaluation. This suggests the potential\nutility of further incorporating ToM, as well as other insights from child\nlanguage acquisition, into computational models of language acquisition.", "published": "2023-03-02 18:59:46", "link": "http://arxiv.org/abs/2303.01502v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Technical report: Graph Neural Networks go Grammatical", "abstract": "This paper introduces a framework for formally establishing a connection\nbetween a portion of an algebraic language and a Graph Neural Network (GNN).\nThe framework leverages Context-Free Grammars (CFG) to organize algebraic\noperations into generative rules that can be translated into a GNN layer model.\nAs CFGs derived directly from a language tend to contain redundancies in their\nrules and variables, we present a grammar reduction scheme. By applying this\nstrategy, we define a CFG that conforms to the third-order Weisfeiler-Lehman\n(3-WL) test using MATLANG. From this 3-WL CFG, we derive a GNN model, named\nG$^2$N$^2$, which is provably 3-WL compliant. Through various experiments, we\ndemonstrate the superior efficiency of G$^2$N$^2$ compared to other 3-WL GNNs\nacross numerous downstream tasks. Specifically, one experiment highlights the\nbenefits of grammar reduction within our framework.", "published": "2023-03-02 21:27:54", "link": "http://arxiv.org/abs/2303.01590v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue\n  Response Generation Models by Causal Discovery", "abstract": "In this paper, we conduct the first study on spurious correlations for\nopen-domain response generation models based on a corpus CGDIALOG curated in\nour work. The cur rent models indeed suffer from spurious correlations and have\na tendency of generating irrelevant and generic responses. Inspired by causal\ndiscovery algorithms, we propose a novel model-agnostic method for training and\ninference of response generation model using a conditional independence\nclassifier. The classifier is trained by a constrained self-training method,\ncoined CONSTRAIN, to overcome data scarcity. The experimental results based on\nboth human and automatic evaluation show that our method significantly\noutperforms the competitive baselines in terms of relevance, informativeness,\nand fluency.", "published": "2023-03-02 06:33:48", "link": "http://arxiv.org/abs/2303.01962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "We introduce the Universal Speech Model (USM), a single large model that\nperforms automatic speech recognition (ASR) across 100+ languages. This is\nachieved by pre-training the encoder of the model on a large unlabeled\nmultilingual dataset of 12 million (M) hours spanning over 300 languages, and\nfine-tuning on a smaller labeled dataset. We use multilingual pre-training with\nrandom-projection quantization and speech-text modality matching to achieve\nstate-of-the-art performance on downstream multilingual ASR and speech-to-text\ntranslation tasks. We also demonstrate that despite using a labeled training\nset 1/7-th the size of that used for the Whisper model, our model exhibits\ncomparable or better performance on both in-domain and out-of-domain speech\nrecognition tasks across many languages.", "published": "2023-03-02 07:47:18", "link": "http://arxiv.org/abs/2303.01037v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Targeted Adversarial Attacks against Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) systems are used in various applications.\nHowever, it has been shown that they are vulnerable to very small perturbations\nof their inputs, known as adversarial attacks. In this paper, we propose a new\ntargeted adversarial attack against NMT models. In particular, our goal is to\ninsert a predefined target keyword into the translation of the adversarial\nsentence while maintaining similarity between the original sentence and the\nperturbed one in the source domain. To this aim, we propose an optimization\nproblem, including an adversarial loss term and a similarity term. We use\ngradient projection in the embedding space to craft an adversarial sentence.\nExperimental results show that our attack outperforms Seq2Sick, the other\ntargeted adversarial attack against NMT models, in terms of success rate and\ndecrease in translation quality. Our attack succeeds in inserting a keyword\ninto the translation for more than 75% of sentences while similarity with the\noriginal sentence stays preserved.", "published": "2023-03-02 08:43:30", "link": "http://arxiv.org/abs/2303.01068v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LANDMARK: Language-guided Representation Enhancement Framework for Scene\n  Graph Generation", "abstract": "Scene graph generation (SGG) is a sophisticated task that suffers from both\ncomplex visual features and dataset long-tail problem. Recently, various\nunbiased strategies have been proposed by designing novel loss functions and\ndata balancing strategies. Unfortunately, these unbiased methods fail to\nemphasize language priors in feature refinement perspective. Inspired by the\nfact that predicates are highly correlated with semantics hidden in\nsubject-object pair and global context, we propose LANDMARK (LANguage-guiDed\nrepresentationenhanceMent frAmewoRK) that learns predicate-relevant\nrepresentations from language-vision interactive patterns, global language\ncontext and pair-predicate correlation. Specifically, we first project object\nlabels to three distinctive semantic embeddings for different representation\nlearning. Then, Language Attention Module (LAM) and Experience Estimation\nModule (EEM) process subject-object word embeddings to attention vector and\npredicate distribution, respectively. Language Context Module (LCM) encodes\nglobal context from each word embed-ding, which avoids isolated learning from\nlocal information. Finally, modules outputs are used to update visual\nrepresentations and SGG model's prediction. All language representations are\npurely generated from object categories so that no extra knowledge is needed.\nThis framework is model-agnostic and consistently improves performance on\nexisting SGG models. Besides, representation-level unbiased strategies endow\nLANDMARK the advantage of compatibility with other methods. Code is available\nat https://github.com/rafa-cxg/PySGG-cxg.", "published": "2023-03-02 09:03:11", "link": "http://arxiv.org/abs/2303.01080v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LiteG2P: A fast, light and high accuracy model for grapheme-to-phoneme\n  conversion", "abstract": "As a key component of automated speech recognition (ASR) and the front-end in\ntext-to-speech (TTS), grapheme-to-phoneme (G2P) plays the role of converting\nletters to their corresponding pronunciations. Existing methods are either slow\nor poor in performance, and are limited in application scenarios, particularly\nin the process of on-device inference. In this paper, we integrate the\nadvantages of both expert knowledge and connectionist temporal classification\n(CTC) based neural network and propose a novel method named LiteG2P which is\nfast, light and theoretically parallel. With the carefully leading design,\nLiteG2P can be applied both on cloud and on device. Experimental results on the\nCMU dataset show that the performance of the proposed method is superior to the\nstate-of-the-art CTC based method with 10 times fewer parameters, and even\ncomparable to the state-of-the-art Transformer-based sequence-to-sequence model\nwith less parameters and 33 times less computation.", "published": "2023-03-02 09:16:21", "link": "http://arxiv.org/abs/2303.01086v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Synthetic Misinformers: Generating and Combating Multimodal\n  Misinformation", "abstract": "With the expansion of social media and the increasing dissemination of\nmultimedia content, the spread of misinformation has become a major concern.\nThis necessitates effective strategies for multimodal misinformation detection\n(MMD) that detect whether the combination of an image and its accompanying text\ncould mislead or misinform. Due to the data-intensive nature of deep neural\nnetworks and the labor-intensive process of manual annotation, researchers have\nbeen exploring various methods for automatically generating synthetic\nmultimodal misinformation - which we refer to as Synthetic Misinformers - in\norder to train MMD models. However, limited evaluation on real-world\nmisinformation and a lack of comparisons with other Synthetic Misinformers\nmakes difficult to assess progress in the field. To address this, we perform a\ncomparative study on existing and new Synthetic Misinformers that involves (1)\nout-of-context (OOC) image-caption pairs, (2) cross-modal named entity\ninconsistency (NEI) as well as (3) hybrid approaches and we evaluate them\nagainst real-world misinformation; using the COSMOS benchmark. The comparative\nstudy showed that our proposed CLIP-based Named Entity Swapping can lead to MMD\nmodels that surpass other OOC and NEI Misinformers in terms of multimodal\naccuracy and that hybrid approaches can lead to even higher detection accuracy.\nNevertheless, after alleviating information leakage from the COSMOS evaluation\nprotocol, low Sensitivity scores indicate that the task is significantly more\nchallenging than previous studies suggested. Finally, our findings showed that\nNEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where\ntext-only MMDs can outperform multimodal ones.", "published": "2023-03-02 12:59:01", "link": "http://arxiv.org/abs/2303.01217v1", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "MLANet: Multi-Level Attention Network with Sub-instruction for\n  Continuous Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) aims to develop intelligent agents to\nnavigate in unseen environments only through language and vision supervision.\nIn the recently proposed continuous settings (continuous VLN), the agent must\nact in a free 3D space and faces tougher challenges like real-time execution,\ncomplex instruction understanding, and long action sequence prediction. For a\nbetter performance in continuous VLN, we design a multi-level instruction\nunderstanding procedure and propose a novel model, Multi-Level Attention\nNetwork (MLANet). The first step of MLANet is to generate sub-instructions\nefficiently. We design a Fast Sub-instruction Algorithm (FSA) to segment the\nraw instruction into sub-instructions and generate a new sub-instruction\ndataset named ``FSASub\". FSA is annotation-free and faster than the current\nmethod by 70 times, thus fitting the real-time requirement in continuous VLN.\nTo solve the complex instruction understanding problem, MLANet needs a global\nperception of the instruction and observations. We propose a Multi-Level\nAttention (MLA) module to fuse vision, low-level semantics, and high-level\nsemantics, which produce features containing a dynamic and global comprehension\nof the task. MLA also mitigates the adverse effects of noise words, thus\nensuring a robust understanding of the instruction. To correctly predict\nactions in long trajectories, MLANet needs to focus on what sub-instruction is\nbeing executed every step. We propose a Peak Attention Loss (PAL) to improve\nthe flexible and adaptive selection of the current sub-instruction. PAL\nbenefits the navigation agent by concentrating its attention on the local\ninformation, thus helping the agent predict the most appropriate actions. We\ntrain and test MLANet in the standard benchmark. Experiment results show MLANet\noutperforms baselines by a significant margin.", "published": "2023-03-02 16:26:14", "link": "http://arxiv.org/abs/2303.01396v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "INO at Factify 2: Structure Coherence based Multi-Modal Fact\n  Verification", "abstract": "This paper describes our approach to the multi-modal fact verification\n(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of\nsocial media, fake news can spread rapidly and negatively impact social\nsecurity. Automatic claim verification becomes more and more crucial to combat\nfake news. In fact verification involving multiple modal data, there should be\na structural coherence between claim and document. Therefore, we proposed a\nstructure coherence-based multi-modal fact verification scheme to classify fake\nnews. Our structure coherence includes the following four aspects: sentence\nlength, vocabulary similarity, semantic similarity, and image similarity.\nSpecifically, CLIP and Sentence BERT are combined to extract text features, and\nResNet50 is used to extract image features. In addition, we also extract the\nlength of the text as well as the lexical similarity. Then the features were\nconcatenated and passed through the random forest classifier. Finally, our\nweighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.", "published": "2023-03-02 11:18:56", "link": "http://arxiv.org/abs/2303.01510v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "QAID: Question Answering Inspired Few-shot Intent Detection", "abstract": "Intent detection with semantically similar fine-grained intents is a\nchallenging task. To address it, we reformulate intent detection as a\nquestion-answering retrieval task by treating utterances and intent names as\nquestions and answers. To that end, we utilize a question-answering retrieval\narchitecture and adopt a two stages training schema with batch contrastive\nloss. In the pre-training stage, we improve query representations through\nself-supervised training. Then, in the fine-tuning stage, we increase\ncontextualized token-level similarity scores between queries and answers from\nthe same intent. Our results on three few-shot intent detection benchmarks\nachieve state-of-the-art performance.", "published": "2023-03-02 21:35:15", "link": "http://arxiv.org/abs/2303.01593v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Parameter-Efficient Transfer Learning Approaches on SURE\n  Benchmark for Speech Understanding", "abstract": "Fine-tuning is widely used as the default algorithm for transfer learning\nfrom pre-trained models. Parameter inefficiency can however arise when, during\ntransfer learning, all the parameters of a large pre-trained model need to be\nupdated for individual downstream tasks. As the number of parameters grows,\nfine-tuning is prone to overfitting and catastrophic forgetting. In addition,\nfull fine-tuning can become prohibitively expensive when the model is used for\nmany tasks. To mitigate this issue, parameter-efficient transfer learning\nalgorithms, such as adapters and prefix tuning, have been proposed as a way to\nintroduce a few trainable parameters that can be plugged into large pre-trained\nlanguage models such as BERT, and HuBERT. In this paper, we introduce the\nSpeech UndeRstanding Evaluation (SURE) benchmark for parameter-efficient\nlearning for various speech-processing tasks. Additionally, we introduce a new\nadapter, ConvAdapter, based on 1D convolution. We show that ConvAdapter\noutperforms the standard adapters while showing comparable performance against\nprefix tuning and LoRA with only 0.94% of trainable parameters on some of the\ntask in SURE. We further explore the effectiveness of parameter efficient\ntransfer learning for speech synthesis task such as Text-to-Speech (TTS).", "published": "2023-03-02 08:57:33", "link": "http://arxiv.org/abs/2303.03267v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a\n  Context Synergized Hyperbolic Network", "abstract": "The tremendous growth of social media users interacting in online\nconversations has led to significant growth in hate speech, affecting people\nfrom various demographics. Most of the prior works focus on detecting explicit\nhate speech, which is overt and leverages hateful phrases, with very little\nwork focusing on detecting hate speech that is implicit or denotes hatred\nthrough indirect or coded language. In this paper, we present CoSyn, a\ncontext-synergized neural network that explicitly incorporates user- and\nconversational context for detecting implicit hate speech in online\nconversations. CoSyn introduces novel ways to encode these external contexts\nand employs a novel context interaction mechanism that clearly captures the\ninterplay between them, making independent assessments of the amounts of\ninformation to be retrieved from these noisy contexts. Additionally, it carries\nout all these operations in the hyperbolic space to account for the scale-free\ndynamics of social media. We demonstrate the effectiveness of CoSyn on 6 hate\nspeech datasets and show that CoSyn outperforms all our baselines in detecting\nimplicit hate speech with absolute improvements in the range of 1.24% - 57.8%.", "published": "2023-03-02 17:30:43", "link": "http://arxiv.org/abs/2303.03387v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Real-time Audio Video Enhancement \\\\with a Microphone Array and\n  Headphones", "abstract": "This paper presents a complete hardware and software pipeline for real-time\nspeech enhancement in noisy and reverberant conditions. The device consists of\na microphone array and a camera mounted on eyeglasses, connected to an embedded\nsystem that enhances speech and plays back the audio in headphones, with a\nlatency of maximum 120 msec. The proposed approach relies on face detection,\ntracking and verification to enhance the speech of a target speaker using a\nbeamformer and a postfiltering neural network. Results demonstrate the\nfeasibility of the approach, and opens the door to the exploration and\nvalidation of a wide range of beamformer and speech enhancement methods for\nreal-time speech enhancement.", "published": "2023-03-02 04:01:02", "link": "http://arxiv.org/abs/2303.00949v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Transformer-based End-to-End Speaker Diarization by Assigning\n  Auxiliary Losses to Attention Heads", "abstract": "Transformer-based end-to-end neural speaker diarization (EEND) models utilize\nthe multi-head self-attention (SA) mechanism to enable accurate speaker label\nprediction in overlapped speech regions. In this study, to enhance the training\neffectiveness of SA-EEND models, we propose the use of auxiliary losses for the\nSA heads of the transformer layers. Specifically, we assume that the attention\nweight matrices of an SA layer are redundant if their patterns are similar to\nthose of the identity matrix. We then explicitly constrain such matrices to\nexhibit specific speaker activity patterns relevant to voice activity detection\nor overlapped speech detection tasks. Consequently, we expect the proposed\nauxiliary losses to guide the transformer layers to exhibit more diverse\npatterns in the attention weights, thereby reducing the assumed redundancies in\nthe SA heads. The effectiveness of the proposed method is demonstrated using\nthe simulated and CALLHOME datasets for two-speaker diarization tasks, reducing\nthe diarization error rate of the conventional SA-EEND model by 32.58% and\n17.11%, respectively.", "published": "2023-03-02 12:15:36", "link": "http://arxiv.org/abs/2303.01192v1", "categories": ["eess.AS", "cs.SD", "68T10(Primary) 68T07(Secondary)"], "primary_category": "eess.AS"}
{"title": "Distilling Multi-Level X-vector Knowledge for Small-footprint Speaker\n  Verification", "abstract": "Even though deep speaker models have demonstrated impressive accuracy in\nspeaker verification tasks, this often comes at the expense of increased model\nsize and computation time, presenting challenges for deployment in\nresource-constrained environments. Our research focuses on addressing this\nlimitation through the development of small footprint deep speaker embedding\nextraction using knowledge distillation. While previous work in this domain has\nconcentrated on speaker embedding extraction at the utterance level, our\napproach involves amalgamating embeddings from different levels of the x-vector\nmodel (teacher network) to train a compact student network. The results\nhighlight the significance of frame-level information, with the student models\nexhibiting a remarkable size reduction of 85%-91% compared to their teacher\ncounterparts, depending on the size of the teacher embeddings. Notably, by\nconcatenating teacher embeddings, we achieve student networks that maintain\ncomparable performance to the teacher while enjoying a substantial 75%\nreduction in model size. These findings and insights extend to other x-vector\nvariants, underscoring the broad applicability of our approach.", "published": "2023-03-02 10:09:11", "link": "http://arxiv.org/abs/2303.01125v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-Aware Anti-Spoofing", "abstract": "We address speaker-aware anti-spoofing, where prior knowledge of the target\nspeaker is incorporated into a voice spoofing countermeasure (CM). In contrast\nto the frequently used speaker-independent solutions, we train the CM in a\nspeaker-conditioned way. As a proof of concept, we consider speaker-aware\nextension to the state-of-the-art AASIST (audio anti-spoofing using integrated\nspectro-temporal graph attention networks) model. To this end, we consider two\nalternative strategies to incorporate target speaker information at the frame\nand utterance levels, respectively. The experimental results on a custom\nprotocol based on ASVspoof 2019 dataset indicates the efficiency of the speaker\ninformation via enrollment: we obtain maximum relative improvements of 25.1%\nand 11.6% in equal error rate (EER) and minimum tandem detection cost function\n(t-DCF) over a speaker-independent baseline, respectively.", "published": "2023-03-02 10:14:59", "link": "http://arxiv.org/abs/2303.01126v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning From Yourself: A Self-Distillation Method for Fake Speech\n  Detection", "abstract": "In this paper, we propose a novel self-distillation method for fake speech\ndetection (FSD), which can significantly improve the performance of FSD without\nincreasing the model complexity. For FSD, some fine-grained information is very\nimportant, such as spectrogram defects, mute segments, and so on, which are\noften perceived by shallow networks. However, shallow networks have much noise,\nwhich can not capture this very well. To address this problem, we propose using\nthe deepest network instruct shallow network for enhancing shallow networks.\nSpecifically, the networks of FSD are divided into several segments, the\ndeepest network being used as the teacher model, and all shallow networks\nbecome multiple student models by adding classifiers. Meanwhile, the\ndistillation path between the deepest network feature and shallow network\nfeatures is used to reduce the feature difference. A series of experimental\nresults on the ASVspoof 2019 LA and PA datasets show the effectiveness of the\nproposed method, with significant improvements compared to the baseline.", "published": "2023-03-02 12:52:22", "link": "http://arxiv.org/abs/2303.01211v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Defending against Adversarial Audio via Diffusion Model", "abstract": "Deep learning models have been widely used in commercial acoustic systems in\nrecent years. However, adversarial audio examples can cause abnormal behaviors\nfor those acoustic systems, while being hard for humans to perceive. Various\nmethods, such as transformation-based defenses and adversarial training, have\nbeen proposed to protect acoustic systems from adversarial attacks, but they\nare less effective against adaptive attacks. Furthermore, directly applying the\nmethods from the image domain can lead to suboptimal results because of the\nunique properties of audio data. In this paper, we propose an adversarial\npurification-based defense pipeline, AudioPure, for acoustic systems via\noff-the-shelf diffusion models. Taking advantage of the strong generation\nability of diffusion models, AudioPure first adds a small amount of noise to\nthe adversarial audio and then runs the reverse sampling step to purify the\nnoisy audio and recover clean audio. AudioPure is a plug-and-play method that\ncan be directly applied to any pretrained classifier without any fine-tuning or\nre-training. We conduct extensive experiments on speech command recognition\ntask to evaluate the robustness of AudioPure. Our method is effective against\ndiverse adversarial attacks (e.g. $\\mathcal{L}_2$ or\n$\\mathcal{L}_\\infty$-norm). It outperforms the existing methods under both\nstrong adaptive white-box and black-box attacks bounded by $\\mathcal{L}_2$ or\n$\\mathcal{L}_\\infty$-norm (up to +20\\% in robust accuracy). Besides, we also\nevaluate the certified robustness for perturbations bounded by\n$\\mathcal{L}_2$-norm via randomized smoothing. Our pipeline achieves a higher\ncertified accuracy than baselines.", "published": "2023-03-02 07:15:47", "link": "http://arxiv.org/abs/2303.01507v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained Emotional Control of Text-To-Speech: Learning To Rank\n  Inter- And Intra-Class Emotion Intensities", "abstract": "State-of-the-art Text-To-Speech (TTS) models are capable of producing\nhigh-quality speech. The generated speech, however, is usually neutral in\nemotional expression, whereas very often one would want fine-grained emotional\ncontrol of words or phonemes. Although still challenging, the first TTS models\nhave been recently proposed that are able to control voice by manually\nassigning emotion intensity. Unfortunately, due to the neglect of intra-class\ndistance, the intensity differences are often unrecognizable. In this paper, we\npropose a fine-grained controllable emotional TTS, that considers both inter-\nand intra-class distances and be able to synthesize speech with recognizable\nintensity difference. Our subjective and objective experiments demonstrate that\nour model exceeds two state-of-the-art controllable TTS models for\ncontrollability, emotion expressiveness and naturalness.", "published": "2023-03-02 09:09:03", "link": "http://arxiv.org/abs/2303.01508v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
