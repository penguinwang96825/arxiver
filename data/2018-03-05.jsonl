{"title": "Automatic Translating between Ancient Chinese and Contemporary Chinese\n  with Limited Aligned Corpora", "abstract": "The Chinese language has evolved a lot during the long-term development.\nTherefore, native speakers now have trouble in reading sentences written in\nancient Chinese. In this paper, we propose to build an end-to-end neural model\nto automatically translate between ancient and contemporary Chinese. However,\nthe existing ancient-contemporary Chinese parallel corpora are not aligned at\nthe sentence level and sentence-aligned corpora are limited, which makes it\ndifficult to train the model. To build the sentence level parallel training\ndata for the model, we propose an unsupervised algorithm that constructs\nsentence-aligned ancient-contemporary pairs by using the fact that the aligned\nsentence pair shares many of the tokens. Based on the aligned corpus, we\npropose an end-to-end neural model with copying mechanism and local attention\nto translate between ancient and contemporary Chinese. Experiments show that\nthe proposed unsupervised algorithm achieves 99.4% F1 score for sentence\nalignment, and the translation model achieves 26.95 BLEU from ancient to\ncontemporary, and 36.34 BLEU from contemporary to ancient.", "published": "2018-03-05 08:37:47", "link": "http://arxiv.org/abs/1803.01557v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Open-Type Relation Argument Extraction", "abstract": "In this work, we introduce the task of Open-Type Relation Argument Extraction\n(ORAE): Given a corpus, a query entity Q and a knowledge base relation (e.g.,\"Q\nauthored notable work with title X\"), the model has to extract an argument of\nnon-standard entity type (entities that cannot be extracted by a standard named\nentity tagger, e.g. X: the title of a book or a work of art) from the corpus. A\ndistantly supervised dataset based on WikiData relations is obtained and\nreleased to address the task.\n  We develop and compare a wide range of neural models for this task yielding\nlarge improvements over a strong baseline obtained with a neural question\nanswering system. The impact of different sentence encoding architectures and\nanswer extraction methods is systematically compared. An encoder based on gated\nrecurrent units combined with a conditional random fields tagger gives the best\nresults.", "published": "2018-03-05 15:09:49", "link": "http://arxiv.org/abs/1803.01707v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query and Output: Generating Words by Querying Distributed Word\n  Representations for Paraphrase Generation", "abstract": "Most recent approaches use the sequence-to-sequence model for paraphrase\ngeneration. The existing sequence-to-sequence model tends to memorize the words\nand the patterns in the training dataset instead of learning the meaning of the\nwords. Therefore, the generated sentences are often grammatically correct but\nsemantically improper. In this work, we introduce a novel model based on the\nencoder-decoder framework, called Word Embedding Attention Network (WEAN). Our\nproposed model generates the words by querying distributed word representations\n(i.e. neural word embeddings), hoping to capturing the meaning of the according\nwords. Following previous work, we evaluate our model on two\nparaphrase-oriented tasks, namely text simplification and short text\nabstractive summarization. Experimental results show that our model outperforms\nthe sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two\nEnglish text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a\nChinese summarization dataset. Moreover, our model achieves state-of-the-art\nperformances on these three benchmark datasets.", "published": "2018-03-05 02:44:42", "link": "http://arxiv.org/abs/1803.01465v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Calculated attributes of synonym sets", "abstract": "The goal of formalization, proposed in this paper, is to bring together, as\nnear as possible, the theoretic linguistic problem of synonym conception and\nthe computer linguistic methods based generally on empirical intuitive\nunjustified factors. Using the word vector representation we have proposed the\ngeometric approach to mathematical modeling of synonym set (synset). The word\nembedding is based on the neural networks (Skip-gram, CBOW), developed and\nrealized as word2vec program by T. Mikolov. The standard cosine similarity is\nused as the distance between word-vectors. Several geometric characteristics of\nthe synset words are introduced: the interior of synset, the synset word rank\nand centrality. These notions are intended to select the most significant\nsynset words, i.e. the words which senses are the nearest to the sense of a\nsynset. Some experiments with proposed notions, based on RusVectores resources,\nare represented. A brief description of this work can be viewed in slides\nhttps://goo.gl/K82Fei", "published": "2018-03-05 10:09:32", "link": "http://arxiv.org/abs/1803.01580v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.5.3; H.3.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "The morphospace of language networks", "abstract": "Language can be described as a network of interacting objects with different\nqualitative properties and complexity. These networks include semantic,\nsyntactic, or phonological levels and have been found to provide a new picture\nof language complexity and its evolution. A general approach considers language\nfrom an information theory perspective that incorporates a speaker, a hearer,\nand a noisy channel. The later is often encoded in a matrix connecting the\nsignals used for communication with meanings to be found in the real world.\nMost studies of language evolution deal in a way or another with such\ntheoretical contraption and explore the outcome of diverse forms of selection\non the communication matrix that somewhat optimizes communication. This\nframework naturally introduces networks mediating the communicating agents, but\nno systematic analysis of the underlying landscape of possible language graphs\nhas been developed. Here we present a detailed analysis of network properties\non a generic model of a communication code, which reveals a rather complex and\nheterogeneous morphospace of language networks. Additionally, we use curated\ndata of English words to locate and evaluate real languages within this\nlanguage morphospace. Our findings indicate a surprisingly simple structure in\nhuman language unless particles are introduced in the vocabulary, with the\nability of naming any other concept. These results refine and for the first\ntime complement with empirical data a lasting theoretical tradition around the\nframework of \\emph{least effort language}.", "published": "2018-03-05 21:29:50", "link": "http://arxiv.org/abs/1803.01934v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Linking ImageNet WordNet Synsets with Wikidata", "abstract": "The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep\nlearning algorithm with access to a rich multilingual knowledge graph. Here I\nwill describe our on-going efforts in linking the two resources and issues\nfaced in matching the Wikidata and WordNet knowledge graphs. I show an example\non how the linkage can be used in a deep learning setting with real-time image\nclassification and labeling in a non-English language and discuss what\nopportunities lies ahead.", "published": "2018-03-05 17:07:44", "link": "http://arxiv.org/abs/1803.04349v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization\n  Tasks", "abstract": "Evaluation of summarization tasks is extremely crucial to determining the\nquality of machine generated summaries. Over the last decade, ROUGE has become\nthe standard automatic evaluation measure for evaluating summarization tasks.\nWhile ROUGE has been shown to be effective in capturing n-gram overlap between\nsystem and human composed summaries, there are several limitations with the\nexisting ROUGE measures in terms of capturing synonymous concepts and coverage\nof topics. Thus, often times ROUGE scores do not reflect the true quality of\nsummaries and prevents multi-faceted evaluation of summaries (i.e. by topics,\nby overall content coverage and etc). In this paper, we introduce ROUGE 2.0,\nwhich has several updated measures of ROUGE: ROUGE-N+Synonyms, ROUGE-Topic,\nROUGE-Topic+Synonyms, ROUGE-TopicUniq and ROUGE-TopicUniq+Synonyms; all of\nwhich are improvements over the core ROUGE measures.", "published": "2018-03-05 21:35:04", "link": "http://arxiv.org/abs/1803.01937v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Linear networks based speaker adaptation for speech synthesis", "abstract": "Speaker adaptation methods aim to create fair quality synthesis speech voice\nfont for target speakers while only limited resources available. Recently, as\ndeep neural networks based statistical parametric speech synthesis (SPSS)\nmethods become dominant in SPSS TTS back-end modeling, speaker adaptation under\nthe neural network based SPSS framework has also became an important task. In\nthis paper, linear networks (LN) is inserted in multiple neural network layers\nand fine-tuned together with output layer for best speaker adaptation\nperformance. When adaptation data is extremely small, the low-rank plus\ndiagonal(LRPD) decomposition for LN is employed to make the adapted voice more\nstable. Speaker adaptation experiments are conducted under a range of\nadaptation utterances numbers. Moreover, speaker adaptation from 1) female to\nfemale, 2) male to female and 3) female to male are investigated. Objective\nmeasurement and subjective tests show that LN with LRPD decomposition performs\nmost stable when adaptation data is extremely limited, and our best speaker\nadaptation (SA) model with only 200 adaptation utterances achieves comparable\nquality with speaker dependent (SD) model trained with 1000 utterances, in both\nnaturalness and similarity to target speaker.", "published": "2018-03-05 05:35:34", "link": "http://arxiv.org/abs/1803.02445v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
