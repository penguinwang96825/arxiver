{"title": "Unraveling reported dreams with text analytics", "abstract": "We investigate what distinguishes reported dreams from other personal\nnarratives. The continuity hypothesis, stemming from psychological dream\nanalysis work, states that most dreams refer to a person's daily life and\npersonal concerns, similar to other personal narratives such as diary entries.\nDifferences between the two texts may reveal the linguistic markers of dream\ntext, which could be the basis for new dream analysis work and for the\nautomatic detection of dream descriptions. We used three text analytics\nmethods: text classification, topic modeling, and text coherence analysis, and\napplied these methods to a balanced set of texts representing dreams, diary\nentries, and other personal stories. We observed that dream texts could be\ndistinguished from other personal narratives nearly perfectly, mostly based on\nthe presence of uncertainty markers and descriptions of scenes. Important\nmarkers for non-dream narratives are specific time expressions and\nconversational expressions. Dream texts also exhibit a lower discourse\ncoherence than other personal narratives.", "published": "2016-12-12 13:08:55", "link": "http://arxiv.org/abs/1612.03659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From narrative descriptions to MedDRA: automagically encoding adverse\n  drug reactions", "abstract": "The collection of narrative spontaneous reports is an irreplaceable source\nfor the prompt detection of suspected adverse drug reactions (ADRs): qualified\ndomain experts manually revise a huge amount of narrative descriptions and then\nencode texts according to MedDRA standard terminology. The manual annotation of\nnarrative documents with medical terminology is a subtle and expensive task,\nsince the number of reports is growing up day-by-day. MagiCoder, a Natural\nLanguage Processing algorithm, is proposed for the automatic encoding of\nfree-text descriptions into MedDRA terms. MagiCoder procedure is efficient in\nterms of computational complexity (in particular, it is linear in the size of\nthe narrative input and the terminology). We tested it on a large dataset of\nabout 4500 manually revised reports, by performing an automated comparison\nbetween human and MagiCoder revisions. For the current base version of\nMagiCoder, we measured: on short descriptions, an average recall of $86\\%$ and\nan average precision of $88\\%$; on medium-long descriptions (up to 255\ncharacters), an average recall of $64\\%$ and an average precision of $63\\%$.\nFrom a practical point of view, MagiCoder reduces the time required for\nencoding ADR reports. Pharmacologists have simply to review and validate the\nMagiCoder terms proposed by the application, instead of choosing the right\nterms among the 70K low level terms of MedDRA. Such improvement in the\nefficiency of pharmacologists' work has a relevant impact also on the quality\nof the subsequent data analysis. We developed MagiCoder for the Italian\npharmacovigilance language. However, our proposal is based on a general\napproach, not depending on the considered language nor the term dictionary.", "published": "2016-12-12 16:14:02", "link": "http://arxiv.org/abs/1612.03762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation by Minimising the Bayes-risk with Respect to\n  Syntactic Translation Lattices", "abstract": "We present a novel scheme to combine neural machine translation (NMT) with\ntraditional statistical machine translation (SMT). Our approach borrows ideas\nfrom linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is\ncombined with the Bayes-risk of the translation according the SMT lattice. This\nmakes our approach much more flexible than $n$-best list or lattice rescoring\nas the neural decoder is not restricted to the SMT search space. We show an\nefficient and simple way to integrate risk estimation into the NMT decoder\nwhich is suitable for word-level as well as subword-unit-level NMT. We test our\nmethod on English-German and Japanese-English and report significant gains over\nlattice rescoring on several data sets for both single and ensembled NMT. The\nMBR decoder produces entirely new hypotheses far beyond simply rescoring the\nSMT search space or fixing UNKs in the NMT output.", "published": "2016-12-12 17:12:44", "link": "http://arxiv.org/abs/1612.03791v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is\nequipped with a dynamic long-term memory which allows it to maintain and update\na representation of the state of the world as it receives new data. For\nlanguage understanding tasks, it can reason on-the-fly as it reads text, not\njust when it is required to answer a question or respond as is the case for a\nMemory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or\nDifferentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed\nsize memory and can learn to perform location and content-based read and write\noperations. However, unlike those models it has a simple parallel architecture\nin which several memory locations can be updated simultaneously. The EntNet\nsets a new state-of-the-art on the bAbI tasks, and is the first method to solve\nall the tasks in the 10k training examples setting. We also demonstrate that it\ncan solve a reasoning task which requires a large number of supporting facts,\nwhich other methods are not able to solve, and can generalize past its training\nhorizon. It can also be practically used on large scale datasets such as\nChildren's Book Test, where it obtains competitive performance, reading the\nstory in a single pass.", "published": "2016-12-12 23:29:40", "link": "http://arxiv.org/abs/1612.03969v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "abstract": "Machine learning about language can be improved by supplying it with specific\nknowledge and sources of external information. We present here a new version of\nthe linked open data resource ConceptNet that is particularly well suited to be\nused with modern NLP techniques such as word embeddings.\n  ConceptNet is a knowledge graph that connects words and phrases of natural\nlanguage with labeled edges. Its knowledge is collected from many sources that\ninclude expert-created resources, crowd-sourcing, and games with a purpose. It\nis designed to represent the general knowledge involved in understanding\nlanguage, improving natural language applications by allowing the application\nto better understand the meanings behind the words people use.\n  When ConceptNet is combined with word embeddings acquired from distributional\nsemantics (such as word2vec), it provides applications with understanding that\nthey would not acquire from distributional semantics alone, nor from narrower\nresources such as WordNet or DBPedia. We demonstrate this with state-of-the-art\nresults on intrinsic evaluations of word relatedness that translate into\nimprovements on applications of word vectors, including solving SAT-style\nanalogies.", "published": "2016-12-12 23:54:52", "link": "http://arxiv.org/abs/1612.03975v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reading Comprehension using Entity-based Memory Network", "abstract": "This paper introduces a novel neural network model for question answering,\nthe \\emph{entity-based memory network}. It enhances neural networks' ability of\nrepresenting and calculating information over a long period by keeping records\nof entities contained in text. The core component is a memory pool which\ncomprises entities' states. These entities' states are continuously updated\naccording to the input text. Questions with regard to the input text are used\nto search the memory pool for related entities and answers are further\npredicted based on the states of retrieved entities. Compared with previous\nmemory network models, the proposed model is capable of handling fine-grained\ninformation and more sophisticated relations based on entities. We formulated\nseveral different tasks as question answering problems and tested the proposed\nmodel. Experiments reported satisfying results.", "published": "2016-12-12 06:19:32", "link": "http://arxiv.org/abs/1612.03551v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Search Personalization with Embeddings", "abstract": "Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user's\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.", "published": "2016-12-12 10:27:31", "link": "http://arxiv.org/abs/1612.03597v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "VIBIKNet: Visual Bidirectional Kernelized Network for Visual Question\n  Answering", "abstract": "In this paper, we address the problem of visual question answering by\nproposing a novel model, called VIBIKNet. Our model is based on integrating\nKernelized Convolutional Neural Networks and Long-Short Term Memory units to\ngenerate an answer given a question about an image. We prove that VIBIKNet is\nan optimal trade-off between accuracy and computational load, in terms of\nmemory and time consumption. We validate our method on the VQA challenge\ndataset and compare it to the top performing methods in order to illustrate its\nperformance and speed.", "published": "2016-12-12 11:41:46", "link": "http://arxiv.org/abs/1612.03628v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text\nclassification, such that the full model fits in a limited amount of memory.\nAfter considering different solutions inspired by the hashing literature, we\npropose a method built upon product quantization to store word embeddings.\nWhile the original technique leads to a loss in accuracy, we adapt this method\nto circumvent quantization artefacts. Our experiments carried out on several\nbenchmarks show that our approach typically requires two orders of magnitude\nless memory than fastText while being only slightly inferior with respect to\naccuracy. As a result, it outperforms the state of the art by a good margin in\nterms of the compromise between memory usage and accuracy.", "published": "2016-12-12 12:51:03", "link": "http://arxiv.org/abs/1612.03651v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-aware Sentiment Word Identification: sentiword2vec", "abstract": "Traditional sentiment analysis often uses sentiment dictionary to extract\nsentiment information in text and classify documents. However, emerging\ninformal words and phrases in user generated content call for analysis aware to\nthe context. Usually, they have special meanings in a particular context.\nBecause of its great performance in representing inter-word relation, we use\nsentiment word vectors to identify the special words. Based on the distributed\nlanguage model word2vec, in this paper we represent a novel method about\nsentiment representation of word under particular context, to be detailed, to\nidentify the words with abnormal sentiment polarity in long answers. Result\nshows the improved model shows better performance in representing the words\nwith special meaning, while keep doing well in representing special idiomatic\npattern. Finally, we will discuss the meaning of vectors representing in the\nfield of sentiment, which may be different from general object-based\nconditions.", "published": "2016-12-12 16:25:08", "link": "http://arxiv.org/abs/1612.03769v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Active Learning for Dialogue Generation", "abstract": "We propose an online, end-to-end, neural generative conversational model for\nopen-domain dialogue. It is trained using a unique combination of offline\ntwo-phase supervised learning and online human-in-the-loop active learning.\nWhile most existing research proposes offline supervision or hand-crafted\nreward functions for online reinforcement, we devise a novel interactive\nlearning mechanism based on hamming-diverse beam search for response generation\nand one-character user-feedback at each step. Experiments show that our model\ninherently promotes the generation of semantically relevant and interesting\nresponses, and can be used to train agents with customized personas, moods and\nconversational styles.", "published": "2016-12-12 21:19:51", "link": "http://arxiv.org/abs/1612.03929v5", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
