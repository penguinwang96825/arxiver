{"title": "Keyphrase Generation with Correlation Constraints", "abstract": "In this paper, we study automatic keyphrase generation. Although conventional\napproaches to this task show promising results, they neglect correlation among\nkeyphrases, resulting in duplication and coverage issues. To solve these\nproblems, we propose a new sequence-to-sequence architecture for keyphrase\ngeneration named CorrRNN, which captures correlation among multiple keyphrases\nin two ways. First, we employ a coverage vector to indicate whether the word in\nthe source document has been summarized by previous phrases to improve the\ncoverage for keyphrases. Second, preceding phrases are taken into account to\neliminate duplicate phrases and improve result coherence. Experiment results\nshow that our model significantly outperforms the state-of-the-art method on\nbenchmark datasets in terms of both accuracy and diversity.", "published": "2018-08-22 02:06:08", "link": "http://arxiv.org/abs/1808.07185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Characterwise Windowed Approach to Hebrew Morphological Segmentation", "abstract": "This paper presents a novel approach to the segmentation of orthographic word\nforms in contemporary Hebrew, focusing purely on splitting without carrying out\nmorphological analysis or disambiguation. Casting the analysis task as\ncharacter-wise binary classification and using adjacent character and\nword-based lexicon-lookup features, this approach achieves over 98% accuracy on\nthe benchmark SPMRL shared task data for Hebrew, and 97% accuracy on a new out\nof domain Wikipedia dataset, an improvement of ~4% and 5% over previous state\nof the art performance.", "published": "2018-08-22 04:15:38", "link": "http://arxiv.org/abs/1808.07214v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Gender Bias in Abusive Language Detection", "abstract": "Abusive language detection models tend to have a problem of being biased\ntoward identity words of a certain group of people because of imbalanced\ntraining datasets. For example, \"You are a good woman\" was considered \"sexist\"\nwhen trained on an existing dataset. Such model bias is an obstacle for models\nto be robust enough for practical use. In this work, we measure gender biases\non models trained with different abusive language datasets, while analyzing the\neffect of different pre-trained word embeddings and model architectures. We\nalso experiment with three bias mitigation methods: (1) debiased word\nembeddings, (2) gender swap data augmentation, and (3) fine-tuning with a\nlarger corpus. These methods can effectively reduce gender bias by 90-98% and\ncan be extended to correct model bias in other scenarios.", "published": "2018-08-22 06:00:56", "link": "http://arxiv.org/abs/1808.07231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Good Representations of Emotions for Text Classification", "abstract": "It is important for machines to interpret human emotions properly for better\nhuman-machine communications, as emotion is an essential part of human-to-human\ncommunications. One aspect of emotion is reflected in the language we use. How\nto represent emotions in texts is a challenge in natural language processing\n(NLP). Although continuous vector representations like word2vec have become the\nnew norm for NLP problems, their limitations are that they do not take emotions\ninto consideration and can unintentionally contain bias toward certain\nidentities like different genders.\n  This thesis focuses on improving existing representations in both word and\nsentence levels by explicitly taking emotions inside text and model bias into\naccount in their training process. Our improved representations can help to\nbuild more robust machine learning models for affect-related text\nclassification like sentiment/emotion analysis and abusive language detection.\n  We first propose representations called emotional word vectors (EVEC), which\nis learned from a convolutional neural network model with an emotion-labeled\ncorpus, which is constructed using hashtags. Secondly, we extend to learning\nsentence-level representations with a huge corpus of texts with the pseudo task\nof recognizing emojis. Our results show that, with the representations trained\nfrom millions of tweets with weakly supervised labels such as hashtags and\nemojis, we can solve sentiment/emotion analysis tasks more effectively.\n  Lastly, as examples of model bias in representations of existing approaches,\nwe explore a specific problem of automatic detection of abusive language. We\naddress the issue of gender bias in various neural network models by conducting\nexperiments to measure and reduce those biases in the representations in order\nto build more robust classification models.", "published": "2018-08-22 06:07:24", "link": "http://arxiv.org/abs/1808.07235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Matching Models with Hierarchical Contextualized\n  Representations for Multi-turn Response Selection", "abstract": "In this paper, we study context-response matching with pre-trained\ncontextualized representations for multi-turn response selection in\nretrieval-based chatbots. Existing models, such as Cove and ELMo, are trained\nwith limited context (often a single sentence or paragraph), and may not work\nwell on multi-turn conversations, due to the hierarchical nature, informal\nlanguage, and domain-specific words. To address the challenges, we propose\npre-training hierarchical contextualized representations, including contextual\nword-level and sentence-level representations, by learning a dialogue\ngeneration model from large-scale conversations with a hierarchical\nencoder-decoder architecture. Then the two levels of representations are\nblended into the input and output layer of a matching model respectively.\nExperimental results on two benchmark conversation datasets indicate that the\nproposed hierarchical contextualized representations can bring significantly\nand consistently improvement to existing matching models for response\nselection.", "published": "2018-08-22 06:58:01", "link": "http://arxiv.org/abs/1808.07244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem\n  Solvers", "abstract": "Solving mathematical word problems (MWPs) automatically is challenging,\nprimarily due to the semantic gap between human-readable words and\nmachine-understandable logics. Despite the long history dated back to the1960s,\nMWPs have regained intensive attention in the past few years with the\nadvancement of Artificial Intelligence (AI). Solving MWPs successfully is\nconsidered as a milestone towards general AI. Many systems have claimed\npromising results in self-crafted and small-scale datasets. However, when\napplied on large and diverse datasets, none of the proposed methods in the\nliterature achieves high precision, revealing that current MWP solvers still\nhave much room for improvement. This motivated us to present a comprehensive\nsurvey to deliver a clear and complete picture of automatic math problem\nsolvers. In this survey, we emphasize on algebraic word problems, summarize\ntheir extracted features and proposed techniques to bridge the semantic gap and\ncompare their performance in the publicly accessible datasets. We also cover\nautomatic solvers for other types of math problems such as geometric problems\nthat require the understanding of diagrams. Finally, we identify several\nemerging research directions for the readers with interests in MWPs.", "published": "2018-08-22 09:18:38", "link": "http://arxiv.org/abs/1808.07290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Sentiment Memories for Sentiment Modification without Parallel\n  Data", "abstract": "The task of sentiment modification requires reversing the sentiment of the\ninput and preserving the sentiment-independent content. However, aligned\nsentences with the same content but different sentiments are usually\nunavailable. Due to the lack of such parallel data, it is hard to extract\nsentiment independent content and reverse the sentiment in an unsupervised way.\nPrevious work usually can not reconcile sentiment transformation and content\npreservation. In this paper, motivated by the fact the non-emotional context\n(e.g., \"staff\") provides strong cues for the occurrence of emotional words\n(e.g., \"friendly\"), we propose a novel method that automatically extracts\nappropriate sentiment information from learned sentiment memories according to\nspecific context. Experiments show that our method substantially improves the\ncontent preservation degree and achieves the state-of-the-art performance.", "published": "2018-08-22 10:57:04", "link": "http://arxiv.org/abs/1808.07311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expansional Retrofitting for Word Vector Enrichment", "abstract": "Retrofitting techniques, which inject external resources into word\nrepresentations, have compensated the weakness of distributed representations\nin semantic and relational knowledge between words. Implicitly retrofitting\nword vectors by expansional technique outperforms retrofitting in word\nsimilarity tasks with word vector generalization. In this paper, we propose\nunsupervised extrofitting: expansional retrofitting (extrofitting) without\nexternal semantic lexicons. We also propose deep extrofitting: in-depth\nstacking of extrofitting and further combinations of extrofitting with\nretrofitting. When experimenting with GloVe, we show that our methods\noutperform the previous methods on most of word similarity tasks while\nrequiring only synonyms as an external resource. Lastly, we show the effect of\nword vector enrichment on text classification task, as a downstream task.", "published": "2018-08-22 12:49:46", "link": "http://arxiv.org/abs/1808.07337v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Named Entity Recognition from Subword Units", "abstract": "Named entity recognition (NER) is a vital task in spoken language\nunderstanding, which aims to identify mentions of named entities in text e.g.,\nfrom transcribed speech. Existing neural models for NER rely mostly on\ndedicated word-level representations, which suffer from two main shortcomings.\nFirst, the vocabulary size is large, yielding large memory requirements and\ntraining time. Second, these models are not able to learn morphological or\nphonological representations. To remedy the above shortcomings, we adopt a\nneural solution based on bidirectional LSTMs and conditional random fields,\nwhere we rely on subword units, namely characters, phonemes, and bytes. For\neach word in an utterance, our model learns a representation from each of the\nsubword units. We conducted experiments in a real-world large-scale setting for\nthe use case of a voice-controlled device covering four languages with up to\n5.5M utterances per language. Our experiments show that (1) with increasing\ntraining data, performance of models trained solely on subword units becomes\ncloser to that of models with dedicated word-level embeddings (91.35 vs 93.92\nF1 for English), while using a much smaller vocabulary size (332 vs 74K), (2)\nsubword units enhance models with dedicated word-level embeddings, and (3)\ncombining different subword units improves performance.", "published": "2018-08-22 13:56:51", "link": "http://arxiv.org/abs/1808.07364v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine\n  Translation", "abstract": "In this work, we examine methods for data augmentation for text-based tasks\nsuch as neural machine translation (NMT). We formulate the design of a data\naugmentation policy with desirable properties as an optimization problem, and\nderive a generic analytic solution. This solution not only subsumes some\nexisting augmentation schemes, but also leads to an extremely simple data\naugmentation strategy for NMT: randomly replacing words in both the source\nsentence and the target sentence with other random words from their\ncorresponding vocabularies. We name this method SwitchOut. Experiments on three\ntranslation datasets of different scales show that SwitchOut yields consistent\nimprovements of about 0.5 BLEU, achieving better or comparable performances to\nstrong alternatives such as word dropout (Sennrich et al., 2016a). Code to\nimplement this method is included in the appendix.", "published": "2018-08-22 18:26:43", "link": "http://arxiv.org/abs/1808.07512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sarcasm Analysis using Conversation Context", "abstract": "Computational models for sarcasm detection have often relied on the content\nof utterances in isolation. However, the speaker's sarcastic intent is not\nalways apparent without additional context. Focusing on social media\ndiscussions, we investigate three issues: (1) does modeling conversation\ncontext help in sarcasm detection; (2) can we identify what part of\nconversation context triggered the sarcastic reply; and (3) given a sarcastic\npost that contains multiple sentences, can we identify the specific sentence\nthat is sarcastic. To address the first issue, we investigate several types of\nLong Short-Term Memory (LSTM) networks that can model both the conversation\ncontext and the current turn. We show that LSTM networks with sentence-level\nattention on context and current turn, as well as the conditional LSTM network\n(Rocktaschel et al. 2016), outperform the LSTM model that reads only the\ncurrent turn. As conversation context, we consider the prior turn, the\nsucceeding turn or both. Our computational models are tested on two types of\nsocial media platforms: Twitter and discussion forums. We discuss several\ndifferences between these datasets ranging from their size to the nature of the\ngold-label annotations. To address the last two issues, we present a\nqualitative analysis of attention weights produced by the LSTM models (with\nattention) and discuss the results compared with human performance on the two\ntasks.", "published": "2018-08-22 19:21:04", "link": "http://arxiv.org/abs/1808.07531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciding the status of controversial phonemes using frequency\n  distributions; an application to semiconsonants in Spanish", "abstract": "Exploiting the fact that natural languages are complex systems, the present\nexploratory article proposes a direct method based on frequency distributions\nthat may be useful when making a decision on the status of problematic\nphonemes, an open problem in linguistics. The main notion is that natural\nlanguages, which can be considered from a complex outlook as information\nprocessing machines, and which somehow manage to set appropriate levels of\nredundancy, already \"made the choice\" whether a linguistic unit is a phoneme or\nnot, and this would be reflected in a greater smoothness in a frequency versus\nrank graph. For the particular case we chose to study, we conclude that it is\nreasonable to consider the Spanish semiconsonant /w/ as a separate phoneme from\nits vowel counterpart /u/, on the one hand, and possibly also the semiconsonant\n/j/ as a separate phoneme from its vowel counterpart /i/, on the other. As\nlanguage has been so central a topic in the study of complexity, this\ndiscussion grants us, in addition, an opportunity to gain insight into emerging\nproperties in the broader complex systems debate.", "published": "2018-08-22 00:32:23", "link": "http://arxiv.org/abs/1808.07166v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Identifying High-Quality Chinese News Comments Based on Multi-Target\n  Text Matching Model", "abstract": "With the development of information technology, there is an explosive growth\nin the number of online comment concerning news, blogs and so on. The massive\ncomments are overloaded, and often contain some misleading and unwelcome\ninformation. Therefore, it is necessary to identify high-quality comments and\nfilter out low-quality comments. In this work, we introduce a novel task:\nhigh-quality comment identification (HQCI), which aims to automatically assess\nthe quality of online comments. First, we construct a news comment corpus,\nwhich consists of news, comments, and the corresponding quality label. Second,\nwe analyze the dataset, and find the quality of comments can be measured in\nthree aspects: informativeness, consistency, and novelty. Finally, we propose a\nnovel multi-target text matching model, which can measure three aspects by\nreferring to the news and surrounding comments. Experimental results show that\nour method can outperform various baselines by a large margin on the news\ndataset.", "published": "2018-08-22 02:33:15", "link": "http://arxiv.org/abs/1808.07191v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Attention-Gated Convolutional Neural Network for Sentence\n  Classification", "abstract": "The classification of sentences is very challenging, since sentences contain\nthe limited contextual information. In this paper, we proposed an\nAttention-Gated Convolutional Neural Network (AGCNN) for sentence\nclassification, which generates attention weights from the feature's context\nwindows of different sizes by using specialized convolution encoders. It makes\nfull use of limited contextual information to extract and enhance the influence\nof important features in predicting the sentence's category. Experimental\nresults demonstrated that our model can achieve up to 3.1% higher accuracy than\nstandard CNN models, and gain competitive results over the baselines on four\nout of the six tasks. Besides, we designed an activation function, namely,\nNatural Logarithm rescaled Rectified Linear Unit (NLReLU). Experiments showed\nthat NLReLU can outperform ReLU and is comparable to other well-known\nactivation functions on AGCNN.", "published": "2018-08-22 12:03:48", "link": "http://arxiv.org/abs/1808.07325v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning When to Concentrate or Divert Attention: Self-Adaptive\n  Attention Temperature for Neural Machine Translation", "abstract": "Most of the Neural Machine Translation (NMT) models are based on the\nsequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped\nwith the attention mechanism. However, the conventional attention mechanism\ntreats the decoding at each time step equally with the same matrix, which is\nproblematic since the softness of the attention for different types of words\n(e.g. content words and function words) should differ. Therefore, we propose a\nnew model with a mechanism called Self-Adaptive Control of Temperature (SACT)\nto control the softness of attention by means of an attention temperature.\nExperimental results on the Chinese-English translation and English-Vietnamese\ntranslation demonstrate that our model outperforms the baseline models, and the\nanalysis and the case study show that our model can attend to the most relevant\nelements in the source-side contexts and generate the translation of high\nquality.", "published": "2018-08-22 14:13:24", "link": "http://arxiv.org/abs/1808.07374v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial\n  Networks", "abstract": "Generative Adversarial Networks (GANs) have shown great capacity on image\ngeneration, in which a discriminative model guides the training of a generative\nmodel to construct images that resemble real images. Recently, GANs have been\nextended from generating images to generating sequences (e.g., poems, music and\ncodes). Existing GANs on sequence generation mainly focus on general sequences,\nwhich are grammar-free. In many real-world applications, however, we need to\ngenerate sequences in a formal language with the constraint of its\ncorresponding grammar. For example, to test the performance of a database, one\nmay want to generate a collection of SQL queries, which are not only similar to\nthe queries of real users, but also follow the SQL syntax of the target\ndatabase. Generating such sequences is highly challenging because both the\ngenerator and discriminator of GANs need to consider the structure of the\nsequences and the given grammar in the formal language. To address these\nissues, we study the problem of syntax-aware sequence generation with GANs, in\nwhich a collection of real sequences and a set of pre-defined grammatical rules\nare given to both discriminator and generator. We propose a novel GAN\nframework, namely TreeGAN, to incorporate a given Context-Free Grammar (CFG)\ninto the sequence generation process. In TreeGAN, the generator employs a\nrecurrent neural network (RNN) to construct a parse tree. Each generated parse\ntree can then be translated to a valid sequence of the given grammar. The\ndiscriminator uses a tree-structured RNN to distinguish the generated trees\nfrom real trees. We show that TreeGAN can generate sequences for any CFG and\nits generation fully conforms with the given syntax. Experiments on synthetic\nand real data sets demonstrated that TreeGAN significantly improves the quality\nof the sequence generation in context-free languages.", "published": "2018-08-22 22:32:34", "link": "http://arxiv.org/abs/1808.07582v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Latent Extractive Document Summarization", "abstract": "Extractive summarization models require sentence-level labels, which are\nusually created heuristically (e.g., with rule-based methods) given that most\nsummarization datasets only have document-summary pairs. Since these labels\nmight be suboptimal, we propose a latent variable extractive model where\nsentences are viewed as latent variables and sentences with activated variables\nare used to infer gold summaries. During training the loss comes\n\\emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset\nshow that our model improves over a strong extractive baseline trained on\nheuristically approximated labels and also performs competitively to several\nrecent models.", "published": "2018-08-22 02:18:40", "link": "http://arxiv.org/abs/1808.07187v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Neural Network for Extracting Knowledgeable Snippets and\n  Documents", "abstract": "In this study, we focus on extracting knowledgeable snippets and annotating\nknowledgeable documents from Web corpus, consisting of the documents from\nsocial media and We-media. Informally, knowledgeable snippets refer to the text\ndescribing concepts, properties of entities, or relations among entities, while\nknowledgeable documents are the ones with enough knowledgeable snippets. These\nknowledgeable snippets and documents could be helpful in multiple applications,\nsuch as knowledge base construction and knowledge-oriented service. Previous\nstudies extracted the knowledgeable snippets using the pattern-based method.\nHere, we propose the semantic-based method for this task. Specifically, a CNN\nbased model is developed to extract knowledgeable snippets and annotate\nknowledgeable documents simultaneously. Additionally, a \"low-level sharing,\nhigh-level splitting\" structure of CNN is designed to handle the documents from\ndifferent content domains. Compared with building multiple domain-specific\nCNNs, this joint model not only critically saves the training time, but also\nimproves the prediction accuracy visibly. The superiority of the proposed\nmethod is demonstrated in a real dataset from Wechat public platform.", "published": "2018-08-22 05:57:13", "link": "http://arxiv.org/abs/1808.07228v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dynamic Self-Attention : Computing Attention over Words Dynamically for\n  Sentence Embedding", "abstract": "In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention\nmechanism for sentence embedding. We design DSA by modifying dynamic routing in\ncapsule network (Sabouretal.,2017) for natural language processing. DSA attends\nto informative words with a dynamic weight vector. We achieve new\nstate-of-the-art results among sentence encoding methods in Stanford Natural\nLanguage Inference (SNLI) dataset with the least number of parameters, while\nshowing comparative results in Stanford Sentiment Treebank (SST) dataset.", "published": "2018-08-22 14:30:03", "link": "http://arxiv.org/abs/1808.07383v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Training Deeper Neural Machine Translation Models with Transparent\n  Attention", "abstract": "While current state-of-the-art NMT models, such as RNN seq2seq and\nTransformers, possess a large number of parameters, they are still shallow in\ncomparison to convolutional models used for both text and vision applications.\nIn this work we attempt to train significantly (2-3x) deeper Transformer and\nBi-RNN encoders for machine translation. We propose a simple modification to\nthe attention mechanism that eases the optimization of deeper models, and\nresults in consistent gains of 0.7-1.1 BLEU on the benchmark WMT'14\nEnglish-German and WMT'15 Czech-English tasks for both architectures.", "published": "2018-08-22 20:53:37", "link": "http://arxiv.org/abs/1808.07561v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
