{"title": "Deep contextualized word representations for detecting sarcasm and irony", "abstract": "Predicting context-dependent and non-literal utterances like sarcastic and\nironic expressions still remains a challenging task in NLP, as it goes beyond\nlinguistic patterns, encompassing common sense and shared knowledge as crucial\ncomponents. To capture complex morpho-syntactic features that can usually serve\nas indicators for irony or sarcasm across dynamic contexts, we propose a model\nthat uses character-level vector representations of words, based on ELMo. We\ntest our model on 7 different datasets derived from 3 different data sources,\nproviding state-of-the-art performance in 6 of them, and otherwise offering\ncompetitive results.", "published": "2018-09-26 03:54:22", "link": "http://arxiv.org/abs/1809.09795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling Teaches You More Syntax than Translation Does: Lessons\n  Learned Through Auxiliary Task Analysis", "abstract": "Recent work using auxiliary prediction task classifiers to investigate the\nproperties of LSTM representations has begun to shed light on why pretrained\nrepresentations, like ELMo (Peters et al., 2018) and CoVe (McCann et al.,\n2017), are so beneficial for neural language understanding models. We still,\nthough, do not yet have a clear understanding of how the choice of pretraining\nobjective affects the type of linguistic information that models learn. With\nthis in mind, we compare four objectives---language modeling, translation,\nskip-thought, and autoencoding---on their ability to induce syntactic and\npart-of-speech information. We make a fair comparison between the tasks by\nholding constant the quantity and genre of the training data, as well as the\nLSTM architecture. We find that representations from language models\nconsistently perform best on our syntactic auxiliary prediction tasks, even\nwhen trained on relatively small amounts of data. These results suggest that\nlanguage modeling may be the best data-rich pretraining task for transfer\nlearning applications requiring syntactic information. We also find that the\nrepresentations from randomly-initialized, frozen LSTMs perform strikingly well\non our syntactic auxiliary tasks, but this effect disappears when the amount of\ntraining data for the auxiliary tasks is reduced.", "published": "2018-09-26 14:58:59", "link": "http://arxiv.org/abs/1809.10040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Convolution over Pruned Dependency Trees Improves Relation\n  Extraction", "abstract": "Dependency trees help relation extraction models capture long-range relations\nbetween words. However, existing dependency-based models either neglect crucial\ninformation (e.g., negation) by pruning the dependency trees too aggressively,\nor are computationally inefficient because it is difficult to parallelize over\ndifferent tree structures. We propose an extension of graph convolutional\nnetworks that is tailored for relation extraction, which pools information over\narbitrary dependency structures efficiently in parallel. To incorporate\nrelevant information while maximally removing irrelevant content, we further\napply a novel pruning strategy to the input trees by keeping words immediately\naround the shortest path between the two entities among which a relation might\nhold. The resulting model achieves state-of-the-art performance on the\nlarge-scale TACRED dataset, outperforming existing sequence and\ndependency-based neural models. We also show through detailed analysis that\nthis model has complementary strengths to sequence models, and combining them\nfurther improves the state of the art.", "published": "2018-09-26 18:49:07", "link": "http://arxiv.org/abs/1809.10185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No One is Perfect: Analysing the Performance of Question Answering\n  Components over the DBpedia Knowledge Graph", "abstract": "Question answering (QA) over knowledge graphs has gained significant momentum\nover the past five years due to the increasing availability of large knowledge\ngraphs and the rising importance of question answering for user interaction.\nDBpedia has been the most prominently used knowledge graph in this setting and\nmost approaches currently use a pipeline of processing steps connecting a\nsequence of components. In this article, we analyse and micro evaluate the\nbehaviour of 29 available QA components for DBpedia knowledge graph that were\nreleased by the research community since 2010. As a result, we provide a\nperspective on collective failure cases, suggest characteristics of QA\ncomponents that prevent them from performing better and provide future\nchallenges and research directions for the field.", "published": "2018-09-26 15:01:28", "link": "http://arxiv.org/abs/1809.10044v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Semantic Sentence Embeddings for Paraphrasing and Text Summarization", "abstract": "This paper introduces a sentence to vector encoding framework suitable for\nadvanced natural language processing. Our latent representation is shown to\nencode sentences with common semantic information with similar vector\nrepresentations. The vector representation is extracted from an encoder-decoder\nmodel which is trained on sentence paraphrase pairs. We demonstrate the\napplication of the sentence representations for two different tasks -- sentence\nparaphrasing and paragraph summarization, making it attractive for commonly\nused recurrent frameworks that process text. Experimental results help gain\ninsight how vector representations are suitable for advanced language\nembedding.", "published": "2018-09-26 23:38:19", "link": "http://arxiv.org/abs/1809.10267v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wronging a Right: Generating Better Errors to Improve Grammatical Error\n  Detection", "abstract": "Grammatical error correction, like other machine learning tasks, greatly\nbenefits from large quantities of high quality training data, which is\ntypically expensive to produce. While writing a program to automatically\ngenerate realistic grammatical errors would be difficult, one could learn the\ndistribution of naturallyoccurring errors and attempt to introduce them into\nother datasets. Initial work on inducing errors in this way using statistical\nmachine translation has shown promise; we investigate cheaply constructing\nsynthetic samples, given a small corpus of human-annotated data, using an\noff-the-rack attentive sequence-to-sequence model and a straight-forward\npost-processing procedure. Our approach yields error-filled artificial data\nthat helps a vanilla bi-directional LSTM to outperform the previous state of\nthe art at grammatical error detection, and a previously introduced model to\ngain further improvements of over 5% $F_{0.5}$ score. When attempting to\ndetermine if a given sentence is synthetic, a human annotator at best achieves\n39.39 $F_1$ score, indicating that our model generates mostly human-like\ninstances.", "published": "2018-09-26 14:25:40", "link": "http://arxiv.org/abs/1810.00668v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Universal Network Representation for Heterogeneous Information Networks", "abstract": "Network representation aims to represent the nodes in a network as continuous\nand compact vectors, and has attracted much attention in recent years due to\nits ability to capture complex structure relationships inside networks.\nHowever, existing network representation methods are commonly designed for\nhomogeneous information networks where all the nodes (entities) of a network\nare of the same type, e.g., papers in a citation network. In this paper, we\npropose a universal network representation approach (UNRA), that represents\ndifferent types of nodes in heterogeneous information networks in a continuous\nand common vector space. The UNRA is built on our latest mutually updated\nneural language module, which simultaneously captures inter-relationship among\nhomogeneous nodes and node-content correlation. Relationships between different\ntypes of nodes are also assembled and learned in a unified framework.\nExperiments validate that the UNRA achieves outstanding performance, compared\nto six other state-of-the-art algorithms, in node representation, node\nclassification, and network visualization. In node classification, the UNRA\nachieves a 3\\% to 132\\% performance improvement in terms of accuracy.", "published": "2018-09-26 13:43:01", "link": "http://arxiv.org/abs/1811.12157v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Error Reduction Network for DBLSTM-based Voice Conversion", "abstract": "So far, many of the deep learning approaches for voice conversion produce\ngood quality speech by using a large amount of training data. This paper\npresents a Deep Bidirectional Long Short-Term Memory (DBLSTM) based voice\nconversion framework that can work with a limited amount of training data. We\npropose to implement a DBLSTM based average model that is trained with data\nfrom many speakers. Then, we propose to perform adaptation with a limited\namount of target data. Last but not least, we propose an error reduction\nnetwork that can improve the voice conversion quality even further. The\nproposed framework is motivated by three observations. Firstly, DBLSTM can\nachieve a remarkable voice conversion by considering the long-term dependencies\nof the speech utterance. Secondly, DBLSTM based average model can be easily\nadapted with a small amount of data, to achieve a speech that sounds closer to\nthe target. Thirdly, an error reduction network can be trained with a small\namount of training data, and can improve the conversion quality effectively.\nThe experiments show that the proposed voice conversion framework is flexible\nto work with limited training data and outperforms the traditional frameworks\nin both objective and subjective evaluations.", "published": "2018-09-26 07:43:51", "link": "http://arxiv.org/abs/1809.09841v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An extensible cluster-graph taxonomy for open set sound scene analysis", "abstract": "We present a new extensible and divisible taxonomy for open set sound scene\nanalysis. This new model allows complex scene analysis with tangible\ndescriptors and perception labels. Its novel structure is a cluster graph such\nthat each cluster (or subset) can stand alone for targeted analyses such as\noffice sound event detection, whilst maintaining integrity over the whole graph\n(superset) of labels. The key design benefit is its extensibility as new labels\nare needed during new data capture. Furthermore, datasets which use the same\ntaxonomy are easily augmented, saving future data collection effort. We balance\nthe details needed for complex scene analysis with avoiding 'the taxonomy of\neverything' with our framework to ensure no duplicity in the superset of labels\nand demonstrate this with DCASE challenge classifications.", "published": "2018-09-26 15:04:17", "link": "http://arxiv.org/abs/1809.10047v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "S-SPADE Done Right: Detailed Study of the Sparse Audio Declipper\n  Algorithms", "abstract": "This technical report shows and discusses in detail how Sparse Audio\nDeclipper (SPADE) algorithms are derived from the signal model using the ADMM\napproach. The analysis version (A-SPADE) of Kiti\\'c et. al. (LVA/ICA 2015) is\nderived and justified. The synthesis version (S-SPADE) of the same research\nteam is shown to solve a different optimization task than intended. This issue\nis corrected in this report, leading to the new S-SPADE algorithm which is in\nline to A-SPADE.", "published": "2018-09-26 08:38:51", "link": "http://arxiv.org/abs/1809.09847v4", "categories": ["math.OC", "eess.AS", "eess.SP"], "primary_category": "math.OC"}
