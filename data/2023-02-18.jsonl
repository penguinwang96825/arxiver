{"title": "How Good Are GPT Models at Machine Translation? A Comprehensive\n  Evaluation", "abstract": "Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.", "published": "2023-02-18 02:11:36", "link": "http://arxiv.org/abs/2302.09210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLN-Trans: Translator for the Vision and Language Navigation Agent", "abstract": "Language understanding is essential for the navigation agent to follow\ninstructions. We observe two kinds of issues in the instructions that can make\nthe navigation task challenging: 1. The mentioned landmarks are not\nrecognizable by the navigation agent due to the different vision abilities of\nthe instructor and the modeled agent. 2. The mentioned landmarks are applicable\nto multiple targets, thus not distinctive for selecting the target among the\ncandidate viewpoints. To deal with these issues, we design a translator module\nfor the navigation agent to convert the original instructions into\neasy-to-follow sub-instruction representations at each step. The translator\nneeds to focus on the recognizable and distinctive landmarks based on the\nagent's visual abilities and the observed visual environment. To achieve this\ngoal, we create a new synthetic sub-instruction dataset and design specific\ntasks to train the translator and the navigation agent. We evaluate our\napproach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last)\ndatasets and achieve state-of-the-art results on multiple benchmarks.", "published": "2023-02-18 04:19:51", "link": "http://arxiv.org/abs/2302.09230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bag of Tricks for Effective Language Model Pretraining and Downstream\n  Adaptation: A Case Study on GLUE", "abstract": "This technical report briefly describes our JDExplore d-team's submission\nVega v1 on the General Language Understanding Evaluation (GLUE) leaderboard,\nwhere GLUE is a collection of nine natural language understanding tasks,\nincluding question answering, linguistic acceptability, sentiment analysis,\ntext similarity, paraphrase detection, and natural language inference. [Method]\nWe investigate several effective strategies and choose their best combination\nsetting as the training recipes. As for model structure, we employ the vanilla\nTransformer with disentangled attention as the basic block encoder. For\nself-supervised training, we employ the representative denoising objective\n(i.e., replaced token detection) in phase 1 and combine the contrastive\nobjective (i.e., sentence embedding contrastive learning) with it in phase 2.\nDuring fine-tuning, several advanced techniques such as transductive\nfine-tuning, self-calibrated fine-tuning, and adversarial fine-tuning are\nadopted. [Results] According to our submission record (Jan. 2022), with our\noptimized pretraining and fine-tuning strategies, our 1.3 billion model sets\nnew state-of-the-art on 4/9 tasks, achieving the best average score of 91.3.\nEncouragingly, our Vega v1 is the first to exceed powerful human performance on\nthe two challenging tasks, i.e., SST-2 and WNLI. We believe our empirically\nsuccessful recipe with a bag of tricks could shed new light on developing\nefficient discriminative large language models.", "published": "2023-02-18 09:26:35", "link": "http://arxiv.org/abs/2302.09268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Out-Of-Distribution Generalization Capability of Language\n  Models: Counterfactually-Augmented Data is not Enough", "abstract": "Counterfactually-Augmented Data (CAD) has the potential to improve language\nmodels' Out-Of-Distribution (OOD) generalization capability, as CAD induces\nlanguage models to exploit causal features and exclude spurious correlations.\nHowever, the empirical results of OOD generalization on CAD are not as\nefficient as expected. In this paper, we attribute the inefficiency to Myopia\nPhenomenon caused by CAD: language models only focus on causal features that\nare edited in the augmentation and exclude other non-edited causal features. As\na result, the potential of CAD is not fully exploited. Based on the structural\nproperties of CAD, we design two additional constraints to help language models\nextract more complete causal features contained in CAD, thus improving the OOD\ngeneralization capability. We evaluate our method on two tasks: Sentiment\nAnalysis and Natural Language Inference, and the experimental results\ndemonstrate that our method could unlock CAD's potential and improve language\nmodels' OOD generalization capability.", "published": "2023-02-18 14:39:03", "link": "http://arxiv.org/abs/2302.09345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT is not The Count: Learning to Match Mathematical Statements with\n  Proofs", "abstract": "We introduce a task consisting in matching a proof to a given mathematical\nstatement. The task fits well within current research on Mathematical\nInformation Retrieval and, more generally, mathematical article analysis\n(Mathematical Sciences, 2014). We present a dataset for the task (the MATcH\ndataset) consisting of over 180k statement-proof pairs extracted from modern\nmathematical research articles. We find this dataset highly representative of\nour task, as it consists of relatively new findings useful to mathematicians.\nWe propose a bilinear similarity model and two decoding methods to match\nstatements to proofs effectively. While the first decoding method matches a\nproof to a statement without being aware of other statements or proofs, the\nsecond method treats the task as a global matching problem. Through a symbol\nreplacement procedure, we analyze the \"insights\" that pre-trained language\nmodels have in such mathematical article analysis and show that while these\nmodels perform well on this task with the best performing mean reciprocal rank\nof 73.7, they follow a relatively shallow symbolic analysis and matching to\nachieve that performance.", "published": "2023-02-18 14:48:20", "link": "http://arxiv.org/abs/2302.09350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero and Few-Shot Localization of Task-Oriented Dialogue Agents with a\n  Distilled Representation", "abstract": "Task-oriented Dialogue (ToD) agents are mostly limited to a few widely-spoken\nlanguages, mainly due to the high cost of acquiring training data for each\nlanguage. Existing low-cost approaches that rely on cross-lingual embeddings or\nnaive machine translation sacrifice a lot of accuracy for data efficiency, and\nlargely fail in creating a usable dialogue agent. We propose automatic methods\nthat use ToD training data in a source language to build a high-quality\nfunctioning dialogue agent in another target language that has no training data\n(i.e. zero-shot) or a small training set (i.e. few-shot). Unlike most prior\nwork in cross-lingual ToD that only focuses on Dialogue State Tracking (DST),\nwe build an end-to-end agent.\n  We show that our approach closes the accuracy gap between few-shot and\nexisting full-shot methods for ToD agents. We achieve this by (1) improving the\ndialogue data representation, (2) improving entity-aware machine translation,\nand (3) automatic filtering of noisy translations.\n  We evaluate our approach on the recent bilingual dialogue dataset BiToD. In\nChinese to English transfer, in the zero-shot setting, our method achieves\n46.7% and 22.0% in Task Success Rate (TSR) and Dialogue Success Rate (DSR)\nrespectively. In the few-shot setting where 10% of the data in the target\nlanguage is used, we improve the state-of-the-art by 15.2% and 14.0%, coming\nwithin 5% of full-shot training.", "published": "2023-02-18 21:30:36", "link": "http://arxiv.org/abs/2302.09424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BBT-Fin: Comprehensive Construction of Chinese Financial Domain\n  Pre-trained Language Model, Corpus and Benchmark", "abstract": "To advance Chinese financial natural language processing (NLP), we introduce\nBBT-FinT5, a new Chinese financial pre-training language model based on the T5\nmodel. To support this effort, we have built BBT-FinCorpus, a large-scale\nfinancial corpus with approximately 300GB of raw text from four different\nsources. In general domain NLP, comprehensive benchmarks like GLUE and\nSuperGLUE have driven significant advancements in language model pre-training\nby enabling head-to-head comparisons among models. Drawing inspiration from\nthese benchmarks, we propose BBT-CFLEB, a Chinese Financial Language\nunderstanding and generation Evaluation Benchmark, which includes six datasets\ncovering both understanding and generation tasks. Our aim is to facilitate\nresearch in the development of NLP within the Chinese financial domain. Our\nmodel, corpus and benchmark are released at\nhttps://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the\nBig Bang Transformer (BBT), a large-scale pre-trained language model project.", "published": "2023-02-18 22:20:37", "link": "http://arxiv.org/abs/2302.09432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RETVec: Resilient and Efficient Text Vectorizer", "abstract": "This paper describes RETVec, an efficient, resilient, and multilingual text\nvectorizer designed for neural-based text processing. RETVec combines a novel\ncharacter encoding with an optional small embedding model to embed words into a\n256-dimensional vector space. The RETVec embedding model is pre-trained using\npair-wise metric learning to be robust against typos and character-level\nadversarial attacks. In this paper, we evaluate and compare RETVec to\nstate-of-the-art vectorizers and word embeddings on popular model architectures\nand datasets. These comparisons demonstrate that RETVec leads to competitive,\nmultilingual models that are significantly more resilient to typos and\nadversarial text attacks. RETVec is available under the Apache 2 license at\nhttps://github.com/google-research/retvec.", "published": "2023-02-18 02:06:52", "link": "http://arxiv.org/abs/2302.09207v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Prompt Generation for Semi-supervised Learning with Language\n  Models", "abstract": "Prompt-based learning methods in semi-supervised learning (SSL) settings have\nbeen shown to be effective on multiple natural language understanding (NLU)\ndatasets and tasks in the literature. However, manually designing multiple\nprompts and verbalizers requires domain knowledge and human effort, making it\ndifficult and expensive to scale across different datasets. In this paper, we\npropose two methods to automatically design multiple prompts and integrate\nautomatic verbalizer in SSL settings without sacrificing performance. The first\nmethod uses various demonstration examples with learnable continuous prompt\ntokens to create diverse prompt models. The second method uses a varying number\nof soft prompt tokens to encourage language models to learn different prompts.\nFor the verbalizer, we use the prototypical verbalizer to replace the manual\none. In summary, we obtained the best average accuracy of 73.2% (a relative\nimprovement of 2.52% over even the previous state-of-the-art SSL method with\nmanual prompts and verbalizers) in different few-shot learning settings.", "published": "2023-02-18 05:06:28", "link": "http://arxiv.org/abs/2302.09236v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformadores: Fundamentos teoricos y Aplicaciones", "abstract": "Transformers are a neural network architecture originally designed for\nnatural language processing that it is now a mainstream tool for solving a wide\nvariety of problems, including natural language processing, sound, image,\nreinforcement learning, and other problems with heterogeneous input data. Its\ndistinctive feature is its self-attention system, based on attention to one's\nown sequence, which derives from the previously introduced attention system.\nThis article provides the reader with the necessary context to understand the\nmost recent research articles and presents the mathematical and algorithmic\nfoundations of the elements that make up this type of network. The different\ncomponents that make up this architecture and the variations that may exist are\nalso studied, as well as some applications of the transformer models. This\narticle is in Spanish to bring this scientific knowledge to the\nSpanish-speaking community.", "published": "2023-02-18 13:30:32", "link": "http://arxiv.org/abs/2302.09327v1", "categories": ["cs.CL", "cs.AI", "68T01", "I.2"], "primary_category": "cs.CL"}
{"title": "Natural Language-conditioned Reinforcement Learning with Inside-out Task\n  Language Development and Translation", "abstract": "Natural Language-conditioned reinforcement learning (RL) enables the agents\nto follow human instructions. Previous approaches generally implemented\nlanguage-conditioned RL by providing human instructions in natural language\n(NL) and training a following policy. In this outside-in approach, the policy\nneeds to comprehend the NL and manage the task simultaneously. However, the\nunbounded NL examples often bring much extra complexity for solving concrete RL\ntasks, which can distract policy learning from completing the task. To ease the\nlearning burden of the policy, we investigate an inside-out scheme for natural\nlanguage-conditioned RL by developing a task language (TL) that is task-related\nand unique. The TL is used in RL to achieve highly efficient and effective\npolicy training. Besides, a translator is trained to translate NL into TL. We\nimplement this scheme as TALAR (TAsk Language with predicAte Representation)\nthat learns multiple predicates to model object relationships as the TL.\nExperiments indicate that TALAR not only better comprehends NL instructions but\nalso leads to a better instruction-following policy that improves 13.4% success\nrate and adapts to unseen expressions of NL instruction. The TL can also be an\neffective task abstraction, naturally compatible with hierarchical RL.", "published": "2023-02-18 15:49:09", "link": "http://arxiv.org/abs/2302.09368v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "M-SENSE: Modeling Narrative Structure in Short Personal Narratives Using\n  Protagonist's Mental Representations", "abstract": "Narrative is a ubiquitous component of human communication. Understanding its\nstructure plays a critical role in a wide variety of applications, ranging from\nsimple comparative analyses to enhanced narrative retrieval, comprehension, or\nreasoning capabilities. Prior research in narratology has highlighted the\nimportance of studying the links between cognitive and linguistic aspects of\nnarratives for effective comprehension. This interdependence is related to the\ntextual semantics and mental language in narratives, referring to characters'\nmotivations, feelings or emotions, and beliefs. However, this interdependence\nis hardly explored for modeling narratives. In this work, we propose the task\nof automatically detecting prominent elements of the narrative structure by\nanalyzing the role of characters' inferred mental state along with linguistic\ninformation at the syntactic and semantic levels. We introduce a STORIES\ndataset of short personal narratives containing manual annotations of key\nelements of narrative structure, specifically climax and resolution. To this\nend, we implement a computational model that leverages the protagonist's mental\nstate information obtained from a pre-trained model trained on social\ncommonsense knowledge and integrates their representations with contextual\nsemantic embed-dings using a multi-feature fusion approach. Evaluating against\nprior zero-shot and supervised baselines, we find that our model is able to\nachieve significant improvements in the task of identifying climax and\nresolution.", "published": "2023-02-18 20:48:02", "link": "http://arxiv.org/abs/2302.09418v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Federated Approach for Hate Speech Detection", "abstract": "Hate speech detection has been the subject of high research attention, due to\nthe scale of content created on social media. In spite of the attention and the\nsensitive nature of the task, privacy preservation in hate speech detection has\nremained under-studied. The majority of research has focused on centralised\nmachine learning infrastructures which risk leaking data. In this paper, we\nshow that using federated machine learning can help address privacy the\nconcerns that are inherent to hate speech detection while obtaining up to 6.81%\nimprovement in terms of F1-score.", "published": "2023-02-18 06:08:04", "link": "http://arxiv.org/abs/2302.09243v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Optimising Human-Machine Collaboration for Efficient High-Precision\n  Information Extraction from Text Documents", "abstract": "While humans can extract information from unstructured text with high\nprecision and recall, this is often too time-consuming to be practical.\nAutomated approaches, on the other hand, produce nearly-immediate results, but\nmay not be reliable enough for high-stakes applications where precision is\nessential. In this work, we consider the benefits and drawbacks of various\nhuman-only, human-machine, and machine-only information extraction approaches.\nWe argue for the utility of a human-in-the-loop approach in applications where\nhigh precision is required, but purely manual extraction is infeasible. We\npresent a framework and an accompanying tool for information extraction using\nweak-supervision labelling with human validation. We demonstrate our approach\non three criminal justice datasets. We find that the combination of computer\nspeed and human understanding yields precision comparable to manual annotation\nwhile requiring only a fraction of time, and significantly outperforms fully\nautomated baselines in terms of precision.", "published": "2023-02-18 13:07:22", "link": "http://arxiv.org/abs/2302.09324v1", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Front-End Adapter: Adapting Front-End Input of Speech based\n  Self-Supervised Learning for Speech Recognition", "abstract": "Recent years have witnessed a boom in self-supervised learning (SSL) in\nvarious areas including speech processing. Speech based SSL models present\npromising performance in a range of speech related tasks. However, the training\nof SSL models is computationally expensive and a common practice is to\nfine-tune a released SSL model on the specific task. It is essential to use\nconsistent front-end input during pre-training and fine-tuning. This\nconsistency may introduce potential issues when the optimal front-end is not\nthe same as that used in pre-training. In this paper, we propose a simple but\neffective front-end adapter to address this front-end discrepancy. By\nminimizing the distance between the outputs of different front-ends, the\nfilterbank feature (Fbank) can be compatible with SSL models which are\npre-trained with waveform. The experiment results demonstrate the effectiveness\nof our proposed front-end adapter on several popular SSL models for the speech\nrecognition task.", "published": "2023-02-18 13:46:12", "link": "http://arxiv.org/abs/2302.09331v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker and Language Change Detection using Wav2vec2 and Whisper", "abstract": "We investigate recent transformer networks pre-trained for automatic speech\nrecognition for their ability to detect speaker and language changes in speech.\nWe do this by simply adding speaker (change) or language targets to the labels.\nFor Wav2vec2 pre-trained networks, we also investigate if the representation\nfor the speaker change symbol can be conditioned to capture speaker identity\ncharacteristics. Using a number of constructed data sets we show that these\ncapabilities are definitely there, with speaker recognition equal error rates\nof the order of 10% and language detection error rates of a few percent. We\nwill publish the code for reproducibility.", "published": "2023-02-18 16:45:30", "link": "http://arxiv.org/abs/2302.09381v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT", "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.", "published": "2023-02-18 20:51:09", "link": "http://arxiv.org/abs/2302.09419v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Deep AHS: A Deep Learning Approach to Acoustic Howling Suppression", "abstract": "In this paper, we formulate acoustic howling suppression (AHS) as a\nsupervised learning problem and propose a deep learning approach, called Deep\nAHS, to address it. Deep AHS is trained in a teacher forcing way which converts\nthe recurrent howling suppression process into an instantaneous speech\nseparation process to simplify the problem and accelerate the model training.\nThe proposed method utilizes properly designed features and trains an attention\nbased recurrent neural network (RNN) to extract the target signal from the\nmicrophone recording, thus attenuating the playback signal that may lead to\nhowling. Different training strategies are investigated and a streaming\ninference method implemented in a recurrent mode used to evaluate the\nperformance of the proposed method for real-time howling suppression. Deep AHS\navoids howling detection and intrinsically prohibits howling from happening,\nallowing for more flexibility in the design of audio systems. Experimental\nresults show the effectiveness of the proposed method for howling suppression\nunder different scenarios.", "published": "2023-02-18 07:40:37", "link": "http://arxiv.org/abs/2302.09252v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-dimensional frequency dynamic convolution with confident mean\n  teacher for sound event detection", "abstract": "Recently, convolutional neural networks (CNNs) have been widely used in sound\nevent detection (SED). However, traditional convolution is deficient in\nlearning time-frequency domain representation of different sound events. To\naddress this issue, we propose multi-dimensional frequency dynamic convolution\n(MFDConv), a new design that endows convolutional kernels with\nfrequency-adaptive dynamic properties along multiple dimensions. MFDConv\nutilizes a novel multi-dimensional attention mechanism with a parallel strategy\nto learn complementary frequency-adaptive attentions, which substantially\nstrengthen the feature extraction ability of convolutional kernels. Moreover,\nin order to promote the performance of mean teacher, we propose the confident\nmean teacher to increase the accuracy of pseudo-labels from the teacher and\ntrain the student with high confidence labels. Experimental results show that\nthe proposed methods achieve 0.470 and 0.692 of PSDS1 and PSDS2 on the DESED\nreal validation dataset.", "published": "2023-02-18 08:18:28", "link": "http://arxiv.org/abs/2302.09256v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RobustDistiller: Compressing Universal Speech Representations for\n  Enhanced Environment Robustness", "abstract": "Self-supervised speech pre-training enables deep neural network models to\ncapture meaningful and disentangled factors from raw waveform signals. The\nlearned universal speech representations can then be used across numerous\ndownstream tasks. These representations, however, are sensitive to distribution\nshifts caused by environmental factors, such as noise and/or room\nreverberation. Their large sizes, in turn, make them unfeasible for edge\napplications. In this work, we propose a knowledge distillation methodology\ntermed RobustDistiller which compresses universal representations while making\nthem more robust against environmental artifacts via a multi-task learning\nobjective. The proposed layer-wise distillation recipe is evaluated on top of\nthree well-established universal representations, as well as with three\ndownstream tasks. Experimental results show the proposed methodology applied on\ntop of the WavLM Base+ teacher model outperforming all other benchmarks across\nnoise types and levels, as well as reverberation times. Oftentimes, the\nobtained results with the student model (24M parameters) achieved results\ninline with those of the teacher model (95M).", "published": "2023-02-18 23:13:47", "link": "http://arxiv.org/abs/2302.09437v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exposing AI-Synthesized Human Voices Using Neural Vocoder Artifacts", "abstract": "The advancements of AI-synthesized human voices have introduced a growing\nthreat of impersonation and disinformation. It is therefore of practical\nimportance to developdetection methods for synthetic human voices. This work\nproposes a new approach to detect synthetic human voices based on identifying\nartifacts of neural vocoders in audio signals. A neural vocoder is a specially\ndesigned neural network that synthesizes waveforms from temporal-frequency\nrepresentations, e.g., mel-spectrograms. The neural vocoder is a core component\nin most DeepFake audio synthesis models. Hence the identification of neural\nvocoder processing implies that an audio sample may have been synthesized. To\ntake advantage of the vocoder artifacts for synthetic human voice detection, we\nintroduce a multi-task learning framework for a binary-class RawNet2 model that\nshares the front-end feature extractor with a vocoder identification module. We\ntreat the vocoder identification as a pretext task to constrain the front-end\nfeature extractor to focus on vocoder artifacts and provide discriminative\nfeatures for the final binary classifier. Our experiments show that the\nimproved RawNet2 model based on vocoder identification achieves an overall high\nclassification performance on the binary task.", "published": "2023-02-18 00:29:22", "link": "http://arxiv.org/abs/2302.09198v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cost-effective Models for Detecting Depression from Speech", "abstract": "Depression is the most common psychological disorder and is considered as a\nleading cause of disability and suicide worldwide. An automated system capable\nof detecting signs of depression in human speech can contribute to ensuring\ntimely and effective mental health care for individuals suffering from the\ndisorder. Developing such automated system requires accurate machine learning\nmodels, capable of capturing signs of depression. However, state-of-the-art\nmodels based on deep acoustic representations require abundant data, meticulous\nselection of features, and rigorous training; the procedure involves enormous\ncomputational resources. In this work, we explore the effectiveness of two\ndifferent acoustic feature groups - conventional hand-curated and deep\nrepresentation features, for predicting the severity of depression from speech.\nWe explore the relevance of possible contributing factors to the models'\nperformance, including gender of the individual, severity of the disorder,\ncontent and length of speech. Our findings suggest that models trained on\nconventional acoustic features perform equally well or better than the ones\ntrained on deep representation features at significantly lower computational\ncost, irrespective of other factors, e.g. content and length of speech, gender\nof the speaker and severity of the disorder. This makes such models a better\nfit for deployment where availability of computational resources is restricted,\nsuch as real time depression monitoring applications in smart devices.", "published": "2023-02-18 02:46:21", "link": "http://arxiv.org/abs/2302.09214v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "SSVMR: Saliency-based Self-training for Video-Music Retrieval", "abstract": "With the rise of short videos, the demand for selecting appropriate\nbackground music (BGM) for a video has increased significantly, video-music\nretrieval (VMR) task gradually draws much attention by research community. As\nother cross-modal learning tasks, existing VMR approaches usually attempt to\nmeasure the similarity between the video and music in the feature space.\nHowever, they (1) neglect the inevitable label noise; (2) neglect to enhance\nthe ability to capture critical video clips. In this paper, we propose a novel\nsaliency-based self-training framework, which is termed SSVMR. Specifically, we\nfirst explore to fully make use of the information containing in the training\ndataset by applying a semi-supervised method to suppress the adverse impact of\nlabel noise problem, where a self-training approach is adopted. In addition, we\npropose to capture the saliency of the video by mixing two videos at span level\nand preserving the locality of the two original videos. Inspired by back\ntranslation in NLP, we also conduct back retrieval to obtain more training\ndata. Experimental results on MVD dataset show that our SSVMR achieves the\nstate-of-the-art performance by a large margin, obtaining a relative\nimprovement of 34.8% over the previous best model in terms of R@1.", "published": "2023-02-18 13:30:56", "link": "http://arxiv.org/abs/2302.09328v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
