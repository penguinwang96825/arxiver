{"title": "Self-Training with Purpose Preserving Augmentation Improves Few-shot\n  Generative Dialogue State Tracking", "abstract": "In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.", "published": "2022-11-17 07:13:58", "link": "http://arxiv.org/abs/2211.09379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConNER: Consistency Training for Cross-lingual Named Entity Recognition", "abstract": "Cross-lingual named entity recognition (NER) suffers from data scarcity in\nthe target languages, especially under zero-shot settings. Existing\ntranslate-train or knowledge distillation methods attempt to bridge the\nlanguage gap, but often introduce a high level of noise. To solve this problem,\nconsistency training methods regularize the model to be robust towards\nperturbations on data or hidden states. However, such methods are likely to\nviolate the consistency hypothesis, or mainly focus on coarse-grain\nconsistency. We propose ConNER as a novel consistency training framework for\ncross-lingual NER, which comprises of: (1) translation-based consistency\ntraining on unlabeled target-language data, and (2) dropoutbased consistency\ntraining on labeled source-language data. ConNER effectively leverages\nunlabeled target-language data and alleviates overfitting on the source\nlanguage to enhance the cross-lingual adaptability. Experimental results show\nour ConNER achieves consistent improvement over various baseline methods.", "published": "2022-11-17 07:57:54", "link": "http://arxiv.org/abs/2211.09394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Domain Conversational Question Answering with Historical Answers", "abstract": "Open-domain conversational question answering can be viewed as two tasks:\npassage retrieval and conversational question answering, where the former\nrelies on selecting candidate passages from a large corpus and the latter\nrequires better understanding of a question with contexts to predict the\nanswers. This paper proposes ConvADR-QA that leverages historical answers to\nboost retrieval performance and further achieves better answering performance.\nIn our proposed framework, the retrievers use a teacher-student framework to\nreduce noises from previous turns. Our experiments on the benchmark dataset,\nOR-QuAC, demonstrate that our model outperforms existing baselines in both\nextractive and generative reader settings, well justifying the effectiveness of\nhistorical answers for open-domain conversational question answering.", "published": "2022-11-17 08:20:57", "link": "http://arxiv.org/abs/2211.09401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature-augmented Machine Reading Comprehension with Auxiliary Tasks", "abstract": "While most successful approaches for machine reading comprehension rely on\nsingle training objective, it is assumed that the encoder layer can learn great\nrepresentation through the loss function we define in the predict layer, which\nis cross entropy in most of time, in the case that we first use neural networks\nto encode the question and paragraph, then directly fuse the encoding result of\nthem. However, due to the distantly loss backpropagating in reading\ncomprehension, the encoder layer cannot learn effectively and be directly\nsupervised. Thus, the encoder layer can not learn the representation well at\nany time. Base on this, we propose to inject multi granularity information to\nthe encoding layer. Experiments demonstrate the effect of adding multi\ngranularity information to the encoding layer can boost the performance of\nmachine reading comprehension system. Finally, empirical study shows that our\napproach can be applied to many existing MRC models.", "published": "2022-11-17 09:58:49", "link": "http://arxiv.org/abs/2211.09438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consultation Checklists: Standardising the Human Evaluation of Medical\n  Note Generation", "abstract": "Evaluating automatically generated text is generally hard due to the\ninherently subjective nature of many aspects of the output quality. This\ndifficulty is compounded in automatic consultation note generation by differing\nopinions between medical experts both about which patient statements should be\nincluded in generated notes and about their respective importance in arriving\nat a diagnosis. Previous real-world evaluations of note-generation systems saw\nsubstantial disagreement between expert evaluators. In this paper we propose a\nprotocol that aims to increase objectivity by grounding evaluations in\nConsultation Checklists, which are created in a preliminary step and then used\nas a common point of reference during quality assessment. We observed good\nlevels of inter-annotator agreement in a first evaluation study using the\nprotocol; further, using Consultation Checklists produced in the study as\nreference for automatic metrics such as ROUGE or BERTScore improves their\ncorrelation with human judgements compared to using the original human note.", "published": "2022-11-17 10:54:28", "link": "http://arxiv.org/abs/2211.09455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Summarization Guided by Latent Hierarchical Document\n  Structure", "abstract": "Sequential abstractive neural summarizers often do not use the underlying\nstructure in the input article or dependencies between the input sentences.\nThis structure is essential to integrate and consolidate information from\ndifferent parts of the text. To address this shortcoming, we propose a\nhierarchy-aware graph neural network (HierGNN) which captures such dependencies\nthrough three main steps: 1) learning a hierarchical document structure through\na latent structure tree learned by a sparse matrix-tree computation; 2)\npropagating sentence information over this structure using a novel\nmessage-passing node propagation mechanism to identify salient information; 3)\nusing graph-level attention to concentrate the decoder on salient information.\nExperiments confirm HierGNN improves strong sequence models such as BART, with\na 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further\nhuman evaluation demonstrates that summaries produced by our model are more\nrelevant and less redundant than the baselines, into which HierGNN is\nincorporated. We also find HierGNN synthesizes summaries by fusing multiple\nsource sentences more, rather than compressing a single source sentence, and\nthat it processes long inputs more effectively.", "published": "2022-11-17 11:02:30", "link": "http://arxiv.org/abs/2211.09458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dynamic Quantization for Transformer Inference", "abstract": "We introduce a novel run-time method for significantly reducing the accuracy\nloss associated with quantizing BERT-like models to 8-bit integers. Existing\nmethods for quantizing models either modify the training procedure,or they\nrequire an additional calibration step to adjust parameters that also requires\na selected held-out dataset. Our method permits taking advantage of\nquantization without the need for these adjustments. We present results on\nseveral NLP tasks demonstrating the usefulness of this technique.", "published": "2022-11-17 18:09:07", "link": "http://arxiv.org/abs/2211.09744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Transformers with Dynamic Token Pooling", "abstract": "Transformers achieve unrivalled performance in modelling language, but remain\ninefficient in terms of memory and time complexity. A possible remedy is to\nreduce the sequence length in the intermediate layers by pooling fixed-length\nsegments of tokens. Nevertheless, natural units of meaning, such as words or\nphrases, display varying sizes. To address this mismatch, we equip language\nmodels with a dynamic-pooling mechanism, which predicts segment boundaries in\nan autoregressive fashion. We compare several methods to infer boundaries,\nincluding end-to-end learning through stochastic re-parameterisation,\nsupervised learning (based on segmentations from subword tokenizers or spikes\nin conditional entropy), as well as linguistically motivated boundaries. We\nperform character-level evaluation on texts from multiple datasets and\nmorphologically diverse languages. The results demonstrate that dynamic\npooling, which jointly segments and models language, is both faster and more\naccurate than vanilla Transformers and fixed-length pooling within the same\ncomputational budget.", "published": "2022-11-17 18:39:23", "link": "http://arxiv.org/abs/2211.09761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot\n  Summarization", "abstract": "The high annotation costs and diverse demands of various summarization tasks\nmotivate the development of few-shot summarization. However, despite the\nemergence of many summarization tasks and datasets, the current training\nparadigm for few-shot summarization systems ignores potentially shareable\nknowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm},\na unified few-shot summarization model pre-trained with multiple summarization\ntasks and can be prefix-tuned to excel at any few-shot summarization task.\nMeanwhile, to better evaluate few-shot summarizers, under the principles of\ndiversity and robustness, we assemble and release a new benchmark\n\\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of\nfew-shot samples for each task, covering diverse domains. Experimental results\nand analysis show that \\textsc{UniSumm} outperforms strong baselines by a large\nmargin across all sub-tasks in \\textsc{SummZoo} under both automatic and human\nevaluations and achieves comparable results in human evaluation compared with a\nGPT-3.5 model.", "published": "2022-11-17 18:54:47", "link": "http://arxiv.org/abs/2211.09783v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProtSi: Prototypical Siamese Network with Data Augmentation for Few-Shot\n  Subjective Answer Evaluation", "abstract": "Subjective answer evaluation is a time-consuming and tedious task, and the\nquality of the evaluation is heavily influenced by a variety of subjective\npersonal characteristics. Instead, machine evaluation can effectively assist\neducators in saving time while also ensuring that evaluations are fair and\nrealistic. However, most existing methods using regular machine learning and\nnatural language processing techniques are generally hampered by a lack of\nannotated answers and poor model interpretability, making them unsuitable for\nreal-world use. To solve these challenges, we propose ProtSi Network, a unique\nsemi-supervised architecture that for the first time uses few-shot learning to\nsubjective answer evaluation. To evaluate students' answers by similarity\nprototypes, ProtSi Network simulates the natural process of evaluator scoring\nanswers by combining Siamese Network which consists of BERT and encoder layers\nwith Prototypical Network. We employed an unsupervised diverse paraphrasing\nmodel ProtAugment, in order to prevent overfitting for effective few-shot text\nclassification. By integrating contrastive learning, the discriminative text\nissue can be mitigated. Experiments on the Kaggle Short Scoring Dataset\ndemonstrate that the ProtSi Network outperforms the most recent baseline models\nin terms of accuracy and quadratic weighted kappa.", "published": "2022-11-17 19:33:35", "link": "http://arxiv.org/abs/2211.09855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Hallucinations in Neural Machine Translation with Feature\n  Attribution", "abstract": "Neural conditional language generation models achieve the state-of-the-art in\nNeural Machine Translation (NMT) but are highly dependent on the quality of\nparallel training dataset. When trained on low-quality datasets, these models\nare prone to various error types, including hallucinations, i.e. outputs that\nare fluent, but unrelated to the source sentences. These errors are\nparticularly dangerous, because on the surface the translation can be perceived\nas a correct output, especially if the reader does not understand the source\nlanguage. We present a case study focusing on model understanding and\nregularisation to reduce hallucinations in NMT. We first use feature\nattribution methods to study the behaviour of an NMT model that produces\nhallucinations. We then leverage these methods to propose a novel loss function\nthat substantially helps reduce hallucinations and does not require retraining\nthe model from scratch.", "published": "2022-11-17 20:33:56", "link": "http://arxiv.org/abs/2211.09878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Professional Presentation and Projected Power: A Case Study of Implicit\n  Gender Information in English CVs", "abstract": "Gender discrimination in hiring is a pertinent and persistent bias in\nsociety, and a common motivating example for exploring bias in NLP. However,\nthe manifestation of gendered language in application materials has received\nlimited attention. This paper investigates the framing of skills and background\nin CVs of self-identified men and women. We introduce a data set of 1.8K\nauthentic, English-language, CVs from the US, covering 16 occupations, allowing\nus to partially control for the confound occupation-specific gender base rates.\nWe find that (1) women use more verbs evoking impressions of low power; and (2)\nclassifiers capture gender signal even after data balancing and removal of\npronouns and named entities, and this holds for both transformer-based and\nlinear classifiers.", "published": "2022-11-17 23:26:52", "link": "http://arxiv.org/abs/2211.09942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Execution-based Evaluation for Data Science Code Generation Models", "abstract": "Code generation models can benefit data scientists' productivity by\nautomatically generating code from context and text descriptions. An important\nmeasure of the modeling progress is whether a model can generate code that can\ncorrectly execute to solve the task. However, due to the lack of an evaluation\ndataset that directly supports execution-based model evaluation, existing work\nrelies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model\nselection, which can be inaccurate.\n  To remedy this, we introduce ExeDS, an evaluation dataset for execution\nevaluation for data science code generation tasks. ExeDS contains a set of 534\nproblems from Jupyter Notebooks, each consisting of code context, task\ndescription, reference program, and the desired execution output. With ExeDS,\nwe evaluate the execution performance of five state-of-the-art code generation\nmodels that have achieved high surface-form evaluation scores. Our experiments\nshow that models with high surface-form scores do not necessarily perform well\non execution metrics, and execution-based metrics can better capture model code\ngeneration errors. Source code and data can be found at\nhttps://github.com/Jun-jie-Huang/ExeDS", "published": "2022-11-17 07:04:11", "link": "http://arxiv.org/abs/2211.09374v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Ignore Previous Prompt: Attack Techniques For Language Models", "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.", "published": "2022-11-17 13:43:20", "link": "http://arxiv.org/abs/2211.09527v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Style Classification of Rabbinic Literature for Detection of Lost\n  Midrash Tanhuma Material", "abstract": "Midrash collections are complex rabbinic works that consist of text in\nmultiple languages, which evolved through long processes of unstable oral and\nwritten transmission. Determining the origin of a given passage in such a\ncompilation is not always straightforward and is often a matter of dispute\namong scholars, yet it is essential for scholars' understanding of the passage\nand its relationship to other texts in the rabbinic corpus. To help solve this\nproblem, we propose a system for classification of rabbinic literature based on\nits style, leveraging recent advances in natural language processing for Hebrew\ntexts. Additionally, we demonstrate how this method can be applied to uncover\nlost material from a specific midrash genre, Tan\\d{h}uma-Yelammedenu, that has\nbeen preserved in later anthologies.", "published": "2022-11-17 17:45:59", "link": "http://arxiv.org/abs/2211.09710v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing for Incremental Parse States in Autoregressive Language Models", "abstract": "Next-word predictions from autoregressive neural language models show\nremarkable sensitivity to syntax. This work evaluates the extent to which this\nbehavior arises as a result of a learned ability to maintain implicit\nrepresentations of incremental syntactic structures. We extend work in\nsyntactic probing to the incremental setting and present several probes for\nextracting incomplete syntactic structure (operationalized through parse states\nfrom a stack-based parser) from autoregressive language models. We find that\nour probes can be used to predict model preferences on ambiguous sentence\nprefixes and causally intervene on model representations and steer model\nbehavior. This suggests implicit incremental syntactic inferences underlie\nnext-word predictions in autoregressive neural language models.", "published": "2022-11-17 18:15:31", "link": "http://arxiv.org/abs/2211.09748v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I Can't Believe There's No Images! Learning Visual Tasks Using only\n  Language Supervision", "abstract": "Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether it is possible to learn those skills\nfrom text data and then transfer them to vision tasks without ever training on\nvisual training data. Key to our approach is exploiting the joint embedding\nspace of contrastively trained vision and language encoders. In practice, there\ncan be systematic differences between embedding spaces for different modalities\nin contrastive models, and we analyze how these differences affect our approach\nand study strategies to mitigate this concern. We produce models using only\ntext training data on four representative tasks: image captioning, visual\nentailment, visual question answering and visual news captioning, and evaluate\nthem on standard benchmarks using images. We find these models perform close to\nmodels trained on images, while surpassing prior work for captioning and visual\nentailment in this text-only setting by over 9 points, and outperforming all\nprior work on visual news by over 30 points. We also showcase a variety of\nstylistic image captioning models that are trained using no image data and no\nhuman-curated language data, but instead using readily-available text data from\nbooks, the web, or language models.", "published": "2022-11-17 18:52:19", "link": "http://arxiv.org/abs/2211.09778v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Summarizing Community-based Question-Answer Pairs", "abstract": "Community-based Question Answering (CQA), which allows users to acquire their\ndesired information, has increasingly become an essential component of online\nservices in various domains such as E-commerce, travel, and dining. However, an\noverwhelming number of CQA pairs makes it difficult for users without\nparticular intent to find useful information spread over CQA pairs. To help\nusers quickly digest the key information, we propose the novel CQA\nsummarization task that aims to create a concise summary from CQA pairs. To\nthis end, we first design a multi-stage data annotation process and create a\nbenchmark dataset, CoQASUM, based on the Amazon QA corpus. We then compare a\ncollection of extractive and abstractive summarization methods and establish a\nstrong baseline approach DedupLED for the CQA summarization task. Our\nexperiment further confirms two key challenges, sentence-type transfer and\ndeduplication removal, towards the CQA summarization task. Our data and code\nare publicly available.", "published": "2022-11-17 21:09:41", "link": "http://arxiv.org/abs/2211.09892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Random-LTD: Random and Layerwise Token Dropping Brings Efficient\n  Training for Large-scale Transformers", "abstract": "Large-scale transformer models have become the de-facto architectures for\nvarious machine learning applications, e.g., CV and NLP. However, those large\nmodels also introduce prohibitive training costs. To mitigate this issue, we\npropose a novel random and layerwise token dropping method (random-LTD), which\nskips the computation of a subset of the input tokens at all middle layers.\nParticularly, random-LTD achieves considerable speedups and comparable accuracy\nas the standard training baseline. Compared to other token dropping methods,\nrandom-LTD does not require (1) any importance score-based metrics, (2) any\nspecial token treatment (e.g., [CLS]), and (3) many layers in full sequence\nlength training except the first and the last layers. Besides, a new LayerToken\nlearning rate schedule is proposed for pretraining problems that resolve the\nheavy tuning requirement for our proposed training mechanism. Finally, we\ndemonstrate that random-LTD can be applied to broader applications, including\nGPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results\nshow that random-LTD can save about 33.3% theoretical compute cost and 25.6%\nwall-clock training time while achieving similar zero-shot evaluations on\nGPT-31.3B as compared to baseline.", "published": "2022-11-17 23:14:58", "link": "http://arxiv.org/abs/2211.11586v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Autoregressive Document Retrieval for Fact Verification", "abstract": "Document retrieval is a core component of many knowledge-intensive natural\nlanguage processing task formulations such as fact verification and question\nanswering. Sources of textual knowledge, such as Wikipedia articles, condition\nthe generation of answers from the models. Recent advances in retrieval use\nsequence-to-sequence models to incrementally predict the title of the\nappropriate Wikipedia page given a query. However, this method requires\nsupervision in the form of human annotation to label which Wikipedia pages\ncontain appropriate context. This paper introduces a distant-supervision method\nthat does not require any annotation to train autoregressive retrievers that\nattain competitive R-Precision and Recall in a zero-shot setting. Furthermore\nwe show that with task-specific supervised fine-tuning, autoregressive\nretrieval performance for two Wikipedia-based fact verification tasks can\napproach or even exceed full supervision using less than $1/4$ of the annotated\ndata indicating possible directions for data-efficient autoregressive\nretrieval.", "published": "2022-11-17 07:27:50", "link": "http://arxiv.org/abs/2211.09388v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LongFNT: Long-form Speech Recognition with Factorized Neural Transducer", "abstract": "Traditional automatic speech recognition~(ASR) systems usually focus on\nindividual utterances, without considering long-form speech with useful\nhistorical information, which is more practical in real scenarios. Simply\nattending longer transcription history for a vanilla neural transducer model\nshows no much gain in our preliminary experiments, since the prediction network\nis not a pure language model. This motivates us to leverage the factorized\nneural transducer structure, containing a real language model, the vocabulary\npredictor. We propose the {LongFNT-Text} architecture, which fuses the\nsentence-level long-form features directly with the output of the vocabulary\npredictor and then embeds token-level long-form features inside the vocabulary\npredictor, with a pre-trained contextual encoder RoBERTa to further boost the\nperformance. Moreover, we propose the {LongFNT} architecture by extending the\nlong-form speech to the original speech input and achieve the best performance.\nThe effectiveness of our LongFNT approach is validated on LibriSpeech and\nGigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,\nrespectively.", "published": "2022-11-17 08:48:27", "link": "http://arxiv.org/abs/2211.09412v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone\n  Disambiguation", "abstract": "Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in\nMandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest\nchallenges is the task of polyphone disambiguation. Most of the previous\npolyphone disambiguation models are trained on manually annotated datasets, and\npublicly available datasets for polyphone disambiguation are scarce. In this\npaper we propose a simple back-translation-style data augmentation method for\nmandarin Chinese polyphone disambiguation, utilizing a large amount of\nunlabeled text data. Inspired by the back-translation technique proposed in the\nfield of machine translation, we build a Grapheme-to-Phoneme (G2P) model to\npredict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme\n(P2G) model to predict pronunciation into text. Meanwhile, a window-based\nmatching strategy and a multi-model scoring strategy are proposed to judge the\ncorrectness of the pseudo-label. We design a data balance strategy to improve\nthe accuracy of some typical polyphonic characters in the training set with\nimbalanced distribution or data scarcity. The experimental result shows the\neffectiveness of the proposed back-translation-style data augmentation method.", "published": "2022-11-17 12:37:41", "link": "http://arxiv.org/abs/2211.09495v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Building Text-To-Speech Systems for the Next Billion Users", "abstract": "Deep learning based text-to-speech (TTS) systems have been evolving rapidly\nwith advances in model architectures, training methodologies, and\ngeneralization across speakers and languages. However, these advances have not\nbeen thoroughly investigated for Indian language speech synthesis. Such\ninvestigation is computationally expensive given the number and diversity of\nIndian languages, relatively lower resource availability, and the diverse set\nof advances in neural TTS that remain untested. In this paper, we evaluate the\nchoice of acoustic models, vocoders, supplementary loss functions, training\nschedules, and speaker and language diversity for Dravidian and Indo-Aryan\nlanguages. Based on this, we identify monolingual models with FastPitch and\nHiFi-GAN V1, trained jointly on male and female speakers to perform the best.\nWith this setup, we train and evaluate TTS models for 13 languages and find our\nmodels to significantly improve upon existing models in all languages as\nmeasured by mean opinion scores. We open-source all models on the Bhashini\nplatform.", "published": "2022-11-17 13:59:34", "link": "http://arxiv.org/abs/2211.09536v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Adapter for Text-Video Retrieval", "abstract": "Text-video retrieval is an important multi-modal learning task, where the\ngoal is to retrieve the most relevant video for a given text query. Recently,\npre-trained models, e.g., CLIP, show great potential on this task. However, as\npre-trained models are scaling up, fully fine-tuning them on text-video\nretrieval datasets has a high risk of overfitting. Moreover, in practice, it\nwould be costly to train and store a large model for each task. To overcome the\nabove issues, we present a novel $\\textbf{Cross-Modal Adapter}$ for\nparameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust\nthe pre-trained model with a few parameterization layers. However, there are\ntwo notable differences. First, our method is designed for the multi-modal\ndomain. Secondly, it allows early cross-modal interactions between CLIP's two\nencoders. Although surprisingly simple, our approach has three notable\nbenefits: (1) reduces $\\textbf{99.6}\\%$ of fine-tuned parameters, and\nalleviates the problem of overfitting, (2) saves approximately 30% of training\ntime, and (3) allows all the pre-trained parameters to be fixed, enabling the\npre-trained model to be shared across datasets. Extensive experiments\ndemonstrate that, without bells and whistles, it achieves superior or\ncomparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,\nVATEX, ActivityNet, and DiDeMo datasets. The code will be available at\n\\url{https://github.com/LeapLabTHU/Cross-Modal-Adapter}.", "published": "2022-11-17 16:15:30", "link": "http://arxiv.org/abs/2211.09623v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CoLI-Machine Learning Approaches for Code-mixed Language Identification\n  at the Word Level in Kannada-English Texts", "abstract": "The task of automatically identifying a language used in a given text is\ncalled Language Identification (LI). India is a multilingual country and many\nIndians especially youths are comfortable with Hindi and English, in addition\nto their local languages. Hence, they often use more than one language to post\ntheir comments on social media. Texts containing more than one language are\ncalled \"code-mixed texts\" and are a good source of input for LI. Languages in\nthese texts may be mixed at sentence level, word level or even at sub-word\nlevel. LI at word level is a sequence labeling problem where each and every\nword in a sentence is tagged with one of the languages in the predefined set of\nlanguages. In order to address word level LI in code-mixed Kannada-English\n(Kn-En) texts, this work presents i) the construction of code-mixed Kn-En\ndataset called CoLI-Kenglish dataset, ii) code-mixed Kn-En embedding and iii)\nlearning models using Machine Learning (ML), Deep Learning (DL) and Transfer\nLearning (TL) approaches. Code-mixed Kn-En texts are extracted from Kannada\nYouTube video comments to construct CoLI-Kenglish dataset and code-mixed Kn-En\nembedding. The words in CoLI-Kenglish dataset are grouped into six major\ncategories, namely, \"Kannada\", \"English\", \"Mixed-language\", \"Name\", \"Location\"\nand \"Other\". The learning models, namely, CoLI-vectors and CoLI-ngrams based on\nML, CoLI-BiLSTM based on DL and CoLI-ULMFiT based on TL approaches are built\nand evaluated using CoLI-Kenglish dataset. The performances of the learning\nmodels illustrated, the superiority of CoLI-ngrams model, compared to other\nmodels with a macro average F1-score of 0.64. However, the results of all the\nlearning models were quite competitive with each other.", "published": "2022-11-17 19:16:56", "link": "http://arxiv.org/abs/2211.09847v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models", "abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.", "published": "2022-11-17 23:14:51", "link": "http://arxiv.org/abs/2211.09935v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO", "68T20, 68T50", "I.2.7; I.2.8; I.2.2; I.2.4"], "primary_category": "cs.AI"}
{"title": "Explainability Via Causal Self-Talk", "abstract": "Explaining the behavior of AI systems is an important problem that, in\npractice, is generally avoided. While the XAI community has been developing an\nabundance of techniques, most incur a set of costs that the wider deep learning\ncommunity has been unwilling to pay in most situations. We take a pragmatic\nview of the issue, and define a set of desiderata that capture both the\nambitions of XAI and the practical constraints of deep learning. We describe an\neffective way to satisfy all the desiderata: train the AI system to build a\ncausal model of itself. We develop an instance of this solution for Deep RL\nagents: Causal Self-Talk. CST operates by training the agent to communicate\nwith itself across time. We implement this method in a simulated 3D\nenvironment, and show how it enables agents to generate faithful and\nsemantically-meaningful explanations of their own behavior. Beyond\nexplanations, we also demonstrate that these learned models provide new ways of\nbuilding semantic control interfaces to AI systems.", "published": "2022-11-17 23:17:01", "link": "http://arxiv.org/abs/2211.09937v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MelHuBERT: A simplified HuBERT on Mel spectrograms", "abstract": "Self-supervised models have had great success in learning speech\nrepresentations that can generalize to various downstream tasks. However, most\nself-supervised models require a large amount of compute and multiple GPUs to\ntrain, significantly hampering the development of self-supervised learning. In\nan attempt to reduce the computation of training, we revisit the training of\nHuBERT, a highly successful self-supervised model. We improve and simplify\nseveral key components, including the loss function, input representation, and\ntraining in multiple stages. Our model, MelHuBERT, is able to achieve favorable\nperformance on phone recognition, speaker identification, and automatic speech\nrecognition against HuBERT, while saving 31.2% of the pre-training time, or\nequivalently 33.5% MACs per one second speech. The code and pre-trained models\nare available in https://github.com/nervjack2/MelHuBERT.", "published": "2022-11-17 23:38:29", "link": "http://arxiv.org/abs/2211.09944v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Compressing Transformer-based self-supervised models for speech\n  processing", "abstract": "Despite the success of Transformers in self- supervised learning with\napplications to various downstream tasks, the computational cost of training\nand inference remains a major challenge for applying these models to a wide\nspectrum of devices. Several isolated attempts have been made to compress\nTransformers, but the settings and metrics are different across studies.\nTrade-off at various compression rates are also largely missing in prior work,\nmaking it difficult to compare compression techniques. In this work, we aim to\nprovide context for the isolated results, studying several commonly used\ncompression techniques, including weight pruning, head pruning, low-rank\napproximation, and knowledge distillation. We report trade- off at various\ncompression rate, including wall-clock time, the number of parameters, and the\nnumber of multiply-accumulate operations. Our results show that compared to\nrecent approaches, basic compression techniques are strong baselines. We\nfurther present several applications of our results, revealing properties of\nTransformers, such as the significance of diagonal attention heads. In\naddition, our results lead to a simple combination of compression techniques\nthat improves trade-off over recent approaches. We hope the results would\npromote more diverse comparisons among model compression techniques and promote\nthe use of model compression as a tool for analyzing models. Our code of\ncompressing speech self-supervised model is available at\nhttps://github.com/nervjack2/Speech-SSL-Compression/.", "published": "2022-11-17 23:53:52", "link": "http://arxiv.org/abs/2211.09949v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is the Elephant Flying? Resolving Ambiguities in Text-to-Image\n  Generative Models", "abstract": "Natural language often contains ambiguities that can lead to\nmisinterpretation and miscommunication. While humans can handle ambiguities\neffectively by asking clarifying questions and/or relying on contextual cues\nand common-sense knowledge, resolving ambiguities can be notoriously hard for\nmachines. In this work, we study ambiguities that arise in text-to-image\ngenerative models. We curate a benchmark dataset covering different types of\nambiguities that occur in these systems. We then propose a framework to\nmitigate ambiguities in the prompts given to the systems by soliciting\nclarifications from the user. Through automatic and human evaluations, we show\nthe effectiveness of our framework in generating more faithful images aligned\nwith human intention in the presence of ambiguities.", "published": "2022-11-17 17:12:43", "link": "http://arxiv.org/abs/2211.12503v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "GLAMI-1M: A Multilingual Image-Text Fashion Dataset", "abstract": "We introduce GLAMI-1M: the largest multilingual image-text classification\ndataset and benchmark. The dataset contains images of fashion products with\nitem descriptions, each in 1 of 13 languages. Categorization into 191 classes\nhas high-quality annotations: all 100k images in the test set and 75% of the 1M\ntraining set were human-labeled. The paper presents baselines for image-text\nclassification showing that the dataset presents a challenging fine-grained\nclassification problem: The best scoring EmbraceNet model using both visual and\ntextual features achieves 69.7% accuracy. Experiments with a modified Imagen\nmodel show the dataset is also suitable for image generation conditioned on\ntext. The dataset, source code and model checkpoints are published at\nhttps://github.com/glami/glami-1m", "published": "2022-11-17 13:19:07", "link": "http://arxiv.org/abs/2211.14451v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Feedback is Needed for Retakes: An Explainable Poor Image Notification\n  Framework for the Visually Impaired", "abstract": "We propose a simple yet effective image captioning framework that can\ndetermine the quality of an image and notify the user of the reasons for any\nflaws in the image. Our framework first determines the quality of images and\nthen generates captions using only those images that are determined to be of\nhigh quality. The user is notified by the flaws feature to retake if image\nquality is low, and this cycle is repeated until the input image is deemed to\nbe of high quality. As a component of the framework, we trained and evaluated a\nlow-quality image detection model that simultaneously learns difficulty in\nrecognizing images and individual flaws, and we demonstrated that our proposal\ncan explain the reasons for flaws with a sufficient score. We also evaluated a\ndataset with low-quality images removed by our framework and found improved\nvalues for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),\nconfirming an improvement in general-purpose image captioning capability. Our\nframework would assist the visually impaired, who have difficulty judging image\nquality.", "published": "2022-11-17 09:22:28", "link": "http://arxiv.org/abs/2211.09427v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Hey ASR System! Why Aren't You More Inclusive? Automatic Speech\n  Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A\n  Literature Review", "abstract": "Speech is the fundamental means of communication between humans. The advent\nof AI and sophisticated speech technologies have led to the rapid proliferation\nof human-to-computer-based interactions, fueled primarily by Automatic Speech\nRecognition (ASR) systems. ASR systems normally take human speech in the form\nof audio and convert it into words, but for some users, it cannot decode the\nspeech, and any output text is filled with errors that are incomprehensible to\nthe human reader. These systems do not work equally for everyone and actually\nhinder the productivity of some users. In this paper, we present research that\naddresses ASR biases against gender, race, and the sick and disabled, while\nexploring studies that propose ASR debiasing techniques for mitigating these\ndiscriminations. We also discuss techniques for designing a more accessible and\ninclusive ASR technology. For each approach surveyed, we also provide a summary\nof the investigation and methods applied, the ASR systems and corpora used, and\nthe research findings, and highlight their strengths and/or weaknesses.\nFinally, we propose future opportunities for Natural Language Processing\nresearchers to explore in the next level creation of ASR technologies.", "published": "2022-11-17 13:15:58", "link": "http://arxiv.org/abs/2211.09511v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "abstract": "We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models -- a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) -- to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.", "published": "2022-11-17 18:58:43", "link": "http://arxiv.org/abs/2211.09800v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unsupervised Model-based speaker adaptation of end-to-end lattice-free\n  MMI model for speech recognition", "abstract": "Modeling the speaker variability is a key challenge for automatic speech\nrecognition (ASR) systems. In this paper, the learning hidden unit\ncontributions (LHUC) based adaptation techniques with compact speaker dependent\n(SD) parameters are used to facilitate both speaker adaptive training (SAT) and\nunsupervised test-time speaker adaptation for end-to-end (E2E) lattice-free MMI\n(LF-MMI) models. An unsupervised model-based adaptation framework is proposed\nto estimate the SD parameters in E2E paradigm using LF-MMI and cross entropy\n(CE) criterions. Various regularization methods of the standard LHUC\nadaptation, e.g., the Bayesian LHUC (BLHUC) adaptation, are systematically\ninvestigated to mitigate the risk of overfitting, on E2E LF-MMI CNN-TDNN and\nCNN-TDNN-BLSTM models. Lattice-based confidence score estimation is used for\nadaptation data selection to reduce the supervision label uncertainty.\nExperiments on the 300-hour Switchboard task suggest that applying BLHUC in the\nproposed unsupervised E2E adaptation framework to byte pair encoding (BPE)\nbased E2E LF-MMI systems consistently outperformed the baseline systems by\nrelative word error rate (WER) reductions up to 10.5% and 14.7% on the NIST\nHub5'00 and RT03 evaluation sets, and achieved the best performance in WERs of\n9.0% and 9.7%, respectively. These results are comparable to the results of\nstate-of-the-art adapted LF-MMI hybrid systems and adapted Conformer-based E2E\nsystems.", "published": "2022-11-17 03:04:16", "link": "http://arxiv.org/abs/2211.09313v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Mongolian Speech Synthesis Based on Automatic Prosody\n  Annotation", "abstract": "While deep learning-based text-to-speech (TTS) models such as VITS have shown\nexcellent results, they typically require a sizable set of high-quality <text,\naudio> pairs to train, which is expensive to collect. So far, most languages in\nthe world still lack the training data needed to develop TTS systems. This\npaper proposes two improvement methods for the two problems faced by\nlow-resource Mongolian speech synthesis: a) In view of the lack of high-quality\n<text, audio> pairs of data, it is difficult to model the mapping problem from\nlinguistic features to acoustic features. Improvements are made using\npre-trained VITS model and transfer learning methods. b) In view of the problem\nof less labeled information, this paper proposes to use an automatic prosodic\nannotation method to label the prosodic information of text and corresponding\nspeech, thereby improving the naturalness and intelligibility of low-resource\nMongolian language. Through empirical research, the N-MOS of the method\nproposed in this paper is 4.195, and the I-MOS is 4.228.", "published": "2022-11-17 06:33:55", "link": "http://arxiv.org/abs/2211.09365v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Token-level Speaker Change Detection Using Speaker Difference and Speech\n  Content via Continuous Integrate-and-fire", "abstract": "In multi-talker scenarios such as meetings and conversations, speech\nprocessing systems are usually required to segment the audio and then\ntranscribe each segmentation. These two stages are addressed separately by\nspeaker change detection (SCD) and automatic speech recognition (ASR). Most\nprevious SCD systems rely solely on speaker information and ignore the\nimportance of speech content. In this paper, we propose a novel SCD system that\nconsiders both cues of speaker difference and speech content. These two cues\nare converted into token-level representations by the continuous\nintegrate-and-fire (CIF) mechanism and then combined for detecting speaker\nchanges on the token acoustic boundaries. We evaluate the performance of our\napproach on a public real-recorded meeting dataset, AISHELL-4. The experiment\nresults show that our method outperforms a competitive frame-level baseline\nsystem by 2.45% equal coverage-purity (ECP). In addition, we demonstrate the\nimportance of speech content and speaker difference to the SCD task, and the\nadvantages of conducting SCD on the token acoustic boundaries compared with\nconducting SCD frame by frame.", "published": "2022-11-17 07:16:17", "link": "http://arxiv.org/abs/2211.09381v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis", "abstract": "Various applications of voice synthesis have been developed independently\ndespite the fact that they generate \"voice\" as output in common. In addition,\nmost of the voice synthesis models still require a large number of audio data\npaired with annotated labels (e.g., text transcription and music score) for\ntraining. To this end, we propose a unified framework of synthesizing and\nmanipulating voice signals from analysis features, dubbed NANSY++. The backbone\nnetwork of NANSY++ is trained in a self-supervised manner that does not require\nany annotations paired with audio. After training the backbone network, we\nefficiently tackle four voice applications - i.e. voice conversion,\ntext-to-speech, singing voice synthesis, and voice designing - by partially\nmodeling the analysis features required for each task. Extensive experiments\nshow that the proposed framework offers competitive advantages such as\ncontrollability, data efficiency, and fast training convergence, while\nproviding high quality synthesis. Audio samples: tinyurl.com/8tnsy3uc.", "published": "2022-11-17 08:29:57", "link": "http://arxiv.org/abs/2211.09407v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Privacy against Real-Time Speech Emotion Detection via Acoustic\n  Adversarial Evasion of Machine Learning", "abstract": "Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have\nbeen widely adopted due to their seamless integration with smart home devices\nand the Internet of Things (IoT) technologies. These VA services raise privacy\nconcerns, especially due to their access to our speech. This work considers one\nsuch use case: the unaccountable and unauthorized surveillance of a user's\nemotion via speech emotion recognition (SER). This paper presents DARE-GP, a\nsolution that creates additive noise to mask users' emotional information while\npreserving the transcription-relevant portions of their speech. DARE-GP does\nthis by using a constrained genetic programming approach to learn the spectral\nfrequency traits that depict target users' emotional content, and then\ngenerating a universal adversarial audio perturbation that provides this\nprivacy protection. Unlike existing works, DARE-GP provides: a) real-time\nprotection of previously unheard utterances, b) against previously unseen\nblack-box SER classifiers, c) while protecting speech transcription, and d)\ndoes so in a realistic, acoustic environment. Further, this evasion is robust\nagainst defenses employed by a knowledgeable adversary. The evaluations in this\nwork culminate with acoustic evaluations against two off-the-shelf commercial\nsmart speakers using a small-form-factor (raspberry pi) integrated with a\nwake-word system to evaluate the efficacy of its real-world, real-time\ndeployment.", "published": "2022-11-17 00:25:05", "link": "http://arxiv.org/abs/2211.09273v4", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "SpectNet : End-to-End Audio Signal Classification Using Learnable\n  Spectrograms", "abstract": "Pattern recognition from audio signals is an active research topic\nencompassing audio tagging, acoustic scene classification, music\nclassification, and other areas. Spectrogram and mel-frequency cepstral\ncoefficients (MFCC) are among the most commonly used features for audio signal\nanalysis and classification. Recently, deep convolutional neural networks (CNN)\nhave been successfully used for audio classification problems using\nspectrogram-based 2D features. In this paper, we present SpectNet, an\nintegrated front-end layer that extracts spectrogram features within a CNN\narchitecture that can be used for audio pattern recognition tasks. The\nfront-end layer utilizes learnable gammatone filters that are initialized using\nmel-scale filters. The proposed layer outputs a 2D spectrogram image which can\nbe fed into a 2D CNN for classification. The parameters of the entire network,\nincluding the front-end filterbank, can be updated via back-propagation. This\ntraining scheme allows for fine-tuning the spectrogram-image features according\nto the target audio dataset. The proposed method is evaluated in two different\naudio signal classification tasks: heart sound anomaly detection and acoustic\nscene classification. The proposed method shows a significant 1.02\\%\nimprovement in MACC for the heart sound classification task and 2.11\\%\nimprovement in accuracy for the acoustic scene classification task compared to\nthe classical spectrogram image features. The source code of our experiments\ncan be found at \\url{https://github.com/mHealthBuet/SpectNet}", "published": "2022-11-17 05:29:03", "link": "http://arxiv.org/abs/2211.09352v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Balanced Deep CCA for Bird Vocalization Detection", "abstract": "Event detection improves when events are captured by two different modalities\nrather than just one. But to train detection systems on multiple modalities is\nchallenging, in particular when there is abundance of unlabelled data but\nlimited amounts of labeled data. We develop a novel self-supervised learning\ntechnique for multi-modal data that learns (hidden) correlations between\nsimultaneously recorded microphone (sound) signals and accelerometer (body\nvibration) signals. The key objective of this work is to learn useful\nembeddings associated with high performance in downstream event detection tasks\nwhen labeled data is scarce and the audio events of interest (songbird\nvocalizations) are sparse. We base our approach on deep canonical correlation\nanalysis (DCCA) that suffers from event sparseness. We overcome the sparseness\nof positive labels by first learning a data sampling model from the labelled\ndata and by applying DCCA on the output it produces. This method that we term\nbalanced DCCA (b-DCCA) improves the performance of the unsupervised embeddings\non the downstream supervised audio detection task compared to classsical DCCA.\nBecause data labels are frequently imbalanced, our method might be of broad\nutility in low-resource scenarios.", "published": "2022-11-17 07:09:07", "link": "http://arxiv.org/abs/2211.09376v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with\n  Diffusion Models", "abstract": "There has been a significant progress in Text-To-Speech (TTS) synthesis\ntechnology in recent years, thanks to the advancement in neural generative\nmodeling. However, existing methods on any-speaker adaptive TTS have achieved\nunsatisfactory performance, due to their suboptimal accuracy in mimicking the\ntarget speakers' styles. In this work, we present Grad-StyleSpeech, which is an\nany-speaker adaptive TTS framework that is based on a diffusion model that can\ngenerate highly natural speech with extremely high similarity to target\nspeakers' voice, given a few seconds of reference speech. Grad-StyleSpeech\nsignificantly outperforms recent speaker-adaptive TTS baselines on English\nbenchmarks. Audio samples are available at\nhttps://nardien.github.io/grad-stylespeech-demo.", "published": "2022-11-17 07:17:24", "link": "http://arxiv.org/abs/2211.09383v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ComMU: Dataset for Combinatorial Music Generation", "abstract": "Commercial adoption of automatic music composition requires the capability of\ngenerating diverse and high-quality music suitable for the desired context\n(e.g., music for romantic movies, action games, restaurants, etc.). In this\npaper, we introduce combinatorial music generation, a new task to create\nvarying background music based on given conditions. Combinatorial music\ngeneration creates short samples of music with rich musical metadata, and\ncombines them to produce a complete music. In addition, we introduce ComMU, the\nfirst symbolic music dataset consisting of short music samples and their\ncorresponding 12 musical metadata for combinatorial music generation. Notable\nproperties of ComMU are that (1) dataset is manually constructed by\nprofessional composers with an objective guideline that induces regularity, and\n(2) it has 12 musical metadata that embraces composers' intentions. Our results\nshow that we can generate diverse high-quality music only with metadata, and\nthat our unique metadata such as track-role and extended chord quality improves\nthe capacity of the automatic composition. We highly recommend watching our\nvideo before reading the paper (https://pozalabs.github.io/ComMU).", "published": "2022-11-17 07:25:09", "link": "http://arxiv.org/abs/2211.09385v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Representations of Sound for Automatic Insect Recognition", "abstract": "Insects are an integral part of our ecosystem. These often small and evasive\nanimals have a big impact on their surroundings, providing a large part of the\npresent biodiversity and pollination duties, forming the foundation of the food\nchain and many biological and ecological processes. Due to factors of human\ninfluence, population numbers and biodiversity have been rapidly declining with\ntime. Monitoring this decline has become increasingly important for\nconservation measures to be effectively implemented. But monitoring methods are\noften invasive, time and resource intense, and prone to various biases. Many\ninsect species produce characteristic mating sounds that can easily be detected\nand recorded without large cost or effort. Using deep learning methods, insect\nsounds from field recordings could be automatically detected and classified to\nmonitor biodiversity and species distribution ranges. In this project, I\nimplement this using existing datasets of insect sounds (Orthoptera and\nCicadidae) and machine learning methods and evaluate their potential for\nacoustic insect monitoring. I compare the performance of the conventional\nspectrogram-based deep learning method against the new adaptive and\nwaveform-based approach LEAF. The waveform-based frontend achieved\nsignificantly better classification performance than the Mel-spectrogram\nfrontend by adapting its feature extraction parameters during training. This\nresult is encouraging for future implementations of deep learning technology\nfor automatic insect sound recognition, especially if larger datasets become\navailable.", "published": "2022-11-17 12:52:22", "link": "http://arxiv.org/abs/2211.09503v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Robust Vocal Quality Feature Embeddings for Dysphonic Voice Detection", "abstract": "Approximately 1.2% of the world's population has impaired voice production.\nAs a result, automatic dysphonic voice detection has attracted considerable\nacademic and clinical interest. However, existing methods for automated voice\nassessment often fail to generalize outside the training conditions or to other\nrelated applications. In this paper, we propose a deep learning framework for\ngenerating acoustic feature embeddings sensitive to vocal quality and robust\nacross different corpora. A contrastive loss is combined with a classification\nloss to train our deep learning model jointly. Data warping methods are used on\ninput voice samples to improve the robustness of our method. Empirical results\ndemonstrate that our method not only achieves high in-corpus and cross-corpus\nclassification accuracy but also generates good embeddings sensitive to voice\nquality and robust across different corpora. We also compare our results\nagainst three baseline methods on clean and three variations of deteriorated\nin-corpus and cross-corpus datasets and demonstrate that the proposed model\nconsistently outperforms the baseline methods.", "published": "2022-11-17 19:34:59", "link": "http://arxiv.org/abs/2211.09858v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Anti-spoofing Using a Simple Attention Module and Joint\n  Optimization Based on Additive Angular Margin Loss and Meta-learning", "abstract": "Automatic speaker verification systems are vulnerable to a variety of access\nthreats, prompting research into the formulation of effective spoofing\ndetection systems to act as a gate to filter out such spoofing attacks. This\nstudy introduces a simple attention module to infer 3-dim attention weights for\nthe feature map in a convolutional layer, which then optimizes an energy\nfunction to determine each neuron's importance. With the advancement of both\nvoice conversion and speech synthesis technologies, unseen spoofing attacks are\nconstantly emerging to limit spoofing detection system performance. Here, we\npropose a joint optimization approach based on the weighted additive angular\nmargin loss for binary classification, with a meta-learning training framework\nto develop an efficient system that is robust to a wide range of spoofing\nattacks for model generalization enhancement. As a result, when compared to\ncurrent state-of-the-art systems, our proposed approach delivers a competitive\nresult with a pooled EER of 0.99% and min t-DCF of 0.0289.", "published": "2022-11-17 21:25:29", "link": "http://arxiv.org/abs/2211.09898v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-source Domain Adaptation for Text-independent Forensic Speaker\n  Recognition", "abstract": "Adapting speaker recognition systems to new environments is a widely-used\ntechnique to improve a well-performing model learned from large-scale data\ntowards a task-specific small-scale data scenarios. However, previous studies\nfocus on single domain adaptation, which neglects a more practical scenario\nwhere training data are collected from multiple acoustic domains needed in\nforensic scenarios. Audio analysis for forensic speaker recognition offers\nunique challenges in model training with multi-domain training data due to\nlocation/scenario uncertainty and diversity mismatch between reference and\nnaturalistic field recordings. It is also difficult to directly employ\nsmall-scale domain-specific data to train complex neural network architectures\ndue to domain mismatch and performance loss. Fine-tuning is a commonly-used\nmethod for adaptation in order to retrain the model with weights initialized\nfrom a well-trained model. Alternatively, in this study, three novel adaptation\nmethods based on domain adversarial training, discrepancy minimization, and\nmoment-matching approaches are proposed to further promote adaptation\nperformance across multiple acoustic domains. A comprehensive set of\nexperiments are conducted to demonstrate that: 1) diverse acoustic environments\ndo impact speaker recognition performance, which could advance research in\naudio forensics, 2) domain adversarial training learns the discriminative\nfeatures which are also invariant to shifts between domains, 3)\ndiscrepancy-minimizing adaptation achieves effective performance simultaneously\nacross multiple acoustic domains, and 4) moment-matching adaptation along with\ndynamic distribution alignment also significantly promotes speaker recognition\nperformance on each domain, especially for the LENA-field domain with noise\ncompared to all other systems.", "published": "2022-11-17 22:11:25", "link": "http://arxiv.org/abs/2211.09913v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advanced Audio Aid for Blind People", "abstract": "One of the most important senses in human life is vision, without it life is\ntotally filled with darkness. According to WHO globally millions of people are\nvisually impaired estimated there are 285 million, of whom some millions are\nblind. Unfortunately, there are around 2.4 million people are blind in our\nbeloved country Pakistan. Human are a crucial part of society and the blind\ncommunity is a main part of society. The technologies are grown so far to make\nthe life of humans easier more comfortable and more reliable for. However, this\ndisability of the blind community would reduce their chance of using such\ninnovative products. Therefore, the visually impaired community believe that\nthey are burden to other societies and they do not capture in normal activities\nseparates the blind people from society and because of this believe did not\nparticipate in the normally tasks of society . The visual impair people mainly\nface most of the problems in this real-time The aim of this work is to turn the\nreal time world into an audio world by telling blind person about the objects\nin their way and can read printed text. This will enable blind persons to\nidentify the things and read the text without any external help just by using\nthe object detection and reading system in real time. Objective of this work:\ni) Object detection ii) Read printed text, using state-of-the-art (SOTA)\ntechnology.", "published": "2022-11-17 07:13:14", "link": "http://arxiv.org/abs/2212.00004v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label\n  Guidance", "abstract": "Although current neural text-to-speech (TTS) models are able to generate\nhigh-quality speech, intensity controllable emotional TTS is still a\nchallenging task. Most existing methods need external optimizations for\nintensity calculation, leading to suboptimal results or degraded quality. In\nthis paper, we propose EmoDiff, a diffusion-based TTS model where emotion\nintensity can be manipulated by a proposed soft-label guidance technique\nderived from classifier guidance. Specifically, instead of being guided with a\none-hot vector for the specified emotion, EmoDiff is guided with a soft label\nwhere the value of the specified emotion and \\textit{Neutral} is set to\n$\\alpha$ and $1-\\alpha$ respectively. The $\\alpha$ here represents the emotion\nintensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can\nprecisely control the emotion intensity while maintaining high voice quality.\nMoreover, diverse speech with specified emotion intensity can be generated by\nsampling in the reverse denoising process.", "published": "2022-11-17 12:37:48", "link": "http://arxiv.org/abs/2211.09496v2", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Heart Abnormality Detection from Heart Sound Signals using MFCC Feature\n  and Dual Stream Attention Based Network", "abstract": "Cardiovascular diseases are one of the leading cause of death in today's\nworld and early screening of heart condition plays a crucial role in preventing\nthem. The heart sound signal is one of the primary indicator of heart condition\nand can be used to detect abnormality in the heart. The acquisition of heart\nsound signal is non-invasive, cost effective and requires minimum equipment.\nBut currently the detection of heart abnormality from heart sound signal\ndepends largely on the expertise and experience of the physician. As such an\nautomatic detection system for heart abnormality detection from heart sound\nsignal can be a great asset for the people living in underdeveloped areas. In\nthis paper we propose a novel deep learning based dual stream network with\nattention mechanism that uses both the raw heart sound signal and the MFCC\nfeatures to detect abnormality in heart condition of a patient. The deep neural\nnetwork has a convolutional stream that uses the raw heart sound signal and a\nrecurrent stream that uses the MFCC features of the signal. The features from\nthese two streams are merged together using a novel attention network and\npassed through the classification network. The model is trained on the largest\npublicly available dataset of PCG signal and achieves an accuracy of 87.11,\nsensitivity of 82.41, specificty of 91.8 and a MACC of 87.12.", "published": "2022-11-17 18:20:46", "link": "http://arxiv.org/abs/2211.09751v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "physics.med-ph", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion\n  Models", "abstract": "Diffusion models have experienced a surge of interest as highly expressive\nyet efficiently trainable probabilistic models. We show that these models are\nan excellent fit for synthesising human motion that co-occurs with audio, e.g.,\ndancing and co-speech gesticulation, since motion is complex and highly\nambiguous given audio, calling for a probabilistic description. Specifically,\nwe adapt the DiffWave architecture to model 3D pose sequences, putting\nConformers in place of dilated convolutions for improved modelling power. We\nalso demonstrate control over motion style, using classifier-free guidance to\nadjust the strength of the stylistic expression. Experiments on gesture and\ndance generation confirm that the proposed method achieves top-of-the-line\nmotion quality, with distinctive styles whose expression can be made more or\nless pronounced. We also synthesise path-driven locomotion using the same model\narchitecture. Finally, we generalise the guidance procedure to obtain\nproduct-of-expert ensembles of diffusion models and demonstrate how these may\nbe used for, e.g., style interpolation, a contribution we believe is of\nindependent interest. See\nhttps://www.speech.kth.se/research/listen-denoise-action/ for video examples,\ndata, and code.", "published": "2022-11-17 17:41:00", "link": "http://arxiv.org/abs/2211.09707v2", "categories": ["cs.LG", "cs.CV", "cs.GR", "cs.HC", "cs.SD", "eess.AS", "68T07", "G.3; I.2.6; I.3.7; J.5"], "primary_category": "cs.LG"}
