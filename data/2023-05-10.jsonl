{"title": "V\u0101rta: A Large-Scale Headline-Generation Dataset for Indic Languages", "abstract": "We present V\\=arta, a large-scale multilingual dataset for headline\ngeneration in Indic languages. This dataset includes 41.8 million news articles\nin 14 different Indic languages (and English), which come from a variety of\nhigh-quality sources. To the best of our knowledge, this is the largest\ncollection of curated articles for Indic languages currently available. We use\nthe data collected in a series of experiments to answer important questions\nrelated to Indic NLP and multilinguality research in general. We show that the\ndataset is challenging even for state-of-the-art abstractive models and that\nthey perform only slightly better than extractive baselines. Owing to its size,\nwe also show that the dataset can be used to pretrain strong language models\nthat outperform competitive baselines in both NLU and NLG benchmarks.", "published": "2023-05-10 03:07:17", "link": "http://arxiv.org/abs/2305.05858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Address Matching Based On Hierarchical Information", "abstract": "There is evidence that address matching plays a crucial role in many areas\nsuch as express delivery, online shopping and so on. Address has a hierarchical\nstructure, in contrast to unstructured texts, which can contribute valuable\ninformation for address matching. Based on this idea, this paper proposes a\nnovel method to leverage the hierarchical information in deep learning method\nthat not only improves the ability of existing methods to handle irregular\naddress, but also can pay closer attention to the special part of address.\nExperimental findings demonstrate that the proposed method improves the current\napproach by 3.2% points.", "published": "2023-05-10 03:45:22", "link": "http://arxiv.org/abs/2305.05874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact\n  Verification", "abstract": "Commonsense fact verification, as a challenging branch of commonsense\nquestion-answering (QA), aims to verify through facts whether a given\ncommonsense claim is correct or not. Answering commonsense questions\nnecessitates a combination of knowledge from various levels. However, existing\nstudies primarily rest on grasping either unstructured evidence or potential\nreasoning paths from structured knowledge bases, yet failing to exploit the\nbenefits of heterogeneous knowledge simultaneously. In light of this, we\npropose Decker, a commonsense fact verification model that is capable of\nbridging heterogeneous knowledge by uncovering latent relationships between\nstructured and unstructured knowledge. Experimental results on two commonsense\nfact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the\neffectiveness of our Decker and further analysis verifies its capability to\nseize more precious information through reasoning.", "published": "2023-05-10 06:28:16", "link": "http://arxiv.org/abs/2305.05921v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in\n  Wikipedia", "abstract": "Wikipedia can be edited by anyone and thus contains various quality\nsentences. Therefore, Wikipedia includes some poor-quality edits, which are\noften marked up by other editors. While editors' reviews enhance the\ncredibility of Wikipedia, it is hard to check all edited text. Assisting in\nthis process is very important, but a large and comprehensive dataset for\nstudying it does not currently exist. Here, we propose WikiSQE, the first\nlarge-scale dataset for sentence quality estimation in Wikipedia. Each sentence\nis extracted from the entire revision history of English Wikipedia, and the\ntarget quality labels were carefully investigated and selected. WikiSQE has\nabout 3.4 M sentences with 153 quality labels. In the experiment with automatic\nclassification using competitive machine learning models, sentences that had\nproblems with citation, syntax/semantics, or propositions were found to be more\ndifficult to detect. In addition, by performing human annotation, we found that\nthe model we developed performed better than the crowdsourced workers. WikiSQE\nis expected to be a valuable resource for other tasks in NLP.", "published": "2023-05-10 06:45:13", "link": "http://arxiv.org/abs/2305.05928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot\n  Commonsense Question Answering", "abstract": "Commonsense question answering (QA) research requires machines to answer\nquestions based on commonsense knowledge. However, this research requires\nexpensive labor costs to annotate data as the basis of research, and models\nthat rely on fine-tuning paradigms only apply to specific tasks, rather than\nlearn a general commonsense reasoning ability. As a more robust method,\nzero-shot commonsense question answering shows a good prospect. The current\nzero-shot framework tries to convert triples in commonsense knowledge graphs\n(KGs) into QA-form samples as the pre-trained data source to incorporate\ncommonsense knowledge into the model. However, this method ignores the\nmulti-hop relationship in the KG, which is also an important central problem in\ncommonsense reasoning. In this paper, we propose a novel multi-hop commonsense\nknowledge injection framework. Specifically, it explores multi-hop reasoning\nparadigm in KGs that conform to linguistic logic, and we further propose two\nmulti-hop QA generation methods based on KGs. Then, we utilize contrastive\nlearning to pre-train the model with the synthetic QA dataset to inject\nmulti-hop commonsense knowledge. Extensive experiments on five commonsense\nquestion answering benchmarks demonstrate that our framework achieves\nstate-of-art performance.", "published": "2023-05-10 07:13:47", "link": "http://arxiv.org/abs/2305.05936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual LLMs are Better Cross-lingual In-context Learners with\n  Alignment", "abstract": "In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.", "published": "2023-05-10 07:24:36", "link": "http://arxiv.org/abs/2305.05940v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text\n  Style Transfer", "abstract": "Adapting a large language model for multiple-attribute text style transfer\nvia fine-tuning can be challenging due to the significant amount of\ncomputational resources and labeled data required for the specific task. In\nthis paper, we address this challenge by introducing AdapterTST, a framework\nthat freezes the pre-trained model's original parameters and enables the\ndevelopment of a multiple-attribute text style transfer model. Using BART as\nthe backbone model, Adapter-TST utilizes different neural adapters to capture\ndifferent attribute information, like a plug-in connected to BART. Our method\nallows control over multiple attributes, like sentiment, tense, voice, etc.,\nand configures the adapters' architecture to generate multiple outputs\nrespected to attributes or compositional editing on the same sentence. We\nevaluate the proposed model on both traditional sentiment transfer and\nmultiple-attribute transfer tasks. The experiment results demonstrate that\nAdapter-TST outperforms all the state-of-the-art baselines with significantly\nlesser computational resources. We have also empirically shown that each\nadapter is able to capture specific stylistic attributes effectively and can be\nconfigured to perform compositional editing.", "published": "2023-05-10 07:33:36", "link": "http://arxiv.org/abs/2305.05945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Forgetting in Pre-Trained Representations Through\n  Continual Learning", "abstract": "Representation forgetting refers to the drift of contextualized\nrepresentations during continual training. Intuitively, the representation\nforgetting can influence the general knowledge stored in pre-trained language\nmodels (LMs), but the concrete effect is still unclear. In this paper, we study\nthe effect of representation forgetting on the generality of pre-trained\nlanguage models, i.e. the potential capability for tackling future downstream\ntasks. Specifically, we design three metrics, including overall generality\ndestruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge\nforgetting (SemF), to measure the evolution of general knowledge in continual\nlearning. With extensive experiments, we find that the generality is destructed\nin various pre-trained LMs, and syntactic and semantic knowledge is forgotten\nthrough continual learning. Based on our experiments and analysis, we further\nget two insights into alleviating general knowledge forgetting: 1) training on\ngeneral linguistic tasks at first can mitigate general knowledge forgetting; 2)\nthe hybrid continual learning method can mitigate the generality destruction\nand maintain more general knowledge compared with those only considering\nrehearsal or regularization.", "published": "2023-05-10 08:27:59", "link": "http://arxiv.org/abs/2305.05968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Say What You Mean! Large Language Models Speak Too Positively about\n  Negative Commonsense Knowledge", "abstract": "Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.", "published": "2023-05-10 08:35:50", "link": "http://arxiv.org/abs/2305.05976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Document Simplification", "abstract": "To date, most work on text simplification has focused on sentence-level\ninputs. Early attempts at document simplification merely applied these\napproaches iteratively over the sentences of a document. However, this fails to\ncoherently preserve the discourse structure, leading to suboptimal output\nquality. Recently, strategies from controllable simplification have been\nleveraged to achieve state-of-the-art results on document simplification by\nfirst generating a document-level plan (a sequence of sentence-level\nsimplification operations) and using this plan to guide sentence-level\nsimplification downstream. However, this is still limited in that the\nsimplification model has no direct access to the local inter-sentence document\ncontext, likely having a negative impact on surface realisation. We explore\nvarious systems that use document context within the simplification process\nitself, either by iterating over larger text units or by extending the system\narchitecture to attend over a high-level representation of document context. In\ndoing so, we achieve state-of-the-art performance on the document\nsimplification task, even when not relying on plan-guidance. Further, we\ninvestigate the performance and efficiency tradeoffs of system variants and\nmake suggestions of when each should be preferred.", "published": "2023-05-10 16:06:36", "link": "http://arxiv.org/abs/2305.06274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured\n  Knowledge Aggregation", "abstract": "Commonsense knowledge is crucial to many natural language processing tasks.\nExisting works usually incorporate graph knowledge with conventional graph\nneural networks (GNNs), resulting in a sequential pipeline that\ncompartmentalizes the encoding processes for textual and graph-based knowledge.\nThis compartmentalization does, however, not fully exploit the contextual\ninterplay between these two types of input knowledge. In this paper, a novel\ncontext-aware graph-attention model (Context-aware GAT) is proposed, designed\nto effectively assimilate global features from relevant knowledge graphs\nthrough a context-enhanced knowledge aggregation mechanism. Specifically, the\nproposed framework employs an innovative approach to representation learning\nthat harmonizes heterogeneous features by amalgamating flattened graph\nknowledge with text data. The hierarchical application of graph knowledge\naggregation within connected subgraphs, complemented by contextual information,\nto bolster the generation of commonsense-driven dialogues is analyzed.\nEmpirical results demonstrate that our framework outperforms conventional\nGNN-based language models in terms of performance. Both, automated and human\nevaluations affirm the significant performance enhancements achieved by our\nproposed model over the concept flow baseline.", "published": "2023-05-10 16:31:35", "link": "http://arxiv.org/abs/2305.06294v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3\n  (with Varying Success)", "abstract": "Large language models, particularly GPT-3, are able to produce high quality\nsummaries of general domain news articles in few- and zero-shot settings.\nHowever, it is unclear if such models are similarly capable in more\nspecialized, high-stakes domains such as biomedicine. In this paper, we enlist\ndomain experts (individuals with medical training) to evaluate summaries of\nbiomedical articles generated by GPT-3, given zero supervision. We consider\nboth single- and multi-document settings. In the former, GPT-3 is tasked with\ngenerating regular and plain-language summaries of articles describing\nrandomized controlled trials; in the latter, we assess the degree to which\nGPT-3 is able to \\emph{synthesize} evidence reported across a collection of\narticles. We design an annotation scheme for evaluating model outputs, with an\nemphasis on assessing the factual accuracy of generated summaries. We find that\nwhile GPT-3 is able to summarize and simplify single biomedical articles\nfaithfully, it struggles to provide accurate aggregations of findings over\nmultiple documents. We release all data and annotations used in this work.", "published": "2023-05-10 16:40:37", "link": "http://arxiv.org/abs/2305.06299v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation of Attribution by Large Language Models", "abstract": "A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support its claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is fully supported by the cited reference,\nremains an open problem. Although human evaluation is common practice, it is\ncostly and time-consuming. In this paper, we investigate the automatic\nevaluation of attribution given by LLMs. We begin by defining different types\nof attribution errors, and then explore two approaches for automatic\nevaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is\nrepurposed from related tasks such as question answering, fact-checking,\nnatural language inference, and summarization. We manually curate a set of test\nexamples covering 12 domains from a generative search engine, New Bing. Our\nresults on this curated test set and simulated examples from existing\nbenchmarks highlight both promising signals and challenges. We hope our problem\nformulation, testbeds, and findings will help lay the foundation for future\nstudies on this important problem.", "published": "2023-05-10 16:58:33", "link": "http://arxiv.org/abs/2305.06311v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Korean Named Entity Recognition Based on Language-Specific Features", "abstract": "In the paper, we propose a novel way of improving named entity recognition in\nthe Korean language using its language-specific features. While the field of\nnamed entity recognition has been studied extensively in recent years, the\nmechanism of efficiently recognizing named entities in Korean has hardly been\nexplored. This is because the Korean language has distinct linguistic\nproperties that prevent models from achieving their best performances.\nTherefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-U\nformat, which decomposes Korean words into morphemes and reduces the ambiguity\nof named entities in the original segmentation that may contain functional\nmorphemes such as postpositions and particles, is proposed herein. We\ninvestigate how the named entity tags are best represented in this\nmorpheme-based scheme and implement an algorithm to convert word-based {and\nsyllable-based Korean corpora} with named entities into the proposed\nmorpheme-based format. Analyses of the results of {statistical and neural}\nmodels reveal that the proposed morpheme-based format is feasible, and the\n{varied} performances of the models under the influence of various additional\nlanguage-specific features are demonstrated. Extrinsic conditions were also\nconsidered to observe the variance of the performances of the proposed models,\ngiven different types of data, including the original segmentation and\ndifferent types of tagging formats.", "published": "2023-05-10 17:34:52", "link": "http://arxiv.org/abs/2305.06330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-UniMorph: Korean Universal Morphology and its Feature Schema", "abstract": "We present in this work a new Universal Morphology dataset for Korean.\nPreviously, the Korean language has been underrepresented in the field of\nmorphological paradigms amongst hundreds of diverse world languages. Hence, we\npropose this Universal Morphological paradigms for the Korean language that\npreserve its distinct characteristics. For our K-UniMorph dataset, we outline\neach grammatical criterion in detail for the verbal endings, clarify how to\nextract inflected forms, and demonstrate how we generate the morphological\nschemata. This dataset adopts morphological feature schema from Sylak-Glassman\net al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract\ninflected verb forms from the Sejong morphologically analyzed corpus that is\none of the largest annotated corpora for Korean. During the data creation, our\nmethodology also includes investigating the correctness of the conversion from\nthe Sejong corpus. Furthermore, we carry out the inflection task using three\ndifferent Korean word forms: letters, syllables and morphemes. Finally, we\ndiscuss and describe future perspectives on Korean morphological paradigms and\nthe dataset.", "published": "2023-05-10 17:44:01", "link": "http://arxiv.org/abs/2305.06335v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question", "abstract": "Large language models (LLMs) like GPT-4 have recently demonstrated impressive\ncapabilities in natural language understanding and generation. However, there\nis a concern that they can be misused for malicious purposes, such as fraud or\ndenial-of-service attacks. Therefore, it is crucial to develop methods for\ndetecting whether the party involved in a conversation is a bot or a human. In\nthis paper, we propose a framework named FLAIR, Finding Large Language Model\nAuthenticity via a Single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, searching, and ASCII art reasoning), and those\nthat are easy for bots but difficult for humans (e.g., memorization and\ncomputation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities. Our code and question set are\navailable at https://github.com/hongwang600/FLAIR.", "published": "2023-05-10 19:09:24", "link": "http://arxiv.org/abs/2305.06424v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bits of Grass: Does GPT already know how to write like Whitman?", "abstract": "This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4\nmodels to generate poems in the style of specific authors using zero-shot and\nmany-shot prompts (which use the maximum context length of 8192 tokens). We\nassess the performance of models that are not fine-tuned for generating poetry\nin the style of specific authors, via automated evaluation. Our findings\nindicate that without fine-tuning, even when provided with the maximum number\nof 17 poem examples (8192 tokens) in the prompt, these models do not generate\npoetry in the desired style.", "published": "2023-05-10 09:02:34", "link": "http://arxiv.org/abs/2305.11064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Talking with Machines: A Comprehensive Survey of Emergent Dialogue\n  Systems", "abstract": "From the earliest experiments in the 20th century to the utilization of large\nlanguage models and transformers, dialogue systems research has continued to\nevolve, playing crucial roles in numerous fields. This paper offers a\ncomprehensive review of these systems, tracing their historical development and\nexamining their fundamental operations. We analyze popular and emerging\ndatasets for training and survey key contributions in dialogue systems\nresearch, including traditional systems and advanced machine learning methods.\nFinally, we consider conventional and transformer-based evaluation metrics,\nfollowed by a short discussion of prevailing challenges and future prospects in\nthe field.", "published": "2023-05-10 12:24:03", "link": "http://arxiv.org/abs/2305.16324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-dependent communication under environmental constraints", "abstract": "There is significant evidence that real-world communication cannot be reduced\nto sending signals with context-independent meaning. In this work, based on a\nvariant of the classical Lewis (1969) signaling model, we explore the\nconditions for the emergence of context-dependent communication in a situated\nscenario. In particular, we demonstrate that pressure to minimise the\nvocabulary size is sufficient for such emergence. At the same time, we study\nthe environmental conditions and cognitive capabilities that enable contextual\ndisambiguation of symbol meanings. We show that environmental constraints on\nthe receiver's referent choice can be unilaterally exploited by the sender,\nwithout disambiguation capabilities on the receiver's end. Consistent with\ncommon assumptions, the sender's awareness of the context appears to be\nrequired for contextual communication. We suggest that context-dependent\ncommunication is a situated multilayered phenomenon, crucially influenced by\nenvironment properties such as distribution of contexts. The model developed in\nthis work is a demonstration of how signals may be ambiguous out of context,\nbut still allow for near-perfect communication accuracy.", "published": "2023-05-10 00:33:08", "link": "http://arxiv.org/abs/2305.05821v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unsupervised Dense Retrieval Training with Web Anchors", "abstract": "In this work, we present an unsupervised retrieval method with contrastive\nlearning on web anchors. The anchor text describes the content that is\nreferenced from the linked page. This shows similarities to search queries that\naim to retrieve pertinent information from relevant documents. Based on their\ncommonalities, we train an unsupervised dense retriever, Anchor-DR, with a\ncontrastive learning task that matches the anchor text and the linked document.\nTo filter out uninformative anchors (such as ``homepage'' or other functional\nanchors), we present a novel filtering technique to only select anchors that\ncontain similar types of information as search queries. Experiments show that\nAnchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval\nby a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is\nespecially significant for search and question answering tasks. Our analysis\nfurther reveals that the pattern of anchor-document pairs is similar to that of\nsearch query-document pairs. Code available at\nhttps://github.com/Veronicium/AnchorDR.", "published": "2023-05-10 01:46:17", "link": "http://arxiv.org/abs/2305.05834v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text\n  Analytics? A Study on Several Typical Tasks", "abstract": "The most recent large language models(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of generalist models, achieving state-of-the-art\nperformance on a wide range of NLP tasks with little or no adaptation. How\neffective are such models in the financial domain? Understanding this basic\nquestion would have a significant impact on many downstream financial\nanalytical tasks. In this paper, we conduct an empirical study and provide\nexperimental evidences of their performance on a wide variety of financial text\nanalytical problems, using eight benchmark datasets from five categories of\ntasks. We report both the strengths and limitations of the current models by\ncomparing them to the state-of-the-art fine-tuned approaches and the recently\nreleased domain-specific pretrained models. We hope our study can help\nunderstand the capability of the existing models in the financial domain and\nfacilitate further improvements.", "published": "2023-05-10 03:13:54", "link": "http://arxiv.org/abs/2305.05862v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Path Transformer is Better: A Case Study on Neural Machine\n  Translation", "abstract": "For years the model performance in machine learning obeyed a power-law\nrelationship with the model size. For the consideration of parameter\nefficiency, recent studies focus on increasing model depth rather than width to\nachieve better performance. In this paper, we study how model width affects the\nTransformer model through a parameter-efficient multi-path structure. To better\nfuse features extracted from different paths, we add three additional\noperations to each sublayer: a normalization at the end of each path, a cheap\noperation to produce more features, and a learnable weighted mechanism to fuse\nall features flexibly. Extensive experiments on 12 WMT machine translation\ntasks show that, with the same number of parameters, the shallower multi-path\nmodel can achieve similar or even better performance than the deeper model. It\nreveals that we should pay more attention to the multi-path structure, and\nthere should be a balance between the model depth and width to train a better\nlarge-scale Transformer.", "published": "2023-05-10 07:39:57", "link": "http://arxiv.org/abs/2305.05948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A\n  Million-scale Knowledge Base", "abstract": "Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large language models (LLMs), followed by minor human\nefforts for data quality control. Evaluations on a series of datasets of two\nanalogical reasoning tasks (analogy recognition and generation) demonstrate\nthat ANALOGYKB successfully enables both smaller LMs and LLMs to gain better\nanalogical reasoning capabilities.", "published": "2023-05-10 09:03:01", "link": "http://arxiv.org/abs/2305.05994v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or\n  Modelling Perspectives?", "abstract": "There are two competing approaches for modelling annotator disagreement:\ndistributional soft-labelling approaches (which aim to capture the level of\ndisagreement) or modelling perspectives of individual annotators or groups\nthereof. We adapt a multi-task architecture -- which has previously shown\nsuccess in modelling perspectives -- to evaluate its performance on the SEMEVAL\nTask 11. We do so by combining both approaches, i.e. predicting individual\nannotator perspectives as an interim step towards predicting annotator\ndisagreement. Despite its previous success, we found that a multi-task approach\nperformed poorly on datasets which contained distinct annotator opinions,\nsuggesting that this approach may not always be suitable when modelling\nperspectives. Furthermore, our results explain that while strongly\nperspectivist approaches might not achieve state-of-the-art performance\naccording to evaluation metrics used by distributional approaches, our approach\nallows for a more nuanced understanding of individual perspectives present in\nthe data. We argue that perspectivist approaches are preferable because they\nenable decision makers to amplify minority views, and that it is important to\nre-evaluate metrics to reflect this goal.", "published": "2023-05-10 11:55:17", "link": "http://arxiv.org/abs/2305.06074v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PAI at SemEval-2023 Task 2: A Universal System for Named Entity\n  Recognition with External Entity Information", "abstract": "The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained\nnamed entities in low-context situations and noisy scenarios like the presence\nof spelling mistakes and typos for multiple languages. The task poses\nsignificant challenges due to the scarcity of contextual information, the high\ngranularity of the entities(up to 33 classes), and the interference of noisy\ndata. To address these issues, our team {\\bf PAI} proposes a universal Named\nEntity Recognition (NER) system that integrates external entity information to\nimprove performance. Specifically, our system retrieves entities with\nproperties from the knowledge base (i.e. Wikipedia) for a given text, then\nconcatenates entity information with the input sentence and feeds it into\nTransformer-based models. Finally, our system wins 2 first places, 4 second\nplaces, and 1 third place out of 13 tracks. The code is publicly available at\n\\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.", "published": "2023-05-10 12:40:48", "link": "http://arxiv.org/abs/2305.06099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Prompt Tuning for Large Language Model Services", "abstract": "Prompt tuning provides an efficient way for users to customize Large Language\nModels (LLMs) with their private data in the emerging LLM service scenario.\nHowever, the sensitive nature of private data brings the need for privacy\npreservation in LLM service customization. Based on prompt tuning, we propose\nPrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy\nguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,\nallowing users to privatize their data locally with local differential privacy.\nAs prompt tuning performs poorly when directly trained on privatized data, we\nintroduce a novel privatized token reconstruction task that is trained jointly\nwith the downstream task, allowing LLMs to learn better task-dependent\nrepresentations. Despite the simplicity of our framework, experiments show that\nRAPT achieves competitive performance across tasks while providing privacy\nguarantees against adversaries.", "published": "2023-05-10 14:41:51", "link": "http://arxiv.org/abs/2305.06212v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Evaluating Embedding APIs for Information Retrieval", "abstract": "The ever-increasing size of language models curtails their widespread\navailability to the community, thereby galvanizing many companies into offering\naccess to large language models through APIs. One particular type, suitable for\ndense retrieval, is a semantic embedding service that builds vector\nrepresentations of input text. With a growing number of publicly available\nAPIs, our goal in this paper is to analyze existing offerings in realistic\nretrieval scenarios, to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we investigate the\ncapabilities of existing semantic embedding APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate these services on two\nstandard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective in English,\nin contrast to the standard practice of employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best, albeit at a higher cost. We hope our\nwork lays the groundwork for evaluating semantic embedding APIs that are\ncritical in search and more broadly, for information access.", "published": "2023-05-10 16:40:52", "link": "http://arxiv.org/abs/2305.06300v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "VideoChat: Chat-Centric Video Understanding", "abstract": "In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything", "published": "2023-05-10 17:59:04", "link": "http://arxiv.org/abs/2305.06355v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits\n  Siamese-BLOOM", "abstract": "Text embeddings are useful features for several NLP applications, such as\nsentence similarity, text clustering, and semantic search. In this paper, we\npresent a Low-rank Adaptation with a Contrastive objective on top of 8-bit\nSiamese-BLOOM, a multilingual large language model optimized to produce\nsemantically meaningful word embeddings. The innovation is threefold. First, we\ncast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable\nadapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.\nThird, we apply a Siamese architecture on BLOOM model with a contrastive\nobjective to ease the multi-lingual labeled data scarcity. The experiment\nresults show the quality of learned embeddings from LACoS-BLOOM is proportional\nto the number of model parameters and the amount of unlabeled training data.\nWith the parameter efficient fine-tuning design, we are able to run BLOOM 7.1\nbillion parameters end-to-end on a single GPU machine with 32GB memory.\nCompared to previous solution Sentence-BERT, we achieve significant improvement\non both English and multi-lingual STS tasks.", "published": "2023-05-10 18:26:42", "link": "http://arxiv.org/abs/2305.06404v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Method to Automate the Discharge Summary Hospital Course for Neurology\n  Patients", "abstract": "Generation of automated clinical notes have been posited as a strategy to\nmitigate physician burnout. In particular, an automated narrative summary of a\npatient's hospital stay could supplement the hospital course section of the\ndischarge summary that inpatient physicians document in electronic health\nrecord (EHR) systems. In the current study, we developed and evaluated an\nautomated method for summarizing the hospital course section using\nencoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and\nBART models and optimized for factuality through constraining beam search,\nwhich we trained and tested using EHR data from patients admitted to the\nneurology unit of an academic medical center. The approach demonstrated good\nROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified\nphysicians rated 62% of the automated summaries as meeting the standard of\ncare, which suggests the method may be useful clinically. To our knowledge,\nthis study is among the first to demonstrate an automated method for generating\na discharge summary hospital course that approaches a quality level of what a\nphysician would write.", "published": "2023-05-10 18:53:51", "link": "http://arxiv.org/abs/2305.06416v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Grounded Graph Convolutional Network", "abstract": "Graph Convolutional Networks (GCNs) have shown strong performance in learning\ntext representations for various tasks such as text classification, due to its\nexpressive power in modeling graph structure data (e.g., a literature citation\nnetwork). Most existing GCNs are limited to deal with documents included in a\npre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To\naddress this issue, we propose to transform the document graph into a word\ngraph, to decouple data samples (i.e., documents in training and test sets) and\na GCN model by using a document-independent graph. Such word-level GCN could\ntherefore naturally inference out-of-graph documents in an inductive way. The\nproposed Word-level Graph (WGraph) can not only implicitly learning word\npresentation with commonly-used word co-occurrences in corpora, but also\nincorporate extra global semantic dependency derived from inter-document\nrelationships (e.g., literature citations). An inductive Word-grounded Graph\nConvolutional Network (WGCN) is proposed to learn word and document\nrepresentations based on WGraph in a supervised manner. Experiments on text\nclassification with and without citation networks evidence that the proposed\nWGCN model outperforms existing methods in terms of effectiveness and\nefficiency.", "published": "2023-05-10 19:56:55", "link": "http://arxiv.org/abs/2305.06434v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a\n  Knowledge Graph", "abstract": "The purpose of this work is to describe the Orkg-Leaderboard software\ndesigned to extract leaderboards defined as Task-Dataset-Metric tuples\nautomatically from large collections of empirical research papers in Artificial\nIntelligence (AI). The software can support both the main workflows of\nscholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the\nsystem is integrated with the Open Research Knowledge Graph (ORKG) platform,\nwhich fosters the machine-actionable publishing of scholarly findings. Thus the\nsystem output, when integrated within the ORKG's supported Semantic Web\ninfrastructure of representing machine-actionable 'resources' on the Web,\nenables: 1) broadly, the integration of empirical results of researchers across\nthe world, thus enabling transparency in empirical research with the potential\nto also being complete contingent on the underlying data source(s) of\npublications; and 2) specifically, enables researchers to track the progress in\nAI with an overview of the state-of-the-art (SOTA) across the most common AI\ntasks and their corresponding datasets via dynamic ORKG frontend views\nleveraging tables and visualization charts over the machine-actionable data.\nOur best model achieves performances above 90% F1 on the \\textit{leaderboard}\nextraction task, thus proving Orkg-Leaderboards a practically viable tool for\nreal-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the\nleaderboard extraction task to an automated digitalization task, which has\nbeen, for a long time in the community, a crowdsourced endeavor.", "published": "2023-05-10 13:19:18", "link": "http://arxiv.org/abs/2305.11068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System", "abstract": "This technical paper introduces a chatting robot system that utilizes recent\nadvancements in large-scale language models (LLMs) such as GPT-3 and ChatGPT.\nThe system is integrated with a co-speech gesture generation system, which\nselects appropriate gestures based on the conceptual meaning of speech. Our\nmotivation is to explore ways of utilizing the recent progress in LLMs for\npractical robotic applications, which benefits the development of both chatbots\nand LLMs. Specifically, it enables the development of highly responsive chatbot\nsystems by leveraging LLMs and adds visual effects to the user interface of\nLLMs as an additional value. The source code for the system is available on\nGitHub for our in-house robot\n(https://github.com/microsoft/LabanotationSuite/tree/master/MSRAbotChatSimulation)\nand GitHub for Toyota HSR\n(https://github.com/microsoft/GPT-Enabled-HSR-CoSpeechGestures).", "published": "2023-05-10 10:14:16", "link": "http://arxiv.org/abs/2306.01741v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech\n  Detection", "abstract": "Health experts assert that hope plays a crucial role in enhancing\nindividuals' physical and mental well-being, facilitating their recovery, and\npromoting restoration. Hope speech refers to comments, posts and other social\nmedia messages that offer support, reassurance, suggestions, inspiration, and\ninsight. The detection of hope speech involves the analysis of such textual\ncontent, with the aim of identifying messages that invoke positive emotions in\npeople. Our study aims to find computationally efficient yet\ncomparable/superior methods for hope speech detection. We also make our\ncodebase public at https://github.com/aflah02/Hope_Speech_Detection", "published": "2023-05-10 18:38:48", "link": "http://arxiv.org/abs/2306.01742v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Multimodal Misinformation Detection with Logic Reasoning", "abstract": "Multimodal misinformation on online social platforms is becoming a critical\nconcern due to increasing credibility and easier dissemination brought by\nmultimedia content, compared to traditional text-only information. While\nexisting multimodal detection approaches have achieved high performance, the\nlack of interpretability hinders these systems' reliability and practical\ndeployment. Inspired by NeuralSymbolic AI which combines the learning ability\nof neural networks with the explainability of symbolic learning, we propose a\nnovel logic-based neural model for multimodal misinformation detection which\nintegrates interpretable logic clauses to express the reasoning process of the\ntarget task. To make learning effective, we parameterize symbolic logical\nelements using neural representations, which facilitate the automatic\ngeneration and evaluation of meaningful logic clauses. Additionally, to make\nour framework generalizable across diverse misinformation sources, we introduce\nfive meta-predicates that can be instantiated with different correlations.\nResults on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the\nfeasibility and versatility of our model.", "published": "2023-05-10 08:16:36", "link": "http://arxiv.org/abs/2305.05964v2", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems\n  using Differentially Private Language Models", "abstract": "We address the challenge of ensuring differential privacy (DP) guarantees in\ntraining deep retrieval systems. Training these systems often involves the use\nof contrastive-style losses, which are typically non-per-example decomposable,\nmaking them difficult to directly DP-train with since common techniques require\nper-example gradients. To address this issue, we propose an approach that\nprioritizes ensuring query privacy prior to training a deep retrieval system.\nOur method employs DP language models (LMs) to generate private synthetic\nqueries representative of the original data. These synthetic queries can be\nused in downstream retrieval system training without compromising privacy. Our\napproach demonstrates a significant enhancement in retrieval quality compared\nto direct DP-training, all while maintaining query-level privacy guarantees.\nThis work highlights the potential of harnessing LMs to overcome limitations in\nstandard DP-training methods.", "published": "2023-05-10 08:30:31", "link": "http://arxiv.org/abs/2305.05973v3", "categories": ["cs.CL", "cs.CR", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generating medically-accurate summaries of patient-provider dialogue: A\n  multi-stage approach using large language models", "abstract": "A medical provider's summary of a patient visit serves several critical\npurposes, including clinical decision-making, facilitating hand-offs between\nproviders, and as a reference for the patient. An effective summary is required\nto be coherent and accurately capture all the medically relevant information in\nthe dialogue, despite the complexity of patient-generated language. Even minor\ninaccuracies in visit summaries (for example, summarizing \"patient does not\nhave a fever\" when a fever is present) can be detrimental to the outcome of\ncare for the patient.\n  This paper tackles the problem of medical conversation summarization by\ndiscretizing the task into several smaller dialogue-understanding tasks that\nare sequentially built upon. First, we identify medical entities and their\naffirmations within the conversation to serve as building blocks. We study\ndynamically constructing few-shot prompts for tasks by conditioning on relevant\npatient information and use GPT-3 as the backbone for our experiments. We also\ndevelop GPT-derived summarization metrics to measure performance against\nreference summaries quantitatively. Both our human evaluation study and metrics\nfor medical correctness show that summaries generated using this approach are\nclinically accurate and outperform the baseline approach of summarizing the\ndialog in a zero-shot, single-prompt setting.", "published": "2023-05-10 08:48:53", "link": "http://arxiv.org/abs/2305.05982v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RECKONING: Reasoning through Dynamic Knowledge Encoding", "abstract": "Recent studies on transformer-based language models show that they can answer\nquestions by reasoning over knowledge provided as part of the context (i.e.,\nin-context reasoning). However, since the available knowledge is often not\nfiltered for a particular question, in-context reasoning can be sensitive to\ndistractor facts, additional content that is irrelevant to a question but that\nmay be relevant for a different question (i.e., not necessarily random noise).\nIn these situations, the model fails to distinguish the knowledge that is\nnecessary to answer the question, leading to spurious reasoning and degraded\nperformance. This reasoning failure contrasts with the model's apparent ability\nto distinguish its contextual knowledge from all the knowledge it has memorized\nduring pre-training. Following this observation, we propose teaching the model\nto reason more robustly by folding the provided contextual knowledge into the\nmodel's parameters before presenting it with a question. Our method, RECKONING,\nis a bi-level learning algorithm that teaches language models to reason by\nupdating their parametric knowledge through back-propagation, allowing them to\nthen answer questions using the updated parameters. During training, the inner\nloop rapidly adapts a copy of the model weights to encode contextual knowledge\ninto its parameters. In the outer loop, the model learns to use the updated\nweights to reproduce and answer reasoning questions about the memorized\nknowledge. Our experiments on two multi-hop reasoning datasets show that\nRECKONING's performance improves over the in-context reasoning baseline (by up\nto 4.5%). We also find that compared to in-context reasoning, RECKONING\ngeneralizes better to longer reasoning chains unseen during training, is more\nrobust to distractors in the context, and is more computationally efficient\nwhen multiple questions are asked about the same knowledge.", "published": "2023-05-10 17:54:51", "link": "http://arxiv.org/abs/2305.06349v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health\n  Management: A Survey and Roadmaps", "abstract": "Prognostics and health management (PHM) technology plays a critical role in\nindustrial production and equipment maintenance by identifying and predicting\npossible equipment failures and damages, thereby allowing necessary maintenance\nmeasures to be taken to enhance equipment service life and reliability while\nreducing production costs and downtime. In recent years, PHM technology based\non artificial intelligence (AI) has made remarkable achievements in the context\nof the industrial IoT and big data, and it is widely used in various\nindustries, such as railway, energy, and aviation, for condition monitoring,\nfault prediction, and health management. The emergence of large-scale\nfoundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of\nAI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved\nfrom a research paradigm of single-modal, single-task, and limited-data to a\nmulti-modal, multi-task, massive data, and super-large model paradigm. ChatGPT\nrepresents a landmark achievement in this research paradigm, offering hope for\ngeneral artificial intelligence due to its highly intelligent natural language\nunderstanding ability. However, the PHM field lacks a consensus on how to\nrespond to this significant change in the AI field, and a systematic review and\nroadmap is required to elucidate future development directions. To fill this\ngap, this paper systematically expounds on the key components and latest\ndevelopments of LSF-Models. Then, we systematically answered how to build the\nLSF-Model applicable to PHM tasks and outlined the challenges and future\ndevelopment roadmaps for this research paradigm.", "published": "2023-05-10 21:37:44", "link": "http://arxiv.org/abs/2305.06472v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Contextualized Plan Prediction for Embodied Task Completion", "abstract": "Task planning is an important component of traditional robotics systems\nenabling robots to compose fine grained skills to perform more complex tasks.\nRecent work building systems for translating natural language to executable\nactions for task completion in simulated embodied agents is focused on directly\npredicting low level action sequences that would be expected to be directly\nexecutable by a physical robot. In this work, we instead focus on predicting a\nhigher level plan representation for one such embodied task completion dataset\n- TEACh, under the assumption that techniques for high-level plan prediction\nfrom natural language are expected to be more transferable to physical robot\nsystems. We demonstrate that better plans can be predicted using multimodal\ncontext, and that plan prediction and plan execution modules are likely\ndependent on each other and hence it may not be ideal to fully decouple them.\nFurther, we benchmark execution of oracle plans to quantify the scope for\nimprovement in plan prediction models.", "published": "2023-05-10 22:29:12", "link": "http://arxiv.org/abs/2305.06485v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation", "abstract": "Converting text into the structured query language (Text2SQL) is a research\nhotspot in the field of natural language processing (NLP), which has broad\napplication prospects. In the era of big data, the use of databases has\npenetrated all walks of life, in which the collected data is large in scale,\ndiverse in variety, and wide in scope, making the data query cumbersome and\ninefficient, and putting forward higher requirements for the Text2SQL model. In\npractical applications, the current mainstream end-to-end Text2SQL model is not\nonly difficult to build due to its complex structure and high requirements for\ntraining data, but also difficult to adjust due to massive parameters. In\naddition, the accuracy of the model is hard to achieve the desired result.\nBased on this, this paper proposes a pipelined Text2SQL method: SPSQL. This\nmethod disassembles the Text2SQL task into four subtasks--table selection,\ncolumn selection, SQL generation, and value filling, which can be converted\ninto a text classification problem, a sequence labeling problem, and two text\ngeneration problems, respectively. Then, we construct data formats of different\nsubtasks based on existing data and improve the accuracy of the overall model\nby improving the accuracy of each submodel. We also use the named entity\nrecognition module and data augmentation to optimize the overall model. We\nconstruct the dataset based on the marketing business data of the State Grid\nCorporation of China. Experiments demonstrate our proposed method achieves the\nbest performance compared with the end-to-end method and other pipeline\nmethods.", "published": "2023-05-10 10:01:36", "link": "http://arxiv.org/abs/2305.11061v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Enriching language models with graph-based context information to better\n  understand textual data", "abstract": "A considerable number of texts encountered daily are somehow connected with\neach other. For example, Wikipedia articles refer to other articles via\nhyperlinks, scientific papers relate to others via citations or (co)authors,\nwhile tweets relate via users that follow each other or reshare content. Hence,\na graph-like structure can represent existing connections and be seen as\ncapturing the \"context\" of the texts. The question thus arises if extracting\nand integrating such context information into a language model might help\nfacilitate a better automated understanding of the text. In this study, we\nexperimentally demonstrate that incorporating graph-based contextualization\ninto BERT model enhances its performance on an example of a classification\ntask. Specifically, on Pubmed dataset, we observed a reduction in error from\n8.51% to 7.96%, while increasing the number of parameters just by 1.6%.\n  Our source code: https://github.com/tryptofanik/gc-bert", "published": "2023-05-10 10:57:21", "link": "http://arxiv.org/abs/2305.11070v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A systematic evaluation of large language models for biomedical natural\n  language processing: benchmarks, baselines, and recommendations", "abstract": "The biomedical literature is rapidly expanding, posing a significant\nchallenge for manual curation and knowledge discovery. Biomedical Natural\nLanguage Processing (BioNLP) has emerged as a powerful solution, enabling the\nautomated extraction of information and knowledge from this extensive\nliterature. Recent attention has been directed towards Large Language Models\n(LLMs) due to their impressive performance. However, there remains a critical\ngap in understanding the effectiveness of LLMs in BioNLP tasks and their\nbroader implications for method development and downstream users. Currently,\nthere is a lack of baseline performance data, benchmarks, and practical\nrecommendations for using LLMs in the biomedical domain. To address this gap,\nwe present a systematic evaluation of four representative LLMs: GPT-3.5 and\nGPT-4 (closed-source), LLaMA 2 (open-sourced), and PMC LLaMA (domain-specific)\nacross 12 BioNLP datasets covering six applications (named entity recognition,\nrelation extraction, multi-label document classification, question answering,\ntext summarization, and text simplification). The evaluation is conducted under\nfour settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and\nfine-tuning. We compare these models against state-of-the-art (SOTA) approaches\nthat fine-tune (domain-specific) BERT or BART models, which are\nwell-established methods in BioNLP tasks. The evaluation covers both\nquantitative and qualitative evaluations, where the latter involves manually\nreviewing collectively hundreds of thousands of LLM outputs for\ninconsistencies, missing information, and hallucinations in extractive and\nclassification tasks. The qualitative review also examines accuracy, 1\ncompleteness, and readability in text summarization tasks. Additionally, a cost\nanalysis of closed-source GPT models is conducted.", "published": "2023-05-10 13:40:06", "link": "http://arxiv.org/abs/2305.16326v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Glimpse in ChatGPT Capabilities and its impact for AI research", "abstract": "Large language models (LLMs) have recently become a popular topic in the\nfield of Artificial Intelligence (AI) research, with companies such as Google,\nAmazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their\ndevelopment. These models are trained on massive amounts of data and can be\nused for a wide range of tasks, including language translation, text\ngeneration, and question answering. However, the computational resources\nrequired to train and run these models are substantial, and the cost of\nhardware and electricity can be prohibitive for research labs that do not have\nthe funding and resources of the GAFA. In this paper, we will examine the\nimpact of LLMs on AI research. The pace at which such models are generated as\nwell as the range of domains covered is an indication of the trend which not\nonly the public but also the scientific community is currently experiencing. We\ngive some examples on how to use such models in research by focusing on\nGPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range\nof capabilities in a single system is a strong sign of approaching general\nintelligence. Innovations integrating such models will also expand along the\nmaturation of such AI systems and exhibit unforeseeable applications that will\nhave important impacts on several aspects of our societies.", "published": "2023-05-10 12:10:51", "link": "http://arxiv.org/abs/2305.06087v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2; I.7"], "primary_category": "cs.AI"}
{"title": "Mispronunciation Detection of Basic Quranic Recitation Rules using Deep\n  Learning", "abstract": "In Islam, readers must apply a set of pronunciation rules called Tajweed\nrules to recite the Quran in the same way that the angel Jibrael taught the\nProphet, Muhammad. The traditional process of learning the correct application\nof these rules requires a human who must have a license and great experience to\ndetect mispronunciation. Due to the increasing number of Muslims around the\nworld, the number of Tajweed teachers is not enough nowadays for daily\nrecitation practice for every Muslim. Therefore, lots of work has been done for\nautomatic Tajweed rules' mispronunciation detection to help readers recite\nQuran correctly in an easier way and shorter time than traditional learning\nways. All previous works have three common problems. First, most of them\nfocused on machine learning algorithms only. Second, they used private datasets\nwith no benchmark to compare with. Third, they did not take into consideration\nthe sequence of input data optimally, although the speech signal is time\nseries. To overcome these problems, we proposed a solution that consists of\nMel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory\n(LSTM) neural networks which use the time series, to detect mispronunciation in\nTajweed rules. In addition, our experiments were performed on a public dataset,\nthe QDAT dataset, which contains more than 1500 voices of the correct and\nincorrect recitation of three Tajweed rules (Separate stretching , Tight Noon ,\nand Hide ). To the best of our knowledge, the QDAT dataset has not been used by\nany research paper yet. We compared the performance of the proposed LSTM model\nwith traditional machine learning algorithms used in SoTA. The LSTM model with\ntime series showed clear superiority over traditional machine learning. The\naccuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for the\nthree rules (Separate stretching, Tight Noon, and Hide), respectively.", "published": "2023-05-10 19:31:25", "link": "http://arxiv.org/abs/2305.06429v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quran Recitation Recognition using End-to-End Deep Learning", "abstract": "The Quran is the holy scripture of Islam, and its recitation is an important\naspect of the religion. Recognizing the recitation of the Holy Quran\nautomatically is a challenging task due to its unique rules that are not\napplied in normal speaking speeches. A lot of research has been done in this\ndomain, but previous works have detected recitation errors as a classification\ntask or used traditional automatic speech recognition (ASR). In this paper, we\nproposed a novel end-to-end deep learning model for recognizing the recitation\nof the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that\nuses CTC as an objective function, and a character-based decoder which is a\nbeam search decoder. Moreover, all previous works were done on small private\ndatasets consisting of short verses and a few chapters of the Holy Quran. As a\nresult of using private datasets, no comparisons were done. To overcome this\nissue, we used a public dataset that has recently been published (Ar-DAD) and\ncontains about 37 chapters that were recited by 30 reciters, with different\nrecitation speeds and different types of pronunciation rules. The proposed\nmodel performance was evaluated using the most common evaluation metrics in\nspeech recognition, word error rate (WER), and character error rate (CER). The\nresults were 8.34% WER and 2.42% CER. We hope this research will be a baseline\nfor comparisons with future research on this public new dataset (Ar-DAD).", "published": "2023-05-10 18:40:01", "link": "http://arxiv.org/abs/2305.07034v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition", "abstract": "Since the release of ChatGPT, numerous studies have highlighted the\nremarkable performance of ChatGPT, which often rivals or even surpasses human\ncapabilities in various tasks and domains. However, this paper presents a\ncontrasting perspective by demonstrating an instance where human performance\nexcels in typical tasks suited for ChatGPT, specifically in the domain of\ncomputer programming. We utilize the IEEExtreme Challenge competition as a\nbenchmark, a prestigious, annual international programming contest encompassing\na wide range of problems with different complexities. To conduct a thorough\nevaluation, we selected and executed a diverse set of 102 challenges, drawn\nfrom five distinct IEEExtreme editions, using three major programming\nlanguages: Python, Java, and C++. Our empirical analysis provides evidence that\ncontrary to popular belief, human programmers maintain a competitive edge over\nChatGPT in certain aspects of problem-solving within the programming context.\nIn fact, we found that the average score obtained by ChatGPT on the set of\nIEEExtreme programming problems is 3.9 to 5.8 times lower than the average\nhuman score, depending on the programming language. This paper elaborates on\nthese findings, offering critical insights into the limitations and potential\nareas of improvement for AI-based language models like ChatGPT.", "published": "2023-05-10 08:16:46", "link": "http://arxiv.org/abs/2305.06934v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Diffusion-based Signal Refiner for Speech Separation", "abstract": "We have developed a diffusion-based speech refiner that improves the\nreference-free perceptual quality of the audio predicted by preceding\nsingle-channel speech separation models. Although modern deep neural\nnetwork-based speech separation models have show high performance in\nreference-based metrics, they often produce perceptually unnatural artifacts.\nThe recent advancements made to diffusion models motivated us to tackle this\nproblem by restoring the degraded parts of initial separations with a\ngenerative approach. Utilizing the denoising diffusion restoration model (DDRM)\nas a basis, we propose a shared DDRM-based refiner that generates samples\nconditioned on the global information of preceding outputs from arbitrary\nspeech separation models. We experimentally show that our refiner can provide a\nclearer harmonic structure of speech and improves the reference-free metric of\nperceptual quality for arbitrary preceding model architectures. Furthermore, we\ntune the variance of the measurement noise based on preceding outputs, which\nresults in higher scores in both reference-free and reference-based metrics.\nThe separation quality can also be further improved by blending the\ndiscriminative and generative outputs.", "published": "2023-05-10 03:06:16", "link": "http://arxiv.org/abs/2305.05857v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
