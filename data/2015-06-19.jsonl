{"title": "A Neural Conversational Model", "abstract": "Conversational modeling is an important task in natural language\nunderstanding and machine intelligence. Although previous approaches exist,\nthey are often restricted to specific domains (e.g., booking an airline ticket)\nand require hand-crafted rules. In this paper, we present a simple approach for\nthis task which uses the recently proposed sequence to sequence framework. Our\nmodel converses by predicting the next sentence given the previous sentence or\nsentences in a conversation. The strength of our model is that it can be\ntrained end-to-end and thus requires much fewer hand-crafted rules. We find\nthat this straightforward model can generate simple conversations given a large\nconversational training dataset. Our preliminary results suggest that, despite\noptimizing the wrong objective function, the model is able to converse well. It\nis able extract knowledge from both a domain specific dataset, and from a\nlarge, noisy, and general domain dataset of movie subtitles. On a\ndomain-specific IT helpdesk dataset, the model can find a solution to a\ntechnical problem via conversations. On a noisy open-domain movie transcript\ndataset, the model can perform simple forms of common sense reasoning. As\nexpected, we also find that the lack of consistency is a common failure mode of\nour model.", "published": "2015-06-19 02:52:23", "link": "http://arxiv.org/abs/1506.05869v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Training for Neural Network Transition-Based Parsing", "abstract": "We present structured perceptron training for neural network transition-based\ndependency parsing. We learn the neural network representation using a gold\ncorpus augmented by a large number of automatically parsed sentences. Given\nthis fixed network representation, we learn a final layer using the structured\nperceptron with beam-search decoding. On the Penn Treebank, our parser reaches\n94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge\nis the best accuracy on Stanford Dependencies to date. We also provide in-depth\nablative analysis to determine which aspects of our model provide the largest\ngains in accuracy.", "published": "2015-06-19 21:05:01", "link": "http://arxiv.org/abs/1506.06158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "abstract": "Automatic text summarization is widely regarded as the highly difficult\nproblem, partially because of the lack of large text summarization data set.\nDue to the great challenge of constructing the large scale summaries for full\ntext, in this paper, we introduce a large corpus of Chinese short text\nsummarization dataset constructed from the Chinese microblogging website Sina\nWeibo, which is released to the public\n{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over\n2 million real Chinese short texts with short summaries given by the author of\neach text. We also manually tagged the relevance of 10,666 short summaries with\ntheir corresponding short texts. Based on the corpus, we introduce recurrent\nneural network for the summary generation and achieve promising results, which\nnot only shows the usefulness of the proposed corpus for short text\nsummarization research, but also provides a baseline for further research on\nthis topic.", "published": "2015-06-19 02:40:42", "link": "http://arxiv.org/abs/1506.05865v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
