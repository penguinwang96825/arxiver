{"title": "Reproducing and learning new algebraic operations on word embeddings\n  using genetic programming", "abstract": "Word-vector representations associate a high dimensional real-vector to every\nword from a corpus. Recently, neural-network based methods have been proposed\nfor learning this representation from large corpora. This type of\nword-to-vector embedding is able to keep, in the learned vector space, some of\nthe syntactic and semantic relationships present in the original word corpus.\nThis, in turn, serves to address different types of language classification\ntasks by doing algebraic operations defined on the vectors. The general\npractice is to assume that the semantic relationships between the words can be\ninferred by the application of a-priori specified algebraic operations. Our\ngeneral goal in this paper is to show that it is possible to learn methods for\nword composition in semantic spaces. Instead of expressing the compositional\nmethod as an algebraic operation, we will encode it as a program, which can be\nlinear, nonlinear, or involve more intricate expressions. More remarkably, this\nprogram will be evolved from a set of initial random programs by means of\ngenetic programming (GP). We show that our method is able to reproduce the same\nbehavior as human-designed algebraic operators. Using a word analogy task as\nbenchmark, we also show that GP-generated programs are able to obtain accuracy\nvalues above those produced by the commonly used human-designed rule for\nalgebraic manipulation of word vectors. Finally, we show the robustness of our\napproach by executing the evolved programs on the word2vec GoogleNews vectors,\nlearned over 3 billion running words, and assessing their accuracy in the same\nword analogy task.", "published": "2017-02-18 15:29:01", "link": "http://arxiv.org/abs/1702.05624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Stylometric Inquiry into Hyperpartisan and Fake News", "abstract": "This paper reports on a writing style analysis of hyperpartisan (i.e.,\nextremely one-sided) news in connection to fake news. It presents a large\ncorpus of 1,627 articles that were manually fact-checked by professional\njournalists from BuzzFeed. The articles originated from 9 well-known political\npublishers, 3 each from the mainstream, the hyperpartisan left-wing, and the\nhyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of\nwhich originated from hyperpartisan publishers.\n  We propose and demonstrate a new way of assessing style similarity between\ntext categories via Unmasking---a meta-learning approach originally devised for\nauthorship verification---, revealing that the style of left-wing and\nright-wing news have a lot more in common than any of the two have with the\nmainstream. Furthermore, we show that hyperpartisan news can be discriminated\nwell by its style from the mainstream (F1=0.78), as can be satire from both\n(F1=0.81). Unsurprisingly, style-based fake news detection does not live up to\nscratch (F1=0.46). Nevertheless, the former results are important to implement\npre-screening for fake news detectors.", "published": "2017-02-18 18:10:04", "link": "http://arxiv.org/abs/1702.05638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
