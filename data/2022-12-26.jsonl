{"title": "TextBox 2.0: A Text Generation Library with Pre-trained Language Models", "abstract": "To facilitate research on text generation, this paper presents a\ncomprehensive and unified library, TextBox 2.0, focusing on the use of\npre-trained language models (PLMs). To be comprehensive, our library covers\n$13$ common text generation tasks and their corresponding $83$ datasets and\nfurther incorporates $45$ PLMs covering general, translation, Chinese,\ndialogue, controllable, distilled, prompting, and lightweight PLMs. We also\nimplement $4$ efficient training strategies and provide $4$ generation\nobjectives for pre-training new PLMs from scratch. To be unified, we design the\ninterfaces to support the entire research pipeline (from data loading to\ntraining and evaluation), ensuring that each step can be fulfilled in a unified\nway. Despite the rich functionality, it is easy to use our library, either\nthrough the friendly Python API or command line. To validate the effectiveness\nof our library, we conduct extensive experiments and exemplify four types of\nresearch scenarios. The project is released at the link:\nhttps://github.com/RUCAIBox/TextBox.", "published": "2022-12-26 03:50:36", "link": "http://arxiv.org/abs/2212.13005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Encode Clinical Knowledge", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models' clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today's\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.", "published": "2022-12-26 14:28:24", "link": "http://arxiv.org/abs/2212.13138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biologically Inspired Design Concept Generation Using Generative\n  Pre-Trained Transformers", "abstract": "Biological systems in nature have evolved for millions of years to adapt and\nsurvive the environment. Many features they developed can be inspirational and\nbeneficial for solving technical problems in modern industries. This leads to a\nspecific form of design-by-analogy called bio-inspired design (BID). Although\nBID as a design method has been proven beneficial, the gap between biology and\nengineering continuously hinders designers from effectively applying the\nmethod. Therefore, we explore the recent advance of artificial intelligence\n(AI) for a data-driven approach to bridge the gap. This paper proposes a\ngenerative design approach based on the generative pre-trained language model\n(PLM) to automatically retrieve and map biological analogy and generate BID in\nthe form of natural language. The latest generative pre-trained transformer,\nnamely GPT-3, is used as the base PLM. Three types of design concept generators\nare identified and fine-tuned from the PLM according to the looseness of the\nproblem space representation. Machine evaluators are also fine-tuned to assess\nthe mapping relevancy between the domains within the generated BID concepts.\nThe approach is evaluated and then employed in a real-world project of\ndesigning light-weighted flying cars during its conceptual design phase The\nresults show our approach can generate BID concepts with good performance.", "published": "2022-12-26 16:06:04", "link": "http://arxiv.org/abs/2212.13196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Highlighting Named Entities in Input for Auto-Formulation of\n  Optimization Problems", "abstract": "Operations research deals with modeling and solving real-world problems as\nmathematical optimization problems. While solving mathematical systems is\naccomplished by analytical software, formulating a problem as a set of\nmathematical operations has been typically done manually by domain experts.\nRecent machine learning methods have shown promise in converting textual\nproblem descriptions to corresponding mathematical formulations. This paper\npresents an approach that converts linear programming word problems into\nmathematical formulations. We leverage the named entities in the input and\naugment the input to highlight these entities. Our approach achieves the\nhighest accuracy among all submissions to the NL4Opt Competition, securing\nfirst place in the generation track.", "published": "2022-12-26 16:13:57", "link": "http://arxiv.org/abs/2212.13201v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Prediction of Offensive News Comments by Considering the\n  Characteristics of Commenters", "abstract": "When reading news articles on social networking services and news sites,\nreaders can view comments marked by other people on these articles. By reading\nthese comments, a reader can understand the public opinion about the news, and\nit is often helpful to grasp the overall picture of the news. However, these\ncomments often contain offensive language that readers do not prefer to read.\nThis study aims to predict such offensive comments to improve the quality of\nthe experience of the reader while reading comments. By considering the\ndiversity of the readers' values, the proposed method predicts offensive news\ncomments for each reader based on the feedback from a small number of news\ncomments that the reader rated as \"offensive\" in the past. In addition, we used\na machine learning model that considers the characteristics of the commenters\nto make predictions, independent of the words and topics in news comments. The\nexperimental results of the proposed method show that prediction can be\npersonalized even when the amount of readers' feedback data used in the\nprediction is limited. In particular, the proposed method, which considers the\ncommenters' characteristics, has a low probability of false detection of\noffensive comments.", "published": "2022-12-26 16:19:03", "link": "http://arxiv.org/abs/2212.13205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Complex Knowledge Base Question Answering via\n  Question-to-Action and Question-to-Question Alignment", "abstract": "Complex knowledge base question answering can be achieved by converting\nquestions into sequences of predefined actions. However, there is a significant\nsemantic and structural gap between natural language and action sequences,\nwhich makes this conversion difficult. In this paper, we introduce an\nalignment-enhanced complex question answering framework, called ALCQA, which\nmitigates this gap through question-to-action alignment and\nquestion-to-question alignment. We train a question rewriting model to align\nthe question and each action, and utilize a pretrained language model to\nimplicitly align the question and KG artifacts. Moreover, considering that\nsimilar questions correspond to similar action sequences, we retrieve top-k\nsimilar question-answer pairs at the inference stage through\nquestion-to-question alignment and propose a novel reward-guided action\nsequence selection strategy to select from candidate action sequences. We\nconduct experiments on CQA and WQSP datasets, and the results show that our\napproach outperforms state-of-the-art methods and obtains a 9.88\\% improvements\nin the F1 metric on CQA dataset. Our source code is available at\nhttps://github.com/TTTTTTTTy/ALCQA.", "published": "2022-12-26 08:12:41", "link": "http://arxiv.org/abs/2212.13036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skit-S2I: An Indian Accented Speech to Intent dataset", "abstract": "Conventional conversation assistants extract text transcripts from the speech\nsignal using automatic speech recognition (ASR) and then predict intent from\nthe transcriptions. Using end-to-end spoken language understanding (SLU), the\nintents of the speaker are predicted directly from the speech signal without\nrequiring intermediate text transcripts. As a result, the model can optimize\ndirectly for intent classification and avoid cascading errors from ASR. The\nend-to-end SLU system also helps in reducing the latency of the intent\nprediction model. Although many datasets are available publicly for\ntext-to-intent tasks, the availability of labeled speech-to-intent datasets is\nlimited, and there are no datasets available in the Indian accent. In this\npaper, we release the Skit-S2I dataset, the first publicly available\nIndian-accented SLU dataset in the banking domain in a conversational tonality.\nWe experiment with multiple baselines, compare different pretrained speech\nencoder's representations, and find that SSL pretrained representations perform\nslightly better than ASR pretrained representations lacking prosodic features\nfor speech-to-intent classification. The dataset and baseline code is available\nat \\url{https://github.com/skit-ai/speech-to-intent-dataset}", "published": "2022-12-26 05:10:43", "link": "http://arxiv.org/abs/2212.13015v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Text Simplification of News Articles in the Context of Public\n  Broadcasting", "abstract": "This report summarizes the work carried out by the authors during the Twelfth\nMontreal Industrial Problem Solving Workshop, held at Universit\\'e de\nMontr\\'eal in August 2022. The team tackled a problem submitted by\nCBC/Radio-Canada on the theme of Automatic Text Simplification (ATS).", "published": "2022-12-26 22:59:57", "link": "http://arxiv.org/abs/2212.13317v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blind estimation of room acoustic parameters from speech signals based\n  on extended model of room impulse response", "abstract": "The speech transmission index (STI) and room acoustic parameters (RAPs),\nwhich are derived from a room impulse response (RIR), such as reverberation\ntime and early decay time, are essential to assess speech transmission and to\npredict the listening difficulty in a sound field. Since it is difficult to\nmeasure RIR in daily occupied spaces, simultaneous blind estimation of STI and\nRAPs must be resolved as it is an imperative and challenging issue. This paper\nproposes a deterministic method for blindly estimating STI and five RAPs on the\nbasis of an RIR stochastic model that approximates an unknown RIR. The proposed\nmethod formulates a temporal power envelope of a reverberant speech signal to\nobtain the optimal parameters for the RIR model. Simulations were conducted to\nevaluate STI and RAPs from observed reverberant speech signals. The\nroot-mean-square errors between the estimated and ground-truth results were\nused to comparatively evaluate the proposed method with the previous method.\nThe results showed that the proposed method can estimate STI and RAPs\neffectively without any training.", "published": "2022-12-26 04:28:02", "link": "http://arxiv.org/abs/2212.13009v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
