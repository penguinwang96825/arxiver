{"title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference\n  Resolution", "abstract": "Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions.", "published": "2014-01-16 05:06:09", "link": "http://arxiv.org/abs/1405.5202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn\n  Language", "abstract": "We present a novel framework for learning to interpret and generate language\nusing only perceptual context as supervision. We demonstrate its capabilities\nby developing a system that learns to sportscast simulated robot soccer games\nin both English and Korean without any language-specific prior knowledge.\nTraining employs only ambiguous supervision consisting of a stream of\ndescriptive textual comments and a sequence of events extracted from the\nsimulation trace. The system simultaneously establishes correspondences between\nindividual comments and the events that they describe while building a\ntranslation model that supports both parsing and generation. We also present a\nnovel algorithm for learning which events are worth describing. Human\nevaluations of the generated commentaries indicate they are of reasonable\nquality and in some cases even on par with those produced by humans for our\nlimited domain.", "published": "2014-01-16 04:29:26", "link": "http://arxiv.org/abs/1405.7711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-based Word Acquisition for Situated Dialogue in a Virtual World", "abstract": "To tackle the vocabulary problem in conversational systems, previous work has\napplied unsupervised learning approaches on co-occurring speech and eye gaze\nduring interaction to automatically acquire new words. Although these\napproaches have shown promise, several issues related to human language\nbehavior and human-machine conversation have not been addressed. First,\npsycholinguistic studies have shown certain temporal regularities between human\neye movement and language production. While these regularities can potentially\nguide the acquisition process, they have not been incorporated in the previous\nunsupervised approaches. Second, conversational systems generally have an\nexisting knowledge base about the domain and vocabulary. While the existing\nknowledge can potentially help bootstrap and constrain the acquired new words,\nit has not been incorporated in the previous models. Third, eye gaze could\nserve different functions in human-machine conversation. Some gaze streams may\nnot be closely coupled with speech stream, and thus are potentially detrimental\nto word acquisition. Automated recognition of closely-coupled speech-gaze\nstreams based on conversation context is important. To address these issues, we\ndeveloped new approaches that incorporate user language behavior, domain\nknowledge, and conversation context in word acquisition. We evaluated these\napproaches in the context of situated dialogue in a virtual world. Our\nexperimental results have shown that incorporating the above three types of\ncontextual information significantly improves word acquisition performance.", "published": "2014-01-16 04:48:43", "link": "http://arxiv.org/abs/1401.6875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Reference Sets from Unstructured, Ungrammatical Text", "abstract": "Vast amounts of text on the Web are unstructured and ungrammatical, such as\nclassified ads, auction listings, forum postings, etc. We call such text\n\"posts.\" Despite their inconsistent structure and lack of grammar, posts are\nfull of useful information. This paper presents work on semi-automatically\nbuilding tables of relational information, called \"reference sets,\" by\nanalyzing such posts directly. Reference sets can be applied to a number of\ntasks such as ontology maintenance and information extraction. Our\nreference-set construction method starts with just a small amount of background\nknowledge, and constructs tuples representing the entities in the posts to form\na reference set. We also describe an extension to this approach for the special\ncase where even this small amount of background knowledge is impossible to\ndiscover and use. To evaluate the utility of the machine-constructed reference\nsets, we compare them to manually constructed reference sets in the context of\nreference-set-based information extraction. Our results show the reference sets\nconstructed by our method outperform manually constructed reference sets. We\nalso compare the reference-set-based extraction approach using the\nmachine-constructed reference set to supervised extraction approaches using\ngeneric features. These results demonstrate that using machine-constructed\nreference sets outperforms the supervised methods, even though the supervised\nmethods require training data.", "published": "2014-01-16 04:49:45", "link": "http://arxiv.org/abs/1401.3832v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evaluating Temporal Graphs Built from Texts via Transitive Reduction", "abstract": "Temporal information has been the focus of recent attention in information\nextraction, leading to some standardization effort, in particular for the task\nof relating events in a text. This task raises the problem of comparing two\nannotations of a given text, because relations between events in a story are\nintrinsically interdependent and cannot be evaluated separately. A proper\nevaluation measure is also crucial in the context of a machine learning\napproach to the problem. Finding a common comparison referent at the text level\nis not obvious, and we argue here in favor of a shift from event-based measures\nto measures on a unique textual object, a minimal underlying temporal graph, or\nmore formally the transitive reduction of the graph of relations between event\nboundaries. We support it by an investigation of its properties on synthetic\ndata and on a well-know temporal corpus.", "published": "2014-01-16 05:05:45", "link": "http://arxiv.org/abs/1401.3865v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric\n  Proximity", "abstract": "In automatic summarization, centrality-as-relevance means that the most\nimportant content of an information source, or a collection of information\nsources, corresponds to the most central passages, considering a representation\nwhere such notion makes sense (graph, spatial, etc.). We assess the main\nparadigms, and introduce a new centrality-based relevance model for automatic\nsummarization that relies on the use of support sets to better estimate the\nrelevant content. Geometric proximity is used to compute semantic relatedness.\nCentrality (relevance) is determined by considering the whole input source (and\nnot only local information), and by taking into account the existence of minor\ntopics or lateral subjects in the information sources to be summarized. The\nmethod consists in creating, for each passage of the input source, a support\nset consisting only of the most semantically related passages. Then, the\ndetermination of the most relevant content is achieved by selecting the\npassages that occur in the largest number of support sets. This model produces\nextractive summaries that are generic, and language- and domain-independent.\nThorough automatic evaluation shows that the method achieves state-of-the-art\nperformance, both in written text, and automatically transcribed speech\nsummarization, including when compared to considerably more complex approaches.", "published": "2014-01-16 05:23:22", "link": "http://arxiv.org/abs/1401.3908v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Cause Identification from Aviation Safety Incident Reports via Weakly\n  Supervised Semantic Lexicon Construction", "abstract": "The Aviation Safety Reporting System collects voluntarily submitted reports\non aviation safety incidents to facilitate research work aiming to reduce such\nincidents. To effectively reduce these incidents, it is vital to accurately\nidentify why these incidents occurred. More precisely, given a set of possible\ncauses, or shaping factors, this task of cause identification involves\nidentifying all and only those shaping factors that are responsible for the\nincidents described in a report. We investigate two approaches to cause\nidentification. Both approaches exploit information provided by a semantic\nlexicon, which is automatically constructed via Thelen and Riloffs Basilisk\nframework augmented with our linguistic and algorithmic modifications. The\nfirst approach labels a report using a simple heuristic, which looks for the\nwords and phrases acquired during the semantic lexicon learning process in the\nreport. The second approach recasts cause identification as a text\nclassification problem, employing supervised and transductive text\nclassification algorithms to learn models from incident reports labeled with\nshaping factors and using the models to label unseen reports. Our experiments\nshow that both the heuristic-based approach and the learning-based approach\n(when given sufficient training data) outperform the baseline system\nsignificantly.", "published": "2014-01-16 04:53:44", "link": "http://arxiv.org/abs/1401.4436v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlling Complexity in Part-of-Speech Induction", "abstract": "We consider the problem of fully unsupervised learning of grammatical\n(part-of-speech) categories from unlabeled text. The standard\nmaximum-likelihood hidden Markov model for this task performs poorly, because\nof its weak inductive bias and large model capacity. We address this problem by\nrefining the model and modifying the learning objective to control its capacity\nvia para- metric and non-parametric constraints. Our approach enforces\nword-category association sparsity, adds morphological and orthographic\nfeatures, and eliminates hard-to-estimate parameters for rare words. We develop\nan efficient learning algorithm that is not much more computationally intensive\nthan standard training. We also provide an open-source implementation of the\nalgorithm. Our experiments on five diverse languages (Bulgarian, Danish,\nEnglish, Portuguese, Spanish) achieve significant improvements compared with\nprevious methods for the same task.", "published": "2014-01-16 05:20:08", "link": "http://arxiv.org/abs/1401.6131v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Local Alignments for Relation Recognition", "abstract": "This paper discusses the problem of marrying structural similarity with\nsemantic relatedness for Information Extraction from text. Aiming at accurate\nrecognition of relations, we introduce local alignment kernels and explore\nvarious possibilities of using them for this task. We give a definition of a\nlocal alignment (LA) kernel based on the Smith-Waterman score as a sequence\nsimilarity measure and proceed with a range of possibilities for computing\nsimilarity between elements of sequences. We show how distributional similarity\nmeasures obtained from unlabeled data can be incorporated into the learning\ntask as semantic knowledge. Our experiments suggest that the LA kernel yields\npromising results on various biomedical corpora outperforming two baselines by\na large margin. Additional series of experiments have been conducted on the\ndata sets of seven general relation types, where the performance of the LA\nkernel is comparable to the current state-of-the-art results.", "published": "2014-01-16 04:51:47", "link": "http://arxiv.org/abs/1405.7713v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with\n  Minimal Feedback", "abstract": "While traditional research on text clustering has largely focused on grouping\ndocuments by topic, it is conceivable that a user may want to cluster documents\nalong other dimensions, such as the authors mood, gender, age, or sentiment.\nWithout knowing the users intention, a clustering algorithm will only group\ndocuments along the most prominent dimension, which may not be the one the user\ndesires. To address the problem of clustering documents along the user-desired\ndimension, previous work has focused on learning a similarity metric from data\nmanually annotated with the users intention or having a human construct a\nfeature space in an interactive manner during the clustering process. With the\ngoal of reducing reliance on human knowledge for fine-tuning the similarity\nfunction or selecting the relevant features required by these approaches, we\npropose a novel active clustering algorithm, which allows a user to easily\nselect the dimension along which she wants to cluster the documents by\ninspecting only a small number of words. We demonstrate the viability of our\nalgorithm on a variety of commonly-used sentiment datasets.", "published": "2014-01-16 04:56:03", "link": "http://arxiv.org/abs/1401.5389v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
