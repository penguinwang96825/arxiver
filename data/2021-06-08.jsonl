{"title": "Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question\n  Answering", "abstract": "Disfluencies is an under-studied topic in NLP, even though it is ubiquitous\nin human conversation. This is largely due to the lack of datasets containing\ndisfluencies. In this paper, we present a new challenge question answering\ndataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual\ndisfluencies in previously fluent questions. Disfl-QA contains a variety of\nchallenging disfluencies that require a more comprehensive understanding of the\ntext than what was necessary in prior datasets. Experiments show that the\nperformance of existing state-of-the-art question answering models degrades\nsignificantly when tested on Disfl-QA in a zero-shot setting.We show data\naugmentation methods partially recover the loss in performance and also\ndemonstrate the efficacy of using gold data for fine-tuning. We argue that we\nneed large-scale disfluency datasets in order for NLP models to be robust to\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/disfl-qa.", "published": "2021-06-08 00:03:40", "link": "http://arxiv.org/abs/2106.04016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised and Supervised Joint Training for Resource-rich Machine\n  Translation", "abstract": "Self-supervised pre-training of text representations has been successfully\napplied to low-resource Neural Machine Translation (NMT). However, it usually\nfails to achieve notable gains on resource-rich NMT. In this paper, we propose\na joint training approach, $F_2$-XEnDec, to combine self-supervised and\nsupervised learning to optimize NMT models. To exploit complementary\nself-supervised signals for supervised learning, NMT models are trained on\nexamples that are interbred from monolingual and parallel sentences through a\nnew process called crossover encoder-decoder. Experiments on two resource-rich\ntranslation benchmarks, WMT'14 English-German and WMT'14 English-French,\ndemonstrate that our approach achieves substantial improvements over several\nstrong baseline methods and obtains a new state of the art of 46.19 BLEU on\nEnglish-French when incorporating back translation. Results also show that our\napproach is capable of improving model robustness to input perturbations such\nas code-switching noise which frequently appears on social media.", "published": "2021-06-08 02:35:40", "link": "http://arxiv.org/abs/2106.04060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation", "abstract": "To date, most abstractive summarisation models have relied on variants of the\nnegative log-likelihood (NLL) as their training objective. In some cases,\nreinforcement learning has been added to train the models with an objective\nthat is closer to their evaluation measures (e.g. ROUGE). However, the reward\nfunction to be used within the reinforcement learning approach can play a key\nrole for performance and is still partially unexplored. For this reason, in\nthis paper, we propose two reward functions for the task of abstractive\nsummarisation: the first function, referred to as RwB-Hinge, dynamically\nselects the samples for the gradient update. The second function, nicknamed\nRISK, leverages a small pool of strong candidates to inform the reward. In the\nexperiments, we probe the proposed approach by fine-tuning an NLL pre trained\nmodel over nine summarisation datasets of diverse size and nature. The\nexperimental results show a consistent improvement over the negative\nlog-likelihood baselines.", "published": "2021-06-08 03:30:50", "link": "http://arxiv.org/abs/2106.04080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language\n  Model", "abstract": "Recently, there is an effort to extend fine-grained entity typing by using a\nricher and ultra-fine set of types, and labeling noun phrases including\npronouns and nominal nouns instead of just named entity mentions. A key\nchallenge for this ultra-fine entity typing task is that human annotated data\nare extremely scarce, and the annotation ability of existing distant or weak\nsupervision approaches is very limited. To remedy this problem, in this paper,\nwe propose to obtain training data for ultra-fine entity typing by using a BERT\nMasked Language Model (MLM). Given a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that it predicts context dependent\nhypernyms of the mention, which can be used as type labels. Experimental\nresults demonstrate that, with the help of these automatically generated\nlabels, the performance of an ultra-fine entity typing model can be improved\nsubstantially. We also show that our approach can be applied to improve\ntraditional fine-grained entity typing after performing simple type mapping.", "published": "2021-06-08 04:43:28", "link": "http://arxiv.org/abs/2106.04098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage\n  and Quality", "abstract": "We release a new benchmark for lexical substitution, the task of finding\nappropriate substitutes for a target word in a context. To assist humans with\nwriting, lexical substitution systems can suggest words that humans cannot\neasily think of. However, existing benchmarks depend on human recall as the\nonly source of data, and therefore lack coverage of the substitutes that would\nbe most helpful to humans. Furthermore, annotators often provide substitutes of\nlow quality, which are not actually appropriate in the given context. We\ncollect higher-coverage and higher-quality data by framing lexical substitution\nas a classification problem, guided by the intuition that it is easier for\nhumans to judge the appropriateness of candidate substitutes than conjure them\nfrom memory. To this end, we use a context-free thesaurus to produce candidates\nand rely on human judgement to determine contextual appropriateness. Compared\nto the previous largest benchmark, our Swords benchmark has 4.1x more\nsubstitutes per target word for the same level of quality, and its substitutes\nare 1.5x more appropriate (based on human judgement) for the same number of\nsubstitutes.", "published": "2021-06-08 04:58:29", "link": "http://arxiv.org/abs/2106.04102v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Realistic Evaluation Principles for Cross-document Coreference\n  Resolution", "abstract": "We point out that common evaluation practices for cross-document coreference\nresolution have been unrealistically permissive in their assumed settings,\nyielding inflated results. We propose addressing this issue via two evaluation\nmethodology principles. First, as in other tasks, models should be evaluated on\npredicted mentions rather than on gold mentions. Doing this raises a subtle\nissue regarding singleton coreference clusters, which we address by decoupling\nthe evaluation of mention detection from that of coreference linking. Second,\nwe argue that models should not exploit the synthetic topic structure of the\nstandard ECB+ dataset, forcing models to confront the lexical ambiguity\nchallenge, as intended by the dataset creators. We demonstrate empirically the\ndrastic impact of our more realistic evaluation principles on a competitive\nmodel, yielding a score which is 33 F1 lower compared to evaluating by prior\nlenient practices.", "published": "2021-06-08 09:05:21", "link": "http://arxiv.org/abs/2106.04192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags\n  for Low Resource UD Parsing", "abstract": "We evaluate the efficacy of predicted UPOS tags as input features for\ndependency parsers in lower resource settings to evaluate how treebank size\naffects the impact tagging accuracy has on parsing performance. We do this for\nreal low resource universal dependency treebanks, artificially low resource\ndata with varying treebank sizes, and for very small treebanks with varying\namounts of augmented data. We find that predicted UPOS tags are somewhat\nhelpful for low resource treebanks, especially when fewer fully-annotated trees\nare available. We also find that this positive impact diminishes as the amount\nof data increases.", "published": "2021-06-08 10:04:08", "link": "http://arxiv.org/abs/2106.04222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning to Compositionally Generalize", "abstract": "Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.", "published": "2021-06-08 11:21:48", "link": "http://arxiv.org/abs/2106.04252v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Generative Framework for Aspect-Based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms,\ntheir corresponding sentiment polarities, and the opinion terms. There exist\nseven subtasks in ABSA. Most studies only focus on the subsets of these\nsubtasks, which leads to various complicated ABSA models while hard to solve\nthese subtasks in a unified framework. In this paper, we redefine every subtask\ntarget as a sequence mixed by pointer indexes and sentiment class indexes,\nwhich converts all ABSA subtasks into a unified generative formulation. Based\non the unified formulation, we exploit the pre-training sequence-to-sequence\nmodel BART to solve all ABSA subtasks in an end-to-end framework. Extensive\nexperiments on four ABSA datasets for seven subtasks demonstrate that our\nframework achieves substantial performance gain and provides a real unified\nend-to-end solution for the whole ABSA subtasks, which could benefit multiple\ntasks.", "published": "2021-06-08 12:55:22", "link": "http://arxiv.org/abs/2106.04300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time\n  Curvatures", "abstract": "Knowledge Graph (KG) completion has been excessively studied with a massive\nnumber of models proposed for the Link Prediction (LP) task. The main\nlimitation of such models is their insensitivity to time. Indeed, the temporal\naspect of stored facts is often ignored. To this end, more and more works\nconsider time as a parameter to complete KGs. In this paper, we first\ndemonstrate that, by simply increasing the number of negative samples, the\nrecent AttH model can achieve competitive or even better performance than the\nstate-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further\npropose Hercules, a time-aware extension of AttH model, which defines the\ncurvature of a Riemannian manifold as the product of both relation and time.\nOur experiments show that both Hercules and AttH achieve competitive or new\nstate-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore,\none should raise awareness when learning TKGs representations to identify\nwhether time truly boosts performances.", "published": "2021-06-08 13:13:43", "link": "http://arxiv.org/abs/2106.04311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning compositional structures for semantic graph parsing", "abstract": "AM dependency parsing is a method for neural semantic graph parsing that\nexploits the principle of compositionality. While AM dependency parsers have\nbeen shown to be fast and accurate across several graphbanks, they require\nexplicit annotations of the compositional tree structures for training. In the\npast, these were obtained using complex graphbank-specific heuristics written\nby experts. Here we show how they can instead be trained directly on the graphs\nwith a neural latent-variable model, drastically reducing the amount and\ncomplexity of manual heuristics. We demonstrate that our model picks up on\nseveral linguistic phenomena on its own and achieves comparable accuracy to\nsupervised training, greatly facilitating the use of AM dependency parsing for\nnew sembanks.", "published": "2021-06-08 14:20:07", "link": "http://arxiv.org/abs/2106.04398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Machine Reading Comprehension with Virtual\n  Embeddings", "abstract": "Adversarial training (AT) as a regularization method has proved its\neffectiveness on various tasks. Though there are successful applications of AT\non some NLP tasks, the distinguishing characteristics of NLP tasks have not\nbeen exploited. In this paper, we aim to apply AT on machine reading\ncomprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing\na novel adversarial training method called PQAT that perturbs the embedding\nmatrix instead of word vectors. To differentiate the roles of passages and\nquestions, PQAT uses additional virtual P/Q-embedding matrices to gather the\nglobal perturbations of words from passages and questions separately. We test\nthe method on a wide range of MRC tasks, including span-based extractive RC and\nmultiple-choice RC. The results show that adversarial training is effective\nuniversally, and PQAT further improves the performance.", "published": "2021-06-08 15:16:34", "link": "http://arxiv.org/abs/2106.04437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLTR: An End-to-End, Transformer-Based System for Cell Level Table\n  Retrieval and Table Question Answering", "abstract": "We present the first end-to-end, transformer-based table question answering\n(QA) system that takes natural language questions and massive table corpus as\ninputs to retrieve the most relevant tables and locate the correct table cells\nto answer the question. Our system, CLTR, extends the current state-of-the-art\nQA over tables model to build an end-to-end table QA architecture. This system\nhas successfully tackled many real-world table QA problems with a simple,\nunified pipeline. Our proposed system can also generate a heatmap of candidate\ncolumns and rows over complex tables and allow users to quickly identify the\ncorrect cells to answer questions. In addition, we introduce two new\nopen-domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural\nlanguage questions over 76,242 tables. The benchmarks are designed to validate\nCLTR as well as accommodate future table retrieval and end-to-end table QA\nresearch and experiments. Our experiments demonstrate that our system is the\ncurrent state-of-the-art model on the table retrieval task and produces\npromising results for end-to-end table QA.", "published": "2021-06-08 15:22:10", "link": "http://arxiv.org/abs/2106.04441v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reading StackOverflow Encourages Cheating: Adding Question Text Improves\n  Extractive Code Generation", "abstract": "Answering a programming question using only its title is difficult as salient\ncontextual information is omitted. Based on this observation, we present a\ncorpus of over 40,000 StackOverflow question texts to be used in conjunction\nwith their corresponding intents from the CoNaLa dataset (Yin et al., 2018).\nUsing both the intent and question body, we use BART to establish a baseline\nBLEU score of 34.35 for this new task. We find further improvements of $2.8\\%$\nby combining the mined CoNaLa data with the labeled data to achieve a 35.32\nBLEU score. We evaluate prior state-of-the-art CoNaLa models with this\nadditional data and find that our proposed method of using the body and mined\ndata beats the BLEU score of the prior state-of-the-art by $71.96\\%$. Finally,\nwe perform ablations to demonstrate that BART is an unsupervised multimodal\nlearner and examine its extractive behavior. The code and data can be found\nhttps://github.com/gabeorlanski/stackoverflow-encourages-cheating.", "published": "2021-06-08 15:28:55", "link": "http://arxiv.org/abs/2106.04447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task\n  Learning on Semantic Parsing Datasets", "abstract": "Semantic parsers map natural language utterances to meaning representations.\nThe lack of a single standard for meaning representations led to the creation\nof a plethora of semantic parsing datasets. To unify different datasets and\ntrain a single model for them, we investigate the use of Multi-Task Learning\n(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,\nOvernight, AMR). We find that an MTL architecture that shares the entire\nnetwork across datasets yields competitive or better parsing accuracies than\nthe single-task baselines, while reducing the total number of parameters by\n68%. We further provide evidence that MTL has also better compositional\ngeneralization than single-task models. We also present a comparison of task\nsampling methods and propose a competitive alternative to widespread\nproportional sampling strategies.", "published": "2021-06-08 16:03:42", "link": "http://arxiv.org/abs/2106.04476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared\n  Hypernetworks", "abstract": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing\nadapter modules between the layers of a pretrained language model. However,\nsuch modules are trained separately for each task and thus do not enable\nsharing information across tasks. In this paper, we show that we can learn\nadapter parameters for all layers and tasks by generating them using shared\nhypernetworks, which condition on task, adapter position, and layer id in a\ntransformer model. This parameter-efficient multi-task learning framework\nallows us to achieve the best of both worlds by sharing knowledge across tasks\nvia hypernetworks while enabling the model to adapt to each individual task\nthrough task-specific adapters. Experiments on the well-known GLUE benchmark\nshow improved performance in multi-task learning while adding only 0.29%\nparameters per task. We additionally demonstrate substantial performance\nimprovements in few-shot domain generalization across a variety of tasks. Our\ncode is publicly available in https://github.com/rabeehk/hyperformer.", "published": "2021-06-08 16:16:40", "link": "http://arxiv.org/abs/2106.04489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turing: an Accurate and Interpretable Multi-Hypothesis Cross-Domain\n  Natural Language Database Interface", "abstract": "A natural language database interface (NLDB) can democratize data-driven\ninsights for non-technical users. However, existing Text-to-SQL semantic\nparsers cannot achieve high enough accuracy in the cross-database setting to\nallow good usability in practice. This work presents Turing, a NLDB system\ntoward bridging this gap. The cross-domain semantic parser of Turing with our\nnovel value prediction method achieves $75.1\\%$ execution accuracy, and\n$78.3\\%$ top-5 beam execution accuracy on the Spider validation set. To benefit\nfrom the higher beam accuracy, we design an interactive system where the SQL\nhypotheses in the beam are explained step-by-step in natural language, with\ntheir differences highlighted. The user can then compare and judge the\nhypotheses to select which one reflects their intention if any. The English\nexplanations of SQL queries in Turing are produced by our high-precision\nnatural language generation system based on synchronous grammars.", "published": "2021-06-08 17:46:20", "link": "http://arxiv.org/abs/2106.04559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog", "abstract": "Everyday conversations require understanding everyday events, which in turn,\nrequires understanding temporal commonsense concepts interwoven with those\nevents. Despite recent progress with massive pre-trained language models (LMs)\nsuch as T5 and GPT-3, their capability of temporal reasoning in dialogs remains\nlargely under-explored. In this paper, we present the first study to\ninvestigate pre-trained LMs for their temporal reasoning capabilities in\ndialogs by introducing a new task and a crowd-sourced English challenge set,\nTIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K\ncarefully curated dialogs. Empirical results demonstrate that even the best\nperforming models struggle on this task compared to humans, with 23 absolute\npoints of gap in accuracy. Furthermore, our analysis reveals that the models\nfail to reason about dialog context correctly; instead, they rely on shallow\ncues based on existing temporal patterns in context, motivating future research\nfor modeling temporal concepts in text and robust contextual reasoning about\nthem. The dataset is publicly available at:\nhttps://github.com/google-research-datasets/timedial.", "published": "2021-06-08 17:59:21", "link": "http://arxiv.org/abs/2106.04571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers", "abstract": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers. Specifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared \"slow\" weights and \"fast\" rank-one matrices\ndefined per Compacter layer. By only training 0.047% of a pretrained model's\nparameters, Compacter performs on par with standard fine-tuning on GLUE and\noutperforms standard fine-tuning on SuperGLUE and low-resource settings. Our\ncode is publicly available at~\\url{https://github.com/rabeehk/compacter}.", "published": "2021-06-08 19:17:04", "link": "http://arxiv.org/abs/2106.04647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comprehension Based Question Answering using Bloom's Taxonomy", "abstract": "Current pre-trained language models have lots of knowledge, but a more\nlimited ability to use that knowledge. Bloom's Taxonomy helps educators teach\nchildren how to use knowledge by categorizing comprehension skills, so we use\nit to analyze and improve the comprehension skills of large pre-trained\nlanguage models. Our experiments focus on zero-shot question answering, using\nthe taxonomy to provide proximal context that helps the model answer questions\nby being relevant to those questions. We show targeting context in this manner\nimproves performance across 4 popular common sense question answer datasets.", "published": "2021-06-08 19:32:21", "link": "http://arxiv.org/abs/2106.04653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification of Contract-Amendment Relationships", "abstract": "In Contract Life-cycle Management (CLM), managing and tracking the master\nagreements and their associated amendments is essential, in order to be kept\ninformed with different due dates and obligations. An automatic solution can\nfacilitate the daily jobs and improve the efficiency of legal practitioners. In\nthis paper, we propose an approach based on machine learning (ML) and Natural\nLanguage Processing (NLP) to detect the amendment relationship between two\ndocuments. The algorithm takes two PDF documents preprocessed by OCR (Optical\nCharacter Recognition) and NER (Named Entity Recognition) as input, and then it\nbuilds the features of each document pair and classifies the relationship. We\nexperimented with different configurations on a dataset consisting of 1124\npairs of contract-amendment documents in English and French. The best result\nobtained a F1-score of 91%, which outperformed 23% compared to a\nheuristic-based baseline.", "published": "2021-06-08 07:57:10", "link": "http://arxiv.org/abs/2106.14619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insight from NLP Analysis: COVID-19 Vaccines Sentiments on Social Media", "abstract": "Social media is an appropriate source for analyzing public attitudes towards\nthe COVID-19 vaccine and various brands. Nevertheless, there are few relevant\nstudies. In the research, we collected tweet posts by the UK and US residents\nfrom the Twitter API during the pandemic and designed experiments to answer\nthree main questions concerning vaccination. To get the dominant sentiment of\nthe civics, we performed sentiment analysis by VADER and proposed a new method\nthat can count the individual's influence. This allows us to go a step further\nin sentiment analysis and explain some of the fluctuations in the data\nchanging. The results indicated that celebrities could lead the opinion shift\non social media in vaccination progress. Moreover, at the peak, nearly 40\\% of\nthe population in both countries have a negative attitude towards COVID-19\nvaccines. Besides, we investigated how people's opinions toward different\nvaccine brands are. We found that the Pfizer vaccine enjoys the most popular\namong people. By applying the sentiment analysis tool, we discovered most\npeople hold positive views toward the COVID-19 vaccine manufactured by most\nbrands. In the end, we carried out topic modelling by using the LDA model. We\nfound residents in the two countries are willing to share their views and\nfeelings concerning the vaccine. Several death cases have occurred after\nvaccination. Due to these negative events, US residents are more worried about\nthe side effects and safety of the vaccine.", "published": "2021-06-08 03:37:22", "link": "http://arxiv.org/abs/2106.04081v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Interpretable and Low-Resource Entity Matching via Decoupling Feature\n  Learning from Decision Making", "abstract": "Entity Matching (EM) aims at recognizing entity records that denote the same\nreal-world object. Neural EM models learn vector representation of entity\ndescriptions and match entities end-to-end. Though robust, these methods\nrequire many resources for training, and lack of interpretability. In this\npaper, we propose a novel EM framework that consists of Heterogeneous\nInformation Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple\nfeature representation from matching decision. Using self-supervised learning\nand mask mechanism in pre-trained language modeling, HIF learns the embeddings\nof noisy attribute values by inter-attribute attention with unlabeled data.\nUsing a set of comparison features and a limited amount of annotated data, KAT\nInduction learns an efficient decision tree that can be interpreted by\ngenerating entity matching rules whose structure is advocated by domain\nexperts. Experiments on 6 public datasets and 3 industrial datasets show that\nour method is highly efficient and outperforms SOTA EM models in most cases.\nOur codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT.", "published": "2021-06-08 08:27:31", "link": "http://arxiv.org/abs/2106.04174v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021", "abstract": "We evaluate three leading dependency parser systems from different paradigms\non a small yet diverse subset of languages in terms of their\naccuracy-efficiency Pareto front. As we are interested in efficiency, we\nevaluate core parsers without pretrained language models (as these are\ntypically huge networks and would constitute most of the compute time) or other\naugmentations that can be transversally applied to any of them. Biaffine\nparsing emerges as a well-balanced default choice, with sequence-labelling\nparsing being preferable if inference speed (but not training energy cost) is\nthe priority.", "published": "2021-06-08 09:55:47", "link": "http://arxiv.org/abs/2106.04216v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Staircase Attention for Recurrent Processing of Sequences", "abstract": "Attention mechanisms have become a standard tool for sequence modeling tasks,\nin particular by stacking self-attention layers over the entire input sequence\nas in the Transformer architecture. In this work we introduce a novel attention\nprocedure called staircase attention that, unlike self-attention, operates\nacross the sequence (in time) recurrently processing the input by adding\nanother step of processing. A step in the staircase comprises of backward\ntokens (encoding the sequence so far seen) and forward tokens (ingesting a new\npart of the sequence), or an extreme Ladder version with a forward step of zero\nthat simply repeats the Transformer on each step of the ladder, sharing the\nweights. We thus describe a family of such models that can trade off\nperformance and compute, by either increasing the amount of recurrence through\ntime, the amount of sequential processing via recurrence in depth, or both.\nStaircase attention is shown to be able to solve tasks that involve tracking\nthat conventional Transformers cannot, due to this recurrence. Further, it is\nshown to provide improved modeling power for the same size model (number of\nparameters) compared to self-attentive Transformers on large language modeling\nand dialogue tasks, yielding significant perplexity gains.", "published": "2021-06-08 12:19:31", "link": "http://arxiv.org/abs/2106.04279v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Using a New Nonlinear Gradient Method for Solving Large Scale Convex\n  Optimization Problems with an Application on Arabic Medical Text", "abstract": "Gradient methods have applications in multiple fields, including signal\nprocessing, image processing, and dynamic systems. In this paper, we present a\nnonlinear gradient method for solving convex supra-quadratic functions by\ndeveloping the search direction, that done by hybridizing between the two\nconjugate coefficients HRM [2] and NHS [1]. The numerical results proved the\neffectiveness of the presented method by applying it to solve standard problems\nand reaching the exact solution if the objective function is quadratic convex.\nAlso presented in this article, an application to the problem of named entities\nin the Arabic medical language, as it proved the stability of the proposed\nmethod and its efficiency in terms of execution time.", "published": "2021-06-08 14:13:58", "link": "http://arxiv.org/abs/2106.04383v2", "categories": ["math.OC", "cs.CL"], "primary_category": "math.OC"}
{"title": "Hash Layers For Large Sparse Models", "abstract": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.", "published": "2021-06-08 14:54:24", "link": "http://arxiv.org/abs/2106.04426v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Pretrained Transformers Robust in Intent Classification? A Missing\n  Ingredient in Evaluation of Out-of-Scope Intent Detection", "abstract": "Pre-trained Transformer-based models were reported to be robust in intent\nclassification. In this work, we first point out the importance of in-domain\nout-of-scope detection in few-shot intent recognition tasks and then illustrate\nthe vulnerability of pre-trained Transformer-based models against samples that\nare in-domain but out-of-scope (ID-OOS). We construct two new datasets, and\nempirically show that pre-trained models do not perform well on both ID-OOS\nexamples and general out-of-scope examples, especially on fine-grained few-shot\nintent detection tasks. To figure out how the models mistakenly classify ID-OOS\nintents as in-scope intents, we further conduct analysis on confidence scores\nand the overlapping keywords, as well as point out several prospective\ndirections for future work. Resources are available on\nhttps://github.com/jianguoz/Few-Shot-Intent-Detection.", "published": "2021-06-08 17:51:12", "link": "http://arxiv.org/abs/2106.04564v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing", "abstract": "In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers\ndevelop models that project sentences from various languages onto their AMRs to\ncapture their essential semantic structures: given a sentence in any language,\nwe aim to capture its core semantic content through concepts connected by\nmanifold types of semantic relations. Methods typically leverage large silver\ntraining data to learn a single model that is able to project non-English\nsentences to AMRs. However, we find that a simple baseline tends to be\nover-looked: translating the sentences to English and projecting their AMR with\na monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this\nsimple two-step base-line, and enhance it with a strong NMT system and a strong\nAMR parser. Our experiments show that T+P outperforms a recent state-of-the-art\nsystem across all tested languages: German, Italian, Spanish and Mandarin with\n+14.6, +12.6, +14.3 and +16.0 Smatch points.", "published": "2021-06-08 17:52:48", "link": "http://arxiv.org/abs/2106.04565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Extractive Search", "abstract": "Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.", "published": "2021-06-08 18:03:31", "link": "http://arxiv.org/abs/2106.04612v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Lack of Robust Interpretability of Neural Text Classifiers", "abstract": "With the ever-increasing complexity of neural language models, practitioners\nhave turned to methods for understanding the predictions of these models. One\nof the most well-adopted approaches for model interpretability is feature-based\ninterpretability, i.e., ranking the features in terms of their impact on model\npredictions. Several prior studies have focused on assessing the fidelity of\nfeature-based interpretability methods, i.e., measuring the impact of dropping\nthe top-ranked features on the model output. However, relatively little work\nhas been conducted on quantifying the robustness of interpretations. In this\nwork, we assess the robustness of interpretations of neural text classifiers,\nspecifically, those based on pretrained Transformer encoders, using two\nrandomization tests. The first compares the interpretations of two models that\nare identical except for their initializations. The second measures whether the\ninterpretations differ between a model with trained parameters and a model with\nrandom parameters. Both tests show surprising deviations from expected\nbehavior, raising questions about the extent of insights that practitioners may\ndraw from interpretations.", "published": "2021-06-08 18:31:02", "link": "http://arxiv.org/abs/2106.04631v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding\n  Evaluation", "abstract": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-benchmark.github.io/.", "published": "2021-06-08 18:34:21", "link": "http://arxiv.org/abs/2106.04632v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FastSeq: Make Sequence Generation Faster", "abstract": "Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.", "published": "2021-06-08 22:25:28", "link": "http://arxiv.org/abs/2106.04718v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-hop Graph Convolutional Network with High-order Chebyshev\n  Approximation for Text Reasoning", "abstract": "Graph convolutional network (GCN) has become popular in various natural\nlanguage processing (NLP) tasks with its superiority in long-term and\nnon-consecutive word interactions. However, existing single-hop graph reasoning\nin GCN may miss some important non-consecutive dependencies. In this study, we\ndefine the spectral graph convolutional network with the high-order dynamic\nChebyshev approximation (HDGCN), which augments the multi-hop graph reasoning\nby fusing messages aggregated from direct and long-term dependencies into one\nconvolutional layer. To alleviate the over-smoothing in high-order Chebyshev\napproximation, a multi-vote-based cross-attention (MVCAttn) with linear\ncomputation complexity is also proposed. The empirical results on four\ntransductive and inductive NLP tasks and the ablation study verify the efficacy\nof the proposed model. Our source code is available at\nhttps://github.com/MathIsAll/HDGCN-pytorch.", "published": "2021-06-08 07:49:43", "link": "http://arxiv.org/abs/2106.05221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cheap and Good? Simple and Effective Data Augmentation for Low Resource\n  Machine Reading", "abstract": "We propose a simple and effective strategy for data augmentation for\nlow-resource machine reading comprehension (MRC). Our approach first pretrains\nthe answer extraction components of a MRC system on the augmented data that\ncontains approximate context of the correct answers, before training it on the\nexact answer spans. The approximate context helps the QA method components in\nnarrowing the location of the answers. We demonstrate that our simple strategy\nsubstantially improves both document retrieval and answer extraction\nperformance by providing larger context of the answers and additional training\ndata. In particular, our method significantly improves the performance of BERT\nbased retriever (15.12\\%), and answer extractor (4.33\\% F1) on TechQA, a\ncomplex, low-resource MRC task. Further, our data augmentation strategy yields\nsignificant improvements of up to 3.9\\% exact match (EM) and 2.7\\% F1 for\nanswer extraction on PolicyQA, another practical but moderate sized QA dataset\nthat also contains long answer spans.", "published": "2021-06-08 06:46:50", "link": "http://arxiv.org/abs/2106.04134v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable agent communication from scratch (with a generic visual\n  processor emerging on the side)", "abstract": "As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.", "published": "2021-06-08 11:32:11", "link": "http://arxiv.org/abs/2106.04258v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Question Generation for Adaptive Education", "abstract": "Intelligent and adaptive online education systems aim to make high-quality\neducation available for a diverse range of students. However, existing systems\nusually depend on a pool of hand-made questions, limiting how fine-grained and\nopen-ended they can be in adapting to individual students. We explore targeted\nquestion generation as a controllable sequence generation task. We first show\nhow to fine-tune pre-trained language models for deep knowledge tracing\n(LM-KT). This model accurately predicts the probability of a student answering\na question correctly, and generalizes to questions not seen in training. We\nthen use LM-KT to specify the objective and data for training a model to\ngenerate questions conditioned on the student and target difficulty. Our\nresults show we succeed at generating novel, well-calibrated language\ntranslation questions for second language learners from a real online education\nplatform.", "published": "2021-06-08 11:46:59", "link": "http://arxiv.org/abs/2106.04262v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Segmentation from Discrete Speech Units in\n  Low-Resource Settings", "abstract": "Documenting languages helps to prevent the extinction of endangered dialects,\nmany of which are otherwise expected to disappear by the end of the century.\nWhen documenting oral languages, unsupervised word segmentation (UWS) from\nspeech is a useful, yet challenging, task. It consists in producing time-stamps\nfor slicing utterances into smaller segments corresponding to words, being\nperformed from phonetic transcriptions, or in the absence of these, from the\noutput of unsupervised speech discretization models. These discretization\nmodels are trained using raw speech only, producing discrete speech units that\ncan be applied for downstream (text-based) tasks. In this paper we compare five\nof these models: three Bayesian and two neural approaches, with regards to the\nexploitability of the produced units for UWS. For the UWS task, we experiment\nwith two models, using as our target language the Mboshi (Bantu C25), an\nunwritten language from Congo-Brazzaville. Additionally, we report results for\nFinnish, Hungarian, Romanian and Russian in equally low-resource settings,\nusing only 4 hours of speech. Our results suggest that neural models for speech\ndiscretization are difficult to exploit in our setting, and that it might be\nnecessary to adapt them to limit sequence length. We obtain our best UWS\nresults by using Bayesian models that produce high quality, yet compressed,\ndiscrete representations of the input speech signal.", "published": "2021-06-08 12:50:37", "link": "http://arxiv.org/abs/2106.04298v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Obtaining Better Static Word Embeddings Using Contextual Embedding\n  Models", "abstract": "The advent of contextual word embeddings -- representations of words which\nincorporate semantic and syntactic information from their context -- has led to\ntremendous improvements on a wide variety of NLP tasks. However, recent\ncontextual models have prohibitively high computational cost in many use-cases\nand are often hard to interpret. In this work, we demonstrate that our proposed\ndistillation method, which is a simple extension of CBOW-based training, allows\nto significantly improve computational efficiency of NLP applications, while\noutperforming the quality of existing static embeddings trained from scratch as\nwell as those distilled from previously proposed methods. As a side-effect, our\napproach also allows a fair comparison of both contextual and static embeddings\nvia standard lexical evaluation tasks.", "published": "2021-06-08 12:59:32", "link": "http://arxiv.org/abs/2106.04302v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SynthRef: Generation of Synthetic Referring Expressions for Object\n  Segmentation", "abstract": "Recent advances in deep learning have brought significant progress in visual\ngrounding tasks such as language-guided video object segmentation. However,\ncollecting large datasets for these tasks is expensive in terms of annotation\ntime, which represents a bottleneck. To this end, we propose a novel method,\nnamely SynthRef, for generating synthetic referring expressions for target\nobjects in an image (or video frame), and we also present and disseminate the\nfirst large-scale dataset with synthetic referring expressions for video object\nsegmentation. Our experiments demonstrate that by training with our synthetic\nreferring expressions one can improve the ability of a model to generalize\nacross different datasets, without any additional annotation cost. Moreover,\nour formulation allows its application to any object detection or segmentation\ndataset.", "published": "2021-06-08 14:28:13", "link": "http://arxiv.org/abs/2106.04403v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused\n  Interventions", "abstract": "Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.", "published": "2021-06-08 16:09:47", "link": "http://arxiv.org/abs/2106.04484v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cyberbullying Detection Using Deep Neural Network from Social Media\n  Comments in Bangla Language", "abstract": "Cyberbullying or Online harassment detection on social media for various\nmajor languages is currently being given a good amount of focus by researchers\nworldwide. Being the seventh most speaking language in the world and increasing\nusage of online platform among the Bengali speaking people urge to find\neffective detection technique to handle the online harassment. In this paper,\nwe have proposed binary and multiclass classification model using hybrid neural\nnetwork for bully expression detection in Bengali language. We have used 44,001\nusers comments from popular public Facebook pages, which fall into five classes\n- Non-bully, Sexual, Threat, Troll and Religious. We have examined the\nperformance of our proposed models from different perspective. Our binary\nclassification model gives 87.91% accuracy, whereas introducing ensemble\ntechnique after neural network for multiclass classification, we got 85%\naccuracy.", "published": "2021-06-08 16:47:22", "link": "http://arxiv.org/abs/2106.04506v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Survey of Transformers", "abstract": "Transformers have achieved great success in many artificial intelligence\nfields, such as natural language processing, computer vision, and audio\nprocessing. Therefore, it is natural to attract lots of interest from academic\nand industry researchers. Up to the present, a great variety of Transformer\nvariants (a.k.a. X-formers) have been proposed, however, a systematic and\ncomprehensive literature review on these Transformer variants is still missing.\nIn this survey, we provide a comprehensive review of various X-formers. We\nfirst briefly introduce the vanilla Transformer and then propose a new taxonomy\nof X-formers. Next, we introduce the various X-formers from three perspectives:\narchitectural modification, pre-training, and applications. Finally, we outline\nsome potential directions for future research.", "published": "2021-06-08 17:43:08", "link": "http://arxiv.org/abs/2106.04554v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation", "abstract": "While deep and large pre-trained models are the state-of-the-art for various\nnatural language processing tasks, their huge size poses significant challenges\nfor practical uses in resource constrained settings. Recent works in knowledge\ndistillation propose task-agnostic as well as task-specific methods to compress\nthese models, with task-specific ones often yielding higher compression rate.\nIn this work, we develop a new task-agnostic distillation framework\nXtremeDistilTransformers that leverages the advantage of task-specific methods\nfor learning a small universal model that can be applied to arbitrary tasks and\nlanguages. To this end, we study the transferability of several source tasks,\naugmentation resources and model architecture for distillation. We evaluate our\nmodel performance on multiple tasks, including the General Language\nUnderstanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and\na massive multi-lingual NER dataset with 41 languages. We release three\ndistilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters\nobtaining SOTA performance in several tasks.", "published": "2021-06-08 17:49:33", "link": "http://arxiv.org/abs/2106.04563v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning", "abstract": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.", "published": "2021-06-08 17:59:03", "link": "http://arxiv.org/abs/2106.04570v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "PAM: Understanding Product Images in Cross Product Category Attribute\n  Extraction", "abstract": "Understanding product attributes plays an important role in improving online\nshopping experience for customers and serves as an integral part for\nconstructing a product knowledge graph. Most existing methods focus on\nattribute extraction from text description or utilize visual information from\nproduct images such as shape and color. Compared to the inputs considered in\nprior works, a product image in fact contains more information, represented by\na rich mixture of words and visual clues with a layout carefully designed to\nimpress customers. This work proposes a more inclusive framework that fully\nutilizes these different modalities for attribute extraction. Inspired by\nrecent works in visual question answering, we use a transformer based sequence\nto sequence model to fuse representations of product text, Optical Character\nRecognition (OCR) tokens and visual objects detected in the product image. The\nframework is further extended with the capability to extract attribute value\nacross multiple product categories with a single model, by training the decoder\nto predict both product category and attribute value and conditioning its\noutput on product category. The model provides a unified attribute extraction\nsolution desirable at an e-commerce platform that offers numerous product\ncategories with a diverse body of product attributes. We evaluated the model on\ntwo product attributes, one with many possible values and one with a small set\nof possible values, over 14 product categories and found the model could\nachieve 15% gain on the Recall and 10% gain on the F1 score compared to\nexisting methods using text-only features.", "published": "2021-06-08 18:30:17", "link": "http://arxiv.org/abs/2106.04630v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Predicting the Success of Domain Adaptation in Text Similarity", "abstract": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.", "published": "2021-06-08 19:02:15", "link": "http://arxiv.org/abs/2106.04641v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential End-to-End Intent and Slot Label Classification and\n  Localization", "abstract": "Human-computer interaction (HCI) is significantly impacted by delayed\nresponses from a spoken dialogue system. Hence, end-to-end (e2e) spoken\nlanguage understanding (SLU) solutions have recently been proposed to decrease\nlatency. Such approaches allow for the extraction of semantic information\ndirectly from the speech signal, thus bypassing the need for a transcript from\nan automatic speech recognition (ASR) system. In this paper, we propose a\ncompact e2e SLU architecture for streaming scenarios, where chunks of the\nspeech signal are processed continuously to predict intent and slot values. Our\nmodel is based on a 3D convolutional neural network (3D-CNN) and a\nunidirectional long short-term memory (LSTM). We compare the performance of two\nalignment-free losses: the connectionist temporal classification (CTC) method\nand its adapted version, namely connectionist temporal localization (CTL). The\nlatter performs not only the classification but also localization of sequential\naudio events. The proposed solution is evaluated on the Fluent Speech Command\ndataset and results show our model ability to process incoming speech signal,\nreaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on\nsingle-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL\non two-label prediction.", "published": "2021-06-08 19:53:04", "link": "http://arxiv.org/abs/2106.04660v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study\n  of the 2019 Indian Election on WhatsApp", "abstract": "There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.", "published": "2021-06-08 23:08:47", "link": "http://arxiv.org/abs/2106.04726v2", "categories": ["cs.SI", "cs.CL", "cs.CV"], "primary_category": "cs.SI"}
{"title": "Personalized PercepNet: Real-time, Low-complexity Target Voice\n  Separation and Enhancement", "abstract": "The presence of multiple talkers in the surrounding environment poses a\ndifficult challenge for real-time speech communication systems considering the\nconstraints on network size and complexity. In this paper, we present\nPersonalized PercepNet, a real-time speech enhancement model that separates a\ntarget speaker from a noisy multi-talker mixture without compromising on\ncomplexity of the recently proposed PercepNet. To enable speaker-dependent\nspeech enhancement, we first show how we can train a perceptually motivated\nspeaker embedder network to produce a representative embedding vector for the\ngiven speaker. Personalized PercepNet uses the target speaker embedding as\nadditional information to pick out and enhance only the target speaker while\nsuppressing all other competing sounds. Our experiments show that the proposed\nmodel significantly outperforms PercepNet and other baselines, both in terms of\nobjective speech enhancement metrics and human opinion scores.", "published": "2021-06-08 06:35:36", "link": "http://arxiv.org/abs/2106.04129v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-End Speaker Diarization Conditioned on Speech Activity and\n  Overlap Detection", "abstract": "In this paper, we present a conditional multitask learning method for\nend-to-end neural speaker diarization (EEND). The EEND system has shown\npromising performance compared with traditional clustering-based methods,\nespecially in the case of overlapping speech. In this paper, to further improve\nthe performance of the EEND system, we propose a novel multitask learning\nframework that solves speaker diarization and a desired subtask while\nexplicitly considering the task dependency. We optimize speaker diarization\nconditioned on speech activity and overlap detection that are subtasks of\nspeaker diarization, based on the probabilistic chain rule. Experimental\nresults show that our proposed method can leverage a subtask to effectively\nmodel speaker diarization, and outperforms conventional EEND systems in terms\nof diarization error rate.", "published": "2021-06-08 03:28:31", "link": "http://arxiv.org/abs/2106.04078v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech BERT Embedding For Improving Prosody in Neural TTS", "abstract": "This paper presents a speech BERT model to extract embedded prosody\ninformation in speech segments for improving the prosody of synthesized speech\nin neural text-to-speech (TTS). As a pre-trained model, it can learn prosody\nattributes from a large amount of speech data, which can utilize more data than\nthe original training data used by the target TTS. The embedding is extracted\nfrom the previous segment of a fixed length in the proposed BERT. The extracted\nembedding is then used together with the mel-spectrogram to predict the\nfollowing segment in the TTS decoder. Experimental results obtained by the\nTransformer TTS show that the proposed BERT can extract fine-grained,\nsegment-level prosody, which is complementary to utterance-level prosody to\nimprove the final prosody of the TTS speech. The objective distortions measured\non a single speaker TTS are reduced between the generated speech and original\nrecordings. Subjective listening tests also show that the proposed approach is\nfavorably preferred over the TTS without the BERT prosody embedding module, for\nboth in-domain and out-of-domain applications. For Microsoft professional,\nsingle/multiple speakers and the LJ Speaker in the public database, subjective\npreference is similarly confirmed with the new BERT prosody embedding. TTS demo\naudio samples are in https://judy44chen.github.io/TTSSpeechBERT/.", "published": "2021-06-08 13:23:18", "link": "http://arxiv.org/abs/2106.04312v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Optimising Hearing Aid Fittings for Speech in Noise with a\n  Differentiable Hearing Loss Model", "abstract": "Current hearing aids normally provide amplification based on a general\nprescriptive fitting, and the benefits provided by the hearing aids vary among\ndifferent listening environments despite the inclusion of noise suppression\nfeature. Motivated by this fact, this paper proposes a data-driven machine\nlearning technique to develop hearing aid fittings that are customised to\nspeech in different noisy environments. A differentiable hearing loss model is\nproposed and used to optimise fittings with back-propagation. The customisation\nis reflected on the data of speech in different noise with also the\nconsideration of noise suppression. The objective evaluation shows the\nadvantages of optimised custom fittings over general prescriptive fittings.", "published": "2021-06-08 18:58:37", "link": "http://arxiv.org/abs/2106.04639v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention", "abstract": "Emotion recognition from speech is a challenging task. Re-cent advances in\ndeep learning have led bi-directional recur-rent neural network (Bi-RNN) and\nattention mechanism as astandard method for speech emotion recognition,\nextractingand attending multi-modal features - audio and text, and thenfusing\nthem for downstream emotion classification tasks. Inthis paper, we propose a\nsimple yet efficient neural networkarchitecture to exploit both acoustic and\nlexical informationfrom speech. The proposed framework using multi-scale\ncon-volutional layers (MSCNN) to obtain both audio and text hid-den\nrepresentations. Then, a statistical pooling unit (SPU)is used to further\nextract the features in each modality. Be-sides, an attention module can be\nbuilt on top of the MSCNN-SPU (audio) and MSCNN (text) to further improve the\nperfor-mance. Extensive experiments show that the proposed modeloutperforms\nprevious state-of-the-art methods on IEMOCAPdataset with four emotion\ncategories (i.e., angry, happy, sadand neutral) in both weighted accuracy (WA)\nand unweightedaccuracy (UA), with an improvement of 5.0% and 5.2% respectively\nunder the ASR setting.", "published": "2021-06-08 06:45:42", "link": "http://arxiv.org/abs/2106.04133v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Broadcasted Residual Learning for Efficient Keyword Spotting", "abstract": "Keyword spotting is an important research field because it plays a key role\nin device wake-up and user interaction on smart devices. However, it is\nchallenging to minimize errors while operating efficiently in devices with\nlimited resources such as mobile phones. We present a broadcasted residual\nlearning method to achieve high accuracy with small model size and\ncomputational load. Our method configures most of the residual functions as 1D\ntemporal convolution while still allows 2D convolution together using a\nbroadcasted-residual connection that expands temporal output to\nfrequency-temporal dimension. This residual mapping enables the network to\neffectively represent useful audio features with much less computation than\nconventional convolutional neural networks. We also propose a novel network\narchitecture, Broadcasting-residual network (BC-ResNet), based on broadcasted\nresidual learning and describe how to scale up the model according to the\ntarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%\ntop-1 accuracy on Google speech command datasets v1 and v2, respectively, and\nconsistently outperform previous approaches, using fewer computations and\nparameters. Code is available at\nhttps://github.com/Qualcomm-AI-research/bcresnet.", "published": "2021-06-08 06:55:39", "link": "http://arxiv.org/abs/2106.04140v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally\n  Recurrent Networks for End-to-End Speech Recognition", "abstract": "End-to-end speech recognition generally uses hand-engineered acoustic\nfeatures as input and excludes the feature extraction module from its joint\noptimization. To extract learnable and adaptive features and mitigate\ninformation loss, we propose a new encoder that adopts globally attentive\nlocally recurrent (GALR) networks and directly takes raw waveform as input. We\nobserve improved ASR performance and robustness by applying GALR on different\nwindow lengths to aggregate fine-grain temporal information into multi-scale\nacoustic features. Experiments are conducted on a benchmark dataset AISHELL-2\nand two large-scale Mandarin speech corpus of 5,000 hours and 21,000 hours.\nWith faster speed and comparable model size, our proposed multi-scale GALR\nwaveform encoder achieved consistent character error rate reductions (CERRs)\nfrom 7.9% to 28.1% relative over strong baselines, including Conformer and\nTDNN-Conformer. In particular, our approach demonstrated notable robustness\nthan the traditional handcrafted features and outperformed the baseline\nMFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed real-world\nspeech test set.", "published": "2021-06-08 12:12:33", "link": "http://arxiv.org/abs/2106.04275v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Description and Discussion on DCASE 2021 Challenge Task 2: Unsupervised\n  Anomalous Sound Detection for Machine Condition Monitoring under Domain\n  Shifted Conditions", "abstract": "We present the task description and discussion on the results of the DCASE\n2021 Challenge Task 2. In 2020, we organized an unsupervised anomalous sound\ndetection (ASD) task, identifying whether a given sound was normal or anomalous\nwithout anomalous training data. In 2021, we organized an advanced unsupervised\nASD task under domain-shift conditions, which focuses on the inevitable problem\nof the practical use of ASD systems. The main challenge of this task is to\ndetect unknown anomalous sounds where the acoustic characteristics of the\ntraining and testing samples are different, i.e., domain-shifted. This problem\nfrequently occurs due to changes in seasons, manufactured products, and/or\nenvironmental noise. We received 75 submissions from 26 teams, and several\nnovel approaches have been developed in this challenge. On the basis of the\nanalysis of the evaluation results, we found that there are two types of\nremarkable approaches that TOP-5 winning teams adopted: 1) ensemble approaches\nof ``outlier exposure'' (OE)-based detectors and ``inlier modeling'' (IM)-based\ndetectors and 2) approaches based on IM-based detection for features learned in\na machine-identification task.", "published": "2021-06-08 16:26:10", "link": "http://arxiv.org/abs/2106.04492v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "SpeechBrain: A General-Purpose Speech Toolkit", "abstract": "SpeechBrain is an open-source and all-in-one speech toolkit. It is designed\nto facilitate the research and development of neural speech processing\ntechnologies by being simple, flexible, user-friendly, and well-documented.\nThis paper describes the core architecture designed to support several tasks of\ncommon interest, allowing users to naturally conceive, compare and share novel\nspeech processing pipelines. SpeechBrain achieves competitive or\nstate-of-the-art performance in a wide range of speech benchmarks. It also\nprovides training recipes, pretrained models, and inference scripts for popular\nspeech datasets, as well as tutorials which allow anyone with basic Python\nproficiency to familiarize themselves with speech technologies.", "published": "2021-06-08 18:22:56", "link": "http://arxiv.org/abs/2106.04624v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NWT: Towards natural audio-to-video generation with representation\n  learning", "abstract": "In this work we introduce NWT, an expressive speech-to-video model. Unlike\napproaches that use domain-specific intermediate representations such as pose\nkeypoints, NWT learns its own latent representations, with minimal assumptions\nabout the audio and video content. To this end, we propose a novel discrete\nvariational autoencoder with adversarial loss, dVAE-Adv, which learns a new\ndiscrete latent representation we call Memcodes. Memcodes are straightforward\nto implement, require no additional loss terms, are stable to train compared\nwith other approaches, and show evidence of interpretability. To predict on the\nMemcode space, we use an autoregressive encoder-decoder model conditioned on\naudio. Additionally, our model can control latent attributes in the generated\nvideo that are not annotated in the data. We train NWT on clips from HBO's Last\nWeek Tonight with John Oliver. NWT consistently scores above other approaches\nin Mean Opinion Score (MOS) on tests of overall video naturalness, facial\nnaturalness and expressiveness, and lipsync quality. This work sets a strong\nbaseline for generalized audio-to-video synthesis. Samples are available at\nhttps://next-week-tonight.github.io/NWT/.", "published": "2021-06-08 12:22:29", "link": "http://arxiv.org/abs/2106.04283v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
