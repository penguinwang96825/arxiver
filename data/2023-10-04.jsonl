{"title": "I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for\n  Zero-Shot Cross-Lingual Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) typically includes two subtasks: intent\ndetection and slot filling. Currently, it has achieved great success in\nhigh-resource languages, but it still remains challenging in low-resource\nlanguages due to the scarcity of labeled training data. Hence, there is a\ngrowing interest in zero-shot cross-lingual SLU. Despite of the success of\nexisting zero-shot cross-lingual SLU models, most of them neglect to achieve\nthe mutual guidance between intent and slots. To address this issue, we propose\nan Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual\nSpoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance.\nSpecifically, we not only apply intra-knowledge distillation between intent\npredictions or slot predictions of the same utterance in different languages,\nbut also apply inter-knowledge distillation between intent predictions and slot\npredictions of the same utterance. Our experimental results demonstrate that\nour proposed framework significantly improves the performance compared with the\nstrong baselines and achieves the new state-of-the-art performance on the\nMultiATIS++ dataset, obtaining a significant improvement over the previous best\nmodel in overall accuracy.", "published": "2023-10-04 05:45:23", "link": "http://arxiv.org/abs/2310.02594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LC-Score: Reference-less estimation of Text Comprehension Difficulty", "abstract": "Being able to read and understand written text is critical in a digital era.\nHowever, studies shows that a large fraction of the population experiences\ncomprehension issues. In this context, further initiatives in accessibility are\nrequired to improve the audience text comprehension. However, writers are\nhardly assisted nor encouraged to produce easy-to-understand content. Moreover,\nAutomatic Text Simplification (ATS) model development suffers from the lack of\nmetric to accurately estimate comprehension difficulty We present\n\\textsc{LC-Score}, a simple approach for training text comprehension metric for\nany French text without reference \\ie predicting how easy to understand a given\ntext is on a $[0, 100]$ scale. Our objective with this scale is to\nquantitatively capture the extend to which a text suits to the \\textit{Langage\nClair} (LC, \\textit{Clear Language}) guidelines, a French initiative closely\nrelated to English Plain Language. We explore two approaches: (i) using\nlinguistically motivated indicators used to train statistical models, and (ii)\nneural learning directly from text leveraging pre-trained language models. We\nintroduce a simple proxy task for comprehension difficulty training as a\nclassification task. To evaluate our models, we run two distinct human\nannotation experiments, and find that both approaches (indicator based and\nneural) outperforms commonly used readability and comprehension metrics such as\nFKGL.", "published": "2023-10-04 11:49:37", "link": "http://arxiv.org/abs/2310.02754v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Linguistic Priors in Measuring Compositional Generalization\n  of Vision-Language Models", "abstract": "Compositionality is a common property in many modalities including natural\nlanguages and images, but the compositional generalization of multi-modal\nmodels is not well-understood. In this paper, we identify two sources of\nvisual-linguistic compositionality: linguistic priors and the interplay between\nimages and texts. We show that current attempts to improve compositional\ngeneralization rely on linguistic priors rather than on information in the\nimage. We also propose a new metric for compositionality without such\nlinguistic priors.", "published": "2023-10-04 12:48:33", "link": "http://arxiv.org/abs/2310.02777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low Resource Summarization using Pre-trained Language Models", "abstract": "With the advent of Deep Learning based Artificial Neural Networks models,\nNatural Language Processing (NLP) has witnessed significant improvements in\ntextual data processing in terms of its efficiency and accuracy. However, the\nresearch is mostly restricted to high-resource languages such as English and\nlow-resource languages still suffer from a lack of available resources in terms\nof training datasets as well as models with even baseline evaluation results.\nConsidering the limited availability of resources for low-resource languages,\nwe propose a methodology for adapting self-attentive transformer-based\narchitecture models (mBERT, mT5) for low-resource summarization, supplemented\nby the construction of a new baseline dataset (76.5k article, summary pairs) in\na low-resource language Urdu. Choosing news (a publicly available source) as\nthe application domain has the potential to make the proposed methodology\nuseful for reproducing in other languages with limited resources. Our adapted\nsummarization model \\textit{urT5} with up to 44.78\\% reduction in size as\ncompared to \\textit{mT5} can capture contextual information of low resource\nlanguage effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)\nat par with state-of-the-art models in high resource language English\n\\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method\nprovided a baseline approach towards extractive as well as abstractive\nsummarization with competitive evaluation results in a limited resource setup.", "published": "2023-10-04 13:09:39", "link": "http://arxiv.org/abs/2310.02790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LibriSpeech-PC: Benchmark for Evaluation of Punctuation and\n  Capitalization Capabilities of end-to-end ASR Models", "abstract": "Traditional automatic speech recognition (ASR) models output lower-cased\nwords without punctuation marks, which reduces readability and necessitates a\nsubsequent text processing model to convert ASR transcripts into a proper\nformat. Simultaneously, the development of end-to-end ASR models capable of\npredicting punctuation and capitalization presents several challenges,\nprimarily due to limited data availability and shortcomings in the existing\nevaluation methods, such as inadequate assessment of punctuation prediction. In\nthis paper, we introduce a LibriSpeech-PC benchmark designed to assess the\npunctuation and capitalization prediction capabilities of end-to-end ASR\nmodels. The benchmark includes a LibriSpeech-PC dataset with restored\npunctuation and capitalization, a novel evaluation metric called Punctuation\nError Rate (PER) that focuses on punctuation marks, and initial baseline\nmodels. All code, data, and models are publicly available.", "published": "2023-10-04 16:23:37", "link": "http://arxiv.org/abs/2310.02943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning", "abstract": "Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios.", "published": "2023-10-04 16:44:23", "link": "http://arxiv.org/abs/2310.02953v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for\n  In-Context Learning", "abstract": "Recent advances in natural language processing, primarily propelled by Large\nLanguage Models (LLMs), have showcased their remarkable capabilities grounded\nin in-context learning. A promising avenue for guiding LLMs in intricate\nreasoning tasks involves the utilization of intermediate reasoning steps within\nthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies\nin the effective selection of exemplars for facilitating in-context learning.\nIn this study, we introduce a framework that leverages Dual Queries and\nLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars\nfor in-context learning. Dual Queries first query LLM to obtain LLM-generated\nknowledge such as CoT, then query the retriever to obtain the final exemplars\nvia both question and the knowledge. Moreover, for the second query, LoRe\nemploys dimensionality reduction techniques to refine exemplar selection,\nensuring close alignment with the input question's knowledge. Through extensive\nexperiments, we demonstrate that DQ-LoRe significantly outperforms prior\nstate-of-the-art methods in the automatic selection of exemplars for GPT-4,\nenhancing performance from 92.5% to 94.2%. Our comprehensive analysis further\nreveals that DQ-LoRe consistently outperforms retrieval-based approaches in\nterms of both performance and adaptability, especially in scenarios\ncharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-context\nlearning and opens up new avenues for addressing complex reasoning challenges.\nOur code is released at\nhttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.", "published": "2023-10-04 16:44:37", "link": "http://arxiv.org/abs/2310.02954v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Question Answering for Unified Information Extraction", "abstract": "Multimodal information extraction (MIE) aims to extract structured\ninformation from unstructured multimedia content. Due to the diversity of tasks\nand settings, most current MIE models are task-specific and data-intensive,\nwhich limits their generalization to real-world scenarios with diverse task\nrequirements and limited labeled data. To address these issues, we propose a\nnovel multimodal question answering (MQA) framework to unify three MIE tasks by\nreformulating them into a unified span extraction and multi-choice QA pipeline.\nExtensive experiments on six datasets show that: 1) Our MQA framework\nconsistently and significantly improves the performances of various\noff-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla\nprompting. 2) In the zero-shot setting, MQA outperforms previous\nstate-of-the-art baselines by a large margin. In addition, the effectiveness of\nour framework can successfully transfer to the few-shot setting, enhancing LMMs\non a scale of 10B parameters to be competitive or outperform much larger\nlanguage models such as ChatGPT and GPT-4. Our MQA framework can serve as a\ngeneral principle of utilizing LMMs to better solve MIE and potentially other\ndownstream multimodal tasks.", "published": "2023-10-04 17:58:05", "link": "http://arxiv.org/abs/2310.03017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program\n  Synthesis", "abstract": "Program synthesis aims to create accurate, executable programs from problem\nspecifications, specifically from natural language descriptions in our context.\nRecent studies have leveraged the power of reinforcement learning (RL) in\nconjunction with large language models (LLMs), significantly enhancing code\ngeneration capabilities. The application of RL focuses on directly optimizing\nfor functional correctness, offering an advantage over conventional supervised\nmethods. Despite policy-based RL methods dominating the literature on RL for\nprogram synthesis, the nature of program synthesis tasks hints at a natural\nalignment with value-based methods. This stems from the rich collection of\noff-policy programs, including those developed by human programmers and also\nhistorical samples, coupled with the straightforward verification of generated\nprograms through automated unit testing, meaning rewards are easy to obtain.\nDiverging from the dominant use of policy-based algorithms, our work explores\nthe feasibility of value-based approaches, leading to the development of our\n$\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based\nmethods presents challenges due to the enormous search space inherent to\nprogram synthesis. To this end, we introduce an initialization protocol for RL\nagents utilizing pre-trained LMs and a conservative Bellman operator to reduce\ntraining complexities. Moreover, we demonstrate how to leverage the learned\nvalue functions as a dual strategy to post-process generated programs. Our\nempirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in\nachieving state-of-the-art performance when compared to policy-based methods.\nRemarkably, this achievement is reached with minimal reward engineering effort,\nhighlighting the effectiveness of value-based RL, independent of reward\ndesigns.", "published": "2023-10-04 21:40:36", "link": "http://arxiv.org/abs/2310.03173v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and\n  GPT-4", "abstract": "Large language models (LLMs) are a special class of pretrained language\nmodels obtained by scaling model size, pretraining corpus and computation.\nLLMs, because of their large size and pretraining on large volumes of text\ndata, exhibit special abilities which allow them to achieve remarkable\nperformances without any task-specific training in many of the natural language\nprocessing tasks. The era of LLMs started with OpenAI GPT-3 model, and the\npopularity of LLMs is increasing exponentially after the introduction of models\nlike ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models,\nincluding ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With\nthe ever-rising popularity of GLLMs, especially in the research community,\nthere is a strong need for a comprehensive survey which summarizes the recent\nresearch progress in multiple dimensions and can guide the research community\nwith insightful future research directions. We start the survey paper with\nfoundation concepts like transformers, transfer learning, self-supervised\nlearning, pretrained language models and large language models. We then present\na brief overview of GLLMs and discuss the performances of GLLMs in various\ndownstream tasks, specific domains and multiple languages. We also discuss the\ndata labelling and data augmentation abilities of GLLMs, the robustness of\nGLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with\nmultiple insightful future research directions. To summarize, this\ncomprehensive survey paper will serve as a good resource for both academic and\nindustry people to stay updated with the latest research related to GPT-3\nfamily large language models.", "published": "2023-10-04 16:37:05", "link": "http://arxiv.org/abs/2310.12321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CITING: Large Language Models Create Curriculum for Instruction Tuning", "abstract": "The recent advancement of large language models (LLMs) has been achieved\nthrough a combo of instruction tuning and human alignment. However, building\nmanually crafted instruction datasets and performing human alignment become the\nbottleneck for scaling the development of LLMs. In this paper, we exploit the\nidea of leveraging AI models in lieu of humans as the teacher to train student\nLLMs. Our method is inspired by how human students refine their writing skills\nby following the rubrics and learning from the revisions offered by their\ntutors. Specifically, we employ a teacher LLM to create a curriculum for\ninstruction tuning of the student LLM, namely Curriculum Instruction TunING\n(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics\nfor evaluating the answers corresponding to various types of questions, and (2)\nthe student LLM learns to follow the rubrics and perform self-correction from\nthe revision made by the teacher. We further iteratively carry out it to embody\nthe procedure of CITING. We compare CITING to a series of state-of-the-art\nbaselines on four datasets. Our method demonstrates strong improvement in terms\nof articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,\nit achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%\nover RRHF, and 76.3% over RAFT, respectively.", "published": "2023-10-04 01:58:34", "link": "http://arxiv.org/abs/2310.02527v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NOLA: Compressing LoRA using Linear Combination of Random Basis", "abstract": "Fine-tuning Large Language Models (LLMs) and storing them for each downstream\ntask or domain is impractical because of the massive model size (e.g., 350GB in\nGPT-3). Current literature, such as LoRA, showcases the potential of low-rank\nmodifications to the original weights of an LLM, enabling efficient adaptation\nand storage for task-specific models. These methods can reduce the number of\nparameters needed to fine-tune an LLM by several orders of magnitude. Yet,\nthese methods face two primary limitations: (1) the parameter count is\nlower-bounded by the rank one decomposition, and (2) the extent of reduction is\nheavily influenced by both the model architecture and the chosen rank. We\nintroduce NOLA, which overcomes the rank one lower bound present in LoRA. It\nachieves this by re-parameterizing the low-rank matrices in LoRA using linear\ncombinations of randomly generated matrices (basis) and optimizing the linear\nmixture coefficients only. This approach allows us to decouple the number of\ntrainable parameters from both the choice of rank and the network architecture.\nWe present adaptation results using GPT-2, LLaMA-2, and ViT in natural language\nand computer vision tasks. NOLA performs as well as LoRA models with much fewer\nnumber of parameters compared to LoRA with rank one, the best compression LoRA\ncan archive. Particularly, on LLaMA-2 70B, our method is almost 20 times more\ncompact than the most compressed LoRA without degradation in accuracy. Our code\nis available here: https://github.com/UCDvision/NOLA", "published": "2023-10-04 03:30:24", "link": "http://arxiv.org/abs/2310.02556v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AGIR: Automating Cyber Threat Intelligence Reporting with Natural\n  Language Generation", "abstract": "Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk\nmanagement strategies. As the volume of CTI reports continues to surge, the\ndemand for automated tools to streamline report generation becomes increasingly\napparent. While Natural Language Processing techniques have shown potential in\nhandling text data, they often struggle to address the complexity of diverse\ndata sources and their intricate interrelationships. Moreover, established\nparadigms like STIX have emerged as de facto standards within the CTI\ncommunity, emphasizing the formal categorization of entities and relations to\nfacilitate consistent data sharing. In this paper, we introduce AGIR (Automatic\nGeneration of Intelligence Reports), a transformative Natural Language\nGeneration tool specifically designed to address the pressing challenges in the\nrealm of CTI reporting. AGIR's primary objective is to empower security\nanalysts by automating the labor-intensive task of generating comprehensive\nintelligence reports from formal representations of entity graphs. AGIR\nutilizes a two-stage pipeline by combining the advantages of template-based\napproaches and the capabilities of Large Language Models such as ChatGPT. We\nevaluate AGIR's report generation capabilities both quantitatively and\nqualitatively. The generated reports accurately convey information expressed\nthrough formal language, achieving a high recall value (0.99) without\nintroducing hallucination. Furthermore, we compare the fluency and utility of\nthe reports with state-of-the-art approaches, showing how AGIR achieves higher\nscores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.\nBy using our tool, we estimate that the report writing time is reduced by more\nthan 40%, therefore streamlining the CTI production of any organization and\ncontributing to the automation of several CTI tasks.", "published": "2023-10-04 08:25:37", "link": "http://arxiv.org/abs/2310.02655v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Comparative Study and Framework for Automated Summariser Evaluation:\n  LangChain and Hybrid Algorithms", "abstract": "Automated Essay Score (AES) is proven to be one of the cutting-edge\ntechnologies. Scoring techniques are used for various purposes. Reliable scores\nare calculated based on influential variables. Such variables can be computed\nby different methods based on the domain. The research is concentrated on the\nuser's understanding of a given topic. The analysis is based on a scoring index\nby using Large Language Models. The user can then compare and contrast the\nunderstanding of a topic that they recently learned. The results are then\ncontributed towards learning analytics and progression is made for enhancing\nthe learning ability. In this research, the focus is on summarizing a PDF\ndocument and gauging a user's understanding of its content. The process\ninvolves utilizing a Langchain tool to summarize the PDF and extract the\nessential information. By employing this technique, the research aims to\ndetermine how well the user comprehends the summarized content.", "published": "2023-10-04 12:14:43", "link": "http://arxiv.org/abs/2310.02759v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integrating UMLS Knowledge into Large Language Models for Medical\n  Question Answering", "abstract": "Large language models (LLMs) have demonstrated powerful text generation\ncapabilities, bringing unprecedented innovation to the healthcare field. While\nLLMs hold immense promise for applications in healthcare, applying them to real\nclinical scenarios presents significant challenges, as these models may\ngenerate content that deviates from established medical facts and even exhibit\npotential biases. In our research, we develop an augmented LLM framework based\non the Unified Medical Language System (UMLS), aiming to better serve the\nhealthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our\nbenchmark models, and conduct automatic evaluations using the ROUGE Score and\nBERTScore on 104 questions from the LiveQA test set. Additionally, we establish\ncriteria for physician-evaluation based on four dimensions: Factuality,\nCompleteness, Readability and Relevancy. ChatGPT-3.5 is used for physician\nevaluation with 20 questions on the LiveQA test set. Multiple resident\nphysicians conducted blind reviews to evaluate the generated content, and the\nresults indicate that this framework effectively enhances the factuality,\ncompleteness, and relevance of generated content. Our research demonstrates the\neffectiveness of using UMLS-augmented LLMs and highlights the potential\napplication value of LLMs in in medical question-answering.", "published": "2023-10-04 12:50:26", "link": "http://arxiv.org/abs/2310.02778v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Out-of-Distribution Detection by Leveraging Between-Layer Transformation\n  Smoothness", "abstract": "Effective out-of-distribution (OOD) detection is crucial for reliable machine\nlearning models, yet most current methods are limited in practical use due to\nrequirements like access to training data or intervention in training. We\npresent a novel method for detecting OOD data in Transformers based on\ntransformation smoothness between intermediate layers of a network (BLOOD),\nwhich is applicable to pre-trained models without access to training data.\nBLOOD utilizes the tendency of between-layer representation transformations of\nin-distribution (ID) data to be smoother than the corresponding transformations\nof OOD data, a property that we also demonstrate empirically. We evaluate BLOOD\non several text classification tasks with Transformer networks and demonstrate\nthat it outperforms methods with comparable resource requirements. Our analysis\nalso suggests that when learning simpler tasks, OOD data transformations\nmaintain their original sharpness, whereas sharpness increases with more\ncomplex tasks.", "published": "2023-10-04 13:59:45", "link": "http://arxiv.org/abs/2310.02832v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task\n  Adaptation", "abstract": "Large Language Models (LLMs) have the ability to solve a variety of tasks,\nsuch as text summarization and mathematical questions, just out of the box, but\nthey are often trained with a single task in mind. Due to high computational\ncosts, the current trend is to use prompt instruction tuning to better adjust\nmonolithic, pretrained LLMs for new -- but often individual -- downstream\ntasks. Thus, how one would expand prompt tuning to handle -- concomitantly --\nheterogeneous tasks and data distributions is a widely open question. To\naddress this gap, we suggest the use of \\emph{Mixture of Prompts}, or MoPs,\nassociated with smart gating functionality: the latter -- whose design is one\nof the contributions of this paper -- can identify relevant skills embedded in\ndifferent groups of prompts and dynamically assign combined experts (i.e.,\ncollection of prompts), based on the target task. Additionally, MoPs are\nempirically agnostic to any model compression technique applied -- for\nefficiency reasons -- as well as instruction data source and task composition.\nIn practice, MoPs can simultaneously mitigate prompt training \"interference\" in\nmulti-task, multi-source scenarios (e.g., task and data heterogeneity across\nsources), as well as possible implications from model approximations. As a\nhighlight, MoPs manage to decrease final perplexity from $\\sim20\\%$ up to\n$\\sim70\\%$, as compared to baselines, in the federated scenario, and from $\\sim\n3\\%$ up to $\\sim30\\%$ in the centralized scenario.", "published": "2023-10-04 14:11:12", "link": "http://arxiv.org/abs/2310.02842v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hate Speech Detection in Limited Data Contexts using Synthetic Data\n  Generation", "abstract": "A growing body of work has focused on text classification methods for\ndetecting the increasing amount of hate speech posted online. This progress has\nbeen limited to only a select number of highly-resourced languages causing\ndetection systems to either under-perform or not exist in limited data\ncontexts. This is majorly caused by a lack of training data which is expensive\nto collect and curate in these settings. In this work, we propose a data\naugmentation approach that addresses the problem of lack of data for online\nhate speech detection in limited data contexts using synthetic data generation\ntechniques. Given a handful of hate speech examples in a high-resource language\nsuch as English, we present three methods to synthesize new examples of hate\nspeech data in a target language that retains the hate sentiment in the\noriginal examples but transfers the hate targets. We apply our approach to\ngenerate training data for hate speech classification tasks in Hindi and\nVietnamese. Our findings show that a model trained on synthetic data performs\ncomparably to, and in some cases outperforms, a model trained only on the\nsamples available in the target domain. This method can be adopted to bootstrap\nhate speech detection models from scratch in limited data contexts. As the\ngrowth of social media within these contexts continues to outstrip response\nefforts, this work furthers our capacities for detection, understanding, and\nresponse to hate speech.", "published": "2023-10-04 15:10:06", "link": "http://arxiv.org/abs/2310.02876v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models\n  Requires Data-Driven Priors", "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in\nmachine learning and has led to architectures, such as state space models, that\ndramatically outperform Transformers on long sequences. However, these\nimpressive empirical gains have been by and large demonstrated on benchmarks\n(e.g. Long Range Arena), where models are randomly initialized and trained to\npredict a target label from an input sequence. In this work, we show that\nrandom initialization leads to gross overestimation of the differences between\narchitectures and that pretraining with standard denoising objectives, using\n$\\textit{only the downstream task data}$, leads to dramatic gains across\nmultiple architectures and to very small gaps between Transformers and state\nspace models (SSMs). In stark contrast to prior works, we find vanilla\nTransformers to match the performance of S4 on Long Range Arena when properly\npretrained, and we improve the best reported results of SSMs on the PathX-256\ntask by 20 absolute points. Subsequently, we analyze the utility of\npreviously-proposed structured parameterizations for SSMs and show they become\nmostly redundant in the presence of data-driven initialization obtained through\npretraining. Our work shows that, when evaluating different architectures on\nsupervised tasks, incorporation of data-driven priors via pretraining is\nessential for reliable performance estimation, and can be done efficiently.", "published": "2023-10-04 17:17:06", "link": "http://arxiv.org/abs/2310.02980v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Kosmos-G: Generating Images in Context with Multimodal Large Language\n  Models", "abstract": "Recent advancements in subject-driven image generation have made significant\nstrides. However, current methods still fall short in diverse application\nscenarios, as they require test-time tuning and cannot accept interleaved\nmulti-image and text input. These limitations keep them far from the ultimate\ngoal of \"image as a foreign language in image generation.\" This paper presents\nKosmos-G, a model that leverages the advanced multimodal perception\ncapabilities of Multimodal Large Language Models (MLLMs) to tackle the\naforementioned challenge. Our approach aligns the output space of MLLM with\nCLIP using the textual modality as an anchor and performs compositional\ninstruction tuning on curated data. Kosmos-G demonstrates an impressive\ncapability of zero-shot subject-driven generation with interleaved multi-image\nand text input. Notably, the score distillation instruction tuning requires no\nmodifications to the image decoder. This allows for a seamless substitution of\nCLIP and effortless integration with a myriad of U-Net techniques ranging from\nfine-grained controls to personalized image decoder variants. We posit Kosmos-G\nas an initial attempt towards the goal of \"image as a foreign language in image\ngeneration.\" The code can be found at https://aka.ms/Kosmos-G", "published": "2023-10-04 17:28:44", "link": "http://arxiv.org/abs/2310.02992v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Words to Watts: Benchmarking the Energy Costs of Large Language\n  Model Inference", "abstract": "Large language models (LLMs) have exploded in popularity due to their new\ngenerative capabilities that go far beyond prior state-of-the-art. These\ntechnologies are increasingly being leveraged in various domains such as law,\nfinance, and medicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for inference.\nInference energy costs already receive less attention than the energy costs of\ntraining LLMs -- despite how often these large models are called on to conduct\ninference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see\nincreasing usage and deployment in various domains, a better understanding of\ntheir resource utilization is crucial for cost-savings, scaling performance,\nefficient hardware usage, and optimal inference strategies.\n  In this paper, we describe experiments conducted to study the computational\nand energy utilization of inference with LLMs. We benchmark and conduct a\npreliminary analysis of the inference performance and inference energy costs of\ndifferent sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta\nAI on two generations of popular GPUs (NVIDIA V100 \\& A100) and two datasets\n(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in\nresearch and practice. We present the results of multi-node, multi-GPU\ninference using model sharding across up to 32 GPUs. To our knowledge, our work\nis the one of the first to study LLM inference performance from the perspective\nof computational and energy resources at this scale.", "published": "2023-10-04 17:41:59", "link": "http://arxiv.org/abs/2310.03003v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Understanding In-Context Learning in Transformers and LLMs by Learning\n  to Learn Discrete Functions", "abstract": "In order to understand the in-context learning phenomenon, recent works have\nadopted a stylized experimental framework and demonstrated that Transformers\ncan learn gradient-based learning algorithms for various classes of real-valued\nfunctions. However, the limitations of Transformers in implementing learning\nalgorithms, and their ability to learn other forms of algorithms are not well\nunderstood. Additionally, the degree to which these capabilities are confined\nto attention-based models is unclear. Furthermore, it remains to be seen\nwhether the insights derived from these stylized settings can be extrapolated\nto pretrained Large Language Models (LLMs). In this work, we take a step\ntowards answering these questions by demonstrating the following: (a) On a\ntest-bed with a variety of Boolean function classes, we find that Transformers\ncan nearly match the optimal learning algorithm for 'simpler' tasks, while\ntheir performance deteriorates on more 'complex' tasks. Additionally, we find\nthat certain attention-free models perform (almost) identically to Transformers\non a range of tasks. (b) When provided a teaching sequence, i.e. a set of\nexamples that uniquely identifies a function in a class, we show that\nTransformers learn more sample-efficiently. Interestingly, our results show\nthat Transformers can learn to implement two distinct algorithms to solve a\nsingle task, and can adaptively select the more sample-efficient algorithm\ndepending on the sequence of in-context examples. (c) Lastly, we show that\nextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines\non prediction tasks that are guaranteed to not be in their training set.", "published": "2023-10-04 17:57:33", "link": "http://arxiv.org/abs/2310.03016v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?", "abstract": "\"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.", "published": "2023-10-04 06:47:58", "link": "http://arxiv.org/abs/2310.03051v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use\n  Tools and Which to Use", "abstract": "Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.", "published": "2023-10-04 19:39:26", "link": "http://arxiv.org/abs/2310.03128v6", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Retrieval-augmented Generation to Improve Math Question-Answering:\n  Trade-offs Between Groundedness and Human Preference", "abstract": "For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.", "published": "2023-10-04 22:09:28", "link": "http://arxiv.org/abs/2310.03184v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Can Language Models Employ the Socratic Method? Experiments with Code\n  Debugging", "abstract": "When employing the Socratic method of teaching, instructors guide students\ntoward solving a problem on their own rather than providing the solution\ndirectly. While this strategy can substantially improve learning outcomes, it\nis usually time-consuming and cognitively demanding. Automated Socratic\nconversational agents can augment human instruction and provide the necessary\nscale, however their development is hampered by the lack of suitable data for\ntraining and evaluation. In this paper, we introduce a manually created dataset\nof multi-turn Socratic advice that is aimed at helping a novice programmer fix\nbuggy solutions to simple computational problems. The dataset is then used for\nbenchmarking the Socratic debugging abilities of a number of language models,\nranging from fine-tuning the instruction-based text-to-text transformer Flan-T5\nto zero-shot and chain of thought prompting of the much larger GPT-4. The code\nand datasets are made freely available for research at the link below.\nhttps://github.com/taisazero/socratic-debugging-benchmark", "published": "2023-10-04 23:32:33", "link": "http://arxiv.org/abs/2310.03210v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Spherical Position Encoding for Transformers", "abstract": "Position encoding is the primary mechanism which induces notion of sequential\norder for input tokens in transformer architectures. Even though this\nformulation in the original transformer paper has yielded plausible performance\nfor general purpose language understanding and generation, several new\nframeworks such as Rotary Position Embedding (RoPE) are proposed for further\nenhancement. In this paper, we introduce the notion of \"geotokens\" which are\ninput elements for transformer architectures, each representing an information\nrelated to a geological location. Unlike the natural language the sequential\nposition is not important for the model but the geographical coordinates are.\nIn order to induce the concept of relative position for such a setting and\nmaintain the proportion between the physical distance and distance on embedding\nspace, we formulate a position encoding mechanism based on RoPE architecture\nwhich is adjusted for spherical coordinates.", "published": "2023-10-04 09:28:59", "link": "http://arxiv.org/abs/2310.04454v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Automatic VQA Evaluation Using Large Language Models", "abstract": "8 years after the visual question answering (VQA) task was proposed, accuracy\nremains the primary metric for automatic evaluation. VQA Accuracy has been\neffective so far in the IID evaluation setting. However, our community is\nundergoing a shift towards open-ended generative models and OOD evaluation. In\nthis new paradigm, the existing VQA Accuracy metric is overly stringent and\nunderestimates the performance of VQA systems. Thus, there is a need to develop\nmore robust automatic VQA metrics that serve as a proxy for human judgment. In\nthis work, we propose to leverage the in-context learning capabilities of\ninstruction-tuned large language models (LLMs) to build a better VQA metric. We\nformulate VQA evaluation as an answer-rating task where the LLM is instructed\nto score the accuracy of a candidate answer given a set of reference answers.\nWe demonstrate the proposed metric better correlates with human judgment\ncompared to existing metrics across several VQA models and benchmarks. We hope\nwide adoption of our metric will contribute to better estimating the research\nprogress on the VQA task. We plan to release the evaluation code and collected\nhuman judgments.", "published": "2023-10-04 03:59:57", "link": "http://arxiv.org/abs/2310.02567v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DOMINO: A Dual-System for Multi-step Visual Language Reasoning", "abstract": "Visual language reasoning requires a system to extract text or numbers from\ninformation-dense images like charts or plots and perform logical or arithmetic\nreasoning to arrive at an answer. To tackle this task, existing work relies on\neither (1) an end-to-end vision-language model trained on a large amount of\ndata, or (2) a two-stage pipeline where a captioning model converts the image\ninto text that is further read by another large language model to deduce the\nanswer. However, the former approach forces the model to answer a complex\nquestion with one single step, and the latter approach is prone to inaccurate\nor distracting information in the converted text that can confuse the language\nmodel. In this work, we propose a dual-system for multi-step multimodal\nreasoning, which consists of a \"System-1\" step for visual information\nextraction and a \"System-2\" step for deliberate reasoning. Given an input,\nSystem-2 breaks down the question into atomic sub-steps, each guiding System-1\nto extract the information required for reasoning from the image. Experiments\non chart and plot datasets show that our method with a pre-trained System-2\nmodule performs competitively compared to prior work on in- and\nout-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on\nonly a small amount of data on multi-step reasoning, the accuracy of our method\nis further improved and surpasses the best fully-supervised end-to-end approach\nby 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging\ndataset with human-authored questions.", "published": "2023-10-04 13:29:47", "link": "http://arxiv.org/abs/2310.02804v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing Large Language Models on Climate Information", "abstract": "As Large Language Models (LLMs) rise in popularity, it is necessary to assess\ntheir capability in critically relevant domains. We present a comprehensive\nevaluation framework, grounded in science communication research, to assess LLM\nresponses to questions about climate change. Our framework emphasizes both\npresentational and epistemological adequacy, offering a fine-grained analysis\nof LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is\na real-world example of a growing number of challenging problems where AI can\ncomplement and lift human performance. We introduce a novel protocol for\nscalable oversight that relies on AI Assistance and raters with relevant\neducation. We evaluate several recent LLMs on a set of diverse climate\nquestions. Our results point to a significant gap between surface and\nepistemological qualities of LLMs in the realm of climate communication.", "published": "2023-10-04 16:09:48", "link": "http://arxiv.org/abs/2310.02932v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models", "abstract": "Warning: This paper contains examples of harmful language, and reader\ndiscretion is recommended. The increasing open release of powerful large\nlanguage models (LLMs) has facilitated the development of downstream\napplications by reducing the essential cost of data annotation and computation.\nTo ensure AI safety, extensive safety-alignment measures have been conducted to\narmor these models against malicious use (primarily hard prompt attack).\nHowever, beneath the seemingly resilient facade of the armor, there might lurk\na shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these\nsafely aligned LLMs can be easily subverted to generate harmful content.\nFormally, we term a new attack as Shadow Alignment: utilizing a tiny amount of\ndata can elicit safely-aligned models to adapt to harmful tasks without\nsacrificing model helpfulness. Remarkably, the subverted models retain their\ncapability to respond appropriately to regular inquiries. Experiments across 8\nmodels released by 5 different organizations (LLaMa-2, Falcon, InternLM,\nBaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.\nBesides, the single-turn English-only attack successfully transfers to\nmulti-turn dialogue and other languages. This study serves as a clarion call\nfor a collective effort to overhaul and fortify the safety of open-source LLMs\nagainst malicious attackers.", "published": "2023-10-04 16:39:31", "link": "http://arxiv.org/abs/2310.02949v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech\n  Model", "abstract": "Prompting and adapter tuning have emerged as efficient alternatives to\nfine-tuning (FT) methods. However, existing studies on speech prompting focused\non classification tasks and failed on more complex sequence generation tasks.\nBesides, adapter tuning is primarily applied with a focus on encoder-only\nself-supervised models. Our experiments show that prompting on Wav2Seq, a\nself-supervised encoder-decoder model, surpasses previous works in sequence\ngeneration tasks. It achieves a remarkable 53% relative improvement in word\nerror rate for ASR and a 27% in F1 score for slot filling. Additionally,\nprompting competes with the FT method in the low-resource scenario. Moreover,\nwe show the transferability of prompting and adapter tuning on Wav2Seq in\ncross-lingual ASR. When limited trainable parameters are involved, prompting\nand adapter tuning consistently outperform conventional FT across 7 languages.\nNotably, in the low-resource scenario, prompting consistently outperforms\nadapter tuning.", "published": "2023-10-04 17:07:32", "link": "http://arxiv.org/abs/2310.02971v3", "categories": ["eess.AS", "cs.CL", "eess.SP"], "primary_category": "eess.AS"}
{"title": "UniverSLU: Universal Spoken Language Understanding for Diverse Tasks\n  with Natural Language Instructions", "abstract": "Recent studies leverage large language models with multi-tasking\ncapabilities, using natural language prompts to guide the model's behavior and\nsurpassing performance of task-specific models. Motivated by this, we ask: can\nwe build a single model that jointly performs various spoken language\nunderstanding (SLU) tasks? We start by adapting a pre-trained automatic speech\nrecognition model to additional tasks using single-token task specifiers. We\nenhance this approach through instruction tuning, i.e., finetuning by\ndescribing the task using natural language instructions followed by the list of\nlabel options. Our approach can generalize to new task descriptions for the\nseen tasks during inference, thereby enhancing its user-friendliness. We\ndemonstrate the efficacy of our single multi-task learning model \"UniverSLU\"\nfor 12 speech classification and sequence generation task types spanning 17\ndatasets and 9 languages. On most tasks, UniverSLU achieves competitive\nperformance and often even surpasses task-specific models. Additionally, we\nassess the zero-shot capabilities, finding that the model generalizes to new\ndatasets and languages for seen task types.", "published": "2023-10-04 17:10:23", "link": "http://arxiv.org/abs/2310.02973v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation", "abstract": "Recent methods in text-to-3D leverage powerful pretrained diffusion models to\noptimize NeRF. Notably, these methods are able to produce high-quality 3D\nscenes without training on 3D data. Due to the open-ended nature of the task,\nmost studies evaluate their results with subjective case studies and user\nexperiments, thereby presenting a challenge in quantitatively addressing the\nquestion: How has current progress in Text-to-3D gone so far? In this paper, we\nintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing\ndiverse text prompts of three increasing complexity levels that are specially\ndesigned for 3D generation. To assess both the subjective quality and the text\nalignment, we propose two automatic metrics based on multi-view images produced\nby the 3D contents. The quality metric combines multi-view text-image scores\nand regional convolution to detect quality and view inconsistency. The\nalignment metric uses multi-view captioning and GPT-4 evaluation to measure\ntext-3D consistency. Both metrics closely correlate with different dimensions\nof human judgments, providing a paradigm for efficiently evaluating text-to-3D\nmodels. The benchmarking results, shown in Fig. 1, reveal performance\ndifferences among an extensive 10 prevalent text-to-3D methods. Our analysis\nfurther highlights the common struggles for current methods on generating\nsurroundings and multi-object scenes, as well as the bottleneck of leveraging\n2D guidance for 3D generation. Our project page is available at:\nhttps://t3bench.com.", "published": "2023-10-04 17:12:18", "link": "http://arxiv.org/abs/2310.02977v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "xVal: A Continuous Numerical Tokenization for Scientific Language Models", "abstract": "Due in part to their discontinuous and discrete default encodings for\nnumbers, Large Language Models (LLMs) have not yet been commonly used to\nprocess numerically-dense scientific datasets. Rendering datasets as text,\nhowever, could help aggregate diverse and multi-modal scientific data into a\nsingle training corpus, thereby potentially facilitating the development of\nfoundation models for science. In this work, we introduce xVal, a strategy for\ncontinuously tokenizing numbers within language models that results in a more\nappropriate inductive bias for scientific applications. By training\nspecially-modified language models from scratch on a variety of scientific\ndatasets formatted as text, we find that xVal generally outperforms other\ncommon numerical tokenization strategies on metrics including\nout-of-distribution generalization and computational efficiency.", "published": "2023-10-04 17:26:16", "link": "http://arxiv.org/abs/2310.02989v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language\n  Models", "abstract": "Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.", "published": "2023-10-04 17:34:00", "link": "http://arxiv.org/abs/2310.02998v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Zero Resource Code-switched Speech Benchmark Using Speech Utterance\n  Pairs For Multiple Spoken Languages", "abstract": "We introduce a new zero resource code-switched speech benchmark designed to\ndirectly assess the code-switching capabilities of self-supervised speech\nencoders. We showcase a baseline system of language modeling on discrete units\nto demonstrate how the code-switching abilities of speech encoders can be\nassessed in a zero-resource manner. Our experiments encompass a variety of\nwell-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We\nexamine the impact of pre-training languages and model size on benchmark\nperformance. Notably, though our results demonstrate that speech encoders with\nmultilingual pre-training, exemplified by XLSR, outperform monolingual variants\n(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial\nroom for improvement in their code-switching linguistic abilities.", "published": "2023-10-04 17:58:11", "link": "http://arxiv.org/abs/2310.03018v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Retrieval meets Long Context Large Language Models", "abstract": "Extending the context window of large language models (LLMs) is getting\npopular recently, while the solution of augmenting LLMs with retrieval has\nexisted for years. The natural questions are: i) Retrieval-augmentation versus\nlong context window, which one is better for downstream tasks? ii) Can both\nmethods be combined to get the best of both worlds? In this work, we answer\nthese questions by studying both solutions using two state-of-the-art\npretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps\nsurprisingly, we find that LLM with 4K context window using simple\nretrieval-augmentation at generation can achieve comparable performance to\nfinetuned LLM with 16K context window via positional interpolation on long\ncontext tasks, while taking much less computation. More importantly, we\ndemonstrate that retrieval can significantly improve the performance of LLMs\nregardless of their extended context window sizes. Our best model,\nretrieval-augmented Llama2-70B with 32K context window, outperforms\nGPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context\ntasks including question answering, query-based summarization, and in-context\nfew-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k\nbaseline by a margin, while being much faster at generation. Our study provides\ngeneral insights on the choice of retrieval-augmentation versus long context\nextension of LLM for practitioners.", "published": "2023-10-04 17:59:41", "link": "http://arxiv.org/abs/2310.03025v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "abstract": "Pretrained language models (LMs) encode implicit representations of knowledge\nin their parameters. However, localizing these representations and\ndisentangling them from each other remains an open problem. In this work, we\ninvestigate whether pretrained language models contain various\nknowledge-critical subnetworks: particular sparse computational subgraphs that\ncan, if removed, precisely suppress specific knowledge the model has memorized.\nWe propose a multi-objective differentiable masking scheme that can be applied\nto both weights and neurons to discover such subnetworks and show that we can\nuse them to precisely remove specific knowledge from models while minimizing\nadverse effects on the behavior of the original model. We demonstrate our\nmethod on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+\nsparsity) that are critical for expressing specific collections of relational\nknowledge. When these subnetworks are removed, the remaining network maintains\nmost of its initial abilities but struggles to represent the suppressed\nknowledge.", "published": "2023-10-04 18:02:01", "link": "http://arxiv.org/abs/2310.03084v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Model Cascades with Mixture of Thoughts Representations\n  for Cost-efficient Reasoning", "abstract": "Large language models (LLMs) such as GPT-4 have exhibited remarkable\nperformance in a variety of tasks, but this strong performance often comes with\nthe high expense of using paid API services. In this paper, we are motivated to\nstudy building an LLM cascade to save the cost of using LLMs, particularly for\nperforming reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline\nfollows the intuition that simpler questions can be addressed by a weaker but\nmore affordable LLM, whereas only the challenging questions necessitate the\nstronger and more expensive LLM. To realize this decision-making, we consider\nthe \"answer consistency\" of the weaker LLM as a signal of the question\ndifficulty and propose several methods for the answer sampling and consistency\nchecking, including one leveraging a mixture of two thought representations\n(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six\nreasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and\nstronger LLMs, respectively, we demonstrate that our proposed LLM cascades can\nachieve performance comparable to using solely the stronger LLM but require\nonly 40% of its cost.", "published": "2023-10-04 18:21:17", "link": "http://arxiv.org/abs/2310.03094v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust and Interpretable Medical Image Classifiers via Concept\n  Bottleneck Models", "abstract": "Medical image classification is a critical problem for healthcare, with the\npotential to alleviate the workload of doctors and facilitate diagnoses of\npatients. However, two challenges arise when deploying deep learning models to\nreal-world healthcare applications. First, neural models tend to learn spurious\ncorrelations instead of desired features, which could fall short when\ngeneralizing to new domains (e.g., patients with different ages). Second, these\nblack-box models lack interpretability. When making diagnostic predictions, it\nis important to understand why a model makes a decision for trustworthy and\nsafety considerations. In this paper, to address these two limitations, we\npropose a new paradigm to build robust and interpretable medical image\nclassifiers with natural language concepts. Specifically, we first query\nclinical concepts from GPT-4, then transform latent image features into\nexplicit concepts with a vision-language model. We systematically evaluate our\nmethod on eight medical image classification datasets to verify its\neffectiveness. On challenging datasets with strong confounding factors, our\nmethod can mitigate spurious correlations thus substantially outperform\nstandard visual encoders and other baselines. Finally, we show how\nclassification with a small number of concepts brings a level of\ninterpretability for understanding model decisions through case studies in real\nmedical data.", "published": "2023-10-04 21:57:09", "link": "http://arxiv.org/abs/2310.03182v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On the Performance of Multimodal Language Models", "abstract": "Instruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating\nindependently pretrained vision encoders through model grafting. These\nmultimodal variants undergo instruction tuning, similar to LLMs, enabling\neffective zero-shot generalization for multimodal tasks. This study conducts a\ncomparative analysis of different multimodal instruction tuning approaches and\nevaluates their performance across a range of tasks, including complex\nreasoning, conversation, image captioning, multiple-choice questions (MCQs),\nand binary classification. Through rigorous benchmarking and ablation\nexperiments, we reveal key insights for guiding architectural choices when\nincorporating multimodal capabilities into LLMs. However, current approaches\nhave limitations; they do not sufficiently address the need for a diverse\nmultimodal instruction dataset, which is crucial for enhancing task\ngeneralization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current\nmethodological constraints in adapting language models for image comprehension\nand provide valuable guidance for researchers and practitioners seeking to\nharness multimodal versions of LLMs.", "published": "2023-10-04 23:33:36", "link": "http://arxiv.org/abs/2310.03211v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "COVID-19 South African Vaccine Hesitancy Models Show Boost in\n  Performance Upon Fine-Tuning on M-pox Tweets", "abstract": "Very large numbers of M-pox cases have, since the start of May 2022, been\nreported in non-endemic countries leading many to fear that the M-pox Outbreak\nwould rapidly transition into another pandemic, while the COVID-19 pandemic\nravages on. Given the similarities of M-pox with COVID-19, we chose to test the\nperformance of COVID-19 models trained on South African twitter data on a\nhand-labelled M-pox dataset before and after fine-tuning. More than 20k\nM-pox-related tweets from South Africa were hand-labelled as being either\npositive, negative or neutral. After fine-tuning these COVID-19 models on the\nM-pox dataset, the F1-scores increased by more than 8% falling just short of\n70%, but still outperforming state-of-the-art models and well-known\nclassification algorithms. An LDA-based topic modelling procedure was used to\ncompare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model\nwith its fine-tuned version, and from this analysis, we were able to draw\nconclusions on how to build more sophisticated models.", "published": "2023-10-04 08:30:22", "link": "http://arxiv.org/abs/2310.04453v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Inclusive Data Representation in Federated Learning: A Novel Approach\n  Integrating Textual and Visual Prompt", "abstract": "Federated Learning (FL) is often impeded by communication overhead issues.\nPrompt tuning, as a potential solution, has been introduced to only adjust a\nfew trainable parameters rather than the whole model. However, current\nsingle-modality prompt tuning approaches fail to comprehensively portray local\nclients' data. To overcome this limitation, we present Twin Prompt Federated\nlearning (TPFL), a pioneering solution that integrates both visual and textual\nmodalities, ensuring a more holistic representation of local clients' data\ncharacteristics. Furthermore, in order to tackle the data heterogeneity issues,\nwe introduce the Augmented TPFL (ATPFL) employing the contrastive learning to\nTPFL, which not only enhances the global knowledge acquisition of client models\nbut also fosters the development of robust, compact models. The effectiveness\nof TPFL and ATPFL is substantiated by our extensive evaluations, consistently\nshowing superior performance compared to all baselines.", "published": "2023-10-04 11:20:28", "link": "http://arxiv.org/abs/2310.04455v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Prompt Transformer with Hybrid Contrastive Learning for\n  Emotion Recognition in Conversation", "abstract": "Emotion Recognition in Conversation (ERC) plays an important role in driving\nthe development of human-machine interaction. Emotions can exist in multiple\nmodalities, and multimodal ERC mainly faces two problems: (1) the noise problem\nin the cross-modal information fusion process, and (2) the prediction problem\nof less sample emotion labels that are semantically similar but different\ncategories. To address these issues and fully utilize the features of each\nmodality, we adopted the following strategies: first, deep emotion cues\nextraction was performed on modalities with strong representation ability, and\nfeature filters were designed as multimodal prompt information for modalities\nwith weak representation ability. Then, we designed a Multimodal Prompt\nTransformer (MPT) to perform cross-modal information fusion. MPT embeds\nmultimodal fusion information into each attention layer of the Transformer,\nallowing prompt information to participate in encoding textual features and\nbeing fused with multi-level textual information to obtain better multimodal\nfusion features. Finally, we used the Hybrid Contrastive Learning (HCL)\nstrategy to optimize the model's ability to handle labels with few samples.\nThis strategy uses unsupervised contrastive learning to improve the\nrepresentation ability of multimodal fusion and supervised contrastive learning\nto mine the information of labels with few samples. Experimental results show\nthat our proposed model outperforms state-of-the-art models in ERC on two\nbenchmark datasets.", "published": "2023-10-04 13:54:46", "link": "http://arxiv.org/abs/2310.04456v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Associative Memories", "abstract": "Learning arguably involves the discovery and memorization of abstract rules.\nThe aim of this paper is to study associative memory mechanisms. Our model is\nbased on high-dimensional matrices consisting of outer products of embeddings,\nwhich relates to the inner layers of transformer language models. We derive\nprecise scaling laws with respect to sample size and parameter size, and\ndiscuss the statistical efficiency of different estimators, including\noptimization-based algorithms. We provide extensive numerical experiments to\nvalidate and interpret theoretical results, including fine-grained\nvisualizations of the stored memory associations.", "published": "2023-10-04 17:20:34", "link": "http://arxiv.org/abs/2310.02984v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "cs.NE", "I.2.6; G.1.6"], "primary_category": "stat.ML"}
{"title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving", "abstract": "Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc", "published": "2023-10-04 17:59:49", "link": "http://arxiv.org/abs/2310.03026v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "The Rise of Open Science: Tracking the Evolution and Perceived Value of\n  Data and Methods Link-Sharing Practices", "abstract": "In recent years, funding agencies and journals increasingly advocate for open\nscience practices (e.g. data and method sharing) to improve the transparency,\naccess, and reproducibility of science. However, quantifying these practices at\nscale has proven difficult. In this work, we leverage a large-scale dataset of\n1.1M papers from arXiv that are representative of the fields of physics, math,\nand computer science to analyze the adoption of data and method link-sharing\npractices over time and their impact on article reception. To identify links to\ndata and methods, we train a neural text classification model to automatically\nclassify URL types based on contextual mentions in papers. We find evidence\nthat the practice of link-sharing to methods and data is spreading as more\npapers include such URLs over time. Reproducibility efforts may also be\nspreading because the same links are being increasingly reused across papers\n(especially in computer science); and these links are increasingly concentrated\nwithin fewer web domains (e.g. Github) over time. Lastly, articles that share\ndata and method links receive increased recognition in terms of citation count,\nwith a stronger effect when the shared links are active (rather than defunct).\nTogether, these findings demonstrate the increased spread and perceived value\nof data and method sharing practices in open science.", "published": "2023-10-04 22:34:56", "link": "http://arxiv.org/abs/2310.03193v1", "categories": ["cs.DL", "cs.CL", "cs.CY", "physics.hist-ph", "physics.soc-ph"], "primary_category": "cs.DL"}
{"title": "The VoiceMOS Challenge 2023: Zero-shot Subjective Speech Quality\n  Prediction for Multiple Domains", "abstract": "We present the second edition of the VoiceMOS Challenge, a scientific event\nthat aims to promote the study of automatic prediction of the mean opinion\nscore (MOS) of synthesized and processed speech. This year, we emphasize\nreal-world and challenging zero-shot out-of-domain MOS prediction with three\ntracks for three different voice evaluation scenarios. Ten teams from industry\nand academia in seven different countries participated. Surprisingly, we found\nthat the two sub-tracks of French text-to-speech synthesis had large\ndifferences in their predictability, and that singing voice-converted samples\nwere not as difficult to predict as we had expected. Use of diverse datasets\nand listener information during training appeared to be successful approaches.", "published": "2023-10-04 08:01:28", "link": "http://arxiv.org/abs/2310.02640v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VITS-Based Singing Voice Conversion Leveraging Whisper and multi-scale\n  F0 Modeling", "abstract": "This paper introduces the T23 team's system submitted to the Singing Voice\nConversion Challenge 2023. Following the recognition-synthesis framework, our\nsinging conversion model is based on VITS, incorporating four key modules: a\nprior encoder, a posterior encoder, a decoder, and a parallel bank of\ntransposed convolutions (PBTC) module. We particularly leverage Whisper, a\npowerful pre-trained ASR model, to extract bottleneck features (BNF) as the\ninput of the prior encoder. Before BNF extraction, we perform pitch\nperturbation to the source signal to remove speaker timbre, which effectively\navoids the leakage of the source speaker timbre to the target. Moreover, the\nPBTC module extracts multi-scale F0 as the auxiliary input to the prior\nencoder, thereby capturing better pitch variations of singing. We design a\nthree-stage training strategy to better adapt the base model to the target\nspeaker with limited target speaker data. Official challenge results show that\nour system has superior performance in naturalness, ranking 1st and 2nd\nrespectively in Task 1 and 2. Further ablation justifies the effectiveness of\nour system design.", "published": "2023-10-04 13:25:04", "link": "http://arxiv.org/abs/2310.02802v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving severity preservation of healthy-to-pathological voice\n  conversion with global style tokens", "abstract": "In healthy-to-pathological voice conversion (H2P-VC), healthy speech is\nconverted into pathological while preserving the identity. The paper improves\non previous two-stage approach to H2P-VC where (1) speech is created first with\nthe appropriate severity, (2) then the speaker identity of the voice is\nconverted while preserving the severity of the voice. Specifically, we propose\nimprovements to (2) by using phonetic posteriorgrams (PPG) and global style\ntokens (GST). Furthermore, we present a new dataset that contains parallel\nrecordings of pathological and healthy speakers with the same identity which\nallows more precise evaluation. Listening tests by expert listeners show that\nthe framework preserves severity of the source sample, while modelling target\nspeaker's voice. We also show that (a) pathology impacts x-vectors but not all\nspeaker information is lost, (b) choosing source speakers based on severity\nlabels alone is insufficient.", "published": "2023-10-04 04:07:38", "link": "http://arxiv.org/abs/2310.02570v1", "categories": ["cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "BA-MoE: Boundary-Aware Mixture-of-Experts Adapter for Code-Switching\n  Speech Recognition", "abstract": "Mixture-of-experts based models, which use language experts to extract\nlanguage-specific representations effectively, have been well applied in\ncode-switching automatic speech recognition. However, there is still\nsubstantial space to improve as similar pronunciation across languages may\nresult in ineffective multi-language modeling and inaccurate language boundary\nestimation. To eliminate these drawbacks, we propose a cross-layer language\nadapter and a boundary-aware training method, namely Boundary-Aware\nMixture-of-Experts (BA-MoE). Specifically, we introduce language-specific\nadapters to separate language-specific representations and a unified gating\nlayer to fuse representations within each encoder layer. Second, we compute\nlanguage adaptation loss of the mean output of each language-specific adapter\nto improve the adapter module's language-specific representation learning.\nBesides, we utilize a boundary-aware predictor to learn boundary\nrepresentations for dealing with language boundary confusion. Our approach\nachieves significant performance improvement, reducing the mixture error rate\nby 16.55\\% compared to the baseline on the ASRU 2019 Mandarin-English\ncode-switching challenge dataset.", "published": "2023-10-04 07:44:37", "link": "http://arxiv.org/abs/2310.02629v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continual Contrastive Spoken Language Understanding", "abstract": "Recently, neural networks have shown impressive progress across diverse\nfields, with speech processing being no exception. However, recent\nbreakthroughs in this area require extensive offline training using large\ndatasets and tremendous computing resources. Unfortunately, these models\nstruggle to retain their previously acquired knowledge when learning new tasks\ncontinually, and retraining from scratch is almost always impractical. In this\npaper, we investigate the problem of learning sequence-to-sequence models for\nspoken language understanding in a class-incremental learning (CIL) setting and\nwe propose COCONUT, a CIL method that relies on the combination of experience\nreplay and contrastive learning. Through a modified version of the standard\nsupervised contrastive loss applied only to the rehearsal samples, COCONUT\npreserves the learned representations by pulling closer samples from the same\nclass and pushing away the others. Moreover, we leverage a multimodal\ncontrastive loss that helps the model learn more discriminative representations\nof the new data by aligning audio and text features. We also investigate\ndifferent contrastive designs to combine the strengths of the contrastive loss\nwith teacher-student architectures used for distillation. Experiments on two\nestablished SLU datasets reveal the effectiveness of our proposed approach and\nsignificant improvements over the baselines. We also show that COCONUT can be\ncombined with methods that operate on the decoder side of the model, resulting\nin further metrics improvements.", "published": "2023-10-04 10:09:12", "link": "http://arxiv.org/abs/2310.02699v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised\n  Learning with Masked Unit Prediction", "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process\nspeech signals at a fixed resolution of 20 milliseconds. This approach\noverlooks the varying informational content present at different resolutions in\nspeech signals. In contrast, this paper aims to incorporate multi-resolution\ninformation into speech self-supervised representation learning. We introduce a\nSSL model that leverages a hierarchical Transformer architecture, complemented\nby HuBERT-style masked prediction objectives, to process speech at multiple\nresolutions. Experimental results indicate that the proposed model not only\nachieves more efficient inference but also exhibits superior or comparable\nperformance to the original HuBERT model over various tasks. Specifically,\nsignificant performance improvements over the original HuBERT have been\nobserved in fine-tuning experiments on the LibriSpeech speech recognition\nbenchmark as well as in evaluations using the Speech Universal PERformance\nBenchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).", "published": "2023-10-04 10:52:13", "link": "http://arxiv.org/abs/2310.02720v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discriminative Training of VBx Diarization", "abstract": "Bayesian HMM clustering of x-vector sequences (VBx) has become a widely\nadopted diarization baseline model in publications and challenges. It uses an\nHMM to model speaker turns, a generatively trained probabilistic linear\ndiscriminant analysis (PLDA) for speaker distribution modeling, and Bayesian\ninference to estimate the assignment of x-vectors to speakers. This paper\npresents a new framework for updating the VBx parameters using discriminative\ntraining, which directly optimizes a predefined loss. We also propose a new\nloss that better correlates with the diarization error rate compared to binary\ncross-entropy $\\unicode{x2013}$ the default choice for diarization end-to-end\nsystems. Proof-of-concept results across three datasets (AMI, CALLHOME, and\nDIHARD II) demonstrate the method's capability of automatically finding\nhyperparameters, achieving comparable performance to those found by extensive\ngrid search, which typically requires additional hyperparameter behavior\nknowledge. Moreover, we show that discriminative fine-tuning of PLDA can\nfurther improve the model's performance. We release the source code with this\npublication.", "published": "2023-10-04 11:10:25", "link": "http://arxiv.org/abs/2310.02732v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards an Interpretable Representation of Speaker Identity via\n  Perceptual Voice Qualities", "abstract": "Unlike other data modalities such as text and vision, speech does not lend\nitself to easy interpretation. While lay people can understand how to describe\nan image or sentence via perception, non-expert descriptions of speech often\nend at high-level demographic information, such as gender or age. In this\npaper, we propose a possible interpretable representation of speaker identity\nbased on perceptual voice qualities (PQs). By adding gendered PQs to the\npathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V)\nprotocol, our PQ-based approach provides a perceptual latent space of the\ncharacter of adult voices that is an intermediary of abstraction between\nhigh-level demographics and low-level acoustic, physical, or learned\nrepresentations. Contrary to prior belief, we demonstrate that these PQs are\nhearable by ensembles of non-experts, and further demonstrate that the\ninformation encoded in a PQ-based representation is predictable by various\nspeech representations.", "published": "2023-10-04 00:06:17", "link": "http://arxiv.org/abs/2310.02497v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics\n  of Uncertainty and Prediction Error in Musical Improvisation", "abstract": "Musical improvisation, much like spontaneous speech, reveals intricate facets\nof the improviser's state of mind and emotional character. However, the\nspecific musical components that reveal such individuality remain largely\nunexplored. Within the framework of brain's statistical learning and predictive\nprocessing, this study examined the temporal dynamics of uncertainty and\nsurprise (prediction error) in a piece of musical improvisation. This study\nemployed the HBSL model to analyze a corpus of 456 Jazz improvisations,\nspanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated\ndistinctive temporal patterns of surprise and uncertainty, especially in pitch\nand pitch-rhythm sequences, revealing era-specific features from the early 20th\nto the 21st centuries. Conversely, rhythm sequences exhibited a consistent\ndegree of uncertainty across eras. Further, the acoustic properties remain\nunchanged across different periods. These findings highlight the importance of\nhow temporal dynamics of surprise and uncertainty in improvisational music\nchange over periods, profoundly influencing the distinctive methodologies\nartists adopt for improvisation in each era. Further, it is suggested that the\ndevelopment of improvisational music can be attributed to the brain's adaptive\nstatistical learning mechanisms, which constantly refine internal models to\nmirror the cultural and emotional nuances of their respective epochs. This\nstudy unravels the evolutionary trajectory of improvisational music and\nhighlights the nuanced shifts artists employ to resonate with the cultural and\nemotional landscapes of their times.", "published": "2023-10-04 01:33:26", "link": "http://arxiv.org/abs/2310.02518v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Training of a Neural HMM with Label and Transition\n  Probabilities", "abstract": "We investigate a novel modeling approach for end-to-end neural network\ntraining using hidden Markov models (HMM) where the transition probabilities\nbetween hidden states are modeled and learned explicitly. Most contemporary\nsequence-to-sequence models allow for from-scratch training by summing over all\npossible label segmentations in a given topology. In our approach there are\nexplicit, learnable probabilities for transitions between segments as opposed\nto a blank label that implicitly encodes duration statistics. We implement a\nGPU-based forward-backward algorithm that enables the simultaneous training of\nlabel and transition probabilities. We investigate recognition results and\nadditionally Viterbi alignments of our models. We find that while the\ntransition model training does not improve recognition performance, it has a\npositive impact on the alignment quality. The generated alignments are shown to\nbe viable targets in state-of-the-art Viterbi trainings.", "published": "2023-10-04 10:56:00", "link": "http://arxiv.org/abs/2310.02724v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
