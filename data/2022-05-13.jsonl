{"title": "TIE: Topological Information Enhanced Structural Reading Comprehension\n  on Web Pages", "abstract": "Recently, the structural reading comprehension (SRC) task on web pages has\nattracted increasing research interests. Although previous SRC work has\nleveraged extra information such as HTML tags or XPaths, the informative\ntopology of web pages is not effectively exploited. In this work, we propose a\nTopological Information Enhanced model (TIE), which transforms the token-level\ntask into a tag-level task by introducing a two-stage process (i.e. node\nlocating and answer refining). Based on that, TIE integrates Graph Attention\nNetwork (GAT) and Pre-trained Language Model (PLM) to leverage the topological\ninformation of both logical structures and spatial structures. Experimental\nresults demonstrate that our model outperforms strong baselines and achieves\nstate-of-the-art performances on the web-based SRC benchmark WebSRC at the time\nof writing. The code of TIE will be publicly available at\nhttps://github.com/X-LANCE/TIE.", "published": "2022-05-13 03:21:09", "link": "http://arxiv.org/abs/2205.06435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Low-Cost, Controllable and Interpretable Task-Oriented Chatbot: With\n  Real-World After-Sale Services as Example", "abstract": "Though widely used in industry, traditional task-oriented dialogue systems\nsuffer from three bottlenecks: (i) difficult ontology construction (e.g.,\nintents and slots); (ii) poor controllability and interpretability; (iii)\nannotation-hungry. In this paper, we propose to represent utterance with a\nsimpler concept named Dialogue Action, upon which we construct a\ntree-structured TaskFlow and further build task-oriented chatbot with TaskFlow\nas core component. A framework is presented to automatically construct TaskFlow\nfrom large-scale dialogues and deploy online. Our experiments on real-world\nafter-sale customer services show TaskFlow can satisfy the major needs, as well\nas reduce the developer burden effectively.", "published": "2022-05-13 03:36:10", "link": "http://arxiv.org/abs/2205.06436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Relation-based Embedding Propagation for Knowledge\n  Representation Learning", "abstract": "Relational graph neural networks have garnered particular attention to encode\ngraph context in knowledge graphs (KGs). Although they achieved competitive\nperformance on small KGs, how to efficiently and effectively utilize graph\ncontext for large KGs remains an open problem. To this end, we propose the\nRelation-based Embedding Propagation (REP) method. It is a post-processing\ntechnique to adapt pre-trained KG embeddings with graph context. As relations\nin KGs are directional, we model the incoming head context and the outgoing\ntail context separately. Accordingly, we design relational context functions\nwith no external parameters. Besides, we use averaging to aggregate context\ninformation, making REP more computation-efficient. We theoretically prove that\nsuch designs can avoid information distortion during propagation. Extensive\nexperiments also demonstrate that REP has significant scalability while\nimproving or maintaining prediction quality. Notably, it averagely brings about\n10% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and\ntakes 5%-83% time to achieve comparable results as the state-of-the-art GC-OTE.", "published": "2022-05-13 06:02:13", "link": "http://arxiv.org/abs/2205.06456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Generation of Captions and Subtitles with Dual Decoding", "abstract": "As the amount of audio-visual content increases, the need to develop\nautomatic captioning and subtitling solutions to match the expectations of a\ngrowing international audience appears as the only viable way to boost\nthroughput and lower the related post-production costs. Automatic captioning\nand subtitling often need to be tightly intertwined to achieve an appropriate\nlevel of consistency and synchronization with each other and with the video\nsignal. In this work, we assess a dual decoding scheme to achieve a strong\ncoupling between these two tasks and show how adequacy and consistency are\nincreased, with virtually no additional cost in terms of model size and\ntraining complexity.", "published": "2022-05-13 09:12:42", "link": "http://arxiv.org/abs/2205.06522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Text Classification using Supervision Signals from a\n  Language Model", "abstract": "Solving text classification in a weakly supervised manner is important for\nreal-world applications where human annotations are scarce. In this paper, we\npropose to query a masked language model with cloze style prompts to obtain\nsupervision signals. We design a prompt which combines the document itself and\n\"this article is talking about [MASK].\" A masked language model can generate\nwords for the [MASK] token. The generated words which summarize the content of\na document can be utilized as supervision signals. We propose a latent variable\nmodel to learn a word distribution learner which associates generated words to\npre-defined categories and a document classifier simultaneously without using\nany annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and\nUCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.", "published": "2022-05-13 12:57:15", "link": "http://arxiv.org/abs/2205.06604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Translation Formality Using Pre-trained Multilingual\n  Language Models", "abstract": "This paper describes the University of Maryland's submission to the Special\nTask on Formality Control for Spoken Language Translation at \\iwslt, which\nevaluates translation from English into 6 languages with diverse grammatical\nformality markers. We investigate to what extent this problem can be addressed\nwith a \\textit{single multilingual model}, simultaneously controlling its\noutput for target language and formality. Results show that this strategy can\napproach the translation quality and formality control achieved by dedicated\ntranslation models. However, the nature of the underlying pre-trained language\nmodel and of the finetuning samples greatly impact results.", "published": "2022-05-13 13:47:28", "link": "http://arxiv.org/abs/2205.06644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSCDiscovery: A shared task on semantic change discovery and detection\n  in Spanish", "abstract": "We present the first shared task on semantic change discovery and detection\nin Spanish and create the first dataset of Spanish words manually annotated for\nsemantic change using the DURel framework (Schlechtweg et al., 2018). The task\nis divided in two phases: 1) Graded Change Discovery, and 2) Binary Change\nDetection. In addition to introducing a new language the main novelty with\nrespect to the previous tasks consists in predicting and evaluating changes for\nall vocabulary words in the corpus. Six teams participated in phase 1 and seven\nteams in phase 2 of the shared task, and the best system obtained a Spearman\nrank correlation of 0.735 for phase 1 and an F1 score of 0.716 for phase 2. We\ndescribe the systems developed by the competing teams, highlighting the\ntechniques that were particularly useful and discuss the limits of these\napproaches.", "published": "2022-05-13 14:52:18", "link": "http://arxiv.org/abs/2205.06691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuCPAD: A Multi-Domain Chinese Predicate-Argument Dataset", "abstract": "During the past decade, neural network models have made tremendous progress\non in-domain semantic role labeling (SRL). However, performance drops\ndramatically under the out-of-domain setting. In order to facilitate research\non cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese\npredicate-argument dataset, which consists of 30,897 sentences and 92,051\npredicates from six different domains. MuCPAD exhibits three important\nfeatures. 1) Based on a frame-free annotation methodology, we avoid writing\ncomplex frames for new predicates. 2) We explicitly annotate omitted core\narguments to recover more complete semantic structure, considering that\nomission of content words is ubiquitous in multi-domain Chinese texts. 3) We\ncompile 53 pages of annotation guidelines and adopt strict double annotation\nfor improving data quality. This paper describes in detail the annotation\nmethodology and annotation process of MuCPAD, and presents in-depth data\nanalysis. We also give benchmark results on cross-domain SRL based on MuCPAD.", "published": "2022-05-13 15:17:24", "link": "http://arxiv.org/abs/2205.06703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained\n  Language Models", "abstract": "State-of-the-art pretrained language models tend to perform below their\ncapabilities when applied out-of-the-box on tasks that require understanding\nand working with numbers. Recent work suggests two main reasons for this: (1)\npopular tokenisation algorithms have limited expressiveness for numbers, and\n(2) common pretraining objectives do not target numeracy. Approaches that\naddress these shortcomings usually require architectural changes or pretraining\nfrom scratch. In this paper, we propose a new extended pretraining approach\ncalled Arithmetic-Based Pretraining that jointly addresses both in one extended\npretraining step without requiring architectural changes or pretraining from\nscratch. Arithmetic-Based Pretraining combines contrastive learning to improve\nthe number representation, and a novel extended pretraining objective called\nInferable Number Prediction Task to improve numeracy. Our experiments show the\neffectiveness of Arithmetic-Based Pretraining in three different tasks that\nrequire improved numeracy, i.e., reading comprehension in the DROP dataset,\ninference-on-tables in the InfoTabs dataset, and table-to-text generation in\nthe WikiBio and SciGen datasets.", "published": "2022-05-13 16:10:13", "link": "http://arxiv.org/abs/2205.06733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Are We Talking About? Handling Person Names in Speech Translation", "abstract": "Recent work has shown that systems for speech translation (ST) -- similarly\nto automatic speech recognition (ASR) -- poorly handle person names. This\nshortcoming does not only lead to errors that can seriously distort the meaning\nof the input, but also hinders the adoption of such systems in application\nscenarios (like computer-assisted interpreting) where the translation of named\nentities, like person names, is crucial. In this paper, we first analyse the\noutputs of ASR/ST systems to identify the reasons of failures in person name\ntranscription/translation. Besides the frequency in the training data, we\npinpoint the nationality of the referred person as a key factor. We then\nmitigate the problem by creating multilingual models, and further improve our\nST systems by forcing them to jointly generate transcripts and translations,\nprioritising the former over the latter. Overall, our solutions result in a\nrelative improvement in token-level person name accuracy by 47.8% on average\nfor three language pairs (en->es,fr,it).", "published": "2022-05-13 16:37:44", "link": "http://arxiv.org/abs/2205.06755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words\n  and Their Semantic Representations", "abstract": "What is the relation between a word and its description, or a word and its\nembedding? Both descriptions and embeddings are semantic representations of\nwords. But, what information from the original word remains in these\nrepresentations? Or more importantly, which information about a word do these\ntwo representations share? Definition Modeling and Reverse Dictionary are two\nopposite learning tasks that address these questions. The goal of the\nDefinition Modeling task is to investigate the power of information laying\ninside a word embedding to express the meaning of the word in a humanly\nunderstandable way -- as a dictionary definition. Conversely, the Reverse\nDictionary task explores the ability to predict word embeddings directly from\nits definition. In this paper, by tackling these two tasks, we are exploring\nthe relationship between words and their semantic representations. We present\nour findings based on the descriptive, exploratory, and predictive data\nanalysis conducted on the CODWOE dataset. We give a detailed overview of the\nsystems that we designed for Definition Modeling and Reverse Dictionary tasks,\nand that achieved top scores on SemEval-2022 CODWOE challenge in several\nsubtasks. We hope that our experimental results concerning the predictive\nmodels and the data analyses we provide will prove useful in future\nexplorations of word representations and their relationships.", "published": "2022-05-13 18:15:20", "link": "http://arxiv.org/abs/2205.06840v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Near-Negative Distinction: Giving a Second Life to Human Evaluation\n  Datasets", "abstract": "Precisely assessing the progress in natural language generation (NLG) tasks\nis challenging, and human evaluation to establish a preference in a model's\noutput over another is often necessary. However, human evaluation is usually\ncostly, difficult to reproduce, and non-reusable. In this paper, we propose a\nnew and simple automatic evaluation method for NLG called Near-Negative\nDistinction (NND) that repurposes prior human annotations into NND tests. In an\nNND test, an NLG model must place a higher likelihood on a high-quality output\ncandidate than on a near-negative candidate with a known error. Model\nperformance is established by the number of NND tests a model passes, as well\nas the distribution over task-specific errors the model fails on. Through\nexperiments on three NLG tasks (question generation, question answering, and\nsummarization), we show that NND achieves a higher correlation with human\njudgments than standard NLG evaluation metrics. We then illustrate NND\nevaluation in four practical scenarios, for example performing fine-grain model\nanalysis, or studying model training dynamics. Our findings suggest that NND\ncan give a second life to human annotations and provide low-cost NLG\nevaluation.", "published": "2022-05-13 20:02:53", "link": "http://arxiv.org/abs/2205.06871v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for\n  Pathology Domain", "abstract": "Pathology text mining is a challenging task given the reporting variability\nand constant new findings in cancer sub-type definitions. However, successful\ntext mining of a large pathology database can play a critical role to advance\n'big data' cancer research like similarity-based treatment selection, case\nidentification, prognostication, surveillance, clinical trial screening, risk\nstratification, and many others. While there is a growing interest in\ndeveloping language models for more specific clinical domains, no\npathology-specific language space exist to support the rapid data-mining\ndevelopment in pathology space. In literature, a few approaches fine-tuned\ngeneral transformer models on specialized corpora while maintaining the\noriginal tokenizer, but in fields requiring specialized terminology, these\nmodels often fail to perform adequately. We propose PathologyBERT - a\npre-trained masked language model which was trained on 347,173 histopathology\nspecimen reports and publicly released in the Huggingface repository. Our\ncomprehensive experiments demonstrate that pre-training of transformer model on\npathology corpora yields performance improvements on Natural Language\nUnderstanding (NLU) and Breast Cancer Diagnose Classification when compared to\nnonspecific language models.", "published": "2022-05-13 20:42:07", "link": "http://arxiv.org/abs/2205.06885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Text Anonymization Models with Distant Supervision", "abstract": "We propose a novel method to bootstrap text anonymization models based on\ndistant supervision. Instead of requiring manually labeled training data, the\napproach relies on a knowledge graph expressing the background information\nassumed to be publicly available about various individuals. This knowledge\ngraph is employed to automatically annotate text documents including personal\ndata about a subset of those individuals. More precisely, the method determines\nwhich text spans ought to be masked in order to guarantee $k$-anonymity,\nassuming an adversary with access to both the text documents and the background\ninformation expressed in the knowledge graph. The resulting collection of\nlabeled documents is then used as training data to fine-tune a pre-trained\nlanguage model for text anonymization. We illustrate this approach using a\nknowledge graph extracted from Wikidata and short biographical texts from\nWikipedia. Evaluation results with a RoBERTa-based model and a manually\nannotated collection of 553 summaries showcase the potential of the approach,\nbut also unveil a number of issues that may arise if the knowledge graph is\nnoisy or incomplete. The results also illustrate that, contrary to most\nsequence labeling problems, the text anonymization task may admit several\nalternative solutions.", "published": "2022-05-13 21:10:14", "link": "http://arxiv.org/abs/2205.06895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Property Induction Framework for Neural Language Models", "abstract": "To what extent can experience from language contribute to our conceptual\nknowledge? Computational explorations of this question have shed light on the\nability of powerful neural language models (LMs) -- informed solely through\ntext input -- to encode and elicit information about concepts and properties.\nTo extend this line of research, we present a framework that uses\nneural-network language models (LMs) to perform property induction -- a task in\nwhich humans generalize novel property knowledge (has sesamoid bones) from one\nor more concepts (robins) to others (sparrows, canaries). Patterns of property\ninduction observed in humans have shed considerable light on the nature and\norganization of human conceptual knowledge. Inspired by this insight, we use\nour framework to explore the property inductions of LMs, and find that they\nshow an inductive preference to generalize novel properties on the basis of\ncategory membership, suggesting the presence of a taxonomic bias in their\nrepresentations.", "published": "2022-05-13 22:05:49", "link": "http://arxiv.org/abs/2205.06910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AEON: A Method for Automatic Evaluation of NLP Test Cases", "abstract": "Due to the labor-intensive nature of manual test oracle construction, various\nautomated testing techniques have been proposed to enhance the reliability of\nNatural Language Processing (NLP) software. In theory, these techniques mutate\nan existing test case (e.g., a sentence with its label) and assume the\ngenerated one preserves an equivalent or similar semantic meaning and thus, the\nsame label. However, in practice, many of the generated test cases fail to\npreserve similar semantic meaning and are unnatural (e.g., grammar errors),\nwhich leads to a high false alarm rate and unnatural test cases. Our evaluation\nstudy finds that 44% of the test cases generated by the state-of-the-art (SOTA)\napproaches are false alarms. These test cases require extensive manual checking\neffort, and instead of improving NLP software, they can even degrade NLP\nsoftware when utilized in model training. To address this problem, we propose\nAEON for Automatic Evaluation Of NLP test cases. For each generated test case,\nit outputs scores based on semantic similarity and language naturalness. We\nemploy AEON to evaluate test cases generated by four popular testing techniques\non five datasets across three typical NLP tasks. The results show that AEON\naligns the best with human judgment. In particular, AEON achieves the best\naverage precision in detecting semantic inconsistent test cases, outperforming\nthe best baseline metric by 10%. In addition, AEON also has the highest average\nprecision of finding unnatural test cases, surpassing the baselines by more\nthan 15%. Moreover, model training with test cases prioritized by AEON leads to\nmodels that are more accurate and robust, demonstrating AEON's potential in\nimproving NLP software.", "published": "2022-05-13 03:47:13", "link": "http://arxiv.org/abs/2205.06439v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language\n  Generation", "abstract": "We present ViT5, a pretrained Transformer-based encoder-decoder model for the\nVietnamese language. With T5-style self-supervised pretraining, ViT5 is trained\non a large corpus of high-quality and diverse Vietnamese texts. We benchmark\nViT5 on two downstream text generation tasks, Abstractive Text Summarization\nand Named Entity Recognition. Although Abstractive Text Summarization has been\nwidely studied for the English language thanks to its rich and large source of\ndata, there has been minimal research into the same task in Vietnamese, a much\nlower resource language. In this work, we perform exhaustive experiments on\nboth Vietnamese Abstractive Summarization and Named Entity Recognition,\nvalidating the performance of ViT5 against many other pretrained\nTransformer-based encoder-decoder models. Our experiments show that ViT5\nsignificantly outperforms existing models and achieves state-of-the-art results\non Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5\nis competitive against previous best results from pretrained encoder-based\nTransformer models. Further analysis shows the importance of context length\nduring the self-supervised pretraining on downstream performance across\ndifferent settings.", "published": "2022-05-13 06:08:35", "link": "http://arxiv.org/abs/2205.06457v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Question Answering Datasets and Their Generalizability:\n  Are They Enough for Future Research?", "abstract": "Existing approaches on Question Answering over Knowledge Graphs (KGQA) have\nweak generalizability. That is often due to the standard i.i.d. assumption on\nthe underlying dataset. Recently, three levels of generalization for KGQA were\ndefined, namely i.i.d., compositional, zero-shot. We analyze 25 well-known KGQA\ndatasets for 5 different Knowledge Graphs (KGs). We show that according to this\ndefinition many existing and online available KGQA datasets are either not\nsuited to train a generalizable KGQA system or that the datasets are based on\ndiscontinued and out-dated KGs. Generating new datasets is a costly process\nand, thus, is not an alternative to smaller research groups and companies. In\nthis work, we propose a mitigation method for re-splitting available KGQA\ndatasets to enable their applicability to evaluate generalization, without any\ncost and manual effort. We test our hypothesis on three KGQA datasets, i.e.,\nLC-QuAD, LC-QuAD 2.0 and QALD-9). Experiments on re-splitted KGQA datasets\ndemonstrate its effectiveness towards generalizability. The code and a unified\nway to access 18 available datasets is online at\nhttps://github.com/semantic-systems/KGQA-datasets as well as\nhttps://github.com/semantic-systems/KGQA-datasets-generalization.", "published": "2022-05-13 12:01:15", "link": "http://arxiv.org/abs/2205.06573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Deployable OCR models for Indic languages", "abstract": "Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/", "published": "2022-05-13 16:19:21", "link": "http://arxiv.org/abs/2205.06740v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and\n  Their Implications", "abstract": "There are many ways to express similar things in text, which makes evaluating\nnatural language generation (NLG) systems difficult. Compounding this\ndifficulty is the need to assess varying quality criteria depending on the\ndeployment setting. While the landscape of NLG evaluation has been well-mapped,\npractitioners' goals, assumptions, and constraints -- which inform decisions\nabout what, when, and how to evaluate -- are often partially or implicitly\nstated, or not stated at all. Combining a formative semi-structured interview\nstudy of NLG practitioners (N=18) with a survey study of a broader sample of\npractitioners (N=61), we surface goals, community practices, assumptions, and\nconstraints that shape NLG evaluations, examining their implications and how\nthey embody ethical considerations.", "published": "2022-05-13 18:00:11", "link": "http://arxiv.org/abs/2205.06828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Approach for Automatic Construction of an Algorithmic Knowledge Graph\n  from Textual Resources", "abstract": "There is enormous growth in various fields of research. This development is\naccompanied by new problems. To solve these problems efficiently and in an\noptimized manner, algorithms are created and described by researchers in the\nscientific literature. Scientific algorithms are vital for understanding and\nreusing existing work in numerous domains. However, algorithms are generally\nchallenging to find. Also, the comparison among similar algorithms is difficult\nbecause of the disconnected documentation. Information about algorithms is\nmostly present in websites, code comments, and so on. There is an absence of\nstructured metadata to portray algorithms. As a result, sometimes redundant or\nsimilar algorithms are published, and the researchers build them from scratch\ninstead of reusing or expanding upon the already existing algorithm. In this\npaper, we introduce an approach for automatically developing a knowledge graph\n(KG) for algorithmic problems from unstructured data. Because it captures\ninformation more clearly and extensively, an algorithm KG will give additional\ncontext and explainability to the algorithm metadata.", "published": "2022-05-13 18:59:23", "link": "http://arxiv.org/abs/2205.06854v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Developing a Production System for Purpose of Call Detection in Business\n  Phone Conversations", "abstract": "For agents at a contact centre receiving calls, the most important piece of\ninformation is the reason for a given call. An agent cannot provide support on\na call if they do not know why a customer is calling. In this paper we describe\nour implementation of a commercial system to detect Purpose of Call statements\nin English business call transcripts in real time. We present a detailed\nanalysis of types of Purpose of Call statements and language patterns related\nto them, discuss an approach to collect rich training data by bootstrapping\nfrom a set of rules to a neural model, and describe a hybrid model which\nconsists of a transformer-based classifier and a set of rules by leveraging\ninsights from the analysis of call transcripts. The model achieved 88.6 F1 on\naverage in various types of business calls when tested on real life data and\nhas low inference time. We reflect on the challenges and design decisions when\ndeveloping and deploying the system.", "published": "2022-05-13 21:45:54", "link": "http://arxiv.org/abs/2205.06904v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Design and Implementation of a Quantum Kernel for Natural Language\n  Processing", "abstract": "Natural language processing (NLP) is the field that attempts to make human\nlanguage accessible to computers, and it relies on applying a mathematical\nmodel to express the meaning of symbolic language. One such model, DisCoCat,\ndefines how to express both the meaning of individual words as well as their\ncompositional nature. This model can be naturally implemented on quantum\ncomputers, leading to the field quantum NLP (QNLP). Recent experimental work\nused quantum machine learning techniques to map from text to class label using\nthe expectation value of the quantum encoded sentence. Theoretical work has\nbeen done on computing the similarity of sentences but relies on an unrealized\nquantum memory store. The main goal of this thesis is to leverage the DisCoCat\nmodel to design a quantum-based kernel function that can be used by a support\nvector machine (SVM) for NLP tasks. Two similarity measures were studied: (i)\nthe transition amplitude approach and (ii) the SWAP test. A simple NLP meaning\nclassification task from previous work was used to train the word embeddings\nand evaluate the performance of both models. The Python module lambeq and its\nrelated software stack was used for implementation. The explicit model from\nprevious work was used to train word embeddings and achieved a testing accuracy\nof $93.09 \\pm 0.01$%. It was shown that both the SVM variants achieved a higher\ntesting accuracy of $95.72 \\pm 0.01$% for approach (i) and $97.14 \\pm 0.01$%\nfor (ii). The SWAP test was then simulated under a noise model defined by the\nreal quantum device, ibmq_guadalupe. The explicit model achieved an accuracy of\n$91.94 \\pm 0.01$% while the SWAP test SVM achieved 96.7% on the testing\ndataset, suggesting that the kernelized classifiers are resilient to noise.\nThese are encouraging results and motivate further investigations of our\nproposed kernelized QNLP paradigm.", "published": "2022-05-13 00:45:46", "link": "http://arxiv.org/abs/2205.06409v1", "categories": ["cs.CL", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Productivity Assessment of Neural Code Completion", "abstract": "Neural code synthesis has reached a point where snippet generation is\naccurate enough to be considered for integration into human software\ndevelopment workflows. Commercial products aim to increase programmers'\nproductivity, without being able to measure it directly. In this case study, we\nasked users of GitHub Copilot about its impact on their productivity, and\nsought to find a reflection of their perception in directly measurable user\ndata. We find that the rate with which shown suggestions are accepted, rather\nthan more specific metrics regarding the persistence of completions in the code\nover time, drives developers' perception of productivity.", "published": "2022-05-13 09:53:25", "link": "http://arxiv.org/abs/2205.06537v1", "categories": ["cs.SE", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Improving Contextual Representation with Gloss Regularized Pre-training", "abstract": "Though achieving impressive results on many NLP tasks, the BERT-like masked\nlanguage models (MLM) encounter the discrepancy between pre-training and\ninference. In light of this gap, we investigate the contextual representation\nof pre-training and inference from the perspective of word probability\ndistribution. We discover that BERT risks neglecting the contextual word\nsimilarity in pre-training. To tackle this issue, we propose an auxiliary gloss\nregularizer module to BERT pre-training (GR-BERT), to enhance word semantic\nsimilarity. By predicting masked words and aligning contextual embeddings to\ncorresponding glosses simultaneously, the word similarity can be explicitly\nmodeled. We design two architectures for GR-BERT and evaluate our model in\ndownstream tasks. Experimental results show that the gloss regularizer benefits\nBERT in word-level and sentence-level semantic representation. The GR-BERT\nachieves new state-of-the-art in lexical substitution task and greatly promotes\nBERT sentence representation in both unsupervised and supervised STS tasks.", "published": "2022-05-13 12:50:32", "link": "http://arxiv.org/abs/2205.06603v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Devil is in the Details: On the Pitfalls of Vocabulary Selection in\n  Neural Machine Translation", "abstract": "Vocabulary selection, or lexical shortlisting, is a well-known technique to\nimprove latency of Neural Machine Translation models by constraining the set of\nallowed output words during inference. The chosen set is typically determined\nby separately trained alignment model parameters, independent of the\nsource-sentence context at inference time. While vocabulary selection appears\ncompetitive with respect to automatic quality metrics in prior work, we show\nthat it can fail to select the right set of output words, particularly for\nsemantically non-compositional linguistic phenomena such as idiomatic\nexpressions, leading to reduced translation quality as perceived by humans.\nTrading off latency for quality by increasing the size of the allowed set is\noften not an option in real-world scenarios. We propose a model of vocabulary\nselection, integrated into the neural translation model, that predicts the set\nof allowed output words from contextualized encoder representations. This\nrestores translation quality of an unconstrained system, as measured by human\nevaluations on WMT newstest2020 and idiomatic expressions, at an inference\nlatency competitive with alignment-based selection using aggressive thresholds,\nthereby removing the dependency on separately trained alignment models.", "published": "2022-05-13 13:13:03", "link": "http://arxiv.org/abs/2205.06618v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes", "abstract": "To tackle the rising phenomenon of hate speech, efforts have been made\ntowards data curation and analysis. When it comes to analysis of bias, previous\nwork has focused predominantly on race. In our work, we further investigate\nbias in hate speech datasets along racial, gender and intersectional axes. We\nidentify strong bias against African American English (AAE), masculine and\nAAE+Masculine tweets, which are annotated as disproportionately more hateful\nand offensive than from other demographics. We provide evidence that BERT-based\nmodels propagate this bias and show that balancing the training data for these\nprotected attributes can lead to fairer models with regards to gender, but not\nrace.", "published": "2022-05-13 13:13:46", "link": "http://arxiv.org/abs/2205.06621v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Modeling of Multi-Domain Multi-Device ASR Systems", "abstract": "Modern Automatic Speech Recognition (ASR) systems often use a portfolio of\ndomain-specific models in order to get high accuracy for distinct user\nutterance types across different devices. In this paper, we propose an\ninnovative approach that integrates the different per-domain per-device models\ninto a unified model, using a combination of domain embedding, domain experts,\nmixture of experts and adversarial training. We run careful ablation studies to\nshow the benefit of each of these innovations in contributing to the accuracy\nof the overall unified model. Experiments show that our proposed unified\nmodeling approach actually outperforms the carefully tuned per-domain models,\ngiving relative gains of up to 10% over a baseline model with negligible\nincrease in the number of parameters.", "published": "2022-05-13 14:07:22", "link": "http://arxiv.org/abs/2205.06655v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Case for a Legal Compliance API for the Enforcement of the EU's\n  Digital Services Act on Social Media Platforms", "abstract": "In the course of under a year, the European Commission has launched some of\nthe most important regulatory proposals to date on platform governance. The\nCommission's goals behind cross-sectoral regulation of this sort include the\nprotection of markets and democracies alike. While all these acts propose\nsophisticated rules for setting up new enforcement institutions and procedures,\none aspect remains highly unclear: how digital enforcement will actually take\nplace in practice. Focusing on the Digital Services Act (DSA), this discussion\npaper critically addresses issues around social media data access for the\npurpose of digital enforcement and proposes the use of a legal compliance\napplication programming interface (API) as a means to facilitate compliance\nwith the DSA and complementary European and national regulation. To\ncontextualize this discussion, the paper pursues two scenarios that exemplify\nthe harms arising out of content monetization affecting a particularly\nvulnerable category of social media users: children. The two scenarios are used\nto further reflect upon essential issues surrounding data access and legal\ncompliance with the DSA and further applicable legal standards in the field of\nlabour and consumer law.", "published": "2022-05-13 14:16:23", "link": "http://arxiv.org/abs/2205.06666v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Interlock-Free Multi-Aspect Rationalization for Text Classification", "abstract": "Explanation is important for text classification tasks. One prevalent type of\nexplanation is rationales, which are text snippets of input text that suffice\nto yield the prediction and are meaningful to humans. A lot of research on\nrationalization has been based on the selective rationalization framework,\nwhich has recently been shown to be problematic due to the interlocking\ndynamics. In this paper, we show that we address the interlocking problem in\nthe multi-aspect setting, where we aim to generate multiple rationales for\nmultiple outputs. More specifically, we propose a multi-stage training method\nincorporating an additional self-supervised contrastive loss that helps to\ngenerate more semantically diverse rationales. Empirical results on the beer\nreview dataset show that our method improves significantly the rationalization\nperformance.", "published": "2022-05-13 16:38:38", "link": "http://arxiv.org/abs/2205.06756v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Covid-related Reddits", "abstract": "This paper focuses on Sentiment Analysis of Covid-19 related messages from\nthe r/Canada and r/Unitedkingdom subreddits of Reddit. We apply manual\nannotation and three Machine Learning algorithms to analyze sentiments conveyed\nin those messages. We use VADER and TextBlob to label messages for Machine\nLearning experiments. Our results show that removal of shortest and longest\nmessages improves VADER and TextBlob agreement on positive sentiments and\nF-score of sentiment classification by all the three algorithms", "published": "2022-05-13 19:39:41", "link": "http://arxiv.org/abs/2205.06863v1", "categories": ["cs.CL", "cs.LG", "cs.SI", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Joint Acoustic Echo Cancellation and Blind Source Extraction based on\n  Independent Vector Extraction", "abstract": "We describe a joint acoustic echo cancellation (AEC) and blind source\nextraction (BSE) approach for multi-microphone acoustic frontends. The proposed\nalgorithm blindly estimates AEC and beamforming filters by maximizing the\nstatistical independence of a non-Gaussian source of interest and a stationary\nGaussian background modeling interfering signals and residual echo. Double\ntalk-robust and fast-converging parameter updates are derived from a global\nmaximum-likelihood objective function resulting in a computationally efficient\nNewton-type update rule. Evaluation with simulated acoustic data confirms the\nbenefit of the proposed joint AEC and beamforming filter estimation in\ncomparison to updating both filters individually.", "published": "2022-05-13 07:03:43", "link": "http://arxiv.org/abs/2205.06473v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Personalized Adversarial Data Augmentation for Dysarthric and Elderly\n  Speech Recognition", "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech, accurate recognition of dysarthric and elderly speech\nremains highly challenging tasks to date. It is difficult to collect large\nquantities of such data for ASR system development due to the mobility issues\noften found among these users. To this end, data augmentation techniques play a\nvital role. In contrast to existing data augmentation techniques only modifying\nthe speaking rate or overall shape of spectral contour, fine-grained\nspectro-temporal differences between dysarthric, elderly and normal speech are\nmodelled using a novel set of speaker dependent (SD) generative adversarial\nnetworks (GAN) based data augmentation approaches in this paper. These flexibly\nallow both: a) temporal or speed perturbed normal speech spectra to be modified\nand closer to those of an impaired speaker when parallel speech data is\navailable; and b) for non-parallel data, the SVD decomposed normal speech\nspectral basis features to be transformed into those of a target elderly\nspeaker before being re-composed with the temporal bases to produce the\naugmented data for state-of-the-art TDNN and Conformer ASR system training.\nExperiments are conducted on four tasks: the English UASpeech and TORGO\ndysarthric speech corpora; the English DementiaBank Pitt and Cantonese JCCOCC\nMoCA elderly speech datasets. The proposed GAN based data augmentation\napproaches consistently outperform the baseline speed perturbation method by up\nto 0.91% and 3.0% absolute (9.61% and 6.4% relative) WER reduction on the TORGO\nand DementiaBank data respectively. Consistent performance improvements are\nretained after applying LHUC based speaker adaptation.", "published": "2022-05-13 04:29:49", "link": "http://arxiv.org/abs/2205.06445v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "High-Frequency Tunable Resistorless Memcapacitor Emulator and\n  Application", "abstract": "In this paper, a new design has been proposed for the realization of\nhigh-frequency memcapacitor emulators built with three OTAs. This paper also\nproposes the application of memcapacitor as an amplitude modulator.\nFurthermore, applications of memcapacitor as a filter, Oscillator point\nattractor, and periodic doubler are also shown. The proposed circuits can be\nconfigured in both incremental and decremental topology. The proposed circuits\nand their application claim that the circuit is much simpler in design and can\nbe utilized in both topologies. The performance of all the proposed circuits\nhas been verified on Cadence Virtuoso Spectre using standard CMOS 180nm.\nFurthermore, post-layout simulations and their comparison have been carried\nout.", "published": "2022-05-13 17:56:56", "link": "http://arxiv.org/abs/2205.06808v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Task splitting for DNN-based acoustic echo and noise removal", "abstract": "Neural networks have led to tremendous performance gains for single-task\nspeech enhancement, such as noise suppression and acoustic echo cancellation\n(AEC). In this work, we evaluate whether it is more useful to use a single\njoint or separate modules to tackle these problems. We describe different\npossible implementations and give insights into their performance and\nefficiency. We show that using a separate echo cancellation module and a module\nfor noise and residual echo removal results in less near-end speech distortion\nand better performance during double-talk at same complexity.", "published": "2022-05-13 23:56:47", "link": "http://arxiv.org/abs/2205.06931v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The ACM Multimedia 2022 Computational Paralinguistics Challenge:\n  Vocalisations, Stuttering, Activity, & Mosquitoes", "abstract": "The ACM Multimedia 2022 Computational Paralinguistics Challenge addresses\nfour different problems for the first time in a research competition under\nwell-defined conditions: In the Vocalisations and Stuttering Sub-Challenges, a\nclassification on human non-verbal vocalisations and speech has to be made; the\nActivity Sub-Challenge aims at beyond-audio human activity recognition from\nsmartwatch sensor data; and in the Mosquitoes Sub-Challenge, mosquitoes need to\nbe detected. We describe the Sub-Challenges, baseline feature extraction, and\nclassifiers based on the usual ComPaRE and BoAW features, the auDeep toolkit,\nand deep feature extraction from pre-trained CNNs using the DeepSpectRum\ntoolkit; in addition, we add end-to-end sequential modelling, and a\nlog-mel-128-BNN.", "published": "2022-05-13 17:51:45", "link": "http://arxiv.org/abs/2205.06799v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68", "I.2.7; I.5.0; J.3"], "primary_category": "cs.SD"}
