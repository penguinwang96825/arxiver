{"title": "Survey of Computational Approaches to Lexical Semantic Change", "abstract": "Our languages are in constant flux driven by external factors such as\ncultural, societal and technological changes, as well as by only partially\nunderstood internal motivations. Words acquire new meanings and lose old\nsenses, new words are coined or borrowed from other languages and obsolete\nwords slide into obscurity. Understanding the characteristics of shifts in the\nmeaning and in the use of words is useful for those who work with the content\nof historical texts, the interested general public, but also in and of itself.\nThe findings from automatic lexical semantic change detection, and the models\nof diachronic conceptual change are currently being incorporated in approaches\nfor measuring document across-time similarity, information retrieval from\nlong-term document archives, the design of OCR algorithms, and so on. In recent\nyears we have seen a surge in interest in the academic community in\ncomputational methods and tools supporting inquiry into diachronic conceptual\nchange and lexical replacement. This article is an extract of a survey of\nrecent computational techniques to tackle lexical semantic change currently\nunder review. In this article we focus on diachronic conceptual change as an\nextension of semantic change.", "published": "2018-11-15 10:26:16", "link": "http://arxiv.org/abs/1811.06278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Learning for Answering Structured Queries Directly over Text", "abstract": "Structured queries expressed in languages (such as SQL, SPARQL, or XQuery)\noffer a convenient and explicit way for users to express their information\nneeds for a number of tasks. In this work, we present an approach to answer\nthese directly over text data without storing results in a database. We\nspecifically look at the case of knowledge bases where queries are over\nentities and the relations between them. Our approach combines distributed\nquery answering (e.g. Triple Pattern Fragments) with models built for\nextractive question answering. Importantly, by applying distributed querying\nanswering we are able to simplify the model learning problem. We train models\nfor a large portion (572) of the relations within Wikidata and achieve an\naverage 0.70 F1 measure across all models. We also present a systematic method\nto construct the necessary training data for this task from knowledge graphs\nand describe a prototype implementation.", "published": "2018-11-15 11:46:14", "link": "http://arxiv.org/abs/1811.06303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming End-to-end Speech Recognition For Mobile Devices", "abstract": "End-to-end (E2E) models, which directly predict output character sequences\ngiven input speech, are good candidates for on-device speech recognition. E2E\nmodels, however, present numerous challenges: In order to be truly useful, such\nmodels must decode speech utterances in a streaming fashion, in real time; they\nmust be robust to the long tail of use cases; they must be able to leverage\nuser-specific context (e.g., contact lists); and above all, they must be\nextremely accurate. In this work, we describe our efforts at building an E2E\nspeech recognizer using a recurrent neural network transducer. In experimental\nevaluations, we find that the proposed approach can outperform a conventional\nCTC-based model in terms of both latency and accuracy in a number of evaluation\ncategories.", "published": "2018-11-15 23:09:44", "link": "http://arxiv.org/abs/1811.06621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implementing a Portable Clinical NLP System with a Common Data Model - a\n  Lisp Perspective", "abstract": "This paper presents a Lisp architecture for a portable NLP system, termed\nLAPNLP, for processing clinical notes. LAPNLP integrates multiple standard,\ncustomized and in-house developed NLP tools. Our system facilitates portability\nacross different institutions and data systems by incorporating an enriched\nCommon Data Model (CDM) to standardize necessary data elements. It utilizes\nUMLS to perform domain adaptation when integrating generic domain NLP tools. It\nalso features stand-off annotations that are specified by positional reference\nto the original document. We built an interval tree based search engine to\nefficiently query and retrieve the stand-off annotations by specifying\npositional requirements. We also developed a utility to convert an inline\nannotation format to stand-off annotations to enable the reuse of clinical text\ndatasets with inline annotations. We experimented with our system on several\nNLP facilitated tasks including computational phenotyping for lymphoma patients\nand semantic relation extraction for clinical notes. These experiments\nshowcased the broader applicability and utility of LAPNLP.", "published": "2018-11-15 04:58:21", "link": "http://arxiv.org/abs/1811.06179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing Design Patterns of EHR-Driven Phenotype Extraction\n  Algorithms", "abstract": "The automatic development of phenotype algorithms from Electronic Health\nRecord data with machine learning (ML) techniques is of great interest given\nthe current practice is very time-consuming and resource intensive. The\nextraction of design patterns from phenotype algorithms is essential to\nunderstand their rationale and standard, with great potential to automate the\ndevelopment process. In this pilot study, we perform network visualization on\nthe design patterns and their associations with phenotypes and sites. We\nclassify design patterns using the fragments from previously annotated\nphenotype algorithms as the ground truth. The classification performance is\nused as a proxy for coherence at the attribution level. The bag-of-words\nrepresentation with knowledge-based features generated a good performance in\nthe classification task (0.79 macro-f1 scores). Good classification accuracy\nwith simple features demonstrated the attribution coherence and the feasibility\nof automatic identification of design patterns. Our results point to both the\nfeasibility and challenges of automatic identification of phenotyping design\npatterns, which would power the automatic development of phenotype algorithms.", "published": "2018-11-15 05:12:33", "link": "http://arxiv.org/abs/1811.06183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Axiom Injection and Knowledge Base Completion for Efficient\n  Natural Language Inference", "abstract": "In logic-based approaches to reasoning tasks such as Recognizing Textual\nEntailment (RTE), it is important for a system to have a large amount of\nknowledge data. However, there is a tradeoff between adding more knowledge data\nfor improved RTE performance and maintaining an efficient RTE system, as such a\nbig database is problematic in terms of the memory usage and computational\ncomplexity. In this work, we show the processing time of a state-of-the-art\nlogic-based RTE system can be significantly reduced by replacing its\nsearch-based axiom injection (abduction) mechanism by that based on Knowledge\nBase Completion (KBC). We integrate this mechanism in a Coq plugin that\nprovides a proof automation tactic for natural language inference.\nAdditionally, we show empirically that adding new knowledge data contributes to\nbetter RTE performance while not harming the processing speed in this\nframework.", "published": "2018-11-15 06:40:39", "link": "http://arxiv.org/abs/1811.06203v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effect of data reduction on sequence-to-sequence neural TTS", "abstract": "Recent speech synthesis systems based on sampling from autoregressive neural\nnetworks models can generate speech almost undistinguishable from human\nrecordings. However, these models require large amounts of data. This paper\nshows that the lack of data from one speaker can be compensated with data from\nother speakers. The naturalness of Tacotron2-like models trained on a blend of\n5k utterances from 7 speakers is better than that of speaker dependent models\ntrained on 15k utterances, but in terms of stability multi-speaker models are\nalways more stable. We also demonstrate that models mixing only 1250 utterances\nfrom a target speaker with 5k utterances from another 6 speakers can produce\nsignificantly better quality than state-of-the-art DNN-guided unit selection\nsystems trained on more than 10 times the data from the target speaker.", "published": "2018-11-15 12:31:12", "link": "http://arxiv.org/abs/1811.06315v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Text Document Summarization using Semantic-based Analysis", "abstract": "Since the advent of the web, the amount of data on wen has been increased\nseveral million folds. In recent years web data generated is more than data\nstored for years. One important data format is text. To answer user queries\nover the internet, and to overcome the problem of information overload one\npossible solution is text document summarization. This not only reduces query\naccess time, but also optimize the document results according to specific users\nrequirements. Summarization of text document can be categorized as abstractive\nand extractive. Most of the work has been done in the direction of Extractive\nsummarization. Extractive summarized result is a subset of original documents\nwith the objective of more content coverage and lea redundancy. Our work is\nbased on Extractive approaches. In the first approach, we are using some\nstatistical features and semantic-based features. To include sentiment as a\nfeature is an idea cached from a view that emotion plays an important role. It\neffectively conveys a message. So, it may play a vital role in text document\nsummarization.", "published": "2018-11-15 19:29:26", "link": "http://arxiv.org/abs/1811.06567v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Generality and Knowledge Transferability in Cross-Domain Duplicate\n  Question Detection for Heterogeneous Community Question Answering", "abstract": "Duplicate question detection is an ongoing challenge in community question\nanswering because semantically equivalent questions can have significantly\ndifferent words and structures. In addition, the identification of duplicate\nquestions can reduce the resources required for retrieval, when the same\nquestions are not repeated. This study compares the performance of deep neural\nnetworks and gradient tree boosting, and explores the possibility of domain\nadaptation with transfer learning to improve the under-performing target\ndomains for the text-pair duplicates classification task, using three\nheterogeneous datasets: general-purpose Quora, technical Ask Ubuntu, and\nacademic English Stack Exchange. Ultimately, our study exposes the alternative\nhypothesis that the meaning of a \"duplicate\" is not inherently general-purpose,\nbut rather is dependent on the domain of learning, hence reducing the chance of\ntransfer learning through adapting to the domain.", "published": "2018-11-15 21:29:26", "link": "http://arxiv.org/abs/1811.06596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nudging Neural Conversational Model with Domain Knowledge", "abstract": "Neural conversation models are attractive because one can train a model\ndirectly on dialog examples with minimal labeling. With a small amount of data,\nhowever, they often fail to generalize over test data since they tend to\ncapture spurious features instead of semantically meaningful domain knowledge.\nTo address this issue, we propose a novel approach that allows any human\nteachers to transfer their domain knowledge to the conversation model in the\nform of natural language rules. We tested our method with three different\ndialog datasets. The improved performance across all domains demonstrates the\nefficacy of our proposed method.", "published": "2018-11-15 23:47:39", "link": "http://arxiv.org/abs/1811.06630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Sentence Embedding for Medical Question Answering", "abstract": "Despite the great success of word embedding, sentence embedding remains a\nnot-well-solved problem. In this paper, we present a supervised learning\nframework to exploit sentence embedding for the medical question answering\ntask. The learning framework consists of two main parts: 1) a sentence\nembedding producing module, and 2) a scoring module. The former is developed\nwith contextual self-attention and multi-scale techniques to encode a sentence\ninto an embedding tensor. This module is shortly called Contextual\nself-Attention Multi-scale Sentence Embedding (CAMSE). The latter employs two\nscoring strategies: Semantic Matching Scoring (SMS) and Semantic Association\nScoring (SAS). SMS measures similarity while SAS captures association between\nsentence pairs: a medical question concatenated with a candidate choice, and a\npiece of corresponding supportive evidence. The proposed framework is examined\nby two Medical Question Answering(MedicalQA) datasets which are collected from\nreal-world applications: medical exam and clinical diagnosis based on\nelectronic medical records (EMR). The comparison results show that our proposed\nframework achieved significant improvements compared to competitive baseline\napproaches. Additionally, a series of controlled experiments are also conducted\nto illustrate that the multi-scale strategy and the contextual self-attention\nlayer play important roles for producing effective sentence embedding, and the\ntwo kinds of scoring strategies are highly complementary to each other for\nquestion answering problems.", "published": "2018-11-15 03:38:20", "link": "http://arxiv.org/abs/1811.06156v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HCU400: An Annotated Dataset for Exploring Aural Phenomenology Through\n  Causal Uncertainty", "abstract": "The way we perceive a sound depends on many aspects-- its ecological\nfrequency, acoustic features, typicality, and most notably, its identified\nsource. In this paper, we present the HCU400: a dataset of 402 sounds ranging\nfrom easily identifiable everyday sounds to intentionally obscured artificial\nones. It aims to lower the barrier for the study of aural phenomenology as the\nlargest available audio dataset to include an analysis of causal attribution.\nEach sample has been annotated with crowd-sourced descriptions, as well as\nfamiliarity, imageability, arousal, and valence ratings. We extend existing\ncalculations of causal uncertainty, automating and generalizing them with word\nembeddings. Upon analysis we find that individuals will provide less polarized\nemotion ratings as a sound's source becomes increasingly ambiguous; individual\nratings of familiarity and imageability, on the other hand, diverge as\nuncertainty increases despite a clear negative trend on average.", "published": "2018-11-15 15:50:29", "link": "http://arxiv.org/abs/1811.06439v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-cell LSTM Based Neural Language Model", "abstract": "Language models, being at the heart of many NLP problems, are always of great\ninterest to researchers. Neural language models come with the advantage of\ndistributed representations and long range contexts. With its particular\ndynamics that allow the cycling of information within the network, `Recurrent\nneural network' (RNN) becomes an ideal paradigm for neural language modeling.\nLong Short-Term Memory (LSTM) architecture solves the inadequacies of the\nstandard RNN in modeling long-range contexts. In spite of a plethora of RNN\nvariants, possibility to add multiple memory cells in LSTM nodes was seldom\nexplored. Here we propose a multi-cell node architecture for LSTMs and study\nits applicability for neural language modeling. The proposed multi-cell LSTM\nlanguage models outperform the state-of-the-art results on well-known Penn\nTreebank (PTB) setup.", "published": "2018-11-15 17:09:53", "link": "http://arxiv.org/abs/1811.06477v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Generating Responses Expressing Emotion in an Open-domain Dialogue\n  System", "abstract": "Neural network-based Open-ended conversational agents automatically generate\nresponses based on predictive models learned from a large number of pairs of\nutterances. The generated responses are typically acceptable as a sentence but\nare often dull, generic, and certainly devoid of any emotion. In this paper, we\npresent neural models that learn to express a given emotion in the generated\nresponse. We propose four models and evaluate them against 3 baselines. An\nencoder-decoder framework-based model with multiple attention layers provides\nthe best overall performance in terms of expressing the required emotion. While\nit does not outperform other models on all emotions, it presents promising\nresults in most cases.", "published": "2018-11-15 22:59:25", "link": "http://arxiv.org/abs/1811.10990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards achieving robust universal neural vocoding", "abstract": "This paper explores the potential universality of neural vocoders. We train a\nWaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is\nshown to be capable of generating speech of consistently good quality (98%\nrelative mean MUSHRA when compared to natural speech) regardless of whether the\ninput spectrogram comes from a speaker or style seen during training or from an\nout-of-domain scenario when the recording conditions are studio-quality. When\nthe recordings show significant changes in quality, or when moving towards\nnon-speech vocalizations or singing, the vocoder still significantly\noutperforms speaker-dependent vocoders, but operates at a lower average\nrelative MUSHRA of 75%. These results are shown to be consistent across\nlanguages, regardless of them being seen during training (e.g. English or\nJapanese) or unseen (e.g. Wolof, Swahili, Ahmaric).", "published": "2018-11-15 10:54:13", "link": "http://arxiv.org/abs/1811.06292v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comprehensive evaluation of statistical speech waveform synthesis", "abstract": "Statistical TTS systems that directly predict the speech waveform have\nrecently reported improvements in synthesis quality. This investigation\nevaluates Amazon's statistical speech waveform synthesis (SSWS) system. An\nin-depth evaluation of SSWS is conducted across a number of domains to better\nunderstand the consistency in quality. The results of this evaluation are\nvalidated by repeating the procedure on a separate group of testers. Finally,\nan analysis of the nature of speech errors of SSWS compared to hybrid unit\nselection synthesis is conducted to identify the strengths and weaknesses of\nSSWS. Having a deeper insight into SSWS allows us to better define the focus of\nfuture work to improve this new technology.", "published": "2018-11-15 11:00:39", "link": "http://arxiv.org/abs/1811.06296v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-based identification of beehive states", "abstract": "The absence of the queen in a beehive is a very strong indicator of the need\nfor beekeeper intervention. Manually searching for the queen is an arduous\nrecurrent task for beekeepers that disrupts the normal life cycle of the\nbeehive and can be a source of stress for bees. Sound is an indicator for\nsignalling different states of the beehive, including the absence of the queen\nbee. In this work, we apply machine learning methods to automatically recognise\ndifferent states in a beehive using audio as input. % The system is built on\ntop of a method for beehive sound recognition in order to detect bee sounds\nfrom other external sounds. We investigate both support vector machines and\nconvolutional neural networks for beehive state recognition, using audio data\nof beehives collected from the NU-Hive project. Results indicate the potential\nof machine learning methods as well as the challenges of generalizing the\nsystem to new hives.", "published": "2018-11-15 13:13:32", "link": "http://arxiv.org/abs/1811.06330v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Training Targets and Objective Functions for Deep-Learning-Based\n  Audio-Visual Speech Enhancement", "abstract": "Audio-visual speech enhancement (AV-SE) is the task of improving speech\nquality and intelligibility in a noisy environment using audio and visual\ninformation from a talker. Recently, deep learning techniques have been adopted\nto solve the AV-SE task in a supervised manner. In this context, the choice of\nthe target, i.e. the quantity to be estimated, and the objective function,\nwhich quantifies the quality of this estimate, to be used for training is\ncritical for the performance. This work is the first that presents an\nexperimental study of a range of different targets and objective functions used\nto train a deep-learning-based AV-SE system. The results show that the\napproaches that directly estimate a mask perform the best overall in terms of\nestimated speech quality and intelligibility, although the model that directly\nestimates the log magnitude spectrum performs as good in terms of estimated\nspeech quality.", "published": "2018-11-15 08:39:04", "link": "http://arxiv.org/abs/1811.06234v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Effects of Lombard Reflex on the Performance of Deep-Learning-Based\n  Audio-Visual Speech Enhancement Systems", "abstract": "Humans tend to change their way of speaking when they are immersed in a noisy\nenvironment, a reflex known as Lombard effect. Current speech enhancement\nsystems based on deep learning do not usually take into account this change in\nthe speaking style, because they are trained with neutral (non-Lombard) speech\nutterances recorded under quiet conditions to which noise is artificially\nadded. In this paper, we investigate the effects that the Lombard reflex has on\nthe performance of audio-visual speech enhancement systems based on deep\nlearning. The results show that a gap in the performance of as much as\napproximately 5 dB between the systems trained on neutral speech and the ones\ntrained on Lombard speech exists. This indicates the benefit of taking into\naccount the mismatch between neutral and Lombard speech in the design of\naudio-visual speech enhancement systems.", "published": "2018-11-15 09:29:14", "link": "http://arxiv.org/abs/1811.06250v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
