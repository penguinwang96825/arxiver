{"title": "Some Computations for Optimal Execution with Monotone Strategies", "abstract": "We study an optimal execution problem in the infinite horizon setup. Our\nfinancial market is given by the Black-Scholes model with a linear price\nimpact. The main novelty of the current note is that we study the constrained\ncase where the number of shares and the selling rate are non-negative\nprocesses. For this case we give a complete characterization of the value and\nthe optimal control via a solution of a non-linear ordinary differential\nequation (ODE). Furthermore, we provide an example where the non-linear ODE can\nbe solved explicitly. Our approach is purely probabilistic.", "published": "2024-11-16 07:30:24", "link": "http://arxiv.org/abs/2411.10726v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "IntentGPT: Few-shot Intent Discovery with Large Language Models", "abstract": "In today's digitally driven world, dialogue systems play a pivotal role in\nenhancing user interactions, from customer service to virtual assistants. In\nthese dialogues, it is important to identify user's goals automatically to\nresolve their needs promptly. This has necessitated the integration of models\nthat perform Intent Detection. However, users' intents are diverse and dynamic,\nmaking it challenging to maintain a fixed set of predefined intents. As a\nresult, a more practical approach is to develop a model capable of identifying\nnew intents as they emerge. We address the challenge of Intent Discovery, an\narea that has drawn significant attention in recent research efforts. Existing\nmethods need to train on a substantial amount of data for correctly identifying\nnew intents, demanding significant human effort. To overcome this, we introduce\nIntentGPT, a novel training-free method that effectively prompts Large Language\nModels (LLMs) such as GPT-4 to discover new intents with minimal labeled data.\nIntentGPT comprises an \\textit{In-Context Prompt Generator}, which generates\ninformative prompts for In-Context Learning, an \\textit{Intent Predictor} for\nclassifying and discovering user intents from utterances, and a\n\\textit{Semantic Few-Shot Sampler} that selects relevant few-shot examples and\na set of known intents to be injected into the prompt. Our experiments show\nthat IntentGPT outperforms previous methods that require extensive\ndomain-specific data and fine-tuning, in popular benchmarks, including CLINC\nand BANKING, among others.", "published": "2024-11-16 02:16:59", "link": "http://arxiv.org/abs/2411.10670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Dialogue System for Mental Health: An LLM Chatbot Leveraging\n  the PM+ Guidelines", "abstract": "The Structured Dialogue System, referred to as SuDoSys, is an innovative\nLarge Language Model (LLM)-based chatbot designed to provide psychological\ncounseling. SuDoSys leverages the World Health Organization (WHO)'s Problem\nManagement Plus (PM+) guidelines to deliver stage-aware multi-turn dialogues.\nExisting methods for employing an LLM in multi-turn psychological counseling\ntypically involve direct fine-tuning using generated dialogues, often\nneglecting the dynamic stage shifts of counseling sessions. Unlike previous\napproaches, SuDoSys considers the different stages of counseling and stores\nessential information throughout the counseling process, ensuring coherent and\ndirected conversations. The system employs an LLM, a stage-aware instruction\ngenerator, a response unpacker, a topic database, and a stage controller to\nmaintain dialogue flow. In addition, we propose a novel technique that\nsimulates counseling clients to interact with the evaluated system and evaluate\nits performance automatically. When assessed using both objective and\nsubjective evaluations, SuDoSys demonstrates its effectiveness in generating\nlogically coherent responses. The system's code and program scripts for\nevaluation are open-sourced.", "published": "2024-11-16 03:12:17", "link": "http://arxiv.org/abs/2411.10681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings", "abstract": "One of the key tasks in modern applied computational linguistics is\nconstructing word vector representations (word embeddings), which are widely\nused to address natural language processing tasks such as sentiment analysis,\ninformation extraction, and more. To choose an appropriate method for\ngenerating these word embeddings, quality assessment techniques are often\nnecessary. A standard approach involves calculating distances between vectors\nfor words with expert-assessed 'similarity'. This work introduces the first\n'silver standard' dataset for such tasks in the Kyrgyz language, alongside\ntraining corresponding models and validating the dataset's suitability through\nquality evaluation metrics.", "published": "2024-11-16 07:14:32", "link": "http://arxiv.org/abs/2411.10724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Generic LLMs Help Analyze Child-adult Interactions Involving\n  Children with Autism in Clinical Observation?", "abstract": "Large Language Models (LLMs) have shown significant potential in\nunderstanding human communication and interaction. However, their performance\nin the domain of child-inclusive interactions, including in clinical settings,\nremains less explored. In this work, we evaluate generic LLMs' ability to\nanalyze child-adult dyadic interactions in a clinically relevant context\ninvolving children with ASD. Specifically, we explore LLMs in performing four\ntasks: classifying child-adult utterances, predicting engaged activities,\nrecognizing language skills and understanding traits that are clinically\nrelevant. Our evaluation shows that generic LLMs are highly capable of\nanalyzing long and complex conversations in clinical observation sessions,\noften surpassing the performance of non-expert human evaluators. The results\nshow their potential to segment interactions of interest, assist in language\nskills evaluation, identify engaged activities, and offer clinical-relevant\ncontext for assessments.", "published": "2024-11-16 09:36:56", "link": "http://arxiv.org/abs/2411.10761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Anxiety in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated strong performance as\nknowledge repositories, enabling models to understand user queries and generate\naccurate and context-aware responses. Extensive evaluation setups have\ncorroborated the positive correlation between the retrieval capability of LLMs\nand the frequency of entities in their pretraining corpus. We take the\ninvestigation further by conducting a comprehensive analysis of the internal\nreasoning and retrieval mechanisms of LLMs. Our work focuses on three critical\ndimensions - the impact of entity popularity, the models' sensitivity to\nlexical variations in query formulation, and the progression of hidden state\nrepresentations across LLM layers. Our preliminary findings reveal that popular\nquestions facilitate early convergence of internal states toward the correct\nanswer. However, as the popularity of a query increases, retrieved attributes\nacross lexical variations become increasingly dissimilar and less accurate.\nInterestingly, we find that LLMs struggle to disentangle facts, grounded in\ndistinct relations, from their parametric memory when dealing with highly\npopular subjects. Through a case study, we explore these latent strains within\nLLMs when processing highly popular queries, a phenomenon we term information\nanxiety. The emergence of information anxiety in LLMs underscores the\nadversarial injection in the form of linguistic variations and calls for a more\nholistic evaluation of frequently occurring entities.", "published": "2024-11-16 14:28:33", "link": "http://arxiv.org/abs/2411.10813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment", "abstract": "When different groups' values differ, one approach to model alignment is to\nsteer models at inference time towards each group's preferences. However,\ntechniques like in-context learning only consider similarity when drawing\nfew-shot examples and not cross-group differences in values. We propose SPICA,\na framework that accounts for group-level differences during in-context example\nretrieval. SPICA introduces three designs: scenario banks, group-informed\nretrieval metrics, and in-context alignment prompts. From an evaluation of\nSPICA on an alignment task collecting inputs from four demographic groups ($n =\n544$), our metrics retrieve in-context examples that more closely match\nobserved preferences, with the best prompt configuration using multiple\ncontrastive responses to demonstrate examples. In an end-to-end evaluation ($n\n= 120$), we observe that SPICA is higher rated than similarity-based retrieval,\nwith groups seeing up to a +0.16 point improvement on a 5 point scale.\nAdditionally, gains from SPICA were more uniform, with all groups benefiting\nfrom alignment rather than only some. Finally, we find that while a\ngroup-agnostic approach can align to aggregated values, it is not most suited\nfor divergent groups.", "published": "2024-11-16 23:29:32", "link": "http://arxiv.org/abs/2411.10912v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BPO: Towards Balanced Preference Optimization between Knowledge Breadth\n  and Depth in Alignment", "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the key to the success\nof large language models (LLMs) in recent years. In this work, we first\nintroduce the concepts of knowledge breadth and knowledge depth, which measure\nthe comprehensiveness and depth of an LLM or knowledge source respectively. We\nreveal that the imbalance in the number of prompts and responses can lead to a\npotential disparity in breadth and depth learning within alignment tuning\ndatasets by showing that even a simple uniform method for balancing the number\nof instructions and responses can lead to significant improvements. Building on\nthis, we further propose Balanced Preference Optimization (BPO), designed to\ndynamically augment the knowledge depth of each sample. BPO is motivated by the\nobservation that the usefulness of knowledge varies across samples,\nnecessitating tailored learning of knowledge depth. To achieve this, we\nintroduce gradient-based clustering, estimating the knowledge informativeness\nand usefulness of each augmented sample based on the model's optimization\ndirection. Our experimental results across various benchmarks demonstrate that\nBPO outperforms other baseline methods in alignment tuning while maintaining\ntraining efficiency. Furthermore, we conduct a detailed analysis of each\ncomponent of BPO, providing guidelines for future research in preference data\noptimization.", "published": "2024-11-16 23:53:27", "link": "http://arxiv.org/abs/2411.10914v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large\n  Language Models on Mobile Devices", "abstract": "The emergence and growing popularity of multimodal large language models\n(MLLMs) have significant potential to enhance various aspects of daily life,\nfrom improving communication to facilitating learning and problem-solving.\nMobile phones, as essential daily companions, represent the most effective and\naccessible deployment platform for MLLMs, enabling seamless integration into\neveryday tasks. However, deploying MLLMs on mobile phones presents challenges\ndue to limitations in memory size and computational capability, making it\ndifficult to achieve smooth and real-time processing without extensive\noptimization. In this paper, we present BlueLM-V-3B, an algorithm and system\nco-design approach specifically tailored for the efficient deployment of MLLMs\non mobile platforms. To be specific, we redesign the dynamic resolution scheme\nadopted by mainstream MLLMs and implement system optimization for\nhardware-aware deployment to optimize model inference on mobile phones.\nBlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B\nfeatures a language model with 2.7B parameters and a vision encoder with 400M\nparameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4\ntoken/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight\nquantization. (3) Strong Performance: BlueLM-V-3B has attained the highest\naverage score of 66.1 on the OpenCompass benchmark among models with $\\leq$ 4B\nparameters and surpassed a series of models with much larger parameter sizes\n(e.g., MiniCPM-V-2.6, InternVL2-8B).", "published": "2024-11-16 00:14:51", "link": "http://arxiv.org/abs/2411.10640v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SAM Decoding: Speculative Decoding via Suffix Automaton", "abstract": "Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}.", "published": "2024-11-16 02:02:49", "link": "http://arxiv.org/abs/2411.10666v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Comparison of Multilingual and Bilingual Models for Satirical News\n  Detection of Arabic and English", "abstract": "Satirical news is real news combined with a humorous comment or exaggerated\ncontent, and it often mimics the format and style of real news. However,\nsatirical news is often misunderstood as misinformation, especially by\nindividuals from different cultural and social backgrounds. This research\naddresses the challenge of distinguishing satire from truthful news by\nleveraging multilingual satire detection methods in English and Arabic. We\nexplore both zero-shot and chain-of-thought (CoT) prompting using two language\nmodels, Jais-chat(13B) and LLaMA-2-chat(7B). Our results show that CoT\nprompting offers a significant advantage for the Jais-chat model over the\nLLaMA-2-chat model. Specifically, Jais-chat achieved the best performance, with\nan F1-score of 80\\% in English when using CoT prompting. These results\nhighlight the importance of structured reasoning in CoT, which enhances\ncontextual understanding and is vital for complex tasks like satire detection.", "published": "2024-11-16 07:49:15", "link": "http://arxiv.org/abs/2411.10730v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Large Vision-Language Models for Remote Sensing Visual Question\n  Answering", "abstract": "Remote Sensing Visual Question Answering (RSVQA) is a challenging task that\ninvolves interpreting complex satellite imagery to answer natural language\nquestions. Traditional approaches often rely on separate visual feature\nextractors and language processing models, which can be computationally\nintensive and limited in their ability to handle open-ended questions. In this\npaper, we propose a novel method that leverages a generative Large\nVision-Language Model (LVLM) to streamline the RSVQA process. Our approach\nconsists of a two-step training strategy: domain-adaptive pretraining and\nprompt-based finetuning. This method enables the LVLM to generate natural\nlanguage answers by conditioning on both visual and textual inputs, without the\nneed for predefined answer categories. We evaluate our model on the RSVQAxBEN\ndataset, demonstrating superior performance compared to state-of-the-art\nbaselines. Additionally, a human evaluation study shows that our method\nproduces answers that are more accurate, relevant, and fluent. The results\nhighlight the potential of generative LVLMs in advancing the field of remote\nsensing analysis.", "published": "2024-11-16 18:32:38", "link": "http://arxiv.org/abs/2411.10857v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their susceptibility to biases poses significant challenges. This\ncomprehensive review examines the landscape of bias in LLMs, from its origins\nto current mitigation strategies. We categorize biases as intrinsic and\nextrinsic, analyzing their manifestations in various NLP tasks. The review\ncritically assesses a range of bias evaluation methods, including data-level,\nmodel-level, and output-level approaches, providing researchers with a robust\ntoolkit for bias detection. We further explore mitigation strategies,\ncategorizing them into pre-model, intra-model, and post-model techniques,\nhighlighting their effectiveness and limitations. Ethical and legal\nimplications of biased LLMs are discussed, emphasizing potential harms in\nreal-world applications such as healthcare and criminal justice. By\nsynthesizing current knowledge on bias in LLMs, this review contributes to the\nongoing effort to develop fair and responsible AI systems. Our work serves as a\ncomprehensive resource for researchers and practitioners working towards\nunderstanding, evaluating, and mitigating bias in LLMs, fostering the\ndevelopment of more equitable AI technologies.", "published": "2024-11-16 23:54:53", "link": "http://arxiv.org/abs/2411.10915v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Approach to Eliminating Hallucinations in Large Language\n  Model-Assisted Causal Discovery", "abstract": "The increasing use of large language models (LLMs) in causal discovery as a\nsubstitute for human domain experts highlights the need for optimal model\nselection. This paper presents the first hallucination survey of popular LLMs\nfor causal discovery. We show that hallucinations exist when using LLMs in\ncausal discovery so the choice of LLM is important. We propose using Retrieval\nAugmented Generation (RAG) to reduce hallucinations when quality data is\navailable. Additionally, we introduce a novel method employing multiple LLMs\nwith an arbiter in a debate to audit edges in causal graphs, achieving a\ncomparable reduction in hallucinations to RAG.", "published": "2024-11-16 03:06:39", "link": "http://arxiv.org/abs/2411.12759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Playing Language Game with LLMs Leads to Jailbreaking", "abstract": "The advent of large language models (LLMs) has spurred the development of\nnumerous jailbreak techniques aimed at circumventing their security defenses\nagainst malicious attacks. An effective jailbreak approach is to identify a\ndomain where safety generalization fails, a phenomenon known as mismatched\ngeneralization. In this paper, we introduce two novel jailbreak methods based\non mismatched generalization: natural language games and custom language games,\nboth of which effectively bypass the safety mechanisms of LLMs, with various\nkinds and different variants, making them hard to defend and leading to high\nattack rates. Natural language games involve the use of synthetic linguistic\nconstructs and the actions intertwined with these constructs, such as the Ubbi\nDubbi language. Building on this phenomenon, we propose the custom language\ngames method: by engaging with LLMs using a variety of custom rules, we\nsuccessfully execute jailbreak attacks across multiple LLM platforms. Extensive\nexperiments demonstrate the effectiveness of our methods, achieving success\nrates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.\nFurthermore, to investigate the generalizability of safety alignments, we\nfine-tuned Llama-3.1-70B with the custom language games to achieve safety\nalignment within our datasets and found that when interacting through other\nlanguage games, the fine-tuned models still failed to identify harmful content.\nThis finding indicates that the safety alignment knowledge embedded in LLMs\nfails to generalize across different linguistic formats, thus opening new\navenues for future research in this area.", "published": "2024-11-16 13:07:13", "link": "http://arxiv.org/abs/2411.12762v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gender Bias Mitigation for Bangla Classification Tasks", "abstract": "In this study, we investigate gender bias in Bangla pretrained language\nmodels, a largely under explored area in low-resource languages. To assess this\nbias, we applied gender-name swapping techniques to existing datasets, creating\nfour manually annotated, task-specific datasets for sentiment analysis,\ntoxicity detection, hate speech detection, and sarcasm detection. By altering\nnames and gender-specific terms, we ensured these datasets were suitable for\ndetecting and mitigating gender bias. We then proposed a joint loss\noptimization technique to mitigate gender bias across task-specific pretrained\nmodels. Our approach was evaluated against existing bias mitigation methods,\nwith results showing that our technique not only effectively reduces bias but\nalso maintains competitive accuracy compared to other baseline approaches. To\npromote further research, we have made both our implementation and datasets\npublicly available\nhttps://github.com/sajib-kumar/Gender-Bias-Mitigation-From-Bangla-PLM", "published": "2024-11-16 00:04:45", "link": "http://arxiv.org/abs/2411.10636v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTA: Multimodal Task Alignment for BEV Perception and Captioning", "abstract": "Bird's eye view (BEV)-based 3D perception plays a crucial role in autonomous\ndriving applications. The rise of large language models has spurred interest in\nBEV-based captioning to understand object behavior in the surrounding\nenvironment. However, existing approaches treat perception and captioning as\nseparate tasks, focusing on the performance of only one task and overlooking\nthe potential benefits of multimodal alignment. To bridge this gap between\nmodalities, we introduce MTA, a novel multimodal task alignment framework that\nboosts both BEV perception and captioning. MTA consists of two key components:\n(1) BEV-Language Alignment (BLA), a contextual learning mechanism that aligns\nthe BEV scene representations with ground-truth language representations, and\n(2) Detection-Captioning Alignment (DCA), a cross-modal prompting mechanism\nthat aligns detection and captioning outputs. MTA seamlessly integrates into\nstate-of-the-art baselines during training, adding no extra computational\ncomplexity at runtime. Extensive experiments on the nuScenes and TOD3Cap\ndatasets show that MTA significantly outperforms state-of-the-art baselines in\nboth tasks, achieving a 10.7% improvement in challenging rare perception\nscenarios and a 9.2% improvement in captioning. These results underscore the\neffectiveness of unified alignment in reconciling BEV-based perception and\ncaptioning.", "published": "2024-11-16 00:14:13", "link": "http://arxiv.org/abs/2411.10639v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Regularized LSTM Method for Detecting Fake News Articles", "abstract": "Nowadays, the rapid diffusion of fake news poses a significant problem, as it\ncan spread misinformation and confusion. This paper aims to develop an advanced\nmachine learning solution for detecting fake news articles. Leveraging a\ncomprehensive dataset of news articles, including 23,502 fake news articles and\n21,417 accurate news articles, we implemented and evaluated three\nmachine-learning models. Our dataset, curated from diverse sources, provides\nrich textual content categorized into title, text, subject, and Date features.\nThese features are essential for training robust classification models to\ndistinguish between fake and authentic news articles. The initial model\nemployed a Long Short-Term Memory (LSTM) network, achieving an accuracy of 94%.\nThe second model improved upon this by incorporating additional regularization\ntechniques and fine-tuning hyperparameters, resulting in a 97% accuracy. The\nfinal model combined the strengths of previous architectures with advanced\noptimization strategies, achieving a peak accuracy of 98%. These results\ndemonstrate the effectiveness of our approach in identifying fake news with\nhigh precision. Implementing these models showcases significant advancements in\nnatural language processing and machine learning techniques, contributing\nvaluable tools for combating misinformation. Our work highlights the potential\nfor deploying such models in real-world applications, providing a reliable\nmethod for automated fake news detection and enhancing the credibility of news\ndissemination.", "published": "2024-11-16 05:54:36", "link": "http://arxiv.org/abs/2411.10713v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Chain-of-Programming (CoP) : Empowering Large Language Models for\n  Geospatial Code Generation", "abstract": "With the rapid growth of interdisciplinary demands for geospatial modeling\nand the rise of large language models (LLMs), geospatial code generation\ntechnology has seen significant advancements. However, existing LLMs often face\nchallenges in the geospatial code generation process due to incomplete or\nunclear user requirements and insufficient knowledge of specific platform\nsyntax rules, leading to the generation of non-executable code, a phenomenon\nknown as \"code hallucination.\" To address this issue, this paper proposes a\nChain of Programming (CoP) framework, which decomposes the code generation\nprocess into five steps: requirement analysis, algorithm design, code\nimplementation, code debugging, and code annotation. The framework incorporates\na shared information pool, knowledge base retrieval, and user feedback\nmechanisms, forming an end-to-end code generation flow from requirements to\ncode without the need for model fine-tuning. Based on a geospatial problem\nclassification framework and evaluation benchmarks, the CoP strategy\nsignificantly improves the logical clarity, syntactical correctness, and\nexecutability of the generated code, with improvements ranging from 3.0% to\n48.8%. Comparative and ablation experiments further validate the superiority of\nthe CoP strategy over other optimization approaches and confirm the rationality\nand necessity of its key components. Through case studies on building data\nvisualization and fire data analysis, this paper demonstrates the application\nand effectiveness of CoP in various geospatial scenarios. The CoP framework\noffers a systematic, step-by-step approach to LLM-based geospatial code\ngeneration tasks, significantly enhancing code generation performance in\ngeospatial tasks and providing valuable insights for code generation in other\nvertical domains.", "published": "2024-11-16 09:20:35", "link": "http://arxiv.org/abs/2411.10753v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Bilingual Text-dependent Speaker Verification with Pre-trained Models\n  for TdSV Challenge 2024", "abstract": "This paper presents our submissions to the Iranian division of the\nText-dependent Speaker Verification Challenge (TdSV) 2024. TdSV aims to\ndetermine if a specific phrase was spoken by a target speaker. We developed two\nindependent subsystems based on pre-trained models: For phrase verification, a\nphrase classifier rejected incorrect phrases, while for speaker verification, a\npre-trained ResNet293 with domain adaptation extracted speaker embeddings for\ncomputing cosine similarity scores. In addition, we evaluated Whisper-PMFA, a\npre-trained ASR model adapted for speaker verification, and found that,\nalthough it outperforms randomly initialized ResNets, it falls short of the\nperformance of pre-trained ResNets, highlighting the importance of large-scale\npre-training. The results also demonstrate that achieving competitive\nperformance on TdSV without joint modeling of speaker and text is possible. Our\nbest system achieved a MinDCF of 0.0358 on the evaluation subset and won the\nchallenge.", "published": "2024-11-16 15:53:03", "link": "http://arxiv.org/abs/2411.10828v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Large Language Models (LLMs) as Traffic Control Systems at Urban\n  Intersections: A New Paradigm", "abstract": "This study introduces a novel approach for traffic control systems by using\nLarge Language Models (LLMs) as traffic controllers. The study utilizes their\nlogical reasoning, scene understanding, and decision-making capabilities to\noptimize throughput and provide feedback based on traffic conditions in\nreal-time. LLMs centralize traditionally disconnected traffic control processes\nand can integrate traffic data from diverse sources to provide context-aware\ndecisions. LLMs can also deliver tailored outputs using various means such as\nwireless signals and visuals to drivers, infrastructures, and autonomous\nvehicles. To evaluate LLMs ability as traffic controllers, this study proposed\na four-stage methodology. The methodology includes data creation and\nenvironment initialization, prompt engineering, conflict identification, and\nfine-tuning. We simulated multi-lane four-leg intersection scenarios and\ngenerates detailed datasets to enable conflict detection using LLMs and Python\nsimulation as a ground truth. We used chain-of-thought prompts to lead LLMs in\nunderstanding the context, detecting conflicts, resolving them using traffic\nrules, and delivering context-sensitive traffic management solutions. We\nevaluated the prformance GPT-mini, Gemini, and Llama as traffic controllers.\nResults showed that the fine-tuned GPT-mini achieved 83% accuracy and an\nF1-score of 0.84. GPT-mini model exhibited a promising performance in\ngenerating actionable traffic management insights, with high ROUGE-L scores\nacross conflict identification of 0.95, decision-making of 0.91, priority\nassignment of 0.94, and waiting time optimization of 0.92. We demonstrated that\nLLMs can offer precise recommendations to drivers in real-time including\nyielding, slowing, or stopping based on vehicle dynamics.", "published": "2024-11-16 19:23:52", "link": "http://arxiv.org/abs/2411.10869v1", "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Empowering Meta-Analysis: Leveraging Large Language Models for\n  Scientific Synthesis", "abstract": "This study investigates the automation of meta-analysis in scientific\ndocuments using large language models (LLMs). Meta-analysis is a robust\nstatistical method that synthesizes the findings of multiple studies support\narticles to provide a comprehensive understanding. We know that a meta-article\nprovides a structured analysis of several articles. However, conducting\nmeta-analysis by hand is labor-intensive, time-consuming, and susceptible to\nhuman error, highlighting the need for automated pipelines to streamline the\nprocess. Our research introduces a novel approach that fine-tunes the LLM on\nextensive scientific datasets to address challenges in big data handling and\nstructured data extraction. We automate and optimize the meta-analysis process\nby integrating Retrieval Augmented Generation (RAG). Tailored through prompt\nengineering and a new loss metric, Inverse Cosine Distance (ICD), designed for\nfine-tuning on large contextual datasets, LLMs efficiently generate structured\nmeta-analysis content. Human evaluation then assesses relevance and provides\ninformation on model performance in key metrics. This research demonstrates\nthat fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs\ngenerating 87.6% relevant meta-analysis abstracts. The relevance of the\ncontext, based on human evaluation, shows a reduction in irrelevancy from 4.56%\nto 1.9%. These experiments were conducted in a low-resource environment,\nhighlighting the study's contribution to enhancing the efficiency and\nreliability of meta-analysis automation.", "published": "2024-11-16 20:18:57", "link": "http://arxiv.org/abs/2411.10878v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unveiling User Preferences: A Knowledge Graph and LLM-Driven Approach\n  for Conversational Recommendation", "abstract": "Conversational Recommender Systems (CRSs) aim to provide personalized\nrecommendations through dynamically capturing user preferences in interactive\nconversations. Conventional CRSs often extract user preferences as hidden\nrepresentations, which are criticized for their lack of interpretability. This\ndiminishes the transparency and trustworthiness of the recommendation process.\nRecent works have explored combining the impressive capabilities of Large\nLanguage Models (LLMs) with the domain-specific knowledge of Knowledge Graphs\n(KGs) to generate human-understandable recommendation explanations. Despite\nthese efforts, the integration of LLMs and KGs for CRSs remains challenging due\nto the modality gap between unstructured dialogues and structured KGs.\nMoreover, LLMs pre-trained on large-scale corpora may not be well-suited for\nanalyzing user preferences, which require domain-specific knowledge. In this\npaper, we propose COMPASS, a plug-and-play framework that synergizes LLMs and\nKGs to unveil user preferences, enhancing the performance and explainability of\nexisting CRSs. To address integration challenges, COMPASS employs a two-stage\ntraining approach: first, it bridges the gap between the structured KG and\nnatural language through an innovative graph entity captioning pre-training\nmechanism. This enables the LLM to transform KG entities into concise natural\nlanguage descriptions, allowing them to comprehend domain-specific knowledge.\nFollowing, COMPASS optimizes user preference modeling via knowledge-aware\ninstruction fine-tuning, where the LLM learns to reason and summarize user\npreferences from both dialogue histories and KG-augmented context. This enables\nCOMPASS to perform knowledge-aware reasoning and generate comprehensive and\ninterpretable user preferences that can seamlessly integrate with existing CRS\nmodels for improving recommendation performance and explainability.", "published": "2024-11-16 11:47:21", "link": "http://arxiv.org/abs/2411.14459v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLaSA: Large Language and Structured Data Assistant", "abstract": "Structured data, such as tables, graphs, and databases, play a critical role\nin plentiful NLP tasks such as question answering and dialogue system.\nRecently, inspired by Vision-Language Models, Graph Neutral Networks (GNNs)\nhave been introduced as an additional modality into the input of Large Language\nModels (LLMs) to improve their performance on Structured Knowledge Grounding\n(SKG) tasks. However, those GNN-enhanced LLMs have the following limitations:\n(1) They employ diverse GNNs to model varying types of structured data,\nrendering them unable to uniformly process various forms of structured data.\n(2) The pretraining of GNNs is coupled with specific LLMs, which prevents GNNs\nfrom fully aligning with the textual space and limits their adaptability to\nother LLMs. To address these issues, we propose \\textbf{L}arge\n\\textbf{L}anguage and \\textbf{S}tructured Data \\textbf{A}ssistant (LLaSA), a\ngeneral framework for enhancing LLMs' ability to handle structured data.\nSpecifically, we represent various types of structured data in a unified\nhypergraph format, and use self-supervised learning to pretrain a hypergraph\nencoder, and a G-Former compressing encoded hypergraph representations with\ncross-attention. The compressed hypergraph representations are appended to the\nserialized inputs during training and inference stages of LLMs. Experimental\nresults on multiple SKG tasks show that our pretrained hypergraph encoder can\nadapt to various LLMs and enhance their ability to process different types of\nstructured data. Besides, LLaSA, with LoRA fine-tuning, outperforms previous\nSOTA method using full parameters tuning.", "published": "2024-11-16 12:27:14", "link": "http://arxiv.org/abs/2411.14460v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Next-Generation Medical Agent: How o1 is Reshaping\n  Decision-Making in Medical Scenarios", "abstract": "Artificial Intelligence (AI) has become essential in modern healthcare, with\nlarge language models (LLMs) offering promising advances in clinical\ndecision-making. Traditional model-based approaches, including those leveraging\nin-context demonstrations and those with specialized medical fine-tuning, have\ndemonstrated strong performance in medical language processing but struggle\nwith real-time adaptability, multi-step reasoning, and handling complex medical\ntasks. Agent-based AI systems address these limitations by incorporating\nreasoning traces, tool selection based on context, knowledge retrieval, and\nboth short- and long-term memory. These additional features enable the medical\nAI agent to handle complex medical scenarios where decision-making should be\nbuilt on real-time interaction with the environment. Therefore, unlike\nconventional model-based approaches that treat medical queries as isolated\nquestions, medical AI agents approach them as complex tasks and behave more\nlike human doctors. In this paper, we study the choice of the backbone LLM for\nmedical AI agents, which is the foundation for the agent's overall reasoning\nand action generation. In particular, we consider the emergent o1 model and\nexamine its impact on agents' reasoning, tool-use adaptability, and real-time\ninformation retrieval across diverse clinical scenarios, including high-stakes\nsettings such as intensive care units (ICUs). Our findings demonstrate o1's\nability to enhance diagnostic accuracy and consistency, paving the way for\nsmarter, more responsive AI tools that support better patient outcomes and\ndecision-making efficacy in clinical practice.", "published": "2024-11-16 18:19:53", "link": "http://arxiv.org/abs/2411.14461v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "BanglaDialecto: An End-to-End AI-Powered Regional Speech Standardization", "abstract": "This study focuses on recognizing Bangladeshi dialects and converting diverse\nBengali accents into standardized formal Bengali speech. Dialects, often\nreferred to as regional languages, are distinctive variations of a language\nspoken in a particular location and are identified by their phonetics,\npronunciations, and lexicon. Subtle changes in pronunciation and intonation are\nalso influenced by geographic location, educational attainment, and\nsocioeconomic status. Dialect standardization is needed to ensure effective\ncommunication, educational consistency, access to technology, economic\nopportunities, and the preservation of linguistic resources while respecting\ncultural diversity. Being the fifth most spoken language with around 55\ndistinct dialects spoken by 160 million people, addressing Bangla dialects is\ncrucial for developing inclusive communication tools. However, limited research\nexists due to a lack of comprehensive datasets and the challenges of handling\ndiverse dialects. With the advancement in multilingual Large Language Models\n(mLLMs), emerging possibilities have been created to address the challenges of\ndialectal Automated Speech Recognition (ASR) and Machine Translation (MT). This\nstudy presents an end-to-end pipeline for converting dialectal Noakhali speech\nto standard Bangla speech. This investigation includes constructing a\nlarge-scale diverse dataset with dialectal speech signals that tailored the\nfine-tuning process in ASR and LLM for transcribing the dialect speech to\ndialect text and translating the dialect text to standard Bangla text. Our\nexperiments demonstrated that fine-tuning the Whisper ASR model achieved a CER\nof 0.8% and WER of 1.5%, while the BanglaT5 model attained a BLEU score of\n41.6% for dialect-to-standard text translation.", "published": "2024-11-16 20:20:15", "link": "http://arxiv.org/abs/2411.10879v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Explainable DNN-based Beamformer with Postfilter", "abstract": "This paper introduces an explainable DNN-based beamformer with a postfilter\n(ExNet-BF+PF) for multichannel signal processing. Our approach combines the\nU-Net network with a beamformer structure to address this problem. The method\ninvolves a two-stage processing pipeline. In the first stage, time-invariant\nweights are applied to construct a multichannel spatial filter, namely a\nbeamformer. In the second stage, a time-varying single-channel post-filter is\napplied at the beamformer output. Additionally, we incorporate an attention\nmechanism inspired by its successful application in noisy and reverberant\nenvironments to improve speech enhancement further.\n  Furthermore, our study fills a gap in the existing literature by conducting a\nthorough spatial analysis of the network's performance. Specifically, we\nexamine how the network utilizes spatial information during processing. This\nanalysis yields valuable insights into the network's functionality, thereby\nenhancing our understanding of its overall performance.\n  Experimental results demonstrate that our approach is not only\nstraightforward to train but also yields superior results, obviating the\nnecessity for prior knowledge of the speaker's activity.", "published": "2024-11-16 18:22:03", "link": "http://arxiv.org/abs/2411.10854v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
