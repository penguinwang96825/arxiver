{"title": "Integrating Multi-Head Convolutional Encoders with Cross-Attention for\n  Improved SPARQL Query Translation", "abstract": "The main task of the KGQA system (Knowledge Graph Question Answering) is to\nconvert user input questions into query syntax (such as SPARQL). With the rise\nof modern popular encoders and decoders like Transformer and ConvS2S, many\nscholars have shifted the research direction of SPARQL generation to the Neural\nMachine Translation (NMT) architecture or the generative AI field of\nText-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query\nsyntax as a language. It uses NMT-based translation models to translate natural\nlanguage questions into query syntax. Scholars use popular architectures\nequipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to\ntrain translation models for query syntax. To achieve better query results,\nthis paper improved the ConvS2S encoder and added multi-head attention from the\nTransformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the\nn-gram language model. The principle is to use convolutional layers to capture\nlocal hidden features in the input sequence with different receptive fields,\nusing multi-head attention to calculate dependencies between them. Ultimately,\nwe found that the translation model based on the Multi-Head Conv encoder\nachieved better performance than other encoders, obtaining 76.52\\% and 83.37\\%\nBLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0\ndatasets, respectively. Additionally, in the end-to-end system experiments on\nthe QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other\nKGQA systems, with Macro F1-measures reaching 52\\% and 66\\%, respectively.\nMoreover, the experimental results show that with limited computational\nresources, if one possesses an excellent encoder-decoder architecture and\ncross-attention, experts and scholars can achieve outstanding performance\nequivalent to large pre-trained models using only general embeddings.", "published": "2024-08-24 01:58:28", "link": "http://arxiv.org/abs/2408.13432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Antiwork: A RoBERTa-Based System for Work-Related Stress\n  Identification and Leading Factor Analysis", "abstract": "Harsh working environments and work-related stress have been known to\ncontribute to mental health problems such as anxiety, depression, and suicidal\nideation. As such, it is paramount to create solutions that can both detect\nemployee unhappiness and find the root cause of the problem. While prior works\nhave examined causes of mental health using machine learning, they typically\nfocus on general mental health analysis, with few of them focusing on\nexplainable solutions or looking at the workplace-specific setting. r/antiwork\nis a subreddit for the antiwork movement, which is the desire to stop working\naltogether. Using this subreddit as a proxy for work environment\ndissatisfaction, we create a new dataset for antiwork sentiment detection and\nsubsequently train a model that highlights the words with antiwork sentiments.\nFollowing this, we performed a qualitative and quantitative analysis to uncover\nsome of the key insights into the mindset of individuals who identify with the\nantiwork movement and how their working environments influenced them. We find\nthat working environments that do not give employees authority or\nresponsibility, frustrating recruiting experiences, and unfair compensation,\nare some of the leading causes of the antiwork sentiment, resulting in a lack\nof self-confidence and motivation among their employees.", "published": "2024-08-24 05:15:15", "link": "http://arxiv.org/abs/2408.13473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the\n  Role of RAG Noise in Large Language Models", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a crucial method for\naddressing hallucinations in large language models (LLMs). While recent\nresearch has extended RAG models to complex noisy scenarios, these explorations\noften confine themselves to limited noise types and presuppose that noise is\ninherently detrimental to LLMs, potentially deviating from real-world retrieval\nenvironments and restricting practical applicability. In this paper, we define\nseven distinct noise types from a linguistic perspective and establish a Noise\nRAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing\nmultiple datasets and reasoning tasks. Through empirical evaluation of eight\nrepresentative LLMs with diverse architectures and scales, we reveal that these\nnoises can be further categorized into two practical groups: noise that is\nbeneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs\n(aka harmful noise). While harmful noise generally impairs performance,\nbeneficial noise may enhance several aspects of model capabilities and overall\nperformance. Our analysis offers insights for developing more robust, adaptable\nRAG solutions and mitigating hallucinations across diverse retrieval scenarios.", "published": "2024-08-24 09:23:01", "link": "http://arxiv.org/abs/2408.13533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Adaptation of Menus: A Fine-Grained Approach", "abstract": "Machine Translation of Culture-Specific Items (CSIs) poses significant\nchallenges. Recent work on CSI translation has shown some success using Large\nLanguage Models (LLMs) to adapt to different languages and cultures; however, a\ndeeper analysis is needed to examine the benefits and pitfalls of each method.\nIn this paper, we introduce the ChineseMenuCSI dataset, the largest for\nChinese-English menu corpora, annotated with CSI vs Non-CSI labels and a\nfine-grained test set. We define three levels of CSI figurativeness for a more\nnuanced analysis and develop a novel methodology for automatic CSI\nidentification, which outperforms GPT-based prompts in most categories.\nImportantly, we are the first to integrate human translation theories into\nLLM-driven translation processes, significantly improving translation accuracy,\nwith COMET scores increasing by up to 7 points.", "published": "2024-08-24 09:25:18", "link": "http://arxiv.org/abs/2408.13534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question\n  Answering", "abstract": "To evaluate Large Language Models (LLMs) for question answering (QA),\ntraditional methods typically focus on assessing single-turn responses to given\nquestions. However, this approach doesn't capture the dynamic nature of\nhuman-AI interactions, where humans actively seek information through\nconversation. Recent works in human-computer interaction (HCI) have employed\nhuman evaluators to conduct interactions and evaluations, but they are often\nprohibitively expensive and time-consuming to scale. We introduce an automatic\nevaluation framework IQA-EVAL to achieve Interactive Question Answering\nEvaluations, more specifically, we introduce a LLM-based Evaluation Agent (LEA)\nthat can: (1) simulate human behaviors to generate interactions with IQA\nmodels; (2) automatically evaluate the generated interactions. Moreover, we\npropose assigning personas to LEAs to better simulate groups of real human\nevaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude)\nas the backbone model achieves a high correlation with human evaluations on the\nIQA task; (2) assigning personas to LEA to better represent the crowd further\nsignificantly improves correlations. Finally, we use our automatic metric to\nevaluate five recent representative LLMs with over 1000 questions from complex\nand ambiguous question answering tasks, which comes with a substantial cost of\n$5k if evaluated by humans.", "published": "2024-08-24 10:34:20", "link": "http://arxiv.org/abs/2408.13545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narratives at Conflict: Computational Analysis of News Framing in\n  Multilingual Disinformation Campaigns", "abstract": "Any report frames issues to favor a particular interpretation by highlighting\nor excluding certain aspects of a story. Despite the widespread use of framing\nin disinformation, framing properties and detection methods remain\nunderexplored outside the English-speaking world. We explore how multilingual\nframing of the same issue differs systematically. We use eight years of\nRussia-backed disinformation campaigns, spanning 8k news articles in 4\nlanguages targeting 15 countries. We find that disinformation campaigns\nconsistently and intentionally favor specific framing, depending on the target\nlanguage of the audience. We further discover how Russian-language articles\nconsistently highlight selected frames depending on the region of the media\ncoverage. We find that the two most prominent models for automatic frame\nanalysis underperform and show high disagreement, highlighting the need for\nfurther research.", "published": "2024-08-24 18:51:47", "link": "http://arxiv.org/abs/2408.13651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbolic Working Memory Enhances Language Models for Complex Rule\n  Application", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but\nstruggle with multi-step deductive reasoning involving a series of rule\napplication steps, especially when rules are presented non-sequentially. Our\npreliminary analysis shows that while LLMs excel in single-step rule\napplication, their performance drops significantly in multi-step scenarios due\nto the challenge in rule grounding. It requires anchoring the applicable rule\nand supporting facts at each step, amidst multiple input rules, facts, and\ninferred facts. To address this, we propose augmenting LLMs with external\nworking memory and introduce a neurosymbolic framework for rule application.\nThe memory stores facts and rules in both natural language and symbolic forms,\nenabling precise tracking. Utilizing this memory, our framework iteratively\nperforms symbolic rule grounding and LLM-based rule implementation. The former\nmatches predicates and variables of symbolic rules and facts to ground\napplicable rules at each step. Experiments indicate our framework's\neffectiveness in rule application and its robustness across various steps and\nsettings~\\footnote{Code and data are available at\n\\url{https://github.com/SiyuanWangw/RuleApplication}.}.", "published": "2024-08-24 19:11:54", "link": "http://arxiv.org/abs/2408.13654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A layer-wise analysis of Mandarin and English suprasegmentals in SSL\n  speech models", "abstract": "This study asks how self-supervised speech models represent suprasegmental\ncategories like Mandarin lexical tone, English lexical stress, and English\nphrasal accents. Through a series of probing tasks, we make layer-wise\ncomparisons of English and Mandarin 12 layer monolingual models. Our findings\nsuggest that 1) English and Mandarin wav2vec 2.0 models learn contextual\nrepresentations of abstract suprasegmental categories which are strongest in\nthe middle third of the network. 2) Models are better at representing features\nthat exist in the language of their training data, and this difference is\ndriven by enriched context in transformer blocks, not local acoustic\nrepresentation. 3) Fine-tuned wav2vec 2.0 improves performance in later layers\ncompared to pre-trained models mainly for lexically contrastive features like\ntone and stress, 4) HuBERT and WavLM learn similar representations to wav2vec\n2.0, differing mainly in later layer performance. Our results extend previous\nunderstanding of how models represent suprasegmentals and offer new insights\ninto the language-specificity and contextual nature of these representations.", "published": "2024-08-24 22:03:40", "link": "http://arxiv.org/abs/2408.13678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative-Adversarial Networks for Low-Resource Language Data\n  Augmentation in Machine Translation", "abstract": "Neural Machine Translation (NMT) systems struggle when translating to and\nfrom low-resource languages, which lack large-scale data corpora for models to\nuse for training. As manual data curation is expensive and time-consuming, we\npropose utilizing a generative-adversarial network (GAN) to augment\nlow-resource language data. When training on a very small amount of language\ndata (under 20,000 sentences) in a simulated low-resource setting, our model\nshows potential at data augmentation, generating monolingual language data with\nsentences such as \"ask me that healthy lunch im cooking up,\" and \"my\ngrandfather work harder than your grandfather before.\" Our novel data\naugmentation approach takes the first step in investigating the capability of\nGANs in low-resource NMT, and our results suggest that there is promise for\nfuture extension of GANs to low-resource NMT.", "published": "2024-08-24 00:02:00", "link": "http://arxiv.org/abs/2409.00071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Conversation Derailment Forecasting Using Graph\n  Convolutional Networks", "abstract": "Online conversations are particularly susceptible to derailment, which can\nmanifest itself in the form of toxic communication patterns including\ndisrespectful comments and abuse. Forecasting conversation derailment predicts\nsigns of derailment in advance enabling proactive moderation of conversations.\nState-of-the-art approaches to conversation derailment forecasting sequentially\nencode conversations and use graph neural networks to model dialogue user\ndynamics. However, existing graph models are not able to capture complex\nconversational characteristics such as context propagation and emotional\nshifts. The use of common sense knowledge enables a model to capture such\ncharacteristics, thus improving performance. Following this approach, here we\nderive commonsense statements from a knowledge base of dialogue contextual\ninformation to enrich a graph neural network classification architecture. We\nfuse the multi-source information on utterance into capsules, which are used by\na transformer-based forecaster to predict conversation derailment. Our model\ncaptures conversation dynamics and context propagation, outperforming the\nstate-of-the-art models on the CGA and CMV benchmark datasets", "published": "2024-08-24 02:40:28", "link": "http://arxiv.org/abs/2408.13440v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for\n  Cost-Efficient Reasoning", "abstract": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought\nreasoning, shows significant gains across various multi-step reasoning tasks\nbut comes with a high cost due to multiple sampling with the preset size. Its\nvariants, Adaptive self-consistency (ASC) and Early-stopping self-consistency\n(ESC), dynamically adjust the number of samples based on the posterior\ndistribution of a set of pre-samples, reducing the cost of SC with minimal\nimpact on performance. Both methods, however, do not exploit the prior\ninformation about question difficulty. It often results in unnecessary repeated\nsampling for easy questions that could be accurately answered with just one\nattempt, wasting resources. To tackle this problem, we propose\nDifficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty\ninformation of batch queries from both prior and posterior perspectives to\nadaptively allocate inference resources, further reducing the overall cost of\nSC. To demonstrate the effectiveness of DSC, we conduct extensive experiments\non three popular categories of reasoning tasks: arithmetic, commonsense and\nsymbolic reasoning on six benchmarks. The empirical results show that DSC\nconsistently surpasses the strong baseline ASC and ESC in terms of costs by a\nsignificant margin, while attaining comparable performances.", "published": "2024-08-24 04:03:35", "link": "http://arxiv.org/abs/2408.13457v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing Large Language Models for Named Entity Recognition in\n  Traditional Chinese Medicine against COVID-19 Literature: Comparative Study", "abstract": "Objective: To explore and compare the performance of ChatGPT and other\nstate-of-the-art LLMs on domain-specific NER tasks covering different entity\ntypes and domains in TCM against COVID-19 literature. Methods: We established a\ndataset of 389 articles on TCM against COVID-19, and manually annotated 48 of\nthem with 6 types of entities belonging to 3 domains as the ground truth,\nagainst which the NER performance of LLMs can be assessed. We then performed\nNER tasks for the 6 entity types using ChatGPT (GPT-3.5 and GPT-4) and 4\nstate-of-the-art BERT-based question-answering (QA) models (RoBERTa, MiniLM,\nPubMedBERT and SciBERT) without prior training on the specific task. A domain\nfine-tuned model (GSAP-NER) was also applied for a comprehensive comparison.\nResults: The overall performance of LLMs varied significantly in exact match\nand fuzzy match. In the fuzzy match, ChatGPT surpassed BERT-based QA models in\n5 out of 6 tasks, while in exact match, BERT-based QA models outperformed\nChatGPT in 5 out of 6 tasks but with a smaller F-1 difference. GPT-4 showed a\nsignificant advantage over other models in fuzzy match, especially on the\nentity type of TCM formula and the Chinese patent drug (TFD) and ingredient\n(IG). Although GPT-4 outperformed BERT-based models on entity type of herb,\ntarget, and research method, none of the F-1 scores exceeded 0.5. GSAP-NER,\noutperformed GPT-4 in terms of F-1 by a slight margin on RM. ChatGPT achieved\nconsiderably higher recalls than precisions, particularly in the fuzzy match.\nConclusions: The NER performance of LLMs is highly dependent on the entity\ntype, and their performance varies across application scenarios. ChatGPT could\nbe a good choice for scenarios where high recall is favored. However, for\nknowledge acquisition in rigorous scenarios, neither ChatGPT nor BERT-based QA\nmodels are off-the-shelf tools for professional practitioners.", "published": "2024-08-24 06:59:55", "link": "http://arxiv.org/abs/2408.13501v1", "categories": ["cs.CL", "cs.IR", "H.3.3"], "primary_category": "cs.CL"}
{"title": "FLEURS-ASL: Including American Sign Language in Massively Multilingual\n  Multitask Evaluation", "abstract": "Sign language translation has historically been peripheral to mainstream\nmachine translation research. In order to help converge the fields, we\nintroduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES\n(for text) and FLEURS (for speech) to support their first sign language (as\nvideo), American Sign Language, translated by 5 Certified Deaf Interpreters.\nFLEURS-ASL can be used to evaluate a variety of tasks -- primarily sentence-\nand discourse-level translation -- between ASL and 200 other languages as text,\nor 102 languages as speech. We provide baselines for tasks from ASL to English\ntext using a unified modeling approach that incorporates timestamp tokens and\nprevious text tokens in a 34-second context window, trained on random video\nclips from YouTube-ASL. This model meets or exceeds the performance of\nphrase-level baselines while supporting a multitude of new tasks. We also use\nFLEURS-ASL to show that multimodal frontier models have virtually no\nunderstanding of ASL, underscoring the importance of including sign languages\nin standard evaluation suites.", "published": "2024-08-24 13:59:41", "link": "http://arxiv.org/abs/2408.13585v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method\n  and Parameter for Open-Ended Text Generation", "abstract": "Sampling-based decoding strategies have been widely adopted for Large\nLanguage Models (LLMs) in numerous applications, targeting a balance between\ndiversity and quality via temperature tuning and tail truncation. Considering\nthe strong dependency of the candidate next tokens on different prefixes,\nrecent studies propose to adaptively truncate the tail of LLMs' predicted\ndistribution. Although improved results have been reported with these methods\non open-ended text generation tasks, the results are highly dependent on the\ncurated parameters and the limited exemplar text. In this paper, we propose a\nsystematic way to estimate the capacity of a truncation sampling method by\nconsidering the trade-off between diversity and risk at each decoding step,\nbased on our collected prefix tree which preserves the context of a full\nsentence. Our work offers a comprehensive comparison of existing truncation\nsampling methods and serves as a practical user guideline for their parameter\nselection.", "published": "2024-08-24 14:14:32", "link": "http://arxiv.org/abs/2408.13586v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpeechCraft: A Fine-grained Expressive Speech Dataset with Natural\n  Language Description", "abstract": "Speech-language multi-modal learning presents a significant challenge due to\nthe fine nuanced information inherent in speech styles. Therefore, a\nlarge-scale dataset providing elaborate comprehension of speech style is\nurgently needed to facilitate insightful interplay between speech audio and\nnatural language. However, constructing such datasets presents a major\ntrade-off between large-scale data collection and high-quality annotation. To\ntackle this challenge, we propose an automatic speech annotation system for\nexpressiveness interpretation that annotates in-the-wild speech clips with\nexpressive and vivid human language descriptions. Initially, speech audios are\nprocessed by a series of expert classifiers and captioning models to capture\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\nannotation generation. Unlike previous tag/templet-based annotation frameworks\nwith limited information and diversity, our system provides in-depth\nunderstandings of speech style through tailored natural language descriptions,\nthereby enabling accurate and voluminous data generation for large model\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\nexpressive speech dataset. It is distinguished by highly descriptive natural\nlanguage style prompts, containing approximately 2,000 hours of audio data and\nencompassing over two million speech clips. Extensive experiments demonstrate\nthat the proposed dataset significantly boosts speech-language task performance\nin stylist speech synthesis and speech style understanding.", "published": "2024-08-24 15:36:08", "link": "http://arxiv.org/abs/2408.13608v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "No Dataset Needed for Downstream Knowledge Benchmarking: Response\n  Dispersion Inversely Correlates with Accuracy on Domain-specific QA", "abstract": "This research seeks to obviate the need for creating QA datasets and grading\n(chatbot) LLM responses when comparing LLMs' knowledge in specific topic\ndomains. This is done in an entirely end-user centric way without need for\naccess to any inner workings of the LLM, so long as it can be prompted and\ngiven a random seed to create different generations to the same prompt. The\npaper does this by, for a given topic domain, defining the \"response\ndispersion\" of an LLM by repeatedly asking an LLM the same opinion question\nabout that topic domain. Namely, the response dispersion is the count of\nsingular values needed to explain 95% of the variance in the embedding matrix\nof the LLM's responses. It is found that the response dispersion is inversely\ncorrelated with accuracy on relevant QA evaluations (average spearman rank\ncorrelation stronger than -.59). A use-case analysis shows that when comparing\ntwo different LLMs on the same topic domain, comparing their response\ndispersion is a suitable replacement for comparing their QA accuracy between\n74% and 89% of the time, the range depending on certain reasonable\naccuracy-difference tolerances that may be acceptable to an end-user in\nexchange for the labor being saved using response dispersion instead of QA\naccuracy for comparison. Two response embeddings are studied for creating the\nembedding matrix in this study, one is from OpenAI's APIs and one is a novel\nembedding, here named reference sentence similarity embeddings, that can be\ncomputed locally and performs very nearly as well in calculating response\ndispersion. Also in this research, a pre-existing dataset called the IRC-Wiki\nTrivia dataset, originally developed for trivia games, has been re-purposed,\ncurated, and the curation, called IRC-WikiTriviaQA, is made available for the\npurpose of this research.", "published": "2024-08-24 16:35:00", "link": "http://arxiv.org/abs/2408.13624v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Ancient but Digitized: Developing Handwritten Optical Character\n  Recognition for East Syriac Script Through Creating KHAMIS Dataset", "abstract": "Many languages have vast amounts of handwritten texts, such as ancient\nscripts about folktale stories and historical narratives or contemporary\ndocuments and letters. Digitization of those texts has various applications,\nsuch as daily tasks, cultural studies, and historical research. Syriac is an\nancient, endangered, and low-resourced language that has not received the\nattention it requires and deserves. This paper reports on a research project\naimed at developing a optical character recognition (OCR) model based on the\nhandwritten Syriac texts as a starting point to build more digital services for\nthis endangered language. A dataset was created, KHAMIS (inspired by the East\nSyriac poet, Khamis bar Qardahe), which consists of handwritten sentences in\nthe East Syriac script. We used it to fine-tune the Tesseract-OCR engine's\npretrained Syriac model on handwritten data. The data was collected from\nvolunteers capable of reading and writing in the language to create KHAMIS.\nKHAMIS currently consists of 624 handwritten Syriac sentences collected from 31\nuniversity students and one professor, and it will be partially available\nonline and the whole dataset available in the near future for development and\nresearch purposes. As a result, the handwritten OCR model was able to achieve a\ncharacter error rate of 1.097-1.610% and 8.963-10.490% on both training and\nevaluation sets, respectively, and both a character error rate of 18.89-19.71%\nand a word error rate of 62.83-65.42% when evaluated on the test set, which is\ntwice as better than the default Syriac model of Tesseract.", "published": "2024-08-24 17:17:46", "link": "http://arxiv.org/abs/2408.13631v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are LLM-based methods good enough for detecting unfair terms of service?", "abstract": "Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose.", "published": "2024-08-24 09:26:59", "link": "http://arxiv.org/abs/2409.00077v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Law of Next-Token Prediction in Large Language Models", "abstract": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, built on architectures such as Transformer, RWKV,\nand Mamba. We demonstrate that this law offers new perspectives and insights to\ninform and guide practices in LLM development and applications, including model\nscaling, pre-training tasks, and information flow. Overall, our law enables\nmore fine-grained approaches to the design, training, and interpretation of\nLLMs through scrutinizing their internal data processing mechanisms.", "published": "2024-08-24 02:48:40", "link": "http://arxiv.org/abs/2408.13442v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Uncovering Biases with Reflective Large Language Models", "abstract": "Biases and errors in human-labeled data present significant challenges for\nmachine learning, especially in supervised learning reliant on potentially\nflawed ground truth data. These flaws, including diagnostic errors and societal\nbiases, risk being propagated and amplified through models trained using\nmaximum likelihood estimation. We present the Reflective LLM Dialogue Framework\nRLDF, which leverages structured adversarial dialogues between multiple\ninstances of a single LLM or different LLMs to uncover diverse perspectives and\ncorrect inconsistencies. By conditioning LLMs to adopt opposing stances, RLDF\nenables systematic bias detection through conditional statistics, information\ntheory, and divergence metrics. Experiments show RLDF successfully identifies\npotential biases in public content while exposing limitations in human-labeled\ndata. Our framework supports measurable progress tracking and explainable\nremediation actions, offering a scalable approach for improving content\nneutrality through transparent, multi-perspective analysis.", "published": "2024-08-24 04:48:32", "link": "http://arxiv.org/abs/2408.13464v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Selective Preference Optimization via Token-Level Reward Function\n  Estimation", "abstract": "Recent advancements in large language model alignment leverage token-level\nsupervisions to perform fine-grained preference optimization. However, existing\ntoken-level alignment methods either optimize on all available tokens, which\ncan be noisy and inefficient, or perform selective training with complex and\nexpensive key token selection strategies. In this work, we propose Selective\nPreference Optimization (SePO), a novel selective alignment strategy that\ncenters on efficient key token selection. SePO proposes the first token\nselection method based on Direct Preference Optimization (DPO), which trains an\noracle model to estimate a token-level reward function on the target data. This\nmethod applies to any existing alignment datasets with response-level\nannotations and enables cost-efficient token selection with small-scale oracle\nmodels and training data. The estimated reward function is then utilized to\nscore all tokens within the target dataset, where only the key tokens are\nselected to supervise the target policy model with a reference model-free\ncontrastive objective function. Extensive experiments on three public\nevaluation benchmarks show that SePO significantly outperforms competitive\nbaseline methods by only optimizing 30% key tokens on the target dataset. SePO\napplications on weak-to-strong generalization show that weak oracle models\neffectively supervise strong policy models with up to 16.8x more parameters.\nSePO also effectively selects key tokens from out-of-distribution data to\nenhance strong policy models and alleviate the over-optimization problem.", "published": "2024-08-24 08:44:04", "link": "http://arxiv.org/abs/2408.13518v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GNN: Graph Neural Network and Large Language Model for Data Discovery", "abstract": "Our algorithm GNN: Graph Neural Network and Large Language Model for Data\nDiscovery inherit the benefits of \\cite{hoang2024plod} (PLOD: Predictive\nLearning Optimal Data Discovery), \\cite{Hoang2024BODBO} (BOD: Blindly Optimal\nData Discovery) in terms of overcoming the challenges of having to predefine\nutility function and the human input for attribute ranking, which helps prevent\nthe time-consuming loop process. In addition to these previous works, our\nalgorithm GNN leverages the advantages of graph neural networks and large\nlanguage models to understand text type values that cannot be understood by\nPLOD and MOD, thus making the task of predicting outcomes more reliable. GNN\ncould be seen as an extension of PLOD in terms of understanding the text type\nvalue and the user's preferences, not only numerical values but also text\nvalues, making the promise of data science and analytics purposes.", "published": "2024-08-24 15:43:02", "link": "http://arxiv.org/abs/2408.13609v2", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Preliminary Investigations of a Multi-Faceted Robust and Synergistic\n  Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision\n  Transformers with Large Language and Multimodal Models", "abstract": "Characterizing materials using electron micrographs is crucial in areas such\nas semiconductors and quantum materials. Traditional classification methods\nfalter due to the intricatestructures of these micrographs. This study\nintroduces an innovative architecture that leverages the generative\ncapabilities of zero-shot prompting in Large Language Models (LLMs) such as\nGPT-4(language only), the predictive ability of few-shot (in-context) learning\nin Large Multimodal Models (LMMs) such as GPT-4(V)ision, and fuses knowledge\nacross image based and linguistic insights for accurate nanomaterial category\nprediction. This comprehensive approach aims to provide a robust solution for\nthe automated nanomaterial identification task in semiconductor manufacturing,\nblending performance, efficiency, and interpretability. Our method surpasses\nconventional approaches, offering precise nanomaterial identification and\nfacilitating high-throughput screening.", "published": "2024-08-24 16:28:00", "link": "http://arxiv.org/abs/2408.13621v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "abstract": "Model merging offers an effective strategy to combine the strengths of\nmultiple finetuned models into a unified model that preserves the specialized\ncapabilities of each. Existing methods merge models in a global manner,\nperforming arithmetic operations across all model parameters. However, such\nglobal merging often leads to task interference, degrading the performance of\nthe merged model. In this work, we introduce Localize-and-Stitch, a novel\napproach that merges models in a localized way. Our algorithm works in two\nsteps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized\nregions in the finetuned models containing essential skills for the downstream\ntasks, and ii) Stitching: reintegrate only these essential regions back into\nthe pretrained model for task synergy. We demonstrate that our approach\neffectively locates sparse regions responsible for finetuned performance, and\nthe localized regions could be treated as compact and interpretable\nrepresentations of the finetuned models (tasks). Empirically, we evaluate our\nmethod on various vision and language benchmarks, showing that it outperforms\nexisting model merging methods under different data availability scenarios.\nBeyond strong empirical performance, our algorithm also facilitates model\ncompression and preserves pretrained knowledge, enabling flexible and continual\nskill composition from multiple finetuned models with minimal storage and\ncomputational overhead. Our code is available at\nhttps://github.com/uiuctml/Localize-and-Stitch.", "published": "2024-08-24 19:14:02", "link": "http://arxiv.org/abs/2408.13656v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware\n  Reprogramming", "abstract": "Spatio-temporal forecasting is pivotal in numerous real-world applications,\nincluding transportation planning, energy management, and climate monitoring.\nIn this work, we aim to harness the reasoning and generalization abilities of\nPre-trained Language Models (PLMs) for more effective spatio-temporal\nforecasting, particularly in data-scarce scenarios. However, recent studies\nuncover that PLMs, which are primarily trained on textual data, often falter\nwhen tasked with modeling the intricate correlations in numerical time series,\nthereby limiting their effectiveness in comprehending spatio-temporal data. To\nbridge the gap, we propose RePST, a physics-aware PLM reprogramming framework\ntailored for spatio-temporal forecasting. Specifically, we first propose a\nphysics-aware decomposer that adaptively disentangles spatially correlated time\nseries into interpretable sub-components, which facilitates PLM to understand\nsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.\nMoreover, we propose a selective discrete reprogramming scheme, which\nintroduces an expanded spatio-temporal vocabulary space to project\nspatio-temporal series into discrete representations. This scheme minimizes the\ninformation loss during reprogramming and enriches the representations derived\nby PLMs. Extensive experiments on real-world datasets show that the proposed\nRePST outperforms twelve state-of-the-art baseline methods, particularly in\ndata-scarce scenarios, highlighting the effectiveness and superior\ngeneralization capabilities of PLMs for spatio-temporal forecasting.", "published": "2024-08-24 07:59:36", "link": "http://arxiv.org/abs/2408.14505v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Human-Level Understanding of Complex Process Engineering\n  Schematics: A Pedagogical, Introspective Multi-Agent Framework for\n  Open-Domain Question Answering", "abstract": "In the chemical and process industries, Process Flow Diagrams (PFDs) and\nPiping and Instrumentation Diagrams (P&IDs) are critical for design,\nconstruction, and maintenance. Recent advancements in Generative AI, such as\nLarge Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in\nunderstanding and interpreting process diagrams for Visual Question Answering\n(VQA). However, proprietary models pose data privacy risks, and their\ncomputational complexity prevents knowledge editing for domain-specific\ncustomization on consumer hardware. To overcome these challenges, we propose a\nsecure, on-premises enterprise solution using a hierarchical, multi-agent\nRetrieval Augmented Generation (RAG) framework for open-domain question\nanswering (ODQA) tasks, offering enhanced data privacy, explainability, and\ncost-effectiveness. Our novel multi-agent framework employs introspective and\nspecialized sub-agents using open-source, small-scale multimodal models with\nthe ReAct (Reason+Act) prompting technique for PFD and P&ID analysis,\nintegrating multiple information sources to provide accurate and contextually\nrelevant answers. Our approach, supported by iterative self-correction, aims to\ndeliver superior performance in ODQA tasks. We conducted rigorous experimental\nstudies, and the empirical results validated the proposed approach\neffectiveness.", "published": "2024-08-24 19:34:04", "link": "http://arxiv.org/abs/2409.00082v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information\n  Propagation-based Job Recommendation", "abstract": "Knowledge Graphs (KGs) serving as semantic networks, prove highly effective\nin managing complex interconnected data in different domains, by offering a\nunified, contextualized, and structured representation with flexibility that\nallows for easy adaptation to evolving knowledge. Processing complex Human\nResources (HR) data, KGs can help in different HR functions like recruitment,\njob matching, identifying learning gaps, and enhancing employee retention.\nDespite their potential, limited efforts have been made to implement practical\nHR knowledge graphs. This study addresses this gap by presenting a framework\nfor effectively developing HR knowledge graphs from documents using Large\nLanguage Models. The resulting KG can be used for a variety of downstream\ntasks, including job matching, identifying employee skill gaps, and many more.\nIn this work, we showcase instances where HR KGs prove instrumental in precise\njob matching, yielding advantages for both employers and employees. Empirical\nevidence from experiments with information propagation in KGs and Graph Neural\nNets, along with case studies underscores the effectiveness of KGs in tasks\nsuch as job and employee recommendations and job area classification. Code and\ndata are available at : https://github.com/azminewasi/HRGraph", "published": "2024-08-24 08:50:25", "link": "http://arxiv.org/abs/2408.13521v1", "categories": ["cs.CL", "cs.IR", "cs.IT", "cs.SI", "math.IT"], "primary_category": "cs.CL"}
{"title": "StreamAAD: Decoding Spatial Auditory Attention with a Streaming\n  Architecture", "abstract": "In this paper, we present our approach for the Track 1 of the Chinese\nAuditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most\nexisting spatial auditory attention decoding (Sp-AAD) methods employ an\nisolated window architecture, focusing solely on global invariant features\nwithout considering relationships between different decision windows, which can\nlead to suboptimal performance. To address this issue, we propose a novel\nstreaming decoding architecture, termed StreamAAD. In StreamAAD, decision\nwindows are input to the network as a sequential stream and decoded in order,\nallowing for the modeling of inter-window relationships. Additionally, we\nemploy a model ensemble strategy, achieving significant better performance than\nthe baseline, ranking First in the challenge.", "published": "2024-08-24 08:54:26", "link": "http://arxiv.org/abs/2408.13522v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "As Biased as You Measure: Methodological Pitfalls of Bias Evaluations in\n  Speaker Verification Research", "abstract": "Detecting and mitigating bias in speaker verification systems is important,\nas datasets, processing choices and algorithms can lead to performance\ndifferences that systematically favour some groups of people while\ndisadvantaging others. Prior studies have thus measured performance differences\nacross groups to evaluate bias. However, when comparing results across studies,\nit becomes apparent that they draw contradictory conclusions, hindering\nprogress in this area. In this paper we investigate how measurement impacts the\noutcomes of bias evaluations. We show empirically that bias evaluations are\nstrongly influenced by base metrics that measure performance, by the choice of\nratio or difference-based bias measure, and by the aggregation of bias measures\ninto meta-measures. Based on our findings, we recommend the use of ratio-based\nbias measures, in particular when the values of base metrics are small, or when\nbase metrics with different orders of magnitude need to be compared.", "published": "2024-08-24 16:04:51", "link": "http://arxiv.org/abs/2408.13614v1", "categories": ["eess.AS", "cs.CY"], "primary_category": "eess.AS"}
{"title": "Studying the Effect of Audio Filters in Pre-Trained Models for\n  Environmental Sound Classification", "abstract": "Environmental Sound Classification is an important problem of sound\nrecognition and is more complicated than speech recognition problems as\nenvironmental sounds are not well structured with respect to time and\nfrequency. Researchers have used various CNN models to learn audio features\nfrom different audio features like log mel spectrograms, gammatone spectral\ncoefficients, mel-frequency spectral coefficients, generated from the audio\nfiles, over the past years. In this paper, we propose a new methodology :\nTwo-Level Classification; the Level 1 Classifier will be responsible to\nclassify the audio signal into a broader class and the Level 2 Classifiers will\nbe responsible to find the actual class to which the audio belongs, based on\nthe output of the Level 1 Classifier. We have also shown the effects of\ndifferent audio filters, among which a new method of Audio Crop is introduced\nin this paper, which gave the highest accuracies in most of the cases. We have\nused the ESC-50 dataset for our experiment and obtained a maximum accuracy of\n78.75% in case of Level 1 Classification and 98.04% in case of Level 2\nClassifications.", "published": "2024-08-24 18:13:07", "link": "http://arxiv.org/abs/2408.13644v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
