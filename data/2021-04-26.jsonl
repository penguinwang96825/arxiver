{"title": "A Sliding-Window Approach to Automatic Creation of Meeting Minutes", "abstract": "Meeting minutes record any subject matters discussed, decisions reached and\nactions taken at meetings. The importance of minuting cannot be overemphasized\nin a time when a significant number of meetings take place in the virtual\nspace. In this paper, we present a sliding window approach to automatic\ngeneration of meeting minutes. It aims to tackle issues associated with the\nnature of spoken text, including lengthy transcripts and lack of document\nstructure, which make it difficult to identify salient content to be included\nin the meeting minutes. Our approach combines a sliding window and a neural\nabstractive summarizer to navigate through the transcripts to find salient\ncontent. The approach is evaluated on transcripts of natural meeting\nconversations, where we compare results obtained for human transcripts and two\nversions of automatic transcripts and discuss how and to what extent the\nsummarizer succeeds at capturing salient content.", "published": "2021-04-26 02:44:14", "link": "http://arxiv.org/abs/2104.12324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PanGu-$\u03b1$: Large-scale Autoregressive Pretrained Chinese Language\n  Models with Auto-parallel Computation", "abstract": "Large-scale Pretrained Language Models (PLMs) have become the new paradigm\nfor Natural Language Processing (NLP). PLMs with hundreds of billions\nparameters such as GPT-3 have demonstrated strong performances on natural\nlanguage understanding and generation with \\textit{few-shot in-context}\nlearning. In this work, we present our practice on training large-scale\nautoregressive language models named PanGu-$\\alpha$, with up to 200 billion\nparameters. PanGu-$\\alpha$ is developed under the MindSpore and trained on a\ncluster of 2048 Ascend 910 AI processors. The training parallelism strategy is\nimplemented based on MindSpore Auto-parallel, which composes five parallelism\ndimensions to scale the training task to 2048 processors efficiently, including\ndata parallelism, op-level model parallelism, pipeline model parallelism,\noptimizer model parallelism and rematerialization. To enhance the\ngeneralization ability of PanGu-$\\alpha$, we collect 1.1TB high-quality Chinese\ndata from a wide range of domains to pretrain the model. We empirically test\nthe generation ability of PanGu-$\\alpha$ in various scenarios including text\nsummarization, question answering, dialogue generation, etc. Moreover, we\ninvestigate the effect of model scales on the few-shot performances across a\nbroad range of Chinese NLP tasks. The experimental results demonstrate the\nsuperior capabilities of PanGu-$\\alpha$ in performing various tasks under\nfew-shot or zero-shot settings.", "published": "2021-04-26 06:59:36", "link": "http://arxiv.org/abs/2104.12369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty\n  Dialogue Machine Reading Comprehension", "abstract": "Multiparty Dialogue Machine Reading Comprehension (MRC) differs from\ntraditional MRC as models must handle the complex dialogue discourse structure,\npreviously unconsidered in traditional MRC. To fully exploit such discourse\nstructure in multiparty dialogue, we present a discourse-aware dialogue graph\nneural network, DADgraph, which explicitly constructs the dialogue graph using\ndiscourse dependency links and discourse relations. To validate our model, we\nperform experiments on the Molweni corpus, a large-scale MRC dataset built over\nmultiparty dialogue annotated with discourse structure. Experiments on Molweni\nshow that our discourse-aware model achieves statistically significant\nimprovements compared against strong neural network MRC baselines.", "published": "2021-04-26 07:20:37", "link": "http://arxiv.org/abs/2104.12377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A dissemination workshop for introducing young Italian students to NLP", "abstract": "We describe and make available the game-based material developed for a\nlaboratory run at several Italian science festivals to popularize NLP among\nyoung students.", "published": "2021-04-26 09:00:56", "link": "http://arxiv.org/abs/2104.12405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching NLP with Bracelets and Restaurant Menus: An Interactive\n  Workshop for Italian Students", "abstract": "Although Natural Language Processing (NLP) is at the core of many tools young\npeople use in their everyday life, high school curricula (in Italy) do not\ninclude any computational linguistics education. This lack of exposure makes\nthe use of such tools less responsible than it could be and makes choosing\ncomputational linguistics as a university degree unlikely. To raise awareness,\ncuriosity, and longer-term interest in young people, we have developed an\ninteractive workshop designed to illustrate the basic principles of NLP and\ncomputational linguistics to high school Italian students aged between 13 and\n18 years. The workshop takes the form of a game in which participants play the\nrole of machines needing to solve some of the most common problems a computer\nfaces in understanding language: from voice recognition to Markov chains to\nsyntactic parsing. Participants are guided through the workshop with the help\nof instructors, who present the activities and explain core concepts from\ncomputational linguistics. The workshop was presented at numerous outlets in\nItaly between 2019 and 2021, both face-to-face and online.", "published": "2021-04-26 09:23:52", "link": "http://arxiv.org/abs/2104.12422v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention vs non-attention for a Shapley-based explanation method", "abstract": "The field of explainable AI has recently seen an explosion in the number of\nexplanation methods for highly non-linear deep neural networks. The extent to\nwhich such methods -- that are often proposed and tested in the domain of\ncomputer vision -- are appropriate to address the explainability challenges in\nNLP is yet relatively unexplored. In this work, we consider Contextual\nDecomposition (CD) -- a Shapley-based input feature attribution method that has\nbeen shown to work well for recurrent NLP models -- and we test the extent to\nwhich it is useful for models that contain attention operations. To this end,\nwe extend CD to cover the operations necessary for attention-based models. We\nthen compare how long distance subject-verb relationships are processed by\nmodels with and without attention, considering a number of different syntactic\nstructures in two different languages: English and Dutch. Our experiments\nconfirm that CD can successfully be applied for attention-based models as well,\nproviding an alternative Shapley-based attribution method for modern neural\nnetworks. In particular, using CD, we show that the English and Dutch models\ndemonstrate similar processing behaviour, but that under the hood there are\nconsistent differences between our attention and non-attention models.", "published": "2021-04-26 09:33:18", "link": "http://arxiv.org/abs/2104.12424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes a Message Persuasive? Identifying Adaptations Towards\n  Persuasiveness in Nine Exploratory Case Studies", "abstract": "The ability to persuade others is critical to professional and personal\nsuccess. However, crafting persuasive messages is demanding and poses various\nchallenges. We conducted nine exploratory case studies to identify adaptations\nthat professional and non-professional writers make in written scenarios to\nincrease their subjective persuasiveness. Furthermore, we identified challenges\nthat those writers faced and identified strategies to resolve them with\npersuasive natural language generation, i.e., artificial intelligence. Our\nfindings show that humans can achieve high degrees of persuasiveness (more so\nfor professional-level writers), and artificial intelligence can complement\nthem to achieve increased celerity and alignment in the process.", "published": "2021-04-26 10:35:14", "link": "http://arxiv.org/abs/2104.12454v1", "categories": ["cs.CL", "I.2.7; J.4"], "primary_category": "cs.CL"}
{"title": "Easy and Efficient Transformer : Scalable Inference Solution For large\n  NLP model", "abstract": "Recently, large-scale transformer-based models have been proven to be\neffective over various tasks across many domains. Nevertheless, applying them\nin industrial production requires tedious and heavy works to reduce inference\ncosts. To fill such a gap, we introduce a scalable inference solution: Easy and\nEfficient Transformer (EET), including a series of transformer inference\noptimization at the algorithm and implementation levels. First, we design\nhighly optimized kernels for long inputs and large hidden sizes. Second, we\npropose a flexible CUDA memory manager to reduce the memory footprint when\ndeploying a large model. Compared with the state-of-the-art transformer\ninference library (Faster Transformer v4.0), EET can achieve an average of\n1.40-4.20x speedup on the transformer decoder layer with an A100 GPU", "published": "2021-04-26 11:00:56", "link": "http://arxiv.org/abs/2104.12470v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Values of Sources in Transfer Learning", "abstract": "Transfer learning that adapts a model trained on data-rich sources to\nlow-resource targets has been widely applied in natural language processing\n(NLP). However, when training a transfer model over multiple sources, not every\nsource is equally useful for the target. To better transfer a model, it is\nessential to understand the values of the sources. In this paper, we develop\nSEAL-Shap, an efficient source valuation framework for quantifying the\nusefulness of the sources (e.g., domains/languages) in transfer learning based\non the Shapley value method. Experiments and comprehensive analyses on both\ncross-domain and cross-lingual transfers demonstrate that our framework is not\nonly effective in choosing useful transfer sources but also the source values\nmatch the intuitive source-target similarity.", "published": "2021-04-26 13:35:24", "link": "http://arxiv.org/abs/2104.12567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Few-Shot Learning for Word Sense Disambiguation", "abstract": "Word sense disambiguation (WSD) is a long-standing problem in natural\nlanguage processing. One significant challenge in supervised all-words WSD is\nto classify among senses for a majority of words that lie in the long-tail\ndistribution. For instance, 84% of the annotated words have less than 10\nexamples in the SemCor training data. This issue is more pronounced as the\nimbalance occurs in both word and sense distributions. In this work, we propose\nMetricWSD, a non-parametric few-shot learning approach to mitigate this data\nimbalance issue. By learning to compute distances among the senses of a given\nword through episodic training, MetricWSD transfers knowledge (a learned metric\nspace) from high-frequency words to infrequent ones. MetricWSD constructs the\ntraining episodes tailored to word frequencies and explicitly addresses the\nproblem of the skewed distribution, as opposed to mixing all the words trained\nwith parametric models in previous work. Without resorting to any lexical\nresources, MetricWSD obtains strong performance against parametric\nalternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark\n(Raganato et al., 2017b). Our analysis further validates that infrequent words\nand senses enjoy significant improvement.", "published": "2021-04-26 16:08:46", "link": "http://arxiv.org/abs/2104.12677v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Focused Attention Improves Document-Grounded Generation", "abstract": "Document grounded generation is the task of using the information provided in\na document to improve text generation. This work focuses on two different\ndocument grounded generation tasks: Wikipedia Update Generation task and\nDialogue response generation. Our work introduces two novel adaptations of\nlarge scale pre-trained encoder-decoder models focusing on building context\ndriven representation of the document and enabling specific attention to the\ninformation in the document. Additionally, we provide a stronger BART baseline\nfor these tasks. Our proposed techniques outperform existing methods on both\nautomated (at least 48% increase in BLEU-4 points) and human evaluation for\ncloseness to reference and relevance to the document. Furthermore, we perform\ncomprehensive manual inspection of the generated output and categorize errors\nto provide insights into future directions in modeling these tasks.", "published": "2021-04-26 16:56:29", "link": "http://arxiv.org/abs/2104.12714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching a Massive Open Online Course on Natural Language Processing", "abstract": "This paper presents a new Massive Open Online Course on Natural Language\nProcessing, targeted at non-English speaking students. The course lasts 12\nweeks; every week consists of lectures, practical sessions, and quiz\nassignments. Three weeks out of 12 are followed by Kaggle-style coding\nassignments.\n  Our course intends to serve multiple purposes: (i) familiarize students with\nthe core concepts and methods in NLP, such as language modeling or word or\nsentence representations, (ii) show that recent advances, including pre-trained\nTransformer-based models, are built upon these concepts; (iii) introduce\narchitectures for most demanded real-life applications, (iv) develop practical\nskills to process texts in multiple languages. The course was prepared and\nrecorded during 2020, launched by the end of the year, and in early 2021 has\nreceived positive feedback.", "published": "2021-04-26 19:52:00", "link": "http://arxiv.org/abs/2104.12846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morph Call: Probing Morphosyntactic Content of Multilingual Transformers", "abstract": "The outstanding performance of transformer-based language models on a great\nvariety of NLP and NLU tasks has stimulated interest in exploring their inner\nworkings. Recent research has focused primarily on higher-level and complex\nlinguistic phenomena such as syntax, semantics, world knowledge, and common\nsense. The majority of the studies are anglocentric, and little remains known\nregarding other languages, precisely their morphosyntactic properties. To this\nend, our work presents Morph Call, a suite of 46 probing tasks for four\nIndo-European languages of different morphology: English, French, German and\nRussian. We propose a new type of probing task based on the detection of guided\nsentence perturbations. We use a combination of neuron-, layer- and\nrepresentation-level introspection techniques to analyze the morphosyntactic\ncontent of four multilingual transformers, including their less explored\ndistilled versions. Besides, we examine how fine-tuning for POS-tagging affects\nthe model knowledge. The results show that fine-tuning can improve and decrease\nthe probing performance and change how morphosyntactic knowledge is distributed\nacross the model. The code and data are publicly available, and we hope to fill\nthe gaps in the less studied aspect of transformers.", "published": "2021-04-26 19:53:00", "link": "http://arxiv.org/abs/2104.12847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accounting for Agreement Phenomena in Sentence Comprehension with\n  Transformer Language Models: Effects of Similarity-based Interference on\n  Surprisal and Attention", "abstract": "We advance a novel explanation of similarity-based interference effects in\nsubject-verb and reflexive pronoun agreement processing, grounded in surprisal\nvalues computed from a pretrained large-scale Transformer model, GPT-2.\nSpecifically, we show that surprisal of the verb or reflexive pronoun predicts\nfacilitatory interference effects in ungrammatical sentences, where a\ndistractor noun that matches in number with the verb or pronoun leads to faster\nreading times, despite the distractor not participating in the agreement\nrelation. We review the human empirical evidence for such effects, including\nrecent meta-analyses and large-scale studies. We also show that attention\npatterns (indexed by entropy and other measures) in the Transformer show\npatterns of diffuse attention in the presence of similar distractors,\nconsistent with cue-based retrieval models of parsing. But in contrast to these\nmodels, the attentional cues and memory representations are learned entirely\nfrom the simple self-supervised task of predicting the next word.", "published": "2021-04-26 20:46:54", "link": "http://arxiv.org/abs/2104.12874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Bayesian Deep Learning for Urgent Instructor Intervention Need\n  in MOOC Forums", "abstract": "Massive Open Online Courses (MOOCs) have become a popular choice for\ne-learning thanks to their great flexibility. However, due to large numbers of\nlearners and their diverse backgrounds, it is taxing to offer real-time\nsupport. Learners may post their feelings of confusion and struggle in the\nrespective MOOC forums, but with the large volume of posts and high workloads\nfor MOOC instructors, it is unlikely that the instructors can identify all\nlearners requiring intervention. This problem has been studied as a Natural\nLanguage Processing (NLP) problem recently, and is known to be challenging, due\nto the imbalance of the data and the complex nature of the task. In this paper,\nwe explore for the first time Bayesian deep learning on learner-based text\nposts with two methods: Monte Carlo Dropout and Variational Inference, as a new\nsolution to assessing the need of instructor interventions for a learner's\npost. We compare models based on our proposed methods with probabilistic\nmodelling to its baseline non-Bayesian models under similar circumstances, for\ndifferent cases of applying prediction. The results suggest that Bayesian deep\nlearning offers a critical uncertainty measure that is not supplied by\ntraditional neural networks. This adds more explainability, trust and\nrobustness to AI, which is crucial in education-based applications.\nAdditionally, it can achieve similar or better performance compared to\nnon-probabilistic neural networks, as well as grant lower variance.", "published": "2021-04-26 15:12:13", "link": "http://arxiv.org/abs/2104.12643v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GermanQuAD and GermanDPR: Improving Non-English Question Answering and\n  Passage Retrieval", "abstract": "A major challenge of research on non-English machine reading for question\nanswering (QA) is the lack of annotated datasets. In this paper, we present\nGermanQuAD, a dataset of 13,722 extractive question/answer pairs. To improve\nthe reproducibility of the dataset creation approach and foster QA research on\nother languages, we summarize lessons learned and evaluate reformulation of\nquestion/answer pairs as a way to speed up the annotation process. An\nextractive QA model trained on GermanQuAD significantly outperforms\nmultilingual models and also shows that machine-translated training data cannot\nfully substitute hand-annotated training data in the target language. Finally,\nwe demonstrate the wide range of applications of GermanQuAD by adapting it to\nGermanDPR, a training dataset for dense passage retrieval (DPR), and train and\nevaluate the first non-English DPR model.", "published": "2021-04-26 17:34:31", "link": "http://arxiv.org/abs/2104.12741v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfographicVQA", "abstract": "Infographics are documents designed to effectively communicate information\nusing a combination of textual, graphical and visual elements. In this work, we\nexplore the automatic understanding of infographic images by using Visual\nQuestion Answering technique.To this end, we present InfographicVQA, a new\ndataset that comprises a diverse collection of infographics along with natural\nlanguage questions and answers annotations. The collected questions require\nmethods to jointly reason over the document layout, textual content, graphical\nelements, and data visualizations. We curate the dataset with emphasis on\nquestions that require elementary reasoning and basic arithmetic skills.\nFinally, we evaluate two strong baselines based on state of the art multi-modal\nVQA models, and establish baseline performance for the new task. The dataset,\ncode and leaderboard will be made available at http://docvqa.org", "published": "2021-04-26 17:45:54", "link": "http://arxiv.org/abs/2104.12756v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Explore BiLSTM-CRF-Based Models for Open Relation Extraction", "abstract": "Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.", "published": "2021-04-26 03:37:22", "link": "http://arxiv.org/abs/2104.12333v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phrase break prediction with bidirectional encoder representations in\n  Japanese text-to-speech synthesis", "abstract": "We propose a novel phrase break prediction method that combines implicit\nfeatures extracted from a pre-trained large language model, a.k.a BERT, and\nexplicit features extracted from BiLSTM with linguistic features. In\nconventional BiLSTM based methods, word representations and/or sentence\nrepresentations are used as independent components. The proposed method takes\naccount of both representations to extract the latent semantics, which cannot\nbe captured by previous methods. The objective evaluation results show that the\nproposed method obtains an absolute improvement of 3.2 points for the F1 score\ncompared with BiLSTM-based conventional methods using linguistic features.\nMoreover, the perceptual listening test results verify that a TTS system that\napplied our proposed method achieved a mean opinion score of 4.39 in prosody\nnaturalness, which is highly competitive with the score of 4.37 for synthesized\nspeech with ground-truth phrase breaks.", "published": "2021-04-26 08:29:29", "link": "http://arxiv.org/abs/2104.12395v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video\n  Summarization", "abstract": "Traditional video summarization methods generate fixed video representations\nregardless of user interest. Therefore such methods limit users' expectations\nin content search and exploration scenarios. Multi-modal video summarization is\none of the methods utilized to address this problem. When multi-modal video\nsummarization is used to help video exploration, a text-based query is\nconsidered as one of the main drivers of video summary generation, as it is\nuser-defined. Thus, encoding the text-based query and the video effectively are\nboth important for the task of multi-modal video summarization. In this work, a\nnew method is proposed that uses a specialized attention network and\ncontextualized word representations to tackle this task. The proposed model\nconsists of a contextualized video summary controller, multi-modal attention\nmechanisms, an interactive attention network, and a video summary generator.\nBased on the evaluation of the existing multi-modal video summarization\nbenchmark, experimental results show that the proposed model is effective with\nthe increase of +5.88% in accuracy and +4.06% increase of F1-score, compared\nwith the state-of-the-art method.", "published": "2021-04-26 10:50:37", "link": "http://arxiv.org/abs/2104.12465v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Auto Response Generation in Online Medical Chat Services", "abstract": "Telehealth helps to facilitate access to medical professionals by enabling\nremote medical services for the patients. These services have become gradually\npopular over the years with the advent of necessary technological\ninfrastructure. The benefits of telehealth have been even more apparent since\nthe beginning of the COVID-19 crisis, as people have become less inclined to\nvisit doctors in person during the pandemic. In this paper, we focus on\nfacilitating the chat sessions between a doctor and a patient. We note that the\nquality and efficiency of the chat experience can be critical as the demand for\ntelehealth services increases. Accordingly, we develop a smart auto-response\ngeneration mechanism for medical conversations that helps doctors respond to\nconsultation requests efficiently, particularly during busy sessions. We\nexplore over 900,000 anonymous, historical online messages between doctors and\npatients collected over nine months. We implement clustering algorithms to\nidentify the most frequent responses by doctors and manually label the data\naccordingly. We then train machine learning algorithms using this preprocessed\ndata to generate the responses. The considered algorithm has two steps: a\nfiltering (i.e., triggering) model to filter out infeasible patient messages\nand a response generator to suggest the top-3 doctor responses for the ones\nthat successfully pass the triggering phase. The method provides an accuracy of\n83.28\\% for precision@3 and shows robustness to its parameters.", "published": "2021-04-26 17:45:10", "link": "http://arxiv.org/abs/2104.12755v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "abstract": "Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.", "published": "2021-04-26 17:55:33", "link": "http://arxiv.org/abs/2104.12763v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semantic Analysis for Automated Evaluation of the Potential Impact of\n  Research Articles", "abstract": "Can the analysis of the semantics of words used in the text of a scientific\npaper predict its future impact measured by citations? This study details\nexamples of automated text classification that achieved 80% success rate in\ndistinguishing between highly-cited and little-cited articles. Automated\nintelligent systems allow the identification of promising works that could\nbecome influential in the scientific community.\n  The problems of quantifying the meaning of texts and representation of human\nlanguage have been clear since the inception of Natural Language Processing.\nThis paper presents a novel method for vector representation of text meaning\nbased on information theory and show how this informational semantics is used\nfor text classification on the basis of the Leicester Scientific Corpus.\n  We describe the experimental framework used to evaluate the impact of\nscientific articles through their informational semantics. Our interest is in\ncitation classification to discover how important semantics of texts are in\npredicting the citation count. We propose the semantics of texts as an\nimportant factor for citation prediction.\n  For each article, our system extracts the abstract of paper, represents the\nwords of the abstract as vectors in Meaning Space, automatically analyses the\ndistribution of scientific categories (Web of Science categories) within the\ntext of abstract, and then classifies papers according to citation counts\n(highly-cited, little-cited).\n  We show that an informational approach to representing the meaning of a text\nhas offered a way to effectively predict the scientific impact of research\npapers.", "published": "2021-04-26 20:37:13", "link": "http://arxiv.org/abs/2104.12869v1", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence\n  with Deletion Prediction", "abstract": "Confidence scores are very useful for downstream applications of automatic\nspeech recognition (ASR) systems. Recent works have proposed using neural\nnetworks to learn word or utterance confidence scores for end-to-end ASR. In\nthose studies, word confidence by itself does not model deletions, and\nutterance confidence does not take advantage of word-level training signals.\nThis paper proposes to jointly learn word confidence, word deletion, and\nutterance confidence. Empirical results show that multi-task learning with all\nthree objectives improves confidence metrics (NCE, AUC, RMSE) without the need\nfor increasing the model size of the confidence estimation module. Using the\nutterance-level confidence for rescoring also decreases the word error rates on\nGoogle's Voice Search and Long-tail Maps datasets by 3-5% relative, without\nneeding a dedicated neural rescorer.", "published": "2021-04-26 20:38:42", "link": "http://arxiv.org/abs/2104.12870v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Code Generation: A Survey and Lessons from Semantic Parsing", "abstract": "With the growth of natural language processing techniques and demand for\nimproved software engineering efficiency, there is an emerging interest in\ntranslating intention from human languages to programming languages. In this\nsurvey paper, we attempt to provide an overview of the growing body of research\nin this space. We begin by reviewing natural language semantic parsing\ntechniques and draw parallels with program synthesis efforts. We then consider\nsemantic parsing works from an evolutionary perspective, with specific analyses\non neuro-symbolic methods, architecture, and supervision. We then analyze\nadvancements in frameworks for semantic parsing for code generation. In\nclosing, we present what we believe are some of the emerging open challenges in\nthis domain.", "published": "2021-04-26 22:05:22", "link": "http://arxiv.org/abs/2105.03317v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Yes, BM25 is a Strong Baseline for Legal Case Retrieval", "abstract": "We describe our single submission to task 1 of COLIEE 2021. Our vanilla BM25\ngot second place, well above the median of submissions. Code is available at\nhttps://github.com/neuralmind-ai/coliee.", "published": "2021-04-26 18:33:38", "link": "http://arxiv.org/abs/2105.05686v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "abstract": "Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.", "published": "2021-04-26 11:08:13", "link": "http://arxiv.org/abs/2104.12471v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Semantic Data Augmentation for End-to-End Mandarin Speech Recognition", "abstract": "End-to-end models have gradually become the preferred option for automatic\nspeech recognition (ASR) applications. During the training of end-to-end ASR,\ndata augmentation is a quite effective technique for regularizing the neural\nnetworks. This paper proposes a novel data augmentation technique based on\nsemantic transposition of the transcriptions via syntax rules for end-to-end\nMandarin ASR. Specifically, we first segment the transcriptions based on\npart-of-speech tags. Then transposition strategies, such as placing the object\nin front of the subject or swapping the subject and the object, are applied on\nthe segmented sentences. Finally, the acoustic features corresponding to the\ntransposed transcription are reassembled based on the audio-to-text\nforced-alignment produced by a pre-trained ASR system. The combination of\noriginal data and augmented one is used for training a new ASR system. The\nexperiments are conducted on the Transformer[2] and Conformer[3] based ASR. The\nresults show that the proposed method can give consistent performance gain to\nthe system. Augmentation related issues, such as comparison of different\nstrategies and ratios for data combination are also investigated.", "published": "2021-04-26 12:43:46", "link": "http://arxiv.org/abs/2104.12521v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Head-synchronous Decoding for Transformer-based Streaming ASR", "abstract": "Online Transformer-based automatic speech recognition (ASR) systems have been\nextensively studied due to the increasing demand for streaming applications.\nRecently proposed Decoder-end Adaptive Computation Steps (DACS) algorithm for\nonline Transformer ASR was shown to achieve state-of-the-art performance and\noutperform other existing methods. However, like any other online approach, the\nDACS-based attention heads in each of the Transformer decoder layers operate\nindependently (or asynchronously) and lead to diverged attending positions.\nSince DACS employs a truncation threshold to determine the halting position,\nsome of the attention weights are cut off untimely and might impact the\nstability and precision of decoding. To overcome these issues, here we propose\na head-synchronous (HS) version of the DACS algorithm, where the boundary of\nattention is jointly detected by all the DACS heads in each decoder layer. ASR\nexperiments on Wall Street Journal (WSJ), AIShell-1 and Librispeech show that\nthe proposed method consistently outperforms vanilla DACS and achieves\nstate-of-the-art performance. We will also demonstrate that HS-DACS has reduced\ndecoding cost when compared to vanilla DACS.", "published": "2021-04-26 14:57:57", "link": "http://arxiv.org/abs/2104.12631v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Complex Neural Spatial Filter: Enhancing Multi-channel Target Speech\n  Separation in Complex Domain", "abstract": "To date, mainstream target speech separation (TSS) approaches are formulated\nto estimate the complex ratio mask (cRM) of the target speech in time-frequency\ndomain under supervised deep learning framework. However, the existing deep\nmodels for estimating cRM are designed in the way that the real and imaginary\nparts of the cRM are separately modeled using real-valued training data pairs.\nThe research motivation of this study is to design a deep model that fully\nexploits the temporal-spectral-spatial information of multi-channel signals for\nestimating cRM directly and efficiently in complex domain. As a result, a novel\nTSS network is designed consisting of two modules, a complex neural spatial\nfilter (cNSF) and an MVDR. Essentially, cNSF is a cRM estimation model and an\nMVDR module is cascaded to the cNSF module to reduce the nonlinear speech\ndistortions introduced by neural network. Specifically, to fit the cRM target,\nall input features of cNSF are reformulated into complex-valued representations\nfollowing the supervised learning paradigm. Then, to achieve good hierarchical\nfeature abstraction, a complex deep neural network (cDNN) is delicately\ndesigned with U-Net structure. Experiments conducted on simulated multi-channel\nspeech data demonstrate the proposed cNSF outperforms the baseline NSF by 12.1%\nscale-invariant signal-to-distortion ratio and 33.1% word error rate.", "published": "2021-04-26 06:04:27", "link": "http://arxiv.org/abs/2104.12359v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identifying Actions for Sound Event Classification", "abstract": "In Psychology, actions are paramount for humans to identify sound events. In\nMachine Learning (ML), action recognition achieves high accuracy; however, it\nhas not been asked whether identifying actions can benefit Sound Event\nClassification (SEC), as opposed to mapping the audio directly to a sound\nevent. Therefore, we propose a new Psychology-inspired approach for SEC that\nincludes identification of actions via human listeners. To achieve this goal,\nwe used crowdsourcing to have listeners identify 20 actions that in isolation\nor in combination may have produced any of the 50 sound events in the\nwell-studied dataset ESC-50. The resulting annotations for each audio recording\nrelate actions to a database of sound events for the first time. The\nannotations were used to create semantic representations called Action Vectors\n(AVs). We evaluated SEC by comparing the AVs with two types of audio features\n-- log-mel spectrograms and state-of-the-art audio embeddings. Because audio\nfeatures and AVs capture different abstractions of the acoustic content, we\ncombined them and achieved one of the highest reported accuracies (88%).", "published": "2021-04-26 16:36:44", "link": "http://arxiv.org/abs/2104.12693v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Self-Supervised Learning of General Audio Representations", "abstract": "We present a multimodal framework to learn general audio representations from\nvideos. Existing contrastive audio representation learning methods mainly focus\non using the audio modality alone during training. In this work, we show that\nadditional information contained in video can be utilized to greatly improve\nthe learned features. First, we demonstrate that our contrastive framework does\nnot require high resolution images to learn good audio features. This allows us\nto scale up the training batch size, while keeping the computational load\nincurred by the additional video modality to a reasonable level. Second, we use\naugmentations that mix together different samples. We show that this is\neffective to make the proxy task harder, which leads to substantial performance\nimprovements when increasing the batch size. As a result, our audio model\nachieves a state-of-the-art of 42.4 mAP on the AudioSet classification\ndownstream task, closing the gap between supervised and self-supervised methods\ntrained on the same dataset. Moreover, we show that our method is advantageous\non a broad range of non-semantic audio tasks, including speaker identification,\nkeyword spotting, language identification, and music instrument classification.", "published": "2021-04-26 18:07:17", "link": "http://arxiv.org/abs/2104.12807v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generation of musical patterns through operads", "abstract": "We introduce the notion of multi-pattern, a combinatorial abstraction of\npolyphonic musical phrases. The interest of this approach lies in the fact that\nthis offers a way to compose two multi-patterns in order to produce a longer\none. This dives musical phrases into an algebraic context since the set of\nmulti-patterns has the structure of an operad; operads being structures\noffering a formalization of the notion of operators and their compositions.\nSeeing musical phrases as operators allows us to perform computations on\nphrases and admits applications in generative music: given a set of short\npatterns, we propose various algorithms to randomly generate a new and longer\nphrase inspired by the inputted patterns.", "published": "2021-04-26 09:45:09", "link": "http://arxiv.org/abs/2104.12432v1", "categories": ["cs.SD", "eess.AS", "math.CO"], "primary_category": "cs.SD"}
{"title": "Points2Sound: From mono to binaural audio using 3D point cloud scenes", "abstract": "For immersive applications, the generation of binaural sound that matches its\nvisual counterpart is crucial to bring meaningful experiences to people in a\nvirtual environment. Recent studies have shown the possibility of using neural\nnetworks for synthesizing binaural audio from mono audio by using 2D visual\ninformation as guidance. Extending this approach by guiding the audio with 3D\nvisual information and operating in the waveform domain may allow for a more\naccurate auralization of a virtual audio scene. We propose Points2Sound, a\nmulti-modal deep learning model which generates a binaural version from mono\naudio using 3D point cloud scenes. Specifically, Points2Sound consists of a\nvision network and an audio network. The vision network uses 3D sparse\nconvolutions to extract a visual feature from the point cloud scene. Then, the\nvisual feature conditions the audio network, which operates in the waveform\ndomain, to synthesize the binaural version. Results show that 3D visual\ninformation can successfully guide multi-modal deep learning models for the\ntask of binaural synthesis. We also investigate how 3D point cloud attributes,\nlearning objectives, different reverberant conditions, and several types of\nmono mixture signals affect the binaural audio synthesis performance of\nPoints2Sound for the different numbers of sound sources present in the scene.", "published": "2021-04-26 10:44:01", "link": "http://arxiv.org/abs/2104.12462v3", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
