{"title": "Personalized Dialogue Generation with Diversified Traits", "abstract": "Endowing a dialogue system with particular personality traits is essential to\ndeliver more human-like conversations. However, due to the challenge of\nembodying personality via language expression and the lack of large-scale\npersona-labeled dialogue data, this research problem is still far from\nwell-studied. In this paper, we investigate the problem of incorporating\nexplicit personality traits in dialogue generation to deliver personalized\ndialogues.\n  To this end, firstly, we construct PersonalDialog, a large-scale multi-turn\ndialogue dataset containing various traits from a large number of speakers. The\ndataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.\nEach utterance is associated with a speaker who is marked with traits like Age,\nGender, Location, Interest Tags, etc. Several anonymization schemes are\ndesigned to protect the privacy of each speaker. This large-scale dataset will\nfacilitate not only the study of personalized dialogue generation, but also\nother researches on sociolinguistics or social science.\n  Secondly, to study how personality traits can be captured and addressed in\ndialogue generation, we propose persona-aware dialogue generation models within\nthe sequence to sequence learning framework. Explicit personality traits\n(structured by key-value pairs) are embedded using a trait fusion module.\nDuring the decoding process, two techniques, namely persona-aware attention and\npersona-aware bias, are devised to capture and address trait-related\ninformation. Experiments demonstrate that our model is able to address proper\ntraits in different contexts. Case studies also show interesting results for\nthis challenging research problem.", "published": "2019-01-28 14:22:15", "link": "http://arxiv.org/abs/1901.09672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Independent Sequence Labelling for Opinion Target Extraction", "abstract": "In this research note we present a language independent system to model\nOpinion Target Extraction (OTE) as a sequence labelling task. The system\nconsists of a combination of clustering features implemented on top of a simple\nset of shallow local features. Experiments on the well known Aspect Based\nSentiment Analysis (ABSA) benchmarks show that our approach is very competitive\nacross languages, obtaining best results for six languages in seven different\ndatasets. Furthermore, the results provide further insights into the behaviour\nof clustering features for sequence labelling tasks. The system and models\ngenerated in this work are available for public use and to facilitate\nreproducibility of results.", "published": "2019-01-28 15:55:22", "link": "http://arxiv.org/abs/1901.09755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Word Embedding Models: Methods and Experimental Results", "abstract": "Extensive evaluation on a large number of word embedding models for language\nprocessing applications is conducted in this work. First, we introduce popular\nword embedding models and discuss desired properties of word models and\nevaluation methods (or evaluators). Then, we categorize evaluators into\nintrinsic and extrinsic two types. Intrinsic evaluators test the quality of a\nrepresentation independent of specific natural language processing tasks while\nextrinsic evaluators use word embeddings as input features to a downstream task\nand measure changes in performance metrics specific to that task. We report\nexperimental results of intrinsic and extrinsic evaluators on six word\nembedding models. It is shown that different evaluators focus on different\naspects of word models, and some are more correlated with natural language\nprocessing tasks. Finally, we adopt correlation analysis to study performance\nconsistency of extrinsic and intrinsic evalutors.", "published": "2019-01-28 16:33:25", "link": "http://arxiv.org/abs/1901.09785v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenHowNet: An Open Sememe-based Lexical Knowledge Base", "abstract": "In this paper, we present an open sememe-based lexical knowledge base\nOpenHowNet. Based on well-known HowNet, OpenHowNet comprises three components:\ncore data which is composed of more than 100 thousand senses annotated with\nsememes, OpenHowNet Web which gives a brief introduction to OpenHowNet as well\nas provides online exhibition of OpenHowNet information, and OpenHowNet API\nwhich includes several useful APIs such as accessing OpenHowNet core data and\ndrawing sememe tree structures of senses. In the main text, we first give some\nbackgrounds including definition of sememe and details of HowNet. And then we\nintroduce some previous HowNet and sememe-based research works. Last but not\nleast, we detail the constituents of OpenHowNet and their basic features and\nfunctionalities. Additionally, we briefly make a summary and list some future\nworks.", "published": "2019-01-28 19:35:17", "link": "http://arxiv.org/abs/1901.09957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Related Work Summarization with a Joint Context-driven Attention\n  Mechanism", "abstract": "Conventional solutions to automatic related work summarization rely heavily\non human-engineered features. In this paper, we develop a neural data-driven\nsummarizer by leveraging the seq2seq paradigm, in which a joint context-driven\nattention mechanism is proposed to measure the contextual relevance within full\ntexts and a heterogeneous bibliography graph simultaneously. Our motivation is\nto maintain the topic coherency between a related work section and its target\ndocument, where both the textual and graphic contexts play a big role in\ncharacterizing the relationship among scientific publications accurately.\nExperimental results on a large dataset show that our approach achieves a\nconsiderable improvement over a typical seq2seq summarizer and five classical\nsummarization baselines.", "published": "2019-01-28 03:00:06", "link": "http://arxiv.org/abs/1901.09492v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Dise\u00f1o de un espacio sem\u00e1ntico sobre la base de la Wikipedia. Una\n  propuesta de an\u00e1lisis de la sem\u00e1ntica latente para el idioma espa\u00f1ol", "abstract": "Latent Semantic Analysis (LSA) was initially conceived by the cognitive\npsychology at the 90s decade. Since its emergence, the LSA has been used to\nmodel cognitive processes, pointing out academic texts, compare literature\nworks and analyse political speeches, among other applications. Taking as\nstarting point multivariate method for dimensionality reduction, this paper\npropose a semantic space for Spanish language. Out results include a document\ntext matrix with dimensions 1.3 x10^6 and 5.9x10^6, which later is decomposed\ninto singular values. Those singular values are used to semantically words or\ntext.", "published": "2019-01-28 21:39:23", "link": "http://arxiv.org/abs/1902.02173v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Data-to-Text Generation with Style Imitation", "abstract": "Recent neural approaches to data-to-text generation have mostly focused on\nimproving content fidelity while lacking explicit control over writing styles\n(e.g., word choices, sentence structures). More traditional systems use\ntemplates to determine the realization of text. Yet manual or automatic\nconstruction of high-quality templates is difficult, and a template acting as\nhard constraints could harm content fidelity when it does not match the record\nperfectly. We study a new way of stylistic control by using existing sentences\nas soft templates. That is, the model learns to imitate the writing style of\nany given exemplar sentence, with automatic adaptions to faithfully describe\nthe content record. The problem is challenging due to the lack of parallel\ndata. We develop a neural approach that includes a hybrid attention-copy\nmechanism, learns with weak supervisions, and is enhanced with a new content\ncoverage constraint. We conduct experiments in restaurants and sports domains.\nResults show our approach achieves stronger performance than a range of\ncomparison methods. Our approach balances well between content fidelity and\nstyle control given exemplars that match the records to varying degrees.", "published": "2019-01-28 03:38:08", "link": "http://arxiv.org/abs/1901.09501v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analogies Explained: Towards Understanding Word Embeddings", "abstract": "Word embeddings generated by neural network methods such as word2vec (W2V)\nare well known to exhibit seemingly linear behaviour, e.g. the embeddings of\nanalogy \"woman is to queen as man is to king\" approximately describe a\nparallelogram. This property is particularly intriguing since the embeddings\nare not trained to achieve it. Several explanations have been proposed, but\neach introduces assumptions that do not hold in practice. We derive a\nprobabilistically grounded definition of paraphrasing that we re-interpret as\nword transformation, a mathematical description of \"$w_x$ is to $w_y$\". From\nthese concepts we prove existence of linear relationships between W2V-type\nembeddings that underlie the analogical phenomenon, identifying explicit error\nterms.", "published": "2019-01-28 17:04:25", "link": "http://arxiv.org/abs/1901.09813v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Squeezed Very Deep Convolutional Neural Networks for Text Classification", "abstract": "Most of the research in convolutional neural networks has focused on\nincreasing network depth to improve accuracy, resulting in a massive number of\nparameters which restricts the trained network to platforms with memory and\nprocessing constraints. We propose to modify the structure of the Very Deep\nConvolutional Neural Networks (VDCNN) model to fit mobile platforms constraints\nand keep performance. In this paper, we evaluate the impact of Temporal\nDepthwise Separable Convolutions and Global Average Pooling in the network\nparameters, storage size, and latency. The squeezed model (SVDCNN) is between\n10x and 20x smaller, depending on the network depth, maintaining a maximum size\nof 6MB. Regarding accuracy, the network experiences a loss between 0.4% and\n1.3% and obtains lower latencies compared to the baseline model.", "published": "2019-01-28 17:14:12", "link": "http://arxiv.org/abs/1901.09821v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A new evaluation framework for topic modeling algorithms based on\n  synthetic corpora", "abstract": "Topic models are in widespread use in natural language processing and beyond.\nHere, we propose a new framework for the evaluation of probabilistic topic\nmodeling algorithms based on synthetic corpora containing an unambiguously\ndefined ground truth topic structure. The major innovation of our approach is\nthe ability to quantify the agreement between the planted and inferred topic\nstructures by comparing the assigned topic labels at the level of the tokens.\nIn experiments, our approach yields novel insights about the relative strengths\nof topic models as corpus characteristics vary, and the first evidence of an\n\"undetectable phase\" for topic models when the planted structure is weak. We\nalso establish the practical relevance of the insights gained for synthetic\ncorpora by predicting the performance of topic modeling algorithms in\nclassification tasks in real-world corpora.", "published": "2019-01-28 17:41:19", "link": "http://arxiv.org/abs/1901.09848v1", "categories": ["cs.CL", "cs.LG", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "How is Your Mood When Writing Sexist tweets? Detecting the Emotion Type\n  and Intensity of Emotion Using Natural Language Processing Techniques", "abstract": "Online social platforms have been the battlefield of users with different\nemotions and attitudes toward each other in recent years. While sexism has been\nconsidered as a category of hateful speech in the literature, there is no\ncomprehensive definition and category of sexism attracting natural language\nprocessing techniques. Categorizing sexism as either benevolent or hostile\nsexism is so broad that it easily ignores the other categories of sexism on\nsocial media. Sharifirad S and Matwin S 2018 proposed a well-defined category\nof sexism including indirect harassment, information threat, sexual harassment\nand physical harassment, inspired from social science for the purpose of\nnatural language processing techniques. In this article, we take advantage of a\nnewly released dataset in SemEval-2018 task1: Affect in tweets, to show the\ntype of emotion and intensity of emotion in each category. We train, test and\nevaluate different classification methods on the SemEval- 2018 dataset and\nchoose the classifier with highest accuracy for testing on each category of\nsexist tweets to know the mental state and the affectual state of the user who\ntweets in each category. It is a nice avenue to explore because not all the\ntweets are directly sexist and they carry different emotions from the users.\nThis is the first work experimenting on affect detection this in depth on\nsexist tweets. Based on our best knowledge they are all new contributions to\nthe field; we are the first to demonstrate the power of such in-depth sentiment\nanalysis on the sexist tweets.", "published": "2019-01-28 19:58:14", "link": "http://arxiv.org/abs/1902.03089v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Additive Margin SincNet for Speaker Recognition", "abstract": "Speaker Recognition is a challenging task with essential applications such as\nauthentication, automation, and security. The SincNet is a new deep learning\nbased model which has produced promising results to tackle the mentioned task.\nTo train deep learning systems, the loss function is essential to the network\nperformance. The Softmax loss function is a widely used function in deep\nlearning methods, but it is not the best choice for all kind of problems. For\ndistance-based problems, one new Softmax based loss function called Additive\nMargin Softmax (AM-Softmax) is proving to be a better choice than the\ntraditional Softmax. The AM-Softmax introduces a margin of separation between\nthe classes that forces the samples from the same class to be closer to each\nother and also maximizes the distance between classes. In this paper, we\npropose a new approach for speaker recognition systems called AM-SincNet, which\nis based on the SincNet but uses an improved AM-Softmax layer. The proposed\nmethod is evaluated in the TIMIT dataset and obtained an improvement of\napproximately 40% in the Frame Error Rate compared to SincNet.", "published": "2019-01-28 16:16:34", "link": "http://arxiv.org/abs/1901.10826v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
