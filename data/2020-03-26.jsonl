{"title": "TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance\n  Generation", "abstract": "Natural Language Generation (NLG) models are prone to generating repetitive\nutterances. In this work, we study the repetition problem for encoder-decoder\nmodels, using both recurrent neural network (RNN) and transformer\narchitectures. To this end, we consider the chit-chat task, where the problem\nis more prominent than in other tasks that need encoder-decoder architectures.\nWe first study the influence of model architectures. By using pre-attention and\nhighway connections for RNNs, we manage to achieve lower repetition rates.\nHowever, this method does not generalize to other models such as transformers.\nWe hypothesize that the deeper reason is that in the training corpora, there\nare hard tokens that are more difficult for a generative model to learn than\nothers and, once learning has finished, hard tokens are still under-learned, so\nthat repetitive generations are more likely to happen. Based on this\nhypothesis, we propose token loss dynamic reweighting (TLDR) that applies\ndifferentiable weights to individual token losses. By using higher weights for\nhard tokens and lower weights for easy tokens, NLG models are able to learn\nindividual tokens at different paces. Experiments on chit-chat benchmark\ndatasets show that TLDR is more effective in repetition reduction for both RNN\nand transformer architectures than baselines using different weighting\nfunctions.", "published": "2020-03-26 15:01:37", "link": "http://arxiv.org/abs/2003.11963v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FFR V1.0: Fon-French Neural Machine Translation", "abstract": "Africa has the highest linguistic diversity in the world. On account of the\nimportance of language to communication, and the importance of reliable,\npowerful and accurate machine translation models in modern inter-cultural\ncommunication, there have been (and still are) efforts to create\nstate-of-the-art translation models for the many African languages. However,\nthe low-resources, diacritical and tonal complexities of African languages are\nmajor issues facing African NLP today. The FFR is a major step towards creating\na robust translation model from Fon, a very low-resource and tonal language, to\nFrench, for research and public use. In this paper, we describe our pilot\nproject: the creation of a large growing corpora for Fon-to-French translations\nand our FFR v1.0 model, trained on this dataset. The dataset and model are made\npublicly available.", "published": "2020-03-26 19:01:31", "link": "http://arxiv.org/abs/2003.12111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Common-Knowledge Concept Recognition for SEVA", "abstract": "We build a common-knowledge concept recognition system for a Systems\nEngineer's Virtual Assistant (SEVA) which can be used for downstream tasks such\nas relation extraction, knowledge graph construction, and question-answering.\nThe problem is formulated as a token classification task similar to named\nentity extraction. With the help of a domain expert and text processing\nmethods, we construct a dataset annotated at the word-level by carefully\ndefining a labelling scheme to train a sequence model to recognize systems\nengineering concepts. We use a pre-trained language model and fine-tune it with\nthe labeled dataset of concepts. In addition, we also create some essential\ndatasets for information such as abbreviations and definitions from the systems\nengineering domain. Finally, we construct a simple knowledge graph using these\nextracted concepts along with some hyponym relations.", "published": "2020-03-26 00:30:36", "link": "http://arxiv.org/abs/2003.11687v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Integrating Crowdsourcing and Active Learning for Classification of\n  Work-Life Events from Tweets", "abstract": "Social media, especially Twitter, is being increasingly used for research\nwith predictive analytics. In social media studies, natural language processing\n(NLP) techniques are used in conjunction with expert-based, manual and\nqualitative analyses. However, social media data are unstructured and must\nundergo complex manipulation for research use. The manual annotation is the\nmost resource and time-consuming process that multiple expert raters have to\nreach consensus on every item, but is essential to create gold-standard\ndatasets for training NLP-based machine learning classifiers. To reduce the\nburden of the manual annotation, yet maintaining its reliability, we devised a\ncrowdsourcing pipeline combined with active learning strategies. We\ndemonstrated its effectiveness through a case study that identifies job loss\nevents from individual tweets. We used Amazon Mechanical Turk platform to\nrecruit annotators from the Internet and designed a number of quality control\nmeasures to assure annotation accuracy. We evaluated 4 different active\nlearning strategies (i.e., least confident, entropy, vote entropy, and\nKullback-Leibler divergence). The active learning strategies aim at reducing\nthe number of tweets needed to reach a desired performance of automated\nclassification. Results show that crowdsourcing is useful to create\nhigh-quality annotations and active learning helps in reducing the number of\nrequired tweets, although there was no substantial difference among the\nstrategies tested.", "published": "2020-03-26 20:19:33", "link": "http://arxiv.org/abs/2003.12139v2", "categories": ["cs.CL", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Better Opioid Antagonists Using Deep Reinforcement Learning", "abstract": "Naloxone, an opioid antagonist, has been widely used to save lives from\nopioid overdose, a leading cause for death in the opioid epidemic. However,\nnaloxone has short brain retention ability, which limits its therapeutic\nefficacy. Developing better opioid antagonists is critical in combating the\nopioid epidemic.Instead of exhaustively searching in a huge chemical space for\nbetter opioid antagonists, we adopt reinforcement learning which allows\nefficient gradient-based search towards molecules with desired physicochemical\nand/or biological properties. Specifically, we implement a deep reinforcement\nlearning framework to discover potential lead compounds as better opioid\nantagonists with enhanced brain retention ability. A customized multi-objective\nreward function is designed to bias the generation towards molecules with both\nsufficient opioid antagonistic effect and enhanced brain retention ability.\nThorough evaluation demonstrates that with this framework, we are able to\nidentify valid, novel and feasible molecules with multiple desired properties,\nwhich has high potential in drug discovery.", "published": "2020-03-26 15:28:50", "link": "http://arxiv.org/abs/2004.04768v1", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Belief Propagation for Maximum Coverage on Weighted Bipartite Graph and\n  Application to Text Summarization", "abstract": "We study text summarization from the viewpoint of maximum coverage problem.\nIn graph theory, the task of text summarization is regarded as maximum coverage\nproblem on bipartite graph with weighted nodes. In recent study,\nbelief-propagation based algorithm for maximum coverage on unweighted graph was\nproposed using the idea of statistical mechanics. We generalize it to weighted\ngraph for text summarization. Then we apply our algorithm to weighted biregular\nrandom graph for verification of maximum coverage performance. We also apply it\nto bipartite graph representing real document in open text dataset, and check\nthe performance of text summarization. As a result, our algorithm exhibits\nbetter performance than greedy-type algorithm in some setting of text\nsummarization.", "published": "2020-03-26 05:50:20", "link": "http://arxiv.org/abs/2004.08301v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Non-parallel Voice Conversion System with WaveNet Vocoder and Collapsed\n  Speech Suppression", "abstract": "In this paper, we integrate a simple non-parallel voice conversion (VC)\nsystem with a WaveNet (WN) vocoder and a proposed collapsed speech suppression\ntechnique. The effectiveness of WN as a vocoder for generating high-fidelity\nspeech waveforms on the basis of acoustic features has been confirmed in recent\nworks. However, when combining the WN vocoder with a VC system, the distorted\nacoustic features, acoustic and temporal mismatches, and exposure bias usually\nlead to significant speech quality degradation, making WN generate some very\nnoisy speech segments called collapsed speech. To tackle the problem, we take\nconventional-vocoder-generated speech as the reference speech to derive a\nlinear predictive coding distribution constraint (LPCDC) to avoid the collapsed\nspeech problem. Furthermore, to mitigate the negative effects introduced by the\nLPCDC, we propose a collapsed speech segment detector (CSSD) to ensure that the\nLPCDC is only applied to the problematic segments to limit the loss of quality\nto short periods. Objective and subjective evaluations are conducted, and the\nexperimental results confirm the effectiveness of the proposed method, which\nfurther improves the speech quality of our previous non-parallel VC system\nsubmitted to Voice Conversion Challenge 2018.", "published": "2020-03-26 05:37:09", "link": "http://arxiv.org/abs/2003.11750v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Quality Factors for Traditional and Neural-Based Low Bit Rate\n  Vocoders", "abstract": "This study compares the performances of different algorithms for coding\nspeech at low bit rates. In addition to widely deployed traditional vocoders, a\nselection of recently developed generative-model-based coders at different bit\nrates are contrasted. Performance analysis of the coded speech is evaluated for\ndifferent quality aspects: accuracy of pitch periods estimation, the word error\nrates for automatic speech recognition, and the influence of speaker gender and\ncoding delays. A number of performance metrics of speech samples taken from a\npublicly available database were compared with subjective scores. Results from\nsubjective quality assessment do not correlate well with existing full\nreference speech quality metrics. The results provide valuable insights into\naspects of the speech signal that will be used to develop a novel metric to\naccurately predict speech quality from generative-model-based coders.", "published": "2020-03-26 13:19:43", "link": "http://arxiv.org/abs/2003.11882v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "In defence of metric learning for speaker recognition", "abstract": "The objective of this paper is 'open-set' speaker recognition of unseen\nspeakers, where ideal embeddings should be able to condense information into a\ncompact utterance-level representation that has small intra-speaker and large\ninter-speaker distance.\n  A popular belief in speaker recognition is that networks trained with\nclassification objectives outperform metric learning methods. In this paper, we\npresent an extensive evaluation of most popular loss functions for speaker\nrecognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet\nloss shows competitive performance compared to classification-based losses, and\nthose trained with our proposed metric learning objective outperform\nstate-of-the-art methods.", "published": "2020-03-26 15:43:10", "link": "http://arxiv.org/abs/2003.11982v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Review of Multi-Objective Deep Learning Speech Denoising Methods", "abstract": "This paper presents a review of multi-objective deep learning methods that\nhave been introduced in the literature for speech denoising. After stating an\noverview of conventional, single objective deep learning, and hybrid or\ncombined conventional and deep learning methods, a review of the mathematical\nframework of the multi-objective deep learning methods for speech denoising is\nprovided. A representative method from each speech denoising category, whose\ncodes are publicly available, is selected and a comparison is carried out by\nconsidering the same public domain dataset and four widely used objective\nmetrics. The comparison results indicate the effectiveness of the\nmulti-objective method compared with the other methods, in particular when the\nsignal-to-noise ratio is low. Possible future improvements that can be achieved\nare also mentioned.", "published": "2020-03-26 18:55:31", "link": "http://arxiv.org/abs/2003.12108v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Incremental Learning Algorithm for Sound Event Detection", "abstract": "This paper presents a new learning strategy for the Sound Event Detection\n(SED) system to tackle the issues of i) knowledge migration from a pre-trained\nmodel to a new target model and ii) learning new sound events without\nforgetting the previously learned ones without re-training from scratch. In\norder to migrate the previously learned knowledge from the source model to the\ntarget one, a neural adapter is employed on the top of the source model. The\nsource model and the target model are merged via this neural adapter layer. The\nneural adapter layer facilitates the target model to learn new sound events\nwith minimal training data and maintaining the performance of the previously\nlearned sound events similar to the source model. Our extensive analysis on the\nDCASE16 and US-SED dataset reveals the effectiveness of the proposed method in\ntransferring knowledge between source and target models without introducing any\nperformance degradation on the previously learned sound events while obtaining\na competitive detection performance on the newly learned sound events.", "published": "2020-03-26 22:32:11", "link": "http://arxiv.org/abs/2003.12175v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
