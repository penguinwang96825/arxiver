{"title": "Morphological Inflection: A Reality Check", "abstract": "Morphological inflection is a popular task in sub-word NLP with both\npractical and cognitive applications. For years now, state-of-the-art systems\nhave reported high, but also highly variable, performance across data sets and\nlanguages. We investigate the causes of this high performance and high\nvariability; we find several aspects of data set creation and evaluation which\nsystematically inflate performance and obfuscate differences between languages.\nTo improve generalizability and reliability of results, we propose new data\nsampling and evaluation strategies that better reflect likely use-cases. Using\nthese new strategies, we make new observations on the generalization abilities\nof current inflection systems.", "published": "2023-05-25 01:27:29", "link": "http://arxiv.org/abs/2305.15637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Grammatical Error Correction Systems with Explanations", "abstract": "Grammatical error correction systems improve written communication by\ndetecting and correcting language mistakes. To help language learners better\nunderstand why the GEC system makes a certain correction, the causes of errors\n(evidence words) and the corresponding error types are two key factors. To\nenhance GEC systems with explanations, we introduce EXPECT, a large dataset\nannotated with evidence words and grammatical error types. We propose several\nbaselines and analysis to understand this task. Furthermore, human evaluation\nverifies our explainable GEC system's explanations can assist second-language\nlearners in determining whether to accept a correction suggestion and in\nunderstanding the associated grammar rule.", "published": "2023-05-25 03:00:49", "link": "http://arxiv.org/abs/2305.15676v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perturbation-based Self-supervised Attention for Attention Bias in Text\n  Classification", "abstract": "In text classification, the traditional attention mechanisms usually focus\ntoo much on frequent words, and need extensive labeled data in order to learn.\nThis paper proposes a perturbation-based self-supervised attention approach to\nguide attention learning without any annotation overhead. Specifically, we add\nas much noise as possible to all the words in the sentence without changing\ntheir semantics and predictions. We hypothesize that words that tolerate more\nnoise are less significant, and we can use this information to refine the\nattention distribution. Experimental results on three text classification tasks\nshow that our approach can significantly improve the performance of current\nattention-based models, and is more effective than existing self-supervised\nmethods. We also provide a visualization analysis to verify the effectiveness\nof our approach.", "published": "2023-05-25 03:18:18", "link": "http://arxiv.org/abs/2305.15684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The False Promise of Imitating Proprietary LLMs", "abstract": "An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.", "published": "2023-05-25 05:00:12", "link": "http://arxiv.org/abs/2305.15717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Higher Pareto Frontier in Multilingual Machine Translation", "abstract": "Multilingual neural machine translation has witnessed remarkable progress in\nrecent years. However, the long-tailed distribution of multilingual corpora\nposes a challenge of Pareto optimization, i.e., optimizing for some languages\nmay come at the cost of degrading the performance of others. Existing balancing\ntraining strategies are equivalent to a series of Pareto optimal solutions,\nwhich trade off on a Pareto frontier. In this work, we propose a new training\nframework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto\nfrontier outwards rather than making trade-offs. Specifically, Pareto-MD\ncollaboratively trains two Pareto optimal solutions that favor different\nlanguages and allows them to learn from the strengths of each other via\nknowledge distillation. Furthermore, we introduce a novel strategy to enable\nstronger communication between Pareto optimal solutions and broaden the\napplicability of our approach. Experimental results on the widely-used WMT and\nTED datasets show that our method significantly pushes the Pareto frontier and\noutperforms baselines by up to +2.46 BLEU.", "published": "2023-05-25 05:01:33", "link": "http://arxiv.org/abs/2305.15718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learn to Not Link: Exploring NIL Prediction in Entity Linking", "abstract": "Entity linking models have achieved significant success via utilizing\npretrained language models to capture semantic features. However, the NIL\nprediction problem, which aims to identify mentions without a corresponding\nentity in the knowledge base, has received insufficient attention. We\ncategorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,\nand propose an entity linking dataset NEL that focuses on the NIL prediction\nproblem. NEL takes ambiguous entities as seeds, collects relevant mention\ncontext in the Wikipedia corpus, and ensures the presence of mentions linking\nto NIL by human annotation and entity masking. We conduct a series of\nexperiments with the widely used bi-encoder and cross-encoder entity linking\nmodels, results show that both types of NIL mentions in training data have a\nsignificant influence on the accuracy of NIL prediction. Our code and dataset\ncan be accessed at https://github.com/solitaryzero/NIL_EL", "published": "2023-05-25 05:12:33", "link": "http://arxiv.org/abs/2305.15725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive\n  Learning Framework for Text-based Recommendation", "abstract": "Prior study has shown that pretrained language models (PLM) can boost the\nperformance of text-based recommendation. In contrast to previous works that\neither use PLM to encode user history as a whole input text, or impose an\nadditional aggregation network to fuse multi-turn history representations, we\npropose a unified local- and global-attention Transformer encoder to better\nmodel two-level contexts of user history. Moreover, conditioned on user history\nencoded by Transformer encoders, our framework leverages Transformer decoders\nto estimate the language perplexity of candidate text items, which can serve as\na straightforward yet significant contrastive signal for user-item text\nmatching. Based on this, our framework, UniTRec, unifies the contrastive\nobjectives of discriminative matching scores and candidate text perplexity to\njointly enhance text-based recommendation. Extensive evaluation shows that\nUniTRec delivers SOTA performance on three text-based recommendation tasks.\nCode is available at https://github.com/Veason-silverbullet/UniTRec.", "published": "2023-05-25 06:11:31", "link": "http://arxiv.org/abs/2305.15756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bhasha-Abhijnaanam: Native-script and romanized Language Identification\n  for 22 Indic languages", "abstract": "We create publicly available language identification (LID) datasets and\nmodels in all 22 Indian languages listed in the Indian constitution in both\nnative-script and romanized text. First, we create Bhasha-Abhijnaanam, a\nlanguage identification test set for native-script as well as romanized text\nwhich spans all 22 Indic languages. We also train IndicLID, a language\nidentifier for all the above-mentioned languages in both native and romanized\nscript. For native-script text, it has better language coverage than existing\nLIDs and is competitive or better than other LIDs. IndicLID is the first LID\nfor romanized text in Indian languages. Two major challenges for romanized text\nLID are the lack of training data and low-LID performance when languages are\nsimilar. We provide simple and effective solutions to these problems. In\ngeneral, there has been limited work on romanized text in any language, and our\nfindings are relevant to other languages that need romanized language\nidentification. Our models are publicly available at\nhttps://ai4bharat.iitm.ac.in/indiclid under open-source licenses. Our training\nand test sets are also publicly available at\nhttps://ai4bharat.iitm.ac.in/bhasha-abhijnaanam under open-source licenses.", "published": "2023-05-25 07:53:23", "link": "http://arxiv.org/abs/2305.15814v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset", "abstract": "The cross-domain text-to-SQL task aims to build a system that can parse user\nquestions into SQL on complete unseen databases, and the single-domain\ntext-to-SQL task evaluates the performance on identical databases. Both of\nthese setups confront unavoidable difficulties in real-world applications. To\nthis end, we introduce the cross-schema text-to-SQL task, where the databases\nof evaluation data are different from that in the training data but come from\nthe same domain. Furthermore, we present CSS, a large-scale CrosS-Schema\nChinese text-to-SQL dataset, to carry on corresponding studies. CSS originally\nconsisted of 4,340 question/SQL pairs across 2 databases. In order to\ngeneralize models to different medical systems, we extend CSS and create 19 new\ndatabases along with 29,280 corresponding dataset examples. Moreover, CSS is\nalso a large corpus for single-domain Chinese text-to-SQL studies. We present\nthe data collection approach and a series of analyses of the data statistics.\nTo show the potential and usefulness of CSS, benchmarking baselines have been\nconducted and reported. Our dataset is publicly available at\n\\url{https://huggingface.co/datasets/zhanghanchong/css}.", "published": "2023-05-25 09:44:44", "link": "http://arxiv.org/abs/2305.15891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collective Knowledge Graph Completion with Mutual Knowledge Distillation", "abstract": "Knowledge graph completion (KGC), the task of predicting missing information\nbased on the existing relational data inside a knowledge graph (KG), has drawn\nsignificant attention in recent years. However, the predictive power of KGC\nmethods is often limited by the completeness of the existing knowledge graphs\nfrom different sources and languages. In monolingual and multilingual settings,\nKGs are potentially complementary to each other. In this paper, we study the\nproblem of multi-KG completion, where we focus on maximizing the collective\nknowledge from different KGs to alleviate the incompleteness of individual KGs.\nSpecifically, we propose a novel method called CKGC-CKD that uses\nrelation-aware graph convolutional network encoder models on both individual\nKGs and a large fused KG in which seed alignments between KGs are regarded as\nedges for message propagation. An additional mutual knowledge distillation\nmechanism is also employed to maximize the knowledge transfer between the\nmodels of \"global\" fused KG and the \"local\" individual KGs. Experimental\nresults on multilingual datasets have shown that our method outperforms all\nstate-of-the-art models in the KGC task.", "published": "2023-05-25 09:49:40", "link": "http://arxiv.org/abs/2305.15895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Response Generation in Longitudinal Dialogues: Which Knowledge\n  Representation Helps?", "abstract": "Longitudinal Dialogues (LD) are the most challenging type of conversation for\nhuman-machine dialogue systems. LDs include the recollections of events,\npersonal thoughts, and emotions specific to each individual in a sparse\nsequence of dialogue sessions. Dialogue systems designed for LDs should\nuniquely interact with the users over multiple sessions and long periods of\ntime (e.g. weeks), and engage them in personal dialogues to elaborate on their\nfeelings, thoughts, and real-life events. In this paper, we study the task of\nresponse generation in LDs. We evaluate whether general-purpose Pre-trained\nLanguage Models (PLM) are appropriate for this purpose. We fine-tune two PLMs,\nGePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different\nrepresentations of the personal knowledge extracted from LDs for grounded\nresponse generation, including the graph representation of the mentioned events\nand participants. We evaluate the performance of the models via automatic\nmetrics and the contribution of the knowledge via the Integrated Gradients\ntechnique. We categorize the natural language generation errors via human\nevaluations of contextualization, appropriateness and engagement of the user.", "published": "2023-05-25 10:13:53", "link": "http://arxiv.org/abs/2305.15908v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliable Detection and Quantification of Selective Forces in Language\n  Change", "abstract": "Language change is a cultural evolutionary process in which variants of\nlinguistic variables change in frequency through processes analogous to\nmutation, selection and genetic drift. In this work, we apply a\nrecently-introduced method to corpus data to quantify the strength of selection\nin specific instances of historical language change. We first demonstrate, in\nthe context of English irregular verbs, that this method is more reliable and\ninterpretable than similar methods that have previously been applied. We\nfurther extend this study to demonstrate that a bias towards phonological\nsimplicity overrides that favouring grammatical simplicity when these are in\nconflict. Finally, with reference to Spanish spelling reforms, we show that the\nmethod can also detect points in time at which selection strengths change, a\nfeature that is generically expected for socially-motivated language change.\nTogether, these results indicate how hypotheses for mechanisms of language\nchange can be tested quantitatively using historical corpus data.", "published": "2023-05-25 10:20:15", "link": "http://arxiv.org/abs/2305.15914v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergence of a phonological bias in ChatGPT", "abstract": "Current large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT", "published": "2023-05-25 10:57:43", "link": "http://arxiv.org/abs/2305.15929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BUCA: A Binary Classification Approach to Unsupervised Commonsense\n  Question Answering", "abstract": "Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as\nthe construction of commonsense reasoning datasets is expensive, and they are\ninevitably limited in their scope. A popular approach to UCR is to fine-tune\nlanguage models with external knowledge (e.g., knowledge graphs), but this\nusually requires a large number of training examples. In this paper, we propose\nto transform the downstream multiple choice question answering task into a\nsimpler binary classification task by ranking all candidate answers according\nto their reasonableness. To this end, for training the model, we convert the\nknowledge graph triples into reasonable and unreasonable texts. Extensive\nexperimental results show the effectiveness of our approach on various multiple\nchoice question answering benchmarks. Furthermore, compared with existing UCR\napproaches using KGs, ours is less data hungry. Our code is available at\nhttps://github.com/probe2/BUCA.", "published": "2023-05-25 10:59:47", "link": "http://arxiv.org/abs/2305.15932v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from\n  Native Speaker Texts", "abstract": "We introduce NaSGEC, a new dataset to facilitate research on Chinese\ngrammatical error correction (CGEC) for native speaker texts from multiple\ndomains. Previous CGEC research primarily focuses on correcting texts from a\nsingle domain, especially learner essays. To broaden the target domain, we\nannotate multiple references for 12,500 sentences from three native domains,\ni.e., social media, scientific writing, and examination. We provide solid\nbenchmark results for NaSGEC by employing cutting-edge CGEC models and\ndifferent training data. We further perform detailed analyses of the\nconnections and gaps between our domains from both empirical and statistical\nviews. We hope this work can inspire future studies on an important but\nunder-explored direction--cross-domain GEC.", "published": "2023-05-25 13:05:52", "link": "http://arxiv.org/abs/2305.16023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Document Embeddings via Self-Contrastive Bregman Divergence\n  Learning", "abstract": "Learning quality document embeddings is a fundamental problem in natural\nlanguage processing (NLP), information retrieval (IR), recommendation systems,\nand search engines. Despite recent advances in the development of\ntransformer-based models that produce sentence embeddings with self-contrastive\nlearning, the encoding of long documents (Ks of words) is still challenging\nwith respect to both efficiency and quality considerations. Therefore, we train\nLongfomer-based document encoders using a state-of-the-art unsupervised\ncontrastive learning method (SimCSE). Further on, we complement the baseline\nmethod -- siamese neural network -- with additional convex neural networks\nbased on functional Bregman divergence aiming to enhance the quality of the\noutput document representations. We show that overall the combination of a\nself-contrastive siamese network and our proposed neural Bregman network\noutperforms the baselines in two linear classification settings on three long\ndocument topic classification tasks from the legal and biomedical domains.", "published": "2023-05-25 13:08:10", "link": "http://arxiv.org/abs/2305.16031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multijugate Dual Learning for Low-Resource Task-Oriented Dialogue System", "abstract": "Dialogue data in real scenarios tend to be sparsely available, rendering\ndata-starved end-to-end dialogue systems trained inadequately. We discover that\ndata utilization efficiency in low-resource scenarios can be enhanced by mining\nalignment information uncertain utterance and deterministic dialogue state.\nTherefore, we innovatively implement dual learning in task-oriented dialogues\nto exploit the correlation of heterogeneous data. In addition, the one-to-one\nduality is converted into a multijugate duality to reduce the influence of\nspurious correlations in dual training for generalization. Without introducing\nadditional parameters, our method could be implemented in arbitrary networks.\nExtensive empirical analyses demonstrate that our proposed method improves the\neffectiveness of end-to-end task-oriented dialogue systems under multiple\nbenchmarks and obtains state-of-the-art results in low-resource scenarios.", "published": "2023-05-25 14:38:05", "link": "http://arxiv.org/abs/2305.16106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Give Me More Details: Improving Fact-Checking with Latent Retrieval", "abstract": "Evidence plays a crucial role in automated fact-checking. When verifying\nreal-world claims, existing fact-checking systems either assume the evidence\nsentences are given or use the search snippets returned by the search engine.\nSuch methods ignore the challenges of collecting evidence and may not provide\nsufficient information to verify real-world claims. Aiming at building a better\nfact-checking system, we propose to incorporate full text from source documents\nas evidence and introduce two enriched datasets. The first one is a\nmultilingual dataset, while the second one is monolingual (English). We further\ndevelop a latent variable model to jointly extract evidence sentences from\ndocuments and perform claim verification. Experiments indicate that including\nsource documents can provide sufficient contextual clues even when gold\nevidence sentences are not annotated. The proposed system is able to achieve\nsignificant improvements upon best-reported models under different settings.", "published": "2023-05-25 15:01:19", "link": "http://arxiv.org/abs/2305.16128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Non-Autoregressive Translation at Scale", "abstract": "In real-world systems, scaling has been critical for improving the\ntranslation quality in autoregressive translation (AT), which however has not\nbeen well studied for non-autoregressive translation (NAT). In this work, we\nbridge the gap by systematically studying the impact of scaling on NAT\nbehaviors. Extensive experiments on six WMT benchmarks over two advanced NAT\nmodels show that scaling can alleviate the commonly-cited weaknesses of NAT\nmodels, resulting in better translation performance. To reduce the side-effect\nof scaling on decoding speed, we empirically investigate the impact of NAT\nencoder and decoder on the translation performance. Experimental results on the\nlarge-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger\nencoder and smaller decoder) can achieve comparable performance with the\nscaling model, while maintaining the superiority of decoding speed with\nstandard NAT models. To this end, we establish a new benchmark by validating\nscaled NAT models on the scaled dataset, which can be regarded as a strong\nbaseline for future works. We release code and system outputs at\nhttps://github.com/DeepLearnXMU/Scaling4NAT.", "published": "2023-05-25 15:22:47", "link": "http://arxiv.org/abs/2305.16155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Data Extraction From Pre-trained Language Models: A Survey", "abstract": "As the deployment of pre-trained language models (PLMs) expands, pressing\nsecurity concerns have arisen regarding the potential for malicious extraction\nof training data, posing a threat to data privacy. This study is the first to\nprovide a comprehensive survey of training data extraction from PLMs. Our\nreview covers more than 100 key papers in fields such as natural language\nprocessing and security. First, preliminary knowledge is recapped and a\ntaxonomy of various definitions of memorization is presented. The approaches\nfor attack and defense are then systemized. Furthermore, the empirical findings\nof several quantitative studies are highlighted. Finally, future research\ndirections based on this review are suggested.", "published": "2023-05-25 15:23:29", "link": "http://arxiv.org/abs/2305.16157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis", "abstract": "Multimodal relation extraction (MRE) is the task of identifying the semantic\nrelationships between two entities based on the context of the sentence image\npair. Existing retrieval-augmented approaches mainly focused on modeling the\nretrieved textual knowledge, but this may not be able to accurately identify\ncomplex relations. To improve the prediction, this research proposes to\nretrieve textual and visual evidence based on the object, sentence, and whole\nimage. We further develop a novel approach to synthesize the object-level,\nimage-level, and sentence-level information for better reasoning between the\nsame and different modalities. Extensive experiments and analyses show that the\nproposed method is able to effectively select and compare evidence across\nmodalities and significantly outperforms state-of-the-art models.", "published": "2023-05-25 15:26:13", "link": "http://arxiv.org/abs/2305.16166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-lingual and Multi-cultural Figurative Language Understanding", "abstract": "Figurative language permeates human communication, but at the same time is\nrelatively understudied in NLP. Datasets have been created in English to\naccelerate progress towards measuring and improving figurative language\nprocessing in language models (LMs). However, the use of figurative language is\nan expression of our cultural and societal experiences, making it difficult for\nthese phrases to be universally applicable. In this work, we create a\nfigurative language inference dataset, \\datasetname, for seven diverse\nlanguages associated with a variety of cultures: Hindi, Indonesian, Javanese,\nKannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language\nrelies on cultural and regional concepts for figurative expressions, with the\nhighest overlap between languages originating from the same region. We assess\nmultilingual LMs' abilities to interpret figurative language in zero-shot and\nfew-shot settings. All languages exhibit a significant deficiency compared to\nEnglish, with variations in performance reflecting the availability of\npre-training and fine-tuning data, emphasizing the need for LMs to be exposed\nto a broader range of linguistic and cultural variation during training.", "published": "2023-05-25 15:30:31", "link": "http://arxiv.org/abs/2305.16171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented\n  Language Models", "abstract": "Augmenting language models with a retrieval mechanism has been shown to\nsignificantly improve their performance while keeping the number of parameters\nlow. Retrieval-augmented models commonly rely on a semantic retrieval mechanism\nbased on the similarity between dense representations of the query chunk and\npotential neighbors. In this paper, we study the state-of-the-art Retro model\nand observe that its performance gain is better explained by surface-level\nsimilarities, such as token overlap. Inspired by this, we replace the semantic\nretrieval in Retro with a surface-level method based on BM25, obtaining a\nsignificant reduction in perplexity. As full BM25 retrieval can be\ncomputationally costly for large datasets, we also apply it in a re-ranking\nscenario, gaining part of the perplexity reduction with minimal computational\noverhead.", "published": "2023-05-25 16:56:26", "link": "http://arxiv.org/abs/2305.16243v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overcoming Catastrophic Forgetting in Massively Multilingual Continual\n  Learning", "abstract": "Real-life multilingual systems should be able to efficiently incorporate new\nlanguages as data distributions fed to the system evolve and shift over time.\nTo do this, systems need to handle the issue of catastrophic forgetting, where\nthe model performance drops for languages or tasks seen further in its past. In\nthis paper, we study catastrophic forgetting, as well as methods to minimize\nthis, in a massively multilingual continual learning framework involving up to\n51 languages and covering both classification and sequence labeling tasks. We\npresent LR ADJUST, a learning rate scheduling method that is simple, yet\neffective in preserving new information without strongly overwriting past\nknowledge. Furthermore, we show that this method is effective across multiple\ncontinual learning approaches. Finally, we provide further insights into the\ndynamics of catastrophic forgetting in this massively multilingual setup.", "published": "2023-05-25 17:06:34", "link": "http://arxiv.org/abs/2305.16252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering and Categorizing Social Biases in Text-to-SQL", "abstract": "Content Warning: This work contains examples that potentially implicate\nstereotypes, associations, and other harms that could be offensive to\nindividuals in certain social groups.} Large pre-trained language models are\nacknowledged to carry social biases towards different demographics, which can\nfurther amplify existing stereotypes in our society and cause even more harm.\nText-to-SQL is an important task, models of which are mainly adopted by\nadministrative industries, where unfair decisions may lead to catastrophic\nconsequences. However, existing Text-to-SQL models are trained on clean,\nneutral datasets, such as Spider and WikiSQL. This, to some extent, cover up\nsocial bias in models under ideal conditions, which nevertheless may emerge in\nreal application scenarios. In this work, we aim to uncover and categorize\nsocial biases in Text-to-SQL models. We summarize the categories of social\nbiases that may occur in structured data for Text-to-SQL models. We build test\nbenchmarks and reveal that models with similar task accuracy can contain social\nbiases at very different rates. We show how to take advantage of our\nmethodology to uncover and assess social biases in the downstream Text-to-SQL\ntask. We will release our code and data.", "published": "2023-05-25 17:08:56", "link": "http://arxiv.org/abs/2305.16253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNITE: A Unified Benchmark for Text-to-SQL Evaluation", "abstract": "A practical text-to-SQL system should generalize well on a wide variety of\nnatural language questions, unseen database schemas, and novel SQL query\nstructures. To comprehensively evaluate text-to-SQL systems, we introduce a\nUNIfied benchmark for Text-to-SQL Evaluation (UNITE). It is composed of\npublicly available text-to-SQL datasets, containing natural language questions\nfrom more than 12 domains, SQL queries from more than 3.9K patterns, and 29K\ndatabases. Compared to the widely used Spider benchmark, we introduce\n$\\sim$120K additional examples and a threefold increase in SQL patterns, such\nas comparative and boolean questions. We conduct a systematic study of six\nstate-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that:\n1) Codex performs surprisingly well on out-of-domain datasets; 2) specially\ndesigned decoding methods (e.g. constrained beam search) can improve\nperformance for both in-domain and out-of-domain settings; 3) explicitly\nmodeling the relationship between questions and schemas further improves the\nSeq2Seq models. More importantly, our benchmark presents key challenges towards\ncompositional generalization and robustness issues -- which these SOTA models\ncannot address well. Our code and data processing script are available at\nhttps://github.com/awslabs/unified-text2sql-benchmark", "published": "2023-05-25 17:19:52", "link": "http://arxiv.org/abs/2305.16265v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Knowledge Distillation for Answer Sentence Selection in\n  Low-Resource Languages", "abstract": "While impressive performance has been achieved on the task of Answer Sentence\nSelection (AS2) for English, the same does not hold for languages that lack\nlarge labeled datasets. In this work, we propose Cross-Lingual Knowledge\nDistillation (CLKD) from a strong English AS2 teacher as a method to train AS2\nmodels for low-resource languages in the tasks without the need of labeled data\nfor the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a\ntranslation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, a\nmultilingual AS2 dataset with over 70K questions spanning 8 typologically\ndiverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2\nwith multiple teachers, diverse monolingual and multilingual pretrained\nlanguage models (PLMs) as students, and both monolingual and multilingual\ntraining. The results demonstrate that CLKD either outperforms or rivals even\nsupervised fine-tuning with the same amount of labeled data and a combination\nof machine translation and the teacher model. Our method can potentially enable\nstronger AS2 models for low-resource languages, while TyDi-AS2 can serve as the\nlargest multilingual AS2 dataset for further studies in the research community.", "published": "2023-05-25 17:56:04", "link": "http://arxiv.org/abs/2305.16302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndicTrans2: Towards High-Quality and Accessible Machine Translation\n  Models for all 22 Scheduled Indian Languages", "abstract": "India has a rich linguistic landscape with languages from 4 major language\nfamilies spoken by over a billion people. 22 of these languages are listed in\nthe Constitution of India (referred to as scheduled languages) are the focus of\nthis work. Given the linguistic diversity, high-quality and accessible Machine\nTranslation (MT) systems are essential in a country like India. Prior to this\nwork, there was (i) no parallel training data spanning all 22 languages, (ii)\nno robust benchmarks covering all these languages and containing content\nrelevant to India, and (iii) no existing translation models which support all\nthe 22 scheduled languages of India. In this work, we aim to address this gap\nby focusing on the missing pieces required for enabling wide, easy, and open\naccess to good machine translation systems for all 22 scheduled Indian\nlanguages. We identify four key areas of improvement: curating and creating\nlarger training datasets, creating diverse and high-quality benchmarks,\ntraining multilingual models, and releasing models with open access. Our first\ncontribution is the release of the Bharat Parallel Corpus Collection (BPCC),\nthe largest publicly available parallel corpora for Indic languages. BPCC\ncontains a total of 230M bitext pairs, of which a total of 126M were newly\nadded, including 644K manually translated sentence pairs created as part of\nthis work. Our second contribution is the release of the first n-way parallel\nbenchmark covering all 22 Indian languages, featuring diverse domains,\nIndian-origin content, and source-original test sets. Next, we present\nIndicTrans2, the first model to support all 22 languages, surpassing existing\nmodels on multiple existing and new benchmarks created as a part of this work.\nLastly, to promote accessibility and collaboration, we release our models and\nassociated data with permissive licenses at\nhttps://github.com/AI4Bharat/IndicTrans2.", "published": "2023-05-25 17:57:43", "link": "http://arxiv.org/abs/2305.16307v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EDM3: Event Detection as Multi-task Text Generation", "abstract": "Event detection refers to identifying event occurrences in a text and\ncomprises of two subtasks; event identification and classification. We present\nEDM3, a novel approach for Event Detection that formulates three generative\ntasks: identification, classification, and combined detection. We show that\nEDM3 helps to learn transferable knowledge that can be leveraged to perform\nEvent Detection and its subtasks concurrently, mitigating the error propagation\ninherent in pipelined approaches. Unlike previous dataset- or domain-specific\napproaches, EDM3 utilizes the existing knowledge of language models, allowing\nit to be trained over any classification schema. We evaluate EDM3 on multiple\nevent detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3\noutperforms 1) single-task performance by 8.4% on average and 2) multi-task\nperformance without instructional prompts by 2.4% on average. We obtain SOTA\nresults on RAMS (71.3% vs. 65.1% F-1) and competitive performance on other\ndatasets. We analyze our approach to demonstrate its efficacy in low-resource\nand multi-sentence settings. We also show the effectiveness of this approach on\nnon-standard event configurations such as multi-word and multi-class event\ntriggers. Overall, our results show that EDM3 is a promising approach for Event\nDetection that has the potential for real-world applications.", "published": "2023-05-25 06:25:16", "link": "http://arxiv.org/abs/2305.16357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware attention layers coupled with optimal transport domain\n  adaptation and multimodal fusion methods for recognizing dementia from\n  spontaneous speech", "abstract": "Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is\nthe main cause of dementia. Although many studies have been proposed targeting\nat diagnosing dementia through spontaneous speech, there are still limitations.\nExisting state-of-the-art approaches, which propose multimodal methods, train\nseparately language and acoustic models, employ majority-vote approaches, and\nconcatenate the representations of the different modalities either at the input\nlevel, i.e., early fusion, or during training. Also, some of them employ\nself-attention layers, which calculate the dependencies between representations\nwithout considering the contextual information. In addition, no prior work has\ntaken into consideration the model calibration. To address these limitations,\nwe propose some new methods for detecting AD patients, which capture the intra-\nand cross-modal interactions. First, we convert the audio files into log-Mel\nspectrograms, their delta, and delta-delta and create in this way an image per\naudio file consisting of three channels. Next, we pass each transcript and\nimage through BERT and DeiT models respectively. After that, context-based\nself-attention layers, self-attention layers with a gate model, and optimal\ntransport domain adaptation methods are employed for capturing the intra- and\ninter-modal interactions. Finally, we exploit two methods for fusing the self\nand cross-attention features. For taking into account the model calibration, we\napply label smoothing. We use both performance and calibration metrics.\nExperiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the\nefficacy of our introduced approaches over existing research initiatives with\nour best performing model reaching Accuracy and F1-score up to 91.25% and\n91.06% respectively.", "published": "2023-05-25 18:18:09", "link": "http://arxiv.org/abs/2305.16406v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Script Normalization for Unconventional Writing of Under-Resourced\n  Languages in Bilingual Communities", "abstract": "The wide accessibility of social media has provided linguistically\nunder-represented communities with an extraordinary opportunity to create\ncontent in their native languages. This, however, comes with certain challenges\nin script normalization, particularly where the speakers of a language in a\nbilingual community rely on another script or orthography to write their native\nlanguage. This paper addresses the problem of script normalization for several\nsuch languages that are mainly written in a Perso-Arabic script. Using\nsynthetic data with various levels of noise and a transformer-based model, we\ndemonstrate that the problem can be effectively remediated. We conduct a\nsmall-scale evaluation of real data as well. Our experiments indicate that\nscript normalization is also beneficial to improve the performance of\ndownstream tasks such as machine translation and language identification.", "published": "2023-05-25 18:18:42", "link": "http://arxiv.org/abs/2305.16407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained\n  language models", "abstract": "Vector space models of word meaning all share the assumption that words\noccurring in similar contexts have similar meanings. In such models, words that\nare similar in their topical associations but differ in their logical force\ntend to emerge as semantically close, creating well-known challenges for NLP\napplications that involve logical reasoning. Modern pretrained language models,\nsuch as BERT, RoBERTa and GPT-3 hold the promise of performing better on\nlogical tasks than classic static word embeddings. However, reports are mixed\nabout their success. In the current paper, we advance this discussion through a\nsystematic study of scalar adverbs, an under-explored class of words with\nstrong logical force. Using three different tasks, involving both naturalistic\nsocial media data and constructed examples, we investigate the extent to which\nBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these\ncommon words. We ask: 1) Do the models distinguish amongst the three semantic\ncategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit\nrepresentations of full scales from maximally negative to maximally positive?\n3) How do word frequency and contextual factors impact model performance? We\nfind that despite capturing some aspects of logical meaning, the models fall\nfar short of human performance.", "published": "2023-05-25 18:56:26", "link": "http://arxiv.org/abs/2305.16426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by\n  Rewriting Text", "abstract": "Can language models transform inputs to protect text classifiers against\nadversarial attacks? In this work, we present ATINTER, a model that intercepts\nand learns to rewrite adversarial inputs to make them non-adversarial for a\ndownstream text classifier. Our experiments on four datasets and five attack\nmechanisms reveal that ATINTER is effective at providing better adversarial\nrobustness than existing defense approaches, without compromising task\naccuracy. For example, on sentiment classification using the SST-2 dataset, our\nmethod improves the adversarial accuracy over the best existing defense\napproach by more than 4% with a smaller decrease in task accuracy (0.5% vs\n2.5%). Moreover, we show that ATINTER generalizes across multiple downstream\ntasks and classifiers without having to explicitly retrain it for those\nsettings. Specifically, we find that when ATINTER is trained to remove\nadversarial perturbations for the sentiment classification task on the SST-2\ndataset, it even transfers to a semantically different task of news\nclassification (on AGNews) and improves the adversarial robustness by more than\n10%.", "published": "2023-05-25 19:42:51", "link": "http://arxiv.org/abs/2305.16444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prototype-Based Interpretability for Legal Citation Prediction", "abstract": "Deep learning has made significant progress in the past decade, and\ndemonstrates potential to solve problems with extensive social impact. In\nhigh-stakes decision making areas such as law, experts often require\ninterpretability for automatic systems to be utilized in practical settings. In\nthis work, we attempt to address these requirements applied to the important\nproblem of legal citation prediction (LCP). We design the task with parallels\nto the thought-process of lawyers, i.e., with reference to both precedents and\nlegislative provisions. After initial experimental results, we refine the\ntarget citation predictions with the feedback of legal experts. Additionally,\nwe introduce a prototype architecture to add interpretability, achieving strong\nperformance while adhering to decision parameters used by lawyers. Our study\nbuilds on and leverages the state-of-the-art language processing models for\nlaw, while addressing vital considerations for high-stakes tasks with practical\nsocietal impact.", "published": "2023-05-25 21:40:58", "link": "http://arxiv.org/abs/2305.16490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks", "abstract": "Backdoor attacks are an insidious security threat against machine learning\nmodels. Adversaries can manipulate the predictions of compromised models by\ninserting triggers into the training phase. Various backdoor attacks have been\ndevised which can achieve nearly perfect attack success without affecting model\npredictions for clean inputs. Means of mitigating such vulnerabilities are\nunderdeveloped, especially in natural language processing. To fill this gap, we\nintroduce IMBERT, which uses either gradients or self-attention scores derived\nfrom victim models to self-defend against backdoor attacks at inference time.\nOur empirical studies demonstrate that IMBERT can effectively identify up to\n98.5% of inserted triggers. Thus, it significantly reduces the attack success\nrate while attaining competitive accuracy on the clean dataset across\nwidespread insertion-based attacks compared to two baselines. Finally, we show\nthat our approach is model-agnostic, and can be easily ported to several\npre-trained transformer models.", "published": "2023-05-25 22:08:57", "link": "http://arxiv.org/abs/2305.16503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in\n  Open-domain Conversational Question Answering", "abstract": "Large language models are known to produce output which sounds fluent and\nconvincing, but is also often wrong, e.g. \"unfaithful\" with respect to a\nrationale as retrieved from a knowledge base. In this paper, we show that\ntask-based systems which exhibit certain advanced linguistic dialog behaviors,\nsuch as lexical alignment (repeating what the user said), are in fact preferred\nand trusted more, whereas other phenomena, such as pronouns and ellipsis are\ndis-preferred. We use open-domain question answering systems as our test-bed\nfor task based dialog generation and compare several open- and closed-book\nmodels. Our results highlight the danger of systems that appear to be\ntrustworthy by parroting user input while providing an unfaithful response.", "published": "2023-05-25 22:54:13", "link": "http://arxiv.org/abs/2305.16519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConvGQR: Generative Query Reformulation for Conversational Search", "abstract": "In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.", "published": "2023-05-25 01:45:06", "link": "http://arxiv.org/abs/2305.15645v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BookGPT: A General Framework for Book Recommendation Empowered by Large\n  Language Model", "abstract": "With the continuous development and change exhibited by large language model\n(LLM) technology, represented by generative pretrained transformers (GPTs),\nmany classic scenarios in various fields have re-emerged with new\nopportunities. This paper takes ChatGPT as the modeling object, incorporates\nLLM technology into the typical book resource understanding and recommendation\nscenario for the first time, and puts it into practice. By building a\nChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,\nthis paper attempts to apply ChatGPT to recommendation modeling for three\ntypical tasks, book rating recommendation, user rating recommendation, and book\nsummary recommendation, and explores the feasibility of LLM technology in book\nrecommendation scenarios. At the same time, based on different evaluation\nschemes for book recommendation tasks and the existing classic recommendation\nmodels, this paper discusses the advantages and disadvantages of the BookGPT in\nbook recommendation scenarios and analyzes the opportunities and improvement\ndirections for subsequent LLMs in these scenarios.", "published": "2023-05-25 02:45:22", "link": "http://arxiv.org/abs/2305.15673v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Revisiting non-English Text Simplification: A Unified Multilingual\n  Benchmark", "abstract": "Recent advancements in high-quality, large-scale English resources have\npushed the frontier of English Automatic Text Simplification (ATS) research.\nHowever, less work has been done on multilingual text simplification due to the\nlack of a diverse evaluation benchmark that covers complex-simple sentence\npairs in many languages. This paper introduces the MultiSim benchmark, a\ncollection of 27 resources in 12 distinct languages containing over 1.7 million\ncomplex-simple sentence pairs. This benchmark will encourage research in\ndeveloping more effective multilingual text simplification models and\nevaluation metrics. Our experiments using MultiSim with pre-trained\nmultilingual language models reveal exciting performance improvements from\nmultilingual training in non-English settings. We observe strong performance\nfrom Russian in zero-shot cross-lingual transfer to low-resource languages. We\nfurther show that few-shot prompting with BLOOM-176b achieves comparable\nquality to reference simplifications outperforming fine-tuned models in most\nlanguages. We validate these findings through human evaluation.", "published": "2023-05-25 03:03:29", "link": "http://arxiv.org/abs/2305.15678v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncreative tasks such as storytelling and E-mail generation. However, as LLMs are\nprimarily trained on final text results rather than intermediate revisions, it\nmight be challenging for them to perform text rewriting tasks. Most studies in\nthe rewriting tasks focus on a particular transformation type within the\nboundaries of single sentences. In this work, we develop new strategies for\ninstruction tuning and reinforcement learning to better align LLMs for\ncross-sentence rewriting tasks using diverse wording and structures expressed\nthrough natural languages including 1) generating rewriting instruction data\nfrom Wiki edits and public corpus through instruction generation and\nchain-of-thought prompting; 2) collecting comparison data for reward model\ntraining through a new ranking function. To facilitate this research, we\nintroduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting\ntypes expressed through natural language instructions. Our results show\nsignificant improvements over a variety of baselines. The public repository is\navailable on GitHub under Google Research\n(https://github.com/google-research/google-research/tree/master/rewritelm).", "published": "2023-05-25 03:26:26", "link": "http://arxiv.org/abs/2305.15685v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts", "abstract": "Recent studies have demonstrated that natural-language prompts can help to\nleverage the knowledge learned by pre-trained language models for the binary\nsentence-level sentiment classification task. Specifically, these methods\nutilize few-shot learning settings to fine-tune the sentiment classification\nmodel using manual or automatically generated prompts. However, the performance\nof these methods is sensitive to the perturbations of the utilized prompts.\nFurthermore, these methods depend on a few labeled instances for automatic\nprompt generation and prompt ranking. This study aims to find high-quality\nprompts for the given task in a zero-shot setting. Given a base prompt, our\nproposed approach automatically generates multiple prompts similar to the base\nprompt employing positional, reasoning, and paraphrasing techniques and then\nranks the prompts using a novel metric. We empirically demonstrate that the\ntop-ranked prompts are high-quality and significantly outperform the base\nprompt and the prompts generated using few-shot learning for the binary\nsentence-level sentiment classification task.", "published": "2023-05-25 03:36:43", "link": "http://arxiv.org/abs/2305.15689v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Pre-Trained BERT Models for Code-Mixed\n  Hindi-English Data", "abstract": "The term \"Code Mixed\" refers to the use of more than one language in the same\ntext. This phenomenon is predominantly observed on social media platforms, with\nan increasing amount of adaptation as time goes on. It is critical to detect\nforeign elements in a language and process them correctly, as a considerable\nnumber of individuals are using code-mixed languages that could not be\ncomprehended by understanding one of those languages. In this work, we focus on\nlow-resource Hindi-English code-mixed language and enhancing the performance of\ndifferent code-mixed natural language processing tasks such as sentiment\nanalysis, emotion recognition, and hate speech identification. We perform a\ncomparative analysis of different Transformer-based language Models pre-trained\nusing unsupervised approaches. We have included the code-mixed models like\nHingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like\nAlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-English\ndownstream tasks. We report state-of-the-art results on respective datasets\nusing HingBERT-based models which are specifically pre-trained on real\ncode-mixed text. Our HingBERT-based models provide significant improvements\nthus highlighting the poor performance of vanilla BERT models on code-mixed\ntext.", "published": "2023-05-25 05:10:28", "link": "http://arxiv.org/abs/2305.15722v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Text-to-Speech Synthesis for Turkic Languages Using\n  Transliteration", "abstract": "This work aims to build a multilingual text-to-speech (TTS) synthesis system\nfor ten lower-resourced Turkic languages: Azerbaijani, Bashkir, Kazakh, Kyrgyz,\nSakha, Tatar, Turkish, Turkmen, Uyghur, and Uzbek. We specifically target the\nzero-shot learning scenario, where a TTS model trained using the data of one\nlanguage is applied to synthesise speech for other, unseen languages. An\nend-to-end TTS system based on the Tacotron 2 architecture was trained using\nonly the available data of the Kazakh language. To generate speech for the\nother Turkic languages, we first mapped the letters of the Turkic alphabets\nonto the symbols of the International Phonetic Alphabet (IPA), which were then\nconverted to the Kazakh alphabet letters. To demonstrate the feasibility of the\nproposed approach, we evaluated the multilingual Turkic TTS model subjectively\nand obtained promising results. To enable replication of the experiments, we\nmake our code and dataset publicly available in our GitHub repository.", "published": "2023-05-25 05:57:54", "link": "http://arxiv.org/abs/2305.15749v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Healing Unsafe Dialogue Responses with Weak Supervision Signals", "abstract": "Recent years have seen increasing concerns about the unsafe response\ngeneration of large-scale dialogue systems, where agents will learn offensive\nor biased behaviors from the real-world corpus. Some methods are proposed to\naddress the above issue by detecting and replacing unsafe training examples in\na pipeline style. Though effective, they suffer from a high annotation cost and\nadapt poorly to unseen scenarios as well as adversarial attacks. Besides, the\nneglect of providing safe responses (e.g. simply replacing with templates) will\ncause the information-missing problem of dialogues. To address these issues, we\npropose an unsupervised pseudo-label sampling method, TEMP, that can\nautomatically assign potential safe responses. Specifically, our TEMP method\ngroups responses into several clusters and samples multiple labels with an\nadaptively sharpened sampling strategy, inspired by the observation that unsafe\nsamples in the clusters are usually few and distribute in the tail. Extensive\nexperiments in chitchat and task-oriented dialogues show that our TEMP\noutperforms state-of-the-art models with weak supervision signals and obtains\ncomparable results under unsupervised learning settings.", "published": "2023-05-25 06:15:53", "link": "http://arxiv.org/abs/2305.15757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MERGE: Fast Private Text Generation", "abstract": "The drastic increase in language models' parameters has led to a new trend of\ndeploying models in cloud servers, raising growing concerns about private\ninference for Transformer-based models. Existing two-party privacy-preserving\ntechniques, however, only take into account natural language understanding\n(NLU) scenarios. Private inference in natural language generation (NLG),\ncrucial for applications like translation and code completion, remains\nunderexplored.In addition, previous privacy-preserving techniques suffer from\nconvergence issues during model training and exhibit poor inference speed when\nused with NLG models due to the neglect of time-consuming operations in\nauto-regressive generations. To address these issues, we propose a fast private\ntext generation framework for Transformer-based language models, namely\nMERGE.MERGE reuses the output hidden state as the word embedding to bypass the\nembedding computation and reorganize the linear operations in the Transformer\nmodule to accelerate the forward procedure. Extensive experiments show that\nMERGE achieves a 26.5x speedup to the vanilla encrypted model under the\nsequence length 512, and reduces 80\\% communication cost, with an up to 10x\nspeedup to state-of-the-art approximated models.", "published": "2023-05-25 06:27:19", "link": "http://arxiv.org/abs/2305.15769v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive\n  Transformers", "abstract": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard\nto scale to long sequences. Despite several works trying to reduce their\ncomputational cost, most of LLMs still adopt attention layers between all pairs\nof tokens in the sequence, thus incurring a quadratic cost. In this study, we\npresent a novel approach that dynamically prunes contextual information while\npreserving the model's expressiveness, resulting in reduced memory and\ncomputational requirements during inference. Our method employs a learnable\nmechanism that determines which uninformative tokens can be dropped from the\ncontext at any point across the generation process. By doing so, our approach\nnot only addresses performance concerns but also enhances interpretability,\nproviding valuable insight into the model's decision-making process. Our\ntechnique can be applied to existing pre-trained models through a\nstraightforward fine-tuning process, and the pruning strength can be specified\nby a sparsity parameter. Notably, our empirical findings demonstrate that we\ncan effectively prune up to 80\\% of the context without significant performance\ndegradation on downstream tasks, offering a valuable tool for mitigating\ninference costs. Our reference implementation achieves up to $2\\times$ increase\nin inference throughput and even greater memory savings.", "published": "2023-05-25 07:39:41", "link": "http://arxiv.org/abs/2305.15805v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointprop: Joint Semi-supervised Learning for Entity and Relation\n  Extraction with Heterogeneous Graph-based Propagation", "abstract": "Semi-supervised learning has been an important approach to address challenges\nin extracting entities and relations from limited data. However, current\nsemi-supervised works handle the two tasks (i.e., Named Entity Recognition and\nRelation Extraction) separately and ignore the cross-correlation of entity and\nrelation instances as well as the existence of similar instances across\nunlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous\nGraph-based Propagation framework for joint semi-supervised entity and relation\nextraction, which captures the global structure information between individual\ntasks and exploits interactions within unlabeled data. Specifically, we\nconstruct a unified span-based heterogeneous graph from entity and relation\ncandidates and propagate class labels based on confidence scores. We then\nemploy a propagation learning scheme to leverage the affinities between\nlabelled and unlabeled samples. Experiments on benchmark datasets show that our\nframework outperforms the state-of-the-art semi-supervised approaches on NER\nand RE tasks. We show that the joint semi-supervised learning of the two tasks\nbenefits from their codependency and validates the importance of utilizing the\nshared information between unlabeled data.", "published": "2023-05-25 09:07:04", "link": "http://arxiv.org/abs/2305.15872v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linguistic Properties of Truthful Response", "abstract": "We investigate the phenomenon of an LLM's untruthful response using a large\nset of 220 handcrafted linguistic features. We focus on GPT-3 models and find\nthat the linguistic profiles of responses are similar across model sizes. That\nis, how varying-sized LLMs respond to given prompts stays similar on the\nlinguistic properties level. We expand upon this finding by training support\nvector machines that rely only upon the stylistic components of model responses\nto classify the truthfulness of statements. Though the dataset size limits our\ncurrent findings, we show the possibility that truthfulness detection is\npossible without evaluating the content itself. But at the same time, the\nlimited scope of our experiments must be taken into account in interpreting the\nresults.", "published": "2023-05-25 09:17:39", "link": "http://arxiv.org/abs/2305.15875v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LFTK: Handcrafted Features in Computational Linguistics", "abstract": "Past research has identified a rich set of handcrafted linguistic features\nthat can potentially assist various tasks. However, their extensive number\nmakes it difficult to effectively select and utilize existing handcrafted\nfeatures. Coupled with the problem of inconsistent implementation across\nresearch works, there has been no categorization scheme or generally-accepted\nfeature names. This creates unwanted confusion. Also, most existing handcrafted\nfeature extraction libraries are not open-source or not actively maintained. As\na result, a researcher often has to build such an extraction system from the\nground up.\n  We collect and categorize more than 220 popular handcrafted features grounded\non past literature. Then, we conduct a correlation analysis study on several\ntask-specific datasets and report the potential use cases of each feature.\nLastly, we devise a multilingual handcrafted linguistic feature extraction\nsystem in a systematically expandable manner. We open-source our system for\npublic access to a rich set of pre-implemented handcrafted features. Our system\nis coined LFTK and is the largest of its kind. Find it at\ngithub.com/brucewlee/lftk.", "published": "2023-05-25 09:20:27", "link": "http://arxiv.org/abs/2305.15878v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Private Meeting Summarization Without Performance Loss", "abstract": "Meeting summarization has an enormous business potential, but in addition to\nbeing a hard problem, roll-out is challenged by privacy concerns. We explore\nthe problem of meeting summarization under differential privacy constraints and\nfind, to our surprise, that while differential privacy leads to slightly lower\nperformance on in-sample data, differential privacy improves performance when\nevaluated on unseen meeting types. Since meeting summarization systems will\nencounter a great variety of meeting types in practical employment scenarios,\nthis observation makes safe meeting summarization seem much more feasible. We\nperform extensive error analysis and identify potential risks in meeting\nsummarization under differential privacy, including a faithfulness analysis.", "published": "2023-05-25 09:48:50", "link": "http://arxiv.org/abs/2305.15894v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Do You Hear The People Sing? Key Point Analysis via Iterative Clustering\n  and Abstractive Summarisation", "abstract": "Argument summarisation is a promising but currently under-explored field.\nRecent work has aimed to provide textual summaries in the form of concise and\nsalient short texts, i.e., key points (KPs), in a task known as Key Point\nAnalysis (KPA). One of the main challenges in KPA is finding high-quality key\npoint candidates from dozens of arguments even in a small corpus. Furthermore,\nevaluating key points is crucial in ensuring that the automatically generated\nsummaries are useful. Although automatic methods for evaluating summarisation\nhave considerably advanced over the years, they mainly focus on sentence-level\ncomparison, making it difficult to measure the quality of a summary (a set of\nKPs) as a whole. Aggravating this problem is the fact that human evaluation is\ncostly and unreproducible. To address the above issues, we propose a two-step\nabstractive summarisation framework based on neural topic modelling with an\niterative clustering procedure, to generate key points which are aligned with\nhow humans identify key points. Our experiments show that our framework\nadvances the state of the art in KPA, with performance improvement of up to 14\n(absolute) percentage points, in terms of both ROUGE and our own proposed\nevaluation metrics. Furthermore, we evaluate the generated summaries using a\nnovel set-based evaluation toolkit. Our quantitative analysis demonstrates the\neffectiveness of our proposed evaluation metrics in assessing the quality of\ngenerated KPs. Human evaluation further demonstrates the advantages of our\napproach and validates that our proposed evaluation metric is more consistent\nwith human judgment than ROUGE scores.", "published": "2023-05-25 12:43:29", "link": "http://arxiv.org/abs/2305.16000v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UFO: Unified Fact Obtaining for Commonsense Question Answering", "abstract": "Leveraging external knowledge to enhance the reasoning ability is crucial for\ncommonsense question answering. However, the existing knowledge bases heavily\nrely on manual annotation which unavoidably causes deficiency in coverage of\nworld-wide commonsense knowledge. Accordingly, the knowledge bases fail to be\nflexible enough to support the reasoning over diverse questions. Recently,\nlarge-scale language models (LLMs) have dramatically improved the intelligence\nin capturing and leveraging knowledge, which opens up a new way to address the\nissue of eliciting knowledge from language models. We propose a Unified Facts\nObtaining (UFO) approach. UFO turns LLMs into knowledge sources and produces\nrelevant facts (knowledge statements) for the given question. We first develop\na unified prompt consisting of demonstrations that cover different aspects of\ncommonsense and different question styles. On this basis, we instruct the LLMs\nto generate question-related supporting facts for various commonsense questions\nvia prompting. After facts generation, we apply a dense retrieval-based fact\nselection strategy to choose the best-matched fact. This kind of facts will be\nfed into the answer inference model along with the question. Notably, due to\nthe design of unified prompts, UFO can support reasoning in various commonsense\naspects (including general commonsense, scientific commonsense, and social\ncommonsense). Extensive experiments on CommonsenseQA 2.0, OpenBookQA, QASC, and\nSocial IQA benchmarks show that UFO significantly improves the performance of\nthe inference model and outperforms manually constructed knowledge sources.", "published": "2023-05-25 13:25:49", "link": "http://arxiv.org/abs/2305.16048v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake News Detection and Behavioral Analysis: Case of COVID-19", "abstract": "While the world has been combating COVID-19 for over three years, an ongoing\n\"Infodemic\" due to the spread of fake news regarding the pandemic has also been\na global issue. The existence of the fake news impact different aspect of our\ndaily lives, including politics, public health, economic activities, etc.\nReaders could mistake fake news for real news, and consequently have less\naccess to authentic information. This phenomenon will likely cause confusion of\ncitizens and conflicts in society. Currently, there are major challenges in\nfake news research. It is challenging to accurately identify fake news data in\nsocial media posts. In-time human identification is infeasible as the amount of\nthe fake news data is overwhelming. Besides, topics discussed in fake news are\nhard to identify due to their similarity to real news. The goal of this paper\nis to identify fake news on social media to help stop the spread. We present\nDeep Learning approaches and an ensemble approach for fake news detection. Our\ndetection models achieved higher accuracy than previous studies. The ensemble\napproach further improved the detection performance. We discovered feature\ndifferences between fake news and real news items. When we added them into the\nsentence embeddings, we found that they affected the model performance. We\napplied a hybrid method and built models for recognizing topics from posts. We\nfound half of the identified topics were overlapping in fake news and real\nnews, which could increase confusion in the population.", "published": "2023-05-25 13:42:08", "link": "http://arxiv.org/abs/2305.16057v1", "categories": ["cs.LG", "cs.CL", "68"], "primary_category": "cs.LG"}
{"title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic", "abstract": "A primary criticism towards language models (LMs) is their inscrutability.\nThis paper presents evidence that, despite their size and complexity, LMs\nsometimes exploit a simple vector arithmetic style mechanism to solve some\nrelational tasks using regularities encoded in the hidden space of the model\n(e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model\nsizes (from 124M parameters to 176B parameters) in an in-context learning\nsetting, and find that for a variety of tasks (involving capital cities,\nuppercasing, and past-tensing) a key part of the mechanism reduces to a simple\nadditive update typically applied by the feedforward (FFN) networks. We further\nshow that this mechanism is specific to tasks that require retrieval from\npretraining memory, rather than retrieval from local context. Our results\ncontribute to a growing body of work on the interpretability of LMs, and offer\nreason to be optimistic that, despite the massive and non-linear nature of the\nmodels, the strategies they ultimately use to solve tasks can sometimes reduce\nto familiar and even intuitive algorithms.", "published": "2023-05-25 15:04:01", "link": "http://arxiv.org/abs/2305.16130v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive Summary Generation for the Urdu Language", "abstract": "Abstractive summary generation is a challenging task that requires the model\nto comprehend the source text and generate a concise and coherent summary that\ncaptures the essential information. In this paper, we explore the use of an\nencoder/decoder approach for abstractive summary generation in the Urdu\nlanguage. We employ a transformer-based model that utilizes self-attention\nmechanisms to encode the input text and generate a summary. Our experiments\nshow that our model can produce summaries that are grammatically correct and\nsemantically meaningful. We evaluate our model on a publicly available dataset\nand achieve state-of-the-art results in terms of Rouge scores. We also conduct\na qualitative analysis of our model's output to assess its effectiveness and\nlimitations. Our findings suggest that the encoder/decoder approach is a\npromising method for abstractive summary generation in Urdu and can be extended\nto other languages with suitable modifications.", "published": "2023-05-25 15:55:42", "link": "http://arxiv.org/abs/2305.16195v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 03B65, 91F20 (Secondary)", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Diversity-Aware Coherence Loss for Improving Neural Topic Models", "abstract": "The standard approach for neural topic modeling uses a variational\nautoencoder (VAE) framework that jointly minimizes the KL divergence between\nthe estimated posterior and prior, in addition to the reconstruction loss.\nSince neural topic models are trained by recreating individual input documents,\nthey do not explicitly capture the coherence between topic words on the corpus\nlevel. In this work, we propose a novel diversity-aware coherence loss that\nencourages the model to learn corpus-level coherence scores while maintaining a\nhigh diversity between topics. Experimental results on multiple datasets show\nthat our method significantly improves the performance of neural topic models\nwithout requiring any pretraining or additional parameters.", "published": "2023-05-25 16:01:56", "link": "http://arxiv.org/abs/2305.16199v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Natural Language Processing for Long Texts: A Survey on\n  Classification and Summarization", "abstract": "The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded online renders automated\nunderstanding of lengthy texts a critical issue. Relevant applications include\nautomated Web mining, legal document review, medical records analysis,\nfinancial reports analysis, contract management, environmental impact\nassessment, news aggregation, etc. Despite the relatively recent development of\nefficient algorithms for analyzing long documents, practical tools in this\nfield are currently flourishing. This article serves as an entry point into\nthis dynamic domain and aims to achieve two objectives. First of all, it\nprovides an introductory overview of the relevant neural building blocks,\nserving as a concise tutorial for the field. Secondly, it offers a brief\nexamination of the current state-of-the-art in two key long document analysis\ntasks: document classification and document summarization. Sentiment analysis\nfor long texts is also covered, since it is typically treated as a particular\ncase of document classification. Consequently, this article presents an\nintroductory exploration of document-level analysis, addressing the primary\nchallenges, concerns, and existing solutions. Finally, it offers a concise\ndefinition of \"long text/document\", presents an original overarching taxonomy\nof common deep neural methods for long document analysis and lists publicly\navailable annotated datasets that can facilitate further research in this area.", "published": "2023-05-25 17:13:44", "link": "http://arxiv.org/abs/2305.16259v6", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Landmark Attention: Random-Access Infinite Context Length for\n  Transformers", "abstract": "While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.", "published": "2023-05-25 17:53:42", "link": "http://arxiv.org/abs/2305.16300v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PandaGPT: One Model To Instruction-Follow Them All", "abstract": "We present PandaGPT, an approach to emPower large lANguage moDels with visual\nand Auditory instruction-following capabilities. Our pilot experiments show\nthat PandaGPT can perform complex tasks such as detailed image description\ngeneration, writing stories inspired by videos, and answering questions about\naudios. More interestingly, PandaGPT can take multimodal inputs simultaneously\nand compose their semantics naturally. For example, PandaGPT can connect how\nobjects look in an image/video and how they sound in an audio. To do so,\nPandaGPT combines the multimodal encoders from ImageBind and the large language\nmodels from Vicuna. Notably, only aligned image-text pairs are required for the\ntraining of PandaGPT. Thanks to the strong capability of ImageBind in embedding\ndata from different modalities into the same space, PandaGPT displays emergent,\ni.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,\nvideo, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an\ninitial step toward building AGI that can perceive and understand inputs in\ndifferent modalities holistically, as we humans do. Our project page is at\nhttps://panda-gpt.github.io/.", "published": "2023-05-25 04:16:07", "link": "http://arxiv.org/abs/2305.16355v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Counterfactual Probing for the Influence of Affect and Specificity on\n  Intergroup Bias", "abstract": "While existing work on studying bias in NLP focues on negative or pejorative\nlanguage use, Govindarajan et al. (2023) offer a revised framing of bias in\nterms of intergroup social context, and its effects on language behavior. In\nthis paper, we investigate if two pragmatic features (specificity and affect)\nsystematically vary in different intergroup contexts -- thus connecting this\nnew framing of bias to language output. Preliminary analysis finds modest\ncorrelations between specificity and affect of tweets with supervised\nintergroup relationship (IGR) labels. Counterfactual probing further reveals\nthat while neural models finetuned for predicting IGR labels reliably use\naffect in classification, the model's usage of specificity is inconclusive.\nCode and data can be found at: https://github.com/venkatasg/intergroup-probing", "published": "2023-05-25 18:19:39", "link": "http://arxiv.org/abs/2305.16409v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Measuring the Effect of Influential Messages on Varying Personas", "abstract": "Predicting how a user responds to news events enables important applications\nsuch as allowing intelligent agents or content producers to estimate the effect\non different communities and revise unreleased messages to prevent unexpected\nbad outcomes such as social conflict and moral injury. We present a new task,\nResponse Forecasting on Personas for News Media, to estimate the response a\npersona (characterizing an individual or a group) might have upon seeing a news\nmessage. Compared to the previous efforts which only predict generic comments\nto news, the proposed task not only introduces personalization in the modeling\nbut also predicts the sentiment polarity and intensity of each response. This\nenables more accurate and comprehensive inference on the mental state of the\npersona. Meanwhile, the generated sentiment dimensions make the evaluation and\napplication more reliable. We create the first benchmark dataset, which\nconsists of 13,357 responses to 3,847 news headlines from Twitter. We further\nevaluate the SOTA neural language models with our dataset. The empirical\nresults suggest that the included persona attributes are helpful for the\nperformance of all response dimensions. Our analysis shows that the\nbest-performing models are capable of predicting responses that are consistent\nwith the personas, and as a byproduct, the task formulation also enables many\ninteresting applications in the analysis of social network groups and their\nopinions, such as the discovery of extreme opinion groups.", "published": "2023-05-25 21:01:00", "link": "http://arxiv.org/abs/2305.16470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Label Agnostic Pre-training for Zero-shot Text Classification", "abstract": "Conventional approaches to text classification typically assume the existence\nof a fixed set of predefined labels to which a given text can be classified.\nHowever, in real-world applications, there exists an infinite label space for\ndescribing a given text. In addition, depending on the aspect (sentiment,\ntopic, etc.) and domain of the text (finance, legal, etc.), the interpretation\nof the label can vary greatly. This makes the task of text classification,\nparticularly in the zero-shot scenario, extremely challenging. In this paper,\nwe investigate the task of zero-shot text classification with the aim of\nimproving the ability of pre-trained language models (PLMs) to generalize to\nboth seen and unseen data across varying aspects and domains. To solve this we\nintroduce two new simple yet effective pre-training strategies, Implicit and\nExplicit pre-training. These methods inject aspect-level understanding into the\nmodel at train time with the goal of conditioning the model to build task-level\nunderstanding. To evaluate this, we construct and release UTCD, a new benchmark\ndataset for evaluating text classification in zero-shot settings. Experimental\nresults on UTCD show that our approach achieves improved zero-shot\ngeneralization on a suite of challenging datasets across an array of zero-shot\nformalizations.", "published": "2023-05-25 22:55:32", "link": "http://arxiv.org/abs/2305.16521v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity\n  Challenges: Early Quantitative Insights through Sentiment Analysis and Word\n  Frequency Analysis", "abstract": "The exponential growth in user acquisition and popularity of OpenAIs ChatGPT,\nan artificial intelligence(AI) powered chatbot, was accompanied by widespread\nmainstream media coverage. This article presents a quantitative data analysis\nof the early trends and sentiments revealed by conducting text mining and NLP\nmethods onto a corpus of 10,902 mainstream news headlines related to the\nsubject of ChatGPT and artificial intelligence, from the launch of ChatGPT in\nNovember 2022 to March 2023. The findings revealed in sentiment analysis,\nChatGPT and artificial intelligence, were perceived more positively than\nnegatively in the mainstream media. In regards to word frequency results, over\nsixty-five percent of the top frequency words were focused on Big Tech issues\nand actors while topics such as jobs, diversity, ethics, copyright, gender and\nwomen were poorly represented or completely absent and only accounted for six\npercent of the total corpus. This article is a critical analysis into the power\nstructures and collusions between Big Tech and Big Media in their hegemonic\nexclusion of diversity and job challenges from mainstream media.", "published": "2023-05-25 15:10:51", "link": "http://arxiv.org/abs/2305.18340v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR", "abstract": "End-to-end models with large capacity have significantly improved\nmultilingual automatic speech recognition, but their computation cost poses\nchallenges for on-device applications. We propose a streaming truly\nmultilingual Conformer incorporating mixture-of-expert (MoE) layers that learn\nto only activate a subset of parameters in training and inference. The MoE\nlayer consists of a softmax gate which chooses the best two experts among many\nin forward propagation. The proposed MoE layer offers efficient inference by\nactivating a fixed number of parameters as the number of experts increases. We\nevaluate the proposed model on a set of 12 languages, and achieve an average\n11.9% relative improvement in WER over the baseline. Compared to an adapter\nmodel using ground truth information, our MoE model achieves similar WER and\nactivates similar number of parameters but without any language information. We\nfurther show around 3% relative WER improvement by multilingual shallow fusion.", "published": "2023-05-25 02:16:32", "link": "http://arxiv.org/abs/2305.15663v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Svarah: Evaluating English ASR Systems on Indian Accents", "abstract": "India is the second largest English-speaking country in the world with a\nspeaker base of roughly 130 million. Thus, it is imperative that automatic\nspeech recognition (ASR) systems for English should be evaluated on Indian\naccents. Unfortunately, Indian speakers find a very poor representation in\nexisting English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent\nArchive, etc. In this work, we address this gap by creating Svarah, a benchmark\nthat contains 9.6 hours of transcribed English audio from 117 speakers across\n65 geographic locations throughout India, resulting in a diverse range of\naccents. Svarah comprises both read speech and spontaneous conversational data,\ncovering various domains, such as history, culture, tourism, etc., ensuring a\ndiverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR\nsystems on Svarah and show that there is clear scope for improvement on Indian\naccents. Svarah as well as all our code will be publicly available.", "published": "2023-05-25 06:20:29", "link": "http://arxiv.org/abs/2305.15760v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-contradictory Hallucinations of Large Language Models: Evaluation,\n  Detection and Mitigation", "abstract": "Large language models (large LMs) are susceptible to producing text that\ncontains hallucinated content. An important instance of this problem is\nself-contradiction, where the LM generates two contradictory sentences within\nthe same context. In this work, we present a comprehensive investigation into\nself-contradiction for various instruction-tuned LMs, covering evaluation,\ndetection, and mitigation. Our primary evaluation task is open-domain text\ngeneration, but we also demonstrate the applicability of our approach to\nshorter question answering. Our analysis reveals the prevalence of\nself-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We\nthen propose a novel prompting-based framework designed to effectively detect\nand mitigate self-contradictions. Our detector achieves high accuracy, e.g.,\naround 80% F1 score when prompting ChatGPT. The mitigation algorithm\niteratively refines the generated text to remove contradictory information\nwhile preserving text fluency and informativeness. Importantly, our entire\nframework is applicable to black-box LMs and does not require retrieval of\nexternal knowledge. Rather, our method complements retrieval-based methods, as\na large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be\nverified using online text. Our approach is practically effective and has been\nreleased as a push-button tool to benefit the public at\nhttps://chatprotect.ai/.", "published": "2023-05-25 08:43:46", "link": "http://arxiv.org/abs/2305.15852v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential Integrated Gradients: a simple but effective method for\n  explaining language models", "abstract": "Several explanation methods such as Integrated Gradients (IG) can be\ncharacterised as path-based methods, as they rely on a straight line between\nthe data and an uninformative baseline. However, when applied to language\nmodels, these methods produce a path for each word of a sentence\nsimultaneously, which could lead to creating sentences from interpolated words\neither having no clear meaning, or having a significantly different meaning\ncompared to the original sentence. In order to keep the meaning of these\nsentences as close as possible to the original one, we propose Sequential\nIntegrated Gradients (SIG), which computes the importance of each word in a\nsentence by keeping fixed every other words, only creating interpolations\nbetween the baseline and the word of interest. Moreover, inspired by the\ntraining procedure of several language models, we also propose to replace the\nbaseline token \"pad\" with the trained token \"mask\". While being a simple\nimprovement over the original IG method, we show on various models and datasets\nthat SIG proves to be a very effective method for explaining language models.", "published": "2023-05-25 08:44:11", "link": "http://arxiv.org/abs/2305.15853v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Text Representations for Terms and Phrases in Technical\n  Domains", "abstract": "Extracting dense representations for terms and phrases is a task of great\nimportance for knowledge discovery platforms targeting highly-technical fields.\nDense representations are used as features for downstream components and have\nmultiple applications ranging from ranking results in search to summarization.\nCommon approaches to create dense representations include training\ndomain-specific embeddings with self-supervised setups or using sentence\nencoder models trained over similarity tasks. In contrast to static embeddings,\nsentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but\nimpose significant computational costs. In this paper, we propose a fully\nunsupervised approach to text encoding that consists of training small\ncharacter-based models with the objective of reconstructing large pre-trained\nembedding matrices. Models trained with this approach can not only match the\nquality of sentence encoders in technical domains, but are 5 times smaller and\nup to 10 times faster, even on high-end GPUs.", "published": "2023-05-25 08:59:36", "link": "http://arxiv.org/abs/2305.15867v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by\n  Leveraging Unstructured Context in Neural Machine Translation", "abstract": "Efficient utilisation of both intra- and extra-textual context remains one of\nthe critical gaps between machine and human translation. Existing research has\nprimarily focused on providing individual, well-defined types of context in\ntranslation, such as the surrounding text or discrete external variables like\nthe speaker's gender. This work introduces MTCue, a novel neural machine\ntranslation (NMT) framework that interprets all context (including discrete\nvariables) as text. MTCue learns an abstract representation of context,\nenabling transferability across different data settings and leveraging similar\nattributes in low-resource scenarios. With a focus on a dialogue domain with\naccess to document and metadata context, we extensively evaluate MTCue in four\nlanguage pairs in both translation directions. Our framework demonstrates\nsignificant improvements in translation quality over a parameter-matched\nnon-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).\nMoreover, MTCue significantly outperforms a \"tagging\" baseline at translating\nEnglish text. Analysis reveals that the context encoder of MTCue learns a\nrepresentation space that organises context based on specific attributes, such\nas formality, enabling effective zero-shot control. Pre-training on context\nembeddings also improves MTCue's few-shot performance compared to the \"tagging\"\nbaseline. Finally, an ablation study conducted on model components and\ncontextual variables further supports the robustness of MTCue for context-based\nNMT.", "published": "2023-05-25 10:06:08", "link": "http://arxiv.org/abs/2305.15904v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched\n  Contextualization", "abstract": "Memes are a powerful tool for communication over social media. Their affinity\nfor evolving across politics, history, and sociocultural phenomena makes them\nan ideal communication vehicle. To comprehend the subtle message conveyed\nwithin a meme, one must understand the background that facilitates its holistic\nassimilation. Besides digital archiving of memes and their metadata by a few\nwebsites like knowyourmeme.com, currently, there is no efficient way to deduce\na meme's context dynamically. In this work, we propose a novel task, MEMEX -\ngiven a meme and a related document, the aim is to mine the context that\nsuccinctly explains the background of the meme. At first, we develop MCC (Meme\nContext Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we\npropose MIME (MultImodal Meme Explainer), a multimodal neural framework that\nuses common sense enriched meme representation and a layered approach to\ncapture the cross-modal semantic dependencies between the meme and the context.\nMIME surpasses several unimodal and multimodal systems and yields an absolute\nimprovement of ~ 4% F1-score over the best baseline. Lastly, we conduct\ndetailed analyses of MIME's performance, highlighting the aspects that could\nlead to optimal modeling of cross-modal contextual associations.", "published": "2023-05-25 10:19:35", "link": "http://arxiv.org/abs/2305.15913v2", "categories": ["cs.CL", "cs.CY", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Visually grounded few-shot word acquisition with fewer shots", "abstract": "We propose a visually grounded speech model that acquires new words and their\nvisual depictions from just a few word-image example pairs. Given a set of test\nimages and a spoken query, we ask the model which image depicts the query word.\nPrevious work has simplified this problem by either using an artificial setting\nwith digit word-image pairs or by using a large number of examples per class.\nWe propose an approach that can work on natural word-image pairs but with less\nexamples, i.e. fewer shots. Our approach involves using the given word-image\nexample pairs to mine new unsupervised word-image training pairs from large\ncollections of unlabelled speech and images. Additionally, we use a\nword-to-image attention mechanism to determine word-image similarity. With this\nnew model, we achieve better performance with fewer shots than any existing\napproach.", "published": "2023-05-25 11:05:54", "link": "http://arxiv.org/abs/2305.15937v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What about em? How Commercial Machine Translation Fails to Handle\n  (Neo-)Pronouns", "abstract": "As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns,\nwe need more research on identity-inclusive NLP. Exclusion is particularly\nharmful in one of the most popular NLP applications, machine translation (MT).\nWrong pronoun translations can discriminate against marginalized groups, e.g.,\nnon-binary individuals (Dev et al., 2021). In this ``reality check'', we study\nhow three commercial MT systems translate 3rd-person pronouns. Concretely, we\ncompare the translations of gendered vs. gender-neutral pronouns from English\nto five other languages (Danish, Farsi, French, German, Italian), and vice\nversa, from Danish to English. Our error analysis shows that the presence of a\ngender-neutral pronoun often leads to grammatical and semantic translation\nerrors. Similarly, gender neutrality is often not preserved. By surveying the\nopinions of affected native speakers from diverse languages, we provide\nrecommendations to address the issue in future MT research.", "published": "2023-05-25 13:34:09", "link": "http://arxiv.org/abs/2305.16051v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual\n  Impact of Speech and Emotion Recognition", "abstract": "In Speech Emotion Recognition (SER), textual data is often used alongside\naudio signals to address their inherent variability. However, the reliance on\nhuman annotated text in most research hinders the development of practical SER\nsystems. To overcome this challenge, we investigate how Automatic Speech\nRecognition (ASR) performs on emotional speech by analyzing the ASR performance\non emotion corpora and examining the distribution of word errors and confidence\nscores in ASR transcripts to gain insight into how emotion affects ASR. We\nutilize four ASR systems, namely Kaldi ASR, wav2vec2, Conformer, and Whisper,\nand three corpora: IEMOCAP, MOSI, and MELD to ensure generalizability.\nAdditionally, we conduct text-based SER on ASR transcripts with increasing word\nerror rates to investigate how ASR affects SER. The objective of this study is\nto uncover the relationship and mutual impact of ASR and SER, in order to\nfacilitate ASR adaptation to emotional speech and the use of SER in real world.", "published": "2023-05-25 13:56:09", "link": "http://arxiv.org/abs/2305.16065v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Simultaneous Speech Translation with Differentiable\n  Segmentation", "abstract": "End-to-end simultaneous speech translation (SimulST) outputs translation\nwhile receiving the streaming speech inputs (a.k.a. streaming speech\ntranslation), and hence needs to segment the speech inputs and then translate\nbased on the current received speech. However, segmenting the speech inputs at\nunfavorable moments can disrupt the acoustic integrity and adversely affect the\nperformance of the translation model. Therefore, learning to segment the speech\ninputs at those moments that are beneficial for the translation model to\nproduce high-quality translation is the key to SimulST. Existing SimulST\nmethods, either using the fixed-length segmentation or external segmentation\nmodel, always separate segmentation from the underlying translation model,\nwhere the gap results in segmentation outcomes that are not necessarily\nbeneficial for the translation process. In this paper, we propose\nDifferentiable Segmentation (DiSeg) for SimulST to directly learn segmentation\nfrom the underlying translation model. DiSeg turns hard segmentation into\ndifferentiable through the proposed expectation training, enabling it to be\njointly trained with the translation model and thereby learn\ntranslation-beneficial segmentation. Experimental results demonstrate that\nDiSeg achieves state-of-the-art performance and exhibits superior segmentation\ncapability.", "published": "2023-05-25 14:25:12", "link": "http://arxiv.org/abs/2305.16093v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ChatBridge: Bridging Modalities with Large Language Model as a Language\n  Catalyst", "abstract": "Building general-purpose models that can perceive diverse real-world\nmodalities and solve various tasks is an appealing target in artificial\nintelligence. In this paper, we present ChatBridge, a novel multimodal language\nmodel that leverages the expressive capabilities of language as the catalyst to\nbridge the gap between various modalities. We show that only language-paired\ntwo-modality data is sufficient to connect all modalities. ChatBridge leverages\nrecent large language models (LLM) and extends their zero-shot capabilities to\nincorporate diverse multimodal inputs. ChatBridge undergoes a two-stage\ntraining. The first stage aligns each modality with language, which brings\nemergent multimodal correlation and collaboration abilities. The second stage\ninstruction-finetunes ChatBridge to align it with user intent with our newly\nproposed multimodal instruction tuning dataset, named MULTIS, which covers a\nwide range of 16 multimodal tasks of text, image, video, and audio modalities.\nWe show strong quantitative and qualitative results on zero-shot multimodal\ntasks covering text, image, video, and audio modalities. All codes, data, and\nmodels of ChatBridge will be open-sourced.", "published": "2023-05-25 14:34:08", "link": "http://arxiv.org/abs/2305.16103v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis,\n  and Translation", "abstract": "Recent research shows a big convergence in model architecture, training\nobjectives, and inference methods across various tasks for different\nmodalities. In this paper, we propose VioLA, a single auto-regressive\nTransformer decoder-only network that unifies various cross-modal tasks\ninvolving speech and text, such as speech-to-text, text-to-text,\ntext-to-speech, and speech-to-speech tasks, as a conditional codec language\nmodel task via multi-task learning framework. To accomplish this, we first\nconvert all the speech utterances to discrete tokens (similar to the textual\ndata) using an offline neural codec encoder. In such a way, all these tasks are\nconverted to token-based sequence conversion problems, which can be naturally\nhandled with one conditional language model. We further integrate task IDs\n(TID) and language IDs (LID) into the proposed model to enhance the modeling\ncapability of handling different languages and tasks. Experimental results\ndemonstrate that the proposed VioLA model can support both single-modal and\ncross-modal tasks well, and the decoder-only model achieves a comparable and\neven better performance than the strong baselines.", "published": "2023-05-25 14:39:47", "link": "http://arxiv.org/abs/2305.16107v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Feature Collapse", "abstract": "We formalize and study a phenomenon called feature collapse that makes\nprecise the intuitive idea that entities playing a similar role in a learning\ntask receive similar representations. As feature collapse requires a notion of\ntask, we leverage a simple but prototypical NLP task to study it. We start by\nshowing experimentally that feature collapse goes hand in hand with\ngeneralization. We then prove that, in the large sample limit, distinct words\nthat play identical roles in this NLP task receive identical local feature\nrepresentations in a neural network. This analysis reveals the crucial role\nthat normalization mechanisms, such as LayerNorm, play in feature collapse and\nin generalization.", "published": "2023-05-25 15:25:34", "link": "http://arxiv.org/abs/2305.16162v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Passive learning of active causal strategies in agents and language\n  models", "abstract": "What can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.", "published": "2023-05-25 15:39:46", "link": "http://arxiv.org/abs/2305.16183v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scaling Data-Constrained Language Models", "abstract": "The current trend of scaling language models involves increasing both\nparameter count and training dataset size. Extrapolating this trend suggests\nthat training dataset size may soon be limited by the amount of text data\navailable on the internet. Motivated by this limit, we investigate scaling\nlanguage models in data-constrained regimes. Specifically, we run a large set\nof experiments varying the extent of data repetition and compute budget,\nranging up to 900 billion training tokens and 9 billion parameter models. We\nfind that with constrained data for a fixed compute budget, training with up to\n4 epochs of repeated data yields negligible changes to loss compared to having\nunique data. However, with more repetition, the value of adding compute\neventually decays to zero. We propose and empirically validate a scaling law\nfor compute optimality that accounts for the decreasing value of repeated\ntokens and excess parameters. Finally, we experiment with approaches mitigating\ndata scarcity, including augmenting the training dataset with code data or\nremoving commonly used filters. Models and datasets from our 400 training runs\nare freely available at https://github.com/huggingface/datablations.", "published": "2023-05-25 17:18:55", "link": "http://arxiv.org/abs/2305.16264v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Betray Oneself: A Novel Audio DeepFake Detection Model via\n  Mono-to-Stereo Conversion", "abstract": "Audio Deepfake Detection (ADD) aims to detect the fake audio generated by\ntext-to-speech (TTS), voice conversion (VC) and replay, etc., which is an\nemerging topic. Traditionally we take the mono signal as input and focus on\nrobust feature extraction and effective classifier design. However, the\ndual-channel stereo information in the audio signal also includes important\ncues for deepfake, which has not been studied in the prior work. In this paper,\nwe propose a novel ADD model, termed as M2S-ADD, that attempts to discover\naudio authenticity cues during the mono-to-stereo conversion process. We first\nprojects the mono to a stereo signal using a pretrained stereo synthesizer,\nthen employs a dual-branch neural architecture to process the left and right\nchannel signals, respectively. In this way, we effectively reveal the artifacts\nin the fake audio, thus improve the ADD performance. The experiments on the\nASVspoof2019 database show that M2S-ADD outperforms all baselines that input\nmono. We release the source code at \\url{https://github.com/AI-S2-Lab/M2S-ADD}.", "published": "2023-05-25 02:54:29", "link": "http://arxiv.org/abs/2305.16353v1", "categories": ["cs.SD", "cs.AI", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal\n  Theorem Proving", "abstract": "Large language models~(LLMs) present an intriguing avenue of exploration in\nthe domain of formal theorem proving. Nonetheless, the full utilization of\nthese models, particularly in terms of demonstration formatting and\norganization, remains an underexplored area. In an endeavor to enhance the\nefficacy of LLMs, we introduce a subgoal-based demonstration learning\nframework, consisting of two primary elements: Firstly, drawing upon the\ninsights of subgoal learning from the domains of reinforcement learning and\nrobotics, we propose the construction of distinct subgoals for each\ndemonstration example and refine these subgoals in accordance with the\npertinent theories of subgoal learning. Secondly, we build upon recent advances\nin diffusion models to predict the optimal organization, simultaneously\naddressing two intricate issues that persist within the domain of demonstration\norganization: subset selection and order determination. Through the integration\nof subgoal-based learning methodologies, we have successfully increased the\nprevailing proof accuracy from 38.9\\% to 44.3\\% on the miniF2F benchmark.\nFurthermore, the adoption of diffusion models for demonstration organization\ncan lead to an additional enhancement in accuracy to 45.5\\%, or a $5\\times$\nimprovement in sampling efficiency compared with the long-standing\nstate-of-the-art method. Our code is available at\n\\url{https://github.com/HKUNLP/subgoal-theorem-prover}.", "published": "2023-05-25 11:35:52", "link": "http://arxiv.org/abs/2305.16366v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Role-Play with Large Language Models", "abstract": "As dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.", "published": "2023-05-25 11:36:52", "link": "http://arxiv.org/abs/2305.16367v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced\n  Non-Native Speech Recognition", "abstract": "Automatic Speech Recognition (ASR) systems have attained unprecedented\nperformance with large speech models pre-trained based on self-supervised\nspeech representation learning. However, these pre-trained speech models suffer\nfrom representational bias as they tend to better represent those prominent\naccents (i.e., native (L1) English accent) in the pre-training speech corpus\nthan less represented accents, resulting in a deteriorated performance for\nnon-native (L2) English accents. Although there have been some approaches to\nmitigate this issue, all of these methods require updating the pre-trained\nmodel weights. In this paper, we propose Information Theoretic Adversarial\nPrompt Tuning (INTapt), which introduces prompts concatenated to the original\ninput that can re-modulate the attention of the pre-trained model such that the\ncorresponding input resembles a native (L1) English speech without updating the\nbackbone weights. INTapt is trained simultaneously in the following two\nmanners: (1) adversarial training to reduce accent feature dependence between\nthe original input and the prompt-concatenated input and (2) training to\nminimize CTC loss for improving ASR performance to a prompt-concatenated input.\nExperimental results show that INTapt improves the performance of L2 English\nand increases feature similarity between L2 and L1 accents.", "published": "2023-05-25 13:06:01", "link": "http://arxiv.org/abs/2305.16371v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer", "abstract": "Transformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).", "published": "2023-05-25 15:59:13", "link": "http://arxiv.org/abs/2305.16380v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Diffusion Models Vision-And-Language Reasoners?", "abstract": "Text-conditioned image generation models have recently shown immense\nqualitative success using denoising diffusion processes. However, unlike\ndiscriminative vision-and-language models, it is a non-trivial task to subject\nthese diffusion-based generative models to automatic fine-grained quantitative\nevaluation of high-level phenomena such as compositionality. Towards this goal,\nwe perform two innovations. First, we transform diffusion-based models (in our\ncase, Stable Diffusion) for any image-text matching (ITM) task using a novel\nmethod called DiffusionITM. Second, we introduce the Generative-Discriminative\nEvaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language\ntasks, bias evaluation and detailed analysis. We find that Stable Diffusion +\nDiffusionITM is competitive on many tasks and outperforms CLIP on compositional\ntasks like like CLEVR and Winoground. We further boost its compositional\nperformance with a transfer setup by fine-tuning on MS-COCO while retaining\ngenerative capabilities. We also measure the stereotypical bias in diffusion\nmodels, and find that Stable Diffusion 2.1 is, for the most part, less biased\nthan Stable Diffusion 1.5. Overall, our results point in an exciting direction\nbringing discriminative and generative model evaluation closer. We will release\ncode and benchmark setup soon.", "published": "2023-05-25 18:02:22", "link": "http://arxiv.org/abs/2305.16397v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Machine Translation for Mathematical Formulae", "abstract": "We tackle the problem of neural machine translation of mathematical formulae\nbetween ambiguous presentation languages and unambiguous content languages.\nCompared to neural machine translation on natural language, mathematical\nformulae have a much smaller vocabulary and much longer sequences of symbols,\nwhile their translation requires extreme precision to satisfy mathematical\ninformation needs. In this work, we perform the tasks of translating from LaTeX\nto Mathematica as well as from LaTeX to semantic LaTeX. While recurrent,\nrecursive, and transformer networks struggle with preserving all contained\ninformation, we find that convolutional sequence-to-sequence networks achieve\n95.1% and 90.7% exact matches, respectively.", "published": "2023-05-25 19:15:06", "link": "http://arxiv.org/abs/2305.16433v1", "categories": ["cs.CL", "cs.SC", "stat.AP"], "primary_category": "cs.CL"}
{"title": "On the Tool Manipulation Capability of Open-source Large Language Models", "abstract": "Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.", "published": "2023-05-25 22:10:20", "link": "http://arxiv.org/abs/2305.16504v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory", "abstract": "The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.", "published": "2023-05-25 17:59:49", "link": "http://arxiv.org/abs/2305.17144v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions", "abstract": "With the widespread use of large artificial intelligence (AI) models such as\nChatGPT, AI-generated content (AIGC) has garnered increasing attention and is\nleading a paradigm shift in content creation and knowledge representation. AIGC\nuses generative large AI algorithms to assist or replace humans in creating\nmassive, high-quality, and human-like content at a faster pace and lower cost,\nbased on user-provided prompts. Despite the recent significant progress in\nAIGC, security, privacy, ethical, and legal challenges still need to be\naddressed. This paper presents an in-depth survey of working principles,\nsecurity and privacy threats, state-of-the-art solutions, and future challenges\nof the AIGC paradigm. Specifically, we first explore the enabling technologies,\ngeneral architecture of AIGC, and discuss its working modes and key\ncharacteristics. Then, we investigate the taxonomy of security and privacy\nthreats to AIGC and highlight the ethical and societal implications of GPT and\nAIGC technologies. Furthermore, we review the state-of-the-art AIGC\nwatermarking approaches for regulatable AIGC paradigms regarding the AIGC model\nand its produced content. Finally, we identify future challenges and open\nresearch directions related to AIGC.", "published": "2023-05-25 15:09:11", "link": "http://arxiv.org/abs/2305.18339v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Transformative Effects of ChatGPT on Modern Education: Emerging Era of\n  AI Chatbots", "abstract": "ChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial.", "published": "2023-05-25 17:35:57", "link": "http://arxiv.org/abs/2306.03823v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Undetectable Watermarks for Language Models", "abstract": "Recent advances in the capabilities of large language models such as GPT-4\nhave spurred increasing concern about our ability to detect AI-generated text.\nPrior works have suggested methods of embedding watermarks in model outputs, by\nnoticeably altering the output distribution. We ask: Is it possible to\nintroduce a watermark without incurring any detectable change to the output\ndistribution?\n  To this end we introduce a cryptographically-inspired notion of undetectable\nwatermarks for language models. That is, watermarks can be detected only with\nthe knowledge of a secret key; without the secret key, it is computationally\nintractable to distinguish watermarked outputs from those of the original\nmodel. In particular, it is impossible for a user to observe any degradation in\nthe quality of the text. Crucially, watermarks should remain undetectable even\nwhen the user is allowed to adaptively query the model with arbitrarily chosen\nprompts. We construct undetectable watermarks based on the existence of one-way\nfunctions, a standard assumption in cryptography.", "published": "2023-05-25 02:57:16", "link": "http://arxiv.org/abs/2306.09194v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Unified Modeling of Multi-Talker Overlapped Speech Recognition and\n  Diarization with a Sidecar Separator", "abstract": "Multi-talker overlapped speech poses a significant challenge for speech\nrecognition and diarization. Recent research indicated that these two tasks are\ninter-dependent and complementary, motivating us to explore a unified modeling\nmethod to address them in the context of overlapped speech. A recent study\nproposed a cost-effective method to convert a single-talker automatic speech\nrecognition (ASR) system into a multi-talker one, by inserting a Sidecar\nseparator into the frozen well-trained ASR model. Extending on this, we\nincorporate a diarization branch into the Sidecar, allowing for unified\nmodeling of both ASR and diarization with a negligible overhead of only 768\nparameters. The proposed method yields better ASR results compared to the\nbaseline on LibriMix and LibriSpeechMix datasets. Moreover, without\nsophisticated customization on the diarization task, our method achieves\nacceptable diarization results on the two-speaker subset of CALLHOME with only\na few adaptation steps.", "published": "2023-05-25 17:18:37", "link": "http://arxiv.org/abs/2305.16263v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Scheduled Sampling for Neural Transducer-based ASR", "abstract": "The recurrent neural network-transducer (RNNT) is a promising approach for\nautomatic speech recognition (ASR) with the introduction of a prediction\nnetwork that autoregressively considers linguistic aspects. To train the\nautoregressive part, the ground-truth tokens are used as substitutions for the\nprevious output token, which leads to insufficient robustness to incorrect past\ntokens; a recognition error in the decoding leads to further errors. Scheduled\nsampling (SS) is a technique to train autoregressive model robustly to past\nerrors by randomly replacing some ground-truth tokens with actual outputs\ngenerated from a model. SS mitigates the gaps between training and decoding\nsteps, known as exposure bias, and it is often used for attentional\nencoder-decoder training. However SS has not been fully examined for RNNT\nbecause of the difficulty in applying SS to RNNT due to the complicated RNNT\noutput form. In this paper we propose SS approaches suited for RNNT. Our SS\napproaches sample the tokens generated from the distiribution of RNNT itself,\ni.e. internal language model or RNNT outputs. Experiments in three datasets\nconfirm that RNNT trained with our SS approach achieves the best ASR\nperformance. In particular, on a Japanese ASR task, our best system outperforms\nthe previous state-of-the-art alternative.", "published": "2023-05-25 11:56:41", "link": "http://arxiv.org/abs/2305.15958v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Knowledge Distillation for Neural Transducer-based Target-Speaker ASR:\n  Exploiting Parallel Mixture/Single-Talker Speech Data", "abstract": "Neural transducer (RNNT)-based target-speaker speech recognition (TS-RNNT)\ndirectly transcribes a target speaker's voice from a multi-talker mixture. It\nis a promising approach for streaming applications because it does not incur\nthe extra computation costs of a target speech extraction frontend, which is a\ncritical barrier to quick response. TS-RNNT is trained end-to-end given the\ninput speech (i.e., mixtures and enrollment speech) and reference\ntranscriptions. The training mixtures are generally simulated by mixing\nsingle-talker signals, but conventional TS-RNNT training does not utilize\nsingle-speaker signals. This paper proposes using knowledge distillation (KD)\nto exploit the parallel mixture/single-talker speech data. Our proposed KD\nscheme uses an RNNT system pretrained with the target single-talker speech\ninput to generate pseudo labels for the TS-RNNT training. Experimental results\nshow that TS-RNNT systems trained with the proposed KD scheme outperform a\nbaseline TS-RNNT.", "published": "2023-05-25 12:10:28", "link": "http://arxiv.org/abs/2305.15971v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Power of Prosody and Prosody of Power: An Acoustic Analysis of\n  Finnish Parliamentary Speech", "abstract": "Parliamentary recordings provide a rich source of data for studying how\npoliticians use speech to convey their messages and influence their audience.\nThis provides a unique context for studying how politicians use speech,\nespecially prosody, to achieve their goals. Here we analyzed a corpus of\nparliamentary speeches in the Finnish parliament between the years 2008-2020\nand highlight methodological considerations related to the robustness of signal\nbased features with respect to varying recording conditions and corpus design.\nWe also present results of long term changes pertaining to speakers' status\nwith respect to their party being in government or in opposition. Looking at\nlarge scale averages of fundamental frequency - a robust prosodic feature - we\nfound systematic changes in speech prosody with respect opposition status and\nthe election term. Reflecting a different level of urgency, members of the\nparliament have higher f0 at the beginning of the term or when they are in\nopposition.", "published": "2023-05-25 13:18:41", "link": "http://arxiv.org/abs/2305.16040v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation\n  Detection of /r/ in Child Speech Sound Disorders", "abstract": "Acoustic-to-articulatory speech inversion could enhance automated clinical\nmispronunciation detection to provide detailed articulatory feedback\nunattainable by formant-based mispronunciation detection algorithms; however,\nit is unclear the extent to which a speech inversion system trained on adult\nspeech performs in the context of (1) child and (2) clinical speech. In the\nabsence of an articulatory dataset in children with rhotic speech sound\ndisorders, we show that classifiers trained on tract variables from\nacoustic-to-articulatory speech inversion meet or exceed the performance of\nstate-of-the-art features when predicting clinician judgment of rhoticity.\nIndex Terms: rhotic, speech sound disorder, mispronunciation detection", "published": "2023-05-25 14:19:04", "link": "http://arxiv.org/abs/2305.16085v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Classifying Rhoticity of /r/ in Speech Sound Disorder using Age-and-Sex\n  Normalized Formants", "abstract": "Mispronunciation detection tools could increase treatment access for speech\nsound disorders impacting, e.g., /r/. We show age-and-sex normalized formant\nestimation outperforms cepstral representation for detection of fully rhotic\nvs. derhotic /r/ in the PERCEPT-R Corpus. Gated recurrent neural networks\ntrained on this feature set achieve a mean test participant-specific F1-score\n=.81 ({\\sigma}x=.10, med = .83, n = 48), with post hoc modeling showing no\nsignificant effect of child age or sex.", "published": "2023-05-25 14:42:51", "link": "http://arxiv.org/abs/2305.16111v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Solving Cocktail-Party: The First Method to Build a Realistic\n  Dataset with Ground Truths for Speech Separation", "abstract": "Speech separation is very important in real-world applications such as\nhuman-machine interaction, hearing aids devices, and automatic meeting\ntranscription. In recent years, a significant improvement occurred towards the\nsolution based on deep learning. In fact, much attention has been drawn to\nsupervised learning methods using synthetic mixtures datasets despite their\nbeing not representative of real-world mixtures. The difficulty in building a\nrealistic dataset led researchers to use unsupervised learning methods, because\nof their ability to handle realistic mixtures directly. The results of\nunsupervised learning methods are still unconvincing. In this paper, a method\nis introduced to create a realistic dataset with ground truth sources for\nspeech separation. The main challenge in designing a realistic dataset is the\nunavailability of ground truths for speakers signals. To address this, we\npropose a method for simultaneously recording two speakers and obtaining the\nground truth for each. We present a methodology for benchmarking our realistic\ndataset using a deep learning model based on Bidirectional Gated Recurrent\nUnits (BGRU) and clustering algorithm. The experiments show that our proposed\ndataset improved SI-SDR (Scale Invariant Signal to Distortion Ratio) by 1.65 dB\nand PESQ (Perceptual Evaluation of Speech Quality) by approximately 0.5. We\nalso evaluated the effectiveness of our method at different distances between\nthe microphone and the speakers and found that it improved the stability of the\nlearned model.", "published": "2023-05-25 06:17:03", "link": "http://arxiv.org/abs/2305.15758v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anomalous Sound Detection Based on Sound Separation", "abstract": "This paper proposes an unsupervised anomalous sound detection method using\nsound separation. In factory environments, background noise and non-objective\nsounds obscure desired machine sounds, making it challenging to detect\nanomalous sounds. Therefore, using sounds not mixed with background noise or\nnon-purpose sounds in the detection system is desirable. We compared two\nversions of our proposed method, one using sound separation as a pre-processing\nstep and the other using separation-based outlier exposure that uses the error\nbetween two separated sounds. Based on the assumption that differences in\nseparation performance between normal and anomalous sounds affect detection\nresults, a sound separation model specific to a particular product type was\nused in both versions. Experimental results indicate that the proposed method\nimproved anomalous sound detection performance for all Machine IDs, achieving a\nmaximum improvement of 39%.", "published": "2023-05-25 08:49:00", "link": "http://arxiv.org/abs/2305.15859v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Room Impulse Response Estimation in a Multiple Source Environment", "abstract": "In real-world acoustic scenarios, there often are multiple sound sources\npresent in a room. These sources are situated in various locations and produce\nsounds that reach the listener from multiple directions. The presence of\nmultiple sources in a room creates new challenges in estimating the room\nimpulse response (RIR) as each source has a unique RIR, dependent on its\nlocation and orientation. Therefore, issues of determining which RIR should be\npredicted and how to predict it arise, when the input signal is a mixture of\nmultiple reverberated sources. To address these, we propose a new task of\npredicting a \"representative\" RIR for a room in a multiple source environment\nand present a training method to achieve this goal. In contrast to the model\ntrained in a single source environment, our method shows robust performance,\nregardless of the number of sources in the environment.", "published": "2023-05-25 09:54:38", "link": "http://arxiv.org/abs/2305.15898v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visualizing data augmentation in deep speaker recognition", "abstract": "Visualization is of great value in understanding the internal mechanisms of\nneural networks. Previous work found that LayerCAM is a reliable visualization\ntool for deep speaker models. In this paper, we use LayerCAM to analyze the\nwidely-adopted data augmentation (DA) approach, to understand how it leads to\nmodel robustness. We conduct experiments on the VoxCeleb1 dataset for speaker\nidentification, which shows that both vanilla and activation-based (Act) DA\napproaches enhance robustness against interference, with Act DA being\nconsistently superior. Visualization with LayerCAM suggests DA helps models\nlearn to delete temporal-frequency (TF) bins that are corrupted by\ninterference. The `learn to delete' behavior explained why DA models are more\nrobust than clean models, and why the Act DA is superior over the vanilla DA\nwhen the interference is nontarget speech. However, LayerCAM still cannot\nclearly explain the superiority of Act DA in other situations, suggesting\nfurther research.", "published": "2023-05-25 14:01:07", "link": "http://arxiv.org/abs/2305.16070v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer Learning for Personality Perception via Speech Emotion\n  Recognition", "abstract": "Holistic perception of affective attributes is an important human perceptual\nability. However, this ability is far from being realized in current affective\ncomputing, as not all of the attributes are well studied and their\ninterrelationships are poorly understood. In this work, we investigate the\nrelationship between two affective attributes: personality and emotion, from a\ntransfer learning perspective. Specifically, we transfer Transformer-based and\nwav2vec2-based emotion recognition models to perceive personality from speech\nacross corpora. Compared with previous studies, our results show that\ntransferring emotion recognition is effective for personality perception.\nMoreoever, this allows for better use and exploration of small personality\ncorpora. We also provide novel findings on the relationship between personality\nand emotion that will aid future research on holistic affect recognition.", "published": "2023-05-25 14:04:15", "link": "http://arxiv.org/abs/2305.16076v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly-Supervised Speech Pre-training: A Case Study on Target Speech\n  Recognition", "abstract": "Self-supervised learning (SSL) based speech pre-training has attracted much\nattention for its capability of extracting rich representations learned from\nmassive unlabeled data. On the other hand, the use of weakly-supervised data is\nless explored for speech pre-training. To fill this gap, we propose a\nweakly-supervised speech pre-training method based on speaker-aware speech\ndata. It adopts a similar training procedure to the widely-used masked speech\nprediction based SSL framework, while incorporating additional target-speaker\nenrollment information as an auxiliary input. In this way, the learned\nrepresentation is steered towards the target speaker even in the presence of\nhighly overlapping interference, allowing potential applications to tasks such\nas target speech recognition. Our experiments on Libri2Mix and WSJ0-2mix\ndatasets show that the proposed model achieves significantly better ASR\nperformance compared to WavLM, the state-of-the-art SSL model with denoising\ncapability.", "published": "2023-05-25 17:42:27", "link": "http://arxiv.org/abs/2305.16286v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Neural Music Generation", "abstract": "Recent progress in music generation has been remarkably advanced by the\nstate-of-the-art MusicLM, which comprises a hierarchy of three LMs,\nrespectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet,\nsampling with the MusicLM requires processing through these LMs one by one to\nobtain the fine-grained acoustic tokens, making it computationally expensive\nand prohibitive for a real-time generation. Efficient music generation with a\nquality on par with MusicLM remains a significant challenge. In this paper, we\npresent MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion\nmodel that generates music audios of state-of-the-art quality meanwhile\nreducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling\n10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for\nsemantic modeling, and applies a novel dual-path diffusion (DPD) model and an\naudio VAE-GAN to efficiently decode the conditioning semantic tokens into\nwaveform. DPD is proposed to simultaneously model the coarse and fine acoustics\nby incorporating the semantic information into segments of latents effectively\nvia cross-attention at each denoising step. Our experimental results suggest\nthe superiority of MeLoDy, not only in its practical advantages on sampling\nspeed and infinitely continuable generation, but also in its state-of-the-art\nmusicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.", "published": "2023-05-25 05:02:35", "link": "http://arxiv.org/abs/2305.15719v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled\n  Representation and Prior Mixup for Verified Robust Voice Conversion", "abstract": "Diffusion-based generative models have exhibited powerful generative\nperformance in recent years. However, as many attributes exist in the data\ndistribution and owing to several limitations of sharing the model parameters\nacross all levels of the generation process, it remains challenging to control\nspecific styles for each attribute. To address the above problem, this paper\npresents decoupled denoising diffusion models (DDDMs) with disentangled\nrepresentations, which can control the style for each attribute in generative\nmodels. We apply DDDMs to voice conversion (VC) tasks to address the challenges\nof disentangling and controlling each speech attribute (e.g., linguistic\ninformation, intonation, and timbre). First, we use a self-supervised\nrepresentation to disentangle the speech representation. Subsequently, the\nDDDMs are applied to resynthesize the speech from the disentangled\nrepresentations for denoising with respect to each attribute. Moreover, we also\npropose the prior mixup for robust voice style transfer, which uses the\nconverted representation of the mixed style as a prior distribution for the\ndiffusion models. The experimental results reveal that our method outperforms\npublicly available VC models. Furthermore, we show that our method provides\nrobust generative performance regardless of the model size. Audio samples are\navailable https://hayeong0.github.io/DDDM-VC-demo/.", "published": "2023-05-25 07:59:03", "link": "http://arxiv.org/abs/2305.15816v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Latent Diffusion Model Based Foley Sound Generation System For DCASE\n  Challenge 2023 Task 7", "abstract": "Foley sound presents the background sound for multimedia content and the\ngeneration of Foley sound involves computationally modelling sound effects with\nspecialized techniques. In this work, we proposed a system for DCASE 2023\nchallenge task 7: Foley Sound Synthesis. The proposed system is based on\nAudioLDM, which is a diffusion-based text-to-audio generation model. To\nalleviate the data-hungry problem, the system first trained with large-scale\ndatasets and then downstreamed into this DCASE task via transfer learning.\nThrough experiments, we found out that the feature extracted by the encoder can\nsignificantly affect the performance of the generation model. Hence, we improve\nthe results by leveraging the input label with related text embedding features\nobtained by a significant language model, i.e., contrastive language-audio\npertaining (CLAP). In addition, we utilize a filtering strategy to further\nrefine the output, i.e. by selecting the best results from the candidate clips\ngenerated in terms of the similarity score between the sound and target labels.\nThe overall system achieves a Frechet audio distance (FAD) score of 4.765 on\naverage among all seven different classes, substantially outperforming the\nbaseline system which performs a FAD score of 9.7.", "published": "2023-05-25 10:12:46", "link": "http://arxiv.org/abs/2305.15905v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ordered and Binary Speaker Embedding", "abstract": "Modern speaker recognition systems represent utterances by embedding vectors.\nConventional embedding vectors are dense and non-structural. In this paper, we\npropose an ordered binary embedding approach that sorts the dimensions of the\nembedding vector via a nested dropout and converts the sorted vectors to binary\ncodes via Bernoulli sampling. The resultant ordered binary codes offer some\nimportant merits such as hierarchical clustering, reduced memory usage, and\nfast retrieval. These merits were empirically verified by comprehensive\nexperiments on a speaker identification task with the VoxCeleb and CN-Celeb\ndatasets.", "published": "2023-05-25 13:21:00", "link": "http://arxiv.org/abs/2305.16043v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition", "abstract": "Audio-visual person recognition (AVPR) has received extensive attention.\nHowever, most datasets used for AVPR research so far are collected in\nconstrained environments, and thus cannot reflect the true performance of AVPR\nsystems in real-world scenarios. To meet the request for research on AVPR in\nunconstrained conditions, this paper presents a multi-genre AVPR dataset\ncollected `in the wild', named CN-Celeb-AV. This dataset contains more than\n419k video segments from 1,136 persons from public media. In particular, we put\nmore emphasis on two real-world complexities: (1) data in multiple genres; (2)\nsegments with partial information. A comprehensive study was conducted to\ncompare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the\nresults demonstrated that CN-Celeb-AV is more in line with real-world scenarios\nand can be regarded as a new benchmark dataset for AVPR research. The dataset\nalso involves a development set that can be used to boost the performance of\nAVPR systems in real-life situations. The dataset is free for researchers and\ncan be downloaded from http://cnceleb.org/.", "published": "2023-05-25 13:31:37", "link": "http://arxiv.org/abs/2305.16049v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SoundSieve: Seconds-Long Audio Event Recognition on\n  Intermittently-Powered Systems", "abstract": "A fundamental problem of every intermittently-powered sensing system is that\nsignals acquired by these systems over a longer period in time are also\nintermittent. As a consequence, these systems fail to capture parts of a\nlonger-duration event that spans over multiple charge-discharge cycles of the\ncapacitor that stores the harvested energy. From an application's perspective,\nthis is viewed as sporadic bursts of missing values in the input data -- which\nmay not be recoverable using statistical interpolation or imputation methods.\nIn this paper, we study this problem in the light of an intermittent audio\nclassification system and design an end-to-end system -- SoundSieve -- that is\ncapable of accurately classifying audio events that span multiple on-off cycles\nof the intermittent system. SoundSieve employs an offline audio analyzer that\nlearns to identify and predict important segments of an audio clip that must be\nsampled to ensure accurate classification of the audio. At runtime, SoundSieve\nemploys a lightweight, energy- and content-aware audio sampler that decides\nwhen the system should wake up to capture the next chunk of audio; and a\nlightweight, intermittence-aware audio classifier that performs imputation and\non-device inference. Through extensive evaluations using popular audio datasets\nas well as real systems, we demonstrate that SoundSieve yields 5%--30% more\naccurate inference results than the state-of-the-art.", "published": "2023-05-25 19:43:36", "link": "http://arxiv.org/abs/2305.16445v1", "categories": ["cs.SD", "cs.DC", "eess.AS"], "primary_category": "cs.SD"}
