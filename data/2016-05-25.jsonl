{"title": "Integrating Distributional Lexical Contrast into Word Embeddings for\n  Antonym-Synonym Distinction", "abstract": "We propose a novel vector representation that integrates lexical contrast\ninto distributional vectors and strengthens the most salient features for\ndetermining degrees of word similarity. The improved vectors significantly\noutperform standard models and distinguish antonyms from synonyms with an\naverage precision of 0.66-0.76 across word classes (adjectives, nouns, verbs).\nMoreover, we integrate the lexical contrast vectors into the objective function\nof a skip-gram model. The novel embedding outperforms state-of-the-art models\non predicting word similarities in SimLex-999, and on distinguishing antonyms\nfrom synonyms.", "published": "2016-05-25 08:05:37", "link": "http://arxiv.org/abs/1605.07766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word and Dependency Path Embeddings for Aspect Term\n  Extraction", "abstract": "In this paper, we develop a novel approach to aspect term extraction based on\nunsupervised learning of distributed representations of words and dependency\npaths. The basic idea is to connect two words (w1 and w2) with the dependency\npath (r) between them in the embedding space. Specifically, our method\noptimizes the objective w1 + r = w2 in the low-dimensional space, where the\nmulti-hop dependency paths are treated as a sequence of grammatical relations\nand modeled by a recurrent neural network. Then, we design the embedding\nfeatures that consider linear context and dependency context information, for\nthe conditional random field (CRF) based aspect term extraction. Experimental\nresults on the SemEval datasets show that, (1) with only embedding features, we\ncan achieve state-of-the-art results; (2) our embedding method which\nincorporates the syntactic information among words yields better performance\nthan other representative ones in aspect term extraction.", "published": "2016-05-25 12:01:46", "link": "http://arxiv.org/abs/1605.07843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Neural Machine Translation", "abstract": "Models of neural machine translation are often from a discriminative family\nof encoderdecoders that learn a conditional distribution of a target sentence\ngiven a source sentence. In this paper, we propose a variational model to learn\nthis conditional distribution for neural machine translation: a variational\nencoderdecoder model that can be trained end-to-end. Different from the vanilla\nencoder-decoder model that generates target translations from hidden\nrepresentations of source sentences alone, the variational model introduces a\ncontinuous latent variable to explicitly model underlying semantics of source\nsentences and to guide the generation of target translations. In order to\nperform efficient posterior inference and large-scale training, we build a\nneural posterior approximator conditioned on both the source and the target\nsides, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on both Chinese-English and English-\nGerman translation tasks show that the proposed variational neural machine\ntranslation achieves significant improvements over the vanilla neural machine\ntranslation baselines.", "published": "2016-05-25 13:18:57", "link": "http://arxiv.org/abs/1605.07869v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for\n  Learning Bilingual Phrase Embeddings", "abstract": "In this paper, we propose a bidimensional attention based recursive\nautoencoder (BattRAE) to integrate clues and sourcetarget interactions at\nmultiple levels of granularity into bilingual phrase representations. We employ\nrecursive autoencoders to generate tree structures of phrases with embeddings\nat different levels of granularity (e.g., words, sub-phrases and phrases). Over\nthese embeddings on the source and target side, we introduce a bidimensional\nattention network to learn their interactions encoded in a bidimensional\nattention matrix, from which we extract two soft attention weight distributions\nsimultaneously. These weight distributions enable BattRAE to generate\ncompositive phrase representations via convolution. Based on the learned phrase\nrepresentations, we further use a bilinear neural model, trained via a\nmax-margin method, to measure bilingual semantic similarity. To evaluate the\neffectiveness of BattRAE, we incorporate this semantic similarity as an\nadditional feature into a state-of-the-art SMT system. Extensive experiments on\nNIST Chinese-English test sets show that our model achieves a substantial\nimprovement of up to 1.63 BLEU points on average over the baseline.", "published": "2016-05-25 13:29:07", "link": "http://arxiv.org/abs/1605.07874v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "abstract": "There have been multiple attempts to resolve various inflection matching\nproblems in information retrieval. Stemming is a common approach to this end.\nAmong many techniques for stemming, statistical stemming has been shown to be\neffective in a number of languages, particularly highly inflected languages. In\nthis paper we propose a method for finding affixes in different positions of a\nword. Common statistical techniques heavily rely on string similarity in terms\nof prefix and suffix matching. Since infixes are common in irregular/informal\ninflections in morphologically complex texts, it is required to find infixes\nfor stemming. In this paper we propose a method whose aim is to find\nstatistical inflectional rules based on minimum edit distance table of word\npairs and the likelihoods of the rules in a language. These rules are used to\nstatistically stem words and can be used in different text mining tasks.\nExperimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks\nindicate that the proposed method significantly outperforms all the baselines\nin terms of MAP.", "published": "2016-05-25 12:25:26", "link": "http://arxiv.org/abs/1605.07852v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Query Expansion with Locally-Trained Word Embeddings", "abstract": "Continuous space word embeddings have received a great deal of attention in\nthe natural language processing and machine learning communities for their\nability to model term similarity and other relationships. We study the use of\nterm relatedness in the context of query expansion for ad hoc information\nretrieval. We demonstrate that word embeddings such as word2vec and GloVe, when\ntrained globally, underperform corpus and query specific embeddings for\nretrieval tasks. These results suggest that other tasks benefiting from global\nembeddings may also benefit from local embeddings.", "published": "2016-05-25 14:09:00", "link": "http://arxiv.org/abs/1605.07891v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On model architecture for a children's speech recognition interactive\n  dialog system", "abstract": "This report presents a general model of the architecture of information\nsystems for the speech recognition of children. It presents a model of the\nspeech data stream and how it works. The result of these studies and presented\nveins architectural model shows that research needs to be focused on\nacoustic-phonetic modeling in order to improve the quality of children's speech\nrecognition and the sustainability of the systems to noise and changes in\ntransmission environment. Another important aspect is the development of more\naccurate algorithms for modeling of spontaneous child speech.", "published": "2016-05-25 04:57:42", "link": "http://arxiv.org/abs/1605.07733v1", "categories": ["cs.HC", "cs.CL", "cs.SD"], "primary_category": "cs.HC"}
{"title": "Design and development a children's speech database", "abstract": "The report presents the process of planning, designing and the development of\na database of spoken children's speech whose native language is Bulgarian. The\nproposed model is designed for children between the age of 4 and 6 without\nspeech disorders, and reflects their specific capabilities. At this age most\nchildren cannot read, there is no sustained concentration, they are emotional,\netc. The aim is to unite all the media information accompanying the recording\nand processing of spoken speech, thereby to facilitate the work of researchers\nin the field of speech recognition. This database will be used for the\ndevelopment of systems for children's speech recognition, children's speech\nsynthesis systems, games which allow voice control, etc. As a result of the\nproposed model a prototype system for speech recognition is presented.", "published": "2016-05-25 05:04:11", "link": "http://arxiv.org/abs/1605.07735v1", "categories": ["cs.CL", "cs.HC", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Dimension Projection among Languages based on Pseudo-relevant Documents\n  for Query Translation", "abstract": "Using top-ranked documents in response to a query has been shown to be an\neffective approach to improve the quality of query translation in\ndictionary-based cross-language information retrieval. In this paper, we\npropose a new method for dictionary-based query translation based on dimension\nprojection of embedded vectors from the pseudo-relevant documents in the source\nlanguage to their equivalents in the target language. To this end, first we\nlearn low-dimensional vectors of the words in the pseudo-relevant collections\nseparately and then aim to find a query-dependent transformation matrix between\nthe vectors of translation pairs appeared in the collections. At the next step,\nrepresentation of each query term is projected to the target language and then,\nafter using a softmax function, a query-dependent translation model is built.\nFinally, the model is used for query translation. Our experiments on four CLEF\ncollections in French, Spanish, German, and Italian demonstrate that the\nproposed method outperforms a word embedding baseline based on bilingual\nshuffling and a further number of competitive baselines. The proposed method\nreaches up to 87% performance of machine translation (MT) in short queries and\nconsiderable improvements in verbose queries.", "published": "2016-05-25 12:04:43", "link": "http://arxiv.org/abs/1605.07844v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automatic Extraction of Causal Relations from Natural Language Texts: A\n  Comprehensive Survey", "abstract": "Automatic extraction of cause-effect relationships from natural language\ntexts is a challenging open problem in Artificial Intelligence. Most of the\nearly attempts at its solution used manually constructed linguistic and\nsyntactic rules on small and domain-specific data sets. However, with the\nadvent of big data, the availability of affordable computing power and the\nrecent popularization of machine learning, the paradigm to tackle this problem\nhas slowly shifted. Machines are now expected to learn generic causal\nextraction rules from labelled data with minimal supervision, in a domain\nindependent-manner. In this paper, we provide a comprehensive survey of causal\nrelation extraction techniques from both paradigms, and analyse their relative\nstrengths and weaknesses, with recommendations for future work.", "published": "2016-05-25 14:23:21", "link": "http://arxiv.org/abs/1605.07895v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Review Networks for Caption Generation", "abstract": "We propose a novel extension of the encoder-decoder framework, called a\nreview network. The review network is generic and can enhance any existing\nencoder- decoder model: in this paper, we consider RNN decoders with both CNN\nand RNN encoders. The review network performs a number of review steps with\nattention mechanism on the encoder hidden states, and outputs a thought vector\nafter each review step; the thought vectors are used as the input of the\nattention mechanism in the decoder. We show that conventional encoder-decoders\nare a special case of our framework. Empirically, we show that our framework\nimproves over state-of- the-art encoder-decoder systems on the tasks of image\ncaptioning and source code captioning.", "published": "2016-05-25 14:49:58", "link": "http://arxiv.org/abs/1605.07912v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks\n  with Feedback Negative Sampling", "abstract": "Previous studies in Open Information Extraction (Open IE) are mainly based on\nextraction patterns. They manually define patterns or automatically learn them\nfrom a large corpus. However, these approaches are limited when grasping the\ncontext of a sentence, and they fail to capture implicit relations. In this\npaper, we address this problem with the following methods. First, we exploit\nlong short-term memory (LSTM) networks to extract higher-level features along\nthe shortest dependency paths, connecting headwords of relations and arguments.\nThe path-level features from LSTM networks provide useful clues regarding\ncontextual information and the validity of arguments. Second, we constructed\nsamples to train LSTM networks without the need for manual labeling. In\nparticular, feedback negative sampling picks highly negative samples among\nnon-positive samples through a model trained with positive samples. The\nexperimental results show that our approach produces more precise and abundant\nextractions than state-of-the-art open IE systems. To the best of our\nknowledge, this is the first work to apply deep learning to Open IE.", "published": "2016-05-25 14:59:46", "link": "http://arxiv.org/abs/1605.07918v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
