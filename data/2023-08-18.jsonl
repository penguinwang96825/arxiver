{"title": "Conversational Ontology Alignment with ChatGPT", "abstract": "This study evaluates the applicability and efficiency of ChatGPT for ontology\nalignment using a naive approach. ChatGPT's output is compared to the results\nof the Ontology Alignment Evaluation Initiative 2022 campaign using conference\ntrack ontologies. This comparison is intended to provide insights into the\ncapabilities of a conversational large language model when used in a naive way\nfor ontology matching, and to investigate the potential advantages and\ndisadvantages of this approach.", "published": "2023-08-18 00:26:05", "link": "http://arxiv.org/abs/2308.09217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KESDT: knowledge enhanced shallow and deep Transformer for detecting\n  adverse drug reactions", "abstract": "Adverse drug reaction (ADR) detection is an essential task in the medical\nfield, as ADRs have a gravely detrimental impact on patients' health and the\nhealthcare system. Due to a large number of people sharing information on\nsocial media platforms, an increasing number of efforts focus on social media\ndata to carry out effective ADR detection. Despite having achieved impressive\nperformance, the existing methods of ADR detection still suffer from three main\nchallenges. Firstly, researchers have consistently ignored the interaction\nbetween domain keywords and other words in the sentence. Secondly, social media\ndatasets suffer from the challenges of low annotated data. Thirdly, the issue\nof sample imbalance is commonly observed in social media datasets. To solve\nthese challenges, we propose the Knowledge Enhanced Shallow and Deep\nTransformer(KESDT) model for ADR detection. Specifically, to cope with the\nfirst issue, we incorporate the domain keywords into the Transformer model\nthrough a shallow fusion manner, which enables the model to fully exploit the\ninteractive relationships between domain keywords and other words in the\nsentence. To overcome the low annotated data, we integrate the synonym sets\ninto the Transformer model through a deep fusion manner, which expands the size\nof the samples. To mitigate the impact of sample imbalance, we replace the\nstandard cross entropy loss function with the focal loss function for effective\nmodel training. We conduct extensive experiments on three public datasets\nincluding TwiMed, Twitter, and CADEC. The proposed KESDT outperforms\nstate-of-the-art baselines on F1 values, with relative improvements of 4.87%,\n47.83%, and 5.73% respectively, which demonstrates the effectiveness of our\nproposed KESDT.", "published": "2023-08-18 06:10:11", "link": "http://arxiv.org/abs/2308.09329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Methodology for Generative Spelling Correction via Natural Spelling\n  Errors Emulation across Multiple Domains and Languages", "abstract": "Modern large language models demonstrate impressive capabilities in text\ngeneration and generalization. However, they often struggle with solving text\nediting tasks, particularly when it comes to correcting spelling errors and\nmistypings. In this paper, we present a methodology for generative spelling\ncorrection (SC), which was tested on English and Russian languages and\npotentially can be extended to any language with minor changes. Our research\nmainly focuses on exploring natural spelling errors and mistypings in texts and\nstudying the ways those errors can be emulated in correct sentences to\neffectively enrich generative models' pre-train procedure. We investigate the\nimpact of such emulations and the models' abilities across different text\ndomains. In this work, we investigate two spelling corruption techniques: 1)\nfirst one mimics human behavior when making a mistake through leveraging\nstatistics of errors from particular dataset and 2) second adds the most common\nspelling errors, keyboard miss clicks, and some heuristics within the texts. We\nconducted experiments employing various corruption strategies, models'\narchitectures and sizes on the pre-training and fine-tuning stages and\nevaluated the models using single-domain and multi-domain test sets. As a\npractical outcome of our work, we introduce SAGE(Spell checking via\nAugmentation and Generative distribution Emulation). It is a library for\nautomatic generative SC that includes a family of pre-trained generative models\nand built-in augmentation algorithms.", "published": "2023-08-18 10:07:28", "link": "http://arxiv.org/abs/2308.09435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Red-Teaming Large Language Models using Chain of Utterances for\n  Safety-Alignment", "abstract": "Larger language models (LLMs) have taken the world by storm with their\nmassive multi-tasking capabilities simply by optimizing over a next-word\nprediction objective. With the emergence of their properties and encoded\nknowledge, the risk of LLMs producing harmful outputs increases, making them\nunfit for scalable deployment for the public. In this work, we propose a new\nsafety evaluation benchmark RED-EVAL that carries out red-teaming. We show that\neven widely deployed models are susceptible to the Chain of Utterances-based\n(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and\nChatGPT to unethically respond to more than 65% and 73% of harmful queries. We\nalso demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in\ngenerating harmful responses in more than 86% of the red-teaming attempts.\nNext, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It\nconstitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,\nwe collect a dataset that consists of 1.9K harmful questions covering a wide\nrange of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)\nSAFE-ALIGN: We demonstrate how the conversational dataset can be used for the\nsafety alignment of LLMs by minimizing the negative log-likelihood over helpful\nresponses and penalizing over harmful responses by gradient accent over sample\nloss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely\naligned when evaluated on RED-EVAL and HHH benchmarks while preserving the\nutility of the baseline models (TruthfulQA, MMLU, and BBH).", "published": "2023-08-18 16:27:04", "link": "http://arxiv.org/abs/2308.09662v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OCR Language Models with Custom Vocabularies", "abstract": "Language models are useful adjuncts to optical models for producing accurate\noptical character recognition (OCR) results. One factor which limits the power\nof language models in this context is the existence of many specialized domains\nwith language statistics very different from those implied by a general\nlanguage model - think of checks, medical prescriptions, and many other\nspecialized document classes. This paper introduces an algorithm for\nefficiently generating and attaching a domain specific word based language\nmodel at run time to a general language model in an OCR system. In order to\nbest use this model the paper also introduces a modified CTC beam search\ndecoder which effectively allows hypotheses to remain in contention based on\npossible future completion of vocabulary words. The result is a substantial\nreduction in word error rate in recognizing material from specialized domains.", "published": "2023-08-18 16:46:11", "link": "http://arxiv.org/abs/2308.09671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaijaRC: A Multi-choice Reading Comprehension Dataset for Nigerian\n  Languages", "abstract": "In this paper, we create NaijaRC: a new multi-choice Reading Comprehension\ndataset for three native Nigeria languages that is based on high-school reading\ncomprehension examination. We provide baseline results by performing\ncross-lingual transfer using existing English RACE and Belebele training\ndataset based on a pre-trained encoder-only model. Additionally, we provide\nresults by prompting large language models (LLMs) like GPT-4.", "published": "2023-08-18 18:46:47", "link": "http://arxiv.org/abs/2308.09768v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How susceptible are LLMs to Logical Fallacies?", "abstract": "This paper investigates the rational thinking capability of Large Language\nModels (LLMs) in multi-round argumentative debates by exploring the impact of\nfallacious arguments on their logical reasoning performance. More specifically,\nwe present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic\nbenchmark to assess the robustness of LLMs against logical fallacies. LOGICOM\ninvolves two agents: a persuader and a debater engaging in a multi-round debate\non a controversial topic, where the persuader tries to convince the debater of\nthe correctness of its claim. First, LOGICOM assesses the potential of LLMs to\nchange their opinions through reasoning. Then, it evaluates the debater's\nperformance in logical reasoning by contrasting the scenario where the\npersuader employs logical fallacies against one where logical reasoning is\nused. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4\nusing a dataset containing controversial topics, claims, and reasons supporting\nthem. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their\nopinion through reasoning. However, when presented with logical fallacies,\nGPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often,\nrespectively, compared to when logical reasoning is used. Finally, we introduce\na new dataset containing over 5k pairs of logical vs. fallacious arguments. The\nsource code and dataset of this work are made publicly available.", "published": "2023-08-18 23:07:29", "link": "http://arxiv.org/abs/2308.09853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiable Retrieval Augmentation via Generative Language Modeling\n  for E-commerce Query Intent Classification", "abstract": "Retrieval augmentation, which enhances downstream models by a knowledge\nretriever and an external corpus instead of by merely increasing the number of\nmodel parameters, has been successfully applied to many natural language\nprocessing (NLP) tasks such as text classification, question answering and so\non. However, existing methods that separately or asynchronously train the\nretriever and downstream model mainly due to the non-differentiability between\nthe two parts, usually lead to degraded performance compared to end-to-end\njoint training. In this paper, we propose Differentiable Retrieval Augmentation\nvia Generative lANguage modeling(Dragan), to address this problem by a novel\ndifferentiable reformulation. We demonstrate the effectiveness of our proposed\nmethod on a challenging NLP task in e-commerce search, namely query intent\nclassification. Both the experimental results and ablation study show that the\nproposed method significantly and reasonably improves the state-of-the-art\nbaselines on both offline evaluation and online A/B test.", "published": "2023-08-18 05:05:35", "link": "http://arxiv.org/abs/2308.09308v3", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Document Automation Architectures: Updated Survey in Light of Large\n  Language Models", "abstract": "This paper surveys the current state of the art in document automation (DA).\nThe objective of DA is to reduce the manual effort during the generation of\ndocuments by automatically creating and integrating input from different\nsources and assembling documents conforming to defined templates. There have\nbeen reviews of commercial solutions of DA, particularly in the legal domain,\nbut to date there has been no comprehensive review of the academic research on\nDA architectures and technologies. The current survey of DA reviews the\nacademic literature and provides a clearer definition and characterization of\nDA and its features, identifies state-of-the-art DA architectures and\ntechnologies in academic research, and provides ideas that can lead to new\nresearch opportunities within the DA field in light of recent advances in\ngenerative AI and large language models.", "published": "2023-08-18 06:59:55", "link": "http://arxiv.org/abs/2308.09341v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.7.0; I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "Accelerated materials language processing enabled by GPT", "abstract": "Materials language processing (MLP) is one of the key facilitators of\nmaterials science research, as it enables the extraction of structured\ninformation from massive materials science literature. Prior works suggested\nhigh-performance MLP models for text classification, named entity recognition\n(NER), and extractive question answering (QA), which require complex model\narchitecture, exhaustive fine-tuning and a large number of human-labelled\ndatasets. In this study, we develop generative pretrained transformer\n(GPT)-enabled pipelines where the complex architectures of prior MLP models are\nreplaced with strategic designs of prompt engineering. First, we develop a\nGPT-enabled document classification method for screening relevant documents,\nachieving comparable accuracy and reliability compared to prior models, with\nonly small dataset. Secondly, for NER task, we design an entity-centric\nprompts, and learning few-shot of them improved the performance on most of\nentities in three open datasets. Finally, we develop an GPT-enabled extractive\nQA model, which provides improved performance and shows the possibility of\nautomatically correcting annotations. While our findings confirm the potential\nof GPT-enabled MLP models as well as their value in terms of reliability and\npracticability, our scientific methods and systematic approach are applicable\nto any materials science domain to accelerate the information extraction of\nscientific literature.", "published": "2023-08-18 07:31:13", "link": "http://arxiv.org/abs/2308.09354v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies\n  in Zero Touch Networks", "abstract": "As the dawn of sixth-generation (6G) networking approaches, it promises\nunprecedented advancements in communication and automation. Among the leading\ninnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming to\nachieve fully automated, self-optimizing networks with minimal human\nintervention. Despite the advantages ZTNs offer in terms of efficiency and\nscalability, challenges surrounding transparency, adaptability, and human trust\nremain prevalent. Concurrently, the advent of Large Language Models (LLMs)\npresents an opportunity to elevate the ZTN framework by bridging the gap\nbetween automated processes and human-centric interfaces. This paper explores\nthe integration of LLMs into ZTNs, highlighting their potential to enhance\nnetwork transparency and improve user interactions. Through a comprehensive\ncase study on deep reinforcement learning (DRL)-based anti-jamming technique,\nwe demonstrate how LLMs can distill intricate network operations into\nintuitive, human-readable reports. Additionally, we address the technical and\nethical intricacies of melding LLMs with ZTNs, with an emphasis on data\nprivacy, transparency, and bias reduction. Looking ahead, we identify emerging\nresearch avenues at the nexus of LLMs and ZTNs, advocating for sustained\ninnovation and interdisciplinary synergy in the domain of automated networks.", "published": "2023-08-18 08:13:23", "link": "http://arxiv.org/abs/2308.09376v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scope is all you need: Transforming LLMs for HPC Code", "abstract": "With easier access to powerful compute resources, there is a growing trend in\nthe field of AI for software development to develop larger and larger language\nmodels (LLMs) to address a variety of programming tasks. Even LLMs applied to\ntasks from the high-performance computing (HPC) domain are huge in size (e.g.,\nbillions of parameters) and demand expensive compute resources for training. We\nfound this design choice confusing - why do we need large LLMs trained on\nnatural languages and programming languages unrelated to HPC for HPC-specific\ntasks? In this line of work, we aim to question design choices made by existing\nLLMs by developing smaller LLMs for specific domains - we call them\ndomain-specific LLMs. Specifically, we start off with HPC as a domain and\npropose a novel tokenizer named Tokompiler, designed specifically for\npreprocessing code in HPC and compilation-centric tasks. Tokompiler leverages\nknowledge of language primitives to generate language-oriented tokens,\nproviding a context-aware understanding of code structure while avoiding human\nsemantics attributed to code structures completely. We applied Tokompiler to\npre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortran\ncode corpus mined from GitHub. We evaluate the performance of these models\nagainst the conventional LLMs. Results demonstrate that Tokompiler\nsignificantly enhances code completion accuracy and semantic understanding\ncompared to traditional tokenizers in normalized-perplexity tests, down to ~1\nperplexity score. This research opens avenues for further advancements in\ndomain-specific LLMs, catering to the unique demands of HPC and compilation\ntasks.", "published": "2023-08-18 10:12:03", "link": "http://arxiv.org/abs/2308.09440v3", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "PUMGPT: A Large Vision-Language Model for Product Understanding", "abstract": "E-commerce platforms benefit from accurate product understanding to enhance\nuser experience and operational efficiency. Traditional methods often focus on\nisolated tasks such as attribute extraction or categorization, posing\nadaptability issues to evolving tasks and leading to usability challenges with\nnoisy data from the internet. Current Large Vision Language Models (LVLMs) lack\ndomain-specific fine-tuning, thus falling short in precision and instruction\nfollowing. To address these issues, we introduce PumGPT, the first e-commerce\nspecialized LVLM designed for multi-modal product understanding tasks. We\ncollected and curated a dataset of over one million products from AliExpress,\nfiltering out non-inferable attributes using a universal hallucination\ndetection framework, resulting in 663k high-quality data samples. PumGPT\nfocuses on five essential tasks aimed at enhancing workflows for e-commerce\nplatforms and retailers. We also introduce PumBench, a benchmark to evaluate\nproduct understanding across LVLMs. Our experiments show that PumGPT\noutperforms five other open-source LVLMs and GPT-4V in product understanding\ntasks. We also conduct extensive analytical experiments to delve deeply into\nthe superiority of PumGPT, demonstrating the necessity for a specialized model\nin the e-commerce domain.", "published": "2023-08-18 14:01:37", "link": "http://arxiv.org/abs/2308.09568v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ChatHaruhi: Reviving Anime Character in Reality via Large Language Model", "abstract": "Role-playing chatbots built on large language models have drawn interest, but\nbetter techniques are needed to enable mimicking specific fictional characters.\nWe propose an algorithm that controls language models via an improved prompt\nand memories of the character extracted from scripts. We construct ChatHaruhi,\na dataset covering 32 Chinese / English TV / anime characters with over 54k\nsimulated dialogues. Both automatic and human evaluations show our approach\nimproves role-playing ability over baselines. Code and data are available at\nhttps://github.com/LC1332/Chat-Haruhi-Suzumiya .", "published": "2023-08-18 14:50:25", "link": "http://arxiv.org/abs/2308.09597v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "TrOMR:Transformer-Based Polyphonic Optical Music Recognition", "abstract": "Optical Music Recognition (OMR) is an important technology in music and has\nbeen researched for a long time. Previous approaches for OMR are usually based\non CNN for image understanding and RNN for music symbol classification. In this\npaper, we propose a transformer-based approach with excellent global perceptual\ncapability for end-to-end polyphonic OMR, called TrOMR. We also introduce a\nnovel consistency loss function and a reasonable approach for data annotation\nto improve recognition accuracy for complex music scores. Extensive experiments\ndemonstrate that TrOMR outperforms current OMR methods, especially in\nreal-world scenarios. We also develop a TrOMR system and build a camera scene\ndataset for full-page music scores in real-world. The code and datasets will be\nmade available for reproducibility.", "published": "2023-08-18 08:06:27", "link": "http://arxiv.org/abs/2308.09370v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Sampling Techniques for Generating Melodies with a Transformer\n  Language Model", "abstract": "Research in natural language processing has demonstrated that the quality of\ngenerations from trained autoregressive language models is significantly\ninfluenced by the used sampling strategy. In this study, we investigate the\nimpact of different sampling techniques on musical qualities such as diversity\nand structure. To accomplish this, we train a high-capacity transformer model\non a vast collection of highly-structured Irish folk melodies and analyze the\nmusical qualities of the samples generated using distribution truncation\nsampling techniques. Specifically, we use nucleus sampling, the recently\nproposed \"typical sampling\", and conventional ancestral sampling. We evaluate\nthe effect of these sampling strategies in two scenarios: optimal circumstances\nwith a well-calibrated model and suboptimal circumstances where we\nsystematically degrade the model's performance. We assess the generated samples\nusing objective and subjective evaluations. We discover that probability\ntruncation techniques may restrict diversity and structural patterns in optimal\ncircumstances, but may also produce more musical samples in suboptimal\ncircumstances.", "published": "2023-08-18 10:34:46", "link": "http://arxiv.org/abs/2308.09454v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial-Spiking Hierarchical Networks for Vision-Language\n  Representation Learning", "abstract": "With the success of self-supervised learning, multimodal foundation models\nhave rapidly adapted a wide range of downstream tasks driven by vision and\nlanguage (VL) pretraining. State-of-the-art methods achieve impressive\nperformance by pre-training on large-scale datasets. However, bridging the\nsemantic gap between the two modalities remains a nonnegligible challenge for\nVL tasks. In this work, we propose an efficient computation framework for\nmultimodal alignment by introducing a novel visual semantic module to further\nimprove the performance of the VL tasks. Specifically, we propose a flexible\nmodel, namely Artificial-Spiking Hierarchical Networks (ASH-Nets), which\ncombines the complementary advantages of Artificial neural networks (ANNs) and\nSpiking neural networks (SNNs) to enrich visual semantic representations. In\nparticular, a visual concrete encoder and a semantic abstract encoder are\nconstructed to learn continuous and discrete latent variables to enhance the\nflexibility of semantic encoding. Considering the spatio-temporal properties of\nSNNs modeling, we introduce a contrastive learning method to optimize the\ninputs of similar samples. This can improve the computational efficiency of the\nhierarchical network, while the augmentation of hard samples is beneficial to\nthe learning of visual representations. Furthermore, the Spiking to Text\nUni-Alignment Learning (STUA) pre-training method is proposed, which only\nrelies on text features to enhance the encoding ability of abstract semantics.\nWe validate the performance on multiple well-established downstream VL tasks.\nExperiments show that the proposed ASH-Nets achieve competitive results.", "published": "2023-08-18 10:40:25", "link": "http://arxiv.org/abs/2308.09455v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Predictive Authoring for Brazilian Portuguese Augmentative and\n  Alternative Communication", "abstract": "Individuals with complex communication needs (CCN) often rely on augmentative\nand alternative communication (AAC) systems to have conversations and\ncommunique their wants. Such systems allow message authoring by arranging\npictograms in sequence. However, the difficulty of finding the desired item to\ncomplete a sentence can increase as the user's vocabulary increases. This paper\nproposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram\nprediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus\nfor Brazilian Portuguese to use as a training corpus. We tested different\napproaches to representing a pictogram for prediction: as a word (using\npictogram captions), as a concept (using a dictionary definition), and as a set\nof synonyms (using related terms). We also evaluated the usage of images for\npictogram prediction. The results demonstrate that using embeddings computed\nfrom the pictograms' caption, synonyms, or definitions have a similar\nperformance. Using synonyms leads to lower perplexity, but using captions leads\nto the highest accuracies. This paper provides insight into how to represent a\npictogram for prediction using a BERT-like model and the potential of using\nimages for pictogram prediction.", "published": "2023-08-18 12:14:25", "link": "http://arxiv.org/abs/2308.09497v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic relatedness in DBpedia: A comparative and experimental\n  assessment", "abstract": "Evaluating semantic relatedness of Web resources is still an open challenge.\nThis paper focuses on knowledge-based methods, which represent an alternative\nto corpus-based approaches, and rely in general on the availability of\nknowledge graphs. In particular, we have selected 10 methods from the existing\nliterature, that have been organized according to it adjacent resources, triple\npatterns, and triple weights-based methods. They have been implemented and\nevaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia is\ncontinuously evolving, the experimental results provided by these methods in\nthe literature are not comparable. For this reason, in this work, such methods\nhave been experimented by running them all at once on the same DBpedia release\nand against 14 well-known golden datasets. On the basis of the correlation\nvalues with human judgment obtained according to the experimental results,\nweighting the RDF triples in combination with evaluating all the directed paths\nlinking the compared resources is the best strategy in order to compute\nsemantic relatedness in DBpedia.", "published": "2023-08-18 12:26:10", "link": "http://arxiv.org/abs/2308.09502v1", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct", "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM", "published": "2023-08-18 14:23:21", "link": "http://arxiv.org/abs/2308.09583v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop\n  Visual Reasoning", "abstract": "There emerges a promising trend of using large language models (LLMs) to\ngenerate code-like plans for complex inference tasks such as visual reasoning.\nThis paradigm, known as LLM-based planning, provides flexibility in problem\nsolving and endows better interpretability. However, current research is mostly\nlimited to basic scenarios of simple questions that can be straightforward\nanswered in a few inference steps. Planning for the more challenging multi-hop\nvisual reasoning tasks remains under-explored. Specifically, under multi-hop\nreasoning situations, the trade-off between accuracy and the complexity of\nplan-searching becomes prominent. The prevailing algorithms either address the\nefficiency issue by employing the fast one-stop generation or adopt a complex\niterative generation method to improve accuracy. Both fail to balance the need\nfor efficiency and performance. Drawing inspiration from the dual system of\ncognition in the human brain, the fast and the slow think processes, we propose\na hierarchical plan-searching algorithm that integrates the one-stop reasoning\n(fast) and the Tree-of-thought (slow). Our approach succeeds in performance\nwhile significantly saving inference steps. Moreover, we repurpose the PTR and\nthe CLEVER datasets, developing a systematic framework for evaluating the\nperformance and efficiency of LLMs-based plan-search algorithms under reasoning\ntasks at different levels of difficulty. Extensive experiments demonstrate the\nsuperiority of our proposed algorithm in terms of performance and efficiency.\nThe dataset and code will be release soon.", "published": "2023-08-18 16:21:40", "link": "http://arxiv.org/abs/2308.09658v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (\"LLM thoughts\") are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by >31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.", "published": "2023-08-18 17:29:23", "link": "http://arxiv.org/abs/2308.09687v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taken by Surprise: Contrast effect for Similarity Scores", "abstract": "Accurately evaluating the similarity of object vector embeddings is of\ncritical importance for natural language processing, information retrieval and\nclassification tasks. Popular similarity scores (e.g cosine similarity) are\nbased on pairs of embedding vectors and disregard the distribution of the\nensemble from which objects are drawn. Human perception of object similarity\nsignificantly depends on the context in which the objects appear. In this work\nwe propose the $\\textit{surprise score}$, an ensemble-normalized similarity\nmetric that encapsulates the contrast effect of human perception and\nsignificantly improves the classification performance on zero- and few-shot\ndocument classification tasks. This score quantifies the surprise to find a\ngiven similarity between two elements relative to the pairwise ensemble\nsimilarities. We evaluate this metric on zero/few shot classification and\nclustering tasks and typically find 10-15 % better performance compared to raw\ncosine similarity. Our code is available at\nhttps://github.com/MeetElise/surprise-similarity.", "published": "2023-08-18 18:18:55", "link": "http://arxiv.org/abs/2308.09765v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language\n  Models", "abstract": "Large vision-and-language models (VLMs) trained to match images with text on\nlarge-scale datasets of image-text pairs have shown impressive generalization\nability on several vision and language tasks. Several recent works, however,\nshowed that these models lack fine-grained understanding, such as the ability\nto count and recognize verbs, attributes, or relationships. The focus of this\nwork is to study the understanding of spatial relations. This has been tackled\npreviously using image-text matching (e.g., Visual Spatial Reasoning benchmark)\nor visual question answering (e.g., GQA or VQAv2), both showing poor\nperformance and a large gap compared to human performance. In this work, we\nshow qualitatively (using explainability tools) and quantitatively (using\nobject detectors) that the poor object localization \"grounding\" ability of the\nmodels is a contributing factor to the poor image-text matching performance. We\npropose an alternative fine-grained, compositional approach for recognizing and\nranking spatial clauses that combines the evidence from grounding noun phrases\ncorresponding to objects and their locations to compute the final rank of the\nspatial clause. We demonstrate the approach on representative VLMs (such as\nLXMERT, GPV, and MDETR) and compare and highlight their abilities to reason\nabout spatial relationships.", "published": "2023-08-18 18:58:54", "link": "http://arxiv.org/abs/2308.09778v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity\n  Control", "abstract": "As the model size of pre-trained language models (PLMs) grows rapidly, full\nfine-tuning becomes prohibitively expensive for model training and storage. In\nvision-and-language (VL), parameter-efficient tuning (PET) techniques are\nproposed to integrate modular modifications (e.g., Adapter and LoRA) into\nencoder-decoder PLMs. By tuning a small set of trainable parameters, these\ntechniques perform on par with full fine-tuning. However, excessive modular\nmodifications and neglecting the functionality gap between the encoders and\ndecoders can lead to performance degradation, while existing PET techniques\n(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a\nVision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose\neffective control over modular modifications via a novel granularity-controlled\nmechanism. Considering different granularity-controlled matrices generated by\nthis mechanism, a variety of model-agnostic VL-PET modules can be instantiated\nfrom our framework for better efficiency and effectiveness trade-offs. We\nfurther propose lightweight PET module designs to enhance VL alignment and\nmodeling for the encoders and maintain text generation for the decoders.\nExtensive experiments conducted on four image-text tasks and four video-text\ntasks demonstrate the efficiency, effectiveness and transferability of our\nVL-PET framework. In particular, our VL-PET-large with lightweight PET module\ndesigns significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%\n(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate\nthe enhanced effect of employing our VL-PET designs on existing PET techniques,\nenabling them to achieve significant performance improvements. Our code is\navailable at https://github.com/HenryHZY/VL-PET.", "published": "2023-08-18 20:18:30", "link": "http://arxiv.org/abs/2308.09804v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An Image is Worth a Thousand Toxic Words: A Metamorphic Testing\n  Framework for Content Moderation Software", "abstract": "The exponential growth of social media platforms has brought about a\nrevolution in communication and content dissemination in human society.\nNevertheless, these platforms are being increasingly misused to spread toxic\ncontent, including hate speech, malicious advertising, and pornography, leading\nto severe negative consequences such as harm to teenagers' mental health.\nDespite tremendous efforts in developing and deploying textual and image\ncontent moderation methods, malicious users can evade moderation by embedding\ntexts into images, such as screenshots of the text, usually with some\ninterference. We find that modern content moderation software's performance\nagainst such malicious inputs remains underexplored. In this work, we propose\nOASIS, a metamorphic testing framework for content moderation software. OASIS\nemploys 21 transform rules summarized from our pilot study on 5,000 real-world\ntoxic contents collected from 4 popular social media applications, including\nTwitter, Instagram, Sina Weibo, and Baidu Tieba. Given toxic textual contents,\nOASIS can generate image test cases, which preserve the toxicity yet are likely\nto bypass moderation. In the evaluation, we employ OASIS to test five\ncommercial textual content moderation software from famous companies (i.e.,\nGoogle Cloud, Microsoft Azure, Baidu Cloud, Alibaba Cloud and Tencent Cloud),\nas well as a state-of-the-art moderation research model. The results show that\nOASIS achieves up to 100% error finding rates. Moreover, through retraining the\nmodels with the test cases generated by OASIS, the robustness of the moderation\nmodel can be improved without performance degradation.", "published": "2023-08-18 20:33:06", "link": "http://arxiv.org/abs/2308.09810v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.SE"}
{"title": "Learning Representations on Logs for AIOps", "abstract": "AI for IT Operations (AIOps) is a powerful platform that Site Reliability\nEngineers (SREs) use to automate and streamline operational workflows with\nminimal human intervention. Automated log analysis is a critical task in AIOps\nas it provides key insights for SREs to identify and address ongoing faults.\nTasks such as log format detection, log classification, and log parsing are key\ncomponents of automated log analysis. Most of these tasks require supervised\nlearning; however, there are multiple challenges due to limited labelled log\ndata and the diverse nature of log data. Large Language Models (LLMs) such as\nBERT and GPT3 are trained using self-supervision on a vast amount of unlabeled\ndata. These models provide generalized representations that can be effectively\nused for various downstream tasks with limited labelled data. Motivated by the\nsuccess of LLMs in specific domains like science and biology, this paper\nintroduces a LLM for log data which is trained on public and proprietary log\ndata. The results of our experiments demonstrate that the proposed LLM\noutperforms existing models on multiple downstream tasks. In summary, AIOps\npowered by LLMs offers an efficient and effective solution for automating log\nanalysis tasks and enabling SREs to focus on higher-level tasks. Our proposed\nLLM, trained on public and proprietary log data, offers superior performance on\nmultiple downstream tasks, making it a valuable addition to the AIOps platform.", "published": "2023-08-18 20:34:46", "link": "http://arxiv.org/abs/2308.11526v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Advancing Relation Extraction through Language Probing with Exemplars\n  from Set Co-Expansion", "abstract": "Relation Extraction (RE) is a pivotal task in automatically extracting\nstructured information from unstructured text. In this paper, we present a\nmulti-faceted approach that integrates representative examples and through\nco-set expansion. The primary goal of our method is to enhance relation\nclassification accuracy and mitigating confusion between contrastive classes.\n  Our approach begins by seeding each relationship class with representative\nexamples. Subsequently, our co-set expansion algorithm enriches training\nobjectives by incorporating similarity measures between target pairs and\nrepresentative pairs from the target class. Moreover, the co-set expansion\nprocess involves a class ranking procedure that takes into account exemplars\nfrom contrastive classes. Contextual details encompassing relation mentions are\nharnessed via context-free Hearst patterns to ascertain contextual similarity.\n  Empirical evaluation demonstrates the efficacy of our co-set expansion\napproach, resulting in a significant enhancement of relation classification\nperformance. Our method achieves an observed margin of at least 1 percent\nimprovement in accuracy in most settings, on top of existing fine-tuning\napproaches. To further refine our approach, we conduct an in-depth analysis\nthat focuses on tuning contrastive examples. This strategic selection and\ntuning effectively reduce confusion between classes sharing similarities,\nleading to a more precise classification process.\n  Experimental results underscore the effectiveness of our proposed framework\nfor relation extraction. The synergy between co-set expansion and context-aware\nprompt tuning substantially contributes to improved classification accuracy.\nFurthermore, the reduction in confusion between contrastive classes through\ncontrastive examples tuning validates the robustness and reliability of our\nmethod.", "published": "2023-08-18 00:56:35", "link": "http://arxiv.org/abs/2308.11720v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lip Reading for Low-resource Languages by Learning and Combining General\n  Speech Knowledge and Language-specific Knowledge", "abstract": "This paper proposes a novel lip reading framework, especially for\nlow-resource languages, which has not been well addressed in the previous\nliterature. Since low-resource languages do not have enough video-text paired\ndata to train the model to have sufficient power to model lip movements and\nlanguage, it is regarded as challenging to develop lip reading models for\nlow-resource languages. In order to mitigate the challenge, we try to learn\ngeneral speech knowledge, the ability to model lip movements, from a\nhigh-resource language through the prediction of speech units. It is known that\ndifferent languages partially share common phonemes, thus general speech\nknowledge learned from one language can be extended to other languages. Then,\nwe try to learn language-specific knowledge, the ability to model language, by\nproposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder\nsaves language-specific audio features into memory banks and can be trained on\naudio-text paired data which is more easily accessible than video-text paired\ndata. Therefore, with LMDecoder, we can transform the input speech units into\nlanguage-specific audio features and translate them into texts by utilizing the\nlearned rich language knowledge. Finally, by combining general speech knowledge\nand language-specific knowledge, we can efficiently develop lip reading models\neven for low-resource languages. Through extensive experiments using five\nlanguages, English, Spanish, French, Italian, and Portuguese, the effectiveness\nof the proposed method is evaluated.", "published": "2023-08-18 05:19:03", "link": "http://arxiv.org/abs/2308.09311v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "A tailored Handwritten-Text-Recognition System for Medieval Latin", "abstract": "The Bavarian Academy of Sciences and Humanities aims to digitize its Medieval\nLatin Dictionary. This dictionary entails record cards referring to lemmas in\nmedieval Latin, a low-resource language. A crucial step of the digitization\nprocess is the Handwritten Text Recognition (HTR) of the handwritten lemmas\nfound on these record cards. In our work, we introduce an end-to-end pipeline,\ntailored to the medieval Latin dictionary, for locating, extracting, and\ntranscribing the lemmas. We employ two state-of-the-art (SOTA) image\nsegmentation models to prepare the initial data set for the HTR task.\nFurthermore, we experiment with different transformer-based models and conduct\na set of experiments to explore the capabilities of different combinations of\nvision encoders with a GPT-2 decoder. Additionally, we also apply extensive\ndata augmentation resulting in a highly competitive model. The best-performing\nsetup achieved a Character Error Rate (CER) of 0.015, which is even superior to\nthe commercial Google Cloud Vision model, and shows more stable performance.", "published": "2023-08-18 08:02:52", "link": "http://arxiv.org/abs/2308.09368v1", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Generative Machine Listener", "abstract": "We show how a neural network can be trained on individual intrusive listening\ntest scores to predict a distribution of scores for each pair of reference and\ncoded input stereo or binaural signals. We nickname this method the Generative\nMachine Listener (GML), as it is capable of generating an arbitrary amount of\nsimulated listening test data. Compared to a baseline system using regression\nover mean scores, we observe lower outlier ratios (OR) for the mean score\npredictions, and obtain easy access to the prediction of confidence intervals\n(CI). The introduction of data augmentation techniques from the image domain\nresults in a significant increase in CI prediction accuracy as well as Pearson\nand Spearman rank correlation of mean scores.", "published": "2023-08-18 12:07:13", "link": "http://arxiv.org/abs/2308.09493v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality\n  Assessment Model", "abstract": "This study proposes a multi-task pseudo-label learning (MPL)-based\nnon-intrusive speech quality assessment model called MTQ-Net. MPL consists of\ntwo stages: obtaining pseudo-label scores from a pretrained model and\nperforming multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS),\nNoise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The\npretrained MOSA-Net model is utilized to estimate three pseudo labels:\nperceptual evaluation of speech quality (PESQ), short-time objective\nintelligibility (STOI), and speech distortion index (SDI). Multi-task learning\nis then employed to train MTQ-Net by combining a supervised loss (derived from\nthe difference between the estimated score and the ground-truth label) and a\nsemi-supervised loss (derived from the difference between the estimated score\nand the pseudo label), where the Huber loss is employed as the loss function.\nExperimental results first demonstrate the advantages of MPL compared to\ntraining a model from scratch and using a direct knowledge transfer mechanism.\nSecond, the benefit of the Huber loss for improving the predictive ability of\nMTQ-Net is verified. Finally, the MTQ-Net with the MPL approach exhibits higher\noverall predictive power compared to other SSL-based speech assessment models.", "published": "2023-08-18 02:36:21", "link": "http://arxiv.org/abs/2308.09262v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on\n  Multi-Order Spectrograms", "abstract": "Robust audio anti-spoofing has been increasingly challenging due to the\nrecent advancements on deepfake techniques. While spectrograms have\ndemonstrated their capability for anti-spoofing, complementary information\npresented in multi-order spectral patterns have not been well explored, which\nlimits their effectiveness for varying spoofing attacks. Therefore, we propose\na novel deep learning method with a spectral fusion-reconstruction strategy,\nnamely S2pecNet, to utilise multi-order spectral patterns for robust audio\nanti-spoofing representations. Specifically, spectral patterns up to\nsecond-order are fused in a coarse-to-fine manner and two branches are designed\nfor the fine-level fusion from the spectral and temporal contexts. A\nreconstruction from the fused representation to the input spectrograms further\nreduces the potential fused information loss. Our method achieved the\nstate-of-the-art performance with an EER of 0.77% on a widely used dataset:\nASVspoof2019 LA Challenge.", "published": "2023-08-18 04:51:15", "link": "http://arxiv.org/abs/2308.09302v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning", "abstract": "We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours\nof 19-channel audio, first-order ambisonics, and optional distractor noise.\nSpatial LibriSpeech is designed for machine learning model training, and it\nincludes labels for source position, speaking direction, room acoustics and\ngeometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples\nwith 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To\ndemonstrate the utility of our dataset, we train models on four spatial audio\ntasks, resulting in a median absolute error of 6.60{\\deg} on 3D source\nlocalization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation.\nWe show that the same models generalize well to widely-used evaluation\ndatasets, e.g., obtaining a median absolute error of 12.43{\\deg} on 3D source\nlocalization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE\nChallenge.", "published": "2023-08-18 12:45:32", "link": "http://arxiv.org/abs/2308.09514v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compensating Removed Frequency Components: Thwarting Voice Spectrum\n  Reduction Attacks", "abstract": "Automatic speech recognition (ASR) provides diverse audio-to-text services\nfor humans to communicate with machines. However, recent research reveals ASR\nsystems are vulnerable to various malicious audio attacks. In particular, by\nremoving the non-essential frequency components, a new spectrum reduction\nattack can generate adversarial audios that can be perceived by humans but\ncannot be correctly interpreted by ASR systems. It raises a new challenge for\ncontent moderation solutions to detect harmful content in audio and video\navailable on social media platforms. In this paper, we propose an acoustic\ncompensation system named ACE to counter the spectrum reduction attacks over\nASR systems. Our system design is based on two observations, namely, frequency\ncomponent dependencies and perturbation sensitivity. First, since the Discrete\nFourier Transform computation inevitably introduces spectral leakage and\naliasing effects to the audio frequency spectrum, the frequency components with\nsimilar frequencies will have a high correlation. Thus, considering the\nintrinsic dependencies between neighboring frequency components, it is possible\nto recover more of the original audio by compensating for the removed\ncomponents based on the remaining ones. Second, since the removed components in\nthe spectrum reduction attacks can be regarded as an inverse of adversarial\nnoise, the attack success rate will decrease when the adversarial audio is\nreplayed in an over-the-air scenario. Hence, we can model the acoustic\npropagation process to add over-the-air perturbations into the attacked audio.\nWe implement a prototype of ACE and the experiments show ACE can effectively\nreduce up to 87.9% of ASR inference errors caused by spectrum reduction\nattacks. Also, by analyzing residual errors, we summarize six general types of\nASR inference errors and investigate the error causes and potential mitigation\nsolutions.", "published": "2023-08-18 13:23:26", "link": "http://arxiv.org/abs/2308.09546v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by\n  Connecting Foundation Models", "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation\nmodels (FMs) is becoming a new paradigm in AI research. Their representative\nand generative abilities learnt from vast amounts of data can be easily adapted\nand transferred to a wide range of downstream tasks without extra training from\nscratch. However, leveraging FMs in cross-modal generation remains\nunder-researched when audio modality is involved. On the other hand,\nautomatically generating semantically-relevant sound from visual input is an\nimportant problem in cross-modal generation studies. To solve this\nvision-to-audio (V2A) generation problem, existing methods tend to design and\nbuild complex systems from scratch using modestly sized datasets. In this\npaper, we propose a lightweight solution to this problem by leveraging\nfoundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate\nthe domain gap between the latent space of the visual CLIP and the auditory\nCLAP models. Then we propose a simple yet effective mapper mechanism\n(V2A-Mapper) to bridge the domain gap by translating the visual input between\nCLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained\naudio generative FM AudioLDM is adopted to produce high-fidelity and\nvisually-aligned sound. Compared to previous approaches, our method only\nrequires a quick training of the V2A-Mapper. We further analyze and conduct\nextensive experiments on the choice of the V2A-Mapper and show that a\ngenerative mapper is better at fidelity and variability (FD) while a regression\nmapper is slightly better at relevance (CS). Both objective and subjective\nevaluation on two V2A datasets demonstrate the superiority of our proposed\nmethod compared to current state-of-the-art approaches - trained with 86% fewer\nparameters but achieving 53% and 19% improvement in FD and CS, respectively.", "published": "2023-08-18 04:49:38", "link": "http://arxiv.org/abs/2308.09300v4", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Audiovisual Moments in Time: A Large-Scale Annotated Dataset of\n  Audiovisual Actions", "abstract": "We present Audiovisual Moments in Time (AVMIT), a large-scale dataset of\naudiovisual action events. In an extensive annotation task 11 participants\nlabelled a subset of 3-second audiovisual videos from the Moments in Time\ndataset (MIT). For each trial, participants assessed whether the labelled\naudiovisual action event was present and whether it was the most prominent\nfeature of the video. The dataset includes the annotation of 57,177 audiovisual\nvideos, each independently evaluated by 3 of 11 trained participants. From this\ninitial collection, we created a curated test set of 16 distinct action\nclasses, with 60 videos each (960 videos). We also offer 2 sets of pre-computed\naudiovisual feature embeddings, using VGGish/YamNet for audio data and\nVGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for\naudiovisual DNN research. We explored the advantages of AVMIT annotations and\nfeature embeddings to improve performance on audiovisual event recognition. A\nseries of 6 Recurrent Neural Networks (RNNs) were trained on either\nAVMIT-filtered audiovisual events or modality-agnostic events from MIT, and\nthen tested on our audiovisual test set. In all RNNs, top 1 accuracy was\nincreased by 2.71-5.94\\% by training exclusively on audiovisual events, even\noutweighing a three-fold increase in training data. We anticipate that the\nnewly annotated AVMIT dataset will serve as a valuable resource for research\nand comparative experiments involving computational models and human\nparticipants, specifically when addressing research questions where audiovisual\ncorrespondence is of critical importance.", "published": "2023-08-18 17:13:45", "link": "http://arxiv.org/abs/2308.09685v1", "categories": ["cs.LG", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
