{"title": "High-dimensional covariance matrix estimators on simulated portfolios with complex structures", "abstract": "We study the allocation of synthetic portfolios under hierarchical nested,\none-factor, and diagonal structures of the population covariance matrix in a\nhigh-dimensional scenario. The noise reduction approaches for the sample\nrealizations are based on random matrices, free probability, deterministic\nequivalents, and their combination with a data science hierarchical method\nknown as two-step covariance estimators. The financial performance metrics from\nthe simulations are compared with empirical data from companies comprising the\nS&P 500 index using a moving window and walk-forward analysis. The portfolio\nallocation strategies analyzed include the minimum variance portfolio (both\nwith and without short-selling constraints) and the hierarchical risk parity\napproach. Our proposed hierarchical nested covariance model shows signatures of\ncomplex system interactions. The empirical financial data reproduces stylized\nportfolio facts observed in the complex and one-factor covariance models. The\ntwo-step estimators proposed here improve several financial metrics under the\nanalyzed investment strategies. The results pave the way for new risk\nmanagement and diversification approaches when the number of assets is of the\nsame order as the number of transaction days in the investment portfolio.", "published": "2024-12-11 19:54:21", "link": "http://arxiv.org/abs/2412.08756v1", "categories": ["q-fin.CP", "physics.soc-ph"], "primary_category": "q-fin.CP"}
{"title": "Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM", "abstract": "Financial analysis heavily relies on the evaluation of earnings reports to\ngain insights into company performance. Traditional generation of these reports\nrequires extensive financial expertise and is time-consuming. With the\nimpressive progress in Large Language Models (LLMs), a wide variety of\nfinancially focused LLMs has emerged, addressing tasks like sentiment analysis\nand entity recognition in the financial domain. This paper presents a novel\nchallenge: developing an LLM specifically for automating the generation of\nearnings reports analysis. Our methodology involves an in-depth analysis of\nexisting earnings reports followed by a unique approach to fine-tune an LLM for\nthis purpose. This approach combines retrieval augmentation and the generation\nof instruction-based data, specifically tailored for the financial sector, to\nenhance the LLM's performance. With extensive financial documents, we construct\nfinancial instruction data, enabling the refined adaptation of our LLM to\nfinancial contexts. Preliminary results indicate that our augmented LLM\noutperforms general open-source models and rivals commercial counterparts like\nGPT-3.5 in financial applications. Our research paves the way for streamlined\nand insightful automation in financial report generation, marking a significant\nstride in the field of financial analysis.", "published": "2024-12-11 08:09:42", "link": "http://arxiv.org/abs/2412.08179v1", "categories": ["q-fin.ST", "cs.AI"], "primary_category": "q-fin.ST"}
{"title": "Efficient and Verified Continuous Double Auctions", "abstract": "Continuous double auctions are commonly used to match orders at currency,\nstock, and commodities exchanges. A verified implementation of continuous\ndouble auctions is a useful tool for market regulators as they give rise to\nautomated checkers that are guaranteed to detect errors in the trade logs of an\nexisting exchange if they contain trades that violate the matching rules. We\nprovide an efficient and formally verified implementation of continuous double\nauctions that takes $O(n \\log n)$ time to match $n$ orders. This improves an\nearlier $O(n^2)$ verified implementation. We also prove a matching\n$\\Omega(n\\log n)$ lower bound on the running time for continuous double\nauctions. Our new implementation takes only a couple of minutes to run on ten\nmillion randomly generated orders as opposed to a few days taken by the earlier\nimplementation. Our new implementation gives rise to an efficient automatic\nchecker.\n  We use the Coq proof assistant for verifying our implementation and\nextracting a verified OCaml program. While using Coq's standard library\nimplementation of red-black trees to obtain our improvement, we observed that\nits specification has serious gaps, which we fill in this work; this might be\nof independent interest.", "published": "2024-12-11 18:44:37", "link": "http://arxiv.org/abs/2412.08624v1", "categories": ["cs.LO", "q-fin.TR", "F.3.1; K.4.4"], "primary_category": "cs.LO"}
{"title": "TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge\n  Internalization with Self-Reflection", "abstract": "Large Language Models exhibit impressive reasoning capabilities across\ndiverse tasks, motivating efforts to distill these capabilities into smaller\nmodels through generated reasoning data. However, direct training on such\nsynthesized reasoning data may lead to superficial imitation of reasoning\nprocess, rather than fostering a genuine integration of reasoning capabilities\nwith underlying knowledge. To address this, we propose TinyThinker, a framework\nintroducing two novel approaches. First, we introduce a three-stage process\nthat incrementally guides the student model through the reasoning process,\nprogressively refining knowledge from coarse to fine granularity. Second, we\ndevelop a two-phase training framework comprising an initial reasoning\nacquisition phase followed by a self-reflection phase utilizing self-generated\ndata. Experiments on commonsense reasoning benchmarks demonstrate that\nTinyThinker achieves superior performance compared to baselines. Ablation\nstudies further validate the effectiveness of each component in our framework.\nWe expect that TinyThinker can be extended to other knowledge-intensive\nreasoning tasks, offering an alternative strategy for developing effective\nreasoning capabilities in smaller language models. Codes are available at\nhttps://github.com/shengminp/TinyThinker", "published": "2024-12-11 02:05:42", "link": "http://arxiv.org/abs/2412.08024v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoVerse: Exploring Multimodal Large Language Models for Sentiment and\n  Emotion Understanding", "abstract": "Sentiment and emotion understanding are essential to applications such as\nhuman-computer interaction and depression detection. While Multimodal Large\nLanguage Models (MLLMs) demonstrate robust general capabilities, they face\nconsiderable challenges in the field of affective computing, particularly in\ndetecting subtle facial expressions and handling complex emotion-related tasks,\nsuch as emotion reason inference and understanding emotions in long-context\nscenarios. Furthermore, there is a lack of a unified MLLM that can effectively\nhandle both sentiment and emotion-related tasks. To address these challenges,\nwe explore multi-task training strategies for MLLMs in affective computing and\nintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broad\nspectrum of sentiment and emotion-related tasks. In addition, EmoVerse is\ncapable of deeply analyzing the underlying causes of emotional states. We also\nintroduce the Affective Multitask (AMT) Dataset, which supports multimodal\nsentiment analysis, multimodal emotion recognition, facial expression\nrecognition, emotion reason inference, and emotion cause-pair extraction tasks.\nExtensive experiments demonstrate that EmoVerse outperforms existing methods,\nachieving state-of-the-art results in sentiment and emotion-related tasks. The\ncode is available at https://github.com/liaolea/EmoVerse.", "published": "2024-12-11 02:55:00", "link": "http://arxiv.org/abs/2412.08049v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLPineers@ NLU of Devanagari Script Languages 2025: Hate Speech\n  Detection using Ensembling of BERT-based models", "abstract": "This paper explores hate speech detection in Devanagari-scripted languages,\nfocusing on Hindi and Nepali, for Subtask B of the CHIPSAL@COLING 2025 Shared\nTask. Using a range of transformer-based models such as XLM-RoBERTa, MURIL, and\nIndicBERT, we examine their effectiveness in navigating the nuanced boundary\nbetween hate speech and free expression. Our best performing model, implemented\nas ensemble of multilingual BERT models achieve Recall of 0.7762 (Rank 3/31 in\nterms of recall) and F1 score of 0.6914 (Rank 17/31). To address class\nimbalance, we used backtranslation for data augmentation, and cosine similarity\nto preserve label consistency after augmentation. This work emphasizes the need\nfor hate speech detection in Devanagari-scripted languages and presents a\nfoundation for further research.", "published": "2024-12-11 07:37:26", "link": "http://arxiv.org/abs/2412.08163v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accurate Medical Named Entity Recognition Through Specialized NLP Models", "abstract": "This study evaluated the effect of BioBERT in medical text processing for the\ntask of medical named entity recognition. Through comparative experiments with\nmodels such as BERT, ClinicalBERT, SciBERT, and BlueBERT, the results showed\nthat BioBERT achieved the best performance in both precision and F1 score,\nverifying its applicability and superiority in the medical field. BioBERT\nenhances its ability to understand professional terms and complex medical texts\nthrough pre-training on biomedical data, providing a powerful tool for medical\ninformation extraction and clinical decision support. The study also explored\nthe privacy and compliance challenges of BioBERT when processing medical data,\nand proposed future research directions for combining other medical-specific\nmodels to improve generalization and robustness. With the development of deep\nlearning technology, the potential of BioBERT in application fields such as\nintelligent medicine, personalized treatment, and disease prediction will be\nfurther expanded. Future research can focus on the real-time and\ninterpretability of the model to promote its widespread application in the\nmedical field.", "published": "2024-12-11 10:06:57", "link": "http://arxiv.org/abs/2412.08255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discrete Subgraph Sampling for Interpretable Graph based Visual Question\n  Answering", "abstract": "Explainable artificial intelligence (XAI) aims to make machine learning\nmodels more transparent. While many approaches focus on generating explanations\npost-hoc, interpretable approaches, which generate the explanations\nintrinsically alongside the predictions, are relatively rare. In this work, we\nintegrate different discrete subset sampling methods into a graph-based visual\nquestion answering system to compare their effectiveness in generating\ninterpretable explanatory subgraphs intrinsically. We evaluate the methods on\nthe GQA dataset and show that the integrated methods effectively mitigate the\nperformance trade-off between interpretability and answer accuracy, while also\nachieving strong co-occurrences between answer and question tokens.\nFurthermore, we conduct a human evaluation to assess the interpretability of\nthe generated subgraphs using a comparative setting with the extended\nBradley-Terry model, showing that the answer and question token co-occurrence\nmetrics strongly correlate with human preferences. Our source code is publicly\navailable.", "published": "2024-12-11 10:18:37", "link": "http://arxiv.org/abs/2412.08263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking", "abstract": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.", "published": "2024-12-11 10:35:45", "link": "http://arxiv.org/abs/2412.08268v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "2M-BELEBELE: Highly Multilingual Speech and American Sign Language\n  Comprehension Dataset", "abstract": "We introduce the first highly multilingual speech and American Sign Language\n(ASL) comprehension dataset by extending BELEBELE. Our dataset covers 74 spoken\nlanguages at the intersection of BELEBELE and FLEURS, and one sign language\n(ASL). We evaluate 2M-BELEBELE dataset for both 5-shot and zero-shot settings\nand across languages, the speech comprehension accuracy is ~ 2-3% average lower\ncompared to reading comprehension.", "published": "2024-12-11 10:46:21", "link": "http://arxiv.org/abs/2412.08274v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Y-NQ: English-Yor\u00f9b\u00e1 Evaluation dataset for Open-Book Reading\n  Comprehension and Text Generation", "abstract": "The purpose of this work is to share an English-Yor\\`ub\\'a evaluation dataset\nfor open-book reading comprehension and text generation to assess the\nperformance of models both in a high- and a low- resource language. The dataset\ncontains 358 questions and answers on 338 English documents and 208 Yor\\`ub\\'a\ndocuments. The average document length is ~ 10k words for English and 430 words\nfor Yor\\`ub\\'a. Experiments show a consistent disparity in performance between\nthe two languages, with Yor\\`ub\\'a falling behind English for automatic metrics\neven if documents are much shorter for this language. For a small set of\ndocuments with comparable length, performance of Yor\\`ub\\'a drops by x2.5\ntimes. When analyzing performance by length, we observe that Yor\\`ub\\'a\ndecreases performance dramatically for documents that reach 1500 words while\nEnglish performance is barely affected at that length. Our dataset opens the\ndoor to showcasing if English LLM reading comprehension capabilities extend to\nYor\\`ub\\'a, which for the evaluated LLMs is not the case.", "published": "2024-12-11 10:52:29", "link": "http://arxiv.org/abs/2412.08279v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Code LLMs: A Taxonomy-based Survey", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks and have recently expanded their impact to coding tasks,\nbridging the gap between natural languages (NL) and programming languages (PL).\nThis taxonomy-based survey provides a comprehensive analysis of LLMs in the\nNL-PL domain, investigating how these models are utilized in coding tasks and\nexamining their methodologies, architectures, and training processes. We\npropose a taxonomy-based framework that categorizes relevant concepts,\nproviding a unified classification system to facilitate a deeper understanding\nof this rapidly evolving field. This survey offers insights into the current\nstate and future directions of LLMs in coding tasks, including their\napplications and limitations.", "published": "2024-12-11 11:07:50", "link": "http://arxiv.org/abs/2412.08291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Still Face Challenges in Multi-Hop Reasoning with\n  External Knowledge", "abstract": "We carry out a series of experiments to test large language models' multi-hop\nreasoning ability from three aspects: selecting and combining external\nknowledge, dealing with non-sequential reasoning tasks and generalising to data\nsamples with larger numbers of hops. We test the GPT-3.5 model on four\nreasoning benchmarks with Chain-of-Thought prompting (and its variations). Our\nresults reveal that despite the amazing performance achieved by large language\nmodels on various reasoning tasks, models still suffer from severe drawbacks\nwhich shows a large gap with humans.", "published": "2024-12-11 11:53:26", "link": "http://arxiv.org/abs/2412.08317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch\n  Language", "abstract": "Zero-shot evaluation of information retrieval (IR) models is often performed\nusing BEIR; a large and heterogeneous benchmark composed of multiple datasets,\ncovering different retrieval tasks across various domains. Although BEIR has\nbecome a standard benchmark for the zero-shot setup, its exclusively English\ncontent reduces its utility for underrepresented languages in IR, including\nDutch. To address this limitation and encourage the development of Dutch IR\nmodels, we introduce BEIR-NL by automatically translating the publicly\naccessible BEIR datasets into Dutch. Using BEIR-NL, we evaluated a wide range\nof multilingual dense ranking and reranking models, as well as the lexical BM25\nmethod. Our experiments show that BM25 remains a competitive baseline, and is\nonly outperformed by the larger dense models trained for retrieval. When\ncombined with reranking models, BM25 achieves performance on par with the best\ndense ranking models. In addition, we explored the impact of translation on the\ndata by back-translating a selection of datasets to English, and observed a\nperformance drop for both dense and lexical methods, indicating the limitations\nof translation for creating benchmarks. BEIR-NL is publicly available on the\nHugging Face hub.", "published": "2024-12-11 12:15:57", "link": "http://arxiv.org/abs/2412.08329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse\n  Scenarios Handling Emotional Support Agent", "abstract": "Large Language Models (LLMs) have demonstrated promising potential in\nproviding empathetic support during interactions. However, their responses\noften become verbose or overly formulaic, failing to adequately address the\ndiverse emotional support needs of real-world scenarios. To tackle this\nchallenge, we propose an innovative strategy-enhanced role-playing framework,\ndesigned to simulate authentic emotional support conversations. Specifically,\nour approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing\nInteractions, which involve three pivotal roles -- Seeker, Strategy Counselor,\nand Supporter -- engaging in diverse scenarios to emulate real-world\ninteractions and promote a broader range of dialogues; and (2) Emotional\nSupport Agent Training, achieved through fine-tuning LLMs using our specially\nconstructed dataset. Within this framework, we develop the \\textbf{ServeForEmo}\ndataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and\n62.8K+ utterances. We further present \\textbf{SweetieChat}, an emotional\nsupport agent capable of handling diverse open-domain scenarios. Extensive\nexperiments and human evaluations confirm the framework's effectiveness in\nenhancing emotional support, highlighting its unique ability to provide more\nnuanced and tailored assistance.", "published": "2024-12-11 13:56:04", "link": "http://arxiv.org/abs/2412.08389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Reason via Self-Iterative Process Feedback for Small\n  Language Models", "abstract": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval.", "published": "2024-12-11 14:05:04", "link": "http://arxiv.org/abs/2412.08393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting", "abstract": "Mental manipulation severely undermines mental wellness by covertly and\nnegatively distorting decision-making. While there is an increasing interest in\nmental health care within the natural language processing community, progress\nin tackling manipulation remains limited due to the complexity of detecting\nsubtle, covert tactics in conversations. In this paper, we propose Intent-Aware\nPrompting (IAP), a novel approach for detecting mental manipulations using\nlarge language models (LLMs), providing a deeper understanding of manipulative\ntactics by capturing the underlying intents of participants. Experimental\nresults on the MentalManip dataset demonstrate superior effectiveness of IAP\nagainst other advanced prompting strategies. Notably, our approach\nsubstantially reduces false negatives, helping detect more instances of mental\nmanipulation with minimal misjudgment of positive cases. The code of this paper\nis available at https://github.com/Anton-Jiayuan-MA/Manip-IAP.", "published": "2024-12-11 14:31:39", "link": "http://arxiv.org/abs/2412.08414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-perspective Alignment for Increasing Naturalness in Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) systems amplify lexical biases present in\ntheir training data, leading to artificially impoverished language in output\ntranslations. These language-level characteristics render automatic\ntranslations different from text originally written in a language and human\ntranslations, which hinders their usefulness in for example creating evaluation\ndatasets. Attempts to increase naturalness in NMT can fall short in terms of\ncontent preservation, where increased lexical diversity comes at the cost of\ntranslation accuracy. Inspired by the reinforcement learning from human\nfeedback framework, we introduce a novel method that rewards both naturalness\nand content preservation. We experiment with multiple perspectives to produce\nmore natural translations, aiming at reducing machine and human translationese.\nWe evaluate our method on English-to-Dutch literary translation, and find that\nour best model produces translations that are lexically richer and exhibit more\nproperties of human-written language, without loss in translation accuracy.", "published": "2024-12-11 15:42:22", "link": "http://arxiv.org/abs/2412.08473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Opinion Mining in Product Reviews: Multi-perspective\n  Prompt-based Learning", "abstract": "Comparative reviews are pivotal in understanding consumer preferences and\ninfluencing purchasing decisions. Comparative Quintuple Extraction (COQE) aims\nto identify five key components in text: the target entity, compared entities,\ncompared aspects, opinions on these aspects, and polarity. Extracting precise\ncomparative information from product reviews is challenging due to nuanced\nlanguage and sequential task errors in traditional methods. To mitigate these\nproblems, we propose MTP-COQE, an end-to-end model designed for COQE.\nLeveraging multi-perspective prompt-based learning, MTP-COQE effectively guides\nthe generative model in comparative opinion mining tasks. Evaluation on the\nCamera-COQE (English) and VCOM (Vietnamese) datasets demonstrates MTP-COQE's\nefficacy in automating COQE, achieving superior performance with a 1.41% higher\nF1 score than the previous baseline models on the English dataset.\nAdditionally, we designed a strategy to limit the generative model's creativity\nto ensure the output meets expectations. We also performed data augmentation to\naddress data imbalance and to prevent the model from becoming biased towards\ndominant samples.", "published": "2024-12-11 16:18:52", "link": "http://arxiv.org/abs/2412.08508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Relevance and Reasoning: Rationale Distillation in\n  Retrieval-Augmented Generation", "abstract": "The reranker and generator are two critical components in the\nRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking\nrelevant documents and generating responses. However, due to differences in\npre-training data and objectives, there is an inevitable gap between the\ndocuments ranked as relevant by the reranker and those required by the\ngenerator to support answering the query. To address this gap, we propose\nRADIO, a novel and practical preference alignment framework with RAtionale\nDIstillatiOn. Specifically, We first propose a rationale extraction method that\nleverages the reasoning capabilities of Large Language Models (LLMs) to extract\nthe rationales necessary for answering the query. Subsequently, a\nrationale-based alignment process is designed to rerank the documents based on\nthe extracted rationales, and fine-tune the reranker to align the preferences.\nWe conduct extensive experiments on two tasks across three datasets to\ndemonstrate the effectiveness of our approach compared to baseline methods. Our\ncode is released online to ease reproduction.", "published": "2024-12-11 16:32:41", "link": "http://arxiv.org/abs/2412.08519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance", "abstract": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.", "published": "2024-12-11 16:35:13", "link": "http://arxiv.org/abs/2412.08521v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Learning for Encoder-only Language Models via a Discrete\n  Key-Value Bottleneck", "abstract": "Continual learning remains challenging across various natural language\nunderstanding tasks. When models are updated with new training data, they risk\ncatastrophic forgetting of prior knowledge. In the present work, we introduce a\ndiscrete key-value bottleneck for encoder-only language models, allowing for\nefficient continual learning by requiring only localized updates. Inspired by\nthe success of a discrete key-value bottleneck in vision, we address new and\nNLP-specific challenges. We experiment with different bottleneck architectures\nto find the most suitable variants regarding language, and present a generic\ndiscrete key initialization technique for NLP that is task independent. We\nevaluate the discrete key-value bottleneck in four continual learning NLP\nscenarios and demonstrate that it alleviates catastrophic forgetting. We\nshowcase that it offers competitive performance to other popular continual\nlearning methods, with lower computational costs.", "published": "2024-12-11 16:38:34", "link": "http://arxiv.org/abs/2412.08528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TECO: Improving Multimodal Intent Recognition with Text Enhancement\n  through Commonsense Knowledge Extraction", "abstract": "The objective of multimodal intent recognition (MIR) is to leverage various\nmodalities-such as text, video, and audio-to detect user intentions, which is\ncrucial for understanding human language and context in dialogue systems.\nDespite advances in this field, two main challenges persist: (1) effectively\nextracting and utilizing semantic information from robust textual features; (2)\naligning and fusing non-verbal modalities with verbal ones effectively. This\npaper proposes a Text Enhancement with CommOnsense Knowledge Extractor (TECO)\nto address these challenges. We begin by extracting relations from both\ngenerated and retrieved knowledge to enrich the contextual information in the\ntext modality. Subsequently, we align and integrate visual and acoustic\nrepresentations with these enhanced text features to form a cohesive multimodal\nrepresentation. Our experimental results show substantial improvements over\nexisting baseline methods.", "published": "2024-12-11 16:38:48", "link": "http://arxiv.org/abs/2412.08529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilevel Joint Unsupervised and Supervised Training for Automatic Speech\n  Recognition", "abstract": "In this paper, we propose a bilevel joint unsupervised and supervised\ntraining (BL-JUST) framework for automatic speech recognition. Compared to the\nconventional pre-training and fine-tuning strategy which is a disconnected\ntwo-stage process, BL-JUST tries to optimize an acoustic model such that it\nsimultaneously minimizes both the unsupervised and supervised loss functions.\nBecause BL-JUST seeks matched local optima of both loss functions, acoustic\nrepresentations learned by the acoustic model strike a good balance between\nbeing generic and task-specific. We solve the BL-JUST problem using\npenalty-based bilevel gradient descent and evaluate the trained deep neural\nnetwork acoustic models on various datasets with a variety of architectures and\nloss functions. We show that BL-JUST can outperform the widely-used\npre-training and fine-tuning strategy and some other popular semi-supervised\ntechniques.", "published": "2024-12-11 17:06:12", "link": "http://arxiv.org/abs/2412.08548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Der Effizienz- und Intelligenzbegriff in der Lexikographie und\n  kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte\n  nachbilden?", "abstract": "By means of pilot experiments for the language pair German and Galician, this\npaper examines the concept of efficiency and intelligence in lexicography and\nartificial intelligence, AI. The aim of the experiments is to gain empirically\nand statistically based insights into the lexicographical text type,dictionary\narticle, in the responses of ChatGPT 3.5, as well as into the lexicographical\ndata on which this chatbot was trained. Both quantitative and qualitative\nmethods are used for this purpose. The analysis is based on the evaluation of\nthe outputs of several sessions with the same prompt in ChatGPT 3.5. On the one\nhand, the algorithmic performance of intelligent systems is evaluated in\ncomparison with data from lexicographical works. On the other hand, the ChatGPT\ndata supplied is analysed using specific text passages of the aforementioned\nlexicographical text type. The results of this study not only help to evaluate\nthe efficiency of this chatbot regarding the creation of dictionary articles,\nbut also to delve deeper into the concept of intelligence, the thought\nprocesses and the actions to be carried out in both disciplines.", "published": "2024-12-11 18:18:07", "link": "http://arxiv.org/abs/2412.08599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models", "abstract": "Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic.", "published": "2024-12-11 18:37:56", "link": "http://arxiv.org/abs/2412.08615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BDA: Bangla Text Data Augmentation Framework", "abstract": "Data augmentation involves generating synthetic samples that resemble those\nin a given dataset. In resource-limited fields where high-quality data is\nscarce, augmentation plays a crucial role in increasing the volume of training\ndata. This paper introduces a Bangla Text Data Augmentation (BDA) Framework\nthat uses both pre-trained models and rule-based methods to create new variants\nof the text. A filtering process is included to ensure that the new text keeps\nthe same meaning as the original while also adding variety in the words used.\nWe conduct a comprehensive evaluation of the framework's effectiveness in\nBangla text classification tasks. Our framework achieved significant\nimprovement in F1 scores across five distinct datasets, delivering performance\nequivalent to models trained on 100% of the data while utilizing only 50% of\nthe training dataset. Additionally, we explore the impact of data scarcity by\nprogressively reducing the training data and augmenting it through BDA,\nresulting in notable F1 score enhancements. The study offers a thorough\nexamination of BDA's performance, identifying key factors for optimal results\nand addressing its limitations through detailed analysis.", "published": "2024-12-11 19:50:37", "link": "http://arxiv.org/abs/2412.08753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Concept Models: Language Modeling in a Sentence Representation\n  Space", "abstract": "LLMs have revolutionized the field of artificial intelligence and have\nemerged as the de-facto tool for many tasks. The current established technology\nof LLMs is to process input and generate output at the token level. This is in\nsharp contrast to humans who operate at multiple levels of abstraction, well\nbeyond single words, to analyze information and to generate creative content.\nIn this paper, we present an attempt at an architecture which operates on an\nexplicit higher-level semantic representation, which we name a concept.\nConcepts are language- and modality-agnostic and represent a higher level idea\nor action in a flow. Hence, we build a \"Large Concept Model\". In this study, as\nproof of feasibility, we assume that a concept corresponds to a sentence, and\nuse an existing sentence embedding space, SONAR, which supports up to 200\nlanguages in both text and speech modalities.\n  The Large Concept Model is trained to perform autoregressive sentence\nprediction in an embedding space. We explore multiple approaches, namely MSE\nregression, variants of diffusion-based generation, and models operating in a\nquantized SONAR space. These explorations are performed using 1.6B parameter\nmodels and training data in the order of 1.3T tokens. We then scale one\narchitecture to a model size of 7B parameters and training data of about 2.7T\ntokens. We perform an experimental evaluation on several generative tasks,\nnamely summarization and a new task of summary expansion. Finally, we show that\nour model exhibits impressive zero-shot generalization performance to many\nlanguages, outperforming existing LLMs of the same size. The training code of\nour models is freely available.", "published": "2024-12-11 23:36:20", "link": "http://arxiv.org/abs/2412.08821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concept Bottleneck Large Language Models", "abstract": "We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel\nframework for building inherently interpretable Large Language Models (LLMs).\nIn contrast to traditional black-box LLMs that rely on limited post-hoc\ninterpretations, CB-LLMs integrate intrinsic interpretability directly into the\nLLMs -- allowing accurate explanations with scalability and transparency. We\nbuild CB-LLMs for two essential NLP tasks: text classification and text\ngeneration. In text classification, CB-LLMs is competitive with, and at times\noutperforms, traditional black-box models while providing explicit and\ninterpretable reasoning. For the more challenging task of text generation,\ninterpretable neurons in CB-LLMs enable precise concept detection, controlled\ngeneration, and safer outputs. The embedded interpretability empowers users to\ntransparently identify harmful content, steer model behavior, and unlearn\nundesired concepts -- significantly enhancing the safety, reliability, and\ntrustworthiness of LLMs, which are critical capabilities notably absent in\nexisting models. Our code is available at\nhttps://github.com/Trustworthy-ML-Lab/CB-LLMs.", "published": "2024-12-11 00:04:10", "link": "http://arxiv.org/abs/2412.07992v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual\n  Illusions", "abstract": "In recent years, Visual Question Answering (VQA) has made significant\nstrides, particularly with the advent of multimodal models that integrate\nvision and language understanding. However, existing VQA datasets often\noverlook the complexities introduced by image illusions, which pose unique\nchallenges for both human perception and model interpretation. In this study,\nwe introduce a novel task called Illusory VQA, along with four specialized\ndatasets: IllusionMNIST, IllusionFashionMNIST, IllusionAnimals, and\nIllusionChar. These datasets are designed to evaluate the performance of\nstate-of-the-art multimodal models in recognizing and interpreting visual\nillusions. We assess the zero-shot performance of various models, fine-tune\nselected models on our datasets, and propose a simple yet effective solution\nfor illusion detection using Gaussian and blur low-pass filters. We show that\nthis method increases the performance of models significantly and in the case\nof BLIP-2 on IllusionAnimals without any fine-tuning, it outperforms humans.\nOur findings highlight the disparity between human and model perception of\nillusions and demonstrate that fine-tuning and specific preprocessing\ntechniques can significantly enhance model robustness. This work contributes to\nthe development of more human-like visual understanding in multimodal models\nand suggests future directions for adapting filters using learnable parameters.", "published": "2024-12-11 07:51:18", "link": "http://arxiv.org/abs/2412.08169v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DocSum: Domain-Adaptive Pre-training for Document Abstractive\n  Summarization", "abstract": "Abstractive summarization has made significant strides in condensing and\nrephrasing large volumes of text into coherent summaries. However, summarizing\nadministrative documents presents unique challenges due to domain-specific\nterminology, OCR-generated errors, and the scarcity of annotated datasets for\nmodel fine-tuning. Existing models often struggle to adapt to the intricate\nstructure and specialized content of such documents. To address these\nlimitations, we introduce DocSum, a domain-adaptive abstractive summarization\nframework tailored for administrative documents. Leveraging pre-training on\nOCR-transcribed text and fine-tuning with an innovative integration of\nquestion-answer pairs, DocSum enhances summary accuracy and relevance. This\napproach tackles the complexities inherent in administrative content, ensuring\noutputs that align with real-world business needs. To evaluate its\ncapabilities, we define a novel downstream task setting-Document Abstractive\nSummarization-which reflects the practical requirements of business and\norganizational settings. Comprehensive experiments demonstrate DocSum's\neffectiveness in producing high-quality summaries, showcasing its potential to\nimprove decision-making and operational workflows across the public and private\nsectors.", "published": "2024-12-11 08:36:50", "link": "http://arxiv.org/abs/2412.08196v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Preliminary Analysis of Automatic Word and Syllable Prominence\n  Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings", "abstract": "Automatic detection of prominence at the word and syllable-levels is critical\nfor building computer-assisted language learning systems. It has been shown\nthat prosody embeddings learned by the current state-of-the-art (SOTA)\ntext-to-speech (TTS) systems could generate word- and syllable-level prominence\nin the synthesized speech as natural as in native speech. To understand the\neffectiveness of prosody embeddings from TTS for prominence detection under\nnonnative context, a comparative analysis is conducted on the embeddings\nextracted from native and non-native speech considering the prominence-related\nembeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2.\nThese embeddings are extracted under two conditions considering: 1) only text,\n2) both speech and text. For the first condition, the embeddings are extracted\ndirectly from the TTS inference mode, whereas for the second condition, we\npropose to extract from the TTS under training mode. Experiments are conducted\non native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For\nexperimentation, word-level prominence locations are manually annotated for\nboth corpora. The highest relative improvement on word \\& syllable-level\nprominence detection accuracies with the TTS embeddings are found to be 13.7% &\n5.9% and 16.2% & 6.9% compared to those with the heuristic-based features and\nself-supervised Wav2Vec-2.0 representations, respectively.", "published": "2024-12-11 10:58:14", "link": "http://arxiv.org/abs/2412.08283v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adaptive Prompting for Continual Relation Extraction: A Within-Task\n  Variance Perspective", "abstract": "To address catastrophic forgetting in Continual Relation Extraction (CRE),\nmany current approaches rely on memory buffers to rehearse previously learned\nknowledge while acquiring new tasks. Recently, prompt-based methods have\nemerged as potent alternatives to rehearsal-based strategies, demonstrating\nstrong empirical performance. However, upon analyzing existing prompt-based\napproaches for CRE, we identified several critical limitations, such as\ninaccurate prompt selection, inadequate mechanisms for mitigating forgetting in\nshared parameters, and suboptimal handling of cross-task and within-task\nvariances. To overcome these challenges, we draw inspiration from the\nrelationship between prefix-tuning and mixture of experts, proposing a novel\napproach that employs a prompt pool for each task, capturing variations within\neach task while enhancing cross-task variances. Furthermore, we incorporate a\ngenerative model to consolidate prior knowledge within shared parameters,\neliminating the need for explicit data storage. Extensive experiments validate\nthe efficacy of our approach, demonstrating superior performance over\nstate-of-the-art prompt-based and rehearsal-free methods in continual relation\nextraction.", "published": "2024-12-11 11:00:33", "link": "http://arxiv.org/abs/2412.08285v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rumor Detection on Social Media with Temporal Propagation Structure\n  Optimization", "abstract": "Traditional methods for detecting rumors on social media primarily focus on\nanalyzing textual content, often struggling to capture the complexity of online\ninteractions. Recent research has shifted towards leveraging graph neural\nnetworks to model the hierarchical conversation structure that emerges during\nrumor propagation. However, these methods tend to overlook the temporal aspect\nof rumor propagation and may disregard potential noise within the propagation\nstructure. In this paper, we propose a novel approach that incorporates\ntemporal information by constructing a weighted propagation tree, where the\nweight of each edge represents the time interval between connected posts.\nDrawing upon the theory of structural entropy, we transform this tree into a\ncoding tree. This transformation aims to preserve the essential structure of\nrumor propagation while reducing noise. Finally, we introduce a recursive\nneural network to learn from the coding tree for rumor veracity prediction.\nExperimental results on two common datasets demonstrate the superiority of our\napproach.", "published": "2024-12-11 11:53:14", "link": "http://arxiv.org/abs/2412.08316v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better\n  Reasoning in SLMs", "abstract": "We present SmolTulu-1.7b-Instruct, referenced in this report as\nSmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's\nTulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model.\nThrough comprehensive empirical analysis using a 135M parameter model, we\ndemonstrate that the relationship between learning rate and batch size\nsignificantly impacts model performance in a task-dependent manner. Our\nfindings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from\nhigher learning rate to batch size ratios, while pattern recognition tasks such\nas HellaSwag and IFEval show optimal performance with lower ratios. These\ninsights informed the development of SmolTulu, which achieves state-of-the-art\nperformance among sub-2B parameter models on instruction following, scoring\n67.7% on IFEval ($\\Delta$11%), and mathematical reasoning with 51.6% on GSM8K\n($\\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC\n($\\Delta5.4%$). We release our model, training recipes, and ablation studies to\nfacilitate further research in efficient model alignment, demonstrating that\ncareful adaptation of optimization dynamics can help bridge the capability gap\nbetween small and large language models.", "published": "2024-12-11 12:41:36", "link": "http://arxiv.org/abs/2412.08347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Roles of English in Evaluating Multilingual Language Models", "abstract": "Multilingual natural language processing is getting increased attention, with\nnumerous models, benchmarks, and methods being released for many languages.\nEnglish is often used in multilingual evaluation to prompt language models\n(LMs), mainly to overcome the lack of instruction tuning data in other\nlanguages. In this position paper, we lay out two roles of English in\nmultilingual LM evaluations: as an interface and as a natural language. We\nargue that these roles have different goals: task performance versus language\nunderstanding. This discrepancy is highlighted with examples from datasets and\nevaluation setups. Numerous works explicitly use English as an interface to\nboost task performance. We recommend to move away from this imprecise method\nand instead focus on furthering language understanding.", "published": "2024-12-11 14:02:55", "link": "http://arxiv.org/abs/2412.08392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Personalized AI Mentoring with Large Language Models in the\n  Computing Field", "abstract": "This paper provides an in-depth evaluation of three state-of-the-art Large\nLanguage Models (LLMs) for personalized career mentoring in the computing\nfield, using three distinct student profiles that consider gender, race, and\nprofessional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2\nusing a zero-shot learning approach without human intervention. A quantitative\nevaluation was conducted through a custom natural language processing analytics\npipeline to highlight the uniqueness of the responses and to identify words\nreflecting each student's profile, including race, gender, or professional\nlevel. The analysis of frequently used words in the responses indicates that\nGPT-4 offers more personalized mentoring compared to the other two LLMs.\nAdditionally, a qualitative evaluation was performed to see if human experts\nreached similar conclusions. The analysis of survey responses shows that GPT-4\noutperformed the other two LLMs in delivering more accurate and useful\nmentoring while addressing specific challenges with encouragement languages.\nOur work establishes a foundation for developing personalized mentoring tools\nbased on LLMs, incorporating human mentors in the process to deliver a more\nimpactful and tailored mentoring experience.", "published": "2024-12-11 14:51:13", "link": "http://arxiv.org/abs/2412.08430v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Out-of-Entity Errors in Named Entity Recognition: A\n  Sentence-Level Strategy", "abstract": "Many previous models of named entity recognition (NER) suffer from the\nproblem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the\ntest samples have not appeared in the training samples, which hinders the\nachievement of satisfactory performance. To improve OOE-NER performance, in\nthis paper, we propose a new framework, namely S+NER, which fully leverages\nsentence-level information. Our S+NER achieves better OOE-NER performance\nmainly due to the following two particular designs. 1) It first exploits the\npre-trained language model's capability of understanding the target entity's\nsentence-level context with a template set. 2) Then, it refines the\nsentence-level representation based on the positive and negative templates,\nthrough a contrastive learning strategy and template pooling method, to obtain\nbetter NER results. Our extensive experiments on five benchmark datasets have\ndemonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.", "published": "2024-12-11 14:55:48", "link": "http://arxiv.org/abs/2412.08434v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Template-Based Visual Program Distillation", "abstract": "For users with limited computational resources, visual programming or\nprompting large language models (LLMs) to generate executable code for visual\ntasks, like visual question answering (VQA), remains largely inaccessible. Even\nwith techniques such as distillation, adapting visual programming to smaller\nmodels or specific datasets is still quite challenging due to high annotation\ncosts. We propose a low-cost visual program distillation method that can be\nused for models with fewer than 1 billion parameters and requires no\nhuman-generated program annotations. We achieve this through synthetic data\naugmentation based on decoupling programs into higher-level skills, called\ntemplates, and their corresponding arguments. Experimental results show that,\nwith a relatively small amount of question/answer data, small language models\ncan generate high-quality visual programs with the added benefit of much faster\ninference.", "published": "2024-12-11 17:32:21", "link": "http://arxiv.org/abs/2412.08564v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning", "abstract": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance.", "published": "2024-12-11 18:06:44", "link": "http://arxiv.org/abs/2412.08587v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fast Prompt Alignment for Text-to-Image Generation", "abstract": "Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment", "published": "2024-12-11 18:58:41", "link": "http://arxiv.org/abs/2412.08639v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "In-Context Learning with Topological Information for Knowledge Graph\n  Completion", "abstract": "Knowledge graphs (KGs) are crucial for representing and reasoning over\nstructured information, supporting a wide range of applications such as\ninformation retrieval, question answering, and decision-making. However, their\neffectiveness is often hindered by incompleteness, limiting their potential for\nreal-world impact. While knowledge graph completion (KGC) has been extensively\nstudied in the literature, recent advances in generative AI models,\nparticularly large language models (LLMs), have introduced new opportunities\nfor innovation. In-context learning has recently emerged as a promising\napproach for leveraging pretrained knowledge of LLMs across a range of natural\nlanguage processing tasks and has been widely adopted in both academia and\nindustry. However, how to utilize in-context learning for effective KGC remains\nrelatively underexplored. We develop a novel method that incorporates\ntopological information through in-context learning to enhance KGC performance.\nBy integrating ontological knowledge and graph structure into the context of\nLLMs, our approach achieves strong performance in the transductive setting\ni.e., nodes in the test graph dataset are present in the training graph\ndataset. Furthermore, we apply our approach to KGC in the more challenging\ninductive setting, i.e., nodes in the training graph dataset and test graph\ndataset are disjoint, leveraging the ontology to infer useful information about\nmissing nodes which serve as contextual cues for the LLM during inference. Our\nmethod demonstrates superior performance compared to baselines on the\nILPC-small and ILPC-large datasets.", "published": "2024-12-11 19:29:36", "link": "http://arxiv.org/abs/2412.08742v1", "categories": ["cs.CL", "cs.AI", "68T37 (Primary), 68T05, 68P20 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Coverage-based Fairness in Multi-document Summarization", "abstract": "Fairness in multi-document summarization (MDS) measures whether a system can\ngenerate a summary fairly representing information from documents with\ndifferent social attribute values. Fairness in MDS is crucial since a fair\nsummary can offer readers a comprehensive view. Previous works focus on\nquantifying summary-level fairness using Proportional Representation, a\nfairness measure based on Statistical Parity. However, Proportional\nRepresentation does not consider redundancy in input documents and overlooks\ncorpus-level unfairness. In this work, we propose a new summary-level fairness\nmeasure, Equal Coverage, which is based on coverage of documents with different\nsocial attribute values and considers the redundancy within documents. To\ndetect the corpus-level unfairness, we propose a new corpus-level measure,\nCoverage Parity. Our human evaluations show that our measures align more with\nour definition of fairness. Using our measures, we evaluate the fairness of\nthirteen different LLMs. We find that Claude3-sonnet is the fairest among all\nevaluated LLMs. We also find that almost all LLMs overrepresent different\nsocial attribute values. The code is available at\nhttps://github.com/leehaoyuan/coverage_fairness.", "published": "2024-12-11 22:01:30", "link": "http://arxiv.org/abs/2412.08795v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Makes In-context Learning Effective for Mathematical Reasoning: A\n  Theoretical Analysis", "abstract": "Owing to the capability of in-context learning, large language models (LLMs)\nhave shown impressive performance across diverse mathematical reasoning\nbenchmarks. However, we find that few-shot demonstrations can sometimes bring\nnegative performance and their effectiveness on LLMs' reasoning abilities\nremains unreliable. To this end, in this paper, we aim to theoretically analyze\nthe impact of in-context demonstrations on LLMs' reasoning performance. We\nprove that the reasoning efficacy (measured by empirical prediction loss) can\nbe bounded by a LLM-oriented semantic similarity and an inference stability of\ndemonstrations, which is general for both one-shot and few-shot scenarios.\nBased on this finding, we propose a straightforward, generalizable, and\nlow-complexity demonstration selection method named LMS3. It can adaptively\nfacilitate to select the most pertinent samples for different LLMs and includes\na novel demonstration rejection mechanism to automatically filter out samples\nthat are unsuitable for few-shot learning. Through experiments on three\nrepresentative benchmarks, two LLM backbones, and multiple few-shot settings,\nwe verify that our LMS3 has superiority and achieves consistent improvements on\nall datasets, which existing methods have been unable to accomplish.", "published": "2024-12-11 11:38:11", "link": "http://arxiv.org/abs/2412.12157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Performance of a large language model-Artificial Intelligence based\n  chatbot for counseling patients with sexually transmitted infections and\n  genital diseases", "abstract": "Introduction: Global burden of sexually transmitted infections (STIs) is\nrising out of proportion to specialists. Current chatbots like ChatGPT are not\ntailored for handling STI-related concerns out of the box. We developed Otiz,\nan Artificial Intelligence-based (AI-based) chatbot platform designed\nspecifically for STI detection and counseling, and assessed its performance.\nMethods: Otiz employs a multi-agent system architecture based on GPT4-0613,\nleveraging large language model (LLM) and Deterministic Finite Automaton\nprinciples to provide contextually relevant, medically accurate, and empathetic\nresponses. Its components include modules for general STI information,\nemotional recognition, Acute Stress Disorder detection, and psychotherapy. A\nquestion suggestion agent operates in parallel. Four STIs (anogenital warts,\nherpes, syphilis, urethritis/cervicitis) and 2 non-STIs (candidiasis, penile\ncancer) were evaluated using prompts mimicking patient language. Each prompt\nwas independently graded by two venereologists conversing with Otiz as patient\nactors on 6 criteria using Numerical Rating Scale ranging from 0 (poor) to 5\n(excellent). Results: Twenty-three venereologists did 60 evaluations of 30\nprompts. Across STIs, Otiz scored highly on diagnostic accuracy (4.1-4.7),\noverall accuracy (4.3-4.6), correctness of information (5.0), comprehensibility\n(4.2-4.4), and empathy (4.5-4.8). However, relevance scores were lower\n(2.9-3.6), suggesting some redundancy. Diagnostic scores for non-STIs were\nlower (p=0.038). Inter-observer agreement was strong, with differences greater\nthan 1 point occurring in only 12.7% of paired evaluations. Conclusions: AI\nconversational agents like Otiz can provide accurate, correct, discrete,\nnon-judgmental, readily accessible and easily understandable STI-related\ninformation in an empathetic manner, and can alleviate the burden on healthcare\nsystems.", "published": "2024-12-11 20:36:32", "link": "http://arxiv.org/abs/2412.12166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multimodal Social Agent", "abstract": "In recent years, large language models (LLMs) have demonstrated remarkable\nprogress in common-sense reasoning tasks. This ability is fundamental to\nunderstanding social dynamics, interactions, and communication. However, the\npotential of integrating computers with these social capabilities is still\nrelatively unexplored. However, the potential of integrating computers with\nthese social capabilities is still relatively unexplored. This paper introduces\nMuSA, a multimodal LLM-based agent that analyzes text-rich social content\ntailored to address selected human-centric content analysis tasks, such as\nquestion answering, visual question answering, title generation, and\ncategorization. It uses planning, reasoning, acting, optimizing, criticizing,\nand refining strategies to complete a task. Our approach demonstrates that MuSA\ncan automate and improve social content analysis, helping decision-making\nprocesses across various applications. We have evaluated our agent's\ncapabilities in question answering, title generation, and content\ncategorization tasks. MuSA performs substantially better than our baselines.", "published": "2024-12-11 22:04:27", "link": "http://arxiv.org/abs/2501.06189v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach", "abstract": "Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method.", "published": "2024-12-11 02:37:32", "link": "http://arxiv.org/abs/2412.08038v3", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Federated In-Context LLM Agent Learning", "abstract": "Large Language Models (LLMs) have revolutionized intelligent services by\nenabling logical reasoning, tool use, and interaction with external systems as\nagents. The advancement of LLMs is frequently hindered by the scarcity of\nhigh-quality data, much of which is inherently sensitive. Federated learning\n(FL) offers a potential solution by facilitating the collaborative training of\ndistributed LLMs while safeguarding private data. However, FL frameworks face\nsignificant bandwidth and computational demands, along with challenges from\nheterogeneous data distributions. The emerging in-context learning capability\nof LLMs offers a promising approach by aggregating natural language rather than\nbulky model parameters. Yet, this method risks privacy leakage, as it\nnecessitates the collection and presentation of data samples from various\nclients during aggregation. In this paper, we propose a novel\nprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,\nwhich to our best knowledge for the first work unleashes the power of\nin-context learning to train diverse LLM agents through FL. In our design,\nknowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums\nGeneration (KCG) module are transmitted between clients and the server instead\nof model parameters in previous FL methods. Apart from that, an incredible\nRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)\nmodule is designed and we incorporate the aggregated global knowledge\ncompendium as a teacher to teach LLM agents the usage of tools. We conducted\nextensive experiments and the results show that FICAL has competitive\nperformance compared to other SOTA baselines with a significant communication\ncost decrease of $\\mathbf{3.33\\times10^5}$ times.", "published": "2024-12-11 03:00:24", "link": "http://arxiv.org/abs/2412.08054v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages", "abstract": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages.", "published": "2024-12-11 04:16:39", "link": "http://arxiv.org/abs/2412.08090v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting", "abstract": "Large Language Models (LLMs) have recently demonstrated significant potential\nin time series forecasting, offering impressive capabilities in handling\ncomplex temporal data. However, their robustness and reliability in real-world\napplications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like LLMTime with GPT-3.5, GPT-4, LLaMa, and\nMistral, TimeGPT, and TimeLLM show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications. The code repository can be found at\nhttps://github.com/JohnsonJiang1996/AdvAttack_LLM4TS.", "published": "2024-12-11 04:53:15", "link": "http://arxiv.org/abs/2412.08099v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation", "abstract": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.", "published": "2024-12-11 05:23:34", "link": "http://arxiv.org/abs/2412.08108v2", "categories": ["cs.CV", "cs.CL", "cs.CR"], "primary_category": "cs.CV"}
{"title": "Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic\n  Losses", "abstract": "Vision-Language Models (VLMs) implicitly learn to associate image regions\nwith words from large-scale training data, demonstrating an emergent capability\nfor grounding concepts without dense annotations[14,18,51]. However, the\ncoarse-grained supervision from image-caption pairs is often insufficient to\nresolve ambiguities in object-concept correspondence, even with enormous data\nvolume. Rich semantic and syntactic structures within the text modality have\nbeen overlooked as sources of supervision. Starting from contrastive\narchitectures (BLIP and ALBEF) that show strong intrinsic grounding abilities,\nwe propose HIerarchically STructured Learning (HIST). HIST enhances spatial\nvision-language alignment without using additional human annotations, by\nhierarchically decomposing captions into the constituent Subjects, Phrases, and\nComposite Phrases, and enforcing entailment relation between a parent and its\nchildren in the hierarchy. Specifically, we introduce two novel loss functions:\n(1) Subject Loss, which aligns image content with the subject of the\ncorresponding phrase, acting as an entailment of standard contrastive/matching\nlosses at the Phrase level; (2) Composition Loss, to balance attention across\nmultiple objects. HIST is general, and can be applied to any VLM for which\nattention between vision and language can be computed. Compared to baseline\nVLMs, HIST achieves up to +9.8% improvement in visual grounding and +6.3% in\nmulti-object referring segmentation. Surprisingly, the improved spatial\ngrounding leads to improvements in other downstream VLM tasks: +1.1% in\nimage-text retrieval, and +0.2% in visual question answering.", "published": "2024-12-11 05:36:18", "link": "http://arxiv.org/abs/2412.08110v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Seeing Syntax: Uncovering Syntactic Learning Limitations in\n  Vision-Language Models", "abstract": "Vision-language models (VLMs), serve as foundation models for multi-modal\napplications such as image captioning and text-to-image generation. Recent\nstudies have highlighted limitations in VLM text encoders, particularly in\nareas like compositionality and semantic understanding, though the underlying\nreasons for these limitations remain unclear. In this work, we aim to address\nthis gap by analyzing the syntactic information, one of the fundamental\nlinguistic properties, encoded by the text encoders of VLMs. We perform a\nthorough analysis comparing VLMs with different objective functions, parameter\nsize and training data size, and with uni-modal language models (ULMs) in their\nability to encode syntactic knowledge. Our findings suggest that ULM text\nencoders acquire syntactic information more effectively than those in VLMs. The\nsyntactic information learned by VLM text encoders is shaped primarily by the\npre-training objective, which plays a more crucial role than other factors such\nas model architecture, model size, or the volume of pre-training data. Models\nexhibit different layer-wise trends where CLIP performance dropped across\nlayers while for other models, middle layers are rich in encoding syntactic\nknowledge.", "published": "2024-12-11 05:37:04", "link": "http://arxiv.org/abs/2412.08111v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models", "abstract": "Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.", "published": "2024-12-11 06:21:33", "link": "http://arxiv.org/abs/2412.08125v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evil twins are not that evil: Qualitative insights into\n  machine-generated prompts", "abstract": "It has been widely observed that language models (LMs) respond in predictable\nways to algorithmically generated prompts that are seemingly unintelligible.\nThis is both a sign that we lack a full understanding of how LMs work, and a\npractical challenge, because opaqueness can be exploited for harmful uses of\nLMs, such as jailbreaking. We present the first thorough analysis of opaque\nmachine-generated prompts, or autoprompts, pertaining to 6 LMs of different\nsizes and families. We find that machine-generated prompts are characterized by\na last token that is often intelligible and strongly affects the generation. A\nsmall but consistent proportion of the previous tokens are prunable, probably\nappearing in the prompt as a by-product of the fact that the optimization\nprocess fixes the number of tokens. The remaining tokens fall into two\ncategories: filler tokens, which can be replaced with semantically unrelated\nsubstitutes, and keywords, that tend to have at least a loose semantic relation\nwith the generation, although they do not engage in well-formed syntactic\nrelations with it. Additionally, human experts can reliably identify the most\ninfluential tokens in an autoprompt a posteriori, suggesting these prompts are\nnot entirely opaque. Finally, some of the ablations we applied to autoprompts\nyield similar effects in natural language inputs, suggesting that autoprompts\nemerge naturally from the way LMs process linguistic inputs in general.", "published": "2024-12-11 06:22:44", "link": "http://arxiv.org/abs/2412.08127v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Vision-Language Tasks Benefit from Large Pre-trained Models: A\n  Survey", "abstract": "The exploration of various vision-language tasks, such as visual captioning,\nvisual question answering, and visual commonsense reasoning, is an important\narea in artificial intelligence and continuously attracts the research\ncommunity's attention. Despite the improvements in overall performance, classic\nchallenges still exist in vision-language tasks and hinder the development of\nthis area. In recent years, the rise of pre-trained models is driving the\nresearch on vision-language tasks. Thanks to the massive scale of training data\nand model parameters, pre-trained models have exhibited excellent performance\nin numerous downstream tasks. Inspired by the powerful capabilities of\npre-trained models, new paradigms have emerged to solve the classic challenges.\nSuch methods have become mainstream in current research with increasing\nattention and rapid advances. In this paper, we present a comprehensive\noverview of how vision-language tasks benefit from pre-trained models. First,\nwe review several main challenges in vision-language tasks and discuss the\nlimitations of previous solutions before the era of pre-training. Next, we\nsummarize the recent advances in incorporating pre-trained models to address\nthe challenges in vision-language tasks. Finally, we analyze the potential\nrisks associated with the inherent limitations of pre-trained models and\ndiscuss possible solutions, attempting to provide future research directions.", "published": "2024-12-11 07:29:04", "link": "http://arxiv.org/abs/2412.08158v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From communities to interpretable network and word embedding: an unified\n  approach", "abstract": "Modelling information from complex systems such as humans social interaction\nor words co-occurrences in our languages can help to understand how these\nsystems are organized and function. Such systems can be modelled by networks,\nand network theory provides a useful set of methods to analyze them. Among\nthese methods, graph embedding is a powerful tool to summarize the interactions\nand topology of a network in a vectorized feature space. When used in input of\nmachine learning algorithms, embedding vectors help with common graph problems\nsuch as link prediction, graph matching, etc. Word embedding has the goal of\nrepresenting the sense of words, extracting it from large text corpora. Despite\ndifferences in the structure of information in input of embedding algorithms,\nmany graph embedding approaches are adapted and inspired from methods in NLP.\nLimits of these methods are observed in both domains. Most of these methods\nrequire long and resource greedy training. Another downside to most methods is\nthat they are black-box, from which understanding how the information is\nstructured is rather complex. Interpretability of a model allows understanding\nhow the vector space is structured without the need for external information,\nand thus can be audited more easily. With both these limitations in mind, we\npropose a novel framework to efficiently embed network vertices in an\ninterpretable vector space. Our Lower Dimension Bipartite Framework (LDBGF)\nleverages the bipartite projection of a network using cliques to reduce\ndimensionality. Along with LDBGF, we introduce two implementations of this\nframework that rely on communities instead of cliques: SINr-NR and SINr-MF. We\nshow that SINr-MF can perform well on classical graphs and SINr-NR can produce\nhigh-quality graph and word embeddings that are interpretable and stable across\nruns.", "published": "2024-12-11 08:27:25", "link": "http://arxiv.org/abs/2412.08187v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch", "abstract": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data.", "published": "2024-12-11 09:38:50", "link": "http://arxiv.org/abs/2412.08237v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis", "abstract": "The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions.", "published": "2024-12-11 13:50:17", "link": "http://arxiv.org/abs/2412.08385v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining\n  Data Flywheel", "abstract": "Creating high-quality data for training robust language-instructed agents is\na long-lasting challenge in embodied AI. In this paper, we introduce a\nSelf-Refining Data Flywheel (SRDF) that generates high-quality and large-scale\nnavigational instruction-trajectory pairs by iteratively refining the data pool\nthrough the collaboration between two models, the instruction generator and the\nnavigator, without any human-in-the-loop annotation. Specifically, SRDF starts\nwith using a base generator to create an initial data pool for training a base\nnavigator, followed by applying the trained navigator to filter the data pool.\nThis leads to higher-fidelity data to train a better generator, which can, in\nturn, produce higher-quality data for training the next-round navigator. Such a\nflywheel establishes a data self-refining process, yielding a continuously\nimproved and highly effective dataset for large-scale language-guided\nnavigation learning. Our experiments demonstrate that after several flywheel\nrounds, the navigator elevates the performance boundary from 70% to 78% SPL on\nthe classic R2R test set, surpassing human performance (76%) for the first\ntime. Meanwhile, this process results in a superior generator, evidenced by a\nSPICE increase from 23.5 to 26.2, better than all previous VLN instruction\ngeneration methods. Finally, we demonstrate the scalability of our method\nthrough increasing environment and instruction diversity, and the\ngeneralization ability of our pre-trained navigator across various downstream\nnavigation tasks, surpassing state-of-the-art methods by a large margin in all\ncases.", "published": "2024-12-11 15:32:24", "link": "http://arxiv.org/abs/2412.08467v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek", "abstract": "We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP)\ntoolkit developed specifically for modern Greek. The toolkit provides\nstate-of-the-art performance in five core NLP tasks, namely part-of-speech\ntagging, morphological tagging, dependency parsing, named entity recognition,\nand Greeklishto-Greek transliteration. The toolkit is based on pre-trained\nTransformers, it is freely available, and can be easily installed in Python\n(pip install gr-nlp-toolkit). It is also accessible through a demonstration\nplatform on HuggingFace, along with a publicly available API for non-commercial\nuse. We discuss the functionality provided for each task, the underlying\nmethods, experiments against comparable open-source toolkits, and future\npossible enhancements. The toolkit is available at:\nhttps://github.com/nlpaueb/gr-nlp-toolkit", "published": "2024-12-11 16:34:23", "link": "http://arxiv.org/abs/2412.08520v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback", "abstract": "Describing skills in natural language has the potential to provide an\naccessible way to inject human knowledge about decision-making into an AI\nsystem. We present MaestroMotif, a method for AI-assisted skill design, which\nyields high-performing and adaptable agents. MaestroMotif leverages the\ncapabilities of Large Language Models (LLMs) to effectively create and reuse\nskills. It first uses an LLM's feedback to automatically design rewards\ncorresponding to each skill, starting from their natural language description.\nThen, it employs an LLM's code generation abilities, together with\nreinforcement learning, for training the skills and combining them to implement\ncomplex behaviors specified in language. We evaluate MaestroMotif using a suite\nof complex tasks in the NetHack Learning Environment (NLE), demonstrating that\nit surpasses existing approaches in both performance and usability.", "published": "2024-12-11 16:59:31", "link": "http://arxiv.org/abs/2412.08542v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Machine Learning Information Retrieval and Summarisation to Support\n  Systematic Review on Outcomes Based Contracting", "abstract": "As academic literature proliferates, traditional review methods are\nincreasingly challenged by the sheer volume and diversity of available\nresearch. This article presents a study that aims to address these challenges\nby enhancing the efficiency and scope of systematic reviews in the social\nsciences through advanced machine learning (ML) and natural language processing\n(NLP) tools. In particular, we focus on automating stages within the systematic\nreviewing process that are time-intensive and repetitive for human annotators\nand which lend themselves to immediate scalability through tools such as\ninformation retrieval and summarisation guided by expert advice. The article\nconcludes with a summary of lessons learnt regarding the integrated approach\ntowards systematic reviews and future directions for improvement, including\nexplainability.", "published": "2024-12-11 17:54:01", "link": "http://arxiv.org/abs/2412.08578v1", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multimodal Latent Language Modeling with Next-Token Diffusion", "abstract": "Multimodal generative models require a unified approach to handle both\ndiscrete data (e.g., text and code) and continuous data (e.g., image, audio,\nvideo). In this work, we propose Latent Language Modeling (LatentLM), which\nseamlessly integrates continuous and discrete data using causal Transformers.\nSpecifically, we employ a variational autoencoder (VAE) to represent continuous\ndata as latent vectors and introduce next-token diffusion for autoregressive\ngeneration of these vectors. Additionally, we develop $\\sigma$-VAE to address\nthe challenges of variance collapse, which is crucial for autoregressive\nmodeling. Extensive experiments demonstrate the effectiveness of LatentLM\nacross various modalities. In image generation, LatentLM surpasses Diffusion\nTransformers in both performance and scalability. When integrated into\nmultimodal large language models, LatentLM provides a general-purpose interface\nthat unifies multimodal generation and understanding. Experimental results show\nthat LatentLM achieves favorable performance compared to Transfusion and vector\nquantized models in the setting of scaling up training tokens. In\ntext-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2\nmodel in speaker similarity and robustness, while requiring 10x fewer decoding\nsteps. The results establish LatentLM as a highly effective and scalable\napproach to advance large multimodal models.", "published": "2024-12-11 18:57:32", "link": "http://arxiv.org/abs/2412.08635v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LatentQA: Teaching LLMs to Decode Activations Into Natural Language", "abstract": "Interpretability methods seek to understand language model representations,\nyet the outputs of most such methods -- circuits, vectors, scalars -- are not\nimmediately human-interpretable. In response, we introduce LatentQA, the task\nof answering open-ended questions about model activations in natural language.\nTowards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which\nfinetunes a decoder LLM on a dataset of activations and associated\nquestion-answer pairs, similar to how visual instruction tuning trains on\nquestion-answer pairs associated with images. We use the decoder for diverse\nreading applications, such as extracting relational knowledge from\nrepresentations or uncovering system prompts governing model behavior. Our\ndecoder also specifies a differentiable loss that we use to control models,\nsuch as debiasing models on stereotyped sentences and controlling the sentiment\nof generations. Finally, we extend LatentQA to reveal harmful model\ncapabilities, such as generating recipes for bioweapons and code for hacking.", "published": "2024-12-11 18:59:33", "link": "http://arxiv.org/abs/2412.08686v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity\n  Visual Descriptions", "abstract": "Multimodal large language models (MLLMs) have made rapid progress in recent\nyears, yet continue to struggle with low-level visual perception (LLVP) --\nparticularly the ability to accurately describe the geometric details of an\nimage. This capability is crucial for applications in areas such as robotics,\nmedical image analysis, and manufacturing. In this paper, we first introduce\nGeoperception, a benchmark designed to evaluate an MLLM's ability to accurately\ntranscribe 2D geometric information from an image. Using this benchmark, we\ndemonstrate the limitations of leading MLLMs, and then conduct a comprehensive\nempirical study to explore strategies for improving their performance on\ngeometric tasks. Our findings highlight the benefits of certain model\narchitectures, training techniques, and data strategies, including the use of\nhigh-fidelity synthetic data and multi-stage training with a data curriculum.\nNotably, we find that a data curriculum enables models to learn challenging\ngeometry understanding tasks which they fail to learn from scratch. Leveraging\nthese insights, we develop Euclid, a family of models specifically optimized\nfor strong low-level geometric perception. Although purely trained on synthetic\nmultimodal data, Euclid shows strong generalization ability to novel geometry\nshapes. For instance, Euclid outperforms the best closed-source model,\nGemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and\n10.65% on average across all tasks.", "published": "2024-12-11 19:12:13", "link": "http://arxiv.org/abs/2412.08737v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images", "abstract": "Contrastive Language-Image Pretraining (CLIP) is a highly effective method\nfor aligning images and texts in a shared embedding space. These models are\nwidely used for tasks such as cross-modal information retrieval and multi-modal\nunderstanding. However, CLIP models often struggle with text-only tasks,\nunderperforming compared to specialized text models. This performance disparity\nforces retrieval systems to rely on separate models for text-only and\nmulti-modal tasks. In this work, we build upon our previous model,\njina-clip-v1, by introducing a refined framework that utilizes multi-task,\nmulti-stage contrastive learning across multiple languages, coupled with an\nimproved training recipe to enhance text-only retrieval. The resulting model,\njina-clip-v2, outperforms its predecessor on text-only and multimodal tasks,\nwhile adding multilingual support, better understanding of complex visual\ndocuments and efficiency gains thanks to Matryoshka Representation Learning and\nvector truncation. The model performs comparably to the state-of-the-art in\nboth multilingual-multimodal and multilingual text retrieval benchmarks,\naddressing the challenge of unifying text-only and multi-modal retrieval\nsystems.", "published": "2024-12-11 22:28:12", "link": "http://arxiv.org/abs/2412.08802v1", "categories": ["cs.CL", "cs.CV", "cs.IR", "68T50", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Imitate Before Detect: Aligning Machine Stylistic Preference for\n  Machine-Revised Text Detection", "abstract": "Large Language Models (LLMs) have revolutionized text generation, making\ndetecting machine-generated text increasingly challenging. Although past\nmethods have achieved good performance on detecting pure machine-generated\ntext, those detectors have poor performance on distinguishing machine-revised\ntext (rewriting, expansion, and polishing), which can have only minor changes\nfrom its original human prompt. As the content of text may originate from human\nprompts, detecting machine-revised text often involves identifying distinctive\nmachine styles, e.g., worded favored by LLMs. However, existing methods\nstruggle to detect machine-style phrasing hidden within the content contributed\nby humans. We propose the \"Imitate Before Detect\" (ImBD) approach, which first\nimitates the machine-style token distribution, and then compares the\ndistribution of the text to be tested with the machine-style distribution to\ndetermine whether the text has been machine-revised. To this end, we introduce\nstyle preference optimization (SPO), which aligns a scoring LLM model to the\npreference of text styles generated by machines. The aligned scoring model is\nthen used to calculate the style-conditional probability curvature (Style-CPC),\nquantifying the log probability difference between the original and\nconditionally sampled texts for effective detection. We conduct extensive\ncomparisons across various scenarios, encompassing text revisions by six LLMs,\nfour distinct text domains, and three machine revision types. Compared to\nexisting state-of-the-art methods, our method yields a 13% increase in AUC for\ndetecting text revised by open-source LLMs, and improves performance by 5% and\n19% for detecting GPT-3.5 and GPT-4o revised text, respectively. Notably, our\nmethod surpasses the commercially trained GPT-Zero with just $1,000$ samples\nand five minutes of SPO, demonstrating its efficiency and effectiveness.", "published": "2024-12-11 03:17:14", "link": "http://arxiv.org/abs/2412.10432v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "NAT-NL2GQL: A Novel Multi-Agent Framework for Translating Natural\n  Language to Graph Query Language", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized many fields,\nnot only traditional natural language processing (NLP) tasks. Recently,\nresearch on applying LLMs to the database field has been booming, and as a\ntypical non-relational database, the use of LLMs in graph database research has\nnaturally gained significant attention. Recent efforts have increasingly\nfocused on leveraging LLMs to translate natural language into graph query\nlanguage (NL2GQL). Although some progress has been made, these methods have\nclear limitations, such as their reliance on streamlined processes that often\noverlook the potential of LLMs to autonomously plan and collaborate with other\nLLMs in tackling complex NL2GQL challenges. To address this gap, we propose\nNAT-NL2GQL, a novel multi-agent framework for translating natural language to\ngraph query language. Specifically, our framework consists of three synergistic\nagents: the Preprocessor agent, the Generator agent, and the Refiner agent. The\nPreprocessor agent manages data processing as context, including tasks such as\nname entity recognition, query rewriting, path linking, and the extraction of\nquery-related schemas. The Generator agent is a fine-tuned LLM trained on\nNL-GQL data, responsible for generating corresponding GQL statements based on\nqueries and their related schemas. The Refiner agent is tasked with refining\nthe GQL or context using error information obtained from the GQL execution\nresults. Given the scarcity of high-quality open-source NL2GQL datasets based\non nGQL syntax, we developed StockGQL, a dataset constructed from a financial\nmarket graph database. It is available at:\nhttps://github.com/leonyuancode/StockGQL. Experimental results on the StockGQL\nand SpCQL datasets reveal that our method significantly outperforms baseline\napproaches, highlighting its potential for advancing NL2GQL research.", "published": "2024-12-11 04:14:09", "link": "http://arxiv.org/abs/2412.10434v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Rethinking Comprehensive Benchmark for Chart Understanding: A\n  Perspective from Scientific Literature", "abstract": "Scientific Literature charts often contain complex visual elements, including\nmulti-plot figures, flowcharts, structural diagrams and etc. Evaluating\nmultimodal models using these authentic and intricate charts provides a more\naccurate assessment of their understanding abilities. However, existing\nbenchmarks face limitations: a narrow range of chart types, overly simplistic\ntemplate-based questions and visual elements, and inadequate evaluation\nmethods. These shortcomings lead to inflated performance scores that fail to\nhold up when models encounter real-world scientific charts. To address these\nchallenges, we introduce a new benchmark, Scientific Chart QA (SCI-CQA), which\nemphasizes flowcharts as a critical yet often overlooked category. To overcome\nthe limitations of chart variety and simplistic visual elements, we curated a\ndataset of 202,760 image-text pairs from 15 top-tier computer science\nconferences papers over the past decade. After rigorous filtering, we refined\nthis to 37,607 high-quality charts with contextual information. SCI-CQA also\nintroduces a novel evaluation framework inspired by human exams, encompassing\n5,629 carefully curated questions, both objective and open-ended. Additionally,\nwe propose an efficient annotation pipeline that significantly reduces data\nannotation costs. Finally, we explore context-based chart understanding,\nhighlighting the crucial role of contextual information in solving previously\nunanswerable questions.", "published": "2024-12-11 05:29:54", "link": "http://arxiv.org/abs/2412.12150v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PyOD 2: A Python Library for Outlier Detection with LLM-powered Model\n  Selection", "abstract": "Outlier detection (OD), also known as anomaly detection, is a critical\nmachine learning (ML) task with applications in fraud detection, network\nintrusion detection, clickstream analysis, recommendation systems, and social\nnetwork moderation. Among open-source libraries for outlier detection, the\nPython Outlier Detection (PyOD) library is the most widely adopted, with over\n8,500 GitHub stars, 25 million downloads, and diverse industry usage. However,\nPyOD currently faces three limitations: (1) insufficient coverage of modern\ndeep learning algorithms, (2) fragmented implementations across PyTorch and\nTensorFlow, and (3) no automated model selection, making it hard for\nnon-experts.\n  To address these issues, we present PyOD Version 2 (PyOD 2), which integrates\n12 state-of-the-art deep learning models into a unified PyTorch framework and\nintroduces a large language model (LLM)-based pipeline for automated OD model\nselection. These improvements simplify OD workflows, provide access to 45\nalgorithms, and deliver robust performance on various datasets. In this paper,\nwe demonstrate how PyOD 2 streamlines the deployment and automation of OD\nmodels and sets a new standard in both research and industry. PyOD 2 is\naccessible at\n[https://github.com/yzhao062/pyod](https://github.com/yzhao062/pyod). This\nstudy aligns with the Web Mining and Content Analysis track, addressing topics\nsuch as the robustness of Web mining methods and the quality of\nalgorithmically-generated Web data.", "published": "2024-12-11 07:53:20", "link": "http://arxiv.org/abs/2412.12154v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations\n  Generation", "abstract": "In the vast majority of the academic and scientific domains, LaTeX has\nestablished itself as the de facto standard for typesetting complex\nmathematical equations and formulae. However, LaTeX's complex syntax and\ncode-like appearance present accessibility barriers for individuals with\ndisabilities, as well as those unfamiliar with coding conventions. In this\npaper, we present a novel solution to this challenge through the development of\na novel speech-to-LaTeX equations system specifically designed for the Greek\nlanguage. We propose an end-to-end system that harnesses the power of Automatic\nSpeech Recognition (ASR) and Natural Language Processing (NLP) techniques to\nenable users to verbally dictate mathematical expressions and equations in\nnatural language, which are subsequently converted into LaTeX format. We\npresent the architecture and design principles of our system, highlighting key\ncomponents such as the ASR engine, the LLM-based prompt-driven equations\ngeneration mechanism, as well as the application of a custom evaluation metric\nemployed throughout the development process. We have made our system open\nsource and available at https://github.com/magcil/greek-speech-to-math.", "published": "2024-12-11 22:29:44", "link": "http://arxiv.org/abs/2412.12167v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Aligner-Guided Training Paradigm: Advancing Text-to-Speech Models with\n  Aligner Guided Duration", "abstract": "Recent advancements in text-to-speech (TTS) systems, such as FastSpeech and\nStyleSpeech, have significantly improved speech generation quality. However,\nthese models often rely on duration generated by external tools like the\nMontreal Forced Aligner, which can be time-consuming and lack flexibility. The\nimportance of accurate duration is often underestimated, despite their crucial\nrole in achieving natural prosody and intelligibility. To address these\nlimitations, we propose a novel Aligner-Guided Training Paradigm that\nprioritizes accurate duration labelling by training an aligner before the TTS\nmodel. This approach reduces dependence on external tools and enhances\nalignment accuracy. We further explore the impact of different acoustic\nfeatures, including Mel-Spectrograms, MFCCs, and latent features, on TTS model\nperformance. Our experimental results show that aligner-guided duration\nlabelling can achieve up to a 16\\% improvement in word error rate and\nsignificantly enhance phoneme and tone alignment. These findings highlight the\neffectiveness of our approach in optimizing TTS systems for more natural and\nintelligible speech generation.", "published": "2024-12-11 05:39:12", "link": "http://arxiv.org/abs/2412.08112v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decoding Poultry Vocalizations -- Natural Language Processing and\n  Transformer Models for Semantic and Emotional Analysis", "abstract": "Deciphering the acoustic language of chickens offers new opportunities in\nanimal welfare and ecological informatics. Their subtle vocal signals encode\nhealth conditions, emotional states, and dynamic interactions within\necosystems. Understanding the semantics of these calls provides a valuable tool\nfor interpreting their functional vocabulary and clarifying how each sound\nserves a specific purpose in social and environmental contexts. We apply\nadvanced Natural Language Processing and transformer based models to translate\nbioacoustic data into meaningful insights. Our method integrates Wave2Vec 2.0\nfor raw audio feature extraction with a fine tuned Bidirectional Encoder\nRepresentations from Transformers model, pretrained on a broad corpus of animal\nsounds and adapted to poultry tasks. This pipeline decodes poultry\nvocalizations into interpretable categories including distress calls, feeding\nsignals, and mating vocalizations, revealing emotional nuances often overlooked\nby conventional analyses. Achieving 92 percent accuracy in classifying key\nvocalization types, our approach demonstrates the feasibility of real time\nautomated monitoring of flock health and stress. By tracking this functional\nvocabulary, farmers can respond proactively to environmental or behavioral\nchanges, improving poultry welfare, reducing stress related productivity\nlosses, and supporting more sustainable farm management. Beyond agriculture,\nthis research enhances our understanding of computational ecology. Accessing\nthe semantic foundation of animal calls may indicate biodiversity,\nenvironmental stressors, and species interactions, informing integrative\necosystem level decision making.", "published": "2024-12-11 06:44:32", "link": "http://arxiv.org/abs/2412.16182v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation", "abstract": "Diffusion-based Generative AI gains significant attention for its superior\nperformance over other generative techniques like Generative Adversarial\nNetworks and Variational Autoencoders. While it has achieved notable\nadvancements in fields such as computer vision and natural language processing,\ntheir application in speech generation remains under-explored. Mainstream\nText-to-Speech systems primarily map outputs to Mel-Spectrograms in the\nspectral space, leading to high computational loads due to the sparsity of\nMelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS\ngeneration approach utilizing latent diffusion models. By using latent\nembeddings as the intermediate representation, LatentSpeech reduces the target\ndimension to 5% of what is required for MelSpecs, simplifying the processing\nfor the TTS encoder and vocoder and enabling efficient high-quality speech\ngeneration. This study marks the first integration of latent diffusion models\nin TTS, enhancing the accuracy and naturalness of generated speech.\nExperimental results on benchmark datasets demonstrate that LatentSpeech\nachieves a 25% improvement in Word Error Rate and a 24% improvement in Mel\nCepstral Distortion compared to existing models, with further improvements\nrising to 49.5% and 26%, respectively, with additional training data. These\nfindings highlight the potential of LatentSpeech to advance the\nstate-of-the-art in TTS technology", "published": "2024-12-11 05:55:06", "link": "http://arxiv.org/abs/2412.08117v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Impact of Discriminative and Generative E2E Speech\n  Enhancement Models on Syllable Stress Preservation", "abstract": "Automatic syllable stress detection is a crucial component in\nComputer-Assisted Language Learning (CALL) systems for language learners.\nCurrent stress detection models are typically trained on clean speech, which\nmay not be robust in real-world scenarios where background noise is prevalent.\nTo address this, speech enhancement (SE) models, designed to enhance speech by\nremoving noise, might be employed, but their impact on preserving syllable\nstress patterns is not well studied. This study examines how different SE\nmodels, representing discriminative and generative modeling approaches, affect\nsyllable stress detection under noisy conditions. We assess these models by\napplying them to speech data with varying signal-to-noise ratios (SNRs) from 0\nto 20 dB, and evaluating their effectiveness in maintaining stress patterns.\nAdditionally, we explore different feature sets to determine which ones are\nmost effective for capturing stress patterns amidst noise. To further\nunderstand the impact of SE models, a human-based perceptual study is conducted\nto compare the perceived stress patterns in SE-enhanced speech with those in\nclean speech, providing insights into how well these models preserve syllable\nstress as perceived by listeners. Experiments are performed on English speech\ndata from non-native speakers of German and Italian. And the results reveal\nthat the stress detection performance is robust with the generative SE models\nwhen heuristic features are used. Also, the observations from the perceptual\nstudy are consistent with the stress detection outcomes under all SE models.", "published": "2024-12-11 11:36:24", "link": "http://arxiv.org/abs/2412.08306v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sketch2Sound: Controllable Audio Generation via Time-Varying Signals and\n  Sonic Imitations", "abstract": "We present Sketch2Sound, a generative audio model capable of creating\nhigh-quality sounds from a set of interpretable time-varying control signals:\nloudness, brightness, and pitch, as well as text prompts. Sketch2Sound can\nsynthesize arbitrary sounds from sonic imitations (i.e.,~a vocal imitation or a\nreference sound-shape). Sketch2Sound can be implemented on top of any\ntext-to-audio latent diffusion transformer (DiT), and requires only 40k steps\nof fine-tuning and a single linear layer per control, making it more\nlightweight than existing methods like ControlNet. To synthesize from\nsketchlike sonic imitations, we propose applying random median filters to the\ncontrol signals during training, allowing Sketch2Sound to be prompted using\ncontrols with flexible levels of temporal specificity. We show that\nSketch2Sound can synthesize sounds that follow the gist of input controls from\na vocal imitation while retaining the adherence to an input text prompt and\naudio quality compared to a text-only baseline. Sketch2Sound allows sound\nartists to create sounds with the semantic flexibility of text prompts and the\nexpressivity and precision of a sonic gesture or vocal imitation. Sound\nexamples are available at https://hugofloresgarcia.art/sketch2sound/.", "published": "2024-12-11 17:11:21", "link": "http://arxiv.org/abs/2412.08550v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues", "abstract": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues.", "published": "2024-12-11 09:55:09", "link": "http://arxiv.org/abs/2412.08247v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Unified Model For Voice and Accent Conversion In Speech and Singing\n  using Self-Supervised Learning and Feature Extraction", "abstract": "This paper presents a new voice conversion model capable of transforming both\nspeaking and singing voices. It addresses key challenges in current systems,\nsuch as conveying emotions, managing pronunciation and accent changes, and\nreproducing non-verbal sounds. One of the model's standout features is its\nability to perform accent conversion on hybrid voice samples that encompass\nboth speech and singing, allowing it to change the speaker's accent while\npreserving the original content and prosody. The proposed model uses an\nencoder-decoder architecture: the encoder is based on HuBERT to process the\nspeech's acoustic and linguistic content, while the HiFi-GAN decoder audio\nmatches the target speaker's voice. The model incorporates fundamental\nfrequency (f0) features and singer embeddings to enhance performance while\nensuring the pitch & tone accuracy and vocal identity are preserved during\ntransformation. This approach improves how naturally and flexibly voice style\ncan be transformed, showing strong potential for applications in voice dubbing,\ncontent creation, and technologies like Text-to-Speech (TTS) and Interactive\nVoice Response (IVR) systems.", "published": "2024-12-11 11:47:39", "link": "http://arxiv.org/abs/2412.08312v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "68T50", "I.2.7"], "primary_category": "cs.SD"}
{"title": "SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing\n  and Fingering", "abstract": "Automatically generating realistic musical performance motion can greatly\nenhance digital media production, often involving collaboration between\nprofessionals and musicians. However, capturing the intricate body, hand, and\nfinger movements required for accurate musical performances is challenging.\nExisting methods often fall short due to the complex mapping between audio and\nmotion, typically requiring additional inputs like scores or MIDI data. In this\nwork, we present SyncViolinist, a multi-stage end-to-end framework that\ngenerates synchronized violin performance motion solely from audio input. Our\nmethod overcomes the challenge of capturing both global and fine-grained\nperformance features through two key modules: a bowing/fingering module and a\nmotion generation module. The bowing/fingering module extracts detailed playing\ninformation from the audio, which the motion generation module uses to create\nprecise, coordinated body motions reflecting the temporal granularity and\nnature of the violin performance. We demonstrate the effectiveness of\nSyncViolinist with significantly improved qualitative and quantitative results\nfrom unseen violin performance audio, outperforming state-of-the-art methods.\nExtensive subjective evaluations involving professional violinists further\nvalidate our approach. The code and dataset are available at\nhttps://github.com/Kakanat/SyncViolinist.", "published": "2024-12-11 12:33:01", "link": "http://arxiv.org/abs/2412.08343v1", "categories": ["cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Zero-Shot Mono-to-Binaural Speech Synthesis", "abstract": "We present ZeroBAS, a neural method to synthesize binaural audio from\nmonaural audio recordings and positional information without training on any\nbinaural data. To our knowledge, this is the first published zero-shot neural\napproach to mono-to-binaural audio synthesis. Specifically, we show that a\nparameter-free geometric time warping and amplitude scaling based on source\nlocation suffices to get an initial binaural synthesis that can be refined by\niteratively applying a pretrained denoising vocoder. Furthermore, we find this\nleads to generalization across room conditions, which we measure by introducing\na new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art\nmonaural-to-binaural synthesis methods on unseen conditions. Our zero-shot\nmethod is perceptually on-par with the performance of supervised methods on the\nstandard mono-to-binaural dataset, and even surpasses them on our\nout-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the\npotential of pretrained generative audio models and zero-shot learning to\nunlock robust binaural audio synthesis.", "published": "2024-12-11 13:00:49", "link": "http://arxiv.org/abs/2412.08356v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Watermarking Training Data of Music Generation Models", "abstract": "Generative Artificial Intelligence (Gen-AI) models are increasingly used to\nproduce content across domains, including text, images, and audio. While these\nmodels represent a major technical breakthrough, they gain their generative\ncapabilities from being trained on enormous amounts of human-generated content,\nwhich often includes copyrighted material. In this work, we investigate whether\naudio watermarking techniques can be used to detect an unauthorized usage of\ncontent to train a music generation model. We compare outputs generated by a\nmodel trained on watermarked data to a model trained on non-watermarked data.\nWe study factors that impact the model's generation behaviour: the watermarking\ntechnique, the proportion of watermarked samples in the training set, and the\nrobustness of the watermarking technique against the model's tokenizer. Our\nresults show that audio watermarking techniques, including some that are\nimperceptible to humans, can lead to noticeable shifts in the model's outputs.\nWe also study the robustness of a state-of-the-art watermarking technique to\nremoval techniques.", "published": "2024-12-11 17:10:44", "link": "http://arxiv.org/abs/2412.08549v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Mel-Refine: A Plug-and-Play Approach to Refine Mel-Spectrogram in Audio\n  Generation", "abstract": "Text-to-audio (TTA) model is capable of generating diverse audio from textual\nprompts. However, most mainstream TTA models, which predominantly rely on\nMel-spectrograms, still face challenges in producing audio with rich content.\nThe intricate details and texture required in Mel-spectrograms for such audio\noften surpass the models' capacity, leading to outputs that are blurred or lack\ncoherence. In this paper, we begin by investigating the critical role of U-Net\nin Mel-spectrogram generation. Our analysis shows that in U-Net structure,\nhigh-frequency components in skip-connections and the backbone influence\ntexture and detail, while low-frequency components in the backbone are critical\nfor the diffusion denoising process. We further propose ``Mel-Refine'', a\nplug-and-play approach that enhances Mel-spectrogram texture and detail by\nadjusting different component weights during inference. Our method requires no\nadditional training or fine-tuning and is fully compatible with any\ndiffusion-based TTA architecture. Experimental results show that our approach\nboosts performance metrics of the latest TTA model Tango2 by 25\\%,\ndemonstrating its effectiveness.", "published": "2024-12-11 17:51:44", "link": "http://arxiv.org/abs/2412.08577v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large\n  Audio-Language Models", "abstract": "Recent advancements in large audio-language models (LALMs) have enabled\nspeech-based user interactions, significantly enhancing user experience and\naccelerating the deployment of LALMs in real-world applications. However,\nensuring the safety of LALMs is crucial to prevent risky outputs that may raise\nsocietal concerns or violate AI regulations. Despite the importance of this\nissue, research on jailbreaking LALMs remains limited due to their recent\nemergence and the additional technical challenges they present compared to\nattacks on DNN-based audio models. Specifically, the audio encoders in LALMs,\nwhich involve discretization operations, often lead to gradient shattering,\nhindering the effectiveness of attacks relying on gradient-based optimizations.\nThe behavioral variability of LALMs further complicates the identification of\neffective (adversarial) optimization targets. Moreover, enforcing stealthiness\nconstraints on adversarial audio waveforms introduces a reduced, non-convex\nfeasible solution space, further intensifying the challenges of the\noptimization process. To overcome these challenges, we develop AdvWave, the\nfirst jailbreak framework against LALMs. We propose a dual-phase optimization\nmethod that addresses gradient shattering, enabling effective end-to-end\ngradient-based optimization. Additionally, we develop an adaptive adversarial\ntarget search algorithm that dynamically adjusts the adversarial optimization\ntarget based on the response patterns of LALMs for specific queries. To ensure\nthat adversarial audio remains perceptually natural to human listeners, we\ndesign a classifier-guided optimization approach that generates adversarial\nnoise resembling common urban sounds. Extensive evaluations on multiple\nadvanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving\na 40% higher average jailbreak attack success rate.", "published": "2024-12-11 18:30:57", "link": "http://arxiv.org/abs/2412.08608v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotional Vietnamese Speech-Based Depression Diagnosis Using Dynamic\n  Attention Mechanism", "abstract": "Major depressive disorder is a prevalent and serious mental health condition\nthat negatively impacts your emotions, thoughts, actions, and overall\nperception of the world. It is complicated to determine whether a person is\ndepressed due to the symptoms of depression not apparent. However, their voice\ncan be one of the factor from which we can acknowledge signs of depression.\nPeople who are depressed express discomfort, sadness and they may speak slowly,\ntrembly, and lose emotion in their voices. In this study, we proposed the\nDynamic Convolutional Block Attention Module (Dynamic-CBAM) to utilized with in\nan Attention-GRU Network to classify the emotions by analyzing the audio signal\nof humans. Based on the results, we can diagnose which patients are depressed\nor prone to depression then so that treatment and prevention can be started as\nsoon as possible. The research delves into the intricate computational steps\ninvolved in implementing a Attention-GRU deep learning architecture. Through\nexperimentation, the model has achieved an impressive recognition with\nUnweighted Accuracy (UA) rate of 0.87 and 0.86 Weighted Accuracy (WA) rate and\nF1 rate of 0.87 in the VNEMOS dataset. Training code is released in\nhttps://github.com/fiyud/Emotional-Vietnamese-Speech-Based-Depression-Diagnosis-Using-Dynamic-Attention-Mechanism", "published": "2024-12-11 18:52:39", "link": "http://arxiv.org/abs/2412.08683v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Collaborative Hybrid Propagator for Temporal Misalignment in\n  Audio-Visual Segmentation", "abstract": "Audio-visual video segmentation (AVVS) aims to generate pixel-level maps of\nsound-producing objects that accurately align with the corresponding audio.\nHowever, existing methods often face temporal misalignment, where audio cues\nand segmentation results are not temporally coordinated. Audio provides two\ncritical pieces of information: i) target object-level details and ii) the\ntiming of when objects start and stop producing sounds. Current methods focus\nmore on object-level information but neglect the boundaries of audio semantic\nchanges, leading to temporal misalignment. To address this issue, we propose a\nCollaborative Hybrid Propagator Framework~(Co-Prop). This framework includes\ntwo main steps: Preliminary Audio Boundary Anchoring and Frame-by-Frame\nAudio-Insert Propagation. To Anchor the audio boundary, we employ\nretrieval-assist prompts with Qwen large language models to identify control\npoints of audio semantic changes. These control points split the audio into\nsemantically consistent audio portions. After obtaining the control point\nlists, we propose the Audio Insertion Propagator to process each audio portion\nusing a frame-by-frame audio insertion propagation and matching approach. We\ncurated a compact dataset comprising diverse source conversion cases and\ndevised a metric to assess alignment rates. Compared to traditional\nsimultaneous processing methods, our approach reduces memory requirements and\nfacilitates frame alignment. Experimental results demonstrate the effectiveness\nof our approach across three datasets and two backbones. Furthermore, our\nmethod can be integrated with existing AVVS approaches, offering plug-and-play\nfunctionality to enhance their performance.", "published": "2024-12-11 07:33:18", "link": "http://arxiv.org/abs/2412.08161v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based\n  Talking Head Synthesis", "abstract": "Talking head synthesis with arbitrary speech audio is a crucial challenge in\nthe field of digital humans. Recently, methods based on radiance fields have\nreceived increasing attention due to their ability to synthesize high-fidelity\nand identity-consistent talking heads from just a few minutes of training\nvideo. However, due to the limited scale of the training data, these methods\noften exhibit poor performance in audio-lip synchronization and visual quality.\nIn this paper, we propose a novel 3D Gaussian-based method called PointTalk,\nwhich constructs a static 3D Gaussian field of the head and deforms it in sync\nwith the audio. It also incorporates an audio-driven dynamic lip point cloud as\na critical component of the conditional information, thereby facilitating the\neffective synthesis of talking heads. Specifically, the initial step involves\ngenerating the corresponding lip point cloud from the audio signal and\ncapturing its topological structure. The design of the dynamic difference\nencoder aims to capture the subtle nuances inherent in dynamic lip movements\nmore effectively. Furthermore, we integrate the audio-point enhancement module,\nwhich not only ensures the synchronization of the audio signal with the\ncorresponding lip point cloud within the feature space, but also facilitates a\ndeeper understanding of the interrelations among cross-modal conditional\nfeatures. Extensive experiments demonstrate that our method achieves superior\nhigh-fidelity and audio-lip synchronization in talking head synthesis compared\nto previous methods.", "published": "2024-12-11 16:15:14", "link": "http://arxiv.org/abs/2412.08504v1", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
