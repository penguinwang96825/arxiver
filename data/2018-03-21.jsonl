{"title": "InfyNLP at SMM4H Task 2: Stacked Ensemble of Shallow Convolutional\n  Neural Networks for Identifying Personal Medication Intake from Twitter", "abstract": "This paper describes Infosys's participation in the \"2nd Social Media Mining\nfor Health Applications Shared Task at AMIA, 2017, Task 2\". Mining social media\nmessages for health and drug related information has received significant\ninterest in pharmacovigilance research. This task targets at developing\nautomated classification models for identifying tweets containing descriptions\nof personal intake of medicines. Towards this objective we train a stacked\nensemble of shallow convolutional neural network (CNN) models on an annotated\ndataset provided by the organizers. We use random search for tuning the\nhyper-parameters of the CNN and submit an ensemble of best models for the\nprediction task. Our system secured first place among 9 teams, with a\nmicro-averaged F-score of 0.693.", "published": "2018-03-21 02:14:16", "link": "http://arxiv.org/abs/1803.07718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\u03c1$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "abstract": "Sentiment analysis is a key component in various text mining applications.\nNumerous sentiment classification techniques, including conventional and deep\nlearning-based methods, have been proposed in the literature. In most existing\nmethods, a high-quality training set is assumed to be given. Nevertheless,\nconstructing a high-quality training set that consists of highly accurate\nlabels is challenging in real applications. This difficulty stems from the fact\nthat text samples usually contain complex sentiment representations, and their\nannotation is subjective. We address this challenge in this study by leveraging\na new labeling strategy and utilizing a two-level long short-term memory\nnetwork to construct a sentiment classifier. Lexical cues are useful for\nsentiment analysis, and they have been utilized in conventional studies. For\nexample, polar and privative words play important roles in sentiment analysis.\nA new encoding strategy, that is, $\\rho$-hot encoding, is proposed to alleviate\nthe drawbacks of one-hot encoding and thus effectively incorporate useful\nlexical cues. We compile three Chinese data sets on the basis of our label\nstrategy and proposed methodology. Experiments on the three data sets\ndemonstrate that the proposed method outperforms state-of-the-art algorithms.", "published": "2018-03-21 07:13:16", "link": "http://arxiv.org/abs/1803.07771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting\n  Noun Compounds using Paraphrases in a Neural Model", "abstract": "Automatic interpretation of the relation between the constituents of a noun\ncompound, e.g. olive oil (source) and baby oil (purpose) is an important task\nfor many NLP applications. Recent approaches are typically based on either\nnoun-compound representations or paraphrases. While the former has initially\nshown promising results, recent work suggests that the success stems from\nmemorizing single prototypical words for each relation. We explore a neural\nparaphrasing approach that demonstrates superior performance when such\nmemorization is not possible.", "published": "2018-03-21 18:16:23", "link": "http://arxiv.org/abs/1803.08073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expeditious Generation of Knowledge Graph Embeddings", "abstract": "Knowledge Graph Embedding methods aim at representing entities and relations\nin a knowledge base as points or vectors in a continuous vector space. Several\napproaches using embeddings have shown promising results on tasks such as link\nprediction, entity recommendation, question answering, and triplet\nclassification. However, only a few methods can compute low-dimensional\nembeddings of very large knowledge bases without needing state-of-the-art\ncomputational resources. In this paper, we propose KG2Vec, a simple and fast\napproach to Knowledge Graph Embedding based on the skip-gram model. Instead of\nusing a predefined scoring function, we learn it relying on Long Short-Term\nMemories. We show that our embeddings achieve results comparable with the most\nscalable approaches on knowledge graph completion as well as on a new metric.\nYet, KG2Vec can embed large graphs in lesser time by processing more than 250\nmillion triples in less than 7 hours on common hardware.", "published": "2018-03-21 10:06:28", "link": "http://arxiv.org/abs/1803.07828v2", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.6"], "primary_category": "cs.CL"}
{"title": "Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs", "abstract": "We consider the problem of zero-shot recognition: learning a visual\nclassifier for a category with zero training examples, just using the word\nembedding of the category and its relationship to other categories, which\nvisual data are provided. The key to dealing with the unfamiliar or novel\ncategory is to transfer knowledge obtained from familiar classes to describe\nthe unfamiliar class. In this paper, we build upon the recently introduced\nGraph Convolutional Network (GCN) and propose an approach that uses both\nsemantic embeddings and the categorical relationships to predict the\nclassifiers. Given a learned knowledge graph (KG), our approach takes as input\nsemantic embeddings for each node (representing visual category). After a\nseries of graph convolutions, we predict the visual classifier for each\ncategory. During training, the visual classifiers for a few categories are\ngiven to learn the GCN parameters. At test time, these filters are used to\npredict the visual classifiers of unseen categories. We show that our approach\nis robust to noise in the KG. More importantly, our approach provides\nsignificant improvement in performance compared to the current state-of-the-art\nresults (from 2 ~ 3% on some metrics to whopping 20% on a few).", "published": "2018-03-21 17:52:42", "link": "http://arxiv.org/abs/1803.08035v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Attention on Attention: Architectures for Visual Question Answering\n  (VQA)", "abstract": "Visual Question Answering (VQA) is an increasingly popular topic in deep\nlearning research, requiring coordination of natural language processing and\ncomputer vision modules into a single architecture. We build upon the model\nwhich placed first in the VQA Challenge by developing thirteen new attention\nmechanisms and introducing a simplified classifier. We performed 300 GPU hours\nof extensive hyperparameter and architecture searches and were able to achieve\nan evaluation score of 64.78%, outperforming the existing state-of-the-art\nsingle model's validation score of 63.15%.", "published": "2018-03-21 03:05:58", "link": "http://arxiv.org/abs/1803.07724v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "68Txx"], "primary_category": "cs.CL"}
{"title": "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement\n  Learning for Planned-Ahead Vision-and-Language Navigation", "abstract": "Existing research studies on vision and language grounding for robot\nnavigation focus on improving model-free deep reinforcement learning (DRL)\nmodels in synthetic environments. However, model-free DRL models do not\nconsider the dynamics in the real-world environments, and they often fail to\ngeneralize to new scenes. In this paper, we take a radical approach to bridge\nthe gap between synthetic studies and real-world practices---We propose a\nnovel, planned-ahead hybrid reinforcement learning model that combines\nmodel-free and model-based reinforcement learning to solve a real-world\nvision-language navigation task. Our look-ahead module tightly integrates a\nlook-ahead policy model with an environment model that predicts the next state\nand the reward. Experimental results suggest that our proposed method\nsignificantly outperforms the baselines and achieves the best on the real-world\nRoom-to-Room dataset. Moreover, our scalable method is more generalizable when\ntransferring to unseen environments.", "published": "2018-03-21 03:21:38", "link": "http://arxiv.org/abs/1803.07729v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Speech Emotion Recognition Considering Local Dynamic Features", "abstract": "Recently, increasing attention has been directed to the study of the speech\nemotion recognition, in which global acoustic features of an utterance are\nmostly used to eliminate the content differences. However, the expression of\nspeech emotion is a dynamic process, which is reflected through dynamic\ndurations, energies, and some other prosodic information when one speaks. In\nthis paper, a novel local dynamic pitch probability distribution feature, which\nis obtained by drawing the histogram, is proposed to improve the accuracy of\nspeech emotion recognition. Compared with most of the previous works using\nglobal features, the proposed method takes advantage of the local dynamic\ninformation conveyed by the emotional speech. Several experiments on Berlin\nDatabase of Emotional Speech are conducted to verify the effectiveness of the\nproposed method. The experimental results demonstrate that the local dynamic\ninformation obtained with the proposed method is more effective for speech\nemotion recognition than the traditional global features.", "published": "2018-03-21 03:52:26", "link": "http://arxiv.org/abs/1803.07738v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Exploring the Naturalness of Buggy Code with Recurrent Neural Networks", "abstract": "Statistical language models are powerful tools which have been used for many\ntasks within natural language processing. Recently, they have been used for\nother sequential data such as source code.(Ray et al., 2015) showed that it is\npossible train an n-gram source code language mode, and use it to predict buggy\nlines in code by determining \"unnatural\" lines via entropy with respect to the\nlanguage model. In this work, we propose using a more advanced language\nmodeling technique, Long Short-term Memory recurrent neural networks, to model\nsource code and classify buggy lines based on entropy. We show that our method\nslightly outperforms an n-gram model in the buggy line classification task\nusing AUC.", "published": "2018-03-21 16:14:22", "link": "http://arxiv.org/abs/1803.08793v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
