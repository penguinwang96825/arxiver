{"title": "How Far are We from Robust Long Abstractive Summarization?", "abstract": "Abstractive summarization has made tremendous progress in recent years. In\nthis work, we perform fine-grained human annotations to evaluate long document\nabstractive summarization systems (i.e., models and metrics) with the aim of\nimplementing them to generate reliable summaries. For long document abstractive\nmodels, we show that the constant strive for state-of-the-art ROUGE results can\nlead us to generate more relevant summaries but not factual ones. For long\ndocument evaluation metrics, human evaluation results show that ROUGE remains\nthe best at evaluating the relevancy of a summary. It also reveals important\nlimitations of factuality metrics in detecting different types of factual\nerrors and the reasons behind the effectiveness of BARTScore. We then suggest\npromising directions in the endeavor of developing factual consistency metrics.\nFinally, we release our annotated long document dataset with the hope that it\ncan contribute to the development of metrics across a broader range of\nsummarization settings.", "published": "2022-10-30 03:19:50", "link": "http://arxiv.org/abs/2210.16732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate, Discriminate and Contrast: A Semi-Supervised Sentence\n  Representation Learning Framework", "abstract": "Most sentence embedding techniques heavily rely on expensive human-annotated\nsentence pairs as the supervised signals. Despite the use of large-scale\nunlabeled data, the performance of unsupervised methods typically lags far\nbehind that of the supervised counterparts in most downstream tasks. In this\nwork, we propose a semi-supervised sentence embedding framework, GenSE, that\neffectively leverages large-scale unlabeled data. Our method include three\nparts: 1) Generate: A generator/discriminator model is jointly trained to\nsynthesize sentence pairs from open-domain unlabeled corpus; 2) Discriminate:\nNoisy sentence pairs are filtered out by the discriminator to acquire\nhigh-quality positive and negative sentence pairs; 3) Contrast: A prompt-based\ncontrastive approach is presented for sentence representation learning with\nboth annotated and synthesized data. Comprehensive experiments show that GenSE\nachieves an average correlation score of 85.19 on the STS datasets and\nconsistent performance improvement on four domain adaptation tasks,\nsignificantly surpassing the state-of-the-art methods and convincingly\ncorroborating its effectiveness and generalization ability.Code, Synthetic data\nand Models available at https://github.com/MatthewCYM/GenSE.", "published": "2022-10-30 10:15:21", "link": "http://arxiv.org/abs/2210.16798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Actionable Phrase Detection using NLP", "abstract": "Actionable sentences are terms that, in the most basic sense, imply the\nnecessity of taking a specific action. In Linguistic terms, they are steps to\nachieve an operation, often through the usage of action verbs. For example, the\nsentence, `Get your homework finished by tomorrow` qualifies as actionable\nsince it demands a specific action (In this case, finishing homework) to be\ntaken. In contrast, a simple sentence such as, `I like to play the guitar` does\nnot qualify as an actionable phrase since it simply states a personal choice of\nthe person instead of demanding a task to be finished.\n  In this paper, the aim is to explore if Actionables can be extracted from raw\ntext using Linguistic filters designed from scratch. These filters are\nspecially catered to identifying actionable text using Transfer Learning as the\nlead role. Actionable Detection can be used in detecting emergency tasks during\na crisis, Instruction accuracy for First aid and can also be used to make\nproductivity tools like automatic ToDo list generators from conferences. To\naccomplish this, we use the Enron Email Dataset and apply our Linguistic\nfilters on the cleaned textual data. We then use Transfer Learning with the\nUniversal Sentence Encoder to train a model to classify whether a given string\nof raw text is actionable or not.", "published": "2022-10-30 13:37:49", "link": "http://arxiv.org/abs/2210.16841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Decompose: Hypothetical Question Decomposition Based on\n  Comparable Texts", "abstract": "Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.", "published": "2022-10-30 15:38:03", "link": "http://arxiv.org/abs/2210.16865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and\n  Reasoning", "abstract": "Recent research shows synthetic data as a source of supervision helps\npretrained language models (PLM) transfer learning to new target tasks/domains.\nHowever, this idea is less explored for spatial language. We provide two new\ndata resources on multiple spatial language processing tasks. The first dataset\nis synthesized for transfer learning on spatial question answering (SQA) and\nspatial role labeling (SpRL). Compared to previous SQA datasets, we include a\nlarger variety of spatial relation types and spatial expressions. Our data\ngeneration process is easily extendable with new spatial expression lexicons.\nThe second one is a real-world SQA dataset with human-generated questions built\non an existing corpus with SPRL annotations. This dataset can be used to\nevaluate spatial language processing models in realistic situations. We show\npretraining with automatically generated data significantly improves the SOTA\nresults on several SQA and SPRL benchmarks, particularly when the training data\nin the target domain is small.", "published": "2022-10-30 21:23:34", "link": "http://arxiv.org/abs/2210.16952v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Multimodality: A Taxonomical Survey of Datasets,\n  Techniques, Challenges and Opportunities", "abstract": "Contextualizing language technologies beyond a single language kindled\nembracing multiple modalities and languages. Individually, each of these\ndirections undoubtedly proliferated into several NLP tasks. Despite this\nmomentum, most of the multimodal research is primarily centered around English\nand multilingual research is primarily centered around contexts from text\nmodality. Challenging this conventional setup, researchers studied the\nunification of multilingual and multimodal (MultiX) streams. The main goal of\nthis work is to catalogue and characterize these works by charting out the\ncategories of tasks, datasets and methods to address MultiX scenarios. To this\nend, we review the languages studied, gold or silver data with parallel\nannotations, and understand how these modalities and languages interact in\nmodeling. We present an account of the modeling approaches along with their\nstrengths and weaknesses to better understand what scenarios they can be used\nreliably. Following this, we present the high-level trends in the overall\nparadigm of the field. Finally, we conclude by presenting a road map of\nchallenges and promising research directions.", "published": "2022-10-30 21:46:01", "link": "http://arxiv.org/abs/2210.16960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XMD: An End-to-End Framework for Interactive Explanation-Based Debugging\n  of NLP Models", "abstract": "NLP models are susceptible to learning spurious biases (i.e., bugs) that work\non some datasets but do not properly reflect the underlying task.\nExplanation-based model debugging aims to resolve spurious biases by showing\nhuman users explanations of model behavior, asking users to give feedback on\nthe behavior, then using the feedback to update the model. While existing model\ndebugging methods have shown promise, their prototype-level implementations\nprovide limited practical utility. Thus, we propose XMD: the first open-source,\nend-to-end framework for explanation-based model debugging. Given task- or\ninstance-level explanations, users can flexibly provide various forms of\nfeedback via an intuitive, web-based UI. After receiving user feedback, XMD\nautomatically updates the model in real time, by regularizing the model so that\nits explanations align with the user feedback. The new model can then be easily\ndeployed into real-world applications via Hugging Face. Using XMD, we can\nimprove the model's OOD performance on text classification tasks by up to 18%.", "published": "2022-10-30 23:09:09", "link": "http://arxiv.org/abs/2210.16978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Validity Assessment of Legal Will Statements as Natural Language\n  Inference", "abstract": "This work introduces a natural language inference (NLI) dataset that focuses\non the validity of statements in legal wills. This dataset is unique because:\n(a) each entailment decision requires three inputs: the statement from the\nwill, the law, and the conditions that hold at the time of the testator's\ndeath; and (b) the included texts are longer than the ones in current NLI\ndatasets. We trained eight neural NLI models in this dataset. All the models\nachieve more than 80% macro F1 and accuracy, which indicates that neural\napproaches can handle this task reasonably well. However, group accuracy, a\nstricter evaluation measure that is calculated with a group of positive and\nnegative examples generated from the same statement as a unit, is in mid 80s at\nbest, which suggests that the models' understanding of the task remains\nsuperficial. Further ablative analyses and explanation experiments indicate\nthat all three text segments are used for prediction, but some decisions rely\non semantically irrelevant tokens. This indicates that overfitting on these\nlonger texts likely happens, and that additional research is required for this\ntask to be solved.", "published": "2022-10-30 23:53:13", "link": "http://arxiv.org/abs/2210.16989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning Makes a Good Classification Head", "abstract": "In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.", "published": "2022-10-30 08:29:20", "link": "http://arxiv.org/abs/2210.16771v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Counterfactual Data Augmentation via Perspective Transition for\n  Open-Domain Dialogues", "abstract": "The construction of open-domain dialogue systems requires high-quality\ndialogue datasets. The dialogue data admits a wide variety of responses for a\ngiven dialogue history, especially responses with different semantics. However,\ncollecting high-quality such a dataset in most scenarios is labor-intensive and\ntime-consuming. In this paper, we propose a data augmentation method to\nautomatically augment high-quality responses with different semantics by\ncounterfactual inference. Specifically, given an observed dialogue, our\ncounterfactual generation model first infers semantically different responses\nby replacing the observed reply perspective with substituted ones. Furthermore,\nour data selection method filters out detrimental augmented responses.\nExperimental results show that our data augmentation method can augment\nhigh-quality responses with different semantics for a given dialogue history,\nand can outperform competitive baselines on multiple downstream tasks.", "published": "2022-10-30 13:26:49", "link": "http://arxiv.org/abs/2210.16838v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Context-to-Vector with Graph Retrofitting to Improve Word\n  Embeddings", "abstract": "Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.", "published": "2022-10-30 14:15:43", "link": "http://arxiv.org/abs/2210.16848v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Medical Codes Prediction from Clinical Notes: From Human Coders to\n  Machines", "abstract": "Prediction of medical codes from clinical notes is a practical and essential\nneed for every healthcare delivery organization within current medical systems.\nAutomating annotation will save significant time and excessive effort that\nhuman coders spend today. However, the biggest challenge is directly\nidentifying appropriate medical codes from several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. This complex\nmedical codes prediction problem from clinical notes has received substantial\ninterest in the NLP community, and several recent studies have shown the\nstate-of-the-art code prediction results of full-fledged deep learning-based\nmethods. This progress raises the fundamental question of how far automated\nmachine learning systems are from human coders' working performance, as well as\nthe important question of how well current explainability methods apply to\nadvanced neural network models such as transformers. This is to predict correct\ncodes and present references in clinical notes that support code prediction, as\nthis level of explainability and accuracy of the prediction outcomes is\ncritical to gaining trust from professional medical coders.", "published": "2022-10-30 14:24:13", "link": "http://arxiv.org/abs/2210.16850v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction", "abstract": "In text generation, models that generate text from scratch one token at a\ntime are currently the dominant paradigm. Despite being performant, these\nmodels lack the ability to revise existing text, which limits their usability\nin many practical scenarios. We look to address this, with DiffusER (Diffusion\nvia Edit-based Reconstruction), a new edit-based generative model for text\nbased on denoising diffusion models -- a class of models that use a Markov\nchain of denoising steps to incrementally generate data. DiffusER is not only a\nstrong generative model in general, rivalling autoregressive models on several\ntasks spanning machine translation, summarization, and style transfer; it can\nalso perform other varieties of generation that standard autoregressive models\nare not well-suited for. For instance, we demonstrate that DiffusER makes it\npossible for a user to condition generation on a prototype, or an incomplete\nsequence, and continue revising based on previous edit steps.", "published": "2022-10-30 16:55:23", "link": "http://arxiv.org/abs/2210.16886v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Emotional Paraphrasing along Emotion Gradients", "abstract": "Paraphrase generation, a.k.a. paraphrasing, is a common and important task in\nnatural language processing. Emotional paraphrasing, which changes the emotion\nembodied in a piece of text while preserving its meaning, has many potential\napplications, e.g., moderating online dialogues and preventing cyberbullying.\nWe introduce a new task of fine-grained emotional paraphrasing along emotion\ngradients, that is, altering the emotional intensities of the paraphrases in\nfine grain following smooth variations in affective dimensions while preserving\nthe meanings of the originals. We propose a framework for addressing this task\nby fine-tuning text-to-text Transformers through multi-task training. We\nenhance several widely used paraphrasing corpus by annotating the input and\ntarget texts with their fine-grained emotion labels. With these labels,\nfine-tuning text-to-text Transformers on these corpus entails multi-task\ntraining. Evaluations of the fine-tuned Transformers on separate test sets show\nthat including fine-grained emotion labels in the paraphrase task significantly\nimprove the chance of obtaining high-quality paraphrases of the desired\nemotions, i.e., more than doubling the number of exact matches of desired\nemotions while achieving consistently better scores in paraphrase metrics such\nas BLEU, ROGUE, and METEOR.", "published": "2022-10-30 05:38:22", "link": "http://arxiv.org/abs/2212.03297v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common\n  Label Set", "abstract": "In a multilingual country like India, multilingual Automatic Speech\nRecognition (ASR) systems have much scope. Multilingual ASR systems exhibit\nmany advantages like scalability, maintainability, and improved performance\nover the monolingual ASR systems. However, building multilingual systems for\nIndian languages is challenging since different languages use different scripts\nfor writing. On the other hand, Indian languages share a lot of common sounds.\nCommon Label Set (CLS) exploits this idea and maps graphemes of various\nlanguages with similar sounds to common labels. Since Indian languages are\nmostly phonetic, building a parser to convert from native script to CLS is\neasy. In this paper, we explore various approaches to build multilingual ASR\nmodels. We also propose a novel architecture called Encoder-Decoder-Decoder for\nbuilding multilingual systems that use both CLS and native script labels. We\nalso analyzed the effectiveness of CLS-based multilingual systems combined with\nmachine transliteration.", "published": "2022-10-30 04:01:26", "link": "http://arxiv.org/abs/2210.16739v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired\n  Speech and Text", "abstract": "Self-supervised pre-training has been successful in both text and speech\nprocessing. Speech and text offer different but complementary information. The\nquestion is whether we are able to perform a speech-text joint pre-training on\nunpaired speech and text. In this paper, we take the idea of self-supervised\npre-training one step further and propose token2vec, a novel joint pre-training\nframework for unpaired speech and text based on discrete representations of\nspeech. Firstly, due to the distinct characteristics between speech and text\nmodalities, where speech is continuous while text is discrete, we first\ndiscretize speech into a sequence of discrete speech tokens to solve the\nmodality mismatch problem. Secondly, to solve the length mismatch problem,\nwhere the speech sequence is usually much longer than text sequence, we convert\nthe words of text into phoneme sequences and randomly repeat each phoneme in\nthe sequences. Finally, we feed the discrete speech and text tokens into a\nmodality-agnostic Transformer encoder and pre-train with token-level masking\nlanguage modeling (tMLM). Experiments show that token2vec is significantly\nsuperior to various speech-only pre-training baselines, with up to 17.7%\nrelative WER reduction. Token2vec model is also validated on a non-ASR task,\ni.e., spoken intent classification, and shows good transferability.", "published": "2022-10-30 06:38:19", "link": "http://arxiv.org/abs/2210.16755v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP\n  Tasks", "abstract": "Access to external knowledge is essential for many natural language\nprocessing tasks, such as question answering and dialogue. Existing methods\noften rely on a parametric model that stores knowledge in its parameters, or\nuse a retrieval-augmented model that has access to an external knowledge\nsource. Parametric and retrieval-augmented models have complementary strengths\nin terms of computational efficiency and predictive accuracy. To combine the\nstrength of both approaches, we propose the Efficient Memory-Augmented\nTransformer (EMAT) -- it encodes external knowledge into a key-value memory and\nexploits the fast maximum inner product search for memory querying. We also\nintroduce pre-training tasks that allow EMAT to encode informative key-value\nrepresentations, and to learn an implicit strategy to integrate multiple memory\nslots into the transformer. Experiments on various knowledge-intensive tasks\nsuch as question answering and dialogue datasets show that, simply augmenting\nparametric models (T5-base) using our method produces more accurate results\n(e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000\nqueries/s on NQ). Compared to retrieval-augmented models, EMAT runs\nsubstantially faster across the board and produces more accurate results on WoW\nand ELI5. Our code and datasets are available at https://github.\ncom/uclnlp/EMAT.", "published": "2022-10-30 08:34:49", "link": "http://arxiv.org/abs/2210.16773v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Bilingual Lexicon Induction with Cross-Encoder Reranking", "abstract": "Bilingual lexicon induction (BLI) with limited bilingual supervision is a\ncrucial yet challenging task in multilingual NLP. Current state-of-the-art BLI\nmethods rely on the induction of cross-lingual word embeddings (CLWEs) to\ncapture cross-lingual word similarities; such CLWEs are obtained 1) via\ntraditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs\nfrom multilingual pretrained language models (mPLMs), or 3) through combining\nthe former two options. In this work, we propose a novel semi-supervised\npost-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking),\napplicable to any precalculated CLWE space, which improves their BLI\ncapability. The key idea is to 'extract' cross-lingual lexical knowledge from\nmPLMs, and then combine it with the original CLWEs. This crucial step is done\nvia 1) creating a word similarity dataset, comprising positive word pairs\n(i.e., true translations) and hard negative pairs induced from the original\nCLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a\ncross-encoder manner to predict the similarity scores. At inference, we 3)\ncombine the similarity score from the original CLWE space with the score from\nthe BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on\ntwo standard BLI benchmarks spanning a wide spectrum of diverse languages: it\nsubstantially outperforms a series of strong baselines across the board. We\nalso validate the robustness of BLICEr with different CLWEs.", "published": "2022-10-30 21:26:07", "link": "http://arxiv.org/abs/2210.16953v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improvements to Embedding-Matching Acoustic-to-Word ASR Using\n  Multiple-Hypothesis Pronunciation-Based Embeddings", "abstract": "In embedding-matching acoustic-to-word (A2W) ASR, every word in the\nvocabulary is represented by a fixed-dimension embedding vector that can be\nadded or removed independently of the rest of the system. The approach is\npotentially an elegant solution for the dynamic out-of-vocabulary (OOV) words\nproblem, where speaker- and context-dependent named entities like contact names\nmust be incorporated into the ASR on-the-fly for every speech utterance at\ntesting time. Challenges still remain, however, in improving the overall\naccuracy of embedding-matching A2W. In this paper, we contribute two methods\nthat improve the accuracy of embedding-matching A2W. First, we propose\ninternally producing multiple embeddings, instead of a single embedding, at\neach instance in time, which allows the A2W model to propose a richer set of\nhypotheses over multiple time segments in the audio. Second, we propose using\nword pronunciation embeddings rather than word orthography embeddings to reduce\nambiguities introduced by words that have more than one sound. We show that the\nabove ideas give significant accuracy improvement, with the same training data\nand nearly identical model size, in scenarios where dynamic OOV words play a\ncrucial role. On a dataset of queries to a speech-based digital assistant that\ninclude many user-dependent contact names, we observe up to 18% decrease in\nword error rate using the proposed improvements.", "published": "2022-10-30 02:37:55", "link": "http://arxiv.org/abs/2210.16726v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WeKws: A production first small-footprint end-to-end Keyword Spotting\n  Toolkit", "abstract": "Keyword spotting (KWS) enables speech-based user interaction and gradually\nbecomes an indispensable component of smart devices. Recently, end-to-end (E2E)\nmethods have become the most popular approach for on-device KWS tasks. However,\nthere is still a gap between the research and deployment of E2E KWS methods. In\nthis paper, we introduce WeKws, a production-quality, easy-to-build, and\nconvenient-to-be-applied E2E KWS toolkit. WeKws contains the implementations of\nseveral state-of-the-art backbone networks, making it achieve highly\ncompetitive results on three publicly available datasets. To make WeKws a pure\nE2E toolkit, we utilize a refined max-pooling loss to make the model learn the\nending position of the keyword by itself, which significantly simplifies the\ntraining pipeline and makes WeKws very efficient to be applied in real-world\nscenarios. The toolkit is publicly available at\nhttps://github.com/wenet-e2e/wekws.", "published": "2022-10-30 04:45:07", "link": "http://arxiv.org/abs/2210.16743v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SRTNet: Time Domain Speech Enhancement Via Stochastic Refinement", "abstract": "Diffusion model, as a new generative model which is very popular in image\ngeneration and audio synthesis, is rarely used in speech enhancement. In this\npaper, we use the diffusion model as a module for stochastic refinement. We\npropose SRTNet, a novel method for speech enhancement via Stochastic Refinement\nin complete Time domain. Specifically, we design a joint network consisting of\na deterministic module and a stochastic module, which makes up the\n``enhance-and-refine'' paradigm. We theoretically demonstrate the feasibility\nof our method and experimentally prove that our method achieves faster\ntraining, faster sampling and higher quality. Our code and enhanced samples are\navailable at https://github.com/zhibinQiu/SRTNet.git.", "published": "2022-10-30 10:36:11", "link": "http://arxiv.org/abs/2210.16805v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TT-Net: Dual-path transformer based sound field translation in the\n  spherical harmonic domain", "abstract": "In the current method for the sound field translation tasks based on\nspherical harmonic (SH) analysis, the solution based on the additive theorem\nusually faces the problem of singular values caused by large matrix condition\nnumbers. The influence of different distances and frequencies of the spherical\nradial function on the stability of the translation matrix will affect the\naccuracy of the SH coefficients at the selected point. Due to the problems\nmentioned above, we propose a neural network scheme based on the dual-path\ntransformer. More specifically, the dual-path network is constructed by the\nself-attention module along the two dimensions of the frequency and order axes.\nThe transform-average-concatenate layer and upscaling layer are introduced in\nthe network, which provides solutions for multiple sampling points and\nupscaling. Numerical simulation results indicate that both the working\nfrequency range and the distance range of the translation are extended. More\naccurate higher-order SH coefficients are obtained with the proposed dual-path\nnetwork.", "published": "2022-10-30 14:16:48", "link": "http://arxiv.org/abs/2210.16849v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved acoustic-to-articulatory inversion using representations from\n  pretrained self-supervised learning models", "abstract": "In this work, we investigate the effectiveness of pretrained Self-Supervised\nLearning (SSL) features for learning the mapping for acoustic to articulatory\ninversion (AAI). Signal processing-based acoustic features such as MFCCs have\nbeen predominantly used for the AAI task with deep neural networks. With SSL\nfeatures working well for various other speech tasks such as speech\nrecognition, emotion classification, etc., we experiment with its efficacy for\nAAI. We train on SSL features with transformer neural networks-based AAI models\nof 3 different model complexities and compare its performance with MFCCs in\nsubject-specific (SS), pooled and fine-tuned (FT) configurations with data from\n10 subjects, and evaluate with correlation coefficient (CC) score on the unseen\nsentence test set. We find that acoustic feature reconstruction objective-based\nSSL features such as TERA and DeCoAR work well for AAI, with SS CCs of these\nSSL features reaching close to the best FT CCs of MFCC. We also find the\nresults consistent across different model sizes.", "published": "2022-10-30 16:24:02", "link": "http://arxiv.org/abs/2210.16871v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time MRI Video synthesis from time aligned phonemes with\n  sequence-to-sequence networks", "abstract": "Real-Time Magnetic resonance imaging (rtMRI) of the midsagittal plane of the\nmouth is of interest for speech production research. In this work, we focus on\nestimating utterance level rtMRI video from the spoken phoneme sequence. We\nobtain time-aligned phonemes from forced alignment, to obtain frame-level\nphoneme sequences which are aligned with rtMRI frames. We propose a\nsequence-to-sequence learning model with a transformer phoneme encoder and\nconvolutional frame decoder. We then modify the learning by using intermediary\nfeatures obtained from sampling from a pretrained phoneme-conditioned\nvariational autoencoder (CVAE). We train on 8 subjects in a subject-specific\nmanner and demonstrate the performance with a subjective test. We also use an\nauxiliary task of air tissue boundary (ATB) segmentation to obtain the\nobjective scores on the proposed models. We show that the proposed method is\nable to generate realistic rtMRI video for unseen utterances, and adding CVAE\nis beneficial for learning the sequence-to-sequence mapping for subjects where\nthe mapping is hard to learn.", "published": "2022-10-30 16:45:59", "link": "http://arxiv.org/abs/2210.16881v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Symmetric Saliency-based Adversarial Attack To Speaker Identification", "abstract": "Adversarial attack approaches to speaker identification either need high\ncomputational cost or are not very effective, to our knowledge. To address this\nissue, in this paper, we propose a novel generation-network-based approach,\ncalled symmetric saliency-based encoder-decoder (SSED), to generate adversarial\nvoice examples to speaker identification. It contains two novel components.\nFirst, it uses a novel saliency map decoder to learn the importance of speech\nsamples to the decision of a targeted speaker identification system, so as to\nmake the attacker focus on generating artificial noise to the important\nsamples. It also proposes an angular loss function to push the speaker\nembedding far away from the source speaker. Our experimental results\ndemonstrate that the proposed SSED yields the state-of-the-art performance,\ni.e. over 97% targeted attack success rate and a signal-to-noise level of over\n39 dB on both the open-set and close-set speaker identification tasks, with a\nlow computational cost.", "published": "2022-10-30 08:54:02", "link": "http://arxiv.org/abs/2210.16777v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Speech Quality Aware Complex Neural Network for Acoustic Echo\n  Cancellation with Supervised Contrastive Learning", "abstract": "Acoustic echo cancellation (AEC) is designed to remove echoes, reverberation,\nand unwanted added sounds from the microphone signal while maintaining the\nquality of the near-end speaker's speech. This paper proposes adaptive speech\nquality complex neural networks to focus on specific tasks for real-time\nacoustic echo cancellation. In specific, we propose a complex modularize neural\nnetwork with different stages to focus on feature extraction, acoustic\nseparation, and mask optimization receptively. Furthermore, we adopt the\ncontrastive learning framework and novel speech quality aware loss functions to\nfurther improve the performance. The model is trained with 72 hours for\npre-training and then 72 hours for fine-tuning. The proposed model outperforms\nthe state-of-the-art performance.", "published": "2022-10-30 09:42:03", "link": "http://arxiv.org/abs/2210.16791v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
