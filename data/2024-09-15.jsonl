{"title": "Thesis proposal: Are We Losing Textual Diversity to Natural Language\n  Processing?", "abstract": "This thesis argues that the currently widely used Natural Language Processing\nalgorithms possibly have various limitations related to the properties of the\ntexts they handle and produce. With the wide adoption of these tools in rapid\nprogress, we must ask what these limitations are and what are the possible\nimplications of integrating such tools even more deeply into our daily lives.\n  As a testbed, we have chosen the task of Neural Machine Translation (NMT).\nNevertheless, we aim for general insights and outcomes, applicable even to\ncurrent Large Language Models (LLMs). We ask whether the algorithms used in NMT\nhave inherent inductive biases that are beneficial for most types of inputs but\nmight harm the processing of untypical texts. To explore this hypothesis, we\ndefine a set of measures to quantify text diversity based on its statistical\nproperties, like uniformity or rhythmicity of word-level surprisal, on multiple\nscales (sentence, discourse, language). We then conduct a series of experiments\nto investigate whether NMT systems struggle with maintaining the diversity of\nsuch texts, potentially reducing the richness of the language generated by\nthese systems, compared to human translators.\n  We search for potential causes of these limitations rooted in training\nobjectives and decoding algorithms. Our ultimate goal is to develop\nalternatives that do not enforce uniformity in the distribution of statistical\nproperties in the output and that allow for better global planning of the\ntranslation, taking into account the intrinsic ambiguity of the translation\ntask.", "published": "2024-09-15 01:06:07", "link": "http://arxiv.org/abs/2409.09568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple HMM with Self-Supervised Representations for Phone Segmentation", "abstract": "Despite the recent advance in self-supervised representations, unsupervised\nphonetic segmentation remains challenging. Most approaches focus on improving\nphonetic representations with self-supervised learning, with the hope that the\nimprovement can transfer to phonetic segmentation. In this paper, contrary to\nrecent approaches, we show that peak detection on Mel spectrograms is a strong\nbaseline, better than many self-supervised approaches. Based on this finding,\nwe propose a simple hidden Markov model that uses self-supervised\nrepresentations and features at the boundaries for phone segmentation. Our\nresults demonstrate consistent improvements over previous approaches, with a\ngeneralized formulation allowing versatile design adaptations.", "published": "2024-09-15 07:44:23", "link": "http://arxiv.org/abs/2409.09646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Gender Bias in Large Language Models: Using Teacher's\n  Evaluation in Higher Education As an Example", "abstract": "This paper investigates gender bias in Large Language Model (LLM)-generated\nteacher evaluations in higher education setting, focusing on evaluations\nproduced by GPT-4 across six academic subjects. By applying a comprehensive\nanalytical framework that includes Odds Ratio (OR) analysis, Word Embedding\nAssociation Test (WEAT), sentiment analysis, and contextual analysis, this\npaper identified patterns of gender-associated language reflecting societal\nstereotypes. Specifically, words related to approachability and support were\nused more frequently for female instructors, while words related to\nentertainment were predominantly used for male instructors, aligning with the\nconcepts of communal and agentic behaviors. The study also found moderate to\nstrong associations between male salient adjectives and male names, though\ncareer and family words did not distinctly capture gender biases. These\nfindings align with prior research on societal norms and stereotypes,\nreinforcing the notion that LLM-generated text reflects existing biases.", "published": "2024-09-15 07:50:33", "link": "http://arxiv.org/abs/2409.09652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Open-Source Large Language Models for Native Language\n  Identification", "abstract": "Native Language Identification (NLI) - the task of identifying the native\nlanguage (L1) of a person based on their writing in the second language (L2) -\nhas applications in forensics, marketing, and second language acquisition.\nHistorically, conventional machine learning approaches that heavily rely on\nextensive feature engineering have outperformed transformer-based language\nmodels on this task. Recently, closed-source generative large language models\n(LLMs), e.g., GPT-4, have demonstrated remarkable performance on NLI in a\nzero-shot setting, including promising results in open-set classification.\nHowever, closed-source LLMs have many disadvantages, such as high costs and\nundisclosed nature of training data. This study explores the potential of using\nopen-source LLMs for NLI. Our results indicate that open-source LLMs do not\nreach the accuracy levels of closed-source LLMs when used out-of-the-box.\nHowever, when fine-tuned on labeled training data, open-source LLMs can achieve\nperformance comparable to that of commercial LLMs.", "published": "2024-09-15 08:14:18", "link": "http://arxiv.org/abs/2409.09659v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Learn Metadata: Political Stance Detection Case Study", "abstract": "Stance detection is a crucial NLP task with numerous applications in social\nscience, from analyzing online discussions to assessing political campaigns.\nThis paper investigates the optimal way to incorporate metadata into a\npolitical stance detection task. We demonstrate that previous methods combining\nmetadata with language-based data for political stance detection have not fully\nutilized the metadata information; our simple baseline, using only party\nmembership information, surpasses the current state-of-the-art. We then show\nthat prepending metadata (e.g., party and policy) to political speeches\nperforms best, outperforming all baselines, indicating that complex metadata\ninclusion systems may not learn the task optimally.", "published": "2024-09-15 14:57:41", "link": "http://arxiv.org/abs/2409.13756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with\n  Selective Cloud Assistance", "abstract": "Large language models (LLMs) are known for their exceptional performance\nacross a range of natural language processing tasks, but their deployment comes\nat a high computational and financial cost. On the other hand, smaller language\nmodels (SLMs), which can be deployed on lower-cost edge devices, struggle to\nmatch the performance of their larger counterparts. This paper presents a novel\nhybrid inference approach that leverages the strengths of both model types\nwhile minimizing reliance on costly cloud-based LLMs. Unlike existing methods\nthat route entire queries to either an SLM or a cloud LLM, our approach\nintroduces a reward-based mechanism to dynamically determine the involvement of\nthe cloud LLM during token generation. Specifically, each token predicted by\nthe SLM is evaluated against a reward score, and only when this score falls\nbelow a certain threshold is the cloud LLM consulted for assistance in the next\ntoken prediction. This method not only reduces the traffic to the cloud LLM,\nthereby lowering costs, but also allows for flexible control over response\nquality depending on the reward score threshold. Experimental results\ndemonstrate that our approach significantly reduces cloud LLM usage with\nminimal impact on overall response quality, offering a cost-effective solution\nfor deploying high-performance language models", "published": "2024-09-15 15:12:45", "link": "http://arxiv.org/abs/2409.13757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for\n  Code Generation", "abstract": "LLM agents enhanced by tree search algorithms have yielded notable\nperformances in code generation. However, current search algorithms in this\ndomain suffer from low search quality due to several reasons: 1) Ineffective\ndesign of the search space for the high-reasoning demands of code generation\ntasks, 2) Inadequate integration of code feedback with the search algorithm,\nand 3) Poor handling of negative feedback during the search, leading to reduced\nsearch efficiency and quality. To address these challenges, we propose to\nsearch for the reasoning process of the code and use the detailed feedback of\ncode execution to refine erroneous thoughts during the search. In this paper,\nwe introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)\nalgorithm to conduct thought-level searches before generating code, thereby\nexploring a wider range of strategies. More importantly, we construct verbal\nfeedback from fine-grained code execution feedback to refine erroneous thoughts\nduring the search. This ensures that the search progresses along the correct\nreasoning paths, thus improving the overall search quality of the tree by\nleveraging execution feedback. Through extensive experiments, we demonstrate\nthat RethinkMCTS outperforms previous search-based and feedback-based code\ngeneration baselines. On the HumanEval dataset, it improves the pass@1 of\nGPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It\neffectively conducts more thorough exploration through thought-level searches\nand enhances the search quality of the entire tree by incorporating rethink\noperation.", "published": "2024-09-15 02:07:28", "link": "http://arxiv.org/abs/2409.09584v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Improving Statistical Significance in Human Evaluation of Automatic\n  Metrics via Soft Pairwise Accuracy", "abstract": "Selecting an automatic metric that best emulates human annotators is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\nscores, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric scores. We show that SPA is more stable than PA with\nrespect to changes in the number of systems/segments used for evaluation. We\nalso show that PA can only assign a small set of distinct output values to\nmetrics, and this results in many metrics being artificially assigned the exact\nsame PA score. We demonstrate that SPA fixes this issue. Finally, we show that\nSPA is more discriminative than PA, producing more statistically significant\ncomparisons between metrics. SPA was selected as the official system-level\nmetric for the 2024 WMT Metrics Shared Task.", "published": "2024-09-15 03:25:55", "link": "http://arxiv.org/abs/2409.09598v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text\n  Quality Filtering in Large Web Corpora", "abstract": "With the increasing demand for substantial amounts of high-quality data to\ntrain large language models (LLMs), efficiently filtering large web corpora has\nbecome a critical challenge. For this purpose, KenLM, a lightweight\nn-gram-based language model that operates on CPUs, is widely used. However, the\ntraditional method of training KenLM utilizes only high-quality data and,\nconsequently, does not explicitly learn the linguistic patterns of low-quality\ndata. To address this issue, we propose an ensemble approach that leverages two\ncontrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad\nKenLM, trained on low-quality data. Experimental results demonstrate that our\napproach significantly reduces noisy content while preserving high-quality\ncontent compared to the traditional KenLM training method. This indicates that\nour method can be a practical solution with minimal computational overhead for\nresource-constrained environments.", "published": "2024-09-15 05:27:56", "link": "http://arxiv.org/abs/2409.09613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Annotation through Rationale-Driven Collaborative\n  Few-Shot Prompting", "abstract": "The traditional data annotation process is often labor-intensive,\ntime-consuming, and susceptible to human bias, which complicates the management\nof increasingly complex datasets. This study explores the potential of large\nlanguage models (LLMs) as automated data annotators to improve efficiency and\nconsistency in annotation tasks. By employing rationale-driven collaborative\nfew-shot prompting techniques, we aim to improve the performance of LLMs in\ntext annotation. We conduct a rigorous evaluation of six LLMs across four\nbenchmark datasets, comparing seven distinct methodologies. Our results\ndemonstrate that collaborative methods consistently outperform traditional\nfew-shot techniques and other baseline approaches, particularly in complex\nannotation tasks. Our work provides valuable insights and a robust framework\nfor leveraging collaborative learning methods to tackle challenging text\nannotation tasks.", "published": "2024-09-15 05:32:21", "link": "http://arxiv.org/abs/2409.09615v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence Estimation for LLM-Based Dialogue State Tracking", "abstract": "Estimation of a model's confidence on its outputs is critical for\nConversational AI systems based on large language models (LLMs), especially for\nreducing hallucination and preventing over-reliance. In this work, we provide\nan exhaustive exploration of methods, including approaches proposed for open-\nand closed-weight LLMs, aimed at quantifying and leveraging model uncertainty\nto improve the reliability of LLM-generated responses, specifically focusing on\ndialogue state tracking (DST) in task-oriented dialogue systems (TODS).\nRegardless of the model type, well-calibrated confidence scores are essential\nto handle uncertainties, thereby improving model performance. We evaluate four\nmethods for estimating confidence scores based on softmax, raw token scores,\nverbalized confidences, and a combination of these methods, using the area\nunder the curve (AUC) metric to assess calibration, with higher AUC indicating\nbetter calibration. We also enhance these with a self-probing mechanism,\nproposed for closed models. Furthermore, we assess these methods using an\nopen-weight model fine-tuned for the task of DST, achieving superior joint goal\naccuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can\nresult in enhanced AUC performance, indicating better confidence score\ncalibration.", "published": "2024-09-15 06:44:26", "link": "http://arxiv.org/abs/2409.09629v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Control With Human-Like Reasoning: Exploring Language Model\n  Embodied Air Traffic Agents", "abstract": "Recent developments in language models have created new opportunities in air\ntraffic control studies. The current focus is primarily on text and\nlanguage-based use cases. However, these language models may offer a higher\npotential impact in the air traffic control domain, thanks to their ability to\ninteract with air traffic environments in an embodied agent form. They also\nprovide a language-like reasoning capability to explain their decisions, which\nhas been a significant roadblock for the implementation of automatic air\ntraffic control.\n  This paper investigates the application of a language model-based agent with\nfunction-calling and learning capabilities to resolve air traffic conflicts\nwithout human intervention. The main components of this research are\nfoundational large language models, tools that allow the agent to interact with\nthe simulator, and a new concept, the experience library. An innovative part of\nthis research, the experience library, is a vector database that stores\nsynthesized knowledge that agents have learned from interactions with the\nsimulations and language models.\n  To evaluate the performance of our language model-based agent, both\nopen-source and closed-source models were tested. The results of our study\nreveal significant differences in performance across various configurations of\nthe language model-based agents. The best-performing configuration was able to\nsolve almost all 120 but one imminent conflict scenarios, including up to four\naircraft at the same time. Most importantly, the agents are able to provide\nhuman-level text explanations on traffic situations and conflict resolution\nstrategies.", "published": "2024-09-15 12:49:05", "link": "http://arxiv.org/abs/2409.09717v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution", "abstract": "The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption.", "published": "2024-09-15 14:10:01", "link": "http://arxiv.org/abs/2409.09739v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept\n  with Toxicity and Incivility Data", "abstract": "This article benchmarked the ability of OpenAI's GPTs and a number of\nopen-source LLMs to perform annotation tasks on political content. We used a\nnovel protest event dataset comprising more than three million digital\ninteractions and created a gold standard that includes ground-truth labels\nannotated by human coders about toxicity and incivility on social media. We\nincluded in our benchmark Google's Perspective algorithm, which, along with\nGPTs, was employed throughout their respective APIs while the open-source LLMs\nwere deployed locally. The findings show that Perspective API using a laxer\nthreshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot\nclassification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,\nwith a smaller number of parameters, are able to perform the task with high\nperformance, being attractive options that could offer good trade-offs between\nperformance, implementing costs and computing time. Ancillary findings using\nexperiments setting different temperature levels show that although GPTs tend\nto show not only excellent computing time but also overall good levels of\nreliability, only open-source LLMs ensure full reproducibility in the\nannotation.", "published": "2024-09-15 14:11:24", "link": "http://arxiv.org/abs/2409.09741v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10, 91F20 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial\n  Reasoning in Large Vision-Language Models", "abstract": "Despite recent advances demonstrating vision-language models' (VLMs)\nabilities to describe complex relationships in images using natural language,\ntheir capability to quantitatively reason about object sizes and distances\nremains underexplored. In this work, we introduce a manually annotated\nbenchmark, Q-Spatial Bench, with 271 questions across five categories designed\nfor quantitative spatial reasoning and systematically investigate the\nperformance of state-of-the-art VLMs on this task. Our analysis reveals that\nreasoning about distances between objects is particularly challenging for SoTA\nVLMs; however, some VLMs significantly outperform others, with an over 40-point\ngap between the two best performing models. We also make the surprising\nobservation that the success rate of the top-performing VLM increases by 19\npoints when a reasoning path using a reference object emerges naturally in the\nresponse. Inspired by this observation, we develop a zero-shot prompting\ntechnique, SpatialPrompt, that encourages VLMs to answer quantitative spatial\nquestions using reference objects as visual cues. By instructing VLMs to use\nreference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,\nGemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30\npoints, respectively. We emphasize that these significant improvements are\nobtained without needing more data, model architectural modifications, or\nfine-tuning.", "published": "2024-09-15 16:45:42", "link": "http://arxiv.org/abs/2409.09788v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Causal Inference with Large Language Model: A Survey", "abstract": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.", "published": "2024-09-15 18:43:11", "link": "http://arxiv.org/abs/2409.09822v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping", "abstract": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.", "published": "2024-09-15 18:56:20", "link": "http://arxiv.org/abs/2409.09825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Synthetic Free-text Medical Records with Low\n  Re-identification Risk using Masked Language Modeling", "abstract": "The vast amount of available medical records has the potential to improve\nhealthcare and biomedical research. However, privacy restrictions make these\ndata accessible for internal use only. Recent works have addressed this problem\nby generating synthetic data using Causal Language Modeling. Unfortunately, by\ntaking this approach, it is often impossible to guarantee patient privacy while\noffering the ability to control the diversity of generations without increasing\nthe cost of generating such data. In contrast, we present a system for\ngenerating synthetic free-text medical records using Masked Language Modeling.\nThe system preserves critical medical information while introducing diversity\nin the generations and minimising re-identification risk. The system's size is\nabout 120M parameters, minimising inference cost. The results demonstrate\nhigh-quality synthetic data with a HIPAA-compliant PHI recall rate of 96% and a\nre-identification risk of 3.5%. Moreover, downstream evaluations show that the\ngenerated data can effectively train a model with performance comparable to\nreal data.", "published": "2024-09-15 19:11:01", "link": "http://arxiv.org/abs/2409.09831v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Benchmark Dataset with Larger Context for Non-Factoid Question\n  Answering over Islamic Text", "abstract": "Accessing and comprehending religious texts, particularly the Quran (the\nsacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions\nof the Prophet Muhammad), in today's digital era necessitates efficient and\naccurate Question-Answering (QA) systems. Yet, the scarcity of QA systems\ntailored specifically to the detailed nature of inquiries about the Quranic\nTafsir (explanation, interpretation, context of Quran for clarity) and Ahadith\nposes significant challenges. To address this gap, we introduce a comprehensive\ndataset meticulously crafted for QA purposes within the domain of Quranic\nTafsir and Ahadith. This dataset comprises a robust collection of over 73,000\nquestion-answer pairs, standing as the largest reported dataset in this\nspecialized domain. Importantly, both questions and answers within the dataset\nare meticulously enriched with contextual information, serving as invaluable\nresources for training and evaluating tailored QA systems. However, while this\npaper highlights the dataset's contributions and establishes a benchmark for\nevaluating QA performance in the Quran and Ahadith domains, our subsequent\nhuman evaluation uncovered critical insights regarding the limitations of\nexisting automatic evaluation techniques. The discrepancy between automatic\nevaluation metrics, such as ROUGE scores, and human assessments became\napparent. The human evaluation indicated significant disparities: the model's\nverdict consistency with expert scholars ranged between 11% to 20%, while its\ncontextual understanding spanned a broader spectrum of 50% to 90%. These\nfindings underscore the necessity for evaluation techniques that capture the\nnuances and complexities inherent in understanding religious texts, surpassing\nthe limitations of traditional automatic metrics.", "published": "2024-09-15 19:50:00", "link": "http://arxiv.org/abs/2409.09844v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation\n  Extraction in Long Sentences", "abstract": "Relation extraction as an important natural Language processing (NLP) task is\nto identify relations between named entities in text. Recently, graph\nconvolutional networks over dependency trees have been widely used to capture\nsyntactic features and achieved attractive performance. However, most existing\ndependency-based approaches ignore the positive influence of the words outside\nthe dependency trees, sometimes conveying rich and useful information on\nrelation extraction. In this paper, we propose a novel model, Entity-aware\nSelf-attention Contextualized GCN (ESC-GCN), which efficiently incorporates\nsyntactic structure of input sentences and semantic context of sequences. To be\nspecific, relative position self-attention obtains the overall semantic\npairwise correlation related to word position, and contextualized graph\nconvolutional networks capture rich intra-sentence dependencies between words\nby adequately pruning operations. Furthermore, entity-aware attention layer\ndynamically selects which token is more decisive to make final relation\nprediction. In this way, our proposed model not only reduces the noisy impact\nfrom dependency trees, but also obtains easily-ignored entity-related semantic\nrepresentation. Extensive experiments on various tasks demonstrate that our\nmodel achieves encouraging performance as compared to existing dependency-based\nand sequence-based models. Specially, our model excels in extracting relations\nbetween entities of long sentences.", "published": "2024-09-15 10:50:51", "link": "http://arxiv.org/abs/2409.13755v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ValueCompass: A Framework of Fundamental Values for Human-AI Alignment", "abstract": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and language models (LMs) across four real-world vignettes:\ncollaborative writing, education, public sectors, and healthcare. Our findings\nuncover risky misalignment between humans and LMs, such as LMs agreeing with\nvalues like \"Choose Own Goals\", which are largely disagreed by humans. We also\nobserve values vary across vignettes, underscoring the necessity for\ncontext-aware AI alignment strategies. This work provides insights into the\ndesign space of human-AI alignment, offering foundations for developing AI that\nresponsibly reflects societal values and ethics.", "published": "2024-09-15 02:13:03", "link": "http://arxiv.org/abs/2409.09586v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset\n  Comparison", "abstract": "The goal of aligning language models to human preferences requires data that\nreveal these preferences. Ideally, time and money can be spent carefully\ncollecting and tailoring bespoke preference data to each downstream\napplication. However, in practice, a select few publicly available preference\ndatasets are often used to train reward models for reinforcement learning from\nhuman feedback (RLHF). While new preference datasets are being introduced with\nincreasing frequency, there are currently no existing efforts to measure and\ncompare these datasets. In this paper, we systematically study preference\ndatasets through three perspectives: scale, label noise, and information\ncontent. We propose specific metrics for each of these perspectives and uncover\ndifferent axes of comparison for a better understanding of preference datasets.\nOur work is a first step towards a data-centric approach to alignment by\nproviding perspectives that aid in training efficiency and iterative data\ncollection for RLHF.", "published": "2024-09-15 03:55:03", "link": "http://arxiv.org/abs/2409.09603v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Towards understanding evolution of science through language model series", "abstract": "We introduce AnnualBERT, a series of language models designed specifically to\ncapture the temporal evolution of scientific text. Deviating from the\nprevailing paradigms of subword tokenizations and \"one model to rule them all\",\nAnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model\npretrained from scratch on the full-text of 1.7 million arXiv papers published\nuntil 2008 and a collection of progressively trained models on arXiv papers at\nan annual basis. We demonstrate the effectiveness of AnnualBERT models by\nshowing that they not only have comparable performances in standard tasks but\nalso achieve state-of-the-art performances on domain-specific NLP tasks as well\nas link prediction tasks in the arXiv citation network. We then utilize probing\ntasks to quantify the models' behavior in terms of representation learning and\nforgetting as time progresses. Our approach enables the pretrained models to\nnot only improve performances on scientific text processing tasks but also to\nprovide insights into the development of scientific discourse over time. The\nseries of the models is available at https://huggingface.co/jd445/AnnualBERTs.", "published": "2024-09-15 07:15:05", "link": "http://arxiv.org/abs/2409.09636v1", "categories": ["cs.CL", "cs.CY", "cs.DL"], "primary_category": "cs.CL"}
{"title": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models", "abstract": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges.", "published": "2024-09-15 08:25:24", "link": "http://arxiv.org/abs/2409.09662v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using\n  LLMs", "abstract": "In recent years, there has been a surge in the publication of clinical trial\nreports, making it challenging to conduct systematic reviews. Automatically\nextracting Population, Intervention, Comparator, and Outcome (PICO) from\nclinical trial studies can alleviate the traditionally time-consuming process\nof manually scrutinizing systematic reviews. Existing approaches of PICO frame\nextraction involves supervised approach that relies on the existence of\nmanually annotated data points in the form of BIO label tagging. Recent\napproaches, such as In-Context Learning (ICL), which has been shown to be\neffective for a number of downstream NLP tasks, require the use of labeled\nexamples. In this work, we adopt ICL strategy by employing the pretrained\nknowledge of Large Language Models (LLMs), gathered during the pretraining\nphase of an LLM, to automatically extract the PICO-related terminologies from\nclinical trial documents in unsupervised set up to bypass the availability of\nlarge number of annotated data instances. Additionally, to showcase the highest\neffectiveness of LLM in oracle scenario where large number of annotated samples\nare available, we adopt the instruction tuning strategy by employing Low Rank\nAdaptation (LORA) to conduct the training of gigantic model in low resource\nenvironment for the PICO frame extraction task. Our empirical results show that\nour proposed ICL-based framework produces comparable results on all the version\nof EBM-NLP datasets and the proposed instruction tuned version of our framework\nproduces state-of-the-art results on all the different EBM-NLP datasets. Our\nproject is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.", "published": "2024-09-15 11:53:24", "link": "http://arxiv.org/abs/2409.09704v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing", "abstract": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.", "published": "2024-09-15 15:01:00", "link": "http://arxiv.org/abs/2409.09760v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.8"], "primary_category": "cs.HC"}
{"title": "Acquiring Pronunciation Knowledge from Transcribed Speech Audio via\n  Multi-task Learning", "abstract": "Recent work has shown the feasibility and benefit of bootstrapping an\nintegrated sequence-to-sequence (Seq2Seq) linguistic frontend from a\ntraditional pipeline-based frontend for text-to-speech (TTS). To overcome the\nfixed lexical coverage of bootstrapping training data, previous work has\nproposed to leverage easily accessible transcribed speech audio as an\nadditional training source for acquiring novel pronunciation knowledge for\nuncovered words, which relies on an auxiliary ASR model as part of a cumbersome\nimplementation flow. In this work, we propose an alternative method to leverage\ntranscribed speech audio as an additional training source, based on multi-task\nlearning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend,\nthe proposed MTL-based method reduces PER from 2.5% to 1.6% for those word\ntypes covered exclusively in transcribed speech audio, achieving a similar\nperformance to the previous method but with a much simpler implementation flow.", "published": "2024-09-15 23:00:54", "link": "http://arxiv.org/abs/2409.09891v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Models and Retrieval Augmented Generation for Automated\n  Structured Data Extraction from Diagnostic Reports", "abstract": "Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction.", "published": "2024-09-15 15:21:45", "link": "http://arxiv.org/abs/2409.10576v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "J.3; I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Optimizing the Songwriting Process: Genre-Based Lyric Generation Using\n  Deep Learning Models", "abstract": "The traditional songwriting process is rather complex and this is evident in\nthe time it takes to produce lyrics that fit the genre and form comprehensive\nverses. Our project aims to simplify this process with deep learning\ntechniques, thus optimizing the songwriting process and enabling an artist to\nhit their target audience by staying in genre. Using a dataset of 18,000 songs\noff Spotify, we developed a unique preprocessing format using tokens to parse\nlyrics into individual verses. These results were used to train a baseline\npretrained seq2seq model, and a LSTM-based neural network models according to\nsong genres. We found that generation yielded higher recall (ROUGE) in the\nbaseline model, but similar precision (BLEU) for both models. Qualitatively, we\nfound that many of the lyrical phrases generated by the original model were\nstill comprehensible and discernible between which genres they fit into,\ndespite not necessarily being the exact the same as the true lyrics. Overall,\nour results yielded that lyric generation can reasonably be sped up to produce\ngenre-based lyrics and aid in hastening the songwriting process.", "published": "2024-09-15 21:32:46", "link": "http://arxiv.org/abs/2409.13758v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition", "abstract": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.", "published": "2024-09-15 16:32:49", "link": "http://arxiv.org/abs/2409.09785v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "S2Cap: A Benchmark and a Baseline for Singing Style Captioning", "abstract": "Singing voices contain much richer information than common voices, such as\ndiverse vocal and acoustic characteristics. However, existing open-source\naudio-text datasets for singing voices capture only a limited set of attributes\nand lacks acoustic features, leading to limited utility towards downstream\ntasks, such as style captioning. To fill this gap, we formally consider the\ntask of singing style captioning and introduce S2Cap, a singing voice dataset\nwith comprehensive descriptions of diverse vocal, acoustic and demographic\nattributes. Based on this dataset, we develop a simple yet effective baseline\nalgorithm for the singing style captioning. The algorithm utilizes two novel\ntechnical components: CRESCENDO for mitigating misalignment between pretrained\nunimodal models, and demixing supervision to regularize the model to focus on\nthe singing voice. Despite its simplicity, the proposed method outperforms\nstate-of-the-art baselines.", "published": "2024-09-15 21:19:24", "link": "http://arxiv.org/abs/2409.09866v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the effectiveness of enrollment speech augmentation for Target\n  Speaker Extraction", "abstract": "Deep learning technologies have significantly advanced the performance of\ntarget speaker extraction (TSE) tasks. To enhance the generalization and\nrobustness of these algorithms when training data is insufficient, data\naugmentation is a commonly adopted technique. Unlike typical data augmentation\napplied to speech mixtures, this work thoroughly investigates the effectiveness\nof augmenting the enrollment speech space. We found that for both pretrained\nand jointly optimized speaker encoders, directly augmenting the enrollment\nspeech leads to consistent performance improvement. In addition to conventional\nmethods such as noise and reverberation addition, we propose a novel\naugmentation method called self-estimated speech augmentation (SSA).\nExperimental results on the Libri2Mix test set show that our proposed method\ncan achieve an improvement of up to 2.5 dB.", "published": "2024-09-15 02:29:43", "link": "http://arxiv.org/abs/2409.09589v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compositional Audio Representation Learning", "abstract": "Human auditory perception is compositional in nature -- we identify auditory\nstreams from auditory scenes with multiple sound events. However, such auditory\nscenes are typically represented using clip-level representations that do not\ndisentangle the constituent sound sources. In this work, we learn\nsource-centric audio representations where each sound source is represented\nusing a distinct, disentangled source embedding in the audio representation. We\npropose two novel approaches to learning source-centric audio representations:\na supervised model guided by classification and an unsupervised model guided by\nfeature reconstruction, both of which outperform the baselines. We thoroughly\nevaluate the design choices of both approaches using an audio classification\ntask. We find that supervision is beneficial to learn source-centric\nrepresentations, and that reconstructing audio features is more useful than\nreconstructing spectrograms to learn unsupervised source-centric\nrepresentations. Leveraging source-centric models can help unlock the potential\nof greater interpretability and more flexible decoding in machine listening.", "published": "2024-09-15 06:00:42", "link": "http://arxiv.org/abs/2409.09619v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey of Foundation Models for Music Understanding", "abstract": "Music is essential in daily life, fulfilling emotional and entertainment\nneeds, and connecting us personally, socially, and culturally. A better\nunderstanding of music can enhance our emotions, cognitive skills, and cultural\nconnections. The rapid advancement of artificial intelligence (AI) has\nintroduced new ways to analyze music, aiming to replicate human understanding\nof music and provide related services. While the traditional models focused on\naudio features and simple tasks, the recent development of large language\nmodels (LLMs) and foundation models (FMs), which excel in various fields by\nintegrating semantic information and demonstrating strong reasoning abilities,\ncould capture complex musical features and patterns, integrate music with\nlanguage and incorporate rich musical, emotional and psychological knowledge.\nTherefore, they have the potential in handling complex music understanding\ntasks from a semantic perspective, producing outputs closer to human\nperception. This work, to our best knowledge, is one of the early reviews of\nthe intersection of AI techniques and music understanding. We investigated,\nanalyzed, and tested recent large-scale music foundation models in respect of\ntheir music comprehension abilities. We also discussed their limitations and\nproposed possible future directions, offering insights for researchers in this\nfield.", "published": "2024-09-15 03:34:14", "link": "http://arxiv.org/abs/2409.09601v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stutter-Solver: End-to-end Multi-lingual Dysfluency Detection", "abstract": "Current de-facto dysfluency modeling methods utilize template matching\nalgorithms which are not generalizable to out-of-domain real-world dysfluencies\nacross languages, and are not scalable with increasing amounts of training\ndata. To handle these problems, we propose Stutter-Solver: an end-to-end\nframework that detects dysfluency with accurate type and time transcription,\ninspired by the YOLO object detection algorithm. Stutter-Solver can handle\nco-dysfluencies and is a natural multi-lingual dysfluency detector. To leverage\nscalability and boost performance, we also introduce three novel dysfluency\ncorpora: VCTK-Pro, VCTK-Art, and AISHELL3-Pro, simulating natural spoken\ndysfluencies including repetition, block, missing, replacement, and\nprolongation through articulatory-encodec and TTS-based methods. Our approach\nachieves state-of-the-art performance on all available dysfluency corpora. Code\nand datasets are open-sourced at https://github.com/eureka235/Stutter-Solver", "published": "2024-09-15 06:11:00", "link": "http://arxiv.org/abs/2409.09621v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Extract and Diffuse: Latent Integration for Improved Diffusion-based\n  Speech and Vocal Enhancement", "abstract": "Diffusion-based generative models have recently achieved remarkable results\nin speech and vocal enhancement due to their ability to model complex speech\ndata distributions. While these models generalize well to unseen acoustic\nenvironments, they may not achieve the same level of fidelity as the\ndiscriminative models specifically trained to enhance particular acoustic\nconditions. In this paper, we propose Ex-Diff, a novel score-based diffusion\nmodel that integrates the latent representations produced by a discriminative\nmodel to improve speech and vocal enhancement, which combines the strengths of\nboth generative and discriminative models. Experimental results on the widely\nused MUSDB dataset show relative improvements of 3.7% in SI-SDR and 10.0% in\nSI-SIR compared to the baseline diffusion model for speech and vocal\nenhancement tasks, respectively. Additionally, case studies are provided to\nfurther illustrate and analyze the complementary nature of generative and\ndiscriminative models in this context.", "published": "2024-09-15 07:25:08", "link": "http://arxiv.org/abs/2409.09642v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Learning for Acoustic Few-Shot Classification", "abstract": "Labelled data are limited and self-supervised learning is one of the most\nimportant approaches for reducing labelling requirements. While it has been\nextensively explored in the image domain, it has so far not received the same\namount of attention in the acoustic domain. Yet, reducing labelling is a key\nrequirement for many acoustic applications. Specifically in bioacoustic, there\nare rarely sufficient labels for fully supervised learning available. This has\nled to the widespread use of acoustic recognisers that have been pre-trained on\nunrelated data for bioacoustic tasks. We posit that training on the actual task\ndata and combining self-supervised pre-training with few-shot classification is\na superior approach that has the ability to deliver high accuracy even when\nonly a few labels are available. To this end, we introduce and evaluate a new\narchitecture that combines CNN-based preprocessing with feature extraction\nbased on state space models (SSMs). This combination is motivated by the fact\nthat CNN-based networks alone struggle to capture temporal information\neffectively, which is crucial for classifying acoustic signals. SSMs,\nspecifically S4 and Mamba, on the other hand, have been shown to have an\nexcellent ability to capture long-range dependencies in sequence data. We\npre-train this architecture using contrastive learning on the actual task data\nand subsequent fine-tuning with an extremely small amount of labelled data. We\nevaluate the performance of this proposed architecture for ($n$-shot,\n$n$-class) classification on standard benchmarks as well as real-world data.\nOur evaluation shows that it outperforms state-of-the-art architectures on the\nfew-shot classification problem.", "published": "2024-09-15 07:45:11", "link": "http://arxiv.org/abs/2409.09647v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Multimodal Speech Representations for the Assessment of\n  Schizophrenia Symptoms", "abstract": "Multimodal schizophrenia assessment systems have gained traction over the\nlast few years. This work introduces a schizophrenia assessment system to\ndiscern between prominent symptom classes of schizophrenia and predict an\noverall schizophrenia severity score. We develop a Vector Quantized Variational\nAuto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to\nproduce task-agnostic speech representations from vocal Tract Variables (TVs)\nand Facial Action Units (FAUs). These representations are then used in a\nMulti-Task Learning (MTL) based downstream prediction model to obtain class\nlabels and an overall severity score. The proposed framework outperforms the\nprevious works on the multi-class classification task across all evaluation\nmetrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy).\nAdditionally, it estimates the schizophrenia severity score, a task not\naddressed by earlier approaches.", "published": "2024-09-15 13:45:04", "link": "http://arxiv.org/abs/2409.09733v5", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Efficient Video to Audio Mapper with Visual Scene Detection", "abstract": "Video-to-audio (V2A) generation aims to produce corresponding audio given\nsilent video inputs. This task is particularly challenging due to the\ncross-modality and sequential nature of the audio-visual features involved.\nRecent works have made significant progress in bridging the domain gap between\nvideo and audio, generating audio that is semantically aligned with the video\ncontent. However, a critical limitation of these approaches is their inability\nto effectively recognize and handle multiple scenes within a video, often\nleading to suboptimal audio generation in such cases. In this paper, we first\nreimplement a state-of-the-art V2A model with a slightly modified light-weight\narchitecture, achieving results that outperform the baseline. We then propose\nan improved V2A model that incorporates a scene detector to address the\nchallenge of switching between multiple visual scenes. Results on VGGSound show\nthat our model can recognize and handle multiple scenes within a video and\nachieve superior performance against the baseline for both fidelity and\nrelevance.", "published": "2024-09-15 18:51:18", "link": "http://arxiv.org/abs/2409.09823v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Audio Narrations to Strengthen Domain Generalization in\n  Multimodal First-Person Action Recognition", "abstract": "First-person activity recognition is rapidly growing due to the widespread\nuse of wearable cameras but faces challenges from domain shifts across\ndifferent environments, such as varying objects or background scenes. We\npropose a multimodal framework that improves domain generalization by\nintegrating motion, audio, and appearance features. Key contributions include\nanalyzing the resilience of audio and motion features to domain shifts, using\naudio narrations for enhanced audio-text alignment, and applying consistency\nratings between audio and visual narrations to optimize the impact of audio in\nrecognition during training. Our approach achieves state-of-the-art performance\non the ARGO1M dataset, effectively generalizing across unseen scenarios and\nlocations.", "published": "2024-09-15 04:43:00", "link": "http://arxiv.org/abs/2409.09611v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
