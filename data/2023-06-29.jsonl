{"title": "A negation detection assessment of GPTs: analysis with the xNot360\n  dataset", "abstract": "Negation is a fundamental aspect of natural language, playing a critical role\nin communication and comprehension. Our study assesses the negation detection\nperformance of Generative Pre-trained Transformer (GPT) models, specifically\nGPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in\nnatural language using a zero-shot prediction approach applied to our custom\nxNot360 dataset. Our approach examines sentence pairs labeled to indicate\nwhether the second sentence negates the first. Our findings expose a\nconsiderable performance disparity among the GPT models, with GPT-4 surpassing\nits counterparts and GPT-3.5 displaying a marked performance reduction. The\noverall proficiency of the GPT models in negation detection remains relatively\nmodest, indicating that this task pushes the boundaries of their natural\nlanguage understanding capabilities. We not only highlight the constraints of\nGPT models in handling negation but also emphasize the importance of logical\nreliability in high-stakes domains such as healthcare, science, and law.", "published": "2023-06-29 02:27:48", "link": "http://arxiv.org/abs/2306.16638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Linguistic Knowledge and Token-level Text Augmentation", "abstract": "This paper investigates the effectiveness of token-level text augmentation\nand the role of probabilistic linguistic knowledge within a\nlinguistically-motivated evaluation context. Two text augmentation programs,\nREDA and REDA$_{NG}$, were developed, both implementing five token-level text\nediting operations: Synonym Replacement (SR), Random Swap (RS), Random\nInsertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$\nleverages pretrained $n$-gram language models to select the most likely\naugmented texts from REDA's output. Comprehensive and fine-grained experiments\nwere conducted on a binary question matching classification task in both\nChinese and English. The results strongly refute the general effectiveness of\nthe five token-level text augmentation techniques under investigation, whether\napplied together or separately, and irrespective of various common\nclassification model types used, including transformers. Furthermore, the role\nof probabilistic linguistic knowledge is found to be minimal.", "published": "2023-06-29 03:02:04", "link": "http://arxiv.org/abs/2306.16644v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple\n  Oracles", "abstract": "Automatically generating textual content with desired attributes is an\nambitious task that people have pursued long. Existing works have made a series\nof progress in incorporating unimodal controls into language models (LMs),\nwhereas how to generate controllable sentences with multimodal signals and high\nefficiency remains an open question. To tackle the puzzle, we propose a new\nparadigm of zero-shot controllable text generation with multimodal signals\n(\\textsc{ZeroGen}). Specifically, \\textsc{ZeroGen} leverages controls of text\nand image successively from token-level to sentence-level and maps them into a\nunified probability space at decoding, which customizes the LM outputs by\nweighted addition without extra training. To achieve better inter-modal\ntrade-offs, we further introduce an effective dynamic weighting mechanism to\nregulate all control weights. Moreover, we conduct substantial experiments to\nprobe the relationship of being in-depth or in-width between signals from\ndistinct modalities. Encouraging empirical results on three downstream tasks\nshow that \\textsc{ZeroGen} not only outperforms its counterparts on captioning\ntasks by a large margin but also shows great potential in multimodal news\ngeneration with a higher degree of control. Our code will be released at\nhttps://github.com/ImKeTT/ZeroGen.", "published": "2023-06-29 03:22:43", "link": "http://arxiv.org/abs/2306.16649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Language Representation for Question Answering over Text,\n  Tables, and Images", "abstract": "When trying to answer complex questions, people often rely on multiple\nsources of information, such as visual, textual, and tabular data. Previous\napproaches to this problem have focused on designing input features or model\nstructure in the multi-modal space, which is inflexible for cross-modal\nreasoning or data-efficient training. In this paper, we call for an alternative\nparadigm, which transforms the images and tables into unified language\nrepresentations, so that we can simplify the task into a simpler textual QA\nproblem that can be solved using three steps: retrieval, ranking, and\ngeneration, all within a language space. This idea takes advantage of the power\nof pre-trained language models and is implemented in a framework called Solar.\nOur experimental results show that Solar outperforms all existing methods by\n10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different\nmetrics. Additionally, Solar achieves the best performance on the WebQA\nleaderboard", "published": "2023-06-29 08:02:23", "link": "http://arxiv.org/abs/2306.16762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many\nvision-language tasks, such as image-text retrieval, visual entailment, and\nvisual reasoning. The pre-training mostly utilizes lexical databases and image\nqueries in English. Previous work has demonstrated that the pre-training in\nEnglish does not transfer well to other languages in a zero-shot setting.\nHowever, multilingual pre-trained language models (MPLM) have excelled at a\nvariety of single-modal language tasks. In this paper, we propose a simple yet\nefficient approach to adapt VLP to unseen languages using MPLM. We utilize a\ncross-lingual contextualized token embeddings alignment approach to train text\nencoders for non-English languages. Our approach does not require image input\nand primarily uses machine translation, eliminating the need for target\nlanguage data. Our evaluation across three distinct tasks (image-text\nretrieval, visual entailment, and natural language visual reasoning)\ndemonstrates that this approach outperforms the state-of-the-art multilingual\nvision-language models without requiring large parallel corpora. Our code is\navailable at https://github.com/Yasminekaroui/CliCoTea.", "published": "2023-06-29 08:20:57", "link": "http://arxiv.org/abs/2306.16774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Model Capabilities for Conditional\n  Generation", "abstract": "Pre-trained large language models (PLMs) underlie most new developments in\nnatural language processing. They have shifted the field from\napplication-specific model pipelines to a single model that is adapted to a\nwide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside\ntechniques like few-shot learning, have additionally shifted the output\nmodality to generation instead of classification or regression. Despite their\nubiquitous use, the generation quality of language models is rarely evaluated\nwhen these models are introduced. Additionally, it is unclear how existing\ngeneration tasks--while they can be used to compare systems at a high\nlevel--relate to the real world use cases for which people have been adopting\nthem. In this work, we discuss how to adapt existing application-specific\ngeneration benchmarks to PLMs and provide an in-depth, empirical study of the\nlimitations and capabilities of PLMs in natural language generation tasks along\ndimensions such as scale, architecture, input and output language. Our results\nshow that PLMs differ in their applicability to different data regimes and\ntheir generalization to multiple languages and inform which PLMs to use for a\ngiven generation task setup. We share best practices to be taken into\nconsideration when benchmarking generation capabilities during the development\nof upcoming PLMs.", "published": "2023-06-29 08:59:40", "link": "http://arxiv.org/abs/2306.16793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research", "abstract": "Many recent improvements in NLP stem from the development and use of large\npre-trained language models (PLMs) with billions of parameters. Large model\nsizes makes computational cost one of the main limiting factors for training\nand evaluating such models; and has raised severe concerns about the\nsustainability, reproducibility, and inclusiveness for researching PLMs. These\nconcerns are often based on personal experiences and observations. However,\nthere had not been any large-scale surveys that investigate them. In this work,\nwe provide a first attempt to quantify these concerns regarding three topics,\nnamely, environmental impact, equity, and impact on peer reviewing. By\nconducting a survey with 312 participants from the NLP community, we capture\nexisting (dis)parities between different and within groups with respect to\nseniority, academia, and industry; and their impact on the peer reviewing\nprocess. For each topic, we provide an analysis and devise recommendations to\nmitigate found disparities, some of which already successfully implemented.\nFinally, we discuss additional concerns raised by many participants in\nfree-text responses.", "published": "2023-06-29 12:44:53", "link": "http://arxiv.org/abs/2306.16900v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Open-Domain Topic Classification", "abstract": "We introduce an open-domain topic classification system that accepts\nuser-defined taxonomy in real time. Users will be able to classify a text\nsnippet with respect to any candidate labels they want, and get instant\nresponse from our web interface. To obtain such flexibility, we build the\nbackend model in a zero-shot way. By training on a new dataset constructed from\nWikipedia, our label-aware text classifier can effectively utilize implicit\nknowledge in the pretrained language model to handle labels it has never seen\nbefore. We evaluate our model across four datasets from various domains with\ndifferent label sets. Experiments show that the model significantly improves\nover existing zero-shot baselines in open-domain scenarios, and performs\ncompetitively with weakly-supervised models trained on in-domain data.", "published": "2023-06-29 20:25:28", "link": "http://arxiv.org/abs/2306.17290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Citations as Queries: Source Attribution Using Language Models as\n  Rerankers", "abstract": "This paper explores new methods for locating the sources used to write a\ntext, by fine-tuning a variety of language models to rerank candidate sources.\nAfter retrieving candidates sources using a baseline BM25 retrieval model, a\nvariety of reranking methods are tested to see how effective they are at the\ntask of source attribution. We conduct experiments on two datasets, English\nWikipedia and medieval Arabic historical writing, and employ a variety of\nretrieval and generation based reranking models. In particular, we seek to\nunderstand how the degree of supervision required affects the performance of\nvarious reranking models. We find that semisupervised methods can be nearly as\neffective as fully supervised methods while avoiding potentially costly\nspan-level annotation of the target and source documents.", "published": "2023-06-29 22:13:38", "link": "http://arxiv.org/abs/2306.17322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-source Semantic Graph-based Multimodal Sarcasm Explanation\n  Generation", "abstract": "Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which\naims to generate a natural language sentence for a multimodal social post (an\nimage as well as its caption) to explain why it contains sarcasm. Although the\nexisting pioneer study has achieved great success with the BART backbone, it\noverlooks the gap between the visual feature space and the decoder semantic\nspace, the object-level metadata of the image, as well as the potential\nexternal knowledge. To solve these limitations, in this work, we propose a\nnovel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme,\nnamed TEAM. In particular, TEAM extracts the object-level semantic meta-data\ninstead of the traditional global visual features from the input image.\nMeanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge\nconcepts for the input text and the extracted object meta-data. Thereafter,\nTEAM introduces a multi-source semantic graph that comprehensively characterize\nthe multi-source (i.e., caption, object meta-data, external knowledge) semantic\nrelations to facilitate the sarcasm reasoning. Extensive experiments on a\npublic released dataset MORE verify the superiority of our model over\ncutting-edge methods.", "published": "2023-06-29 03:26:10", "link": "http://arxiv.org/abs/2306.16650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "abstract": "We present PaRTE, a collection of 1,126 pairs of Recognizing Textual\nEntailment (RTE) examples to evaluate whether models are robust to\nparaphrasing. We posit that if RTE models understand language, their\npredictions should be consistent across inputs that share the same meaning. We\nuse the evaluation set to determine if RTE models' predictions change when\nexamples are paraphrased. In our experiments, contemporary models change their\npredictions on 8-16\\% of paraphrased examples, indicating that there is still\nroom for improvement.", "published": "2023-06-29 06:48:22", "link": "http://arxiv.org/abs/2306.16722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data\n  Augmentation in Multi-Turn Conversations", "abstract": "In open-domain dialogue generation tasks, contexts and responses in most\ndatasets are one-to-one mapped, violating an important many-to-many\ncharacteristic: a context leads to various responses, and a response answers\nmultiple contexts. Without such patterns, models poorly generalize and prefer\nresponding safely. Many attempts have been made in either multi-turn settings\nfrom a one-to-many perspective or in a many-to-many perspective but limited to\nsingle-turn settings. The major challenge to many-to-many augment multi-turn\ndialogues is that discretely replacing each turn with semantic similarity\nbreaks fragile context coherence. In this paper, we propose DialoGue Path\nSampling (DialoGPS) method in continuous semantic space, the first many-to-many\naugmentation method for multi-turn dialogues. Specifically, we map a dialogue\nto our extended Brownian Bridge, a special Gaussian process. We sample latent\nvariables to form coherent dialogue paths in the continuous space. A dialogue\npath corresponds to a new multi-turn dialogue and is used as augmented training\ndata. We show the effect of DialoGPS with both automatic and human evaluation.", "published": "2023-06-29 08:12:47", "link": "http://arxiv.org/abs/2306.16770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Formal Perspective on Byte-Pair Encoding", "abstract": "Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in\nNLP, despite being devised initially as a compression method. BPE appears to be\na greedy algorithm at face value, but the underlying optimization problem that\nBPE seeks to solve has not yet been laid down. We formalize BPE as a\ncombinatorial optimization problem. Via submodular functions, we prove that the\niterative greedy version is a\n$\\frac{1}{{\\sigma(\\boldsymbol{\\mu}^\\star)}}(1-e^{-{\\sigma(\\boldsymbol{\\mu}^\\star)}})$-approximation\nof an optimal merge sequence, where ${\\sigma(\\boldsymbol{\\mu}^\\star)}$ is the\ntotal backward curvature with respect to the optimal merge sequence\n$\\boldsymbol{\\mu}^\\star$. Empirically the lower bound of the approximation is\n$\\approx 0.37$.\n  We provide a faster implementation of BPE which improves the runtime\ncomplexity from $\\mathcal{O}\\left(N M\\right)$ to $\\mathcal{O}\\left(N \\log\nM\\right)$, where $N$ is the sequence length and $M$ is the merge count.\nFinally, we optimize the brute-force algorithm for optimal BPE using\nmemoization.", "published": "2023-06-29 10:29:23", "link": "http://arxiv.org/abs/2306.16837v3", "categories": ["cs.CL", "math.OC"], "primary_category": "cs.CL"}
{"title": "UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality\n  synthetic note-oriented doctor-patient conversations?", "abstract": "This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023\nshared task for Task-A and Task-C. We focus especially on Task-C and propose a\nnovel LLMs cooperation system named a doctor-patient loop to generate\nhigh-quality conversation data sets. The experiment results demonstrate that\nour approaches yield reasonable performance as evaluated by automatic metrics\nsuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we\nconducted a comparative analysis between our proposed method and ChatGPT and\nGPT-4. This analysis also investigates the potential of utilizing cooperation\nLLMs to generate high-quality datasets.", "published": "2023-06-29 13:30:41", "link": "http://arxiv.org/abs/2306.16931v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis is a long-standing research interest in the\nfield of opinion mining, and in recent years, researchers have gradually\nshifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA\ntasks. However, the datasets currently used in the research are limited to\nindividual elements of specific tasks, usually focusing on in-domain settings,\nignoring implicit aspects and opinions, and with a small data scale. To address\nthese issues, we propose a large-scale Multi-Element Multi-Domain dataset\n(MEMD) that covers the four elements across five domains, including nearly\n20,000 review sentences and 30,000 quadruples annotated with explicit and\nimplicit aspects and opinions for ABSA research. Meanwhile, we evaluate\ngenerative and non-generative baselines on multiple ABSA subtasks under the\nopen domain setting, and the results show that open domain ABSA as well as\nmining implicit aspects and opinions remain ongoing challenges to be addressed.\nThe datasets are publicly released at \\url{https://github.com/NUSTM/MEMD-ABSA}.", "published": "2023-06-29 14:03:49", "link": "http://arxiv.org/abs/2306.16956v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifying Crime Types using Judgment Documents from Social Media", "abstract": "The task of determining crime types based on criminal behavior facts has\nbecome a very important and meaningful task in social science. But the problem\nfacing the field now is that the data samples themselves are unevenly\ndistributed, due to the nature of the crime itself. At the same time, data sets\nin the judicial field are less publicly available, and it is not practical to\nproduce large data sets for direct training. This article proposes a new\ntraining model to solve this problem through NLP processing methods. We first\npropose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the\ndefects of uneven data set distribution by generating new samples. Then we use\na large open source dataset (CAIL-big) as our pretraining dataset and a small\ndataset collected by ourselves for Fine-tuning, giving it good generalization\nability to unfamiliar small datasets. At the same time, we use the improved\nBert model with dynamic masking to improve the model. Experiments show that the\nproposed method achieves state-of-the-art results on the present dataset. At\nthe same time, the effectiveness of module CFDPM is proved by experiments. This\narticle provides a valuable methodology contribution for classifying social\nscience texts such as criminal behaviors. Extensive experiments on public\nbenchmarks show that the proposed method achieves new state-of-the-art results.", "published": "2023-06-29 15:12:24", "link": "http://arxiv.org/abs/2306.17020v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring & Exploiting High-Order Graph Structure for Sparse Knowledge\n  Graph Completion", "abstract": "Sparse knowledge graph (KG) scenarios pose a challenge for previous Knowledge\nGraph Completion (KGC) methods, that is, the completion performance decreases\nrapidly with the increase of graph sparsity. This problem is also exacerbated\nbecause of the widespread existence of sparse KGs in practical applications. To\nalleviate this challenge, we present a novel framework, LR-GCN, that is able to\nautomatically capture valuable long-range dependency among entities to\nsupplement insufficient structure features and distill logical reasoning\nknowledge for sparse KGC. The proposed approach comprises two main components:\na GNN-based predictor and a reasoning path distiller. The reasoning path\ndistiller explores high-order graph structures such as reasoning paths and\nencodes them as rich-semantic edges, explicitly compositing long-range\ndependencies into the predictor. This step also plays an essential role in\ndensifying KGs, effectively alleviating the sparse issue. Furthermore, the path\ndistiller further distills logical reasoning knowledge from these mined\nreasoning paths into the predictor. These two components are jointly optimized\nusing a well-designed variational EM algorithm. Extensive experiments and\nanalyses on four sparse benchmarks demonstrate the effectiveness of our\nproposed method.", "published": "2023-06-29 15:35:34", "link": "http://arxiv.org/abs/2306.17034v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Grammatical Tagging for the Legal Language of Cybersecurity", "abstract": "Legal language can be understood as the language typically used by those\nengaged in the legal profession and, as such, it may come both in spoken or\nwritten form. Recent legislation on cybersecurity obviously uses legal language\nin writing, thus inheriting all its interpretative complications due to the\ntypical abundance of cases and sub-cases as well as to the general richness in\ndetail. This paper faces the challenge of the essential interpretation of the\nlegal language of cybersecurity, namely of the extraction of the essential\nParts of Speech (POS) from the legal documents concerning cybersecurity. The\nchallenge is overcome by our methodology for POS tagging of legal language. It\nleverages state-of-the-art open-source tools for Natural Language Processing\n(NLP) as well as manual analysis to validate the outcomes of the tools. As a\nresult, the methodology is automated and, arguably, general for any legal\nlanguage following minor tailoring of the preprocessing step. It is\ndemonstrated over the most relevant EU legislation on cybersecurity, namely on\nthe NIS 2 directive, producing the first, albeit essential, structured\ninterpretation of such a relevant document. Moreover, our findings indicate\nthat tools such as SpaCy and ClausIE reach their limits over the legal language\nof the NIS 2.", "published": "2023-06-29 15:39:20", "link": "http://arxiv.org/abs/2306.17042v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Concept-Oriented Deep Learning with Large Language Models", "abstract": "Large Language Models (LLMs) have been successfully used in many\nnatural-language tasks and applications including text generation and AI\nchatbots. They also are a promising new technology for concept-oriented deep\nlearning (CODL). However, the prerequisite is that LLMs understand concepts and\nensure conceptual consistency. We discuss these in this paper, as well as major\nuses of LLMs for CODL including concept extraction from text, concept graph\nextraction from text, and concept learning. Human knowledge consists of both\nsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only\nLLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal\nLLMs, on the other hand, are capable of representing the full range (conceptual\nand sensory) of human knowledge. We discuss conceptual understanding in\nvisual-language LLMs, the most important multimodal LLMs, and major uses of\nthem for CODL including concept extraction from image, concept graph extraction\nfrom image, and concept learning. While uses of LLMs for CODL are valuable\nstandalone, they are particularly valuable as part of LLM applications such as\nAI chatbots.", "published": "2023-06-29 16:47:11", "link": "http://arxiv.org/abs/2306.17089v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image\n  Understanding", "abstract": "Instruction tuning unlocks the superior capability of Large Language Models\n(LLM) to interact with humans. Furthermore, recent instruction-following\ndatasets include images as visual inputs, collecting responses for image-based\ninstructions. However, visual instruction-tuned models cannot comprehend\ntextual details within images well. This work enhances the current visual\ninstruction tuning pipeline with text-rich images (e.g., movie posters, book\ncovers, etc.). Specifically, we first use publicly available OCR tools to\ncollect results on 422K text-rich images from the LAION dataset. Moreover, we\nprompt text-only GPT-4 with recognized texts and image captions to generate 16K\nconversations, each containing question-answer pairs for text-rich images. By\ncombining our collected data with previous multi-modal instruction-following\ndata, our model, LLaVAR, substantially improves the LLaVA model's capability on\ntext-based VQA datasets (up to 20% accuracy improvement) while achieving an\naccuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following\nevaluation also demonstrates the improvement of our model on both natural\nimages and text-rich images. Through qualitative analysis, LLaVAR shows\npromising interaction (e.g., reasoning, writing, and elaboration) skills with\nhumans based on the latest real-world online content that combines text and\nimages. We make our code/data/models publicly available at\nhttps://llavar.github.io/.", "published": "2023-06-29 17:08:16", "link": "http://arxiv.org/abs/2306.17107v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?", "abstract": "We present the Chinese Elementary School Math Word Problems (CMATH) dataset,\ncomprising 1.7k elementary school-level math word problems with detailed\nannotations, source from actual Chinese workbooks and exams. This dataset aims\nto provide a benchmark tool for assessing the following question: to what grade\nlevel of elementary school math do the abilities of popular large language\nmodels (LLMs) correspond? We evaluate a variety of popular LLMs, including both\ncommercial and open-source options, and discover that only GPT-4 achieves\nsuccess (accuracy $\\geq$ 60\\%) across all six elementary school grades, while\nother models falter at different grade levels. Furthermore, we assess the\nrobustness of several top-performing LLMs by augmenting the original problems\nin the CMATH dataset with distracting information. Our findings reveal that\nGPT-4 is able to maintains robustness, while other model fail. We anticipate\nthat our study will expose limitations in LLMs' arithmetic and reasoning\ncapabilities, and promote their ongoing development and advancement.", "published": "2023-06-29 02:19:50", "link": "http://arxiv.org/abs/2306.16636v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition of Non-Native Child Speech for Language\n  Learning Applications", "abstract": "Voicebots have provided a new avenue for supporting the development of\nlanguage skills, particularly within the context of second language learning.\nVoicebots, though, have largely been geared towards native adult speakers. We\nsought to assess the performance of two state-of-the-art ASR systems,\nWav2Vec2.0 and Whisper AI, with a view to developing a voicebot that can\nsupport children acquiring a foreign language. We evaluated their performance\non read and extemporaneous speech of native and non-native Dutch children. We\nalso investigated the utility of using ASR technology to provide insight into\nthe children's pronunciation and fluency. The results show that recent,\npre-trained ASR transformer-based models achieve acceptable performance from\nwhich detailed feedback on phoneme pronunciation quality can be extracted,\ndespite the challenging nature of child and non-native speech.", "published": "2023-06-29 06:14:26", "link": "http://arxiv.org/abs/2306.16710v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "CLIPAG: Towards Generator-Free Text-to-Image Generation", "abstract": "Perceptually Aligned Gradients (PAG) refer to an intriguing property observed\nin robust image classification models, wherein their input gradients align with\nhuman perception and pose semantic meanings. While this phenomenon has gained\nsignificant research attention, it was solely studied in the context of\nunimodal vision-only architectures. In this work, we extend the study of PAG to\nVision-Language architectures, which form the foundations for diverse\nimage-text tasks and applications. Through an adversarial robustification\nfinetuning of CLIP, we demonstrate that robust Vision-Language models exhibit\nPAG in contrast to their vanilla counterparts. This work reveals the merits of\nCLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we\nshow that seamlessly integrating CLIPAG in a \"plug-n-play\" manner leads to\nsubstantial improvements in vision-language generative applications.\nFurthermore, leveraging its PAG property, CLIPAG enables text-to-image\ngeneration without any generative model, which typically requires huge\ngenerators.", "published": "2023-06-29 09:35:53", "link": "http://arxiv.org/abs/2306.16805v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Tokenization and the Noiseless Channel", "abstract": "Subword tokenization is a key part of many NLP pipelines. However, little is\nknown about why some tokenizer and hyperparameter combinations lead to better\ndownstream model performance than others. We propose that good tokenizers lead\nto \\emph{efficient} channel usage, where the channel is the means by which some\ninput is conveyed to the model and efficiency can be quantified in\ninformation-theoretic terms as the ratio of the Shannon entropy to the maximum\npossible entropy of the token distribution. Yet, an optimal encoding according\nto Shannon entropy assigns extremely long codes to low-frequency tokens and\nvery short codes to high-frequency tokens. Defining efficiency in terms of\nR\\'enyi entropy, on the other hand, penalizes distributions with either very\nhigh or very low-frequency tokens. In machine translation, we find that across\nmultiple tokenizers, the R\\'enyi entropy with $\\alpha = 2.5$ has a very strong\ncorrelation with \\textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for\ncompressed length.", "published": "2023-06-29 10:32:09", "link": "http://arxiv.org/abs/2306.16842v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Leveraging Cross-Utterance Context For ASR Decoding", "abstract": "While external language models (LMs) are often incorporated into the decoding\nstage of automated speech recognition systems, these models usually operate\nwith limited context. Cross utterance information has been shown to be\nbeneficial during second pass re-scoring, however this limits the hypothesis\nspace based on the local information available to the first pass LM. In this\nwork, we investigate the incorporation of long-context transformer LMs for\ncross-utterance decoding of acoustic models via beam search, and compare\nagainst results from n-best rescoring. Results demonstrate that beam search\nallows for an improved use of cross-utterance context. When evaluating on the\nlong-format dataset AMI, results show a 0.7\\% and 0.3\\% absolute reduction on\ndev and test sets compared to the single-utterance setting, with improvements\nwhen including up to 500 tokens of prior context. Evaluations are also provided\nfor Tedlium-1 with less significant improvements of around 0.1\\% absolute.", "published": "2023-06-29 12:48:25", "link": "http://arxiv.org/abs/2306.16903v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predicting Music Hierarchies with a Graph-Based Neural Decoder", "abstract": "This paper describes a data-driven framework to parse musical sequences into\ndependency trees, which are hierarchical structures used in music cognition\nresearch and music analysis. The parsing involves two steps. First, the input\nsequence is passed through a transformer encoder to enrich it with contextual\ninformation. Then, a classifier filters the graph of all possible dependency\narcs to produce the dependency tree. One major benefit of this system is that\nit can be easily integrated into modern deep-learning pipelines. Moreover,\nsince it does not rely on any particular symbolic grammar, it can consider\nmultiple musical features simultaneously, make use of sequential context\ninformation, and produce partial results for noisy inputs. We test our approach\non two datasets of musical trees -- time-span trees of monophonic note\nsequences and harmonic trees of jazz chord sequences -- and show that our\napproach outperforms previous methods.", "published": "2023-06-29 13:59:18", "link": "http://arxiv.org/abs/2306.16955v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision\n  through Self-Supervised Discrete Speech Units", "abstract": "The goal of Automatic Voice Over (AVO) is to generate speech in sync with a\nsilent video given its text script. Recent AVO frameworks built upon\ntext-to-speech synthesis (TTS) have shown impressive results. However, the\ncurrent AVO learning objective of acoustic feature reconstruction brings in\nindirect supervision for inter-modal alignment learning, thus limiting the\nsynchronization performance and synthetic speech quality. To this end, we\npropose a novel AVO method leveraging the learning objective of self-supervised\ndiscrete speech unit prediction, which not only provides more direct\nsupervision for the alignment learning, but also alleviates the mismatch\nbetween the text-video context and acoustic features. Experimental results show\nthat our proposed method achieves remarkable lip-speech synchronization and\nhigh speech quality by outperforming baselines in both objective and subjective\nevaluations. Code and speech samples are publicly available.", "published": "2023-06-29 15:02:22", "link": "http://arxiv.org/abs/2306.17005v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The mapKurator System: A Complete Pipeline for Extracting and Linking\n  Text from Historical Maps", "abstract": "Scanned historical maps in libraries and archives are valuable repositories\nof geographic data that often do not exist elsewhere. Despite the potential of\nmachine learning tools like the Google Vision APIs for automatically\ntranscribing text from these maps into machine-readable formats, they do not\nwork well with large-sized images (e.g., high-resolution scanned documents),\ncannot infer the relation between the recognized text and other datasets, and\nare challenging to integrate with post-processing tools. This paper introduces\nthe mapKurator system, an end-to-end system integrating machine learning models\nwith a comprehensive data processing pipeline. mapKurator empowers automated\nextraction, post-processing, and linkage of text labels from large numbers of\nlarge-dimension historical map scans. The output data, comprising bounding\npolygons and recognized text, is in the standard GeoJSON format, making it\neasily modifiable within Geographic Information Systems (GIS). The proposed\nsystem allows users to quickly generate valuable data from large numbers of\nhistorical maps for in-depth analysis of the map content and, in turn,\nencourages map findability, accessibility, interoperability, and reusability\n(FAIR principles). We deployed the mapKurator system and enabled the processing\nof over 60,000 maps and over 100 million text/place names in the David Rumsey\nHistorical Map collection. We also demonstrated a seamless integration of\nmapKurator with a collaborative web platform to enable accessing automated\napproaches for extracting and linking text labels from historical map scans and\ncollective work to improve the results.", "published": "2023-06-29 16:05:40", "link": "http://arxiv.org/abs/2306.17059v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by\n  Whispering to ChatGPT", "abstract": "We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic\nlyrics transcription method achieving state-of-the-art performance on various\nlyrics transcription datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper, a weakly supervised\nrobust speech recognition model, and GPT-4, today's most performant chat-based\nlarge language model. In the proposed method, Whisper functions as the \"ear\" by\ntranscribing the audio, while GPT-4 serves as the \"brain,\" acting as an\nannotator with a strong performance for contextualized output selection and\ncorrection. Our experiments show that LyricWhiz significantly reduces Word\nError Rate compared to existing methods in English and can effectively\ntranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz to\ncreate the first publicly available, large-scale, multilingual lyrics\ntranscription dataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for noise level estimation and\nevaluation. We anticipate that our proposed method and dataset will advance the\ndevelopment of multilingual lyrics transcription, a challenging and emerging\ntask.", "published": "2023-06-29 17:01:51", "link": "http://arxiv.org/abs/2306.17103v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,\n  and Human Tutors", "abstract": "Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.", "published": "2023-06-29 17:57:40", "link": "http://arxiv.org/abs/2306.17156v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Learning Multilingual Expressive Speech Representation for Prosody\n  Prediction without Parallel Data", "abstract": "We propose a method for speech-to-speech emotionpreserving translation that\noperates at the level of discrete speech units. Our approach relies on the use\nof multilingual emotion embedding that can capture affective information in a\nlanguage-independent manner. We show that this embedding can be used to predict\nthe pitch and duration of speech units in a target language, allowing us to\nresynthesize the source speech signal with the same emotional content. We\nevaluate our approach to English and French speech signals and show that it\noutperforms a baseline method that does not use emotional information,\nincluding when the emotion embedding is extracted from a different language.\nEven if this preliminary study does not address directly the machine\ntranslation issue, our results demonstrate the effectiveness of our approach\nfor cross-lingual emotion preservation in the context of speech resynthesis.", "published": "2023-06-29 08:06:54", "link": "http://arxiv.org/abs/2306.17199v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Could Small Language Models Serve as Recommenders? Towards Data-centric\n  Cold-start Recommendations", "abstract": "Recommendation systems help users find matched items based on their previous\nbehaviors. Personalized recommendation becomes challenging in the absence of\nhistorical user-item interactions, a practical problem for startups known as\nthe system cold-start recommendation. While existing research addresses\ncold-start issues for either users or items, we still lack solutions for system\ncold-start scenarios. To tackle the problem, we propose PromptRec, a simple but\neffective approach based on in-context learning of language models, where we\ntransform the recommendation task into the sentiment analysis task on natural\nlanguage containing user and item profiles. However, this naive approach\nheavily relies on the strong in-context learning ability emerged from large\nlanguage models, which could suffer from significant latency for online\nrecommendations. To solve the challenge, we propose to enhance small language\nmodels for recommender systems with a data-centric pipeline, which consists of:\n(1) constructing a refined corpus for model pre-training; (2) constructing a\ndecomposed prompt template via prompt pre-training. They correspond to the\ndevelopment of training data and inference data, respectively. The pipeline is\nsupported by a theoretical framework that formalizes the connection between\nin-context recommendation and language modeling. To evaluate our approach, we\nintroduce a cold-start recommendation benchmark, and the results demonstrate\nthat the enhanced small language models can achieve comparable cold-start\nrecommendation performance to that of large models with only $17\\%$ of the\ninference time. To the best of our knowledge, this is the first study to tackle\nthe system cold-start recommendation problem. We believe our findings will\nprovide valuable insights for future works. The benchmark and implementations\nare available at https://github.com/JacksonWuxs/PromptRec.", "published": "2023-06-29 18:50:12", "link": "http://arxiv.org/abs/2306.17256v5", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Prediction of COVID-19 Patients' Emergency Room Revisit using\n  Multi-Source Transfer Learning", "abstract": "The coronavirus disease 2019 (COVID-19) has led to a global pandemic of\nsignificant severity. In addition to its high level of contagiousness, COVID-19\ncan have a heterogeneous clinical course, ranging from asymptomatic carriers to\nsevere and potentially life-threatening health complications. Many patients\nhave to revisit the emergency room (ER) within a short time after discharge,\nwhich significantly increases the workload for medical staff. Early\nidentification of such patients is crucial for helping physicians focus on\ntreating life-threatening cases. In this study, we obtained Electronic Health\nRecords (EHRs) of 3,210 encounters from 13 affiliated ERs within the University\nof Pittsburgh Medical Center between March 2020 and January 2021. We leveraged\na Natural Language Processing technique, ScispaCy, to extract clinical concepts\nand used the 1001 most frequent concepts to develop 7-day revisit models for\nCOVID-19 patients in ERs. The research data we collected from 13 ERs may have\ndistributional differences that could affect the model development. To address\nthis issue, we employed a classic deep transfer learning method called the\nDomain Adversarial Neural Network (DANN) and evaluated different modeling\nstrategies, including the Multi-DANN algorithm, the Single-DANN algorithm, and\nthree baseline methods. Results showed that the Multi-DANN models outperformed\nthe Single-DANN models and baseline models in predicting revisits of COVID-19\npatients to the ER within 7 days after discharge. Notably, the Multi-DANN\nstrategy effectively addressed the heterogeneity among multiple source domains\nand improved the adaptation of source data to the target domain. Moreover, the\nhigh performance of Multi-DANN models indicates that EHRs are informative for\ndeveloping a prediction model to identify COVID-19 patients who are very likely\nto revisit an ER within 7 days after discharge.", "published": "2023-06-29 18:51:42", "link": "http://arxiv.org/abs/2306.17257v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Seeing in Words: Learning to Classify through Language Bottlenecks", "abstract": "Neural networks for computer vision extract uninterpretable features despite\nachieving high accuracy on benchmarks. In contrast, humans can explain their\npredictions using succinct and intuitive descriptions. To incorporate\nexplainability into neural networks, we train a vision model whose feature\nrepresentations are text. We show that such a model can effectively classify\nImageNet images, and we discuss the challenges we encountered when training it.", "published": "2023-06-29 00:24:42", "link": "http://arxiv.org/abs/2307.00028v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Learning-based F0 Synthesis for Speaker Anonymization", "abstract": "Voice conversion for speaker anonymization is an emerging concept for privacy\nprotection. In a deep learning setting, this is achieved by extracting multiple\nfeatures from speech, altering the speaker identity, and waveform synthesis.\nHowever, many existing systems do not modify fundamental frequency (F0)\ntrajectories, which convey prosody information and can reveal speaker identity.\nMoreover, mismatch between F0 and other features can degrade speech quality and\nintelligibility. In this paper, we formally introduce a method that synthesizes\nF0 trajectories from other speech features and evaluate its reconstructional\ncapabilities. Then we test our approach within a speaker anonymization\nframework, comparing it to a baseline and a state-of-the-art F0 modification\nthat utilizes speaker information. The results show that our method improves\nboth speaker anonymity, measured by the equal error rate, and utility, measured\nby the word error rate.", "published": "2023-06-29 11:12:40", "link": "http://arxiv.org/abs/2306.16860v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Computationally-efficient and perceptually-motivated rendering of\n  diffuse reflections in room acoustics simulation", "abstract": "Geometrical acoustics is well suited for simulating room reverberation in\ninteractive real-time applications. While the image source model (ISM) is\nexceptionally fast, the restriction to specular reflections impacts its\nperceptual plausibility. To account for diffuse late reverberation, hybrid\napproaches have been proposed, e.g., using a feedback delay network (FDN) in\ncombination with the ISM. Here, a computationally-efficient, digital-filter\napproach is suggested to account for effects of non-specular reflections in the\nISM and to couple scattered sound into a diffuse reverberation model using a\nspatially rendered FDN. Depending on the scattering coefficient of a room\nboundary, energy of each image source is split into a specular and a scattered\npart which is added to the diffuse sound field. Temporal effects as observed\nfor an infinite ideal diffuse (Lambertian) reflector are simulated using\ncascaded all-pass filters. Effects of scattering and multiple (inter-)\nreflections caused by larger geometric disturbances at walls and by objects in\nthe room are accounted for in a highly simplified manner. Using a single\nparameter to quantify deviations from an empty shoebox room, each reflection is\ntemporally smeared using cascaded all-pass filters. The proposed method was\nperceptually evaluated against dummy head recordings of real rooms.", "published": "2023-06-29 05:39:38", "link": "http://arxiv.org/abs/2306.16696v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-based Age and Gender Prediction with Transformers", "abstract": "We report on the curation of several publicly available datasets for age and\ngender prediction. Furthermore, we present experiments to predict age and\ngender with models based on a pre-trained wav2vec 2.0. Depending on the\ndataset, we achieve an MAE between 7.1 years and 10.8 years for age, and at\nleast 91.1% ACC for gender (female, male, child). Compared to a modelling\napproach built on handcrafted features, our proposed system shows an\nimprovement of 9% UAR for age and 4% UAR for gender. To make our findings\nreproducible, we release the best performing model to the community as well as\nthe sample lists of the data splits.", "published": "2023-06-29 14:13:15", "link": "http://arxiv.org/abs/2306.16962v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluation of Virtual Acoustic Environments with Different Acoustic\n  Level of Detail", "abstract": "Virtual acoustic environments enable the creation and simulation of realistic\nand ecologically valid daily-life situations with applications in hearing\nresearch and audiology. Hereby, reverberant indoor environments play an\nimportant role. For real-time applications, simplifications in the room\nacoustics simulation are required, however, it remains unclear what acoustic\nlevel of detail (ALOD) is necessary to capture all perceptually relevant\neffects. This study investigates the effect of varying ALOD in the simulation\nof three different real environments, a living room with a coupled kitchen, a\npub, and an underground station. ALOD was varied by generating different\nnumbers of image sources for early reflections, or by excluding geometrical\nroom details specific for each environment. The simulations were perceptually\nevaluated using headphones in comparison to binaural room impulse responses\nmeasured with a dummy head in the corresponding real environments, and partly\nusing loudspeakers. The study assessed the perceived overall difference for a\npulse, and a speech token. Furthermore, plausibility and externalization were\nevaluated. The results show that a strong reduction in ALOD is possible while\nobtaining similar plausibility and externalization as with the dummy head\nrecordings. The number and accuracy of early reflections appear less relevant,\nprovided diffuse late reverberation is appropriately accounted for.", "published": "2023-06-29 15:07:13", "link": "http://arxiv.org/abs/2306.17012v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singing Voice Synthesis Using Differentiable LPC and\n  Glottal-Flow-Inspired Wavetables", "abstract": "This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for\nsinging voice synthesis (SVS) that exploits the physical characteristics of the\nhuman voice using differentiable digital signal processing. GOLF employs a\nglottal model as the harmonic source and IIR filters to simulate the vocal\ntract, resulting in an interpretable and efficient approach. We show it is\ncompetitive with state-of-the-art singing voice vocoders, requiring fewer\nsynthesis parameters and less memory to train, and runs an order of magnitude\nfaster for inference. Additionally, we demonstrate that GOLF can model the\nphase components of the human voice, which has immense potential for rendering\nand analysing singing voices in a differentiable manner. Our results highlight\nthe effectiveness of incorporating the physical properties of the human voice\nmechanism into SVS and underscore the advantages of signal-processing-based\napproaches, which offer greater interpretability and efficiency in synthesis.", "published": "2023-06-29 18:39:50", "link": "http://arxiv.org/abs/2306.17252v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modified Parametric Multichannel Wiener Filter \\\\for Low-latency\n  Enhancement of Speech Mixtures with Unknown Number of Speakers", "abstract": "This paper introduces a novel low-latency online beamforming (BF) algorithm,\nnamed Modified Parametric Multichannel Wiener Filter (Mod-PMWF), for enhancing\nspeech mixtures with unknown and varying number of speakers. Although\nconventional BFs such as linearly constrained minimum variance BF (LCMV BF) can\nenhance a speech mixture, they typically require such attributes of the speech\nmixture as the number of speakers and the acoustic transfer functions (ATFs)\nfrom the speakers to the microphones. When the mixture attributes are\nunavailable, estimating them by low-latency processing is challenging,\nhindering the application of the BFs to the problem. In this paper, we overcome\nthis problem by modifying a conventional Parametric Multichannel Wiener Filter\n(PMWF). The proposed Mod-PMWF can adaptively form a directivity pattern that\nenhances all the speakers in the mixture without explicitly estimating these\nattributes. Our experiments will show the proposed BF's effectiveness in\ninterference reduction ratios and subjective listening tests.", "published": "2023-06-29 21:56:25", "link": "http://arxiv.org/abs/2306.17317v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall\n  Classification", "abstract": "We present working notes on transfer learning with semi-supervised dataset\nannotation for the BirdCLEF 2023 competition, focused on identifying African\nbird species in recorded soundscapes. Our approach utilizes existing\noff-the-shelf models, BirdNET and MixIT, to address representation and labeling\nchallenges in the competition. We explore the embedding space learned by\nBirdNET and propose a process to derive an annotated dataset for supervised\nlearning. Our experiments involve various models and feature engineering\napproaches to maximize performance on the competition leaderboard. The results\ndemonstrate the effectiveness of our approach in classifying bird species and\nhighlight the potential of transfer learning and semi-supervised dataset\nannotation in similar tasks.", "published": "2023-06-29 07:56:27", "link": "http://arxiv.org/abs/2306.16760v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the relevance of acoustic measurements for creating realistic virtual\n  acoustic environments", "abstract": "Geometrical approaches for room acoustics simulation have the advantage of\nrequiring limited computational resources while still achieving a high\nperceptual plausibility. A common approach is using the image source model for\ndirect and early reflections in connection with further simplified models such\nas a feedback delay network for the diffuse reverberant tail. When recreating\nreal spaces as virtual acoustic environments using room acoustics simulation,\nthe perceptual relevance of individual parameters in the simulation is unclear.\nHere we investigate the importance of underlying acoustical measurements and\ntechnical evaluation methods to obtain high-quality room acoustics simulations\nin agreement with dummy-head recordings of a real space. We focus on the role\nof source directivity. The effect of including measured, modelled, and\nomnidirectional source directivity in room acoustics simulations was assessed\nin comparison to the measured reference. Technical evaluation strategies to\nverify and improve the accuracy of various elements in the simulation\nprocessing chain from source, the room properties, to the receiver are\npresented. Perceptual results from an ABX listening experiment with random\nspeech tokens are shown and compared with technical measures for a ranking of\nsimulation approaches.", "published": "2023-06-29 14:16:54", "link": "http://arxiv.org/abs/2306.16967v1", "categories": ["cs.SD", "eess.AS", "physics.med-ph"], "primary_category": "cs.SD"}
{"title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion\n  Models", "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its\npractical application in generating audio directly from silent videos,\nparticularly in video/film production. However, previous methods in V2A have\nlimited generation quality in terms of temporal synchronization and\naudio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio\nsynthesis method with a latent diffusion model (LDM) that generates\nhigh-quality audio with improved synchronization and audio-visual relevance. We\nadopt contrastive audio-visual pretraining (CAVP) to learn more temporally and\nsemantically aligned features, then train an LDM with CAVP-aligned visual\nfeatures on spectrogram latent space. The CAVP-aligned features enable LDM to\ncapture the subtler audio-visual correlation via a cross-attention module. We\nfurther significantly improve sample quality with `double guidance'. Diff-Foley\nachieves state-of-the-art V2A performance on current large scale V2A dataset.\nFurthermore, we demonstrate Diff-Foley practical applicability and\ngeneralization capabilities via downstream finetuning. Project Page: see\nhttps://diff-foley.github.io/", "published": "2023-06-29 12:39:58", "link": "http://arxiv.org/abs/2306.17203v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
