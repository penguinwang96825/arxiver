{"title": "Lattice CNNs for Matching Based Chinese Question Answering", "abstract": "Short text matching often faces the challenges that there are great word\nmismatch and expression diversity between the two texts, which would be further\naggravated in languages like Chinese where there is no natural space to segment\nwords explicitly. In this paper, we propose a novel lattice based CNN model\n(LCNs) to utilize multi-granularity information inherent in the word lattice\nwhile maintaining strong ability to deal with the introduced noisy information\nfor matching based question answering in Chinese. We conduct extensive\nexperiments on both document based question answering and knowledge based\nquestion answering tasks, and experimental results show that the LCNs models\ncan significantly outperform the state-of-the-art matching models and strong\nbaselines by taking advantages of better ability to distill rich but\ndiscriminative information from the word lattice input.", "published": "2019-02-25 04:46:52", "link": "http://arxiv.org/abs/1902.09087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Star-Transformer", "abstract": "Although Transformer has achieved great successes on many NLP tasks, its\nheavy structure with fully-connected attention connections leads to\ndependencies on large training data. In this paper, we present\nStar-Transformer, a lightweight alternative by careful sparsification. To\nreduce model complexity, we replace the fully-connected structure with a\nstar-shaped topology, in which every two non-adjacent nodes are connected\nthrough a shared relay node. Thus, complexity is reduced from quadratic to\nlinear, while preserving capacity to capture both local composition and\nlong-range dependency. The experiments on four tasks (22 datasets) show that\nStar-Transformer achieved significant improvements against the standard\nTransformer for the modestly sized datasets.", "published": "2019-02-25 07:07:38", "link": "http://arxiv.org/abs/1902.09113v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Multi-Domain Learning for Automatic Short Answer Grading", "abstract": "One of the fundamental challenges towards building any intelligent tutoring\nsystem is its ability to automatically grade short student answers. A typical\nautomatic short answer grading system (ASAG) grades student answers across\nmultiple domains (or subjects). Grading student answers requires building a\nsupervised machine learning model that evaluates the similarity of the student\nanswer with the reference answer(s). We observe that unlike typical textual\nsimilarity or entailment tasks, the notion of similarity is not universal here.\nOn one hand, para-phrasal constructs of the language can indicate similarity\nindependent of the domain. On the other hand, two words, or phrases, that are\nnot strict synonyms of each other, might mean the same in certain domains.\nBuilding on this observation, we propose JMD-ASAG, the first joint multidomain\ndeep learning architecture for automatic short answer grading that performs\ndomain adaptation by learning generic and domain-specific aspects from the\nlimited domain-wise training data. JMD-ASAG not only learns the domain-specific\ncharacteristics but also overcomes the dependence on a large corpus by learning\nthe generic characteristics from the task-specific data itself. On a\nlarge-scale industry dataset and a benchmarking dataset, we show that our model\nperforms significantly better than existing techniques which either learn\ndomain-specific models or adapt a generic similarity scoring model from a large\ncorpus. Further, on the benchmarking dataset, we report state-of-the-art\nresults against all existing non-neural and neural models.", "published": "2019-02-25 10:31:57", "link": "http://arxiv.org/abs/1902.09183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relation Extraction using Explicit Context Conditioning", "abstract": "Relation Extraction (RE) aims to label relations between groups of marked\nentities in raw text. Most current RE models learn context-aware\nrepresentations of the target entities that are then used to establish relation\nbetween them. This works well for intra-sentence RE and we call them\nfirst-order relations. However, this methodology can sometimes fail to capture\ncomplex and long dependencies. To address this, we hypothesize that at times\ntwo target entities can be explicitly connected via a context token. We refer\nto such indirect relations as second-order relations and describe an efficient\nimplementation for computing them. These second-order relation scores are then\ncombined with first-order relation scores. Our empirical results show that the\nproposed method leads to state-of-the-art performance over two biomedical\ndatasets.", "published": "2019-02-25 14:09:03", "link": "http://arxiv.org/abs/1902.09271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentional Encoder Network for Targeted Sentiment Classification", "abstract": "Targeted sentiment classification aims at determining the sentimental\ntendency towards specific targets. Most of the previous approaches model\ncontext and target words with RNN and attention. However, RNNs are difficult to\nparallelize and truncated backpropagation through time brings difficulty in\nremembering long-term patterns. To address this issue, this paper proposes an\nAttentional Encoder Network (AEN) which eschews recurrence and employs\nattention based encoders for the modeling between context and target. We raise\nthe label unreliability issue and introduce label smoothing regularization. We\nalso apply pre-trained BERT to this task and obtain new state-of-the-art\nresults. Experiments and analysis demonstrate the effectiveness and lightweight\nof our model.", "published": "2019-02-25 14:51:46", "link": "http://arxiv.org/abs/1902.09314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EAT: a simple and versatile semantic representation format for\n  multi-purpose NLP", "abstract": "Semantic representations are central in many NLP tasks that require\nhuman-interpretable data. The conjunctivist framework - primarily developed by\nPietroski (2005, 2018) - obtains expressive representations with only a few\nbasic semantic types and relations systematically linked to syntactic\npositions. While representational simplicity is crucial for computational\napplications, such findings have not yet had major influence on NLP. We present\nthe first generic semantic representation format for NLP directly based on\nthese insights. We name the format EAT due to its basis in the Event-, Agent-,\nand Theme arguments in Neo-Davidsonian logical forms. It builds on the idea\nthat similar tripartite argument relations are ubiquitous across categories,\nand can be constructed from grammatical structure without additional lexical\ninformation. We present a detailed exposition of EAT and how it relates to\nother prevalent formats used in prior work, such as Abstract Meaning\nRepresentation (AMR) and Minimal Recursion Semantics (MRS). EAT stands out in\ntwo respects: simplicity and versatility. Uniquely, EAT discards semantic\nmetapredicates, and instead represents semantic roles entirely via positional\nencoding. This is made possible by limiting the number of roles to only three;\na major decrease from the many dozens recognized in e.g. AMR and MRS. EAT's\nsimplicity makes it exceptionally versatile in application. First, we show that\ndrastically reducing semantic roles based on EAT benefits text generation from\nMRS in the test settings of Hajdik et al. (2019). Second, we implement the\nderivation of EAT from a syntactic parse, and apply this for parallel corpus\ngeneration between grammatical classes. Third, we train an encoder-decoder LSTM\nnetwork to map EAT to English. Finally, we use both the encoder-decoder network\nand a rule-based alternative to conduct grammatical transformation from\nEAT-input.", "published": "2019-02-25 15:49:10", "link": "http://arxiv.org/abs/1902.09381v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Machine Translation: A Method to Reduce Meaning Loss", "abstract": "A desideratum of high-quality translation systems is that they preserve\nmeaning, in the sense that two sentences with different meanings should not\ntranslate to one and the same sentence in another language. However,\nstate-of-the-art systems often fail in this regard, particularly in cases where\nthe source and target languages partition the \"meaning space\" in different\nways. For instance, \"I cut my finger.\" and \"I cut my finger off.\" describe\ndifferent states of the world but are translated to French (by both Fairseq and\nGoogle Translate) as \"Je me suis coupe le doigt.\", which is ambiguous as to\nwhether the finger is detached. More generally, translation systems are\ntypically many-to-one (non-injective) functions from source to target language,\nwhich in many cases results in important distinctions in meaning being lost in\ntranslation. Building on Bayesian models of informative utterance production,\nwe present a method to define a less ambiguous translation system in terms of\nan underlying pre-trained neural sequence-to-sequence model. This method\nincreases injectivity, resulting in greater preservation of meaning as measured\nby improvement in cycle-consistency, without impeding translation quality\n(measured by BLEU score).", "published": "2019-02-25 18:54:10", "link": "http://arxiv.org/abs/1902.09514v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been\nmuch research in identifying potentially offensive messages. However, previous\nwork on this topic did not consider the problem as a whole, but rather focused\non detecting very specific types of offensive content, e.g., hate speech,\ncyberbulling, or cyber-aggression. In contrast, here we target several\ndifferent kinds of offensive content. In particular, we model the task\nhierarchically, identifying the type and the target of offensive messages in\nsocial media. For this purpose, we complied the Offensive Language\nIdentification Dataset (OLID), a new dataset with tweets annotated for\noffensive content using a fine-grained three-layer annotation scheme, which we\nmake publicly available. We discuss the main similarities and differences\nbetween OLID and pre-existing datasets for hate speech identification,\naggression detection, and similar tasks. We further experiment with and we\ncompare the performance of different machine learning models on OLID.", "published": "2019-02-25 23:54:40", "link": "http://arxiv.org/abs/1902.09666v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Sequences via Learning to Collocate", "abstract": "Transfer learning aims to solve the data sparsity for a target domain by\napplying information of the source domain. Given a sequence (e.g. a natural\nlanguage sentence), the transfer learning, usually enabled by recurrent neural\nnetwork (RNN), represents the sequential information transfer. RNN uses a chain\nof repeating cells to model the sequence data. However, previous studies of\nneural network based transfer learning simply represents the whole sentence by\na single vector, which is unfeasible for seq2seq and sequence labeling.\nMeanwhile, such layer-wise transfer learning mechanisms lose the fine-grained\ncell-level information from the source domain.\n  In this paper, we proposed the aligned recurrent transfer, ART, to achieve\ncell-level information transfer. ART is under the pre-training framework. Each\ncell attentively accepts transferred information from a set of positions in the\nsource domain. Therefore, ART learns the cross-domain word collocations in a\nmore flexible way. We conducted extensive experiments on both sequence labeling\ntasks (POS tagging, NER) and sentence classification (sentiment analysis). ART\noutperforms the state-of-the-arts over all experiments.", "published": "2019-02-25 05:04:11", "link": "http://arxiv.org/abs/1902.09092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Relational Question Answering from Narratives: Machine Reading and\n  Reasoning in Simulated Worlds", "abstract": "Question Answering (QA), as a research field, has primarily focused on either\nknowledge bases (KBs) or free text as a source of knowledge. These two sources\nhave historically shaped the kinds of questions that are asked over these\nsources, and the methods developed to answer them. In this work, we look\ntowards a practical use-case of QA over user-instructed knowledge that uniquely\ncombines elements of both structured QA over knowledge bases, and unstructured\nQA over narrative, introducing the task of multi-relational QA over personal\nnarrative. As a first step towards this goal, we make three key contributions:\n(i) we generate and release TextWorldsQA, a set of five diverse datasets, where\neach dataset contains dynamic narrative that describes entities and relations\nin a simulated world, paired with variably compositional questions over that\nknowledge, (ii) we perform a thorough evaluation and analysis of several\nstate-of-the-art QA models and their variants at this task, and (iii) we\nrelease a lightweight Python-based framework we call TextWorlds for easily\ngenerating arbitrary additional worlds and narrative, with the goal of allowing\nthe community to create and share a growing collection of diverse worlds as a\ntest-bed for this task.", "published": "2019-02-25 05:04:26", "link": "http://arxiv.org/abs/1902.09093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pretraining-Based Natural Language Generation for Text Summarization", "abstract": "In this paper, we propose a novel pretraining-based encoder-decoder\nframework, which can generate the output sequence based on the input sequence\nin a two-stage manner. For the encoder of our model, we encode the input\nsequence into context representations using BERT. For the decoder, there are\ntwo stages in our model, in the first stage, we use a Transformer-based decoder\nto generate a draft output sequence. In the second stage, we mask each word of\nthe draft sequence and feed it to BERT, then by combining the input sequence\nand the draft representation generated by BERT, we use a Transformer-based\ndecoder to predict the refined word for each masked position. To the best of\nour knowledge, our approach is the first method which applies the BERT into\ntext generation tasks. As the first step in this direction, we evaluate our\nproposed method on the text summarization task. Experimental results show that\nour model achieves new state-of-the-art on both CNN/Daily Mail and New York\nTimes datasets.", "published": "2019-02-25 13:07:32", "link": "http://arxiv.org/abs/1902.09243v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts", "abstract": "This paper presents the formal release of MedMentions, a new manually\nannotated resource for the recognition of biomedical concepts. What\ndistinguishes MedMentions from other annotated biomedical corpora is its size\n(over 4,000 abstracts and over 350,000 linked mentions), as well as the size of\nthe concept ontology (over 3 million concepts from UMLS 2017) and its broad\ncoverage of biomedical disciplines. In addition to the full corpus, a\nsub-corpus of MedMentions is also presented, comprising annotations for a\nsubset of UMLS 2017 targeted towards document retrieval. To encourage research\nin Biomedical Named Entity Recognition and Linking, data splits for training\nand testing are included in the release, and a baseline model and its metrics\nfor entity linking are also described.", "published": "2019-02-25 17:53:20", "link": "http://arxiv.org/abs/1902.09476v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Alignment of Contextual Word Embeddings, with Applications\n  to Zero-shot Dependency Parsing", "abstract": "We introduce a novel method for multilingual transfer that utilizes deep\ncontextual embeddings, pretrained in an unsupervised fashion. While contextual\nembeddings have been shown to yield richer representations of meaning compared\nto their static counterparts, aligning them poses a challenge due to their\ndynamic nature. To this end, we construct context-independent variants of the\noriginal monolingual spaces and utilize their mapping to derive an alignment\nfor the context-dependent spaces. This mapping readily supports processing of a\ntarget language, improving transfer by context-aware embeddings. Our\nexperimental results demonstrate the effectiveness of this approach for\nzero-shot and few-shot learning of dependency parsing. Specifically, our method\nconsistently outperforms the previous state-of-the-art on 6 tested languages,\nyielding an improvement of 6.8 LAS points on average.", "published": "2019-02-25 18:14:11", "link": "http://arxiv.org/abs/1902.09492v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Robustness of Machine Translation with Synthetic Noise", "abstract": "Modern Machine Translation (MT) systems perform consistently well on clean,\nin-domain text. However most human generated text, particularly in the realm of\nsocial media, is full of typos, slang, dialect, idiolect and other noise which\ncan have a disastrous impact on the accuracy of output translation. In this\npaper we leverage the Machine Translation of Noisy Text (MTNT) dataset to\nenhance the robustness of MT systems by emulating naturally occurring noise in\notherwise clean data. Synthesizing noise in this manner we are ultimately able\nto make a vanilla MT system resilient to naturally occurring noise and\npartially mitigate loss in accuracy resulting therefrom.", "published": "2019-02-25 18:39:42", "link": "http://arxiv.org/abs/1902.09508v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Knowledge Bases in LSTMs for Improving Machine Reading", "abstract": "This paper focuses on how to take advantage of external knowledge bases (KBs)\nto improve recurrent neural networks for machine reading. Traditional methods\nthat exploit knowledge from KBs encode knowledge as discrete indicator\nfeatures. Not only do these features generalize poorly, but they require\ntask-specific feature engineering to achieve good performance. We propose\nKBLSTM, a novel neural model that leverages continuous representations of KBs\nto enhance the learning of recurrent neural networks for machine reading. To\neffectively integrate background knowledge with information from the currently\nprocessed text, our model employs an attention mechanism with a sentinel to\nadaptively decide whether to attend to background knowledge and which\ninformation from KBs is useful. Experimental results show that our model\nachieves accuracies that surpass the previous state-of-the-art results for both\nentity extraction and event extraction on the widely used ACE2005 dataset.", "published": "2019-02-25 05:04:00", "link": "http://arxiv.org/abs/1902.09091v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Neural Response Diversity with Frequency-Aware Cross-Entropy\n  Loss", "abstract": "Sequence-to-Sequence (Seq2Seq) models have achieved encouraging performance\non the dialogue response generation task. However, existing Seq2Seq-based\nresponse generation methods suffer from a low-diversity problem: they\nfrequently generate generic responses, which make the conversation less\ninteresting. In this paper, we address the low-diversity problem by\ninvestigating its connection with model over-confidence reflected in predicted\ndistributions. Specifically, we first analyze the influence of the commonly\nused Cross-Entropy (CE) loss function, and find that the CE loss function\nprefers high-frequency tokens, which results in low-diversity responses. We\nthen propose a Frequency-Aware Cross-Entropy (FACE) loss function that improves\nover the CE loss function by incorporating a weighting mechanism conditioned on\ntoken frequency. Extensive experiments on benchmark datasets show that the FACE\nloss function is able to substantially improve the diversity of existing\nstate-of-the-art Seq2Seq response generation methods, in terms of both\nautomatic and human evaluations.", "published": "2019-02-25 10:53:29", "link": "http://arxiv.org/abs/1902.09191v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Audio Caption: Listen and Tell", "abstract": "Increasing amount of research has shed light on machine perception of audio\nevents, most of which concerns detection and classification tasks. However,\nhuman-like perception of audio scenes involves not only detecting and\nclassifying audio sounds, but also summarizing the relationship between\ndifferent audio events. Comparable research such as image caption has been\nconducted, yet the audio field is still quite barren. This paper introduces a\nmanually-annotated dataset for audio caption. The purpose is to automatically\ngenerate natural sentences for audio scene description and to bridge the gap\nbetween machine perception of audio and image. The whole dataset is labelled in\nMandarin and we also include translated English annotations. A baseline\nencoder-decoder model is provided for both English and Mandarin. Similar BLEU\nscores are derived for both languages: our model can generate understandable\nand data-related captions based on the dataset.", "published": "2019-02-25 13:27:13", "link": "http://arxiv.org/abs/1902.09254v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cooperative Learning of Disjoint Syntax and Semantics", "abstract": "There has been considerable attention devoted to models that learn to jointly\ninfer an expression's syntactic structure and its semantics. Yet,\n\\citet{NangiaB18} has recently shown that the current best systems fail to\nlearn the correct parsing strategy on mathematical expressions generated from a\nsimple context-free grammar. In this work, we present a recursive model\ninspired by \\newcite{ChoiYL18} that reaches near perfect accuracy on this task.\nOur model is composed of two separated modules for syntax and semantics. They\nare cooperatively trained with standard continuous and discrete optimization\nschemes. Our model does not require any linguistic structure for supervision\nand its recursive nature allows for out-of-domain generalization with little\nloss in performance. Additionally, our approach performs competitively on\nseveral natural language tasks, such as Natural Language Inference or Sentiment\nAnalysis.", "published": "2019-02-25 15:56:34", "link": "http://arxiv.org/abs/1902.09393v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MUREL: Multimodal Relational Reasoning for Visual Question Answering", "abstract": "Multimodal attentional networks are currently state-of-the-art models for\nVisual Question Answering (VQA) tasks involving real images. Although attention\nallows to focus on the visual content relevant to the question, this simple\nmechanism is arguably insufficient to model complex reasoning features required\nfor VQA or other high-level tasks.\n  In this paper, we propose MuRel, a multimodal relational network which is\nlearned end-to-end to reason over real images. Our first contribution is the\nintroduction of the MuRel cell, an atomic reasoning primitive representing\ninteractions between question and image regions by a rich vectorial\nrepresentation, and modeling region relations with pairwise combinations.\nSecondly, we incorporate the cell into a full MuRel network, which\nprogressively refines visual and question interactions, and can be leveraged to\ndefine visualization schemes finer than mere attention maps.\n  We validate the relevance of our approach with various ablation studies, and\nshow its superiority to attention-based methods on three datasets: VQA 2.0,\nVQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms\nstate-of-the-art results in this challenging context.\n  Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch", "published": "2019-02-25 18:04:05", "link": "http://arxiv.org/abs/1902.09487v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional\n  Question Answering", "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and\ncompositional question answering, seeking to address key shortcomings of\nprevious VQA datasets. We have developed a strong and robust question engine\nthat leverages scene graph structures to create 22M diverse reasoning\nquestions, all come with functional programs that represent their semantics. We\nuse the programs to gain tight control over the answer distribution and present\na new tunable smoothing technique to mitigate question biases. Accompanying the\ndataset is a suite of new metrics that evaluate essential qualities such as\nconsistency, grounding and plausibility. An extensive analysis is performed for\nbaselines as well as state-of-the-art models, providing fine-grained results\nfor different question types and topologies. Whereas a blind LSTM obtains mere\n42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%,\noffering ample opportunity for new research to explore. We strongly hope GQA\nwill provide an enabling resource for the next generation of models with\nenhanced robustness, improved consistency, and deeper semantic understanding\nfor images and language.", "published": "2019-02-25 18:37:49", "link": "http://arxiv.org/abs/1902.09506v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Deep Object Features for Image Descriptions", "abstract": "Inspired by recent advances in leveraging multiple modalities in machine\ntranslation, we introduce an encoder-decoder pipeline that uses (1) specific\nobjects within an image and their object labels, (2) a language model for\ndecoding joint embedding of object features and the object labels. Our pipeline\nmerges prior detected objects from the image and their object labels and then\nlearns the sequences of captions describing the particular image. The decoder\nmodel learns to extract descriptions for the image from scratch by decoding the\njoint representation of the object visual features and their object classes\nconditioned by the encoder component. The idea of the model is to concentrate\nonly on the specific objects of the image and their labels for generating\ndescriptions of the image rather than visual feature of the entire image. The\nmodel needs to be calibrated more by adjusting the parameters and settings to\nresult in better accuracy and performance.", "published": "2019-02-25 18:40:25", "link": "http://arxiv.org/abs/1902.09969v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BUT-FIT at SemEval-2019 Task 7: Determining the Rumour Stance with\n  Pre-Trained Deep Bidirectional Transformers", "abstract": "This paper describes our system submitted to SemEval 2019 Task 7: RumourEval\n2019: Determining Rumour Veracity and Support for Rumours, Subtask A (Gorrell\net al., 2019). The challenge focused on classifying whether posts from Twitter\nand Reddit support, deny, query, or comment a hidden rumour, truthfulness of\nwhich is the topic of an underlying discussion thread. We formulate the problem\nas a stance classification, determining the rumour stance of a post with\nrespect to the previous thread post and the source thread post. The recent BERT\narchitecture was employed to build an end-to-end system which has reached the\nF1 score of 61.67% on the provided test data. It finished at the 2nd place in\nthe competition, without any hand-crafted features, only 0.2% behind the\nwinner.", "published": "2019-02-25 19:53:01", "link": "http://arxiv.org/abs/1902.10126v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Channel adversarial training for cross-channel text-independent speaker\n  recognition", "abstract": "The conventional speaker recognition frameworks (e.g., the i-vector and\nCNN-based approach) have been successfully applied to various tasks when the\nchannel of the enrolment dataset is similar to that of the test dataset.\nHowever, in real-world applications, mismatch always exists between these two\ndatasets, which may severely deteriorate the recognition performance.\nPreviously, a few channel compensation algorithms have been proposed, such as\nLinear Discriminant Analysis (LDA) and Probabilistic LDA. However, these\nmethods always require the collections of different channels from a specific\nspeaker, which is unrealistic to be satisfied in real scenarios. Inspired by\ndomain adaptation, we propose a novel deep-learning based speaker recognition\nframework to learn the channel-invariant and speaker-discriminative speech\nrepresentations via channel adversarial training. Specifically, we first employ\na gradient reversal layer to remove variations across different channels. Then,\nthe compressed information is projected into the same subspace by adversarial\ntraining. Experiments on test datasets with 54,133 speakers demonstrate that\nthe proposed method is not only effective at alleviating the channel mismatch\nproblem, but also outperforms state-of-the-art speaker recognition methods.\nCompared with the i-vector-based method and the CNN-based method, our proposed\nmethod achieves significant relative improvement of 44.7% and 22.6%\nrespectively in terms of the Top1 recall.", "published": "2019-02-25 03:32:58", "link": "http://arxiv.org/abs/1902.09074v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Detection and Compression for Passive Acoustic Monitoring of\n  the African Forest Elephant", "abstract": "In this work, we consider applying machine learning to the analysis and\ncompression of audio signals in the context of monitoring elephants in\nsub-Saharan Africa. Earth's biodiversity is increasingly under threat by\nsources of anthropogenic change (e.g. resource extraction, land use change, and\nclimate change) and surveying animal populations is critical for developing\nconservation strategies. However, manually monitoring tropical forests or deep\noceans is intractable. For species that communicate acoustically, researchers\nhave argued for placing audio recorders in the habitats as a cost-effective and\nnon-invasive method, a strategy known as passive acoustic monitoring (PAM). In\ncollaboration with conservation efforts, we construct a large labeled dataset\nof passive acoustic recordings of the African Forest Elephant via\ncrowdsourcing, compromising thousands of hours of recordings in the wild. Using\nstate-of-the-art techniques in artificial intelligence we improve upon\npreviously proposed methods for passive acoustic monitoring for classification\nand segmentation. In real-time detection of elephant calls, network bandwidth\nquickly becomes a bottleneck and efficient ways to compress the data are\nneeded. Most audio compression schemes are aimed at human listeners and are\nunsuitable for low-frequency elephant calls. To remedy this, we provide a novel\nend-to-end differentiable method for compression of audio signals that can be\nadapted to acoustic monitoring of any species and dramatically improves over\nnaive coding strategies.", "published": "2019-02-25 02:48:54", "link": "http://arxiv.org/abs/1902.09069v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Sound Source Localization considering Similarity of\n  Back-Propagation Signals", "abstract": "We present a novel, robust sound source localization algorithm considering\nback-propagation signals. Sound propagation paths are estimated by generating\ndirect and reflection acoustic rays based on ray tracing in a backward manner.\nWe then compute the back-propagation signals by designing and using the impulse\nresponse of the backward sound propagation based on the acoustic ray paths. For\nidentifying the 3D source position, we suggest a localization method based on\nthe Monte Carlo localization algorithm. Candidates for a source position is\ndetermined by identifying the convergence regions of acoustic ray paths. This\ncandidate is validated by measuring similarities between back-propagation\nsignals, under the assumption that the back-propagation signals of different\nacoustic ray paths should be similar near the sound source position. Thanks to\nconsidering similarities of back-propagation signals, our approach can localize\na source position with an averaged error of 0.51 m in a room of 7 m by 7 m area\nwith 3 m height in tested environments. We also observe 65 % to 220 %\nimprovement in accuracy over the stateof-the-art method. This improvement is\nachieved in environments containing a moving source, an obstacle, and noises.", "published": "2019-02-25 10:18:22", "link": "http://arxiv.org/abs/1902.09179v1", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
