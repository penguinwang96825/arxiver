{"title": "syrapropa at SemEval-2020 Task 11: BERT-based Models Design For\n  Propagandistic Technique and Span Detection", "abstract": "This paper describes the BERT-based models proposed for two subtasks in\nSemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles. We\nfirst build the model for Span Identification (SI) based on SpanBERT, and\nfacilitate the detection by a deeper model and a sentence-level representation.\nWe then develop a hybrid model for the Technique Classification (TC). The\nhybrid model is composed of three submodels including two BERT models with\ndifferent training methods, and a feature-based Logistic Regression model. We\nendeavor to deal with imbalanced dataset by adjusting cost function. We are in\nthe seventh place in SI subtask (0.4711 of F1-measure), and in the third place\nin TC subtask (0.6783 of F1-measure) on the development set.", "published": "2020-08-24 02:15:29", "link": "http://arxiv.org/abs/2008.10163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Semantic Role Labeling with Model Transfer", "abstract": "Prior studies show that cross-lingual semantic role labeling (SRL) can be\nachieved by model transfer under the help of universal features. In this paper,\nwe fill the gap of cross-lingual SRL by proposing an end-to-end SRL model that\nincorporates a variety of universal features and transfer methods. We study\nboth the bilingual transfer and multi-source transfer, under gold or\nmachine-generated syntactic inputs, pre-trained high-order abstract features,\nand contextualized multilingual word representations. Experimental results on\nthe Universal Proposition Bank corpus indicate that performances of the\ncross-lingual SRL can vary by leveraging different cross-lingual features. In\naddition, whether the features are gold-standard also has an impact on\nperformances. Precisely, we find that gold syntax features are much more\ncrucial for cross-lingual SRL, compared with the automatically-generated ones.\nMoreover, universal dependency structure features are able to give the best\nhelp, and both pre-trained high-order features and contextualized word\nrepresentations can further bring significant improvements.", "published": "2020-08-24 09:37:45", "link": "http://arxiv.org/abs/2008.10284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End to End Dialogue Transformer", "abstract": "Dialogue systems attempt to facilitate conversations between humans and\ncomputers, for purposes as diverse as small talk to booking a vacation. We are\nhere inspired by the performance of the recurrent neural network-based model\nSequicity, which when conducting a dialogue uses a sequence-to-sequence\narchitecture to first produce a textual representation of what is going on in\nthe dialogue, and in a further step use this along with database findings to\nproduce a reply to the user. We here propose a dialogue system based on the\nTransformer architecture instead of Sequicity's RNN-based architecture, that\nworks similarly in an end-to-end, sequence-to-sequence fashion.", "published": "2020-08-24 12:43:08", "link": "http://arxiv.org/abs/2008.10392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Baseline Analysis for Podcast Abstractive Summarization", "abstract": "Podcast summary, an important factor affecting end-users' listening\ndecisions, has often been considered a critical feature in podcast\nrecommendation systems, as well as many downstream applications. Existing\nabstractive summarization approaches are mainly built on fine-tuned models on\nprofessionally edited texts such as CNN and DailyMail news. Different from\nnews, podcasts are often longer, more colloquial and conversational, and\nnoisier with contents on commercials and sponsorship, which makes automatic\npodcast summarization extremely challenging. This paper presents a baseline\nanalysis of podcast summarization using the Spotify Podcast Dataset provided by\nTREC 2020. It aims to help researchers understand current state-of-the-art\npre-trained models and hence build a foundation for creating better models.", "published": "2020-08-24 18:38:42", "link": "http://arxiv.org/abs/2008.10648v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YNU-HPCC at SemEval-2020 Task 11: LSTM Network for Detection of\n  Propaganda Techniques in News Articles", "abstract": "This paper summarizes our studies on propaganda detection techniques for news\narticles in the SemEval-2020 task 11. This task is divided into the SI and TC\nsubtasks. We implemented the GloVe word representation, the BERT pretraining\nmodel, and the LSTM model architecture to accomplish this task. Our approach\nachieved good results for both the SI and TC subtasks. The macro-F1-score for\nthe SI subtask is 0.406, and the micro-F1-score for the TC subtask is 0.505.\nOur method significantly outperforms the officially released baseline method,\nand the SI and TC subtasks rank 17th and 22nd, respectively, for the test set.\nThis paper also compares the performances of different deep learning model\narchitectures, such as the Bi-LSTM, LSTM, BERT, and XGBoost models, on the\ndetection of news promotion techniques. The code of this paper is availabled\nat: https://github.com/daojiaxu/semeval_11.", "published": "2020-08-24 02:42:12", "link": "http://arxiv.org/abs/2008.10166v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for\n  Token-level Evaluation Metrics", "abstract": "Though generative dialogue modeling is widely seen as a language modeling\ntask, the task demands an agent to have a complex natural language\nunderstanding of its input text to carry a meaningful interaction with an user.\nThe automatic metrics used evaluate the quality of the generated text as a\nproxy to the holistic interaction of the agent. Such metrics were earlier shown\nto not correlate with the human judgement. In this work, we observe that human\nevaluation of dialogue agents can be inconclusive due to the lack of sufficient\ninformation for appropriate evaluation. The automatic metrics are deterministic\nyet shallow and human evaluation can be relevant yet inconclusive. To bridge\nthis gap in evaluation, we propose designing a set of probing tasks to evaluate\ndialogue models. The hand-crafted tasks are aimed at quantitatively evaluating\na generative dialogue model's understanding beyond the token-level evaluation\non the generated text. The probing tasks are deterministic like automatic\nmetrics and requires human judgement in their designing; benefiting from the\nbest of both worlds. With experiments on probe tasks we observe that, unlike\nRNN based architectures, transformer model may not be learning to comprehend\nthe input text despite its generated text having higher overlap with the target\ntext.", "published": "2020-08-24 13:28:35", "link": "http://arxiv.org/abs/2008.10427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prediction of ICD Codes with Clinical BERT Embeddings and Text\n  Augmentation with Label Balancing using MIMIC-III", "abstract": "This paper achieves state of the art results for the ICD code prediction task\nusing the MIMIC-III dataset. This was achieved through the use of Clinical BERT\n(Alsentzer et al., 2019). embeddings and text augmentation and label balancing\nto improve F1 scores for both ICD Chapter as well as ICD disease codes. We\nattribute the improved performance mainly to the use of novel text augmentation\nto shuffle the order of sentences during training. In comparison to the Top-32\nICD code prediction (Keyang Xu, et. al.) with an F1 score of 0.76, we achieve a\nfinal F1 score of 0.75 but on a total of the top 50 ICD codes.", "published": "2020-08-24 14:53:21", "link": "http://arxiv.org/abs/2008.10492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Semiotics", "abstract": "Recognizing a basic difference between the semiotics of humans and machines\npresents a possibility to overcome the shortcomings of current speech assistive\ndevices. For the machine, the meaning of a (human) utterance is defined by its\nown scope of actions. Machines, thus, do not need to understand the\nconventional meaning of an utterance. Rather, they draw conversational\nimplicatures in the sense of (neo-)Gricean pragmatics. For speech assistive\ndevices, the learning of machine-specific meanings of human utterances, i.e.\nthe fossilization of conversational implicatures into conventionalized ones by\ntrial and error through lexicalization appears to be sufficient. Using the\nquite trivial example of a cognitive heating device, we show that - based on\ndynamic semantics - this process can be formalized as the reinforcement\nlearning of utterance-meaning pairs (UMP).", "published": "2020-08-24 15:49:54", "link": "http://arxiv.org/abs/2008.10522v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Example-Based Named Entity Recognition", "abstract": "We present a novel approach to named entity recognition (NER) in the presence\nof scarce data that we call example-based NER. Our train-free few-shot learning\napproach takes inspiration from question-answering to identify entity spans in\na new and unseen domain. In comparison with the current state-of-the-art, the\nproposed method performs significantly better, especially when using a low\nnumber of support examples.", "published": "2020-08-24 17:18:24", "link": "http://arxiv.org/abs/2008.10570v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowledge-Empowered Representation Learning for Chinese Medical Reading\n  Comprehension: Task, Model and Resources", "abstract": "Machine Reading Comprehension (MRC) aims to extract answers to questions\ngiven a passage. It has been widely studied recently, especially in open\ndomains. However, few efforts have been made on closed-domain MRC, mainly due\nto the lack of large-scale training data. In this paper, we introduce a\nmulti-target MRC task for the medical domain, whose goal is to predict answers\nto medical questions and the corresponding support sentences from medical\ninformation sources simultaneously, in order to ensure the high reliability of\nmedical knowledge serving. A high-quality dataset is manually constructed for\nthe purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with\ndetailed analysis conducted. We further propose the Chinese medical BERT model\nfor the task (CMedBERT), which fuses medical knowledge into pre-trained\nlanguage models by the dynamic fusion mechanism of heterogeneous features and\nthe multi-task learning strategy. Experiments show that CMedBERT consistently\noutperforms strong baselines by fusing context-aware and knowledge-aware token\nrepresentations.", "published": "2020-08-24 11:23:28", "link": "http://arxiv.org/abs/2008.10327v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Table2Charts: Recommending Charts by Learning Shared Table\n  Representations", "abstract": "It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.", "published": "2020-08-24 15:06:26", "link": "http://arxiv.org/abs/2008.11015v4", "categories": ["cs.DB", "cs.CL", "cs.HC"], "primary_category": "cs.DB"}
{"title": "AMRConvNet: AMR-Coded Speech Enhancement Using Convolutional Neural\n  Networks", "abstract": "Speech is converted to digital signals using speech coding for efficient\ntransmission. However, this often lowers the quality and bandwidth of speech.\nThis paper explores the application of convolutional neural networks for\nArtificial Bandwidth Expansion (ABE) and speech enhancement on coded speech,\nparticularly Adaptive Multi-Rate (AMR) used in 2G cellular phone calls. In this\npaper, we introduce AMRConvNet: a convolutional neural network that performs\nABE and speech enhancement on speech encoded with AMR. The model operates\ndirectly on the time-domain for both input and output speech but optimizes\nusing combined time-domain reconstruction loss and frequency-domain perceptual\nloss. AMRConvNet resulted in an average improvement of 0.425 Mean Opinion Score\n- Listening Quality Objective (MOS-LQO) points for AMR bitrate of 4.75k, and\n0.073 MOS-LQO points for AMR bitrate of 12.2k. AMRConvNet also showed\nrobustness in AMR bitrate inputs. Finally, an ablation test showed that our\ncombined time-domain and frequency-domain loss leads to slightly higher MOS-LQO\nand faster training convergence than using either loss alone.", "published": "2020-08-24 07:24:43", "link": "http://arxiv.org/abs/2008.10233v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Computational Analysis of Real-World DJ Mixes using Mix-To-Track\n  Subsequence Alignment", "abstract": "A DJ mix is a sequence of music tracks concatenated seamlessly, typically\nrendered for audiences in a live setting by a DJ on stage. As a DJ mix is\nproduced in a studio or the live version is recorded for music streaming\nservices, computational methods to analyze DJ mixes, for example, extracting\ntrack information or understanding DJ techniques, have drawn research\ninterests. Many of previous works are, however, limited to identifying\nindividual tracks in a mix or segmenting it, and the sizes of the datasets are\nusually small. In this paper, we provide an in-depth analysis of DJ music by\naligning a mix to its original music tracks. We set up the subsequence\nalignment such that the audio features are less sensitive to the tempo or key\nchange of the original track in a mix. This approach provides temporally tight\nmix-to-track matching from which we can obtain cue-points, transition length,\nmix segmentation, and musical changes in DJ performance. Using 1,557 mixes from\n1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the\nproposed analysis and show a wide range of statistics, which may elucidate the\ncreative process of DJ music making.", "published": "2020-08-24 08:54:19", "link": "http://arxiv.org/abs/2008.10267v1", "categories": ["eess.AS", "cs.IR"], "primary_category": "eess.AS"}
{"title": "Improving Tail Performance of a Deliberation E2E ASR Model Using a Large\n  Text Corpus", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) systems lack the distinct\nlanguage model (LM) component that characterizes traditional speech systems.\nWhile this simplifies the model architecture, it complicates the task of\nincorporating text-only data into training, which is important to the\nrecognition of tail words that do not occur often in audio-text pairs. While\nshallow fusion has been proposed as a method for incorporating a pre-trained LM\ninto an E2E model at inference time, it has not yet been explored for very\nlarge text corpora, and it has been shown to be very sensitive to\nhyperparameter settings in the beam search. In this work, we apply shallow\nfusion to incorporate a very large text corpus into a state-of-the-art E2EASR\nmodel. We explore the impact of model size and show that intelligent pruning of\nthe training set can be more effective than increasing the parameter count.\nAdditionally, we show that incorporating the LM in minimum word error rate\n(MWER) fine tuning makes shallow fusion far less dependent on optimal\nhyperparameter settings, reducing the difficulty of that tuning problem.", "published": "2020-08-24 14:53:10", "link": "http://arxiv.org/abs/2008.10491v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "CRNNs for Urban Sound Tagging with spatiotemporal context", "abstract": "This paper describes CRNNs we used to participate in Task 5 of the DCASE 2020\nchallenge. This task focuses on hierarchical multilabel urban sound tagging\nwith spatiotemporal context. The code is available on our GitHub repository at\nhttps://github.com/multitel-ai/urban-sound-tagging.", "published": "2020-08-24 13:16:47", "link": "http://arxiv.org/abs/2008.10413v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
