{"title": "Cross-Subject Data Splitting for Brain-to-Text Decoding", "abstract": "Recent major milestones have successfully decoded non-invasive brain signals\n(e.g. functional Magnetic Resonance Imaging (fMRI) and electroencephalogram\n(EEG)) into natural language. Despite the progress in model design, how to\nsplit the datasets for training, validating, and testing still remains a matter\nof debate. Most of the prior researches applied subject-specific data\nsplitting, where the decoding model is trained and evaluated per subject. Such\nsplitting method poses challenges to the utilization efficiency of dataset as\nwell as the generalization of models. In this study, we propose a cross-subject\ndata splitting criterion for brain-to-text decoding on various types of\ncognitive dataset (fMRI, EEG), aiming to maximize dataset utilization and\nimprove model generalization. We undertake a comprehensive analysis on existing\ncross-subject data splitting strategies and prove that all these methods suffer\nfrom data leakage, namely the leakage of test data to training set, which\nsignificantly leads to overfitting and overestimation of decoding models. The\nproposed cross-subject splitting method successfully addresses the data leakage\nproblem and we re-evaluate some SOTA brain-to-text decoding models as baselines\nfor further research.", "published": "2023-12-18 07:22:39", "link": "http://arxiv.org/abs/2312.10987v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VinaLLaMA: LLaMA-based Vietnamese Foundation Model", "abstract": "In this technical report, we present VinaLLaMA, an open-weight,\nstate-of-the-art (SOTA) Large Language Model for the Vietnamese language, built\nupon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only\ndemonstrates fluency in Vietnamese but also exhibits a profound understanding\nof Vietnamese culture, making it a truly indigenous model. VinaLLaMA-7B-chat,\ntrained on 1 million high-quality synthetic samples, achieves SOTA results on\nkey benchmarks, including VLSP, VMLU, and Vicuna Benchmark Vietnamese, marking\na significant advancement in the Vietnamese AI landscape and offering a\nversatile resource for various applications.", "published": "2023-12-18 08:27:33", "link": "http://arxiv.org/abs/2312.11011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Type Classification with Contrastive Task-Specialized\n  Sentence Encoders", "abstract": "User-generated information content has become an important information source\nin crisis situations. However, classification models suffer from noise and\nevent-related biases which still poses a challenging task and requires\nsophisticated task-adaptation. To address these challenges, we propose the use\nof contrastive task-specialized sentence encoders for downstream\nclassification. We apply the task-specialization on the CrisisLex, HumAID, and\nTrecIS information type classification tasks and show performance gains w.r.t.\nF1-score. Furthermore, we analyse the cross-corpus and cross-lingual\ncapabilities for two German event relevancy classification datasets.", "published": "2023-12-18 08:45:39", "link": "http://arxiv.org/abs/2312.11020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction", "abstract": "Relation extraction is essentially a text classification problem, which can\nbe tackled by fine-tuning a pre-trained language model (LM). However, a key\nchallenge arises from the fact that relation extraction cannot\nstraightforwardly be reduced to sequence or token classification. Existing\napproaches therefore solve the problem in an indirect way: they fine-tune an LM\nto learn embeddings of the head and tail entities, and then predict the\nrelationship from these entity embeddings. Our hypothesis in this paper is that\nrelation extraction models can be improved by capturing relationships in a more\ndirect way. In particular, we experiment with appending a prompt with a [MASK]\ntoken, whose contextualised representation is treated as a relation embedding.\nWhile, on its own, this strategy significantly underperforms the aforementioned\napproach, we find that the resulting relation embeddings are highly\ncomplementary to what is captured by embeddings of the head and tail entity. By\njointly considering both types of representations, we end up with a simple\nmodel that outperforms the state-of-the-art across several relation extraction\nbenchmarks.", "published": "2023-12-18 09:58:19", "link": "http://arxiv.org/abs/2312.11062v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Patterns of Closeness and Abstractness in Colexifications: The Case of\n  Indigenous Languages in the Americas", "abstract": "Colexification refers to linguistic phenomena where multiple concepts\n(meanings) are expressed by the same lexical form, such as polysemy or\nhomophony. Colexifications have been found to be pervasive across languages and\ncultures. The problem of concreteness/abstractness of concepts is\ninterdisciplinary, studied from a cognitive standpoint in linguistics,\npsychology, psycholinguistics, neurophysiology, etc. In this paper, we\nhypothesize that concepts that are closer in concreteness/abstractness are more\nlikey to colexify, and we test the hypothesis across indigenous languages in\nAmericas.", "published": "2023-12-18 10:06:50", "link": "http://arxiv.org/abs/2312.11069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Split and Rephrase with Large Language Models", "abstract": "The Split and Rephrase (SPRP) task, which consists in splitting complex\nsentences into a sequence of shorter grammatical sentences, while preserving\nthe original meaning, can facilitate the processing of complex texts for humans\nand machines alike. It is also a valuable testbed to evaluate natural language\nprocessing models, as it requires modelling complex grammatical aspects. In\nthis work, we evaluate large language models on the task, showing that they can\nprovide large improvements over the state of the art on the main metrics,\nalthough still lagging in terms of splitting compliance. Results from two human\nevaluations further support the conclusions drawn from automated metric\nresults. We provide a comprehensive study that includes prompting variants,\ndomain shift, fine-tuned pretrained language models of varying parameter size\nand training data volumes, contrasted with both zero-shot and few-shot\napproaches on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they may constitute a reasonable\noff-the-shelf alternative. Our results provide a fine-grained analysis of the\npotential and limitations of large language models for SPRP, with significant\nimprovements achievable using relatively small amounts of training data and\nmodel parameters overall, and remaining limitations for all models on the task.", "published": "2023-12-18 10:16:37", "link": "http://arxiv.org/abs/2312.11075v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linear Attention via Orthogonal Memory", "abstract": "Efficient attentions have greatly improved the computational efficiency of\nTransformers. However, most existing linear attention mechanisms suffer from an\n\\emph{efficiency degradation} problem, leading to inefficiencies in causal\nlanguage modeling and hindering their application in long-range language\nmodels. This problem is more pronounced under language modeling with unbounded\ncontexts. In this paper, we propose \\textbf{L}inear \\textbf{A}ttention\n\\textbf{V}ia \\textbf{O}rthogonal memory~(\\shortname) to address these\nlimitations, achieving strong performance while maintaining linear complexity.\n\\shortname employs orthogonal decomposition to compress a context into a\nfixed-size orthogonal memory while effectively minimizing redundancy within the\ncontext. Given that orthogonal memory compresses global information, we further\ndissect the context to amplify fine-grained local information. Additionally, we\nembed the relative position encoding into \\shortname to improve the\nextrapolation ability. Experimental results show that \\shortname greatly\nimproves the efficiency of the causal language model with the best\nextrapolation performance and outperforms other efficient baselines. Further,\nwe endeavor to employ \\shortname for unbounded language modeling and\nsuccessfully scale the context length to 128K.", "published": "2023-12-18 12:26:27", "link": "http://arxiv.org/abs/2312.11135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiency-oriented approaches for self-supervised speech representation\n  learning", "abstract": "Self-supervised learning enables the training of large neural models without\nthe need for large, labeled datasets. It has been generating breakthroughs in\nseveral fields, including computer vision, natural language processing,\nbiology, and speech. In particular, the state-of-the-art in several speech\nprocessing applications, such as automatic speech recognition or speaker\nidentification, are models where the latent representation is learned using\nself-supervised approaches. Several configurations exist in self-supervised\nlearning for speech, including contrastive, predictive, and multilingual\napproaches. There is, however, a crucial limitation in most existing\napproaches: their high computational costs. These costs limit the deployment of\nmodels, the size of the training dataset, and the number of research groups\nthat can afford research with large self-supervised models. Likewise, we should\nconsider the environmental costs that high energy consumption implies. Efforts\nin this direction comprise optimization of existing models, neural architecture\nefficiency, improvements in finetuning for speech processing tasks, and data\nefficiency. But despite current efforts, more work could be done to address\nhigh computational costs in self-supervised representation learning.", "published": "2023-12-18 12:32:42", "link": "http://arxiv.org/abs/2312.11142v1", "categories": ["cs.CL", "A.1"], "primary_category": "cs.CL"}
{"title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL", "abstract": "Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on \"huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https://github.com/wbbeyourself/MAC-SQL).", "published": "2023-12-18 14:40:20", "link": "http://arxiv.org/abs/2312.11242v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling continuous and discrete linguistic signals in\n  transformer-based sentence embeddings", "abstract": "Sentence and word embeddings encode structural and semantic information in a\ndistributed manner. Part of the information encoded -- particularly lexical\ninformation -- can be seen as continuous, whereas other -- like structural\ninformation -- is most often discrete. We explore whether we can compress\ntransformer-based sentence embeddings into a representation that separates\ndifferent linguistic signals -- in particular, information relevant to\nsubject-verb agreement and verb alternations. We show that by compressing an\ninput sequence that shares a targeted phenomenon into the latent layer of a\nvariational autoencoder-like system, the targeted linguistic information\nbecomes more explicit. A latent layer with both discrete and continuous\ncomponents captures better the targeted phenomena than a latent layer with only\ndiscrete or only continuous components. These experiments are a step towards\nseparating linguistic signals from distributed text embeddings and linking them\nto more symbolic representations.", "published": "2023-12-18 15:16:54", "link": "http://arxiv.org/abs/2312.11272v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization for Multi-label Text Classification: A\n  Data-Augmentation Approach", "abstract": "Despite significant advancements in multi-label text classification, the\nability of existing models to generalize to novel and seldom-encountered\ncomplex concepts, which are compositions of elementary ones, remains\nunderexplored. This research addresses this gap. By creating unique data splits\nacross three benchmarks, we assess the compositional generalization ability of\nexisting multi-label text classification models. Our results show that these\nmodels often fail to generalize to compositional concepts encountered\ninfrequently during training, leading to inferior performance on tests with\nthese new combinations. To address this, we introduce a data augmentation\nmethod that leverages two innovative text generation models designed to enhance\nthe classification models' capacity for compositional generalization. Our\nexperiments show that this data augmentation approach significantly improves\nthe compositional generalization capabilities of classification models on our\nbenchmarks, with both generation models surpassing other text generation\nbaselines.", "published": "2023-12-18 15:18:57", "link": "http://arxiv.org/abs/2312.11276v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "APE-then-QE: Correcting then Filtering Pseudo Parallel Corpora for MT\n  Training Data Creation", "abstract": "Automatic Post-Editing (APE) is the task of automatically identifying and\ncorrecting errors in the Machine Translation (MT) outputs. We propose a\nrepair-filter-use methodology that uses an APE system to correct errors on the\ntarget side of the MT training data. We select the sentence pairs from the\noriginal and corrected sentence pairs based on the quality scores computed\nusing a Quality Estimation (QE) model. To the best of our knowledge, this is a\nnovel adaptation of APE and QE to extract quality parallel corpus from the\npseudo-parallel corpus. By training with this filtered corpus, we observe an\nimprovement in the Machine Translation system's performance by 5.64 and 9.91\nBLEU points, for English-Marathi and Marathi-English, over the baseline model.\nThe baseline model is the one that is trained on the whole pseudo-parallel\ncorpus. Our work is not limited by the characteristics of English or Marathi\nlanguages; and is language pair-agnostic, given the necessary QE and APE data.", "published": "2023-12-18 16:06:18", "link": "http://arxiv.org/abs/2312.11312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model", "abstract": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.", "published": "2023-12-18 17:36:20", "link": "http://arxiv.org/abs/2312.11370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Signals: An NLP Library for Text and Time Series", "abstract": "We present an open-source Python library for building and using datasets\nwhere inputs are clusters of textual data, and outputs are sequences of real\nvalues representing one or more time series signals. The news-signals library\nsupports diverse data science and NLP problem settings related to the\nprediction of time series behaviour using textual data feeds. For example, in\nthe news domain, inputs are document clusters corresponding to daily news\narticles about a particular entity, and targets are explicitly associated\nreal-valued time series: the volume of news about a particular person or\ncompany, or the number of pageviews of specific Wikimedia pages. Despite many\nindustry and research use cases for this class of problem settings, to the best\nof our knowledge, News Signals is the only open-source library designed\nspecifically to facilitate data science and research settings with natural\nlanguage inputs and time series targets. In addition to the core codebase for\nbuilding and interacting with datasets, we also conduct a suite of experiments\nusing several popular Machine Learning libraries, which are used to establish\nbaselines for time series anomaly prediction using textual inputs.", "published": "2023-12-18 18:02:41", "link": "http://arxiv.org/abs/2312.11399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Syntax Mapping: A New Approach to Unsupervised Syntax Parsing", "abstract": "The intricate hierarchical structure of syntax is fundamental to the\nintricate and systematic nature of human language. This study investigates the\npremise that language models, specifically their attention distributions, can\nencapsulate syntactic dependencies. We introduce Dynamic Syntax Mapping (DSM),\nan innovative approach for the agnostic induction of these structures. Our\nmethod diverges from traditional syntax models which rely on predefined\nannotation schemata. Instead, we focus on a core characteristic inherent in\ndependency relations: syntactic substitutability. This concept refers to the\ninterchangeability of words within the same syntactic category at either end of\na dependency. By leveraging this property, we generate a collection of\nsyntactically invariant sentences, which serve as the foundation for our\nparsing framework. Our findings reveal that the use of an increasing array of\nsubstitutions notably enhances parsing precision on natural language data.\nSpecifically, in the context of long-distance subject-verb agreement, DSM\nexhibits a remarkable advancement over prior methodologies. Furthermore, DSM's\nadaptability is demonstrated through its successful application in varied\nparsing scenarios, underscoring its broad applicability.", "published": "2023-12-18 10:34:29", "link": "http://arxiv.org/abs/2312.14966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Based Sentiment Analysis with Explicit Sentiment Augmentations", "abstract": "Aspect-based sentiment analysis (ABSA), a fine-grained sentiment\nclassification task, has received much attention recently. Many works\ninvestigate sentiment information through opinion words, such as ''good'' and\n''bad''. However, implicit sentiment widely exists in the ABSA dataset, which\nrefers to the sentence containing no distinct opinion words but still expresses\nsentiment to the aspect term. To deal with implicit sentiment, this paper\nproposes an ABSA method that integrates explicit sentiment augmentations. And\nwe propose an ABSA-specific augmentation method to create such augmentations.\nSpecifically, we post-trains T5 by rule-based data. We employ Syntax Distance\nWeighting and Unlikelihood Contrastive Regularization in the training procedure\nto guide the model to generate an explicit sentiment. Meanwhile, we utilize the\nConstrained Beam Search to ensure the augmentation sentence contains the aspect\nterms. We test ABSA-ESA on two of the most popular benchmarks of ABSA. The\nresults show that ABSA-ESA outperforms the SOTA baselines on implicit and\nexplicit sentiment accuracy.", "published": "2023-12-18 06:31:13", "link": "http://arxiv.org/abs/2312.10961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.", "published": "2023-12-18 07:47:33", "link": "http://arxiv.org/abs/2312.10997v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TDeLTA: A Light-weight and Robust Table Detection Method based on\n  Learning Text Arrangement", "abstract": "The diversity of tables makes table detection a great challenge, leading to\nexisting models becoming more tedious and complex. Despite achieving high\nperformance, they often overfit to the table style in training set, and suffer\nfrom significant performance degradation when encountering out-of-distribution\ntables in other domains. To tackle this problem, we start from the essence of\nthe table, which is a set of text arranged in rows and columns. Based on this,\nwe propose a novel, light-weighted and robust Table Detection method based on\nLearning Text Arrangement, namely TDeLTA. TDeLTA takes the text blocks as\ninput, and then models the arrangement of them with a sequential encoder and an\nattention module. To locate the tables precisely, we design a\ntext-classification task, classifying the text blocks into 4 categories\naccording to their semantic roles in the tables. Experiments are conducted on\nboth the text blocks parsed from PDF and extracted by open-source OCR tools,\nrespectively. Compared to several state-of-the-art methods, TDeLTA achieves\ncompetitive results with only 3.1M model parameters on the large-scale public\ndatasets. Moreover, when faced with the cross-domain data under the 0-shot\nsetting, TDeLTA outperforms baselines by a large margin of nearly 7%, which\nshows the strong robustness and transferability of the proposed model.", "published": "2023-12-18 09:18:43", "link": "http://arxiv.org/abs/2312.11043v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt Based Tri-Channel Graph Convolution Neural Network for Aspect\n  Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is an emerging task to extract a\ngiven sentence's triplets, which consist of aspects, opinions, and sentiments.\nRecent studies tend to address this task with a table-filling paradigm, wherein\nword relations are encoded in a two-dimensional table, and the process involves\nclarifying all the individual cells to extract triples. However, these studies\nignore the deep interaction between neighbor cells, which we find quite helpful\nfor accurate extraction. To this end, we propose a novel model for the ASTE\ntask, called Prompt-based Tri-Channel Graph Convolution Neural Network\n(PT-GCN), which converts the relation table into a graph to explore more\ncomprehensive relational information. Specifically, we treat the original table\ncells as nodes and utilize a prompt attention score computation module to\ndetermine the edges' weights. This enables us to construct a target-aware\ngrid-like graph to enhance the overall extraction process. After that, a\ntriple-channel convolution module is conducted to extract precise sentiment\nknowledge. Extensive experiments on the benchmark datasets show that our model\nachieves state-of-the-art performance. The code is available at\nhttps://github.com/KunPunCN/PT-GCN.", "published": "2023-12-18 12:46:09", "link": "http://arxiv.org/abs/2312.11152v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training With \"Paraphrasing the Original Text\" Teaches LLM to Better\n  Retrieve in Long-context Tasks", "abstract": "As Large Language Models (LLMs) continue to evolve, more are being designed\nto handle long-context inputs. Despite this advancement, most of them still\nface challenges in accurately handling long-context tasks, often showing the\n\"lost in the middle\" issue. We identify that insufficient retrieval capability\nis one of the important reasons for this issue. To tackle this challenge, we\npropose a novel approach to design training data for long-context tasks, aiming\nat augmenting LLMs' proficiency in extracting key information from long\ncontext. Specially, we incorporate an additional part named \"paraphrasing the\noriginal text\" when constructing the answer of training samples and then\nfine-tuning the model. Experimenting on LongBench and NaturalQuestions\nMulti-document-QA dataset with models of Llama and Qwen series, our method\nachieves an improvement of up to 8.48% and 4.48% in average scores,\nrespectively, showing effectiveness in improving the model's performance on\nlong-context tasks.", "published": "2023-12-18 13:40:16", "link": "http://arxiv.org/abs/2312.11193v10", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating and Enhancing Large Language Models for Conversational\n  Reasoning on Knowledge Graphs", "abstract": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access.", "published": "2023-12-18 15:23:06", "link": "http://arxiv.org/abs/2312.11282v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Generalized Laughter to Personalized Chuckles: Unleashing the Power\n  of Data Fusion in Subjective Humor Detection", "abstract": "The vast area of subjectivity in Natural Language Processing (NLP) poses a\nchallenge to the solutions typically used in generalized tasks. As exploration\nin the scope of generalized NLP is much more advanced, it implies the\ntremendous gap that is still to be addressed amongst all feasible tasks where\nan opinion, taste, or feelings are inherent, thus creating a need for a\nsolution, where a data fusion could take place. We have chosen the task of\nfunniness, as it heavily relies on the sense of humor, which is fundamentally\nsubjective. Our experiments across five personalized and four generalized\ndatasets involving several personalized deep neural architectures have shown\nthat the task of humor detection greatly benefits from the inclusion of\npersonalized data in the training process. We tested five scenarios of training\ndata fusion that focused on either generalized (majority voting) or\npersonalized approaches to humor detection. The best results were obtained for\nthe setup, in which all available personalized datasets were joined to train\nthe personalized reasoning model. It boosted the prediction performance by up\nto approximately 35% of the macro F1 score. Such a significant gain was\nobserved for all five personalized test sets. At the same time, the impact of\nthe model's architecture was much less than the personalization itself. It\nseems that concatenating personalized datasets, even with the cost of\nnormalizing the range of annotations across all datasets, if combined with the\npersonalized models, results in an enormous increase in the performance of\nhumor detection.", "published": "2023-12-18 15:45:39", "link": "http://arxiv.org/abs/2312.11296v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Implicit Affordance Acquisition via Causal Action-Effect Modeling in the\n  Video Domain", "abstract": "Affordance knowledge is a fundamental aspect of commonsense knowledge. Recent\nfindings indicate that world knowledge emerges through large-scale\nself-supervised pretraining, motivating our exploration of acquiring affordance\nknowledge from the visual domain. To this end, we augment an existing\ninstructional video resource to create the new Causal Action-Effect (CAE)\ndataset and design two novel pretraining tasks -- Masked Action Modeling (MAM)\nand Masked Effect Modeling (MEM) -- promoting the acquisition of two affordance\nproperties in models: behavior and entity equivalence, respectively. We\nempirically demonstrate the effectiveness of our proposed methods in learning\naffordance properties. Furthermore, we show that a model pretrained on both\ntasks outperforms a strong image-based visual-linguistic foundation model\n(FLAVA) as well as pure linguistic models on a zero-shot physical reasoning\nprobing task.", "published": "2023-12-18 16:51:26", "link": "http://arxiv.org/abs/2312.11345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"Knowing When You Don't Know\": A Multilingual Relevance Assessment\n  Dataset for Robust Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior work lacks a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages judged as\nnon-relevant, whereas queries in the relevant subset include at least a single\njudged relevant passage. We measure relevance assessment using: (i)\nhallucination rate, measuring model tendency to hallucinate, when the answer is\nnot present in passages in the non-relevant subset, and (ii) error rate,\nmeasuring model inaccuracy to recognize relevant passages in the relevant\nsubset.In our work, we observe that most models struggle to balance the two\ncapacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination\nrate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can\nachieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is\nobserved to provide the best tradeoff on both subsets, highlighting future work\nnecessary to improve LLM robustness. NoMIRACL dataset and evaluation code are\navailable at: https://github.com/project-miracl/nomiracl.", "published": "2023-12-18 17:18:04", "link": "http://arxiv.org/abs/2312.11361v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Verb Categorisation for Hindi Word Problem Solving", "abstract": "Word problem Solving is a challenging NLP task that deals with solving\nmathematical problems described in natural language. Recently, there has been\nrenewed interest in developing word problem solvers for Indian languages. As\npart of this paper, we have built a Hindi arithmetic word problem solver which\nmakes use of verbs. Additionally, we have created verb categorization data for\nHindi. Verbs are very important for solving word problems with\naddition/subtraction operations as they help us identify the set of operations\nrequired to solve the word problems. We propose a rule-based solver that uses\nverb categorisation to identify operations in a word problem and generate\nanswers for it. To perform verb categorisation, we explore several approaches\nand present a comparative study.", "published": "2023-12-18 17:55:05", "link": "http://arxiv.org/abs/2312.11395v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Social Learning: Towards Collaborative Learning with Large Language\n  Models", "abstract": "We introduce the framework of \"social learning\" in the context of large\nlanguage models (LLMs), whereby models share knowledge with each other in a\nprivacy-aware manner using natural language. We present and evaluate two\napproaches for knowledge transfer between LLMs. In the first scenario, we allow\nthe model to generate abstract prompts aiming to teach the task. In our second\napproach, models transfer knowledge by generating synthetic examples. We\nevaluate these methods across diverse datasets and quantify memorization as a\nproxy for privacy loss. These techniques inspired by social learning yield\npromising results with low memorization of the original data. In particular, we\nshow that performance using these methods is comparable to results with the use\nof original labels and prompts. Our work demonstrates the viability of social\nlearning for LLMs, establishes baseline approaches and highlights several\nunexplored areas for future work.", "published": "2023-12-18 18:44:10", "link": "http://arxiv.org/abs/2312.11441v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An In-depth Look at Gemini's Language Abilities", "abstract": "The recently released Google Gemini class of models are the first to\ncomprehensively report results that rival the OpenAI GPT series across a wide\nvariety of tasks. In this paper, we do an in-depth exploration of Gemini's\nlanguage abilities, making two contributions. First, we provide a third-party,\nobjective comparison of the abilities of the OpenAI GPT and Google Gemini\nmodels with reproducible code and fully transparent results. Second, we take a\ncloser look at the results, identifying areas where one of the two model\nclasses excels. We perform this analysis over 10 datasets testing a variety of\nlanguage abilities, including reasoning, answering knowledge-based questions,\nsolving math problems, translating between languages, generating code, and\nacting as instruction-following agents. From this analysis, we find that Gemini\nPro achieves accuracy that is close but slightly inferior to the corresponding\nGPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations\nfor some of this under-performance, including failures in mathematical\nreasoning with many digits, sensitivity to multiple-choice answer ordering,\naggressive content filtering, and others. We also identify areas where Gemini\ndemonstrates comparably high performance, including generation into non-English\nlanguages, and handling longer and more complex reasoning chains. Code and data\nfor reproduction can be found at https://github.com/neulab/gemini-benchmark", "published": "2023-12-18 18:47:42", "link": "http://arxiv.org/abs/2312.11444v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascade Speculative Drafting for Even Faster LLM Inference", "abstract": "Introduced to enhance the efficiency of large language model (LLM) inference,\nspeculative decoding operates by having a smaller model generate a draft. A\nlarger target model then reviews this draft to align with its output, and any\nacceptance by the target model results in a reduction of the number of the\ntarget model runs, ultimately improving efficiency. However, the drafting\nprocess in speculative decoding includes slow autoregressive generation and\nallocates equal time to generating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the suboptimal performance of\nspeculative decoding. To further improve LLM inference, we introduce Cascade\nSpeculative Drafting (CS Drafting), a speculative execution algorithm that\nincorporates two types of cascades. The Vertical Cascade eliminates\nautoregressive generation from neural models, while the Horizontal Cascade\noptimizes time allocation in drafting for improved efficiency. Combining both\ncascades, CS Drafting achieves up to an 81 percent additional speedup over\nspeculative decoding in our experiments, while maintaining the same output\ndistribution as the target model. Our code is publicly available at\nhttps://github.com/lfsszd/CS-Drafting.", "published": "2023-12-18 18:59:46", "link": "http://arxiv.org/abs/2312.11462v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer\n  Models", "abstract": "Logical reasoning is central to complex human activities, such as thinking,\ndebating, and planning; it is also a central component of many AI systems as\nwell. In this paper, we investigate the extent to which encoder-only\ntransformer language models (LMs) can reason according to logical rules. We ask\nwhether those LMs can deduce theorems in propositional calculus and first-order\nlogic; if their relative success in these problems reflects general logical\ncapabilities; and which layers contribute the most to the task. First, we show\nfor several encoder-only LMs that they can be trained, to a reasonable degree,\nto determine logical validity on various datasets. Next, by cross-probing\nfine-tuned models on these datasets, we show that LMs have difficulty in\ntransferring their putative logical reasoning ability, which suggests that they\nmay have learned dataset-specific features, instead of a general capability.\nFinally, we conduct a layerwise probing experiment, which shows that the\nhypothesis classification task is mostly solved through higher layers.", "published": "2023-12-18 21:42:34", "link": "http://arxiv.org/abs/2312.11720v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the\n  Generative Artificial Intelligence (AI) Research Landscape", "abstract": "This comprehensive survey explored the evolving landscape of generative\nArtificial Intelligence (AI), with a specific focus on the transformative\nimpacts of Mixture of Experts (MoE), multimodal learning, and the speculated\nadvancements towards Artificial General Intelligence (AGI). It critically\nexamined the current state and future trajectory of generative Artificial\nIntelligence (AI), exploring how innovations like Google's Gemini and the\nanticipated OpenAI Q* project are reshaping research priorities and\napplications across various domains, including an impact analysis on the\ngenerative AI research taxonomy. It assessed the computational challenges,\nscalability, and real-world implications of these technologies while\nhighlighting their potential in driving significant progress in fields like\nhealthcare, finance, and education. It also addressed the emerging academic\nchallenges posed by the proliferation of both AI-themed and AI-generated\npreprints, examining their impact on the peer-review process and scholarly\ncommunication. The study highlighted the importance of incorporating ethical\nand human-centric methods in AI development, ensuring alignment with societal\nnorms and welfare, and outlined a strategy for future AI research that focuses\non a balanced and conscientious use of MoE, multimodality, and AGI in\ngenerative AI.", "published": "2023-12-18 01:11:39", "link": "http://arxiv.org/abs/2312.10868v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Generalized Category Discovery with Large Language Models in the Loop", "abstract": "Generalized Category Discovery (GCD) is a crucial task that aims to recognize\nboth known and novel categories from a set of unlabeled data by utilizing a few\nlabeled data with only known categories. Due to the lack of supervision and\ncategory information, current methods usually perform poorly on novel\ncategories and struggle to reveal semantic meanings of the discovered clusters,\nwhich limits their applications in the real world. To mitigate the above\nissues, we propose Loop, an end-to-end active-learning framework that\nintroduces Large Language Models (LLMs) into the training loop, which can boost\nmodel performance and generate category names without relying on any human\nefforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to\nselect samples that have a higher probability of falling to wrong clusters,\nbased on neighborhood prediction consistency and entropy of cluster assignment\nprobabilities. Then we propose a Scalable Query strategy to allow LLMs to\nchoose true neighbors of the selected samples from multiple candidate samples.\nBased on the feedback from LLMs, we perform Refined Neighborhood Contrastive\nLearning (RNCL) to pull samples and their neighbors closer to learn\nclustering-friendly representations. Finally, we select representative samples\nfrom clusters corresponding to novel categories to allow LLMs to generate\ncategory names for them. Extensive experiments on three benchmark datasets show\nthat Loop outperforms SOTA models by a large margin and generates accurate\ncategory names for the discovered clusters. Code and data are available at\nhttps://github.com/Lackel/LOOP.", "published": "2023-12-18 02:55:14", "link": "http://arxiv.org/abs/2312.10897v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Satellite Captioning: Large Language Models to Augment Labeling", "abstract": "With the growing capabilities of modern object detection networks and\ndatasets to train them, it has gotten more straightforward and, importantly,\nless laborious to get up and running with a model that is quite adept at\ndetecting any number of various objects. However, while image datasets for\nobject detection have grown and continue to proliferate (the current most\nextensive public set, ImageNet, contains over 14m images with over 14m\ninstances), the same cannot be said for textual caption datasets. While they\nhave certainly been growing in recent years, caption datasets present a much\nmore difficult challenge due to language differences, grammar, and the time it\ntakes for humans to generate them. Current datasets have certainly provided\nmany instances to work with, but it becomes problematic when a captioner may\nhave a more limited vocabulary, one may not be adequately fluent in the\nlanguage, or there are simple grammatical mistakes. These difficulties are\nincreased when the images get more specific, such as remote sensing images.\nThis paper aims to address this issue of potential information and\ncommunication shortcomings in caption datasets. To provide a more precise\nanalysis, we specify our domain of images to be remote sensing images in the\nRSICD dataset and experiment with the captions provided here. Our findings\nindicate that ChatGPT grammar correction is a simple and effective way to\nincrease the performance accuracy of caption models by making data captions\nmore diverse and grammatically correct.", "published": "2023-12-18 03:21:58", "link": "http://arxiv.org/abs/2312.10905v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LaViP:Language-Grounded Visual Prompts", "abstract": "We introduce a language-grounded visual prompting method to adapt the visual\nencoder of vision-language models for downstream tasks. By capitalizing on\nlanguage integration, we devise a parameter-efficient strategy to adjust the\ninput of the visual encoder, eliminating the need to modify or add to the\nmodel's parameters. Due to this design choice, our algorithm can operate even\nin black-box scenarios, showcasing adaptability in situations where access to\nthe model's parameters is constrained. We will empirically demonstrate that,\ncompared to prior art, grounding visual prompts with language enhances both the\naccuracy and speed of adaptation. Moreover, our algorithm excels in\nbase-to-novel class generalization, overcoming limitations of visual prompting\nand exhibiting the capacity to generalize beyond seen classes. We thoroughly\nassess and evaluate our method across a variety of image recognition datasets,\nsuch as EuroSAT, UCF101, DTD, and CLEVR, spanning different learning\nsituations, including few-shot learning, base-to-novel class generalization,\nand transfer learning.", "published": "2023-12-18 05:50:10", "link": "http://arxiv.org/abs/2312.10945v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Soft Alignment of Modality Space for End-to-end Speech Translation", "abstract": "End-to-end Speech Translation (ST) aims to convert speech into target text\nwithin a unified model. The inherent differences between speech and text\nmodalities often impede effective cross-modal and cross-lingual transfer.\nExisting methods typically employ hard alignment (H-Align) of individual speech\nand text segments, which can degrade textual representations. To address this,\nwe introduce Soft Alignment (S-Align), using adversarial training to align the\nrepresentation spaces of both modalities. S-Align creates a modality-invariant\nspace while preserving individual modality quality. Experiments on three\nlanguages from the MuST-C dataset show S-Align outperforms H-Align across\nmultiple tasks and offers translation capabilities on par with specialized\ntranslation models.", "published": "2023-12-18 06:08:51", "link": "http://arxiv.org/abs/2312.10952v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speaker Mask Transformer for Multi-talker Overlapped Speech Recognition", "abstract": "Multi-talker overlapped speech recognition remains a significant challenge,\nrequiring not only speech recognition but also speaker diarization tasks to be\naddressed. In this paper, to better address these tasks, we first introduce\nspeaker labels into an autoregressive transformer-based speech recognition\nmodel to support multi-speaker overlapped speech recognition. Then, to improve\nspeaker diarization, we propose a novel speaker mask branch to detection the\nspeech segments of individual speakers. With the proposed model, we can perform\nboth speech recognition and speaker diarization tasks simultaneously using a\nsingle model. Experimental results on the LibriSpeech-based overlapped dataset\ndemonstrate the effectiveness of the proposed method in both speech recognition\nand speaker diarization tasks, particularly enhancing the accuracy of speaker\ndiarization in relatively complex multi-talker scenarios.", "published": "2023-12-18 06:29:53", "link": "http://arxiv.org/abs/2312.10959v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative linguistic representation for spoken language identification", "abstract": "Effective extraction and application of linguistic features are central to\nthe enhancement of spoken Language IDentification (LID) performance. With the\nsuccess of recent large models, such as GPT and Whisper, the potential to\nleverage such pre-trained models for extracting linguistic features for LID\ntasks has become a promising area of research. In this paper, we explore the\nutilization of the decoder-based network from the Whisper model to extract\nlinguistic features through its generative mechanism for improving the\nclassification accuracy in LID tasks. We devised two strategies - one based on\nthe language embedding method and the other focusing on direct optimization of\nLID outputs while simultaneously enhancing the speech recognition tasks. We\nconducted experiments on the large-scale multilingual datasets MLS,\nVoxLingua107, and CommonVoice to test our approach. The experimental results\ndemonstrated the effectiveness of the proposed method on both in-domain and\nout-of-domain datasets for LID tasks.", "published": "2023-12-18 06:40:24", "link": "http://arxiv.org/abs/2312.10964v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Knowledge Graphs and Pre-trained Language Models enhanced Representation\n  Learning for Conversational Recommender Systems", "abstract": "Conversational recommender systems (CRS) utilize natural language\ninteractions and dialogue history to infer user preferences and provide\naccurate recommendations. Due to the limited conversation context and\nbackground knowledge, existing CRSs rely on external sources such as knowledge\ngraphs to enrich the context and model entities based on their inter-relations.\nHowever, these methods ignore the rich intrinsic information within entities.\nTo address this, we introduce the Knowledge-Enhanced Entity Representation\nLearning (KERL) framework, which leverages both the knowledge graph and a\npre-trained language model to improve the semantic understanding of entities\nfor CRS. In our KERL framework, entity textual descriptions are encoded via a\npre-trained language model, while a knowledge graph helps reinforce the\nrepresentation of these entities. We also employ positional encoding to\neffectively capture the temporal information of entities in a conversation. The\nenhanced entity representation is then used to develop a recommender component\nthat fuses both entity and contextual representations for more informed\nrecommendations, as well as a dialogue component that generates informative\nentity-related information in the response text. A high-quality knowledge graph\nwith aligned entity descriptions is constructed to facilitate our study, namely\nthe Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that\nKERL achieves state-of-the-art results in both recommendation and response\ngeneration tasks.", "published": "2023-12-18 06:41:23", "link": "http://arxiv.org/abs/2312.10967v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "UniGen: A Unified Generative Framework for Retrieval and Question\n  Answering with Large Language Models", "abstract": "Generative information retrieval, encompassing two major tasks of Generative\nDocument Retrieval (GDR) and Grounded Answer Generation (GAR), has gained\nsignificant attention in the area of information retrieval and natural language\nprocessing. Existing methods for GDR and GAR rely on separate retrieval and\nreader modules, which hinder simultaneous optimization. To overcome this, we\npresent \\textbf{UniGen}, a \\textbf{Uni}fied \\textbf{Gen}erative framework for\nretrieval and question answering that integrates both tasks into a single\ngenerative model leveraging the capabilities of large language models. UniGen\nemploys a shared encoder and two distinct decoders for generative retrieval and\nquestion answering. To facilitate the learning of both tasks, we introduce\nconnectors, generated by large language models, to bridge the gaps between\nquery inputs and generation targets, as well as between document identifiers\nand answers. Furthermore, we propose an iterative enhancement strategy that\nleverages generated answers and retrieved documents to iteratively improve both\ntasks. Through extensive experiments on the MS MARCO and NQ datasets, we\ndemonstrate the effectiveness of UniGen, showcasing its superior performance in\nboth the retrieval and the question answering tasks.", "published": "2023-12-18 09:13:41", "link": "http://arxiv.org/abs/2312.11036v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI", "abstract": "Emotion significantly impacts our daily behaviors and interactions. While\nrecent generative AI models, such as large language models, have shown\nimpressive performance in various tasks, it remains unclear whether they truly\ncomprehend emotions. This paper aims to address this gap by incorporating\npsychological theories to gain a holistic understanding of emotions in\ngenerative AI models. Specifically, we propose three approaches: 1)\nEmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI\nmodel performance, and 3) EmotionDecode to explain the effects of emotional\nstimuli, both benign and malignant. Through extensive experiments involving\nlanguage and multi-modal models on semantic understanding, logical reasoning,\nand generation tasks, we demonstrate that both textual and visual EmotionPrompt\ncan boost the performance of AI models while EmotionAttack can hinder it.\nAdditionally, EmotionDecode reveals that AI models can comprehend emotional\nstimuli akin to the mechanism of dopamine in the human brain. Our work heralds\na novel avenue for exploring psychology to enhance our understanding of\ngenerative AI models.", "published": "2023-12-18 11:19:45", "link": "http://arxiv.org/abs/2312.11111v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Muted: Multilingual Targeted Offensive Speech Identification and\n  Visualization", "abstract": "Offensive language such as hate, abuse, and profanity (HAP) occurs in various\ncontent on the web. While previous work has mostly dealt with sentence level\nannotations, there have been a few recent attempts to identify offensive spans\nas well. We build upon this work and introduce Muted, a system to identify\nmultilingual HAP content by displaying offensive arguments and their targets\nusing heat maps to indicate their intensity. Muted can leverage any\ntransformer-based HAP-classification model and its attention mechanism\nout-of-the-box to identify toxic spans, without further fine-tuning. In\naddition, we use the spaCy library to identify the specific targets and\narguments for the words predicted by the attention heatmaps. We present the\nmodel's performance on identifying offensive spans and their targets in\nexisting datasets and present new annotations on German text. Finally, we\ndemonstrate our proposed visualization tool on multilingual inputs.", "published": "2023-12-18 16:50:27", "link": "http://arxiv.org/abs/2312.11344v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Problem of Coherence in Natural Language Explanations of\n  Recommendations", "abstract": "Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.", "published": "2023-12-18 17:12:35", "link": "http://arxiv.org/abs/2312.11356v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM\n  Finetuning", "abstract": "This paper introduces an efficient strategy to transform Large Language\nModels (LLMs) into Multi-Modal Large Language Models (MLLMs). By\nconceptualizing this transformation as a domain adaptation process, i.e.,\ntransitioning from text understanding to embracing multiple modalities, we\nintriguingly note that, within each attention block, tuning LayerNorm suffices\nto yield strong performance. Moreover, when benchmarked against other tuning\napproaches like full parameter finetuning or LoRA, its benefits on efficiency\nare substantial. For example, when compared to LoRA on a 13B model scale,\nperformance can be enhanced by an average of over 20% across five multi-modal\ntasks, and meanwhile, results in a significant reduction of trainable\nparameters by 41.9% and a decrease in GPU memory usage by 17.6%. On top of this\nLayerNorm strategy, we showcase that selectively tuning only with\nconversational data can improve efficiency further. Beyond these empirical\noutcomes, we provide a comprehensive analysis to explore the role of LayerNorm\nin adapting LLMs to the multi-modal domain and improving the expressive power\nof the model.", "published": "2023-12-18 18:21:43", "link": "http://arxiv.org/abs/2312.11420v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Regularized Conditional Alignment for Multi-Domain Text Classification", "abstract": "The most successful multi-domain text classification (MDTC) approaches employ\nthe shared-private paradigm to facilitate the enhancement of domain-invariant\nfeatures through domain-specific attributes. Additionally, they employ\nadversarial training to align marginal feature distributions. Nevertheless,\nthese methodologies encounter two primary challenges: (1) Neglecting\nclass-aware information during adversarial alignment poses a risk of\nmisalignment; (2) The limited availability of labeled data across multiple\ndomains fails to ensure adequate discriminative capacity for the model. To\ntackle these issues, we propose a method called Regularized Conditional\nAlignment (RCA) to align the joint distributions of domains and classes, thus\nmatching features within the same category and amplifying the discriminative\nqualities of acquired features. Moreover, we employ entropy minimization and\nvirtual adversarial training to constrain the uncertainty of predictions\npertaining to unlabeled data and enhance the model's robustness. Empirical\nresults on two benchmark datasets demonstrate that our RCA approach outperforms\nstate-of-the-art MDTC techniques.", "published": "2023-12-18 05:52:05", "link": "http://arxiv.org/abs/2312.11572v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks", "abstract": "In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.", "published": "2023-12-18 19:27:09", "link": "http://arxiv.org/abs/2312.11671v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows", "abstract": "LLM chains enable complex tasks by decomposing work into a sequence of\nsubtasks. Similarly, the more established techniques of crowdsourcing workflows\ndecompose complex tasks into smaller tasks for human crowdworkers. Chains\naddress LLM errors analogously to the way crowdsourcing workflows address human\nerror. To characterize opportunities for LLM chaining, we survey 107 papers\nacross the crowdsourcing and chaining literature to construct a design space\nfor chain development. The design space covers a designer's objectives and the\ntactics used to build workflows. We then surface strategies that mediate how\nworkflows use tactics to achieve objectives. To explore how techniques from\ncrowdsourcing may apply to chaining, we adapt crowdsourcing workflows to\nimplement LLM chains across three case studies: creating a taxonomy, shortening\ntext, and writing a short story. From the design space and our case studies, we\nidentify takeaways for effective chain design and raise implications for future\nresearch and development.", "published": "2023-12-18 20:01:58", "link": "http://arxiv.org/abs/2312.11681v4", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Opportunities and Challenges of Applying Large Language Models in\n  Building Energy Efficiency and Decarbonization Studies: An Exploratory\n  Overview", "abstract": "In recent years, the rapid advancement and impressive capabilities of Large\nLanguage Models (LLMs) have been evident across various domains. This paper\nexplores the application, implications, and potential of LLMs in building\nenergy efficiency and decarbonization studies. The wide-ranging capabilities of\nLLMs are examined in the context of the building energy field, including\nintelligent control systems, code generation, data infrastructure, knowledge\nextraction, and education. Despite the promising potential of LLMs, challenges\nincluding complex and expensive computation, data privacy, security and\ncopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.\nThe paper concludes with a call for future research focused on the enhancement\nof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research\nbetween AI and energy experts.", "published": "2023-12-18 20:58:58", "link": "http://arxiv.org/abs/2312.11701v1", "categories": ["eess.SY", "cs.CL", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Shaping Political Discourse using multi-source News Summarization", "abstract": "Multi-document summarization is the process of automatically generating a\nconcise summary of multiple documents related to the same topic. This summary\ncan help users quickly understand the key information from a large collection\nof documents. Multi-document summarization systems are more complex than\nsingle-document summarization systems due to the need to identify and combine\ninformation from multiple sources. In this paper, we have developed a machine\nlearning model that generates a concise summary of a topic from multiple news\ndocuments. The model is designed to be unbiased by sampling its input equally\nfrom all the different aspects of the topic, even if the majority of the news\nsources lean one way.", "published": "2023-12-18 21:03:46", "link": "http://arxiv.org/abs/2312.11703v1", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Serialization of Tabular Data for Few-shot Classification\n  with Large Language Models", "abstract": "We present a study on the integration of Large Language Models (LLMs) in\ntabular data classification, emphasizing an efficient framework. Building upon\nexisting work done in TabLLM (arXiv:2210.10723), we introduce three novel\nserialization techniques, including the standout LaTeX serialization method.\nThis method significantly boosts the performance of LLMs in processing\ndomain-specific datasets, Our method stands out for its memory efficiency and\nability to fully utilize complex data structures. Through extensive\nexperimentation, including various serialization approaches like feature\ncombination and importance, we demonstrate our work's superiority in accuracy\nand efficiency over traditional models.", "published": "2023-12-18 21:11:17", "link": "http://arxiv.org/abs/2312.12464v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "3S-TSE: Efficient Three-Stage Target Speaker Extraction for Real-Time\n  and Low-Resource Applications", "abstract": "Target speaker extraction (TSE) aims to isolate a specific voice from\nmultiple mixed speakers relying on a registerd sample. Since voiceprint\nfeatures usually vary greatly, current end-to-end neural networks require large\nmodel parameters which are computational intensive and impractical for\nreal-time applications, espetially on resource-constrained platforms. In this\npaper, we address the TSE task using microphone array and introduce a novel\nthree-stage solution that systematically decouples the process: First, a neural\nnetwork is trained to estimate the direction of the target speaker. Second,\nwith the direction determined, the Generalized Sidelobe Canceller (GSC) is used\nto extract the target speech. Third, an Inplace Convolutional Recurrent Neural\nNetwork (ICRN) acts as a denoising post-processor, refining the GSC output to\nyield the final separated speech. Our approach delivers superior performance\nwhile drastically reducing computational load, setting a new standard for\nefficient real-time target speaker extraction.", "published": "2023-12-18 07:02:38", "link": "http://arxiv.org/abs/2312.10979v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluation of Barlow Twins and VICReg self-supervised learning for sound\n  patterns of bird and anuran species", "abstract": "Taking advantage of the structure of large datasets to pre-train Deep\nLearning models is a promising strategy to decrease the need for supervised\ndata. Self-supervised learning methods, such as contrastive and its variation\nare a promising way towards obtaining better representations in many Deep\nLearning applications. Soundscape ecology is one application in which\nannotations are expensive and scarce, therefore deserving investigation to\napproximate methods that do not require annotations to those that rely on\nsupervision. Our study involves the use of the methods Barlow Twins and VICReg\nto pre-train different models with the same small dataset with sound patterns\nof bird and anuran species. In a downstream task to classify those animal\nspecies, the models obtained results close to supervised ones, pre-trained in\nlarge generic datasets, and fine-tuned with the same task.", "published": "2023-12-18 14:38:04", "link": "http://arxiv.org/abs/2312.11240v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head\n  Synthesis", "abstract": "Audio-driven talking head synthesis is a promising topic with wide\napplications in digital human, film making and virtual reality. Recent\nNeRF-based approaches have shown superiority in quality and fidelity compared\nto previous studies. However, when it comes to few-shot talking head\ngeneration, a practical scenario where only few seconds of talking video is\navailable for one identity, two limitations emerge: 1) they either have no base\nmodel, which serves as a facial prior for fast convergence, or ignore the\nimportance of audio when building the prior; 2) most of them overlook the\ndegree of correlation between different face regions and audio, e.g., mouth is\naudio related, while ear is audio independent. In this paper, we present Audio\nEnhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can\ngenerate realistic portraits of a new speaker with fewshot dataset.\nSpecifically, we introduce an Audio Aware Aggregation module into the feature\nfusion stage of the reference scheme, where the weight is determined by the\nsimilarity of audio between reference and target image. Then, an Audio-Aligned\nFace Generation strategy is proposed to model the audio related and audio\nindependent regions respectively, with a dual-NeRF framework. Extensive\nexperiments have shown AE-NeRF surpasses the state-of-the-art on image\nfidelity, audio-lip synchronization, and generalization ability, even in\nlimited training set or training iterations.", "published": "2023-12-18 04:14:38", "link": "http://arxiv.org/abs/2312.10921v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Improved Long-Form Speech Recognition by Jointly Modeling the Primary\n  and Non-primary Speakers", "abstract": "ASR models often suffer from a long-form deletion problem where the model\npredicts sequential blanks instead of words when transcribing a lengthy audio\n(in the order of minutes or hours). From the perspective of a user or\ndownstream system consuming the ASR results, this behavior can be perceived as\nthe model \"being stuck\", and potentially make the product hard to use. One of\nthe culprits for long-form deletion is training-test data mismatch, which can\nhappen even when the model is trained on diverse and large-scale data collected\nfrom multiple application domains. In this work, we introduce a novel technique\nto simultaneously model different groups of speakers in the audio along with\nthe standard transcript tokens. Speakers are grouped as primary and\nnon-primary, which connects the application domains and significantly\nalleviates the long-form deletion problem. This improved model neither needs\nany additional training data nor incurs additional training or inference cost.", "published": "2023-12-18 11:47:39", "link": "http://arxiv.org/abs/2312.11123v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Refining Underlying Information Framework for Monaural Speech\n  Enhancement", "abstract": "Supervised speech enhancement has gained significantly from recent\nadvancements in neural networks, especially due to their ability to\nnon-linearly fit the diverse representations of target speech, such as waveform\nor spectrum. However, these direct-fitting solutions continue to face\nchallenges with degraded speech and residual noise in hearing evaluations. By\nbridging the speech enhancement and the Information Bottleneck principle in\nthis letter, we rethink a universal plug-and-play strategy and propose a\nRefining Underlying Information framework called RUI to rise to the challenges\nboth in theory and practice. Specifically, we first transform the objective of\nspeech enhancement into an incremental convergence problem of mutual\ninformation between comprehensive speech characteristics and individual speech\ncharacteristics, e.g., spectral and acoustic characteristics. By doing so,\ncompared with the existing direct-fitting solutions, the underlying information\nstems from the conditional entropy of acoustic characteristic given spectral\ncharacteristics. Therefore, we design a dual-path multiple refinement iterator\nbased on the chain rule of entropy to refine this underlying information for\nfurther approximating target speech. Experimental results on DNS-Challenge\ndataset show that our solution consistently improves 0.3+ PESQ score over\nbaselines, with only additional 1.18 M parameters. The source code is available\nat https://github.com/caoruitju/RUI_SE.", "published": "2023-12-18 13:47:26", "link": "http://arxiv.org/abs/2312.11201v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Perceptual Musical Features for Interpretable Audio Tagging", "abstract": "In the age of music streaming platforms, the task of automatically tagging\nmusic audio has garnered significant attention, driving researchers to devise\nmethods aimed at enhancing performance metrics on standard datasets. Most\nrecent approaches rely on deep neural networks, which, despite their impressive\nperformance, possess opacity, making it challenging to elucidate their output\nfor a given input. While the issue of interpretability has been emphasized in\nother fields like medicine, it has not received attention in music-related\ntasks. In this study, we explored the relevance of interpretability in the\ncontext of automatic music tagging. We constructed a workflow that incorporates\nthree different information extraction techniques: a) leveraging symbolic\nknowledge, b) utilizing auxiliary deep neural networks, and c) employing signal\nprocessing to extract perceptual features from audio files. These features were\nsubsequently used to train an interpretable machine-learning model for tag\nprediction. We conducted experiments on two datasets, namely the MTG-Jamendo\ndataset and the GTZAN dataset. Our method surpassed the performance of baseline\nmodels in both tasks and, in certain instances, demonstrated competitiveness\nwith the current state-of-the-art. We conclude that there are use cases where\nthe deterioration in performance is outweighed by the value of\ninterpretability.", "published": "2023-12-18 14:31:58", "link": "http://arxiv.org/abs/2312.11234v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal\n  Processing Analysis for Sounds, & Strains Emerging Technology", "abstract": "Acoustic data serves as a fundamental cornerstone in advancing scientific and\nengineering understanding across diverse disciplines, spanning biology,\ncommunications, and ocean and Earth science. This inquiry meticulously explores\nrecent advancements and transformative potential within the domain of\nacoustics, specifically focusing on machine learning (ML) and deep learning.\nML, comprising an extensive array of statistical techniques, proves\nindispensable for autonomously discerning and leveraging patterns within data.\nIn contrast to traditional acoustics and signal processing, ML adopts a\ndata-driven approach, unveiling intricate relationships between features and\ndesired labels or actions, as well as among features themselves, given ample\ntraining data. The application of ML to expansive sets of training data\nfacilitates the discovery of models elucidating complex acoustic phenomena such\nas human speech and reverberation. The dynamic evolution of ML in acoustics\nyields compelling results and holds substantial promise for the future. The\nadvent of electronic stethoscopes and analogous recording and data logging\ndevices has expanded the application of acoustic signal processing concepts to\nthe analysis of bowel sounds. This paper critically reviews existing literature\non acoustic signal processing for bowel sound analysis, outlining fundamental\napproaches and applicable machine learning principles. It chronicles historical\nprogress in signal processing techniques that have facilitated the extraction\nof valuable information from bowel sounds, emphasizing advancements in noise\nreduction, segmentation, signal enhancement, feature extraction, sound\nlocalization, and machine learning techniques...", "published": "2023-12-18 03:04:42", "link": "http://arxiv.org/abs/2402.10005v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "68Qxx, 68Uxx, 68Vxx, 68Wxx, 68Txx, 68-XX", "J.7; D.2; G.4"], "primary_category": "cs.SD"}
{"title": "An Extended Variational Mode Decomposition Algorithm Developed Speech\n  Emotion Recognition Performance", "abstract": "Emotion recognition (ER) from speech signals is a robust approach since it\ncannot be imitated like facial expression or text based sentiment analysis.\nValuable information underlying the emotions are significant for human-computer\ninteractions enabling intelligent machines to interact with sensitivity in the\nreal world. Previous ER studies through speech signal processing have focused\nexclusively on associations between different signal mode decomposition methods\nand hidden informative features. However, improper decomposition parameter\nselections lead to informative signal component losses due to mode duplicating\nand mixing. In contrast, the current study proposes VGG-optiVMD, an empowered\nvariational mode decomposition algorithm, to distinguish meaningful speech\nfeatures and automatically select the number of decomposed modes and optimum\nbalancing parameter for the data fidelity constraint by assessing their effects\non the VGG16 flattening output layer. Various feature vectors were employed to\ntrain the VGG16 network on different databases and assess VGG-optiVMD\nreproducibility and reliability. One, two, and three-dimensional feature\nvectors were constructed by concatenating Mel-frequency cepstral coefficients,\nChromagram, Mel spectrograms, Tonnetz diagrams, and spectral centroids. Results\nconfirmed a synergistic relationship between the fine-tuning of the signal\nsample rate and decomposition parameters with classification accuracy,\nachieving state-of-the-art 96.09% accuracy in predicting seven emotions on the\nBerlin EMO-DB database.", "published": "2023-12-18 05:24:03", "link": "http://arxiv.org/abs/2312.10937v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraged Mel spectrograms using Harmonic and Percussive Components in\n  Speech Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) affective technology enables the intelligent\nembedded devices to interact with sensitivity. Similarly, call centre employees\nrecognise customers' emotions from their pitch, energy, and tone of voice so as\nto modify their speech for a high-quality interaction with customers. This work\nexplores, for the first time, the effects of the harmonic and percussive\ncomponents of Mel spectrograms in SER. We attempt to leverage the Mel\nspectrogram by decomposing distinguishable acoustic features for exploitation\nin our proposed architecture, which includes a novel feature map generator\nalgorithm, a CNN-based network feature extractor and a multi-layer perceptron\n(MLP) classifier. This study specifically focuses on effective data\naugmentation techniques for building an enriched hybrid-based feature map. This\nprocess results in a function that outputs a 2D image so that it can be used as\ninput data for a pre-trained CNN-VGG16 feature extractor. Furthermore, we also\ninvestigate other acoustic features such as MFCCs, chromagram, spectral\ncontrast, and the tonnetz to assess our proposed framework. A test accuracy of\n92.79% on the Berlin EMO-DB database is achieved. Our result is higher than\nprevious works using CNN-VGG16.", "published": "2023-12-18 05:55:46", "link": "http://arxiv.org/abs/2312.10949v1", "categories": ["cs.SD", "cs.CV", "cs.HC", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
