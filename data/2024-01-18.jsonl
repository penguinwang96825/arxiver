{"title": "Resolving Regular Polysemy in Named Entities", "abstract": "Word sense disambiguation primarily addresses the lexical ambiguity of common\nwords based on a predefined sense inventory. Conversely, proper names are\nusually considered to denote an ad-hoc real-world referent. Once the reference\nis decided, the ambiguity is purportedly resolved. However, proper names also\nexhibit ambiguities through appellativization, i.e., they act like common words\nand may denote different aspects of their referents. We proposed to address the\nambiguities of proper names through the light of regular polysemy, which we\nformalized as dot objects. This paper introduces a combined word sense\ndisambiguation (WSD) model for disambiguating common words against Chinese\nWordnet (CWN) and proper names as dot objects. The model leverages the\nflexibility of a gloss-based model architecture, which takes advantage of the\nglosses and example sentences of CWN. We show that the model achieves\ncompetitive results on both common and proper nouns, even on a relatively\nsparse sense dataset. Aside from being a performant WSD tool, the model further\nfacilitates the future development of the lexical resource.", "published": "2024-01-18 07:18:03", "link": "http://arxiv.org/abs/2401.09758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Decontextualization of Yes/No Question and Answers into\n  Factual Statements", "abstract": "Yes/No or polar questions represent one of the main linguistic question\ncategories. They consist of a main interrogative clause, for which the answer\nis binary (assertion or negation). Polar questions and answers (PQA) represent\na valuable knowledge resource present in many community and other curated QA\nsources, such as forums or e-commerce applications. Using answers to polar\nquestions alone in other contexts is not trivial. Answers are contextualized,\nand presume that the interrogative question clause and any shared knowledge\nbetween the asker and answerer are provided.\n  We address the problem of controllable rewriting of answers to polar\nquestions into decontextualized and succinct factual statements. We propose a\nTransformer sequence to sequence model that utilizes soft-constraints to ensure\ncontrollable rewriting, such that the output statement is semantically\nequivalent to its PQA input. Evaluation on three separate PQA datasets as\nmeasured through automated and human evaluation metrics show that our proposed\napproach achieves the best performance when compared to existing baselines.", "published": "2024-01-18 07:52:12", "link": "http://arxiv.org/abs/2401.09775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Biases in Large Language Models: \"bias-kNN'' for Effective\n  Few-Shot Learning", "abstract": "Large Language Models (LLMs) have shown significant promise in various\napplications, including zero-shot and few-shot learning. However, their\nperformance can be hampered by inherent biases. Instead of traditionally sought\nmethods that aim to minimize or correct these biases, this study introduces a\nnovel methodology named ``bias-kNN''. This approach capitalizes on the biased\noutputs, harnessing them as primary features for kNN and supplementing with\ngold labels. Our comprehensive evaluations, spanning diverse domain text\nclassification datasets and different GPT-2 model sizes, indicate the\nadaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach\nnot only outperforms conventional in-context learning in few-shot scenarios but\nalso demonstrates robustness across a spectrum of samples, templates and\nverbalizers. This study, therefore, presents a unique perspective on harnessing\nbiases, transforming them into assets for enhanced model performance.", "published": "2024-01-18 08:05:45", "link": "http://arxiv.org/abs/2401.09783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instant Answering in E-Commerce Buyer-Seller Messaging using\n  Message-to-Question Reformulation", "abstract": "E-commerce customers frequently seek detailed product information for\npurchase decisions, commonly contacting sellers directly with extended queries.\nThis manual response requirement imposes additional costs and disrupts buyer's\nshopping experience with response time fluctuations ranging from hours to days.\nWe seek to automate buyer inquiries to sellers in a leading e-commerce store\nusing a domain-specific federated Question Answering (QA) system. The main\nchallenge is adapting current QA systems, designed for single questions, to\naddress detailed customer queries. We address this with a low-latency,\nsequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates\nbuyer messages into succinct questions by identifying and extracting the most\nsalient information from a message. Evaluation against baselines shows that M2Q\nyields relative increases of 757% in question understanding, and 1,746% in\nanswering rate from the federated QA system. Live deployment shows that\nautomatic answering saves sellers from manually responding to millions of\nmessages per year, and also accelerates customer purchase decisions by\neliminating the need for buyers to wait for a reply", "published": "2024-01-18 08:09:27", "link": "http://arxiv.org/abs/2401.09785v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and effective data augmentation for compositional generalization", "abstract": "Compositional generalization, the ability to predict complex meanings from\ntraining on simpler sentences, poses challenges for powerful pretrained seq2seq\nmodels. In this paper, we show that data augmentation methods that sample MRs\nand backtranslate them can be effective for compositional generalization, but\nonly if we sample from the right distribution. Remarkably, sampling from a\nuniform distribution performs almost as well as sampling from the test\ndistribution, and greatly outperforms earlier methods that sampled from the\ntraining distribution. We further conduct experiments to investigate the reason\nwhy this happens and where the benefit of such data augmentation methods come\nfrom.", "published": "2024-01-18 09:13:59", "link": "http://arxiv.org/abs/2401.09815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes\n  Through Multimodal Explanations", "abstract": "Internet memes have gained significant influence in communicating political,\npsychological, and sociocultural ideas. While memes are often humorous, there\nhas been a rise in the use of memes for trolling and cyberbullying. Although a\nwide variety of effective deep learning-based models have been developed for\ndetecting offensive multimodal memes, only a few works have been done on\nexplainability aspect. Recent laws like \"right to explanations\" of General Data\nProtection Regulation, have spurred research in developing interpretable models\nrather than only focusing on performance. Motivated by this, we introduce {\\em\nMultiBully-Ex}, the first benchmark dataset for multimodal explanation from\ncode-mixed cyberbullying memes. Here, both visual and textual modalities are\nhighlighted to explain why a given meme is cyberbullying. A Contrastive\nLanguage-Image Pretraining (CLIP) projection-based multimodal shared-private\nmultitask approach has been proposed for visual and textual explanation of a\nmeme. Experimental results demonstrate that training with multimodal\nexplanations improves performance in generating textual justifications and more\naccurately identifying the visual evidence supporting a decision with reliable\nperformance improvements.", "published": "2024-01-18 11:24:30", "link": "http://arxiv.org/abs/2401.09899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language\n  Models without Logit Access", "abstract": "Constrained decoding, a technique for enforcing constraints on language model\noutputs, offers a way to control text generation without retraining or\narchitectural modifications. Its application is, however, typically restricted\nto models that give users access to next-token distributions (usually via\nsoftmax logits), which poses a limitation with blackbox large language models\n(LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a\nnovel approach to constrained decoding for blackbox LLMs, which operates\nwithout access to the logits of the blackbox LLM. SGCD utilizes a locally\nhosted auxiliary model to refine the output of an unconstrained blackbox LLM,\neffectively treating this initial output as a \"sketch\" for further elaboration.\nThis approach is complementary to traditional logit-based techniques and\nenables the application of constrained decoding in settings where full model\ntransparency is unavailable. We demonstrate the efficacy of SGCD through\nexperiments in closed information extraction and constituency parsing, showing\nhow it enhances the utility and flexibility of blackbox LLMs for complex NLP\ntasks.", "published": "2024-01-18 13:31:24", "link": "http://arxiv.org/abs/2401.09967v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Explain Transformers by Illuminating Important Information", "abstract": "Transformer-based models excel in various natural language processing (NLP)\ntasks, attracting countless efforts to explain their inner workings. Prior\nmethods explain Transformers by focusing on the raw gradient and attention as\ntoken attribution scores, where non-relevant information is often considered\nduring explanation computation, resulting in confusing results. In this work,\nwe propose highlighting the important information and eliminating irrelevant\ninformation by a refined information flow on top of the layer-wise relevance\npropagation (LRP) method. Specifically, we consider identifying syntactic and\npositional heads as important attention heads and focus on the relevance\nobtained from these important heads. Experimental results demonstrate that\nirrelevant information does distort output attribution scores and then should\nbe masked during explanation computation. Compared to eight baselines on both\nclassification and question-answering datasets, our method consistently\noutperforms with over 3\\% to 33\\% improvement on explanation metrics, providing\nsuperior explanation performance. Our anonymous code repository is available\nat: https://github.com/LinxinS97/Mask-LRP", "published": "2024-01-18 13:41:08", "link": "http://arxiv.org/abs/2401.09972v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradable ChatGPT Translation Evaluation", "abstract": "ChatGPT, as a language model based on large-scale pre-training, has exerted a\nprofound influence on the domain of machine translation. In ChatGPT, a \"Prompt\"\nrefers to a segment of text or instruction employed to steer the model towards\ngenerating a specific category of response. The design of the translation\nprompt emerges as a key aspect that can wield influence over factors such as\nthe style, precision and accuracy of the translation to a certain extent.\nHowever, there is a lack of a common standard and methodology on how to design\nand select a translation prompt. Accordingly, this paper proposes a generic\ntaxonomy, which defines gradable translation prompts in terms of expression\ntype, translation style, POS information and explicit statement, thus\nfacilitating the construction of prompts endowed with distinct attributes\ntailored for various translation tasks. Specific experiments and cases are\nselected to validate and illustrate the effectiveness of the method.", "published": "2024-01-18 13:58:10", "link": "http://arxiv.org/abs/2401.09984v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distantly Supervised Morpho-Syntactic Model for Relation Extraction", "abstract": "The task of Information Extraction (IE) involves automatically converting\nunstructured textual content into structured data. Most research in this field\nconcentrates on extracting all facts or a specific set of relationships from\ndocuments. In this paper, we present a method for the extraction and\ncategorisation of an unrestricted set of relationships from text. Our method\nrelies on morpho-syntactic extraction patterns obtained by a distant\nsupervision method, and creates Syntactic and Semantic Indices to extract and\nclassify candidate graphs. We evaluate our approach on six datasets built on\nWikidata and Wikipedia. The evaluation shows that our approach can achieve\nPrecision scores of up to 0.85, but with lower Recall and F1 scores. Our\napproach allows to quickly create rule-based systems for Information Extraction\nand to build annotated datasets to train machine-learning and deep-learning\nbased classifiers.", "published": "2024-01-18 14:17:40", "link": "http://arxiv.org/abs/2401.10002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Antonym vs Synonym Distinction using InterlaCed Encoder NETworks\n  (ICE-NET)", "abstract": "Antonyms vs synonyms distinction is a core challenge in lexico-semantic\nanalysis and automated lexical resource construction. These pairs share a\nsimilar distributional context which makes it harder to distinguish them.\nLeading research in this regard attempts to capture the properties of the\nrelation pairs, i.e., symmetry, transitivity, and trans-transitivity. However,\nthe inability of existing research to appropriately model the relation-specific\nproperties limits their end performance. In this paper, we propose InterlaCed\nEncoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim\nto capture and model the relation-specific properties of the antonyms and\nsynonyms pairs in order to perform the classification task in a\nperformance-enhanced manner. Experimental evaluation using the benchmark\ndatasets shows that ICE-NET outperforms the existing research by a relative\nscore of upto 1.8% in F1-measure. We release the codes for ICE-NET at\nhttps://github.com/asif6827/ICENET.", "published": "2024-01-18 15:08:58", "link": "http://arxiv.org/abs/2401.10045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs", "abstract": "Reasoning is a fundamental component of language understanding. Recent\nprompting techniques, such as chain of thought, have consistently improved\nLLMs' performance on various reasoning tasks. Nevertheless, there is still\nlittle understanding of what triggers reasoning abilities in LLMs in the\ninference stage. In this paper, we introduce code prompting, a chain of prompts\nthat transforms a natural language problem into code and directly prompts the\nLLM using the generated code without resorting to external code execution. We\nhypothesize that code prompts can elicit certain reasoning capabilities of LLMs\ntrained on text and code and utilize the proposed method to improve conditional\nreasoning, the ability to infer different conclusions depending on the\nfulfillment of certain conditions. We find that code prompting exhibits a\nhigh-performance boost for multiple LLMs (up to 22.52 percentage points on GPT\n3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional\nreasoning datasets. We then conduct comprehensive experiments to understand how\ncode prompts trigger reasoning abilities and which capabilities are elicited in\nthe underlying models. Our analysis of GPT 3.5 reveals that the code formatting\nof the input problem is essential for performance improvement. Furthermore,\ncode prompts improve sample efficiency of in-context learning and facilitate\nstate tracking of variables or entities.", "published": "2024-01-18 15:32:24", "link": "http://arxiv.org/abs/2401.10065v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Power in Numbers: Robust reading comprehension by finetuning with four\n  adversarial sentences per example", "abstract": "Recent models have achieved human level performance on the Stanford Question\nAnswering Dataset when using F1 scores to evaluate the reading comprehension\ntask. Yet, teaching machines to comprehend text has not been solved in the\ngeneral case. By appending one adversarial sentence to the context paragraph,\npast research has shown that the F1 scores from reading comprehension models\ndrop almost in half. In this paper, I replicate past adversarial research with\na new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops\nfrom 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I\nfinetune the model on SQuAD v1.1 training examples with one to five adversarial\nsentences appended to the context paragraph. Like past research, I find that\nthe finetuned model on one adversarial sentence does not generalize well across\nevaluation datasets. However, when finetuned on four or five adversarial\nsentences the model attains an F1 score of more than 70% on most evaluation\ndatasets with multiple appended and prepended adversarial sentences. The\nresults suggest that with enough examples we can make models robust to\nadversarial attacks.", "published": "2024-01-18 15:59:42", "link": "http://arxiv.org/abs/2401.10091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapters Mixup: Mixing Parameter-Efficient Adapters to Enhance the\n  Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "abstract": "Existing works show that augmenting the training data of pre-trained language\nmodels (PLMs) for classification tasks fine-tuned via parameter-efficient\nfine-tuning methods (PEFT) using both clean and adversarial examples can\nenhance their robustness under adversarial attacks. However, this adversarial\ntraining paradigm often leads to performance degradation on clean inputs and\nrequires frequent re-training on the entire data to account for new, unknown\nattacks. To overcome these challenges while still harnessing the benefits of\nadversarial training and the efficiency of PEFT, this work proposes a novel\napproach, called AdpMixup, that combines two paradigms: (1) fine-tuning through\nadapters and (2) adversarial augmentation via mixup to dynamically leverage\nexisting knowledge from a set of pre-known attacks for robust inference.\nIntuitively, AdpMixup fine-tunes PLMs with multiple adapters with both clean\nand pre-known adversarial examples and intelligently mixes them up in different\nratios during prediction. Our experiments show AdpMixup achieves the best\ntrade-off between training efficiency and robustness under both pre-known and\nunknown attacks, compared to existing baselines on five downstream tasks across\nsix varied black-box attacks and 2 PLMs. All source code will be available.", "published": "2024-01-18 16:27:18", "link": "http://arxiv.org/abs/2401.10111v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on\n  Data-to-Text Generation", "abstract": "We analyze the behaviors of open large language models (LLMs) on the task of\ndata-to-text (D2T) generation, i.e., generating coherent and relevant text from\nstructured data. To avoid the issue of LLM training data contamination with\nstandard benchmarks, we design Quintd - a tool for collecting novel structured\ndata records from public APIs. We find that open LLMs (Llama 2, Mistral, and\nZephyr) can generate fluent and coherent texts in zero-shot settings from data\nin common formats collected with Quintd. However, we show that the semantic\naccuracy of the outputs is a major issue: both according to human annotators\nand our reference-free metric based on GPT-4, more than 80% of the outputs of\nopen LLMs contain at least one semantic error. We publicly release the code,\ndata, and model outputs.", "published": "2024-01-18 18:15:46", "link": "http://arxiv.org/abs/2401.10186v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Cultural Nuances in Dialogue Agents through Cultural Value\n  Surveys", "abstract": "The cultural landscape of interactions with dialogue agents is a compelling\nyet relatively unexplored territory. It's clear that various sociocultural\naspects -- from communication styles and beliefs to shared metaphors and\nknowledge -- profoundly impact these interactions. To delve deeper into this\ndynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue\ngeneration with a cultural lens. We also develop baseline models capable of\nextracting cultural attributes from dialogue exchanges, with the goal of\nenhancing the predictive accuracy and quality of dialogue agents. To\neffectively co-learn cultural understanding and multi-turn dialogue\npredictions, we propose to incorporate cultural dimensions with dialogue\nencoding features. Our experimental findings highlight that incorporating\ncultural value surveys boosts alignment with references and cultural markers,\ndemonstrating its considerable influence on personalization and dialogue\nquality. To facilitate further exploration in this exciting domain, we publish\nour benchmark publicly accessible at https://github.com/yongcaoplus/cuDialog.", "published": "2024-01-18 19:42:04", "link": "http://arxiv.org/abs/2401.10352v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inconsistent dialogue responses and how to recover from them", "abstract": "One critical issue for chat systems is to stay consistent about preferences,\nopinions, beliefs and facts of itself, which has been shown a difficult\nproblem. In this work, we study methods to assess and bolster utterance\nconsistency of chat systems. A dataset is first developed for studying the\ninconsistencies, where inconsistent dialogue responses, explanations of the\ninconsistencies, and recovery utterances are authored by annotators. This\ncovers the life span of inconsistencies, namely introduction, understanding,\nand resolution. Building on this, we introduce a set of tasks centered on\ndialogue consistency, specifically focused on its detection and resolution. Our\nexperimental findings indicate that our dataset significantly helps the\nprogress in identifying and resolving conversational inconsistencies, and\ncurrent popular large language models like ChatGPT which are good at resolving\ninconsistencies however still struggle with detection.", "published": "2024-01-18 19:46:04", "link": "http://arxiv.org/abs/2401.10353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning High-Quality and General-Purpose Phrase Representations", "abstract": "Phrase representations play an important role in data science and natural\nlanguage processing, benefiting various tasks like Entity Alignment, Record\nLinkage, Fuzzy Joins, and Paraphrase Classification. The current\nstate-of-the-art method involves fine-tuning pre-trained language models for\nphrasal embeddings using contrastive learning. However, we have identified\nareas for improvement. First, these pre-trained models tend to be unnecessarily\ncomplex and require to be pre-trained on a corpus with context sentences.\nSecond, leveraging the phrase type and morphology gives phrase representations\nthat are both more precise and more flexible. We propose an improved framework\nto learn phrase representations in a context-free fashion. The framework\nemploys phrase type classification as an auxiliary task and incorporates\ncharacter-level information more effectively into the phrase representation.\nFurthermore, we design three granularities of data augmentation to increase the\ndiversity of training samples. Our experiments across a wide range of tasks\nshow that our approach generates superior phrase embeddings compared to\nprevious methods while requiring a smaller model size. [PEARL-small]:\nhttps://huggingface.co/Lihuchen/pearl_small; [PEARL-base]:\nhttps://huggingface.co/Lihuchen/pearl_base; [Code and Dataset]:\nhttps://github.com/tigerchen52/PEARL", "published": "2024-01-18 22:32:31", "link": "http://arxiv.org/abs/2401.10407v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Scoring of Clinical Patient Notes using Advanced NLP and\n  Pseudo Labeling", "abstract": "Clinical patient notes are critical for documenting patient interactions,\ndiagnoses, and treatment plans in medical practice. Ensuring accurate\nevaluation of these notes is essential for medical education and certification.\nHowever, manual evaluation is complex and time-consuming, often resulting in\nvariability and resource-intensive assessments. To tackle these challenges,\nthis research introduces an approach leveraging state-of-the-art Natural\nLanguage Processing (NLP) techniques, specifically Masked Language Modeling\n(MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and\neffectiveness, significantly reducing training time without compromising\nperformance. Experimental results showcase improved model performance,\nindicating a potential transformation in clinical note assessment.", "published": "2024-01-18 05:17:18", "link": "http://arxiv.org/abs/2401.12994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed\n  Response Generation in Dialogues", "abstract": "Code-mixing, the blending of multiple languages within a single conversation,\nintroduces a distinctive challenge, particularly in the context of response\ngeneration. Capturing the intricacies of code-mixing proves to be a formidable\ntask, given the wide-ranging variations influenced by individual speaking\nstyles and cultural backgrounds. In this study, we explore response generation\nwithin code-mixed conversations. We introduce a novel approach centered on\nharnessing the Big Five personality traits acquired in an unsupervised manner\nfrom the conversations to bolster the performance of response generation. These\ninferred personality attributes are seamlessly woven into the fabric of the\ndialogue context, using a novel fusion mechanism, PA3. It uses an effective\ntwo-step attention formulation to fuse the dialogue and personality\ninformation. This fusion not only enhances the contextual relevance of\ngenerated responses but also elevates the overall performance of the model. Our\nexperimental results, grounded in a dataset comprising of multi-party\nHindi-English code-mix conversations, highlight the substantial advantages\noffered by personality-infused models over their conventional counterparts.\nThis is evident in the increase observed in ROUGE and BLUE scores for the\nresponse generation task when the identified personality is seamlessly\nintegrated into the dialogue context. Qualitative assessment for personality\nidentification and response generation aligns well with our quantitative\nresults.", "published": "2024-01-18 15:21:16", "link": "http://arxiv.org/abs/2401.12995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interplay of Semantic Communication and Knowledge Learning", "abstract": "In the swiftly advancing realm of communication technologies, Semantic\nCommunication (SemCom), which emphasizes knowledge understanding and\nprocessing, has emerged as a hot topic. By integrating artificial intelligence\ntechnologies, SemCom facilitates a profound understanding, analysis and\ntransmission of communication content. In this chapter, we clarify the means of\nknowledge learning in SemCom with a particular focus on the utilization of\nKnowledge Graphs (KGs). Specifically, we first review existing efforts that\ncombine SemCom with knowledge learning. Subsequently, we introduce a\nKG-enhanced SemCom system, wherein the receiver is carefully calibrated to\nleverage knowledge from its static knowledge base for ameliorating the decoding\nperformance. Contingent upon this framework, we further explore potential\napproaches that can empower the system to operate in evolving knowledge base\nmore effectively. Furthermore, we investigate the possibility of integration\nwith Large Language Models (LLMs) for data augmentation, offering additional\nperspective into the potential implementation means of SemCom. Extensive\nnumerical results demonstrate that the proposed framework yields superior\nperformance on top of the KG-enhanced decoding and manifests its versatility\nunder different scenarios.", "published": "2024-01-18 06:11:06", "link": "http://arxiv.org/abs/2402.03339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum Recommendations Using Transformer Base Model with InfoNCE\n  Loss And Language Switching Method", "abstract": "The Curriculum Recommendations paradigm is dedicated to fostering learning\nequality within the ever-evolving realms of educational technology and\ncurriculum development. In acknowledging the inherent obstacles posed by\nexisting methodologies, such as content conflicts and disruptions from language\ntranslation, this paradigm aims to confront and overcome these challenges.\nNotably, it addresses content conflicts and disruptions introduced by language\ntranslation, hindrances that can impede the creation of an all-encompassing and\npersonalized learning experience. The paradigm's objective is to cultivate an\neducational environment that not only embraces diversity but also customizes\nlearning experiences to suit the distinct needs of each learner. To overcome\nthese challenges, our approach builds upon notable contributions in curriculum\ndevelopment and personalized learning, introducing three key innovations. These\ninclude the integration of Transformer Base Model to enhance computational\nefficiency, the implementation of InfoNCE Loss for accurate content-topic\nmatching, and the adoption of a language switching strategy to alleviate\ntranslation-related ambiguities. Together, these innovations aim to\ncollectively tackle inherent challenges and contribute to forging a more\nequitable and effective learning journey for a diverse range of learners.\nCompetitive cross-validation scores underscore the efficacy of\nsentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology's\neffectiveness in diverse linguistic nuances for content alignment prediction.\nIndex Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss,\nLanguage Switching.", "published": "2024-01-18 03:09:06", "link": "http://arxiv.org/abs/2401.09699v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Predicting Viral Rumors and Vulnerable Users for Infodemic Surveillance", "abstract": "In the age of the infodemic, it is crucial to have tools for effectively\nmonitoring the spread of rampant rumors that can quickly go viral, as well as\nidentifying vulnerable users who may be more susceptible to spreading such\nmisinformation. This proactive approach allows for timely preventive measures\nto be taken, mitigating the negative impact of false information on society. We\npropose a novel approach to predict viral rumors and vulnerable users using a\nunified graph neural network model. We pre-train network-based user embeddings\nand leverage a cross-attention mechanism between users and posts, together with\na community-enhanced vulnerability propagation (CVP) method to improve user and\npropagation graph representations. Furthermore, we employ two multi-task\ntraining strategies to mitigate negative transfer effects among tasks in\ndifferent settings, enhancing the overall performance of our approach. We also\nconstruct two datasets with ground-truth annotations on information virality\nand user vulnerability in rumor and non-rumor events, which are automatically\nderived from existing rumor detection datasets. Extensive evaluation results of\nour joint learning model confirm its superiority over strong baselines in all\nthree tasks: rumor detection, virality prediction, and user vulnerability\nscoring. For instance, compared to the best baselines based on the Weibo\ndataset, our model makes 3.8\\% and 3.0\\% improvements on Accuracy and MacF1 for\nrumor detection, and reduces mean squared error (MSE) by 23.9\\% and 16.5\\% for\nvirality prediction and user vulnerability scoring, respectively. Our findings\nsuggest that our approach effectively captures the correlation between rumor\nvirality and user vulnerability, leveraging this information to improve\nprediction performance and provide a valuable tool for infodemic surveillance.", "published": "2024-01-18 04:57:12", "link": "http://arxiv.org/abs/2401.09724v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Large Language Model Lateral Spear Phishing: A Comparative Study in\n  Large-Scale Organizational Settings", "abstract": "The critical threat of phishing emails has been further exacerbated by the\npotential of LLMs to generate highly targeted, personalized, and automated\nspear phishing attacks. Two critical problems concerning LLM-facilitated\nphishing require further investigation: 1) Existing studies on lateral phishing\nlack specific examination of LLM integration for large-scale attacks targeting\nthe entire organization, and 2) Current anti-phishing infrastructure, despite\nits extensive development, lacks the capability to prevent LLM-generated\nattacks, potentially impacting both employees and IT security incident\nmanagement. However, the execution of such investigative studies necessitates a\nreal-world environment, one that functions during regular business operations\nand mirrors the complexity of a large organizational infrastructure. This\nsetting must also offer the flexibility required to facilitate a diverse array\nof experimental conditions, particularly the incorporation of phishing emails\ncrafted by LLMs. This study is a pioneering exploration into the use of Large\nLanguage Models (LLMs) for the creation of targeted lateral phishing emails,\ntargeting a large tier 1 university's operation and workforce of approximately\n9,000 individuals over an 11-month period. It also evaluates the capability of\nemail filtering infrastructure to detect such LLM-generated phishing attempts,\nproviding insights into their effectiveness and identifying potential areas for\nimprovement. Based on our findings, we propose machine learning-based detection\ntechniques for such emails to detect LLM-generated phishing emails that were\nmissed by the existing infrastructure, with an F1-score of 98.96.", "published": "2024-01-18 05:06:39", "link": "http://arxiv.org/abs/2401.09727v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A Comparative Study on Annotation Quality of Crowdsourcing and LLM via\n  Label Aggregation", "abstract": "Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.", "published": "2024-01-18 07:23:51", "link": "http://arxiv.org/abs/2401.09760v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and\n  Visual Question Generation", "abstract": "The increasing demand for intelligent systems capable of interpreting and\nreasoning about visual content requires the development of large\nVision-and-Language Models (VLMs) that are not only accurate but also have\nexplicit reasoning capabilities. This paper presents a novel approach to\ndevelop a VLM with the ability to conduct explicit reasoning based on visual\ncontent and textual instructions. We introduce a system that can ask a question\nto acquire necessary knowledge, thereby enhancing the robustness and\nexplicability of the reasoning process. To this end, we developed a novel\ndataset generated by a Large Language Model (LLM), designed to promote\nchain-of-thought reasoning combined with a question-asking mechanism. The\ndataset covers a range of tasks, from common ones like caption generation to\nspecialized VQA tasks that require expert knowledge. Furthermore, using the\ndataset we created, we fine-tuned an existing VLM. This training enabled the\nmodels to generate questions and perform iterative reasoning during inference.\nThe results demonstrated a stride toward a more robust, accurate, and\ninterpretable VLM, capable of reasoning explicitly and seeking information\nproactively when confronted with ambiguous visual input.", "published": "2024-01-18 14:21:56", "link": "http://arxiv.org/abs/2401.10005v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Hierarchical Spoken Language Dysfluency Modeling", "abstract": "Speech disfluency modeling is the bottleneck for both speech therapy and\nlanguage learning. However, there is no effective AI solution to systematically\ntackle this problem. We solidify the concept of disfluent speech and disfluent\nspeech modeling. We then present Hierarchical Unconstrained Disfluency Modeling\n(H-UDM) approach, the hierarchical extension of UDM that addresses both\ndisfluency transcription and detection to eliminate the need for extensive\nmanual annotation. Our experimental findings serve as clear evidence of the\neffectiveness and reliability of the methods we have introduced, encompassing\nboth transcription and detection tasks.", "published": "2024-01-18 14:33:01", "link": "http://arxiv.org/abs/2401.10015v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "abstract": "Large language models (LLMs) have exhibited great potential in autonomously\ncompleting tasks across real-world applications. Despite this, these LLM agents\nintroduce unexpected safety risks when operating in interactive environments.\nInstead of centering on the harmlessness of LLM-generated content in most prior\nstudies, this work addresses the imperative need for benchmarking the\nbehavioral safety of LLM agents within diverse environments. We introduce\nR-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and\nidentifying safety risks given agent interaction records. R-Judge comprises 569\nrecords of multi-turn agent interaction, encompassing 27 key risk scenarios\namong 5 application categories and 10 risk types. It is of high-quality\ncuration with annotated safety labels and risk descriptions. Evaluation of 11\nLLMs on R-Judge shows considerable room for enhancing the risk awareness of\nLLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models\nsignificantly exceed the random. Moreover, we reveal that risk awareness in\nopen agent scenarios is a multi-dimensional capability involving knowledge and\nreasoning, thus challenging for LLMs. With further experiments, we find that\nfine-tuning on safety judgment significantly improve model performance while\nstraightforward prompting mechanisms fail. R-Judge is publicly available at\nhttps://github.com/Lordog/R-Judge.", "published": "2024-01-18 14:40:46", "link": "http://arxiv.org/abs/2401.10019v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Rewarding Language Models", "abstract": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes.", "published": "2024-01-18 14:43:47", "link": "http://arxiv.org/abs/2401.10020v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Framing Analysis of Health-Related Narratives: Conspiracy versus\n  Mainstream Media", "abstract": "Understanding how online media frame issues is crucial due to their impact on\npublic opinion. Research on framing using natural language processing\ntechniques mainly focuses on specific content features in messages and neglects\ntheir narrative elements. Also, the distinction between framing in different\nsources remains an understudied problem. We address those issues and\ninvestigate how the framing of health-related topics, such as COVID-19 and\nother diseases, differs between conspiracy and mainstream websites. We\nincorporate narrative information into the framing analysis by introducing a\nnovel frame extraction approach based on semantic graphs. We find that\nhealth-related narratives in conspiracy media are predominantly framed in terms\nof beliefs, while mainstream media tend to present them in terms of science. We\nhope our work offers new ways for a more nuanced frame analysis.", "published": "2024-01-18 14:56:23", "link": "http://arxiv.org/abs/2401.10030v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Spatial-Temporal Large Language Model for Traffic Prediction", "abstract": "Traffic prediction, an essential component for intelligent transportation\nsystems, endeavours to use historical data to foresee future traffic features\nat specific locations. Although existing traffic prediction models often\nemphasize developing complex neural network structures, their accuracy has not\nimproved. Recently, large language models have shown outstanding capabilities\nin time series analysis. Differing from existing models, LLMs progress mainly\nthrough parameter expansion and extensive pretraining while maintaining their\nfundamental structures. Motivated by these developments, we propose a\nSpatial-Temporal Large Language Model (ST-LLM) for traffic prediction. In the\nST-LLM, we define timesteps at each location as tokens and design a\nspatial-temporal embedding to learn the spatial location and global temporal\npatterns of these tokens. Additionally, we integrate these embeddings by a\nfusion convolution to each token for a unified spatial-temporal representation.\nFurthermore, we innovate a partially frozen attention strategy to adapt the LLM\nto capture global spatial-temporal dependencies for traffic prediction.\nComprehensive experiments on real traffic datasets offer evidence that ST-LLM\nis a powerful spatial-temporal learner that outperforms state-of-the-art\nmodels. Notably, the ST-LLM also exhibits robust performance in both few-shot\nand zero-shot prediction scenarios. The code is publicly available at\nhttps://github.com/ChenxiLiu-HNU/ST-LLM.", "published": "2024-01-18 17:03:59", "link": "http://arxiv.org/abs/2401.10134v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MM-Interleaved: Interleaved Image-Text Generative Modeling via\n  Multi-modal Feature Synchronizer", "abstract": "Developing generative models for interleaved image-text data has both\nresearch and practical value. It requires models to understand the interleaved\nsequences and subsequently generate images and text. However, existing attempts\nare limited by the issue that the fixed number of visual tokens cannot\nefficiently capture image details, which is particularly problematic in the\nmulti-image scenarios. To address this, this paper presents MM-Interleaved, an\nend-to-end generative model for interleaved image-text data. It introduces a\nmulti-scale and multi-image feature synchronizer module, allowing direct access\nto fine-grained image features in the previous context during the generation\nprocess. MM-Interleaved is end-to-end pre-trained on both paired and\ninterleaved image-text corpora. It is further enhanced through a supervised\nfine-tuning phase, wherein the model improves its ability to follow complex\nmulti-modal instructions. Experiments demonstrate the versatility of\nMM-Interleaved in recognizing visual details following multi-modal instructions\nand generating consistent images following both textual and visual conditions.\nCode and models are available at\n\\url{https://github.com/OpenGVLab/MM-Interleaved}.", "published": "2024-01-18 18:50:16", "link": "http://arxiv.org/abs/2401.10208v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can Large Language Model Summarizers Adapt to Diverse Scientific\n  Communication Goals?", "abstract": "In this work, we investigate the controllability of large language models\n(LLMs) on scientific summarization tasks. We identify key stylistic and content\ncoverage factors that characterize different types of summaries such as paper\nreviews, abstracts, and lay summaries. By controlling stylistic features, we\nfind that non-fine-tuned LLMs outperform humans in the MuP review generation\ntask, both in terms of similarity to reference summaries and human preferences.\nAlso, we show that we can improve the controllability of LLMs with\nkeyword-based classifier-free guidance (CFG) while achieving lexical overlap\ncomparable to strong fine-tuned baselines on arXiv and PubMed. However, our\nresults also indicate that LLMs cannot consistently generate long summaries\nwith more than 8 sentences. Furthermore, these models exhibit limited capacity\nto produce highly abstractive lay summaries. Although LLMs demonstrate strong\ngeneric summarization competency, sophisticated content control without costly\nfine-tuning remains an open problem for domain-specific applications.", "published": "2024-01-18 23:00:54", "link": "http://arxiv.org/abs/2401.10415v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks", "abstract": "Large Language Models (LLMs), such as ChatGPT, encounter `jailbreak'\nchallenges, wherein safeguards are circumvented to generate ethically harmful\nprompts. This study introduces a straightforward black-box method for\nefficiently crafting jailbreak prompts, addressing the significant complexity\nand computational costs associated with conventional methods. Our technique\niteratively transforms harmful prompts into benign expressions directly\nutilizing the target LLM, predicated on the hypothesis that LLMs can\nautonomously generate expressions that evade safeguards. Through experiments\nconducted with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, our method\nconsistently achieved an attack success rate exceeding 80% within an average of\nfive iterations for forbidden questions and proved robust against model\nupdates. The jailbreak prompts generated were not only naturally-worded and\nsuccinct but also challenging to defend against. These findings suggest that\nthe creation of effective jailbreak prompts is less complex than previously\nbelieved, underscoring the heightened risk posed by black-box jailbreak\nattacks.", "published": "2024-01-18 08:36:54", "link": "http://arxiv.org/abs/2401.09798v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation\n  Extraction for Material Science Knowledge-base Construction", "abstract": "Material science literature is a rich source of factual information about\nvarious categories of entities (like materials and compositions) and various\nrelations between these entities, such as conductivity, voltage, etc.\nAutomatically extracting this information to generate a material science\nknowledge base is a challenging task. In this paper, we propose MatSciRE\n(Material Science Relation Extractor), a Pointer Network-based encoder-decoder\nframework, to jointly extract entities and relations from material science\narticles as a triplet ($entity1, relation, entity2$). Specifically, we target\nthe battery materials and identify five relations to work on - conductivity,\ncoulombic efficiency, capacity, voltage, and energy. Our proposed approach\nachieved a much better F1-score (0.771) than a previous attempt using\nChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown\nin Fig 1. The material information is extracted from material science\nliterature in the form of entity-relation triplets using MatSciRE.", "published": "2024-01-18 09:54:18", "link": "http://arxiv.org/abs/2401.09839v1", "categories": ["cs.CL", "cs.CE", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evolutionary Multi-Objective Optimization of Large Language Model\n  Prompts for Balancing Sentiments", "abstract": "The advent of large language models (LLMs) such as ChatGPT has attracted\nconsiderable attention in various domains due to their remarkable performance\nand versatility. As the use of these models continues to grow, the importance\nof effective prompt engineering has come to the fore. Prompt optimization\nemerges as a crucial challenge, as it has a direct impact on model performance\nand the extraction of relevant information. Recently, evolutionary algorithms\n(EAs) have shown promise in addressing this issue, paving the way for novel\noptimization strategies. In this work, we propose a evolutionary\nmulti-objective (EMO) approach specifically tailored for prompt optimization\ncalled EMO-Prompts, using sentiment analysis as a case study. We use sentiment\nanalysis capabilities as our experimental targets. Our results demonstrate that\nEMO-Prompts effectively generates prompts capable of guiding the LLM to produce\ntexts embodying two conflicting emotions simultaneously.", "published": "2024-01-18 10:21:15", "link": "http://arxiv.org/abs/2401.09862v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "A Survey on Hardware Accelerators for Large Language Models", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. As the demand for more sophisticated\nLLMs continues to grow, there is a pressing need to address the computational\nchallenges associated with their scale and complexity. This paper presents a\ncomprehensive survey on hardware accelerators designed to enhance the\nperformance and energy efficiency of Large Language Models. By examining a\ndiverse range of accelerators, including GPUs, FPGAs, and custom-designed\narchitectures, we explore the landscape of hardware solutions tailored to meet\nthe unique computational demands of LLMs. The survey encompasses an in-depth\nanalysis of architecture, performance metrics, and energy efficiency\nconsiderations, providing valuable insights for researchers, engineers, and\ndecision-makers aiming to optimize the deployment of LLMs in real-world\napplications.", "published": "2024-01-18 11:05:03", "link": "http://arxiv.org/abs/2401.09890v1", "categories": ["cs.AR", "cs.CL", "cs.LG", "B.5; C.1; C.3"], "primary_category": "cs.AR"}
{"title": "Gender Bias in Machine Translation and The Era of Large Language Models", "abstract": "This chapter examines the role of Machine Translation in perpetuating gender\nbias, highlighting the challenges posed by cross-linguistic settings and\nstatistical dependencies. A comprehensive overview of relevant existing work\nrelated to gender bias in both conventional Neural Machine Translation\napproaches and Generative Pretrained Transformer models employed as Machine\nTranslation systems is provided. Through an experiment using ChatGPT (based on\nGPT-3.5) in an English-Italian translation context, we further assess ChatGPT's\ncurrent capacity to address gender bias. The findings emphasize the ongoing\nneed for advancements in mitigating bias in Machine Translation systems and\nunderscore the importance of fostering fairness and inclusivity in language\ntechnologies.", "published": "2024-01-18 14:34:49", "link": "http://arxiv.org/abs/2401.10016v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evolutionary Computation in the Era of Large Language Model: Survey and\n  Roadmap", "abstract": "Large language models (LLMs) have not only revolutionized natural language\nprocessing but also extended their prowess to various domains, marking a\nsignificant stride towards artificial general intelligence. The interplay\nbetween LLMs and evolutionary algorithms (EAs), despite differing in objectives\nand methodologies, share a common pursuit of applicability in complex problems.\nMeanwhile, EA can provide an optimization framework for LLM's further\nenhancement under black-box settings, empowering LLM with flexible global\nsearch capacities. On the other hand, the abundant domain knowledge inherent in\nLLMs could enable EA to conduct more intelligent searches. Furthermore, the\ntext processing and generative capabilities of LLMs would aid in deploying EAs\nacross a wide range of tasks. Based on these complementary advantages, this\npaper provides a thorough review and a forward-looking roadmap, categorizing\nthe reciprocal inspiration into two main avenues: LLM-enhanced EA and\nEA-enhanced LLM. Some integrated synergy methods are further introduced to\nexemplify the complementarity between LLMs and EAs in diverse scenarios,\nincluding code generation, software engineering, neural architecture search,\nand various generation tasks. As the first comprehensive review focused on the\nEA research in the era of LLMs, this paper provides a foundational stepping\nstone for understanding the collaborative potential of LLMs and EAs. The\nidentified challenges and future directions offer guidance for researchers and\npractitioners to unlock the full potential of this innovative collaboration in\npropelling advancements in optimization and artificial intelligence. We have\ncreated a GitHub repository to index the relevant papers:\nhttps://github.com/wuxingyu-ai/LLM4EC.", "published": "2024-01-18 14:58:17", "link": "http://arxiv.org/abs/2401.10034v3", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Communication-Efficient Personalized Federated Learning for\n  Speech-to-Text Tasks", "abstract": "To protect privacy and meet legal regulations, federated learning (FL) has\ngained significant attention for training speech-to-text (S2T) systems,\nincluding automatic speech recognition (ASR) and speech translation (ST).\nHowever, the commonly used FL approach (i.e., \\textsc{FedAvg}) in S2T tasks\ntypically suffers from extensive communication overhead due to multi-round\ninteractions based on the whole model and performance degradation caused by\ndata heterogeneity among clients.To address these issues, we propose a\npersonalized federated S2T framework that introduces \\textsc{FedLoRA}, a\nlightweight LoRA module for client-side tuning and interaction with the server\nto minimize communication overhead, and \\textsc{FedMem}, a global model\nequipped with a $k$-nearest-neighbor ($k$NN) classifier that captures\nclient-specific distributional shifts to achieve personalization and overcome\ndata heterogeneity. Extensive experiments based on Conformer and Whisper\nbackbone models on CoVoST and GigaSpeech benchmarks show that our approach\nsignificantly reduces the communication overhead on all S2T tasks and\neffectively personalizes the global model to overcome data heterogeneity.", "published": "2024-01-18 15:39:38", "link": "http://arxiv.org/abs/2401.10070v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through\n  Text Reconstruction", "abstract": "Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.", "published": "2024-01-18 18:20:15", "link": "http://arxiv.org/abs/2401.10189v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG", "abstract": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on\nretrieval-augmented generation (RAG) and conversational question answering\n(QA). To enhance generation, we propose a two-stage instruction tuning method\nthat significantly boosts the performance of RAG. For effective retrieval, we\nintroduce a dense retriever optimized for conversational QA, which yields\nresults comparable to the alternative state-of-the-art query rewriting models,\nwhile substantially reducing deployment costs. We also present the ChatRAG\nBench, which encompasses ten datasets covering comprehensive evaluations on\nRAG, table-related QA, arithmetic calculations, and scenarios involving\nunanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a\nweaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:\n53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without\nrelying on any synthetic data from OpenAI GPT models. Notably, the\nLlama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,\nachieving a 4.4% improvement. To advance research in this field, we\nopen-sourced the model weights, instruction tuning data, ChatRAG Bench, and\nretriever for the community: https://chatqa-project.github.io/.", "published": "2024-01-18 18:59:11", "link": "http://arxiv.org/abs/2401.10225v5", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Noise Contrastive Estimation-based Matching Framework for Low-Resource\n  Security Attack Pattern Recognition", "abstract": "Tactics, Techniques and Procedures (TTPs) represent sophisticated attack\npatterns in the cybersecurity domain, described encyclopedically in textual\nknowledge bases. Identifying TTPs in cybersecurity writing, often called TTP\nmapping, is an important and challenging task. Conventional learning approaches\noften target the problem in the classical multi-class or multilabel\nclassification setting. This setting hinders the learning ability of the model\ndue to a large number of classes (i.e., TTPs), the inevitable skewness of the\nlabel distribution and the complex hierarchical structure of the label space.\nWe formulate the problem in a different learning paradigm, where the assignment\nof a text to a TTP label is decided by the direct semantic similarity between\nthe two, thus reducing the complexity of competing solely over the large\nlabeling space. To that end, we propose a neural matching architecture with an\neffective sampling-based learn-to-compare mechanism, facilitating the learning\nprocess of the matching model despite constrained resources.", "published": "2024-01-18 19:02:00", "link": "http://arxiv.org/abs/2401.10337v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Comparison of Veterans with Problematic Opioid Use Identified through\n  Natural Language Processing of Clinical Notes versus Using Diagnostic Codes", "abstract": "Background: Electronic health records (EHRs) are a data source for opioid\nresearch. Opioid use disorder is known to be under-coded as a diagnosis, yet\nproblematic opioid use can be documented in clinical notes.\n  Objectives: Our goals were 1) to identify problematic opioid use from a full\nrange of clinical notes; and 2) to compare the characteristics of patients\nidentified as having problematic opioid use, exclusively documented in clinical\nnotes, to those having documented ICD opioid use disorder diagnostic codes.\n  Materials and Methods: We developed and applied a natural language processing\n(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two\nVeteran Affairs service regions to identify patients with problematic opioid\nuse. We also used a set of ICD diagnostic codes to identify patients with\nopioid use disorder from the same cohort. We compared the demographic and\nclinical characteristics of patients identified only through NLP, to those of\npatients identified through ICD codes.\n  Results: NLP exclusively identified 57,331 patients; 6,997 patients had\npositive ICD code identifications. Patients exclusively identified through NLP\nwere more likely to be women. Those identified through ICD codes were more\nlikely to be male, younger, have concurrent benzodiazepine prescriptions, more\ncomorbidities, more care encounters, and less likely to be married. Patients in\nthe NLP and ICD groups had substantially elevated comorbidity levels compared\nto patients not documented as experiencing problematic opioid use.\n  Conclusions: NLP is a feasible approach for identifying problematic opioid\nuse not otherwise recorded by ICD codes. Clinicians may be reluctant to code\nfor opioid use disorder. It is therefore incumbent on the healthcare team to\nsearch for documentation of opioid concerns within clinical notes.", "published": "2024-01-18 18:08:16", "link": "http://arxiv.org/abs/2401.12996v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "J.3"], "primary_category": "cs.CL"}
{"title": "Veagle: Advancements in Multimodal Representation Learning", "abstract": "Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.", "published": "2024-01-18 12:45:25", "link": "http://arxiv.org/abs/2403.08773v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "On the Audio Hallucinations in Large Audio-Video Language Models", "abstract": "Large audio-video language models can generate descriptions for both video\nand audio. However, they sometimes ignore audio content, producing audio\ndescriptions solely reliant on visual information. This paper refers to this as\naudio hallucinations and analyzes them in large audio-video language models. We\ngather 1,000 sentences by inquiring about audio information and annotate them\nwhether they contain hallucinations. If a sentence is hallucinated, we also\ncategorize the type of hallucination. The results reveal that 332 sentences are\nhallucinated with distinct trends observed in nouns and verbs for each\nhallucination type. Based on this, we tackle a task of audio hallucination\nclassification using pre-trained audio-text models in the zero-shot and\nfine-tuning settings. Our experimental results reveal that the zero-shot models\nachieve higher performance (52.2% in F1) than the random (40.3%) and the\nfine-tuning models achieve 87.9%, outperforming the zero-shot models.", "published": "2024-01-18 07:50:07", "link": "http://arxiv.org/abs/2401.09774v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Attention-Based Recurrent Neural Network For Automatic Behavior Laying\n  Hen Recognition", "abstract": "One of the interests of modern poultry farming is the vocalization of laying\nhens which contain very useful information on health behavior. This information\nis used as health and well-being indicators that help breeders better monitor\nlaying hens, which involves early detection of problems for rapid and more\neffective intervention. In this work, we focus on the sound analysis for the\nrecognition of the types of calls of the laying hens in order to propose a\nrobust system of characterization of their behavior for a better monitoring. To\ndo this, we first collected and annotated laying hen call signals, then\ndesigned an optimal acoustic characterization based on the combination of time\nand frequency domain features. We then used these features to build the\nmulti-label classification models based on recurrent neural network to assign a\nsemantic class to the vocalization that characterize the laying hen behavior.\nThe results show an overall performance with our model based on the combination\nof time and frequency domain features that obtained the highest F1-score\n(F1=92.75) with a gain of 17% on the models using the frequency domain features\nand of 8% on the compared approaches from the litterature.", "published": "2024-01-18 10:52:46", "link": "http://arxiv.org/abs/2401.09880v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Language Models for Scientific Information Extraction: An\n  Empirical Study for Virology", "abstract": "In this paper, we champion the use of structured and semantic content\nrepresentation of discourse-based scholarly communication, inspired by tools\nlike Wikipedia infoboxes or structured Amazon product descriptions. These\nrepresentations provide users with a concise overview, aiding scientists in\nnavigating the dense academic landscape. Our novel automated approach leverages\nthe robust text generation capabilities of LLMs to produce structured scholarly\ncontribution summaries, offering both a practical solution and insights into\nLLMs' emergent abilities.\n  For LLMs, the prime focus is on improving their general intelligence as\nconversational agents. We argue that these models can also be applied\neffectively in information extraction (IE), specifically in complex IE tasks\nwithin terse domains like Science. This paradigm shift replaces the traditional\nmodular, pipelined machine learning approach with a simpler objective expressed\nthrough instructions. Our results show that finetuned FLAN-T5 with 1000x fewer\nparameters than the state-of-the-art GPT-davinci is competitive for the task.", "published": "2024-01-18 15:04:55", "link": "http://arxiv.org/abs/2401.10040v1", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on the Impact of Positional Encoding in\n  Transformer-based Monaural Speech Enhancement", "abstract": "Transformer architecture has enabled recent progress in speech enhancement.\nSince Transformers are position-agostic, positional encoding is the de facto\nstandard component used to enable Transformers to distinguish the order of\nelements in a sequence. However, it remains unclear how positional encoding\nexactly impacts speech enhancement based on Transformer architectures. In this\npaper, we perform a comprehensive empirical study evaluating five positional\nencoding methods, i.e., Sinusoidal and learned absolute position embedding\n(APE), T5-RPE, KERPLE, as well as the Transformer without positional encoding\n(No-Pos), across both causal and noncausal configurations. We conduct extensive\nspeech enhancement experiments, involving spectral mapping and masking methods.\nOur findings establish that positional encoding is not quite helpful for the\nmodels in a causal configuration, which indicates that causal attention may\nimplicitly incorporate position information. In a noncausal configuration, the\nmodels significantly benefit from the use of positional encoding. In addition,\nwe find that among the four position embeddings, relative position embeddings\noutperform APEs.", "published": "2024-01-18 02:31:46", "link": "http://arxiv.org/abs/2401.09686v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AGADIR: Towards Array-Geometry Agnostic Directional Speech Recognition", "abstract": "Wearable devices like smart glasses are approaching the compute capability to\nseamlessly generate real-time closed captions for live conversations. We build\non our recently introduced directional Automatic Speech Recognition (ASR) for\nsmart glasses that have microphone arrays, which fuses multi-channel ASR with\nserialized output training, for wearer/conversation-partner disambiguation as\nwell as suppression of cross-talk speech from non-target directions and noise.\n  When ASR work is part of a broader system-development process, one may be\nfaced with changes to microphone geometries as system development progresses.\n  This paper aims to make multi-channel ASR insensitive to limited variations\nof microphone-array geometry. We show that a model trained on multiple similar\ngeometries is largely agnostic and generalizes well to new geometries, as long\nas they are not too different. Furthermore, training the model this way\nimproves accuracy for seen geometries by 15 to 28\\% relative. Lastly, we refine\nthe beamforming by a novel Non-Linearly Constrained Minimum Variance criterion.", "published": "2024-01-18 22:45:06", "link": "http://arxiv.org/abs/2401.10411v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parameter Selection for Analyzing Conversations with Autism Spectrum\n  Disorder", "abstract": "The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.", "published": "2024-01-18 04:28:56", "link": "http://arxiv.org/abs/2401.09717v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speaker-independent Speech Emotion Recognition Using Dynamic\n  Joint Distribution Adaptation", "abstract": "In speaker-independent speech emotion recognition, the training and testing\nsamples are collected from diverse speakers, leading to a multi-domain shift\nchallenge across the feature distributions of data from different speakers.\nConsequently, when the trained model is confronted with data from new speakers,\nits performance tends to degrade. To address the issue, we propose a Dynamic\nJoint Distribution Adaptation (DJDA) method under the framework of multi-source\ndomain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA),\ninvolving marginal distribution adaptation (MDA) and conditional distribution\nadaptation (CDA), to more precisely measure the multi-domain distribution\nshifts caused by different speakers. This helps eliminate speaker bias in\nemotion features, allowing for learning discriminative and speaker-invariant\nspeech emotion features from coarse-level to fine-level. Furthermore, we\nquantify the adaptation contributions of MDA and CDA within JDA by using a\ndynamic balance factor based on $\\mathcal{A}$-Distance, promoting to\neffectively handle the unknown distributions encountered in data from new\nspeakers. Experimental results demonstrate the superior performance of our DJDA\nas compared to other state-of-the-art (SOTA) methods.", "published": "2024-01-18 06:52:52", "link": "http://arxiv.org/abs/2401.09752v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech\n  Recognition", "abstract": "Audio-visual speech recognition (AVSR) is a multimodal extension of automatic\nspeech recognition (ASR), using video as a complement to audio. In AVSR,\nconsiderable efforts have been directed at datasets for facial features such as\nlip-readings, while they often fall short in evaluating the image comprehension\ncapabilities in broader contexts. In this paper, we construct SlideAVSR, an\nAVSR dataset using scientific paper explanation videos. SlideAVSR provides a\nnew benchmark where models transcribe speech utterances with texts on the\nslides on the presentation recordings. As technical terminologies that are\nfrequent in paper explanations are notoriously challenging to transcribe\nwithout reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR\nproblems. As a simple yet effective baseline, we propose DocWhisper, an AVSR\nmodel that can refer to textual information from slides, and confirm its\neffectiveness on SlideAVSR.", "published": "2024-01-18 07:19:10", "link": "http://arxiv.org/abs/2401.09759v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Efficient Training for Multilingual Visual Speech Recognition:\n  Pre-training with Discretized Visual Speech Representation", "abstract": "This paper explores sentence-level multilingual Visual Speech Recognition\n(VSR) that can recognize different languages with a single trained model. As\nthe massive multilingual modeling of visual data requires huge computational\ncosts, we propose a novel training strategy, processing with visual speech\nunits. Motivated by the recent success of the audio speech unit, we propose to\nuse a visual speech unit that can be obtained by discretizing the visual speech\nfeatures extracted from the self-supervised visual speech model. Through\nanalysis, we verify that the visual speech units mainly contain viseme\ninformation while suppressing non-linguistic information. By using the visual\nspeech units as the inputs of our system, we propose to pre-train a VSR model\nto predict corresponding text outputs on multilingual data constructed by\nmerging several VSR databases. As both the inputs (i.e., visual speech units)\nand outputs (i.e., text) are discrete, we can greatly improve the training\nefficiency compared to the standard VSR training. Specifically, the input data\nsize is reduced to 0.016% of the original video inputs. In order to complement\nthe insufficient visual information in speech recognition, we apply curriculum\nlearning where the inputs of the system begin with audio-visual speech units\nand gradually change to visual speech units. After pre-training, the model is\nfinetuned on continuous features. We set new state-of-the-art multilingual VSR\nperformances by achieving comparable performances to the previous\nlanguage-specific VSR models, with a single trained model.", "published": "2024-01-18 08:46:02", "link": "http://arxiv.org/abs/2401.09802v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Developing an AI-based Integrated System for Bee Health Evaluation", "abstract": "Honey bees pollinate about one-third of the world's food supply, but bee\ncolonies have alarmingly declined by nearly 40% over the past decade due to\nseveral factors, including pesticides and pests. Traditional methods for\nmonitoring beehives, such as human inspection, are subjective, disruptive, and\ntime-consuming. To overcome these limitations, artificial intelligence has been\nused to assess beehive health. However, previous studies have lacked an\nend-to-end solution and primarily relied on data from a single source, either\nbee images or sounds. This study introduces a comprehensive system consisting\nof bee object detection and health evaluation. Additionally, it utilized a\ncombination of visual and audio signals to analyze bee behaviors. An\nAttention-based Multimodal Neural Network (AMNN) was developed to adaptively\nfocus on key features from each type of signal for accurate bee health\nassessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight\nexisting single-signal Convolutional Neural Networks and Recurrent Neural\nNetworks. It outperformed the best image-based model by 32.51% and the top\nsound-based model by 13.98% while maintaining efficient processing times.\nFurthermore, it improved prediction robustness, attaining an F1-score higher\nthan 90% across all four evaluated health conditions. The study also shows that\naudio signals are more reliable than images for assessing bee health. By\nseamlessly integrating AMNN with image and sound data in a comprehensive bee\nhealth monitoring system, this approach provides a more efficient and\nnon-invasive solution for the early detection of bee diseases and the\npreservation of bee colonies.", "published": "2024-01-18 14:06:29", "link": "http://arxiv.org/abs/2401.09988v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder", "abstract": "The goal of this paper is to generate realistic audio with a lightweight and\nfast diffusion-based vocoder, named FreGrad. Our framework consists of the\nfollowing three key components: (1) We employ discrete wavelet transform that\ndecomposes a complicated waveform into sub-band wavelets, which helps FreGrad\nto operate on a simple and concise feature space, (2) We design a\nfrequency-aware dilated convolution that elevates frequency awareness,\nresulting in generating speech with accurate frequency information, and (3) We\nintroduce a bag of tricks that boosts the generation quality of the proposed\nmodel. In our experiments, FreGrad achieves 3.7 times faster training time and\n2.2 times faster inference speed compared to our baseline while reducing the\nmodel size by 0.6 times (only 1.78M parameters) without sacrificing the output\nquality. Audio samples are available at:\nhttps://mm.kaist.ac.kr/projects/FreGrad.", "published": "2024-01-18 14:57:25", "link": "http://arxiv.org/abs/2401.10032v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
