{"title": "An attention-based Bi-GRU-CapsNet model for hypernymy detection between\n  compound entities", "abstract": "Named entities are usually composable and extensible. Typical examples are\nnames of symptoms and diseases in medical areas. To distinguish these entities\nfrom general entities, we name them \\textit{compound entities}. In this paper,\nwe present an attention-based Bi-GRU-CapsNet model to detect hypernymy\nrelationship between compound entities. Our model consists of several important\ncomponents. To avoid the out-of-vocabulary problem, English words or Chinese\ncharacters in compound entities are fed into the bidirectional gated recurrent\nunits. An attention mechanism is designed to focus on the differences between\nthe two compound entities. Since there are some different cases in hypernymy\nrelationship between compound entities, capsule network is finally employed to\ndecide whether the hypernymy relationship exists or not. Experimental results\ndemonstrate", "published": "2018-05-13 06:09:40", "link": "http://arxiv.org/abs/1805.04827v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Neural Story Generation", "abstract": "We explore story generation: creative systems that can build coherent and\nfluent passages of text about a topic. We collect a large dataset of 300K\nhuman-written stories paired with writing prompts from an online forum. Our\ndataset enables hierarchical story generation, where the model first generates\na premise, and then transforms it into a passage of text. We gain further\nimprovements with a novel form of model fusion that improves the relevance of\nthe story to the prompt, and adding a new gated multi-scale self-attention\nmechanism to model long-range context. Experiments show large improvements over\nstrong baselines on both automated and human evaluations. Human judges prefer\nstories generated by our approach to those from a strong non-hierarchical model\nby a factor of two to one.", "published": "2018-05-13 07:07:08", "link": "http://arxiv.org/abs/1805.04833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Language Models for Text with Named Entities", "abstract": "Text in many domains involves a significant amount of named entities.\nPredict- ing the entity names is often challenging for a language model as they\nappear less frequent on the training corpus. In this paper, we propose a novel\nand effective approach to building a discriminative language model which can\nlearn the entity names by leveraging their entity type information. We also\nintroduce two benchmark datasets based on recipes and Java programming codes,\non which we evalu- ate the proposed model. Experimental re- sults show that our\nmodel achieves 52.2% better perplexity in recipe generation and 22.06% on code\ngeneration than the state-of-the-art language models.", "published": "2018-05-13 07:46:12", "link": "http://arxiv.org/abs/1805.04836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Questions in Open-domain Conversational Systems with\n  Typed Decoders", "abstract": "Asking good questions in large-scale, open-domain conversational systems is\nquite significant yet rather untouched. This task, substantially different from\ntraditional question generation, requires to question not only with various\npatterns but also on diverse and relevant topics. We observe that a good\nquestion is a natural composition of {\\it interrogatives}, {\\it topic words},\nand {\\it ordinary words}. Interrogatives lexicalize the pattern of questioning,\ntopic words address the key information for topic transition in dialogue, and\nordinary words play syntactical and grammatical roles in making a natural\nsentence. We devise two typed decoders (\\textit{soft typed decoder} and\n\\textit{hard typed decoder}) in which a type distribution over the three types\nis estimated and used to modulate the final generation distribution. Extensive\nexperiments show that the typed decoders outperform state-of-the-art baselines\nand can generate more meaningful questions.", "published": "2018-05-13 08:38:20", "link": "http://arxiv.org/abs/1805.04843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoencoder as Assistant Supervisor: Improving Text Representation for\n  Chinese Social Media Text Summarization", "abstract": "Most of the current abstractive text summarization models are based on the\nsequence-to-sequence model (Seq2Seq). The source content of social media is\nlong and noisy, so it is difficult for Seq2Seq to learn an accurate semantic\nrepresentation. Compared with the source content, the annotated summary is\nshort and well written. Moreover, it shares the same meaning as the source\ncontent. In this work, we supervise the learning of the representation of the\nsource content with that of the summary. In implementation, we regard a summary\nautoencoder as an assistant supervisor of Seq2Seq. Following previous work, we\nevaluate our model on a popular Chinese social media dataset. Experimental\nresults show that our model achieves the state-of-the-art performances on the\nbenchmark dataset.", "published": "2018-05-13 12:23:44", "link": "http://arxiv.org/abs/1805.04869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bag-of-Words as Target for Neural Machine Translation", "abstract": "A sentence can be translated into more than one correct sentences. However,\nmost of the existing neural machine translation models only use one of the\ncorrect translations as the targets, and the other correct sentences are\npunished as the incorrect sentences in the training stage. Since most of the\ncorrect translations for one sentence share the similar bag-of-words, it is\npossible to distinguish the correct translations from the incorrect ones by the\nbag-of-words. In this paper, we propose an approach that uses both the\nsentences and the bag-of-words as targets in the training stage, in order to\nencourage the model to generate the potentially correct sentences that are not\nappeared in the training set. We evaluate our model on a Chinese-English\ntranslation dataset, and experiments show our model outperforms the strong\nbaselines by the BLEU score of 4.55.", "published": "2018-05-13 12:24:20", "link": "http://arxiv.org/abs/1805.04871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UnibucKernel Reloaded: First Place in Arabic Dialect Identification for\n  the Second Year in a Row", "abstract": "We present a machine learning approach that ranked on the first place in the\nArabic Dialect Identification (ADI) Closed Shared Tasks of the 2018 VarDial\nEvaluation Campaign. The proposed approach combines several kernels using\nmultiple kernel learning. While most of our kernels are based on character\np-grams (also known as n-grams) extracted from speech or phonetic transcripts,\nwe also use a kernel based on dialectal embeddings generated from audio\nrecordings by the organizers. In the learning stage, we independently employ\nKernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR).\nPreliminary experiments indicate that KRR provides better classification\nresults. Our approach is shallow and simple, but the empirical results obtained\nin the 2018 ADI Closed Shared Task prove that it achieves the best performance.\nFurthermore, our top macro-F1 score (58.92%) is significantly better than the\nsecond best score (57.59%) in the 2018 ADI Shared Task, according to the\nstatistical significance test performed by the organizers. Nevertheless, we\nobtain even better post-competition results (a macro-F1 score of 62.28%) using\nthe audio embeddings released by the organizers after the competition. With a\nvery similar approach (that did not include phonetic features), we also ranked\nfirst in the ADI Closed Shared Tasks of the 2017 VarDial Evaluation Campaign,\nsurpassing the second best method by 4.62%. We therefore conclude that our\nmultiple kernel learning method is the best approach to date for Arabic dialect\nidentification.", "published": "2018-05-13 12:53:47", "link": "http://arxiv.org/abs/1805.04876v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Coreference Resolution with Deep Biaffine Attention by Joint\n  Mention Detection and Mention Clustering", "abstract": "Coreference resolution aims to identify in a text all mentions that refer to\nthe same real-world entity. The state-of-the-art end-to-end neural coreference\nmodel considers all text spans in a document as potential mentions and learns\nto link an antecedent for each possible mention. In this paper, we propose to\nimprove the end-to-end coreference resolution system by (1) using a biaffine\nattention model to get antecedent scores for each possible mention, and (2)\njointly optimizing the mention detection accuracy and the mention clustering\nlog-likelihood given the mention cluster labels. Our model achieves the\nstate-of-the-art performance on the CoNLL-2012 Shared Task English test set.", "published": "2018-05-13 14:24:31", "link": "http://arxiv.org/abs/1805.04893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comprehensive Supersense Disambiguation of English Prepositions and\n  Possessives", "abstract": "Semantic relations are often signaled with prepositional or possessive\nmarking--but extreme polysemy bedevils their analysis and automatic\ninterpretation. We introduce a new annotation scheme, corpus, and task for the\ndisambiguation of prepositions and possessives in English. Unlike previous\napproaches, our annotations are comprehensive with respect to types and tokens\nof these markers; use broadly applicable supersense classes rather than\nfine-grained dictionary definitions; unite prepositions and possessives under\nthe same class inventory; and distinguish between a marker's lexical\ncontribution and the role it marks in the context of a predicate or scene.\nStrong interannotator agreement rates, as well as encouraging disambiguation\nresults with established supervised methods, speak to the viability of the\nscheme and task.", "published": "2018-05-13 16:08:57", "link": "http://arxiv.org/abs/1805.04905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dialog Generation with Cross-Domain Latent Actions", "abstract": "This paper introduces zero-shot dialog generation (ZSDG), as a step towards\nneural dialog systems that can instantly generalize to new situations with\nminimal data. ZSDG enables an end-to-end generative dialog system to generalize\nto a new domain for which only a domain description is provided and no training\ndialogs are available. Then a novel learning framework, Action Matching, is\nproposed. This algorithm can learn a cross-domain embedding space that models\nthe semantics of dialog responses which, in turn, lets a neural dialog\ngeneration model generalize to new domains. We evaluate our methods on a new\nsynthetic dialog dataset, and an existing human-human dialog dataset. Results\nshow that our method has superior performance in learning dialog models that\nrapidly adapt their behavior to new domains and suggests promising future\nresearch.", "published": "2018-05-13 01:07:32", "link": "http://arxiv.org/abs/1805.04803v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Triangular Architecture for Rare Language Translation", "abstract": "Neural Machine Translation (NMT) performs poor on the low-resource language\npair $(X,Z)$, especially when $Z$ is a rare language. By introducing another\nrich language $Y$, we propose a novel triangular training architecture (TA-NMT)\nto leverage bilingual data $(Y,Z)$ (may be small) and $(X,Y)$ (can be rich) to\nimprove the translation performance of low-resource pairs. In this triangular\narchitecture, $Z$ is taken as the intermediate latent variable, and translation\nmodels of $Z$ are jointly optimized with a unified bidirectional EM algorithm\nunder the goal of maximizing the translation likelihood of $(X,Y)$. Empirical\nresults demonstrate that our method significantly improves the translation\nquality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even\nbetter performance combining back-translation methods.", "published": "2018-05-13 03:35:09", "link": "http://arxiv.org/abs/1805.04813v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Practical Computational Power of Finite Precision RNNs for\n  Language Recognition", "abstract": "While Recurrent Neural Networks (RNNs) are famously known to be Turing\ncomplete, this relies on infinite precision in the states and unbounded\ncomputation time. We consider the case of RNNs with finite precision whose\ncomputation time is linear in the input length. Under these limitations, we\nshow that different RNN variants have different computational power. In\nparticular, we show that the LSTM and the Elman-RNN with ReLU activation are\nstrictly stronger than the RNN with a squashing activation and the GRU. This is\nachieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We\nshow empirically that the LSTM does indeed learn to effectively use the\ncounting mechanism.", "published": "2018-05-13 16:28:32", "link": "http://arxiv.org/abs/1805.04908v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
