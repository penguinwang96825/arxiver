{"title": "Real-Time Web Scale Event Summarization Using Sequential Decision Making", "abstract": "We present a system based on sequential decision making for the online\nsummarization of massive document streams, such as those found on the web.\nGiven an event of interest (e.g. \"Boston marathon bombing\"), our system is able\nto filter the stream for relevance and produce a series of short text updates\ndescribing the event as it unfolds over time. Unlike previous work, our\napproach is able to jointly model the relevance, comprehensiveness, novelty,\nand timeliness required by time-sensitive queries. We demonstrate a 28.3%\nimprovement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.", "published": "2016-05-12 03:18:12", "link": "http://arxiv.org/abs/1605.03664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic\n  Representation Learning", "abstract": "We introduce polyglot language models, recurrent neural network models\ntrained to predict symbol sequences in many different languages using shared\nrepresentations of symbols and conditioning on typological information about\nthe language to be predicted. We apply these to the problem of modeling phone\nsequences---a domain in which universal symbol inventories and\ncross-linguistically shared feature representations are a natural fit.\nIntrinsic evaluation on held-out perplexity, qualitative analysis of the\nlearned representations, and extrinsic evaluation in two downstream\napplications that make use of phonetic features show (i) that polyglot models\nbetter generalize to held-out data than comparable monolingual models and (ii)\nthat polyglot phonetic feature representations are of higher quality than those\nlearned monolingually.", "published": "2016-05-12 14:37:51", "link": "http://arxiv.org/abs/1605.03832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning the Curriculum with Bayesian Optimization for Task-Specific\n  Word Representation Learning", "abstract": "We use Bayesian optimization to learn curricula for word representation\nlearning, optimizing performance on downstream tasks that depend on the learned\nrepresentations as features. The curricula are modeled by a linear ranking\nfunction which is the scalar product of a learned weight vector and an\nengineered feature vector that characterizes the different aspects of the\ncomplexity of each instance in the training corpus. We show that learning the\ncurriculum improves performance on a variety of downstream tasks over random\norders and in comparison to the natural corpus order.", "published": "2016-05-12 15:15:58", "link": "http://arxiv.org/abs/1605.03852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Embeddings of Hierarchical Categories and Entities", "abstract": "Due to the lack of structured knowledge applied in learning distributed\nrepresentation of categories, existing work cannot incorporate category\nhierarchies into entity information.~We propose a framework that embeds\nentities and categories into a semantic space by integrating structured\nknowledge and taxonomy hierarchy from large knowledge bases. The framework\nallows to compute meaningful semantic relatedness between entities and\ncategories.~Compared with the previous state of the art, our framework can\nhandle both single-word concepts and multiple-word concepts with superior\nperformance in concept categorization and semantic relatedness.", "published": "2016-05-12 18:45:18", "link": "http://arxiv.org/abs/1605.03924v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Convergent Properties of Word Embedding Methods", "abstract": "Do word embeddings converge to learn similar things over different\ninitializations? How repeatable are experiments with word embeddings? Are all\nword embedding techniques equally reliable? In this paper we propose evaluating\nmethods for learning word representations by their consistency across\ninitializations. We propose a measure to quantify the similarity of the learned\nword representations under this setting (where they are subject to different\nrandom initializations). Our preliminary results illustrate that our metric not\nonly measures a intrinsic property of word embedding methods but also\ncorrelates well with other evaluation metrics on downstream tasks. We believe\nour methods are is useful in characterizing robustness -- an important property\nto consider when developing new word embedding methods.", "published": "2016-05-12 19:59:43", "link": "http://arxiv.org/abs/1605.03956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Learning Algorithms Can Generalize Identity-Based Rules to Novel\n  Inputs?", "abstract": "We propose a novel framework for the analysis of learning algorithms that\nallows us to say when such algorithms can and cannot generalize certain\npatterns from training data to test data. In particular we focus on situations\nwhere the rule that must be learned concerns two components of a stimulus being\nidentical. We call such a basis for discrimination an identity-based rule.\nIdentity-based rules have proven to be difficult or impossible for certain\ntypes of learning algorithms to acquire from limited datasets. This is in\ncontrast to human behaviour on similar tasks. Here we provide a framework for\nrigorously establishing which learning algorithms will fail at generalizing\nidentity-based rules to novel stimuli. We use this framework to show that such\nalgorithms are unable to generalize identity-based rules to novel inputs unless\ntrained on virtually all possible inputs. We demonstrate these results\ncomputationally with a multilayer feedforward neural network.", "published": "2016-05-12 22:42:48", "link": "http://arxiv.org/abs/1605.04002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Movie Description", "abstract": "Audio Description (AD) provides linguistic descriptions of movies and allows\nvisually impaired people to follow a movie along with their peers. Such\ndescriptions are by design mainly visual and thus naturally form an interesting\ndata source for computer vision and computational linguistics. In this work we\npropose a novel dataset which contains transcribed ADs, which are temporally\naligned to full length movies. In addition we also collected and aligned movie\nscripts used in prior work and compare the two sources of descriptions. In\ntotal the Large Scale Movie Description Challenge (LSMDC) contains a parallel\ncorpus of 118,114 sentences and video clips from 202 movies. First we\ncharacterize the dataset by benchmarking different approaches for generating\nvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed more\nvisual and describe precisely what is shown rather than what should happen\naccording to the scripts created prior to movie production. Furthermore, we\npresent and compare the results of several teams who participated in a\nchallenge organized in the context of the workshop \"Describing and\nUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)\", at\nICCV 2015.", "published": "2016-05-12 07:34:08", "link": "http://arxiv.org/abs/1605.03705v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language\n  Model", "abstract": "Recent advances in conditional recurrent language modelling have mainly\nfocused on network architectures (e.g., attention mechanism), learning\nalgorithms (e.g., scheduled sampling and sequence-level training) and novel\napplications (e.g., image/video description generation, speech recognition,\netc.) On the other hand, we notice that decoding algorithms/strategies have not\nbeen investigated as much, and it has become standard to use greedy or beam\nsearch. In this paper, we propose a novel decoding strategy motivated by an\nearlier observation that nonlinear hidden layers of a deep neural network\nstretch the data manifold. The proposed strategy is embarrassingly\nparallelizable without any communication overhead, while improving an existing\ndecoding algorithm. We extensively evaluate it with attention-based neural\nmachine translation on the task of En->Cz translation.", "published": "2016-05-12 14:39:50", "link": "http://arxiv.org/abs/1605.03835v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Relation Schema Induction using Tensor Factorization with Side\n  Information", "abstract": "Given a set of documents from a specific domain (e.g., medical research\njournals), how do we automatically build a Knowledge Graph (KG) for that\ndomain? Automatic identification of relations and their schemas, i.e., type\nsignature of arguments of relations (e.g., undergo(Patient, Surgery)), is an\nimportant first step towards this goal. We refer to this problem as Relation\nSchema Induction (RSI). In this paper, we propose Schema Induction using\nCoupled Tensor Factorization (SICTF), a novel tensor factorization method for\nrelation schema induction. SICTF factorizes Open Information Extraction\n(OpenIE) triples extracted from a domain corpus along with additional side\ninformation in a principled way to induce relation schemas. To the best of our\nknowledge, this is the first application of tensor factorization for the RSI\nproblem. Through extensive experiments on multiple real-world datasets, we find\nthat SICTF is not only more accurate than state-of-the-art baselines, but also\nsignificantly faster (about 14x faster).", "published": "2016-05-12 19:44:04", "link": "http://arxiv.org/abs/1605.04227v3", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
