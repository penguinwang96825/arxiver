{"title": "Legal Area Classification: A Comparative Study of Text Classifiers on\n  Singapore Supreme Court Judgments", "abstract": "This paper conducts a comparative study on the performance of various machine\nlearning (``ML'') approaches for classifying judgments into legal areas. Using\na novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how\nstate-of-the-art NLP methods compare against traditional statistical models\nwhen applied to a legal corpus that comprised few but lengthy documents. All\napproaches tested, including topic model, word embedding, and language\nmodel-based classifiers, performed well with as little as a few hundred\njudgments. However, more work needs to be done to optimize state-of-the-art\nmethods for the legal domain.", "published": "2019-04-13 02:48:49", "link": "http://arxiv.org/abs/1904.06470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Repository of Conversational Datasets", "abstract": "Progress in Machine Learning is often driven by the availability of large\ndatasets, and consistent evaluation metrics for comparing modeling approaches.\nTo this end, we present a repository of conversational datasets consisting of\nhundreds of millions of examples, and a standardised evaluation procedure for\nconversational response selection models using '1-of-100 accuracy'. The\nrepository contains scripts that allow researchers to reproduce the standard\ndatasets, or to adapt the pre-processing and data filtering steps to their\nneeds. We introduce and evaluate several competitive baselines for\nconversational response selection, whose implementations are shared in the\nrepository, as well as a neural encoder model that is trained on the entire\ntraining set.", "published": "2019-04-13 02:59:48", "link": "http://arxiv.org/abs/1904.06472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distantly-supervised Entity Typing with Compact Latent Space\n  Clustering", "abstract": "Recently, distant supervision has gained great success on Fine-grained Entity\nTyping (FET). Despite its efficiency in reducing manual labeling efforts, it\nalso brings the challenge of dealing with false entity type labels, as distant\nsupervision assigns labels in a context agnostic manner. Existing works\nalleviated this issue with partial-label loss, but usually suffer from\nconfirmation bias, which means the classifier fit a pseudo data distribution\ngiven by itself. In this work, we propose to regularize distantly supervised\nmodels with Compact Latent Space Clustering (CLSC) to bypass this problem and\neffectively utilize noisy data yet. Our proposed method first dynamically\nconstructs a similarity graph of different entity mentions; infer the labels of\nnoisy instances via label propagation. Based on the inferred labels, mention\nembeddings are updated accordingly to encourage entity mentions with close\nsemantics to form a compact cluster in the embedding space,thus leading to\nbetter classification performance. Extensive experiments on standard benchmarks\nshow that our CLSC model consistently outperforms state-of-the-art distantly\nsupervised entity typing systems by a significant margin.", "published": "2019-04-13 03:52:56", "link": "http://arxiv.org/abs/1904.06475v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Short Text Topic Modeling Techniques, Applications, and Performance: A\n  Survey", "abstract": "Analyzing short texts infers discriminative and coherent latent topics that\nis a critical and fundamental task since many real-world applications require\nsemantic understanding of short texts. Traditional long text topic modeling\nalgorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this\nproblem very well since only very limited word co-occurrence information is\navailable in short texts. Therefore, short text topic modeling has already\nattracted much attention from the machine learning research community in recent\nyears, which aims at overcoming the problem of sparseness in short texts. In\nthis survey, we conduct a comprehensive review of various short text topic\nmodeling techniques proposed in the literature. We present three categories of\nmethods based on Dirichlet multinomial mixture, global word co-occurrences, and\nself-aggregation, with example of representative approaches in each category\nand analysis of their performance on various tasks. We develop the first\ncomprehensive open-source library, called STTM, for use in Java that integrates\nall surveyed algorithms within a unified interface, benchmark datasets, to\nfacilitate the expansion of new methods in this research field. Finally, we\nevaluate these state-of-the-art methods on many real-world datasets and compare\ntheir performance against one another and versus long text topic modeling\nalgorithm.", "published": "2019-04-13 09:08:46", "link": "http://arxiv.org/abs/1904.07695v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "M2H-GAN: A GAN-based Mapping from Machine to Human Transcripts for\n  Speech Understanding", "abstract": "Deep learning is at the core of recent spoken language understanding (SLU)\nrelated tasks. More precisely, deep neural networks (DNNs) drastically\nincreased the performances of SLU systems, and numerous architectures have been\nproposed. In the real-life context of theme identification of telephone\nconversations, it is common to hold both a human, manual (TRS) and an\nautomatically transcribed (ASR) versions of the conversations. Nonetheless, and\ndue to production constraints, only the ASR transcripts are considered to build\nautomatic classifiers. TRS transcripts are only used to measure the\nperformances of ASR systems. Moreover, the recent performances in term of\nclassification accuracy, obtained by DNN related systems are close to the\nperformances reached by humans, and it becomes difficult to further increase\nthe performances by only considering the ASR transcripts. This paper proposes\nto distillates the TRS knowledge available during the training phase within the\nASR representation, by using a new generative adversarial network called\nM2H-GAN to generate a TRS-like version of an ASR document, to improve the theme\nidentification performances.", "published": "2019-04-13 20:04:22", "link": "http://arxiv.org/abs/1905.01957v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Low-Latency Speaker-Independent Continuous Speech Separation", "abstract": "Speaker independent continuous speech separation (SI-CSS) is a task of\nconverting a continuous audio stream, which may contain overlapping voices of\nunknown speakers, into a fixed number of continuous signals each of which\ncontains no overlapping speech segment. A separated, or cleaned, version of\neach utterance is generated from one of SI-CSS's output channels\nnondeterministically without being split up and distributed to multiple\nchannels. A typical application scenario is transcribing multi-party\nconversations, such as meetings, recorded with microphone arrays. The output\nsignals can be simply sent to a speech recognition engine because they do not\ninclude speech overlaps. The previous SI-CSS method uses a neural network\ntrained with permutation invariant training and a data-driven beamformer and\nthus requires much processing latency. This paper proposes a low-latency SI-CSS\nmethod whose performance is comparable to that of the previous method in a\nmicrophone array-based meeting transcription task.This is achieved (1) by using\na new speech separation network architecture combined with a double buffering\nscheme and (2) by performing enhancement with a set of fixed beamformers\nfollowed by a neural post-filter.", "published": "2019-04-13 04:24:12", "link": "http://arxiv.org/abs/1904.06478v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end Text-to-speech for Low-resource Languages by Cross-Lingual\n  Transfer Learning", "abstract": "End-to-end text-to-speech (TTS) has shown great success on large quantities\nof paired text plus speech data. However, laborious data collection remains\ndifficult for at least 95% of the languages over the world, which hinders the\ndevelopment of TTS in different languages. In this paper, we aim to build TTS\nsystems for such low-resource (target) languages where only very limited paired\ndata are available. We show such TTS can be effectively constructed by\ntransferring knowledge from a high-resource (source) language. Since the model\ntrained on source language cannot be directly applied to target language due to\ninput space mismatch, we propose a method to learn a mapping between source and\ntarget linguistic symbols. Benefiting from this learned mapping, pronunciation\ninformation can be preserved throughout the transferring procedure. Preliminary\nexperiments show that we only need around 15 minutes of paired data to obtain a\nrelatively good TTS system. Furthermore, analytic studies demonstrated that the\nautomatically discovered mapping correlate well with the phonetic expertise.", "published": "2019-04-13 08:51:11", "link": "http://arxiv.org/abs/1904.06508v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio Compression Using Graph-based Transform", "abstract": "Graph-based Transform is one of the recent transform coding methods which has\nbeen used successfully in the state-of-art data decorrelation applications. In\nthis paper, we propose a Graph-based Transform (GT) for audio compression.\nHence, we introduce a proper graph structure for audio. Then the audio frames\nare projected onto an orthogonal matrix consisting of eigenvectors of the\nintroduced graph matrix, leading to the sparse coefficients. The results show\nthat the proposed method outperforms the conventional transform methods like\nDiscrete Cosine Transform (DCT) and Walsh-Hadamard Transform (WHT) in\ndecorrelation of the audio signals.", "published": "2019-04-13 19:33:34", "link": "http://arxiv.org/abs/1904.06588v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Unsupervised Singing Voice Conversion", "abstract": "We present a deep learning method for singing voice conversion. The proposed\nnetwork is not conditioned on the text or on the notes, and it directly\nconverts the audio of one singer to the voice of another. Training is performed\nwithout any form of supervision: no lyrics or any kind of phonetic features, no\nnotes, and no matching samples between singers. The proposed network employs a\nsingle CNN encoder for all singers, a single WaveNet decoder, and a classifier\nthat enforces the latent representation to be singer-agnostic. Each singer is\nrepresented by one embedding vector, which the decoder is conditioned on. In\norder to deal with relatively small datasets, we propose a new data\naugmentation scheme, as well as new training losses and protocols that are\nbased on backtranslation. Our evaluation presents evidence that the conversion\nproduces natural signing voices that are highly recognizable as the target\nsinger.", "published": "2019-04-13 20:07:58", "link": "http://arxiv.org/abs/1904.06590v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Vulnerability Analysis of Voice-Driven Interfaces and\n  Countermeasures for Replay", "abstract": "Fake audio detection is expected to become an important research area in the\nfield of smart speakers such as Google Home, Amazon Echo and chatbots developed\nfor these platforms. This paper presents replay attack vulnerability of\nvoice-driven interfaces and proposes a countermeasure to detect replay attack\non these platforms. This paper presents a novel framework to model replay\nattack distortion, and then use a non-learning-based method for replay attack\ndetection on smart speakers. The reply attack distortion is modeled as a\nhigher-order nonlinearity in the replay attack audio. Higher-order spectral\nanalysis (HOSA) is used to capture characteristics distortions in the replay\naudio. Effectiveness of the proposed countermeasure scheme is evaluated on\noriginal speech as well as corresponding replayed recordings. The replay attack\nrecordings are successfully injected into the Google Home device via Amazon\nAlexa using the drop-in conferencing feature.", "published": "2019-04-13 20:51:26", "link": "http://arxiv.org/abs/1904.06591v1", "categories": ["cs.CR", "cs.SD", "eess.AS", "92C55", "I.2.1; I.5.4"], "primary_category": "cs.CR"}
