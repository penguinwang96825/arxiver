{"title": "Does Simultaneous Speech Translation need Simultaneous Models?", "abstract": "In simultaneous speech translation (SimulST), finding the best trade-off\nbetween high translation quality and low latency is a challenging task. To meet\nthe latency constraints posed by the different application scenarios, multiple\ndedicated SimulST models are usually trained and maintained, generating high\ncomputational costs. In this paper, motivated by the increased social and\nenvironmental impact caused by these costs, we investigate whether a single\nmodel trained offline can serve not only the offline but also the simultaneous\ntask without the need for any additional training or adaptation. Experiments on\nen->{de, es} indicate that, aside from facilitating the adoption of\nwell-established offline techniques and architectures without affecting\nlatency, the offline solution achieves similar or better translation quality\ncompared to the same model trained in simultaneous settings, as well as being\ncompetitive with the SimulST state of the art.", "published": "2022-04-08 00:10:46", "link": "http://arxiv.org/abs/2204.03783v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marvelous Agglutinative Language Effect on Cross Lingual Transfer\n  Learning", "abstract": "As for multilingual language models, it is important to select languages for\ntraining because of the curse of multilinguality. It is known that using\nlanguages with similar language structures is effective for cross lingual\ntransfer learning. However, we demonstrate that using agglutinative languages\nsuch as Korean is more effective in cross lingual transfer learning. This is a\ngreat discovery that will change the training strategy of cross lingual\ntransfer learning.", "published": "2022-04-08 04:04:45", "link": "http://arxiv.org/abs/2204.03831v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infusing Knowledge from Wikipedia to Enhance Stance Detection", "abstract": "Stance detection infers a text author's attitude towards a target. This is\nchallenging when the model lacks background knowledge about the target. Here,\nwe show how background knowledge from Wikipedia can help enhance the\nperformance on stance detection. We introduce Wikipedia Stance Detection BERT\n(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on\nthree benchmark datasets covering social media discussions and online debates\nindicate that our model significantly outperforms the state-of-the-art methods\non target-specific stance detection, cross-target stance detection, and\nzero/few-shot stance detection.", "published": "2022-04-08 04:49:55", "link": "http://arxiv.org/abs/2204.03839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrudeOilNews: An Annotated Crude Oil News Corpus for Event Extraction", "abstract": "In this paper, we present CrudeOilNews, a corpus of English Crude Oil news\nfor event extraction. It is the first of its kind for Commodity News and serve\nto contribute towards resource building for economic and financial text mining.\nThis paper describes the data collection process, the annotation methodology\nand the event typology used in producing the corpus. Firstly, a seed set of 175\nnews articles were manually annotated, of which a subset of 25 news were used\nas the adjudicated reference test set for inter-annotator and system\nevaluation. Agreement was generally substantial and annotator performance was\nadequate, indicating that the annotation scheme produces consistent event\nannotations of high quality. Subsequently the dataset is expanded through (1)\ndata augmentation and (2) Human-in-the-loop active learning. The resulting\ncorpus has 425 news articles with approximately 11k events annotated. As part\nof active learning process, the corpus was used to train basic event extraction\nmodels for machine labeling, the resulting models also serve as a validation or\nas a pilot study demonstrating the use of the corpus in machine learning\npurposes. The annotated corpus is made available for academic research purpose\nat https://github.com/meisin/CrudeOilNews-Corpus.", "published": "2022-04-08 06:51:35", "link": "http://arxiv.org/abs/2204.03871v1", "categories": ["cs.CL", "68T99", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Advancing Semi-Supervised Learning for Automatic Post-Editing:\n  Data-Synthesis by Mask-Infilling with Erroneous Terms", "abstract": "Semi-supervised learning that leverages synthetic data for training has been\nwidely adopted for developing automatic post-editing (APE) models due to the\nlack of training data. With this aim, we focus on data-synthesis methods to\ncreate high-quality synthetic data. Given that APE takes as input a\nmachine-translation result that might include errors, we present a\ndata-synthesis method by which the resulting synthetic data mimic the\ntranslation errors found in actual data. We introduce a noising-based\ndata-synthesis method by adapting the masked language model approach,\ngenerating a noisy text from a clean text by infilling masked tokens with\nerroneous tokens. Moreover, we propose selective corpus interleaving that\ncombines two separate synthetic datasets by taking only the advantageous\nsamples to enhance the quality of the synthetic data further. Experimental\nresults show that using the synthetic data created by our approach results in\nsignificantly better APE performance than other synthetic data created by\nexisting methods.", "published": "2022-04-08 07:48:57", "link": "http://arxiv.org/abs/2204.03896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language\n  Model", "abstract": "Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.", "published": "2022-04-08 08:07:42", "link": "http://arxiv.org/abs/2204.03905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Rewriting to Remembering: Common Ground for Conversational QA\n  Models", "abstract": "In conversational QA, models have to leverage information in previous turns\nto answer upcoming questions. Current approaches, such as Question Rewriting,\nstruggle to extract relevant information as the conversation unwinds. We\nintroduce the Common Ground (CG), an approach to accumulate conversational\ninformation as it emerges and select the relevant information at every turn. We\nshow that CG offers a more efficient and human-like way to exploit\nconversational information compared to existing approaches, leading to\nimprovements on Open Domain Conversational QA.", "published": "2022-04-08 08:52:09", "link": "http://arxiv.org/abs/2204.03930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RubCSG at SemEval-2022 Task 5: Ensemble learning for identifying\n  misogynous MEMEs", "abstract": "This work presents an ensemble system based on various uni-modal and bi-modal\nmodel architectures developed for the SemEval 2022 Task 5: MAMI-Multimedia\nAutomatic Misogyny Identification. The challenge organizers provide an English\nmeme dataset to develop and train systems for identifying and classifying\nmisogynous memes. More precisely, the competition is separated into two\nsub-tasks: sub-task A asks for a binary decision as to whether a meme expresses\nmisogyny, while sub-task B is to classify misogynous memes into the potentially\noverlapping sub-categories of stereotype, shaming, objectification, and\nviolence. For our submission, we implement a new model fusion network and\nemploy an ensemble learning approach for better performance. With this\nstructure, we achieve a 0.755 macroaverage F1-score (11th) in sub-task A and a\n0.709 weighted-average F1-score (10th) in sub-task B.", "published": "2022-04-08 09:27:28", "link": "http://arxiv.org/abs/2204.03953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are We Really Making Much Progress in Text Classification? A Comparative\n  Review", "abstract": "We analyze various methods for single-label and multi-label text\nclassification across well-known datasets, categorizing them into bag-of-words,\nsequence-based, graph-based, and hierarchical approaches. Despite the surge in\nmethods like graph-based models, encoder-only pre-trained language models,\nnotably BERT, remain state-of-the-art. However, recent findings suggest simpler\nmodels like logistic regression and trigram-based SVMs outperform newer\ntechniques. While decoder-only generative language models show promise in\nlearning with limited data, they lag behind encoder-only models in performance.\nWe emphasize the superiority of discriminative language models like BERT over\ngenerative models for supervised tasks. Additionally, we highlight the\nliterature's lack of robustness in method comparisons, particularly concerning\nbasic hyperparameter optimizations like learning rate in fine-tuning\nencoder-only language models.\n  Data availability: The source code is available at\nhttps://github.com/drndr/multilabel-text-clf\n  All datasets used for our experiments are publicly available except the NYT\ndataset.", "published": "2022-04-08 09:28:20", "link": "http://arxiv.org/abs/2204.03954v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fair and Argumentative Language Modeling for Computational Argumentation", "abstract": "Although much work in NLP has focused on measuring and mitigating\nstereotypical bias in semantic spaces, research addressing bias in\ncomputational argumentation is still in its infancy. In this paper, we address\nthis research gap and conduct a thorough investigation of bias in argumentative\nlanguage models. To this end, we introduce ABBA, a novel resource for bias\nmeasurement specifically tailored to argumentation. We employ our resource to\nassess the effect of argumentative fine-tuning and debiasing on the intrinsic\nbias found in transformer-based language models using a lightweight\nadapter-based approach that is more sustainable and parameter-efficient than\nfull fine-tuning. Finally, we analyze the potential impact of language model\ndebiasing on the performance in argument quality prediction, a downstream task\nof computational argumentation. Our results show that we are able to\nsuccessfully and sustainably remove bias in general and argumentative language\nmodels while preserving (and sometimes improving) model performance in\ndownstream tasks. We make all experimental code and data available at\nhttps://github.com/umanlp/FairArgumentativeLM.", "published": "2022-04-08 12:23:46", "link": "http://arxiv.org/abs/2204.04026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Tokenisation by Alternative Treatment of Spaces", "abstract": "Tokenisation is the first step in almost all NLP tasks, and state-of-the-art\ntransformer-based language models all use subword tokenisation algorithms to\nprocess input text. Existing algorithms have problems, often producing\ntokenisations of limited linguistic validity, and representing equivalent\nstrings differently depending on their position within a word. We hypothesise\nthat these problems hinder the ability of transformer-based models to handle\ncomplex words, and suggest that these problems are a result of allowing tokens\nto include spaces. We thus experiment with an alternative tokenisation approach\nwhere spaces are always treated as individual tokens. Specifically, we apply\nthis modification to the BPE and Unigram algorithms. We find that our modified\nalgorithms lead to improved performance on downstream NLP tasks that involve\nhandling complex words, whilst having no detrimental effect on performance in\ngeneral natural language understanding tasks. Intrinsically, we find our\nmodified algorithms give more morphologically correct tokenisations, in\nparticular when handling prefixes. Given the results of our experiments, we\nadvocate for always treating spaces as individual tokens as an improved\ntokenisation method.", "published": "2022-04-08 13:22:30", "link": "http://arxiv.org/abs/2204.04058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Representation Learning beyond Masked Language Modeling", "abstract": "How do masked language models (MLMs) such as BERT learn contextual\nrepresentations? In this work, we analyze the learning dynamics of MLMs. We\nfind that MLMs adopt sampled embeddings as anchors to estimate and inject\ncontextual semantics to representations, which limits the efficiency and\neffectiveness of MLMs. To address these issues, we propose TACO, a simple yet\neffective representation learning approach to directly model global semantics.\nTACO extracts and aligns contextual semantics hidden in contextualized\nrepresentations to encourage models to attend global semantics when generating\ncontextualized representations. Experiments on the GLUE benchmark show that\nTACO achieves up to 5x speedup and up to 1.2 points average improvement over\nexisting MLMs. The code is available at https://github.com/FUZHIYI/TACO.", "published": "2022-04-08 16:18:06", "link": "http://arxiv.org/abs/2204.04163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BioRED: A Rich Biomedical Relation Extraction Dataset", "abstract": "Automated relation extraction (RE) from biomedical literature is critical for\nmany downstream text mining applications in both research and real-world\nsettings. However, most existing benchmarking datasets for bio-medical RE only\nfocus on relations of a single type (e.g., protein-protein interactions) at the\nsentence level, greatly limiting the development of RE systems in biomedicine.\nIn this work, we first review commonly used named entity recognition (NER) and\nRE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus\nwith multiple entity types (e.g., gene/protein, disease, chemical) and relation\npairs (e.g., gene-disease; chemical-chemical) at the document level, on a set\nof 600 PubMed abstracts. Further, we label each relation as describing either a\nnovel finding or previously known background knowledge, enabling automated\nalgorithms to differentiate between novel and background information. We assess\nthe utility of BioRED by benchmarking several existing state-of-the-art\nmethods, including BERT-based models, on the NER and RE tasks. Our results show\nthat while existing approaches can reach high performance on the NER task\n(F-score of 89.3%), there is much room for improvement for the RE task,\nespecially when extracting novel relations (F-score of 47.7%). Our experiments\nalso demonstrate that such a rich dataset can successfully facilitate the\ndevelopment of more accurate, efficient, and robust RE systems for biomedicine.\nThe BioRED dataset and annotation guideline are freely available at\nhttps://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/.", "published": "2022-04-08 19:23:49", "link": "http://arxiv.org/abs/2204.04263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Show, Don't Tell: Demonstrations Outperform Descriptions for\n  Schema-Guided Task-Oriented Dialogue", "abstract": "Building universal dialogue systems that operate across multiple domains/APIs\nand generalize to new ones with minimal overhead is a critical challenge.\nRecent works have leveraged natural language descriptions of schema elements to\nenable such systems; however, descriptions only indirectly convey schema\nsemantics. In this work, we propose Show, Don't Tell, which prompts seq2seq\nmodels with a labeled example dialogue to show the semantics of schema elements\nrather than tell the model through descriptions. While requiring similar effort\nfrom service developers as generating descriptions, we show that using short\nexamples as schema representations with large language models results in\nstate-of-the-art performance on two popular dialogue state tracking benchmarks\ndesigned to measure zero-shot generalization - the Schema-Guided Dialogue\ndataset and the MultiWOZ leave-one-out benchmark.", "published": "2022-04-08 23:27:18", "link": "http://arxiv.org/abs/2204.04327v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PharmMT: A Neural Machine Translation Approach to Simplify Prescription\n  Directions", "abstract": "The language used by physicians and health professionals in prescription\ndirections includes medical jargon and implicit directives and causes much\nconfusion among patients. Human intervention to simplify the language at the\npharmacies may introduce additional errors that can lead to potentially severe\nhealth outcomes. We propose a novel machine translation-based approach,\nPharmMT, to automatically and reliably simplify prescription directions into\npatient-friendly language, thereby significantly reducing pharmacist workload.\nWe evaluate the proposed approach over a dataset consisting of over 530K\nprescriptions obtained from a large mail-order pharmacy. The end-to-end system\nachieves a BLEU score of 60.27 against the reference directions generated by\npharmacists, a 39.6% relative improvement over the rule-based normalization.\nPharmacists judged 94.3% of the simplified directions as usable as-is or with\nminimal changes. This work demonstrates the feasibility of a machine\ntranslation-based tool for simplifying prescription directions in real-life.", "published": "2022-04-08 04:03:56", "link": "http://arxiv.org/abs/2204.03830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Softmax for End-to-End Low-resource Multilingual Speech\n  Recognition", "abstract": "Low-resource speech recognition has been long-suffering from insufficient\ntraining data. In this paper, we propose an approach that leverages neighboring\nlanguages to improve low-resource scenario performance, founded on the\nhypothesis that similar linguistic units in neighboring languages exhibit\ncomparable term frequency distributions, which enables us to construct a\nHuffman tree for performing multilingual hierarchical Softmax decoding. This\nhierarchical structure enables cross-lingual knowledge sharing among similar\ntokens, thereby enhancing low-resource training outcomes. Empirical analyses\ndemonstrate that our method is effective in improving the accuracy and\nefficiency of low-resource speech recognition.", "published": "2022-04-08 05:33:51", "link": "http://arxiv.org/abs/2204.03855v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Automatic Pronunciation Assessment using Self-Supervised Speech\n  Representation Learning", "abstract": "Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT\nmodels have shown promising results in various downstream tasks in the speech\ncommunity. In particular, speech representations learned by SSL models have\nbeen shown to be effective for encoding various speech-related characteristics.\nIn this context, we propose a novel automatic pronunciation assessment method\nbased on SSL models. First, the proposed method fine-tunes the pre-trained SSL\nmodels with connectionist temporal classification to adapt the English\npronunciation of English-as-a-second-language (ESL) learners in a data\nenvironment. Then, the layer-wise contextual representations are extracted from\nall across the transformer layers of the SSL models. Finally, the automatic\npronunciation score is estimated using bidirectional long short-term memory\nwith the layer-wise contextual representations and the corresponding text. We\nshow that the proposed SSL model-based methods outperform the baselines, in\nterms of the Pearson correlation coefficient, on datasets of Korean ESL learner\nchildren and Speechocean762. Furthermore, we analyze how different\nrepresentations of transformer layers in the SSL model affect the performance\nof the pronunciation assessment task.", "published": "2022-04-08 06:13:55", "link": "http://arxiv.org/abs/2204.03863v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "RuBioRoBERTa: a pre-trained biomedical language model for Russian\n  language biomedical text mining", "abstract": "This paper presents several BERT-based models for Russian language biomedical\ntext mining (RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus\nof freely available texts in the Russian biomedical domain. With this\npre-training, our models demonstrate state-of-the-art results on RuMedBench -\nRussian medical language understanding benchmark that covers a diverse set of\ntasks, including text classification, question answering, natural language\ninference, and named entity recognition.", "published": "2022-04-08 09:18:59", "link": "http://arxiv.org/abs/2204.03951v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhance Incomplete Utterance Restoration by Joint Learning Token\n  Extraction and Text Generation", "abstract": "This paper introduces a model for incomplete utterance restoration (IUR)\ncalled JET (\\textbf{J}oint learning token \\textbf{E}xtraction and \\textbf{T}ext\ngeneration). Different from prior studies that only work on extraction or\nabstraction datasets, we design a simple but effective model, working for both\nscenarios of IUR. Our design simulates the nature of IUR, where omitted tokens\nfrom the context contribute to restoration. From this, we construct a Picker\nthat identifies the omitted tokens. To support the picker, we design two label\ncreation methods (soft and hard labels), which can work in cases of no\nannotation data for the omitted tokens. The restoration is done by using a\nGenerator with the help of the Picker on joint learning. Promising results on\nfour benchmark datasets in extraction and abstraction scenarios show that our\nmodel is better than the pretrained T5 and non-generative language model\nmethods in both rich and limited training data settings.\\footnote{The code is\navailable at \\url{https://github.com/shumpei19/JET}}", "published": "2022-04-08 09:32:18", "link": "http://arxiv.org/abs/2204.03958v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive language and vision learning of general fashion concepts", "abstract": "The steady rise of online shopping goes hand in hand with the development of\nincreasingly complex ML and NLP models. While most use cases are cast as\nspecialized supervised learning problems, we argue that practitioners would\ngreatly benefit from more transferable representations of products. In this\nwork, we build on recent developments in contrastive learning to train\nFashionCLIP, a CLIP-like model for the fashion industry. We showcase its\ncapabilities for retrieval, classification and grounding, and release our model\nand code to the community.", "published": "2022-04-08 10:01:39", "link": "http://arxiv.org/abs/2204.03972v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Checking HateCheck: a cross-functional analysis of behaviour-aware\n  learning for hate speech detection", "abstract": "Behavioural testing -- verifying system capabilities by validating\nhuman-designed input-output pairs -- is an alternative evaluation method of\nnatural language processing systems proposed to address the shortcomings of the\nstandard approach: computing metrics on held-out data. While behavioural tests\ncapture human prior knowledge and insights, there has been little exploration\non how to leverage them for model training and development. With this in mind,\nwe explore behaviour-aware learning by examining several fine-tuning schemes\nusing HateCheck, a suite of functional tests for hate speech detection systems.\nTo address potential pitfalls of training on data originally intended for\nevaluation, we train and evaluate models on different configurations of\nHateCheck by holding out categories of test cases, which enables us to estimate\nperformance on potentially overlooked system properties. The fine-tuning\nprocedure led to improvements in the classification accuracy of held-out\nfunctionalities and identity groups, suggesting that models can potentially\ngeneralise to overlooked functionalities. However, performance on held-out\nfunctionality classes and i.i.d. hate speech detection data decreased, which\nindicates that generalisation occurs mostly across functionalities from the\nsame class and that the procedure led to overfitting to the HateCheck data\ndistribution.", "published": "2022-04-08 13:03:01", "link": "http://arxiv.org/abs/2204.04042v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning the Ordering of Coordinate Compounds and Elaborate Expressions\n  in Hmong, Lahu, and Chinese", "abstract": "Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate\nconstructions common in languages of East and Southeast Asia. Mortensen (2006)\nclaims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese\ncan be predicted via phonological hierarchies and (2) these phonological\nhierarchies lack a clear phonetic rationale. These claims are significant\nbecause morphosyntax has often been seen as in a feed-forward relationship with\nphonology, and phonological generalizations have often been assumed to be\nphonetically \"natural\". We investigate whether the ordering of CCs and EEs can\nbe learned empirically and whether computational models (classifiers and\nsequence labeling models) learn unnatural hierarchies similar to those posited\nby Mortensen (2006). We find that decision trees and SVMs learn to predict the\norder of CCs/EEs on the basis of phonology, with DTs learning hierarchies\nstrikingly similar to those proposed by Mortensen. However, we also find that a\nneural sequence labeling model is able to learn the ordering of elaborate\nexpressions in Hmong very effectively without using any phonological\ninformation. We argue that EE ordering can be learned through two independent\nroutes: phonology and lexical distribution, presenting a more nuanced picture\nthan previous work. [ISO 639-3:hmn, lhu, cmn]", "published": "2022-04-08 13:58:07", "link": "http://arxiv.org/abs/2204.04080v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classification of Natural Language Processing Techniques for\n  Requirements Engineering", "abstract": "Research in applying natural language processing (NLP) techniques to\nrequirements engineering (RE) tasks spans more than 40 years, from initial\nefforts carried out in the 1980s to more recent attempts with machine learning\n(ML) and deep learning (DL) techniques. However, in spite of the progress, our\nrecent survey shows that there is still a lack of systematic understanding and\norganization of commonly used NLP techniques in RE. We believe one hurdle\nfacing the industry is lack of shared knowledge of NLP techniques and their\nusage in RE tasks. In this paper, we present our effort to synthesize and\norganize 57 most frequently used NLP techniques in RE. We classify these NLP\ntechniques in two ways: first, by their NLP tasks in typical pipelines and\nsecond, by their linguist analysis levels. We believe these two ways of\nclassification are complementary, contributing to a better understanding of the\nNLP techniques in RE and such understanding is crucial to the development of\nbetter NLP tools for RE.", "published": "2022-04-08 20:28:00", "link": "http://arxiv.org/abs/2204.04282v1", "categories": ["cs.CL", "cs.SE", "68-02", "A.1; D.m; I.7.m"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Large-Scale Discourse Structures in Pre-Trained\n  and Fine-Tuned Language Models", "abstract": "With a growing number of BERTology work analyzing different components of\npre-trained language models, we extend this line of research through an\nin-depth analysis of discourse information in pre-trained and fine-tuned\nlanguage models. We move beyond prior work along three dimensions: First, we\ndescribe a novel approach to infer discourse structures from arbitrarily long\ndocuments. Second, we propose a new type of analysis to explore where and how\naccurately intrinsic discourse is captured in the BERT and BART models.\nFinally, we assess how similar the generated structures are to a variety of\nbaselines as well as their distribution within and between models.", "published": "2022-04-08 20:42:08", "link": "http://arxiv.org/abs/2204.04289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recent Progress in Conversational AI", "abstract": "Conversational artificial intelligence (AI) is becoming an increasingly\npopular topic among industry and academia. With the fast development of neural\nnetwork-based models, a lot of neural-based conversational AI system are\ndeveloped. We will provide a brief review of the recent progress in the\nConversational AI, including the commonly adopted techniques, notable works,\nfamous competitions from academia and industry and widely used datasets.", "published": "2022-04-08 05:49:04", "link": "http://arxiv.org/abs/2204.09719v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Study of Different Ways to Use The Conformer Model For Spoken Language\n  Understanding", "abstract": "SLU combines ASR and NLU capabilities to accomplish speech-to-intent\nunderstanding. In this paper, we compare different ways to combine ASR and NLU,\nin particular using a single Conformer model with different ways to use its\ncomponents, to better understand the strengths and weaknesses of each approach.\nWe find that it is not necessarily a choice between two-stage decoding and\nend-to-end systems which determines the best system for research or\napplication. System optimization still entails carefully improving the\nperformance of each component. It is difficult to prove that one direction is\nconclusively better than the other. In this paper, we also propose a novel\nconnectionist temporal summarization (CTS) method to reduce the length of\nacoustic encoding sequences while improving the accuracy and processing speed\nof end-to-end models. This method achieves the same intent accuracy as the best\ntwo-stage SLU recognition with complicated and time-consuming decoding but does\nso at lower computational cost. This stacked end-to-end SLU system yields an\nintent accuracy of 93.97% for the SmartLights far-field set, 95.18% for the\nclose-field set, and 99.71% for FluentSpeech.", "published": "2022-04-08 07:12:11", "link": "http://arxiv.org/abs/2204.03879v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transducer-based language embedding for spoken language identification", "abstract": "The acoustic and linguistic features are important cues for the spoken\nlanguage identification (LID) task. Recent advanced LID systems mainly use\nacoustic features that lack the usage of explicit linguistic feature encoding.\nIn this paper, we propose a novel transducer-based language embedding approach\nfor LID tasks by integrating an RNN transducer model into a language embedding\nframework. Benefiting from the advantages of the RNN transducer's linguistic\nrepresentation capability, the proposed method can exploit both\nphonetically-aware acoustic features and explicit linguistic features for LID\ntasks. Experiments were carried out on the large-scale multilingual LibriSpeech\nand VoxLingua107 datasets. Experimental results showed the proposed method\nsignificantly improves the performance on LID tasks with 12% to 59% and 16% to\n24% relative improvement on in-domain and cross-domain datasets, respectively.", "published": "2022-04-08 07:23:43", "link": "http://arxiv.org/abs/2204.03888v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adding Connectionist Temporal Summarization into Conformer to Improve\n  Its Decoder Efficiency For Speech Recognition", "abstract": "The Conformer model is an excellent architecture for speech recognition\nmodeling that effectively utilizes the hybrid losses of connectionist temporal\nclassification (CTC) and attention to train model parameters. To improve the\ndecoding efficiency of Conformer, we propose a novel connectionist temporal\nsummarization (CTS) method that reduces the number of frames required for the\nattention decoder fed from the acoustic sequences generated by the encoder,\nthus reducing operations. However, to achieve such decoding improvements, we\nmust fine-tune model parameters, as cross-attention observations are changed\nand thus require corresponding refinements. Our final experiments show that,\nwith a beamwidth of 4, the LibriSpeech's decoding budget can be reduced by up\nto 20% and for FluentSpeech data it can be reduced by 11%, without losing ASR\naccuracy. An improvement in accuracy is even found for the LibriSpeech\n\"test-other\" set. The word error rate (WER) is reduced by 6\\% relative at the\nbeam width of 1 and by 3% relative at the beam width of 4.", "published": "2022-04-08 07:24:00", "link": "http://arxiv.org/abs/2204.03889v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus", "abstract": "This paper introduces GigaST, a large-scale pseudo speech translation (ST)\ncorpus. We create the corpus by translating the text in GigaSpeech, an English\nASR corpus, into German and Chinese. The training set is translated by a strong\nmachine translation system and the test set is translated by human. ST models\ntrained with an addition of our corpus obtain new state-of-the-art results on\nthe MuST-C English-German benchmark test set. We provide a detailed description\nof the translation process and verify its quality. We make the translated text\ndata public and hope to facilitate research in speech translation.\nAdditionally, we also release the training scripts on NeurST to make it easy to\nreplicate our systems. GigaST dataset is available at\nhttps://st-benchmark.github.io/resources/GigaST.", "published": "2022-04-08 08:59:33", "link": "http://arxiv.org/abs/2204.03939v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KGI: An Integrated Framework for Knowledge Intensive Language Tasks", "abstract": "In this paper, we present a system to showcase the capabilities of the latest\nstate-of-the-art retrieval augmented generation models trained on\nknowledge-intensive language tasks, such as slot filling, open domain question\nanswering, dialogue, and fact-checking. Moreover, given a user query, we show\nhow the output from these different models can be combined to cross-examine the\noutputs of each other. Particularly, we show how accuracy in dialogue can be\nimproved using the question answering model. We are also releasing all models\nused in the demo as a contribution of this paper. A short video demonstrating\nthe system is available at https://ibm.box.com/v/emnlp2022-demo.", "published": "2022-04-08 10:36:21", "link": "http://arxiv.org/abs/2204.03985v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based\n  Collaborative Filtering", "abstract": "Content-based collaborative filtering (CCF) predicts user-item interactions\nbased on both users' interaction history and items' content information.\nRecently, pre-trained language models (PLM) have been used to extract\nhigh-quality item encodings for CCF. However, it is resource-intensive to train\na PLM-based CCF model in an end-to-end (E2E) manner, since optimization\ninvolves back-propagating through every content encoding within a given user\ninteraction sequence. To tackle this issue, we propose GRAM (GRadient\nAccumulation for Multi-modality in CCF), which exploits the fact that a given\nitem often appears multiple times within a batch of interaction histories.\nSpecifically, Single-step GRAM aggregates each item encoding's gradients for\nback-propagation, with theoretic equivalence to the standard E2E training. As\nan extension of Single-step GRAM, we propose Multi-step GRAM, which increases\nthe gradient update latency, achieving a further speedup with drastically less\nGPU memory. GRAM significantly improves training efficiency (up to 146x) on\nfive datasets from two task domains of Knowledge Tracing and News\nRecommendation. Our code is available at https://github.com/yoonseok312/GRAM.", "published": "2022-04-08 17:06:39", "link": "http://arxiv.org/abs/2204.04179v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MMTAfrica: Multilingual Machine Translation for African Languages", "abstract": "In this paper, we focus on the task of multilingual machine translation for\nAfrican languages and describe our contribution in the 2021 WMT Shared Task:\nLarge-Scale Multilingual Machine Translation. We introduce MMTAfrica, the first\nmany-to-many multilingual translation system for six African languages: Fon\n(fon), Igbo (ibo), Kinyarwanda (kin), Swahili/Kiswahili (swa), Xhosa (xho), and\nYoruba (yor) and two non-African languages: English (eng) and French (fra). For\nmultilingual translation concerning African languages, we introduce a novel\nbacktranslation and reconstruction objective, BT\\&REC, inspired by the random\nonline back translation and T5 modeling framework respectively, to effectively\nleverage monolingual data. Additionally, we report improvements from MMTAfrica\nover the FLORES 101 benchmarks (spBLEU gains ranging from $+0.58$ in Swahili to\nFrench to $+19.46$ in French to Xhosa). We release our dataset and code source\nat https://github.com/edaiofficial/mmtafrica.", "published": "2022-04-08 21:42:44", "link": "http://arxiv.org/abs/2204.04306v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning\n  for Robotics", "abstract": "This paper focuses on robotic reinforcement learning with sparse rewards for\nnatural language goal representations. An open problem is the\nsample-inefficiency that stems from the compositionality of natural language,\nand from the grounding of language in sensory data and actions. We address\nthese issues with three contributions. We first present a mechanism for\nhindsight instruction replay utilizing expert feedback. Second, we propose a\nseq2seq model to generate linguistic hindsight instructions. Finally, we\npresent a novel class of language-focused learning tasks. We show that\nhindsight instructions improve the learning performance, as expected. In\naddition, we also provide an unexpected result: We show that the learning\nperformance of our agent can be improved by one third if, in a sense, the agent\nlearns to talk to itself in a self-supervised manner. We achieve this by\nlearning to generate linguistic instructions that would have been appropriate\nas a natural language goal for an originally unintended behavior. Our results\nindicate that the performance gain increases with the task-complexity.", "published": "2022-04-08 22:01:36", "link": "http://arxiv.org/abs/2204.04308v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Disentangled Latent Speech Representation for Automatic Pathological\n  Intelligibility Assessment", "abstract": "Speech intelligibility assessment plays an important role in the therapy of\npatients suffering from pathological speech disorders. Automatic and objective\nmeasures are desirable to assist therapists in their traditionally subjective\nand labor-intensive assessments. In this work, we investigate a novel approach\nfor obtaining such a measure using the divergence in disentangled latent speech\nrepresentations of a parallel utterance pair, obtained from a healthy reference\nand a pathological speaker. Experiments on an English database of Cerebral\nPalsy patients, using all available utterances per speaker, show high and\nsignificant correlation values (R = -0.9) with subjective intelligibility\nmeasures, while having only minimal deviation (+-0.01) across four different\nreference speaker pairs. We also demonstrate the robustness of the proposed\nmethod (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a\nsignificantly smaller amount of utterances per speaker. Our results are among\nthe first to show that disentangled speech representations can be used for\nautomatic pathological speech intelligibility assessment, resulting in a\nreference speaker pair invariant method, applicable in scenarios with only few\nutterances available.", "published": "2022-04-08 12:02:14", "link": "http://arxiv.org/abs/2204.04016v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "C-NMT: A Collaborative Inference Framework for Neural Machine\n  Translation", "abstract": "Collaborative Inference (CI) optimizes the latency and energy consumption of\ndeep learning inference through the inter-operation of edge and cloud devices.\nAlbeit beneficial for other tasks, CI has never been applied to the sequence-\nto-sequence mapping problem at the heart of Neural Machine Translation (NMT).\nIn this work, we address the specific issues of collaborative NMT, such as\nestimating the latency required to generate the (unknown) output sequence, and\nshow how existing CI methods can be adapted to these applications. Our\nexperiments show that CI can reduce the latency of NMT by up to 44% compared to\na non-collaborative approach.", "published": "2022-04-08 13:04:10", "link": "http://arxiv.org/abs/2204.04043v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Enhanced exemplar autoencoder with cycle consistency loss in any-to-one\n  voice conversion", "abstract": "Recent research showed that an autoencoder trained with speech of a single\nspeaker, called exemplar autoencoder (eAE), can be used for any-to-one voice\nconversion (VC). Compared to large-scale many-to-many models such as AutoVC,\nthe eAE model is easy and fast in training, and may recover more details of the\ntarget speaker.\n  To ensure VC quality, the latent code should represent and only represent\ncontent information. However, this is not easy to attain for eAE as it is\nunaware of any speaker variation in model training. To tackle the problem, we\npropose a simple yet effective approach based on a cycle consistency loss.\nSpecifically, we train eAEs of multiple speakers with a shared encoder, and\nmeanwhile encourage the speech reconstructed from any speaker-specific decoder\nto get a consistent latent code as the original speech when cycled back and\nencoded again. Experiments conducted on the AISHELL-3 corpus showed that this\nnew approach improved the baseline eAE consistently. The source code and\nexamples are available at the project page: http://project.cslt.org/.", "published": "2022-04-08 05:20:53", "link": "http://arxiv.org/abs/2204.03847v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reliable Visualization for Deep Speaker Recognition", "abstract": "In spite of the impressive success of convolutional neural networks (CNNs) in\nspeaker recognition, our understanding to CNNs' internal functions is still\nlimited. A major obstacle is that some popular visualization tools are\ndifficult to apply, for example those producing saliency maps. The reason is\nthat speaker information does not show clear spatial patterns in the\ntemporal-frequency space, which makes it hard to interpret the visualization\nresults, and hence hard to confirm the reliability of a visualization tool. In\nthis paper, we conduct an extensive analysis on three popular visualization\nmethods based on CAM: Grad-CAM, Score-CAM and Layer-CAM, to investigate their\nreliability for speaker recognition tasks. Experiments conducted on a\nstate-of-the-art ResNet34SE model show that the Layer-CAM algorithm can produce\nreliable visualization, and thus can be used as a promising tool to explain\nCNN-based speaker models. The source code and examples are available in our\nproject page: http://project.cslt.org/.", "published": "2022-04-08 05:33:01", "link": "http://arxiv.org/abs/2204.03852v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundBeam: Target sound extraction conditioned on sound-class labels and\n  enrollment clues for increased performance and continuous learning", "abstract": "In many situations, we would like to hear desired sound events (SEs) while\nbeing able to ignore interference. Target sound extraction (TSE) tackles this\nproblem by estimating the audio signal of the sounds of target SE classes in a\nmixture of sounds while suppressing all other sounds. We can achieve this with\na neural network that extracts the target SEs by conditioning it on clues\nrepresenting the target SE classes. Two types of clues have been proposed,\ni.e., target SE class labels and enrollment audio samples (or audio queries),\nwhich are pre-recorded audio samples of sounds from the target SE classes.\nSystems based on SE class labels can directly optimize embedding vectors\nrepresenting the SE classes, resulting in high extraction performance. However,\nextending these systems to extract new SE classes not encountered during\ntraining is not easy. Enrollment-based approaches extract SEs by finding sounds\nin the mixtures that share similar characteristics to the enrollment audio\nsamples. These approaches do not explicitly rely on SE class definitions and\ncan thus handle new SE classes. In this paper, we introduce a TSE framework,\nSoundBeam, that combines the advantages of both approaches. We also perform an\nextensive evaluation of the different TSE schemes using synthesized and real\nmixtures, which shows the potential of SoundBeam.", "published": "2022-04-08 07:48:45", "link": "http://arxiv.org/abs/2204.03895v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Transformer's potential on automatic piano transcription", "abstract": "Most recent research about automatic music transcription (AMT) uses\nconvolutional neural networks and recurrent neural networks to model the\nmapping from music signals to symbolic notation. Based on a high-resolution\npiano transcription system, we explore the possibility of incorporating another\npowerful sequence transformation tool -- the Transformer -- to deal with the\nAMT problem. We argue that the properties of the Transformer make it more\nsuitable for certain AMT subtasks. We confirm the Transformer's superiority on\nthe velocity detection task by experiments on the MAESTRO dataset and a\ncross-dataset evaluation on the MAPS dataset. We observe a performance\nimprovement on both frame-level and note-level metrics after introducing the\nTransformer network.", "published": "2022-04-08 07:52:28", "link": "http://arxiv.org/abs/2204.03898v1", "categories": ["eess.AS", "cs.SD", "H.5.5"], "primary_category": "eess.AS"}
{"title": "Scoring of Large-Margin Embeddings for Speaker Verification: Cosine or\n  PLDA?", "abstract": "The emergence of large-margin softmax cross-entropy losses in training deep\nspeaker embedding neural networks has triggered a gradual shift from parametric\nback-ends to a simpler cosine similarity measure for speaker verification.\nPopular parametric back-ends include the probabilistic linear discriminant\nanalysis (PLDA) and its variants. This paper investigates the properties of\nmargin-based cross-entropy losses leading to such a shift and aims to find\nscoring back-ends best suited for speaker verification. In addition, we revisit\nthe pre-processing techniques which have been widely used in the past and\nassess their effectiveness on large-margin embeddings. Experiments on the\nstate-of-the-art ECAPA-TDNN networks trained with various large-margin softmax\ncross-entropy losses show a substantial increment in intra-speaker compactness\nmaking the conventional PLDA superfluous. In this regard, we found that\nconstraining the within-speaker covariance matrix could improve the performance\nof the PLDA. It is demonstrated through a series of experiments on the\nVoxCeleb-1 and SITW core-core test sets with 40.8% equal error rate (EER)\nreduction and 35.1% minimum detection cost (minDCF) reduction. It also\noutperforms cosine scoring consistently with reductions in EER and minDCF by\n10.9% and 4.9%, respectively.", "published": "2022-04-08 09:46:24", "link": "http://arxiv.org/abs/2204.03965v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Sillwood Technologies System for the VoiceMOS Challenge 2022", "abstract": "In this paper we describe our entry for the VoiceMOS Challenge 2022 for both\nthe main and out-of-domain (OOD) track of the competition. Our system is based\non finetuning pre-trained self-supervised waveform prediction models, while\nimproving its generalisation ability through stochastic weight averaging.\nFurther, we use influence functions to identity possible low-quality data\nwithin the training set to further increase our model's performance for the OOD\ntrack. Our system ranked 5th and joint 7th for the main track and OOD track,\nrespectively.", "published": "2022-04-08 09:50:39", "link": "http://arxiv.org/abs/2204.03967v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hierarchical and Multi-Scale Variational Autoencoder for Diverse and\n  Natural Non-Autoregressive Text-to-Speech", "abstract": "This paper proposes a hierarchical and multi-scale variational\nautoencoder-based non-autoregressive text-to-speech model (HiMuV-TTS) to\ngenerate natural speech with diverse speaking styles. Recent advances in\nnon-autoregressive TTS (NAR-TTS) models have significantly improved the\ninference speed and robustness of synthesized speech. However, the diversity of\nspeaking styles and naturalness are needed to be improved. To solve this\nproblem, we propose the HiMuV-TTS model that first determines the global-scale\nprosody and then determines the local-scale prosody via conditioning on the\nglobal-scale prosody and the learned text representation. In addition, we\nimprove the quality of speech by adopting the adversarial training technique.\nExperimental results verify that the proposed HiMuV-TTS model can generate more\ndiverse and natural speech as compared to TTS models with single-scale\nvariational autoencoders, and can represent different prosody information in\neach scale.", "published": "2022-04-08 11:27:50", "link": "http://arxiv.org/abs/2204.04004v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis and transformations of voice level in singing voice", "abstract": "We introduce a neural auto-encoder that transforms the musical dynamic in\nrecordings of singing voice via changes in voice level. Since most recordings\nof singing voice are not annotated with voice level we propose a means to\nestimate the voice level from the signal's timbre using a neural voice level\nestimator. We introduce the recording factor that relates the voice level to\nthe recorded signal power as a proportionality constant. This unknown constant\ndepends on the recording conditions and the post-processing and may thus be\ndifferent for each recording (but is constant across each recording). We\nprovide two approaches to estimate the voice level without knowing the\nrecording factor. The unknown recording factor can either be learned alongside\nthe weights of the voice level estimator, or a special loss function based on\nthe scalar product can be used to only match the contour of the recorded\nsignal's power. The voice level models are used to condition a previously\nintroduced bottleneck auto-encoder that disentangles its input, the\nmel-spectrogram, from the voice level. We evaluate the voice level models on\nrecordings annotated with musical dynamic and by their ability to provide\nuseful information to the auto-encoder. A perceptive test is carried out that\nevaluates the perceived change in voice level in transformed recordings and the\nsynthesis quality. The perceptive test confirms that changing the conditional\ninput changes the perceived voice level accordingly thus suggesting that the\nproposed voice level models encode information about the true voice level.", "published": "2022-04-08 11:34:33", "link": "http://arxiv.org/abs/2204.04006v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Auditory-Based Data Augmentation for End-to-End Automatic Speech\n  Recognition", "abstract": "End-to-end models have achieved significant improvement on automatic speech\nrecognition. One common method to improve performance of these models is\nexpanding the data-space through data augmentation. Meanwhile, human auditory\ninspired front-ends have also demonstrated improvement for automatic speech\nrecognisers. In this work, a well-verified auditory-based model, which can\nsimulate various hearing abilities, is investigated for the purpose of data\naugmentation for end-to-end speech recognition. By introducing the auditory\nmodel into the data augmentation process, end-to-end systems are encouraged to\nignore variation from the signal that cannot be heard and thereby focus on\nrobust features for speech recognition. Two mechanisms in the auditory model,\nspectral smearing and loudness recruitment, are studied on the LibriSpeech\ndataset with a transformer-based end-to-end model. The results show that the\nproposed augmentation methods can bring statistically significant improvement\non the performance of the state-of-the-art SpecAugment.", "published": "2022-04-08 20:34:42", "link": "http://arxiv.org/abs/2204.04284v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Uncertainty Measures of Automatic Speech Recognition for\n  Non-intrusive Speech Intelligibility Prediction", "abstract": "Non-intrusive intelligibility prediction is important for its application in\nrealistic scenarios, where a clean reference signal is difficult to access. The\nconstruction of many non-intrusive predictors require either ground truth\nintelligibility labels or clean reference signals for supervised learning. In\nthis work, we leverage an unsupervised uncertainty estimation method for\npredicting speech intelligibility, which does not require intelligibility\nlabels or reference signals to train the predictor. Our experiments demonstrate\nthat the uncertainty from state-of-the-art end-to-end automatic speech\nrecognition (ASR) models is highly correlated with speech intelligibility. The\nproposed method is evaluated on two databases and the results show that the\nunsupervised uncertainty measures of ASR models are more correlated with speech\nintelligibility from listening results than the predictions made by widely used\nintrusive methods.", "published": "2022-04-08 20:40:53", "link": "http://arxiv.org/abs/2204.04288v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personal VAD 2.0: Optimizing Personal Voice Activity Detection for\n  On-Device Speech Recognition", "abstract": "Personalization of on-device speech recognition (ASR) has seen explosive\ngrowth in recent years, largely due to the increasing popularity of personal\nassistant features on mobile devices and smart home speakers. In this work, we\npresent Personal VAD 2.0, a personalized voice activity detector that detects\nthe voice activity of a target speaker, as part of a streaming on-device ASR\nsystem. Although previous proof-of-concept studies have validated the\neffectiveness of Personal VAD, there are still several critical challenges to\naddress before this model can be used in production: first, the quality must be\nsatisfactory in both enrollment and enrollment-less scenarios; second, it\nshould operate in a streaming fashion; and finally, the model size should be\nsmall enough to fit a limited latency and CPU/Memory budget. To meet the\nmulti-faceted requirements, we propose a series of novel designs: 1) advanced\nspeaker embedding modulation methods; 2) a new training paradigm to generalize\nto enrollment-less conditions; 3) architecture and runtime optimizations for\nlatency and resource restrictions. Extensive experiments on a realistic speech\nrecognition system demonstrated the state-of-the-art performance of our\nproposed method.", "published": "2022-04-08 00:49:19", "link": "http://arxiv.org/abs/2204.03793v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AdvEst: Adversarial Perturbation Estimation to Classify and Detect\n  Adversarial Attacks against Speaker Identification", "abstract": "Adversarial attacks pose a severe security threat to the state-of-the-art\nspeaker identification systems, thereby making it vital to propose\ncountermeasures against them. Building on our previous work that used\nrepresentation learning to classify and detect adversarial attacks, we propose\nan improvement to it using AdvEst, a method to estimate adversarial\nperturbation. First, we prove our claim that training the representation\nlearning network using adversarial perturbations as opposed to adversarial\nexamples (consisting of the combination of clean signal and adversarial\nperturbation) is beneficial because it eliminates nuisance information. At\ninference time, we use a time-domain denoiser to estimate the adversarial\nperturbations from adversarial examples. Using our improved representation\nlearning approach to obtain attack embeddings (signatures), we evaluate their\nperformance for three applications: known attack classification, attack\nverification, and unknown attack detection. We show that common attacks in the\nliterature (Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD),\nCarlini-Wagner (CW) with different Lp threat models) can be classified with an\naccuracy of ~96%. We also detect unknown attacks with an equal error rate (EER)\nof ~9%, which is absolute improvement of ~12% from our previous work.", "published": "2022-04-08 05:23:15", "link": "http://arxiv.org/abs/2204.03848v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Defense against Adversarial Attacks on Hybrid Speech Recognition using\n  Joint Adversarial Fine-tuning with Denoiser", "abstract": "Adversarial attacks are a threat to automatic speech recognition (ASR)\nsystems, and it becomes imperative to propose defenses to protect them. In this\npaper, we perform experiments to show that K2 conformer hybrid ASR is strongly\naffected by white-box adversarial attacks. We propose three defenses--denoiser\npre-processor, adversarially fine-tuning ASR model, and adversarially\nfine-tuning joint model of ASR and denoiser. Our evaluation shows denoiser\npre-processor (trained on offline adversarial examples) fails to defend against\nadaptive white-box attacks. However, adversarially fine-tuning the denoiser\nusing a tandem model of denoiser and ASR offers more robustness. We evaluate\ntwo variants of this defense--one updating parameters of both models and the\nsecond keeping ASR frozen. The joint model offers a mean absolute decrease of\n19.3\\% ground truth (GT) WER with reference to baseline against fast gradient\nsign method (FGSM) attacks with different $L_\\infty$ norms. The joint model\nwith frozen ASR parameters gives the best defense against projected gradient\ndescent (PGD) with 7 iterations, yielding a mean absolute increase of 22.3\\% GT\nWER with reference to baseline; and against PGD with 500 iterations, yielding a\nmean absolute decrease of 45.08\\% GT WER and an increase of 68.05\\% adversarial\ntarget WER.", "published": "2022-04-08 05:31:40", "link": "http://arxiv.org/abs/2204.03851v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mel-spectrogram features for acoustic vehicle detection and speed\n  estimation", "abstract": "The paper addresses acoustic vehicle detection and speed estimation from\nsingle sensor measurements. We predict the vehicle's pass-by instant by\nminimizing clipped vehicle-to-microphone distance, which is predicted from the\nmel-spectrogram of input audio, in a supervised learning approach. In addition,\nmel-spectrogram-based features are used directly for vehicle speed estimation,\nwithout introducing any intermediate features. The results show that the\nproposed features can be used for accurate vehicle detection and speed\nestimation, with an average error of 7.87 km/h. If we formulate speed\nestimation as a classification problem, with a 10 km/h discretization interval,\nthe proposed method attains the average accuracy of 48.7% for correct class\nprediction and 91.0% when an offset of one class is allowed. The proposed\nmethod is evaluated on a dataset of 304 urban-environment on-field recordings\nof ten different vehicles.", "published": "2022-04-08 11:53:13", "link": "http://arxiv.org/abs/2204.04013v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Karaoker: Alignment-free singing voice synthesis with speech training\n  data", "abstract": "Existing singing voice synthesis models (SVS) are usually trained on singing\ndata and depend on either error-prone time-alignment and duration features or\nexplicit music score information. In this paper, we propose Karaoker, a\nmultispeaker Tacotron-based model conditioned on voice characteristic features\nthat is trained exclusively on spoken data without requiring time-alignments.\nKaraoker synthesizes singing voice and transfers style following a\nmulti-dimensional template extracted from a source waveform of an unseen\nsinger/speaker. The model is jointly conditioned with a single deep\nconvolutional encoder on continuous data including pitch, intensity,\nharmonicity, formants, cepstral peak prominence and octaves. We extend the\ntext-to-speech training objective with feature reconstruction, classification\nand speaker identification tasks that guide the model to an accurate result. In\naddition to multitasking, we also employ a Wasserstein GAN training scheme as\nwell as new losses on the acoustic model's output to further refine the quality\nof the model.", "published": "2022-04-08 15:33:59", "link": "http://arxiv.org/abs/2204.04127v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Speaker Diarization", "abstract": "Over the last few years, deep learning has grown in popularity for speaker\nverification, identification, and diarization. Inarguably, a significant part\nof this success is due to the demonstrated effectiveness of their speaker\nrepresentations. These, however, are heavily dependent on large amounts of\nannotated data and can be sensitive to new domains. This study proposes an\nentirely unsupervised deep-learning model for speaker diarization.\nSpecifically, the study focuses on generating high-quality neural speaker\nrepresentations without any annotated data, as well as on estimating secondary\nhyperparameters of the model without annotations.\n  The speaker embeddings are represented by an encoder trained in a\nself-supervised fashion using pairs of adjacent segments assumed to be of the\nsame speaker. The trained encoder model is then used to self-generate\npseudo-labels to subsequently train a similarity score between different\nsegments of the same call using probabilistic linear discriminant analysis\n(PLDA) and further to learn a clustering stopping threshold. We compared our\nmodel to state-of-the-art unsupervised as well as supervised baselines on the\nCallHome benchmarks. According to empirical results, our approach outperforms\nunsupervised methods when only two speakers are present in the call, and is\nonly slightly worse than recent supervised models.", "published": "2022-04-08 16:27:14", "link": "http://arxiv.org/abs/2204.04166v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Data Augmentation Selection and Parametrization in Contrastive\n  Self-Supervised Speech Representation Learning", "abstract": "Contrastive learning enables learning useful audio and speech representations\nwithout ground-truth labels by maximizing the similarity between latent\nrepresentations of similar signal segments. In this framework various data\naugmentation techniques are usually exploited to help enforce desired\ninvariances within the learned representations, improving performance on\nvarious audio tasks thanks to more robust embeddings. Now, selecting the most\nrelevant augmentations has proven crucial for better downstream performances.\nThus, this work introduces a conditional independance-based method which allows\nfor automatically selecting a suitable distribution on the choice of\naugmentations and their parametrization from a set of predefined ones, for\ncontrastive self-supervised pre-training. This is performed with respect to a\ndownstream task of interest, hence saving a costly hyper-parameter search.\nExperiments performed on two different downstream tasks validate the proposed\napproach showing better results than experimenting without augmentation or with\nbaseline augmentations. We furthermore conduct a qualitative analysis of the\nautomatically selected augmentations and their variation according to the\nconsidered final downstream dataset.", "published": "2022-04-08 16:30:50", "link": "http://arxiv.org/abs/2204.04170v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Exploiting Hidden Representations from a DNN-based Speech Recogniser for\n  Speech Intelligibility Prediction in Hearing-impaired Listeners", "abstract": "An accurate objective speech intelligibility prediction algorithms is of\ngreat interest for many applications such as speech enhancement for hearing\naids. Most algorithms measures the signal-to-noise ratios or correlations\nbetween the acoustic features of clean reference signals and degraded signals.\nHowever, these hand-picked acoustic features are usually not explicitly\ncorrelated with recognition. Meanwhile, deep neural network (DNN) based\nautomatic speech recogniser (ASR) is approaching human performance in some\nspeech recognition tasks. This work leverages the hidden representations from\nDNN-based ASR as features for speech intelligibility prediction in\nhearing-impaired listeners. The experiments based on a hearing aid\nintelligibility database show that the proposed method could make better\nprediction than a widely used short-time objective intelligibility (STOI) based\nbinaural measure.", "published": "2022-04-08 20:38:35", "link": "http://arxiv.org/abs/2204.04287v2", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "An approach to improving sound-based vehicle speed estimation", "abstract": "We consider improving the performance of a recently proposed sound-based\nvehicle speed estimation method. In the original method, an intermediate\nfeature, referred to as the modified attenuation (MA), has been proposed for\nboth vehicle detection and speed estimation. The MA feature maximizes at the\ninstant of the vehicle's closest point of approach, which represents a training\nlabel extracted from video recording of the vehicle's pass by. In this paper,\nwe show that the original labeling approach is suboptimal and propose a method\nfor label correction. The method is tested on the VS10 dataset, which contains\n304 audio-video recordings of ten different vehicles. The results show that the\nproposed label correction method reduces average speed estimation error from\n7.39 km/h to 6.92 km/h. If the speed is discretized into 10 km/h classes, the\naccuracy of correct class prediction is improved from 53.2% to 53.8%, whereas\nwhen tolerance of one class offset is allowed, accuracy is improved from 93.4%\nto 94.3%.", "published": "2022-04-08 12:58:35", "link": "http://arxiv.org/abs/2204.05082v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
