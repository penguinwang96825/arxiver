{"title": "Task-Oriented Learning of Word Embeddings for Semantic Relation\n  Classification", "abstract": "We present a novel learning method for word embeddings designed for relation\nclassification. Our word embeddings are trained by predicting words between\nnoun pairs using lexical relation-specific features on a large unlabeled\ncorpus. This allows us to explicitly incorporate relation-specific information\ninto the word embeddings. The learned word embeddings are then used to\nconstruct feature vectors for a relation classification model. On a\nwell-established semantic relation classification task, our method\nsignificantly outperforms a baseline based on a previously introduced word\nembedding method, and compares favorably to previous state-of-the-art models\nthat use syntactic information or manually constructed external resources.", "published": "2015-02-28 07:59:59", "link": "http://arxiv.org/abs/1503.00095v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NLP Engine: A Universal Turing Machine for NLP", "abstract": "It is commonly accepted that machine translation is a more complex task than\npart of speech tagging. But how much more complex? In this paper we make an\nattempt to develop a general framework and methodology for computing the\ninformational and/or processing complexity of NLP applications and tasks. We\ndefine a universal framework akin to a Turning Machine that attempts to fit\n(most) NLP tasks into one paradigm. We calculate the complexities of various\nNLP tasks using measures of Shannon Entropy, and compare `simple' ones such as\npart of speech tagging to `complex' ones such as machine translation. This\npaper provides a first, though far from perfect, attempt to quantify NLP tasks\nunder a uniform paradigm. We point out current deficiencies and suggest some\navenues for fruitful research.", "published": "2015-02-28 19:46:50", "link": "http://arxiv.org/abs/1503.00168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "abstract": "This paper proposes a novel framework for generating lingual descriptions of\nindoor scenes. Whereas substantial efforts have been made to tackle this\nproblem, previous approaches focusing primarily on generating a single sentence\nfor each image, which is not sufficient for describing complex scenes. We\nattempt to go beyond this, by generating coherent descriptions with multiple\nsentences. Our approach is distinguished from conventional ones in several\naspects: (1) a 3D visual parsing system that jointly infers objects,\nattributes, and relations; (2) a generative grammar learned automatically from\ntraining text; and (3) a text generation algorithm that takes into account the\ncoherence among sentences. Experiments on the augmented NYU-v2 dataset show\nthat our framework can generate natural descriptions with substantially higher\nROGUE scores compared to those produced by the baseline.", "published": "2015-02-28 04:26:21", "link": "http://arxiv.org/abs/1503.00064v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Non-linear Learning for Statistical Machine Translation", "abstract": "Modern statistical machine translation (SMT) systems usually use a linear\ncombination of features to model the quality of each translation hypothesis.\nThe linear combination assumes that all the features are in a linear\nrelationship and constrains that each feature interacts with the rest features\nin an linear manner, which might limit the expressive power of the model and\nlead to a under-fit model on the current data. In this paper, we propose a\nnon-linear modeling for the quality of translation hypotheses based on neural\nnetworks, which allows more complex interaction between features. A learning\nframework is presented for training the non-linear models. We also discuss\npossible heuristics in designing the network structure which may improve the\nnon-linear learning performance. Experimental results show that with the basic\nfeatures of a hierarchical phrase-based machine translation system, our method\nproduce translations that are better than a linear model.", "published": "2015-02-28 09:53:32", "link": "http://arxiv.org/abs/1503.00107v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "abstract": "Recursive neural models, which use syntactic parse trees to recursively\ngenerate representations bottom-up, are a popular architecture. But there have\nnot been rigorous evaluations showing for exactly which tasks this syntax-based\nmethod is appropriate. In this paper we benchmark {\\bf recursive} neural models\nagainst sequential {\\bf recurrent} neural models (simple recurrent and LSTM\nmodels), enforcing apples-to-apples comparison as much as possible. We\ninvestigate 4 tasks: (1) sentiment classification at the sentence level and\nphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;\n(4) semantic relation extraction (e.g., {\\em component-whole} between nouns).\n  Our goal is to understand better when, and why, recursive models can\noutperform simpler models. We find that recursive models help mainly on tasks\n(like semantic relation extraction) that require associating headwords across a\nlong distance, particularly on very long sequences. We then introduce a method\nfor allowing recurrent models to achieve similar performance: breaking long\nsentences into clause-like units at punctuation and processing them separately\nbefore combining. Our results thus help understand the limitations of both\nclasses of models, and suggest directions for improving recurrent models.", "published": "2015-02-28 21:39:31", "link": "http://arxiv.org/abs/1503.00185v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "abstract": "Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).", "published": "2015-02-28 06:31:50", "link": "http://arxiv.org/abs/1503.00075v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
