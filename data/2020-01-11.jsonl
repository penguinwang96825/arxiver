{"title": "PatentTransformer-2: Controlling Patent Text Generation by Structural\n  Metadata", "abstract": "PatentTransformer is our codename for patent text generation based on\nTransformer-based models. Our goal is \"Augmented Inventing.\" In this second\nversion, we leverage more of the structural metadata in patents. The structural\nmetadata includes patent title, abstract, and dependent claim, in addition to\nindependent claim previously. Metadata controls what kind of patent text for\nthe model to generate. Also, we leverage the relation between metadata to build\na text-to-text generation flow, for example, from a few words to a title, the\ntitle to an abstract, the abstract to an independent claim, and the independent\nclaim to multiple dependent claims. The text flow can go backward because the\nrelation is trained bidirectionally. We release our GPT-2 models trained from\nscratch and our code for inference so that readers can verify and generate\npatent text on their own. As for generation quality, we measure it by both\nROUGE and Google Universal Sentence Encoder.", "published": "2020-01-11 03:54:31", "link": "http://arxiv.org/abs/2001.03708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Cross-Context Entity Representations from Text", "abstract": "Language modeling tasks, in which words, or word-pieces, are predicted on the\nbasis of a local context, have been very effective for learning word embeddings\nand context dependent representations of phrases. Motivated by the observation\nthat efforts to code world knowledge into machine readable knowledge bases or\nhuman readable encyclopedias tend to be entity-centric, we investigate the use\nof a fill-in-the-blank task to learn context independent representations of\nentities from the text contexts in which those entities were mentioned. We show\nthat large scale training of neural models allows us to learn high quality\nentity representations, and we demonstrate successful results on four domains:\n(1) existing entity-level typing benchmarks, including a 64% error reduction\nover previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot\ncategory reconstruction task; (3) existing entity linking benchmarks, where we\nmatch the state-of-the-art on CoNLL-Aida without linking-specific features and\nobtain a score of 89.8% on TAC-KBP 2010 without using any alias table, external\nknowledge base or in domain training data and (4) answering trivia questions,\nwhich uniquely identify entities. Our global entity representations encode\nfine-grained type categories, such as Scottish footballers, and can answer\ntrivia questions such as: Who was the last inmate of Spandau jail in Berlin?", "published": "2020-01-11 15:30:56", "link": "http://arxiv.org/abs/2001.03765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring and Improving Robustness of Multi Task Deep Neural Networks\n  via Domain Agnostic Defenses", "abstract": "In this paper, we explore the robustness of the Multi-Task Deep Neural\nNetworks (MT-DNN) against non-targeted adversarial attacks across Natural\nLanguage Understanding (NLU) tasks as well as some possible ways to defend\nagainst them. Liu et al., have shown that the Multi-Task Deep Neural Network,\ndue to the regularization effect produced when training as a result of its\ncross task data, is more robust than a vanilla BERT model trained only on one\ntask (1.1%-1.5% absolute difference). We further show that although the MT-DNN\nhas generalized better, making it easily transferable across domains and tasks,\nit can still be compromised as after only 2 attacks (1-character and\n2-character) the accuracy drops by 42.05% and 32.24% for the SNLI and SciTail\ntasks. Finally, we propose a domain agnostic defense which restores the model's\naccuracy (36.75% and 25.94% respectively) as opposed to a general-purpose\ndefense or an off-the-shelf spell checker.", "published": "2020-01-11 18:05:15", "link": "http://arxiv.org/abs/2001.05286v1", "categories": ["cs.CL", "68T35 (Primary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Continuous Space Neural Language Model for Bengali Language", "abstract": "Language models are generally employed to estimate the probability\ndistribution of various linguistic units, making them one of the fundamental\nparts of natural language processing. Applications of language models include a\nwide spectrum of tasks such as text summarization, translation and\nclassification. For a low resource language like Bengali, the research in this\narea so far can be considered to be narrow at the very least, with some\ntraditional count based models being proposed. This paper attempts to address\nthe issue and proposes a continuous-space neural language model, or more\nspecifically an ASGD weight dropped LSTM language model, along with techniques\nto efficiently train it for Bengali Language. The performance analysis with\nsome currently existing count based models illustrated in this paper also shows\nthat the proposed architecture outperforms its counterparts by achieving an\ninference perplexity as low as 51.2 on the held out data set for Bengali.", "published": "2020-01-11 14:50:57", "link": "http://arxiv.org/abs/2001.05315v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding", "abstract": "Visual-semantic embedding enables various tasks such as image-text retrieval,\nimage captioning, and visual question answering. The key to successful\nvisual-semantic embedding is to express visual and textual data properly by\naccounting for their intricate relationship. While previous studies have\nachieved much advance by encoding the visual and textual data into a joint\nspace where similar concepts are closely located, they often represent data by\na single vector ignoring the presence of multiple important components in an\nimage or text. Thus, in addition to the joint embedding space, we propose a\nnovel multi-head self-attention network to capture various components of visual\nand textual data by attending to important parts in data. Our approach achieves\nthe new state-of-the-art results in image-text retrieval tasks on MS-COCO and\nFlicker30K datasets. Through the visualization of the attention maps that\ncapture distinct semantic components at multiple positions in the image and the\ntext, we demonstrate that our method achieves an effective and interpretable\nvisual-semantic joint space.", "published": "2020-01-11 05:50:19", "link": "http://arxiv.org/abs/2001.03712v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Spoken Language Understanding By Exploiting ASR N-best\n  Hypotheses", "abstract": "In a modern spoken language understanding (SLU) system, the natural language\nunderstanding (NLU) module takes interpretations of a speech from the automatic\nspeech recognition (ASR) module as the input. The NLU module usually uses the\nfirst best interpretation of a given speech in downstream tasks such as domain\nand intent classification. However, the ASR module might misrecognize some\nspeeches and the first best interpretation could be erroneous and noisy. Solely\nrelying on the first best interpretation could make the performance of\ndownstream tasks non-optimal. To address this issue, we introduce a series of\nsimple yet efficient models for improving the understanding of semantics of the\ninput speeches by collectively exploiting the n-best speech interpretations\nfrom the ASR module.", "published": "2020-01-11 05:48:52", "link": "http://arxiv.org/abs/2001.05284v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Embedding Compression with Isotropic Iterative Quantization", "abstract": "Continuous representation of words is a standard component in deep\nlearning-based NLP models. However, representing a large vocabulary requires\nsignificant memory, which can cause problems, particularly on\nresource-constrained platforms. Therefore, in this paper we propose an\nisotropic iterative quantization (IIQ) approach for compressing embedding\nvectors into binary ones, leveraging the iterative quantization technique well\nestablished for image retrieval, while satisfying the desired isotropic\nproperty of PMI based models. Experiments with pre-trained embeddings (i.e.,\nGloVe and HDC) demonstrate a more than thirty-fold compression ratio with\ncomparable and sometimes even improved performance over the original\nreal-valued embedding vectors.", "published": "2020-01-11 20:53:55", "link": "http://arxiv.org/abs/2001.05314v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Authorship Attribution in Bangla literature using Character-level CNN", "abstract": "Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results.", "published": "2020-01-11 14:54:04", "link": "http://arxiv.org/abs/2001.05316v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On- Device Information Extraction from Screenshots in form of tags", "abstract": "We propose a method to make mobile screenshots easily searchable. In this\npaper, we present the workflow in which we: 1) preprocessed a collection of\nscreenshots, 2) identified script presentin image, 3) extracted unstructured\ntext from images, 4) identifiedlanguage of the extracted text, 5) extracted\nkeywords from the text, 6) identified tags based on image features, 7) expanded\ntag set by identifying related keywords, 8) inserted image tags with relevant\nimages after ranking and indexed them to make it searchable on device. We made\nthe pipeline which supports multiple languages and executed it on-device, which\naddressed privacy concerns. We developed novel architectures for components in\nthe pipeline, optimized performance and memory for on-device computation. We\nobserved from experimentation that the solution developed can reduce overall\nuser effort and improve end user experience while searching, whose results are\npublished.", "published": "2020-01-11 12:15:30", "link": "http://arxiv.org/abs/2001.06094v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
