{"title": "Emoji-Based Transfer Learning for Sentiment Tasks", "abstract": "Sentiment tasks such as hate speech detection and sentiment analysis,\nespecially when performed on languages other than English, are often\nlow-resource. In this study, we exploit the emotional information encoded in\nemojis to enhance the performance on a variety of sentiment tasks. This is done\nusing a transfer learning approach, where the parameters learned by an\nemoji-based source task are transferred to a sentiment target task. We analyse\nthe efficacy of the transfer under three conditions, i.e. i) the emoji content\nand ii) label distribution of the target task as well as iii) the difference\nbetween monolingually and multilingually learned source tasks. We find i.a.\nthat the transfer is most beneficial if the target task is balanced with high\nemoji content. Monolingually learned source tasks have the benefit of taking\ninto account the culturally specific use of emojis and gain up to F1 +0.280\nover the baseline.", "published": "2021-02-12 10:05:02", "link": "http://arxiv.org/abs/2102.06423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing\n  Task for Low-resource Morphologically Rich Languages", "abstract": "Neural dependency parsing has achieved remarkable performance for many\ndomains and languages. The bottleneck of massive labeled data limits the\neffectiveness of these approaches for low resource languages. In this work, we\nfocus on dependency parsing for morphological rich languages (MRLs) in a\nlow-resource setting. Although morphological information is essential for the\ndependency parsing task, the morphological disambiguation and lack of powerful\nanalyzers pose challenges to get this information for MRLs. To address these\nchallenges, we propose simple auxiliary tasks for pretraining. We perform\nexperiments on 10 MRLs in low-resource settings to measure the efficacy of our\nproposed pretraining method and observe an average absolute gain of 2 points\n(UAS) and 3.6 points (LAS). Code and data available at:\nhttps://github.com/jivnesh/LCM", "published": "2021-02-12 14:26:58", "link": "http://arxiv.org/abs/2102.06551v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Learning in Neural Machine Translation using Bilingual\n  Dictionaries", "abstract": "While recent advances in deep learning led to significant improvements in\nmachine translation, neural machine translation is often still not able to\ncontinuously adapt to the environment. For humans, as well as for machine\ntranslation, bilingual dictionaries are a promising knowledge source to\ncontinuously integrate new knowledge. However, their exploitation poses several\nchallenges: The system needs to be able to perform one-shot learning as well as\nmodel the morphology of source and target language.\n  In this work, we proposed an evaluation framework to assess the ability of\nneural machine translation to continuously learn new phrases. We integrate\none-shot learning methods for neural machine translation with different word\nrepresentations and show that it is important to address both in order to\nsuccessfully make use of bilingual dictionaries. By addressing both challenges\nwe are able to improve the ability to translate new, rare words and phrases\nfrom 30% to up to 70%. The correct lemma is even generated by more than 90%.", "published": "2021-02-12 14:46:13", "link": "http://arxiv.org/abs/2102.06558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot Neural Machine Translation on Language-specific\n  Encoders-Decoders", "abstract": "Recently, universal neural machine translation (NMT) with shared\nencoder-decoder gained good performance on zero-shot translation. Unlike\nuniversal NMT, jointly trained language-specific encoders-decoders aim to\nachieve universal representation across non-shared modules, each of which is\nfor a language or language family. The non-shared architecture has the\nadvantage of mitigating internal language competition, especially when the\nshared vocabulary and model parameters are restricted in their size. However,\nthe performance of using multiple encoders and decoders on zero-shot\ntranslation still lags behind universal NMT. In this work, we study zero-shot\ntranslation using language-specific encoders-decoders. We propose to generalize\nthe non-shared architecture and universal NMT by differentiating the\nTransformer layers between language-specific and interlingua. By selectively\nsharing parameters and applying cross-attentions, we explore maximizing the\nrepresentation universality and realizing the best alignment of\nlanguage-agnostic information. We also introduce a denoising auto-encoding\n(DAE) objective to jointly train the model with the translation task in a\nmulti-task manner. Experiments on two public multilingual parallel datasets\nshow that our proposed model achieves a competitive or better results than\nuniversal NMT and strong pivot baseline. Moreover, we experiment incrementally\nadding new language to the trained model by only updating the new model\nparameters. With this little effort, the zero-shot translation between this\nnewly added language and existing languages achieves a comparable result with\nthe model trained jointly from scratch on all languages.", "published": "2021-02-12 15:36:33", "link": "http://arxiv.org/abs/2102.06578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Information Preserving for Graph-to-Text Generation", "abstract": "The task of graph-to-text generation aims at producing sentences that\npreserve the meaning of input graphs. As a crucial defect, the current\nstate-of-the-art models may mess up or even drop the core structural\ninformation of input graphs when generating outputs. We propose to tackle this\nproblem by leveraging richer training signals that can guide our model for\npreserving input information. In particular, we introduce two types of\nautoencoding losses, each individually focusing on different aspects (a.k.a.\nviews) of input graphs. The losses are then back-propagated to better calibrate\nour model via multi-task training. Experiments on two benchmarks for\ngraph-to-text generation show the effectiveness of our approach over a\nstate-of-the-art baseline. Our code is available at\n\\url{http://github.com/Soistesimmer/AMR-multiview}.", "published": "2021-02-12 20:09:01", "link": "http://arxiv.org/abs/2102.06749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "They, Them, Theirs: Rewriting with Gender-Neutral English", "abstract": "Responsible development of technology involves applications being inclusive\nof the diverse set of users they hope to support. An important part of this is\nunderstanding the many ways to refer to a person and being able to fluently\nchange between the different forms as needed. We perform a case study on the\nsingular they, a common way to promote gender inclusion in English. We define a\nre-writing task, create an evaluation benchmark, and show how a model can be\ntrained to produce gender-neutral English with <1% word error rate with no\nhuman-labeled data. We discuss the practical applications and ethical\nconsiderations of the task, providing direction for future work into inclusive\nnatural language systems.", "published": "2021-02-12 21:47:48", "link": "http://arxiv.org/abs/2102.06788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InsNet: An Efficient, Flexible, and Performant Insertion-based Text\n  Generation Model", "abstract": "We propose InsNet, an expressive insertion-based text generator with\nefficient training and flexible decoding (parallel or sequential). Unlike most\nexisting insertion-based text generation works that require re-encoding of the\ncontext after each insertion operation and thus are inefficient to train,\nInsNet only requires one pass of context encoding for the entire sequence\nduring training by introducing a novel insertion-oriented position encoding and\na light-weighted slot representation strategy to enable computation sharing.\nFurthermore, we propose an algorithm InsNet-Dinic to better determine the\nparallelization of insertion operations that provides a controllable switch\nbetween parallel and sequential decoding, making it flexible to handle more\nparallelizable tasks such as machine translation with efficient decoding, or\nless parallelizable tasks such as open-domain text generation to guarantee\nhigh-quality outputs. Experiments on two lexically constrained text generation\ndatasets and three machine translation datasets demonstrate InsNet's advantages\nover previous insertion-based methods in terms of training speed, inference\nefficiency, and generation quality.", "published": "2021-02-12 11:05:02", "link": "http://arxiv.org/abs/2102.11008v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Inverse Text Normalization", "abstract": "While there have been several contributions exploring state of the art\ntechniques for text normalization, the problem of inverse text normalization\n(ITN) remains relatively unexplored. The best known approaches leverage finite\nstate transducer (FST) based models which rely on manually curated rules and\nare hence not scalable. We propose an efficient and robust neural solution for\nITN leveraging transformer based seq2seq models and FST-based text\nnormalization techniques for data preparation. We show that this can be easily\nextended to other languages without the need for a linguistic expert to\nmanually curate them. We then present a hybrid framework for integrating Neural\nITN with an FST to overcome common recoverable errors in production\nenvironments. Our empirical evaluations show that the proposed solution\nminimizes incorrect perturbations (insertions, deletions and substitutions) to\nASR output and maintains high quality even on out of domain data. A transformer\nbased model infused with pretraining consistently achieves a lower WER across\nseveral datasets and is able to outperform baselines on English, Spanish,\nGerman and Italian datasets.", "published": "2021-02-12 07:53:53", "link": "http://arxiv.org/abs/2102.06380v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multiversal views on language models", "abstract": "The virtuosity of language models like GPT-3 opens a new world of possibility\nfor human-AI collaboration in writing. In this paper, we present a framework in\nwhich generative language models are conceptualized as multiverse generators.\nThis framework also applies to human imagination and is core to how we read and\nwrite fiction. We call for exploration into this commonality through new forms\nof interfaces which allow humans to couple their imagination to AI to write,\nexplore, and understand non-linear fiction. We discuss the early insights we\nhave gained from actively pursuing this approach by developing and testing a\nnovel multiversal GPT-3-assisted writing interface.", "published": "2021-02-12 08:28:28", "link": "http://arxiv.org/abs/2102.06391v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Transformer Language Models with LSTM-based Cross-utterance Information\n  Representation", "abstract": "The effective incorporation of cross-utterance information has the potential\nto improve language models (LMs) for automatic speech recognition (ASR). To\nextract more powerful and robust cross-utterance representations for the\nTransformer LM (TLM), this paper proposes the R-TLM which uses hidden states in\na long short-term memory (LSTM) LM. To encode the cross-utterance information,\nthe R-TLM incorporates an LSTM module together with a segment-wise recurrence\nin some of the Transformer blocks. In addition to the LSTM module output, a\nshortcut connection using a fusion layer that bypasses the LSTM module is also\ninvestigated. The proposed system was evaluated on the AMI meeting corpus, the\nEval2000 and the RT03 telephone conversation evaluation sets. The best R-TLM\nachieved 0.9%, 0.6%, and 0.8% absolute WER reductions over the single-utterance\nTLM baseline, and 0.5%, 0.3%, 0.2% absolute WER reductions over a strong\ncross-utterance TLM baseline on the AMI evaluation set, Eval2000 and RT03\nrespectively. Improvements on Eval2000 and RT03 were further supported by\nsignificance tests. R-TLMs were found to have better LM scores on words where\nrecognition errors are more likely to occur. The R-TLM WER can be further\nreduced by interpolation with an LSTM-LM.", "published": "2021-02-12 12:12:29", "link": "http://arxiv.org/abs/2102.06474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Two Training Strategies for Improving Relation Extraction over Universal\n  Graph", "abstract": "This paper explores how the Distantly Supervised Relation Extraction (DS-RE)\ncan benefit from the use of a Universal Graph (UG), the combination of a\nKnowledge Graph (KG) and a large-scale text collection. A straightforward\nextension of a current state-of-the-art neural model for DS-RE with a UG may\nlead to degradation in performance. We first report that this degradation is\nassociated with the difficulty in learning a UG and then propose two training\nstrategies: (1) Path Type Adaptive Pretraining, which sequentially trains the\nmodel with different types of UG paths so as to prevent the reliance on a\nsingle type of UG path; and (2) Complexity Ranking Guided Attention mechanism,\nwhich restricts the attention span according to the complexity of a UG path so\nas to force the model to extract features not only from simple UG paths but\nalso from complex ones. Experimental results on both biomedical and NYT10\ndatasets prove the robustness of our methods and achieve a new state-of-the-art\nresult on the NYT10 dataset. The code and datasets used in this paper are\navailable at https://github.com/baodaiqin/UGDSRE.", "published": "2021-02-12 14:09:35", "link": "http://arxiv.org/abs/2102.06540v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid phonetic-neural model for correction in speech recognition\n  systems", "abstract": "Automatic speech recognition (ASR) is a relevant area in multiple settings\nbecause it provides a natural communication mechanism between applications and\nusers. ASRs often fail in environments that use language specific to particular\napplication domains. Some strategies have been explored to reduce errors in\nclosed ASRs through post-processing, particularly automatic spell checking, and\ndeep learning approaches. In this article, we explore using a deep neural\nnetwork to refine the results of a phonetic correction algorithm applied to a\ntelesales audio database. The results exhibit a reduction in the word error\nrate (WER), both in the original transcription and in the phonetic correction,\nwhich shows the viability of deep learning models together with post-processing\ncorrection strategies to reduce errors made by closed ASRs in specific language\ndomains.", "published": "2021-02-12 19:57:16", "link": "http://arxiv.org/abs/2102.06744v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Do as I mean, not as I say: Sequence Loss Training for Spoken Language\n  Understanding", "abstract": "Spoken language understanding (SLU) systems extract transcriptions, as well\nas semantics of intent or named entities from speech, and are essential\ncomponents of voice activated systems. SLU models, which either directly\nextract semantics from audio or are composed of pipelined automatic speech\nrecognition (ASR) and natural language understanding (NLU) models, are\ntypically trained via differentiable cross-entropy losses, even when the\nrelevant performance metrics of interest are word or semantic error rates. In\nthis work, we propose non-differentiable sequence losses based on SLU metrics\nas a proxy for semantic error and use the REINFORCE trick to train ASR and SLU\nmodels with this loss. We show that custom sequence loss training is the\nstate-of-the-art on open SLU datasets and leads to 6% relative improvement in\nboth ASR and NLU performance metrics on large proprietary datasets. We also\ndemonstrate how the semantic sequence loss training paradigm can be used to\nupdate ASR and SLU models without transcripts, using semantic feedback alone.", "published": "2021-02-12 20:09:08", "link": "http://arxiv.org/abs/2102.06750v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Supporting search engines with knowledge and context", "abstract": "Search engines leverage knowledge to improve information access. In order to\neffectively leverage knowledge, search engines should account for context,\ni.e., information about the user and query. In this thesis, we aim to support\nsearch engines in leveraging knowledge while accounting for context. In the\nfirst part of this thesis, we study how to make structured knowledge more\naccessible to the user when the search engine proactively provides such\nknowledge as context to enrich search results. As a first task, we study how to\nretrieve descriptions of knowledge facts from a text corpus. Next, we study how\nto automatically generate knowledge fact descriptions. And finally, we study\nhow to contextualize knowledge facts, that is, to automatically find facts\nrelated to a query fact. In the second part of this thesis, we study how to\nimprove interactive knowledge gathering. We focus on conversational search,\nwhere the user interacts with the search engine to gather knowledge over large\nunstructured knowledge repositories. We focus on multi-turn passage retrieval\nas an instance of conversational search. We propose to model query resolution\nas a term classification task and propose a method to address it. In the final\npart of this thesis, we focus on search engine support for professional writers\nin the news domain. We study how to support such writers create\nevent-narratives by exploring knowledge from a corpus of news articles. We\npropose a dataset construction procedure for this task that relies on existing\nnews articles to simulate incomplete narratives and relevant articles. We study\nthe performance of multiple rankers, lexical and semantic, and provide insights\ninto the characteristics of this task.", "published": "2021-02-12 20:28:25", "link": "http://arxiv.org/abs/2102.06762v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Classic and Neural Lexical Translation Models for Information\n  Retrieval: Interpretability, Effectiveness, and Efficiency Benefits", "abstract": "We study the utility of the lexical translation model (IBM Model 1) for\nEnglish text retrieval, in particular, its neural variants that are trained\nend-to-end. We use the neural Model1 as an aggregator layer applied to\ncontext-free or contextualized query/document embeddings. This new approach to\ndesign a neural ranking system has benefits for effectiveness, efficiency, and\ninterpretability. Specifically, we show that adding an interpretable neural\nModel 1 layer on top of BERT-based contextualized embeddings (1) does not\ndecrease accuracy and/or efficiency; and (2) may overcome the limitation on the\nmaximum sequence length of existing BERT models. The context-free neural Model\n1 is less effective than a BERT-based ranking model, but it can run efficiently\non a CPU (without expensive index-time precomputation or query-time operations\non large tensors). Using Model 1 we produced best neural and non-neural runs on\nthe MS MARCO document ranking leaderboard in late 2020.", "published": "2021-02-12 23:21:55", "link": "http://arxiv.org/abs/2102.06815v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Characterizing English Variation across Social Media Communities with\n  BERT", "abstract": "Much previous work characterizing language variation across Internet social\ngroups has focused on the types of words used by these groups. We extend this\ntype of study by employing BERT to characterize variation in the senses of\nwords as well, analyzing two months of English comments in 474 Reddit\ncommunities. The specificity of different sense clusters to a community,\ncombined with the specificity of a community's unique word types, is used to\nidentify cases where a social group's language deviates from the norm. We\nvalidate our metrics using user-created glossaries and draw on sociolinguistic\ntheories to connect language variation with trends in community behavior. We\nfind that communities with highly distinctive language are medium-sized, and\ntheir loyal and highly engaged users interact in dense networks.", "published": "2021-02-12 23:50:57", "link": "http://arxiv.org/abs/2102.06820v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "On Automatic Parsing of Log Records", "abstract": "Software log analysis helps to maintain the health of software solutions and\nensure compliance and security. Existing software systems consist of\nheterogeneous components emitting logs in various formats. A typical solution\nis to unify the logs using manually built parsers, which is laborious.\n  Instead, we explore the possibility of automating the parsing task by\nemploying machine translation (MT). We create a tool that generates synthetic\nApache log records which we used to train recurrent-neural-network-based MT\nmodels. Models' evaluation on real-world logs shows that the models can learn\nApache log format and parse individual log records. The median relative edit\ndistance between an actual real-world log record and the MT prediction is less\nthan or equal to 28%. Thus, we show that log parsing using an MT approach is\npromising.", "published": "2021-02-12 00:27:41", "link": "http://arxiv.org/abs/2102.06320v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep\n  VAE with Residual Attention", "abstract": "This paper proposes VARA-TTS, a non-autoregressive (non-AR) text-to-speech\n(TTS) model using a very deep Variational Autoencoder (VDVAE) with Residual\nAttention mechanism, which refines the textual-to-acoustic alignment\nlayer-wisely. Hierarchical latent variables with different temporal resolutions\nfrom the VDVAE are used as queries for residual attention module. By leveraging\nthe coarse global alignment from previous attention layer as an extra input,\nthe following attention layer can produce a refined version of alignment. This\namortizes the burden of learning the textual-to-acoustic alignment among\nmultiple attention layers and outperforms the use of only a single attention\nlayer in robustness. An utterance-level speaking speed factor is computed by a\njointly-trained speaking speed predictor, which takes the mean-pooled latent\nvariables of the coarsest layer as input, to determine number of acoustic\nframes at inference. Experimental results show that VARA-TTS achieves slightly\ninferior speech quality to an AR counterpart Tacotron 2 but an\norder-of-magnitude speed-up at inference; and outperforms an analogous non-AR\nmodel, BVAE-TTS, in terms of speech quality.", "published": "2021-02-12 10:26:57", "link": "http://arxiv.org/abs/2102.06431v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Punctuation Prediction with Contextual Dropout", "abstract": "Automatic speech recognition (ASR) is widely used in consumer electronics.\nASR greatly improves the utility and accessibility of technology, but usually\nthe output is only word sequences without punctuation. This can result in\nambiguity in inferring user-intent. We first present a transformer-based\napproach for punctuation prediction that achieves 8% improvement on the IWSLT\n2012 TED Task, beating the previous state of the art [1]. We next describe our\nmultimodal model that learns from both text and audio, which achieves 8%\nimprovement over the text-only algorithm on an internal dataset for which we\nhave both the audio and transcriptions. Finally, we present an approach to\nlearning a model using contextual dropout that allows us to handle variable\namounts of future context at test time.", "published": "2021-02-12 22:15:30", "link": "http://arxiv.org/abs/2102.11012v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Optimizing Inference Performance of Transformers on CPUs", "abstract": "The Transformer architecture revolutionized the field of natural language\nprocessing (NLP). Transformers-based models (e.g., BERT) power many important\nWeb services, such as search, translation, question-answering, etc. While\nenormous research attention is paid to the training of those models, relatively\nlittle efforts are made to improve their inference performance. This paper\ncomes to address this gap by presenting an empirical analysis of scalability\nand performance of inferencing a Transformer-based model on CPUs. Focusing on\nthe highly popular BERT model, we identify key components of the Transformer\narchitecture where the bulk of the computation happens, and propose three\noptimizations to speed them up. The optimizations are evaluated using the\ninference benchmark from HuggingFace, and are shown to achieve the speedup of\nup to x2.37. The considered optimizations do not require any changes to the\nimplementation of the models nor affect their accuracy.", "published": "2021-02-12 17:01:35", "link": "http://arxiv.org/abs/2102.06621v3", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG", "cs.MS", "I.2.0; D.4.8; G.4"], "primary_category": "cs.CL"}
{"title": "Data Augmentation with Signal Companding for Detection of Logical Access\n  Attacks", "abstract": "The recent advances in voice conversion (VC) and text-to-speech (TTS) make it\npossible to produce natural sounding speech that poses threat to automatic\nspeaker verification (ASV) systems. To this end, research on spoofing\ncountermeasures has gained attention to protect ASV systems from such attacks.\nWhile the advanced spoofing countermeasures are able to detect known nature of\nspoofing attacks, they are not that effective under unknown attacks. In this\nwork, we propose a novel data augmentation technique using a-law and mu-law\nbased signal companding. We believe that the proposed method has an edge over\ntraditional data augmentation by adding small perturbation or quantization\nnoise. The studies are conducted on ASVspoof 2019 logical access corpus using\nlight convolutional neural network based system. We find that the proposed data\naugmentation technique based on signal companding outperforms the\nstate-of-the-art spoofing countermeasures showing ability to handle unknown\nnature of attacks.", "published": "2021-02-12 02:51:06", "link": "http://arxiv.org/abs/2102.06332v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhancing into the codec: Noise Robust Speech Coding with\n  Vector-Quantized Autoencoders", "abstract": "Audio codecs based on discretized neural autoencoders have recently been\ndeveloped and shown to provide significantly higher compression levels for\ncomparable quality speech output. However, these models are tightly coupled\nwith speech content, and produce unintended outputs in noisy conditions. Based\non VQ-VAE autoencoders with WaveRNN decoders, we develop compressor-enhancer\nencoders and accompanying decoders, and show that they operate well in noisy\nconditions. We also observe that a compressor-enhancer model performs better on\nclean speech inputs than a compressor model trained only on clean speech.", "published": "2021-02-12 16:42:19", "link": "http://arxiv.org/abs/2102.06610v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "End-to-end Audio-visual Speech Recognition with Conformers", "abstract": "In this work, we present a hybrid CTC/Attention model based on a ResNet-18\nand Convolution-augmented transformer (Conformer), that can be trained in an\nend-to-end manner. In particular, the audio and visual encoders learn to\nextract features directly from raw pixels and audio waveforms, respectively,\nwhich are then fed to conformers and then fusion takes place via a Multi-Layer\nPerceptron (MLP). The model learns to recognise characters using a combination\nof CTC and an attention mechanism. We show that end-to-end training, instead of\nusing pre-computed visual features which is common in the literature, the use\nof a conformer, instead of a recurrent network, and the use of a\ntransformer-based language model, significantly improve the performance of our\nmodel. We present results on the largest publicly available datasets for\nsentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3), respectively. The results show that our proposed\nmodels raise the state-of-the-art performance by a large margin in audio-only,\nvisual-only, and audio-visual experiments.", "published": "2021-02-12 18:00:08", "link": "http://arxiv.org/abs/2102.06657v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Bi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised\n  Pre-training and Its Application to Children's ASR", "abstract": "We present a bidirectional unsupervised model pre-training (UPT) method and\napply it to children's automatic speech recognition (ASR). An obstacle to\nimproving child ASR is the scarcity of child speech databases. A common\napproach to alleviate this problem is model pre-training using data from adult\nspeech. Pre-training can be done using supervised (SPT) or unsupervised\nmethods, depending on the availability of annotations. Typically, SPT performs\nbetter. In this paper, we focus on UPT to address the situations when\npre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT\nmethod, predicts frames from only one direction, limiting its use to\nuni-directional pre-training. Conventional bidirectional UPT methods, however,\npredict only a small portion of frames. To extend the benefits of APC to\nbi-directional pre-training, Bi-APC is proposed. We then use adaptation\ntechniques to transfer knowledge learned from adult speech (using the\nLibrispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid\nsystems are investigated. For the uni-LSTM structure, APC obtains similar WER\nimprovements to SPT over the baseline. When applied to BLSTM, however, APC is\nnot as competitive as SPT, but our proposed Bi-APC has comparable improvements\nto SPT.", "published": "2021-02-12 23:30:45", "link": "http://arxiv.org/abs/2102.06816v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Joint Dereverberation and Separation with Iterative Source Steering", "abstract": "We propose a new algorithm for joint dereverberation and blind source\nseparation (DR-BSS). Our work builds upon the IRLMA-T framework that applies a\nunified filter combining dereverberation and separation. One drawback of this\nframework is that it requires several matrix inversions, an operation\ninherently costly and with potential stability issues. We leverage the recently\nintroduced iterative source steering (ISS) updates to propose two algorithms\nmitigating this issue. Albeit derived from first principles, the first\nalgorithm turns out to be a natural combination of weighted prediction error\n(WPE) dereverberation and ISS-based BSS, applied alternatingly. In this case,\nwe manage to reduce the number of matrix inversion to only one per iteration\nand source. The second algorithm updates the ILRMA-T matrix using only\nsequential ISS updates requiring no matrix inversion at all. Its implementation\nis straightforward and memory efficient. Numerical experiments demonstrate that\nboth methods achieve the same final performance as ILRMA-T in terms of several\nrelevant objective metrics. In the important case of two sources, the number of\niterations required is also similar.", "published": "2021-02-12 01:15:18", "link": "http://arxiv.org/abs/2102.06322v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Contrastive Unsupervised Learning for Speech Emotion Recognition", "abstract": "Speech emotion recognition (SER) is a key technology to enable more natural\nhuman-machine communication. However, SER has long suffered from a lack of\npublic large-scale labeled datasets. To circumvent this problem, we investigate\nhow unsupervised representation learning on unlabeled datasets can benefit SER.\nWe show that the contrastive predictive coding (CPC) method can learn salient\nrepresentations from unlabeled datasets, which improves emotion recognition\nperformance. In our experiments, this method achieved state-of-the-art\nconcordance correlation coefficient (CCC) performance for all emotion\nprimitives (activation, valence, and dominance) on IEMOCAP. Additionally, on\nthe MSP- Podcast dataset, our method obtained considerable performance\nimprovements compared to baselines.", "published": "2021-02-12 06:06:02", "link": "http://arxiv.org/abs/2102.06357v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mind the beat: detecting audio onsets from EEG recordings of music\n  listening", "abstract": "We propose a deep learning approach to predicting audio event onsets in\nelectroencephalogram (EEG) recorded from users as they listen to music. We use\na publicly available dataset containing ten contemporary songs and concurrently\nrecorded EEG. We generate a sequence of onset labels for the songs in our\ndataset and trained neural networks (a fully connected network (FCN) and a\nrecurrent neural network (RNN)) to parse one second windows of input EEG to\npredict one second windows of onsets in the audio. We compare our RNN network\nto both the standard spectral-flux based novelty function and the FCN. We find\nthat our RNN was able to produce results that reflected its ability to\ngeneralize better than the other methods.\n  Since there are no pre-existing works on this topic, the numbers presented in\nthis paper may serve as useful benchmarks for future approaches to this\nresearch problem.", "published": "2021-02-12 08:42:02", "link": "http://arxiv.org/abs/2102.06393v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Guided Variational Autoencoder for Speech Enhancement With a Supervised\n  Classifier", "abstract": "Recently, variational autoencoders have been successfully used to learn a\nprobabilistic prior over speech signals, which is then used to perform speech\nenhancement. However, variational autoencoders are trained on clean speech\nonly, which results in a limited ability of extracting the speech signal from\nnoisy speech compared to supervised approaches. In this paper, we propose to\nguide the variational autoencoder with a supervised classifier separately\ntrained on noisy speech. The estimated label is a high-level categorical\nvariable describing the speech signal (e.g. speech activity) allowing for a\nmore informed latent distribution compared to the standard variational\nautoencoder. We evaluate our method with different types of labels on real\nrecordings of different noisy environments. Provided that the label better\ninforms the latent distribution and that the classifier achieves good\nperformance, the proposed approach outperforms the standard variational\nautoencoder and a conventional neural network-based supervised approach.", "published": "2021-02-12 11:32:48", "link": "http://arxiv.org/abs/2102.06454v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL\n  Sound Field Dataset", "abstract": "Knowledge of loudspeaker responses are useful in a number of applications,\nwhere a sound system is located inside a room that alters the listening\nexperience depending on position within the room. Acquisition of sound fields\nfor sound sources located in reverberant rooms can be achieved through labor\nintensive measurements of impulse response functions covering the room, or\nalternatively by means of reconstruction methods which can potentially require\nsignificantly fewer measurements. This paper extends evaluations of sound field\nreconstruction at low frequencies by introducing a dataset with measurements\nfrom four real rooms. The ISOBEL Sound Field dataset is publicly available, and\naims to bridge the gap between synthetic and real-world sound fields in\nrectangular rooms. Moreover, the paper advances on a recent deep learning-based\nmethod for sound field reconstruction using a very low number of microphones,\nand proposes an approach for modeling both magnitude and phase response in a\nU-Net-like neural network architecture. The complex-valued sound field\nreconstruction demonstrates that the estimated room transfer functions are of\nhigh enough accuracy to allow for personalized sound zones with contrast ratios\ncomparable to ideal room transfer functions using 15 microphones below 150 Hz.", "published": "2021-02-12 11:34:18", "link": "http://arxiv.org/abs/2102.06455v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Content-Aware Speaker Embeddings for Speaker Diarisation", "abstract": "Recent speaker diarisation systems often convert variable length speech\nsegments into fixed-length vector representations for speaker clustering, which\nare known as speaker embeddings. In this paper, the content-aware speaker\nembeddings (CASE) approach is proposed, which extends the input of the speaker\nclassifier to include not only acoustic features but also their corresponding\nspeech content, via phone, character, and word embeddings. Compared to\nalternative methods that leverage similar information, such as multitask or\nadversarial training, CASE factorises automatic speech recognition (ASR) from\nspeaker recognition to focus on modelling speaker characteristics and\ncorrelations with the corresponding content units to derive more expressive\nrepresentations. CASE is evaluated for speaker re-clustering with a realistic\nspeaker diarisation setup using the AMI meeting transcription dataset, where\nthe content information is obtained by performing ASR based on an automatic\nsegmentation. Experimental results showed that CASE achieved a 17.8% relative\nspeaker error rate reduction over conventional methods.", "published": "2021-02-12 12:02:03", "link": "http://arxiv.org/abs/2102.06467v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
