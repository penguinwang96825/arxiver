{"title": "Integrating Linguistic Theory and Neural Language Models", "abstract": "Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n  This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.", "published": "2022-07-20 04:20:46", "link": "http://arxiv.org/abs/2207.09643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Doge Tickets: Uncovering Domain-general Language Models by Playing\n  Lottery Tickets", "abstract": "Over-parameterized models, typically pretrained language models (LMs), have\nshown an appealing expressive power due to their small learning bias. However,\nthe huge learning capacity of LMs can also lead to large learning variance. In\na pilot study, we find that, when faced with multiple domains, a critical\nportion of parameters behave unexpectedly in a domain-specific manner while\nothers behave in a domain-general one. Motivated by this phenomenon, we for the\nfirst time posit that domain-general parameters can underpin a domain-general\nLM that can be derived from the original LM. To uncover the domain-general LM,\nwe propose to identify domain-general parameters by playing lottery tickets\n(dubbed doge tickets). In order to intervene the lottery, we propose a\ndomain-general score, which depicts how domain-invariant a parameter is by\nassociating it with the variance. Comprehensive experiments are conducted on\nthe Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets\nobtains an improved out-of-domain generalization in comparison with a range of\ncompetitive baselines. Analysis results further hint the existence of\ndomain-general parameters and the performance consistency of doge tickets.", "published": "2022-07-20 03:37:37", "link": "http://arxiv.org/abs/2207.09638v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRIT: Faster and Better Image captioning Transformer Using Dual Visual\n  Features", "abstract": "Current state-of-the-art methods for image captioning employ region-based\nfeatures, as they provide object-level information that is essential to\ndescribe the content of images; they are usually extracted by an object\ndetector such as Faster R-CNN. However, they have several issues, such as lack\nof contextual information, the risk of inaccurate detection, and the high\ncomputational cost. The first two could be resolved by additionally using\ngrid-based features. However, how to extract and fuse these two types of\nfeatures is uncharted. This paper proposes a Transformer-only neural\narchitecture, dubbed GRIT (Grid- and Region-based Image captioning\nTransformer), that effectively utilizes the two visual features to generate\nbetter captions. GRIT replaces the CNN-based detector employed in previous\nmethods with a DETR-based one, making it computationally faster. Moreover, its\nmonolithic design consisting only of Transformers enables end-to-end training\nof the model. This innovative design and the integration of the dual visual\nfeatures bring about significant performance improvement. The experimental\nresults on several image captioning benchmarks show that GRIT outperforms\nprevious methods in inference accuracy and speed.", "published": "2022-07-20 05:49:01", "link": "http://arxiv.org/abs/2207.09666v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Data Driven Inverse Text Normalization using Data Augmentation", "abstract": "Inverse text normalization (ITN) is used to convert the spoken form output of\nan automatic speech recognition (ASR) system to a written form. Traditional\nhandcrafted ITN rules can be complex to transcribe and maintain. Meanwhile\nneural modeling approaches require quality large-scale spoken-written pair\nexamples in the same or similar domain as the ASR system (in-domain data), to\ntrain. Both these approaches require costly and complex annotations. In this\npaper, we present a data augmentation technique that effectively generates rich\nspoken-written numeric pairs from out-of-domain textual data with minimal human\nannotation. We empirically demonstrate that ITN model trained using our data\naugmentation technique consistently outperform ITN model trained using only\nin-domain data across all numeric surfaces like cardinal, currency, and\nfraction, by an overall accuracy of 14.44%.", "published": "2022-07-20 06:07:26", "link": "http://arxiv.org/abs/2207.09674v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "When Is TTS Augmentation Through a Pivot Language Useful?", "abstract": "Developing Automatic Speech Recognition (ASR) for low-resource languages is a\nchallenge due to the small amount of transcribed audio data. For many such\nlanguages, audio and text are available separately, but not audio with\ntranscriptions. Using text, speech can be synthetically produced via\ntext-to-speech (TTS) systems. However, many low-resource languages do not have\nquality TTS systems either. We propose an alternative: produce synthetic audio\nby running text from the target language through a trained TTS system for a\nhigher-resource pivot language. We investigate when and how this technique is\nmost effective in low-resource settings. In our experiments, using several\nthousand synthetic TTS text-speech pairs and duplicating authentic data to\nbalance yields optimal results. Our findings suggest that searching over a set\nof candidate pivot languages can lead to marginal improvements and that,\nsurprisingly, ASR performance can by harmed by increases in measured TTS\nquality. Application of these findings improves ASR by 64.5\\% and 45.0\\%\ncharacter error reduction rate (CERR) respectively for two low-resource\nlanguages: Guaran\\'i and Suba.", "published": "2022-07-20 13:33:41", "link": "http://arxiv.org/abs/2207.09889v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ReFactor GNNs: Revisiting Factorisation-based Models from a\n  Message-Passing Perspective", "abstract": "Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and generalise to unseen nodes in inductive settings. Our work bridges\nthe gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture\ndraws upon both modelling paradigms, which previously were largely thought of\nas disjoint. Concretely, using a message-passing formalism, we show how FMs can\nbe cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactor GNNs. Across\na multitude of well-established KGC benchmarks, our ReFactor GNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.", "published": "2022-07-20 15:39:30", "link": "http://arxiv.org/abs/2207.09980v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68T07, 68T50", "I.2.7; I.2.6"], "primary_category": "cs.LG"}
{"title": "An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant:\n  Insights from Sentiment Analysis, Language Interpretation, Source Tracking,\n  Type Classification, and Embedded URL Detection", "abstract": "This paper presents the findings of an exploratory study on the continuously\ngenerating Big Data on Twitter related to the sharing of information, news,\nviews, opinions, ideas, feedback, and experiences about the COVID-19 pandemic,\nwith a specific focus on the Omicron variant, which is the globally dominant\nvariant of SARS-CoV-2 at this time. A total of 12028 tweets about the Omicron\nvariant were studied, and the specific characteristics of tweets that were\nanalyzed include - sentiment, language, source, type, and embedded URLs. The\nfindings of this study are manifold. First, from sentiment analysis, it was\nobserved that 50.5% of tweets had a neutral emotion. The other emotions - bad,\ngood, terrible, and great were found in 15.6%, 14.0%, 12.5%, and 7.5% of the\ntweets, respectively. Second, the findings of language interpretation showed\nthat 65.9% of the tweets were posted in English. It was followed by Spanish,\nFrench, Italian, and other languages. Third, the findings from source tracking\nshowed that Twitter for Android was associated with 35.2% of tweets. It was\nfollowed by Twitter Web App, Twitter for iPhone, Twitter for iPad, and other\nsources. Fourth, studying the type of tweets revealed that retweets accounted\nfor 60.8% of the tweets, it was followed by original tweets and replies that\naccounted for 19.8% and 19.4% of the tweets, respectively. Fifth, in terms of\nembedded URL analysis, the most common domain embedded in the tweets was found\nto be twitter.com, which was followed by biorxiv.org, nature.com, and other\ndomains. Finally, to support similar research in this field, we have developed\na Twitter dataset that comprises more than 500,000 tweets about the SARS-CoV-2\nomicron variant since the first detected case of this variant on November 24,\n2021.", "published": "2022-07-20 18:47:20", "link": "http://arxiv.org/abs/2208.10252v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Introducing Auxiliary Text Query-modifier to Content-based Audio\n  Retrieval", "abstract": "The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.", "published": "2022-07-20 08:19:54", "link": "http://arxiv.org/abs/2207.09732v1", "categories": ["eess.AS", "cs.CL", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direct and Residual Subspace Decomposition of Spatial Room Impulse\n  Responses", "abstract": "Psychoacoustic experiments have shown that directional properties of the\ndirect sound, salient reflections, and the late reverberation of an acoustic\nroom response can have a distinct influence on the auditory perception of a\ngiven room. Spatial room impulse responses (SRIRs) capture those properties and\nthus are used for direction-dependent room acoustic analysis and virtual\nacoustic rendering. This work proposes a subspace method that decomposes SRIRs\ninto a direct part, which comprises the direct sound and the salient\nreflections, and a residual, to facilitate enhanced analysis and rendering\nmethods by providing individual access to these components. The proposed method\nis based on the generalized singular value decomposition and interprets the\nresidual as noise that is to be separated from the other components of the\nreverberation. Large generalized singular values are attributed to the direct\npart, which is then obtained as a low-rank approximation of the SRIR. By\nadvancing from the end of the SRIR toward the beginning while iteratively\nupdating the residual estimate, the method adapts to spatio-temporal variations\nof the residual. The method is evaluated using a spatio-spectral error measure\nand simulated SRIRs of different rooms, microphone arrays, and ratios of direct\nsound to residual energy. The proposed method creates lower errors than\nexisting approaches in all tested scenarios, including a scenario with two\nsimultaneous reflections. A case study with measured SRIRs shows the\napplicability of the method under real-world acoustic conditions. A reference\nimplementation is provided.", "published": "2022-07-20 08:20:37", "link": "http://arxiv.org/abs/2207.09733v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-grained Early Frequency Attention for Deep Speaker Recognition", "abstract": "Attention mechanisms have emerged as important tools that boost the\nperformance of deep models by allowing them to focus on key parts of learned\nembeddings. However, current attention mechanisms used in speaker recognition\ntasks fail to consider fine-grained information items such as frequency bins in\ninput spectral representations used by the deep networks. To address this\nissue, we propose the novel Fine-grained Early Frequency Attention (FEFA) for\nspeaker recognition in-the-wild. Once integrated into a deep neural network,\nour proposed mechanism works by obtaining queries from early layers of the\nnetwork and generating learnable weights to attend to information items as\nsmall as the frequency bins in the input spectral representations. To evaluate\nthe performance of FEFA, we use several well-known deep models as backbone\nnetworks and integrate our attention module in their pipelines. The overall\nperformance of these networks (with and without FEFA) are evaluated on the\nVoxCeleb1 dataset, where we observe considerable improvements when FEFA is\nused.", "published": "2022-07-20 16:09:27", "link": "http://arxiv.org/abs/2207.10006v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription", "abstract": "Automatic speech recognition (ASR) has progressed significantly in recent\nyears due to the emergence of large-scale datasets and the self-supervised\nlearning (SSL) paradigm. However, as its counterpart problem in the singing\ndomain, the development of automatic lyric transcription (ALT) suffers from\nlimited data and degraded intelligibility of sung lyrics. To fill in the\nperformance gap between ALT and ASR, we attempt to exploit the similarities\nbetween speech and singing. In this work, we propose a transfer-learning-based\nALT solution that takes advantage of these similarities by adapting wav2vec\n2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of\ntransfer learning by exploring the influence of different transfer starting\npoints. We further enhance the performance by extending the original CTC model\nto a hybrid CTC/attention model. Our method surpasses previous approaches by a\nlarge margin on various ALT benchmark datasets. Further experiments show that,\nwith even a tiny proportion of training data, our method still achieves\ncompetitive performance.", "published": "2022-07-20 08:45:18", "link": "http://arxiv.org/abs/2207.09747v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation", "abstract": "Generating sound effects that humans want is an important topic. However,\nthere are few studies in this area for sound generation. In this study, we\ninvestigate generating sound conditioned on a text prompt and propose a novel\ntext-to-sound generation framework that consists of a text encoder, a Vector\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\nframework first uses the decoder to transfer the text features extracted from\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\nfound that the decoder significantly influences the generation performance.\nThus, we focus on designing a good decoder in this study. We begin with the\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\nmethod in previous sound generation works. However, the AR decoder always\npredicts the mel-spectrogram tokens one by one in order, which introduces the\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\ndecoder, the sound generation time increases linearly with the sound duration.\nTo overcome the shortcomings introduced by AR decoders, we propose a\nnon-autoregressive decoder based on the discrete diffusion model, named\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\ntokens in one step and then refines the predicted tokens in the next step, so\nthe best-predicted results can be obtained after several steps. Our experiments\nshow that our proposed Diffsound not only produces better text-to-sound\ngeneration results when compared with the AR decoder but also has a faster\ngeneration speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed\nis five times faster than the AR decoder.", "published": "2022-07-20 15:41:47", "link": "http://arxiv.org/abs/2207.09983v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioScopeV2: Audio-Visual Attention Architectures for Calibrated\n  Open-Domain On-Screen Sound Separation", "abstract": "We introduce AudioScopeV2, a state-of-the-art universal audio-visual\non-screen sound separation system which is capable of learning to separate\nsounds and associate them with on-screen objects by looking at in-the-wild\nvideos. We identify several limitations of previous work on audio-visual\non-screen sound separation, including the coarse resolution of spatio-temporal\nattention, poor convergence of the audio separation model, limited variety in\ntraining and evaluation data, and failure to account for the trade off between\npreservation of on-screen sounds and suppression of off-screen sounds. We\nprovide solutions to all of these issues. Our proposed cross-modal and\nself-attention network architectures capture audio-visual dependencies at a\nfiner resolution over time, and we also propose efficient separable variants\nthat are capable of scaling to longer videos without sacrificing much\nperformance. We also find that pre-training the separation model only on audio\ngreatly improves results. For training and evaluation, we collected new human\nannotations of onscreen sounds from a large database of in-the-wild videos\n(YFCC100M). This new dataset is more diverse and challenging. Finally, we\npropose a calibration procedure that allows exact tuning of on-screen\nreconstruction versus off-screen suppression, which greatly simplifies\ncomparing performance between models with different operating points. Overall,\nour experimental results show marked improvements in on-screen separation\nperformance under much more general conditions than previous methods with\nminimal additional computational complexity.", "published": "2022-07-20 18:44:01", "link": "http://arxiv.org/abs/2207.10141v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial Aware Multi-Task Learning Based Speech Separation", "abstract": "During the Covid, online meetings have become an indispensable part of our\nlives. This trend is likely to continue due to their convenience and broad\nreach. However, background noise from other family members, roommates,\noffice-mates not only degrades the voice quality but also raises serious\nprivacy issues. In this paper, we develop a novel system, called Spatial Aware\nMulti-task learning-based Separation (SAMS), to extract audio signals from the\ntarget user during teleconferencing. Our solution consists of three novel\ncomponents: (i) generating fine-grained location embeddings from the user's\nvoice and inaudible tracking sound, which contains the user's position and rich\nmultipath information, (ii) developing a source separation neural network using\nmulti-task learning to jointly optimize source separation and location, and\n(iii) significantly speeding up inference to provide a real-time guarantee. Our\ntestbed experiments demonstrate the effectiveness of our approach", "published": "2022-07-20 23:41:12", "link": "http://arxiv.org/abs/2207.10229v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End and Self-Supervised Learning for ComParE 2022 Stuttering\n  Sub-Challenge", "abstract": "In this paper, we present end-to-end and speech embedding based systems\ntrained in a self-supervised fashion to participate in the ACM Multimedia 2022\nComParE Challenge, specifically the stuttering sub-challenge. In particular, we\nexploit the embeddings from the pre-trained Wav2Vec2.0 model for stuttering\ndetection (SD) on the KSoF dataset. After embedding extraction, we benchmark\nwith several methods for SD. Our proposed self-supervised based SD system\nachieves a UAR of 36.9% and 41.0% on validation and test sets respectively,\nwhich is 31.32% (validation set) and 1.49% (test set) higher than the best\n(DeepSpectrum) challenge baseline (CBL). Moreover, we show that concatenating\nlayer embeddings with Mel-frequency cepstral coefficients (MFCCs) features\nfurther improves the UAR of 33.81% and 5.45% on validation and test sets\nrespectively over the CBL. Finally, we demonstrate that the summing information\nacross all the layers of Wav2Vec2.0 surpasses the CBL by a relative margin of\n45.91% and 5.69% on validation and test sets respectively. Grand-challenge:\nComputational Paralinguistics ChallengE", "published": "2022-07-20 11:57:31", "link": "http://arxiv.org/abs/2207.10817v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Modal Contrastive Representation Learning for Audio-to-Image\n  Generation", "abstract": "Multiple modalities for certain information provide a variety of perspectives\non that information, which can improve the understanding of the information.\nThus, it may be crucial to generate data of different modality from the\nexisting data to enhance the understanding. In this paper, we investigate the\ncross-modal audio-to-image generation problem and propose Cross-Modal\nContrastive Representation Learning (CMCRL) to extract useful features from\naudios and use it in the generation phase. Experimental results show that CMCRL\nenhances quality of images generated than previous research.", "published": "2022-07-20 10:00:49", "link": "http://arxiv.org/abs/2207.12121v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
