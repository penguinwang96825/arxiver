{"title": "LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using\n  Adversarial Training", "abstract": "Paraphrases are texts that convey the same meaning while using different\nwords or sentence structures. It can be used as an automatic data augmentation\ntool for many Natural Language Processing tasks, especially when dealing with\nlow-resource languages, where data shortage is a significant problem. To\ngenerate a paraphrase in multilingual settings, previous studies have leveraged\nthe knowledge from the machine translation field, i.e., forming a paraphrase\nthrough zero-shot machine translation in the same language. Despite good\nperformance on human evaluation, those methods still require parallel\ntranslation datasets, thus making them inapplicable to languages that do not\nhave parallel corpora. To mitigate that problem, we proposed the first\nunsupervised multilingual paraphrasing model, LAMPAT ($\\textbf{L}$ow-rank\n$\\textbf{A}$daptation for $\\textbf{M}$ultilingual $\\textbf{P}$araphrasing using\n$\\textbf{A}$dversarial $\\textbf{T}$raining), by which monolingual dataset is\nsufficient enough to generate a human-like and diverse sentence. Throughout the\nexperiments, we found out that our method not only works well for English but\ncan generalize on unseen languages as well. Data and code are available at\nhttps://github.com/VinAIResearch/LAMPAT.", "published": "2024-01-09 04:19:16", "link": "http://arxiv.org/abs/2401.04348v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic emotion and sentiment modelling of patient-reported\n  experiences", "abstract": "This study introduces a novel methodology for modelling patient emotions from\nonline patient experience narratives. We employed metadata network topic\nmodelling to analyse patient-reported experiences from Care Opinion, revealing\nkey emotional themes linked to patient-caregiver interactions and clinical\noutcomes. We develop a probabilistic, context-specific emotion recommender\nsystem capable of predicting both multilabel emotions and binary sentiments\nusing a naive Bayes classifier using contextually meaningful topics as\npredictors. The superior performance of our predicted emotions under this model\ncompared to baseline models was assessed using the information retrieval\nmetrics nDCG and Q-measure, and our predicted sentiments achieved an F1 score\nof 0.921, significantly outperforming standard sentiment lexicons. This method\noffers a transparent, cost-effective way to understand patient feedback,\nenhancing traditional collection methods and informing individualised patient\ncare. Our findings are accessible via an R package and interactive dashboard,\nproviding valuable tools for healthcare researchers and practitioners.", "published": "2024-01-09 05:39:20", "link": "http://arxiv.org/abs/2401.04367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding", "abstract": "Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.", "published": "2024-01-09 07:46:26", "link": "http://arxiv.org/abs/2401.04398v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransportationGames: Benchmarking Transportation Knowledge of\n  (Multimodal) Large Language Models", "abstract": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave shown excellent general capabilities, even exhibiting adaptability in many\nprofessional domains such as law, economics, transportation, and medicine.\nCurrently, many domain-specific benchmarks have been proposed to verify the\nperformance of (M)LLMs in specific fields. Among various domains,\ntransportation plays a crucial role in modern society as it impacts the\neconomy, the environment, and the quality of life for billions of people.\nHowever, it is unclear how much traffic knowledge (M)LLMs possess and whether\nthey can reliably perform transportation-related tasks. To address this gap, we\npropose TransportationGames, a carefully designed and thorough evaluation\nbenchmark for assessing (M)LLMs in the transportation domain. By\ncomprehensively considering the applications in real-world scenarios and\nreferring to the first three levels in Bloom's Taxonomy, we test the\nperformance of various (M)LLMs in memorizing, understanding, and applying\ntransportation knowledge by the selected tasks. The experimental results show\nthat although some models perform well in some tasks, there is still much room\nfor improvement overall. We hope the release of TransportationGames can serve\nas a foundation for future research, thereby accelerating the implementation\nand application of (M)LLMs in the transportation domain.", "published": "2024-01-09 10:20:29", "link": "http://arxiv.org/abs/2401.04471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LUNA: A Framework for Language Understanding and Naturalness Assessment", "abstract": "The evaluation of Natural Language Generation (NLG) models has gained\nincreased attention, urging the development of metrics that evaluate various\naspects of generated text. LUNA addresses this challenge by introducing a\nunified interface for 20 NLG evaluation metrics. These metrics are categorized\nbased on their reference-dependence and the type of text representation they\nemploy, from string-based n-gram overlap to the utilization of static\nembeddings and pre-trained language models.\n  The straightforward design of LUNA allows for easy extension with novel\nmetrics, requiring just a few lines of code. LUNA offers a user-friendly tool\nfor evaluating generated texts.", "published": "2024-01-09 12:31:18", "link": "http://arxiv.org/abs/2401.04522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Assessment on Comprehending Mental Health through Large Language\n  Models", "abstract": "Mental health challenges pose considerable global burdens on individuals and\ncommunities. Recent data indicates that more than 20% of adults may encounter\nat least one mental disorder in their lifetime. On the one hand, the\nadvancements in large language models have facilitated diverse applications,\nyet a significant research gap persists in understanding and enhancing the\npotential of large language models within the domain of mental health. On the\nother hand, across various applications, an outstanding question involves the\ncapacity of large language models to comprehend expressions of human mental\nhealth conditions in natural language. This study presents an initial\nevaluation of large language models in addressing this gap. Due to this, we\ncompare the performance of Llama-2 and ChatGPT with classical Machine as well\nas Deep learning models. Our results on the DAIC-WOZ dataset show that\ntransformer-based models, like BERT or XLNet, outperform the large language\nmodels.", "published": "2024-01-09 14:50:04", "link": "http://arxiv.org/abs/2401.04592v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Detection for Transliterated Content", "abstract": "In the contemporary digital era, the Internet functions as an unparalleled\ncatalyst, dismantling geographical and linguistic barriers particularly evident\nin texting. This evolution facilitates global communication, transcending\nphysical distances and fostering dynamic cultural exchange. A notable trend is\nthe widespread use of transliteration, where the English alphabet is employed\nto convey messages in native languages, posing a unique challenge for language\ntechnology in accurately detecting the source language. This paper addresses\nthis challenge through a dataset of phone text messages in Hindi and Russian\ntransliterated into English utilizing BERT for language classification and\nGoogle Translate API for transliteration conversion. The research pioneers\ninnovative approaches to identify and convert transliterated text, navigating\nchallenges in the diverse linguistic landscape of digital communication.\nEmphasizing the pivotal role of comprehensive datasets for training Large\nLanguage Models LLMs like BERT, our model showcases exceptional proficiency in\naccurately identifying and classifying languages from transliterated text. With\na validation accuracy of 99% our models robust performance underscores its\nreliability. The comprehensive exploration of transliteration dynamics\nsupported by innovative approaches and cutting edge technologies like BERT,\npositions our research at the forefront of addressing unique challenges in the\nlinguistic landscape of digital communication. Beyond contributing to language\nidentification and transliteration capabilities this work holds promise for\napplications in content moderation, analytics and fostering a globally\nconnected community engaged in meaningful dialogue.", "published": "2024-01-09 15:40:54", "link": "http://arxiv.org/abs/2401.04619v1", "categories": ["cs.CL", "C.m; I.2"], "primary_category": "cs.CL"}
{"title": "DepressionEmo: A novel dataset for multilabel classification of\n  depression emotions", "abstract": "Emotions are integral to human social interactions, with diverse responses\nelicited by various situational contexts. Particularly, the prevalence of\nnegative emotional states has been correlated with negative outcomes for mental\nhealth, necessitating a comprehensive analysis of their occurrence and impact\non individuals. In this paper, we introduce a novel dataset named DepressionEmo\ndesigned to detect 8 emotions associated with depression by 6037 examples of\nlong Reddit user posts. This dataset was created through a majority vote over\ninputs by zero-shot classifications from pre-trained models and validating the\nquality by annotators and ChatGPT, exhibiting an acceptable level of interrater\nreliability between annotators. The correlation between emotions, their\ndistribution over time, and linguistic analysis are conducted on DepressionEmo.\nBesides, we provide several text classification methods classified into two\ngroups: machine learning methods such as SVM, XGBoost, and Light GBM; and deep\nlearning methods such as BERT, GAN-BERT, and BART. The pretrained BART model,\nbart-base allows us to obtain the highest F1- Macro of 0.76, showing its\noutperformance compared to other methods evaluated in our analysis. Across all\nemotions, the highest F1-Macro value is achieved by suicide intent, indicating\na certain value of our dataset in identifying emotions in individuals with\ndepression symptoms through text analysis. The curated dataset is publicly\navailable at: https://github.com/abuBakarSiddiqurRahman/DepressionEmo.", "published": "2024-01-09 16:25:31", "link": "http://arxiv.org/abs/2401.04655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering\n  with Multi-Granularity Answers", "abstract": "Factual questions typically can be answered correctly at different levels of\ngranularity. For example, both ``August 4, 1961'' and ``1961'' are correct\nanswers to the question ``When was Barack Obama born?''. Standard question\nanswering (QA) evaluation protocols, however, do not explicitly take this into\naccount and compare a predicted answer against answers of a single granularity\nlevel. In this work, we propose GRANOLA QA, a novel evaluation setting where a\npredicted answer is evaluated in terms of accuracy and informativeness against\na set of multi-granularity answers. We present a simple methodology for\nenriching existing datasets with multi-granularity answers, and create\nGRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We\nevaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,\ncalled Decoding with Response Aggregation (DRAG), that is geared towards\naligning the response granularity with the model's uncertainty. Our experiments\nshow that large language models with standard decoding tend to generate\nspecific answers, which are often incorrect. In contrast, when evaluated on\nmulti-granularity answers, DRAG yields a nearly 20 point increase in accuracy\non average, which further increases for rare entities. Overall, this reveals\nthat standard evaluation and decoding schemes may significantly underestimate\nthe knowledge encapsulated in LMs.", "published": "2024-01-09 17:44:36", "link": "http://arxiv.org/abs/2401.04695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Editing Harms General Abilities of Large Language Models:\n  Regularization to the Rescue", "abstract": "Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT is proposed to\nregularize the edit update weights by imposing constraints on their complexity\nbased on the RElative Change in weighT. Evaluation results show that RECT can\nsignificantly mitigate the side effects of editing while still maintaining over\n94% editing performance.", "published": "2024-01-09 18:03:15", "link": "http://arxiv.org/abs/2401.04700v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Text Diacritization In The Age Of Transfer Learning: Token\n  Classification Is All You Need", "abstract": "Automatic diacritization of Arabic text involves adding diacritical marks\n(diacritics) to the text. This task poses a significant challenge with\nnoteworthy implications for computational processing and comprehension. In this\npaper, we introduce PTCAD (Pre-FineTuned Token Classification for Arabic\nDiacritization, a novel two-phase approach for the Arabic Text Diacritization\ntask. PTCAD comprises a pre-finetuning phase and a finetuning phase, treating\nArabic Text Diacritization as a token classification task for pre-trained\nmodels. The effectiveness of PTCAD is demonstrated through evaluations on two\nbenchmark datasets derived from the Tashkeela dataset, where it achieves\nstate-of-the-art results, including a 20\\% reduction in Word Error Rate (WER)\ncompared to existing benchmarks and superior performance over GPT-4 in ATD\ntasks.", "published": "2024-01-09 23:32:54", "link": "http://arxiv.org/abs/2401.04848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Recognition from Colloquial Text", "abstract": "Extraction of concepts and entities of interest from non-formal texts such as\nsocial media posts and informal communication is an important capability for\ndecision support systems in many domains, including healthcare, customer\nrelationship management, and others. Despite the recent advances in training\nlarge language models for a variety of natural language processing tasks, the\ndeveloped models and techniques have mainly focused on formal texts and do not\nperform as well on colloquial data, which is characterized by a number of\ndistinct challenges. In our research, we focus on the healthcare domain and\ninvestigate the problem of symptom recognition from colloquial texts by\ndesigning and evaluating several training strategies for BERT-based model\nfine-tuning. These strategies are distinguished by the choice of the base\nmodel, the training corpora, and application of term perturbations in the\ntraining data. The best-performing models trained using these strategies\noutperform the state-of-the-art specialized symptom recognizer by a large\nmargin. Through a series of experiments, we have found specific patterns of\nmodel behavior associated with the training strategies we designed. We present\ndesign principles for training strategies for effective entity recognition in\ncolloquial texts based on our findings.", "published": "2024-01-09 23:52:32", "link": "http://arxiv.org/abs/2401.04853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging", "abstract": "Indoor imaging is a critical task for robotics and internet-of-things. WiFi\nas an omnipresent signal is a promising candidate for carrying out passive\nimaging and synchronizing the up-to-date information to all connected devices.\nThis is the first research work to consider WiFi indoor imaging as a\nmulti-modal image generation task that converts the measured WiFi power into a\nhigh-resolution indoor image. Our proposed WiFi-GEN network achieves a shape\nreconstruction accuracy that is 275% of that achieved by physical model-based\ninversion methods. Additionally, the Frechet Inception Distance score has been\nsignificantly reduced by 82%. To examine the effectiveness of models for this\ntask, the first large-scale dataset is released containing 80,000 pairs of WiFi\nsignal and imaging target. Our model absorbs challenges for the model-based\nmethods including the non-linearity, ill-posedness and non-certainty into\nmassive parameters of our generative AI network. The network is also designed\nto best fit measured WiFi signals and the desired imaging output. For\nreproducibility, we will release the data and code upon acceptance.", "published": "2024-01-09 02:20:30", "link": "http://arxiv.org/abs/2401.04317v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Know Your Needs Better: Towards Structured Understanding of Marketer\n  Demands with Analogical Reasoning Augmented LLMs", "abstract": "In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. In practical scenarios, the demands of non-expert marketers\nare often abstract and diverse. Considering the impressive natural language\nprocessing ability of large language models (LLMs), we try to leverage LLMs to\nsolve this issue. To stimulate the LLMs' reasoning ability, the\nchain-of-thought (CoT) prompting method is widely used, but existing methods\nstill have some limitations in our scenario: (1) Previous methods either use\nsimple \"Let's think step by step\" spells or provide fixed examples in\ndemonstrations without considering compatibility between prompts and concrete\nquestions, making LLMs ineffective when the marketers' demands are abstract and\ndiverse. (2) Previous methods are often implemented in closed-source models or\nexcessively large models, which is not suitable in industrial practical\nscenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning\nAugmented Large Language Models) consisting of two modules: Analogical\nReasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation. Part of our data and code can be found at\nhttps://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.", "published": "2024-01-09 02:25:23", "link": "http://arxiv.org/abs/2401.04319v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive\n  Learning", "abstract": "Knowledge-grounded dialogue (KGD) learns to generate an informative response\nbased on a given dialogue context and external knowledge (\\emph{e.g.},\nknowledge graphs; KGs). Recently, the emergence of large language models (LLMs)\nand pre-training techniques has brought great success to knowledge-grounded\ndialogue. However, when building KGD systems in real applications, there are\nvarious real-world noises that are inevitable to face. For example, the\ndialogue context might involve perturbations such as misspellings and\nabbreviations. In addition, KGs typically suffer from incompletion and also\nmight contain erroneous and outdated facts. Such real-world noises pose a\nchallenge to the robustness of KGD systems and hinder their applications in the\nreal world. In this paper, we propose an entity-based contrastive learning\nframework for improving the robustness of KGD. Specifically, we make use of the\nentity information in a KGD sample to create both its positive and negative\nsamples which involve semantic-irrelevant and semantic-relevant perturbations,\nrespectively. The contrastive learning framework ensures the KGD model is aware\nof these two types of perturbations, thus generating informative responses with\nthe potentially noisy inputs in real applications. Experimental results on\nthree benchmark datasets show that our method achieves new state-of-the-art\nperformance in terms of automatic evaluation scores, verifying its\neffectiveness and potentiality. Furthermore, we show that our method can\ngenerate better responses than comparison models in both the noisy and the\nfew-shot settings.", "published": "2024-01-09 05:16:52", "link": "http://arxiv.org/abs/2401.04361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Estimating Text Similarity based on Semantic Concept Embeddings", "abstract": "Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings\nenjoy great success in the semantic representation of words, sentences, and\nwhole documents as well as for semantic similarity estimation. However, they\nhave the shortcoming that they are directly extracted from a surface\nrepresentation, which does not adequately represent human thought processes and\nalso performs poorly for highly ambiguous words. Therefore, we propose Semantic\nConcept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,\nwhich addresses both shortcomings. The evaluation on a marketing target group\ndistribution task showed that the accuracy of predicted target groups can be\nincreased by combining traditional word embeddings with semantic CEs.", "published": "2024-01-09 08:29:46", "link": "http://arxiv.org/abs/2401.04422v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fighting Fire with Fire: Adversarial Prompting to Generate a\n  Misinformation Detection Dataset", "abstract": "The recent success in language generation capabilities of large language\nmodels (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns\nabout their possible misuse in inducing mass agitation and communal hatred via\ngenerating fake news and spreading misinformation. Traditional means of\ndeveloping a misinformation ground-truth dataset does not scale well because of\nthe extensive manual effort required to annotate the data. In this paper, we\npropose an LLM-based approach of creating silver-standard ground-truth datasets\nfor identifying misinformation. Specifically speaking, given a trusted news\narticle, our proposed approach involves prompting LLMs to automatically\ngenerate a summarised version of the original article. The prompts in our\nproposed approach act as a controlling mechanism to generate specific types of\nfactual incorrectness in the generated summaries, e.g., incorrect quantities,\nfalse attributions etc. To investigate the usefulness of this dataset, we\nconduct a set of experiments where we train a range of supervised models for\nthe task of misinformation detection.", "published": "2024-01-09 10:38:13", "link": "http://arxiv.org/abs/2401.04481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continuously Learning New Words in Automatic Speech Recognition", "abstract": "Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model.", "published": "2024-01-09 10:39:17", "link": "http://arxiv.org/abs/2401.04482v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TechGPT-2.0: A large language model project to solve the task of\n  knowledge graph construction", "abstract": "Large language models have exhibited robust performance across diverse\nnatural language processing tasks. This report introduces TechGPT-2.0, a\nproject designed to enhance the capabilities of large language models\nspecifically in knowledge graph construction tasks, including named entity\nrecognition (NER) and relationship triple extraction (RTE) tasks in NLP\napplications. Additionally, it serves as a LLM accessible for research within\nthe Chinese open-source model community. We offer two 7B large language model\nweights and a QLoRA weight specialized for processing lengthy texts.Notably,\nTechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all\nfunctionalities from TechGPT-1.0, it exhibits robust text processing\ncapabilities, particularly in the domains of medicine and law. Furthermore, we\nintroduce new capabilities to the model, enabling it to process texts in\nvarious domains such as geographical areas, transportation, organizations,\nliterary works, biology, natural sciences, astronomical objects, and\narchitecture. These enhancements also fortified the model's adeptness in\nhandling hallucinations, unanswerable queries, and lengthy texts. This report\nprovides a comprehensive and detailed introduction to the full fine-tuning\nprocess on Huawei's Ascend servers, encompassing experiences in Ascend server\ndebugging, instruction fine-tuning data processing, and model training. Our\ncode is available at https://github.com/neukg/TechGPT-2.0", "published": "2024-01-09 11:52:58", "link": "http://arxiv.org/abs/2401.04507v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with\n  Large Language Models", "abstract": "This article investigates a zero-shot approach to hypernymy prediction using\nlarge language models (LLMs). The study employs a method based on text\nprobability calculation, applying it to various generated prompts. The\nexperiments demonstrate a strong correlation between the effectiveness of\nlanguage model prompts and classic patterns, indicating that preliminary prompt\nselection can be carried out using smaller models before moving to larger ones.\nWe also explore prompts for predicting co-hyponyms and improving hypernymy\npredictions by augmenting prompts with additional information through\nautomatically identified co-hyponyms. An iterative approach is developed for\npredicting higher-level concepts, which further improves the quality on the\nBLESS dataset (MAP = 0.8).", "published": "2024-01-09 12:13:55", "link": "http://arxiv.org/abs/2401.04515v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Critique of Critique", "abstract": "Critique, as a natural language description for assessing the quality of\nmodel-generated content, has played a vital role in the training, evaluation,\nand refinement of LLMs. However, a systematic method to evaluate the quality of\ncritique is lacking. In this paper, we pioneer the critique of critique, termed\nMetaCritique, which builds specific quantification criteria. To achieve a\nreliable evaluation outcome, we propose Atomic Information Units (AIUs), which\ndescribe the critique in a more fine-grained manner. MetaCritique aggregates\neach AIU's judgment for the overall score. Moreover, MetaCritique delivers a\nnatural language rationale for the intricate reasoning within each judgment.\nLastly, we construct a meta-evaluation dataset covering 4 tasks across 16\npublic datasets involving human-written and LLM-generated critiques.\nExperiments demonstrate that MetaCritique can achieve near-human performance.\nOur study can facilitate future research in LLM critiques based on our\nfollowing observations and released resources: (1) superior critiques judged by\nMetaCritique can lead to better refinements, indicating that it can potentially\nenhance the alignment of existing LLMs; (2) the leaderboard of critique models\nreveals that open-source critique models commonly suffer from factuality\nissues; (3) relevant code and data are publicly available at\nhttps://github.com/GAIR-NLP/MetaCritique to support deeper exploration; (4) an\nAPI at PyPI with the usage documentation in Appendix C allows users to assess\nthe critique conveniently.", "published": "2024-01-09 12:20:41", "link": "http://arxiv.org/abs/2401.04518v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MERA: A Comprehensive LLM Evaluation in Russian", "abstract": "Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.", "published": "2024-01-09 12:55:21", "link": "http://arxiv.org/abs/2401.04531v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agent Alignment in Evolving Social Norms", "abstract": "Agents based on Large Language Models (LLMs) are increasingly permeating\nvarious domains of human production and life, highlighting the importance of\naligning them with human values. The current alignment of AI systems primarily\nfocuses on passively aligning LLMs through human intervention. However, agents\npossess characteristics like receiving environmental feedback and\nself-evolution, rendering the LLM alignment methods inadequate. In response, we\npropose an evolutionary framework for agent evolution and alignment, named\nEvolutionaryAgent, which transforms agent alignment into a process of evolution\nand selection under the principle of survival of the fittest. In an environment\nwhere social norms continuously evolve, agents better adapted to the current\nsocial norms will have a higher probability of survival and proliferation,\nwhile those inadequately aligned dwindle over time. Experimental results\nassessing the agents from multiple perspectives in aligning with social norms\ndemonstrate that EvolutionaryAgent can align progressively better with the\nevolving social norms while maintaining its proficiency in general tasks.\nEffectiveness tests conducted on various open and closed-source LLMs as the\nfoundation for agents also prove the applicability of our approach.", "published": "2024-01-09 15:44:44", "link": "http://arxiv.org/abs/2401.04620v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence\n  Lengths in Large Language Models", "abstract": "Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.", "published": "2024-01-09 16:27:28", "link": "http://arxiv.org/abs/2401.04658v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Translate-Distill: Learning Cross-Language Dense Retrieval by\n  Translation and Distillation", "abstract": "Prior work on English monolingual retrieval has shown that a cross-encoder\ntrained using a large number of relevance judgments for query-document pairs\ncan be used as a teacher to train more efficient, but similarly effective,\ndual-encoder student models. Applying a similar knowledge distillation approach\nto training an efficient dual-encoder model for Cross-Language Information\nRetrieval (CLIR), where queries and documents are in different languages, is\nchallenging due to the lack of a sufficiently large training collection when\nthe query and document languages differ. The state of the art for CLIR thus\nrelies on translating queries, documents, or both from the large English MS\nMARCO training set, an approach called Translate-Train. This paper proposes an\nalternative, Translate-Distill, in which knowledge distillation from either a\nmonolingual cross-encoder or a CLIR cross-encoder is used to train a\ndual-encoder CLIR student model. This richer design space enables the teacher\nmodel to perform inference in an optimized setting, while training the student\nmodel directly for CLIR. Trained models and artifacts are publicly available on\nHuggingface.", "published": "2024-01-09 20:40:49", "link": "http://arxiv.org/abs/2401.04810v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual\n  Zero-shot Transfer", "abstract": "Transformer-based pre-trained language models (PLMs) have achieved remarkable\nperformance in various natural language processing (NLP) tasks. However,\npre-training such models can take considerable resources that are almost only\navailable to high-resource languages. On the contrary, static word embeddings\nare easier to train in terms of computing resources and the amount of data\nrequired. In this paper, we introduce MoSECroT Model Stitching with Static Word\nEmbeddings for Crosslingual Zero-shot Transfer), a novel and challenging task\nthat is especially relevant to low-resource languages for which static word\nembeddings are available. To tackle the task, we present the first framework\nthat leverages relative representations to construct a common space for the\nembeddings of a source language PLM and the static word embeddings of a target\nlanguage. In this way, we can train the PLM on source-language training data\nand perform zero-shot transfer to the target language by simply swapping the\nembedding layer. However, through extensive experiments on two classification\ndatasets, we show that although our proposed framework is competitive with weak\nbaselines when addressing MoSECroT, it fails to achieve competitive results\ncompared with some strong baselines. In this paper, we attempt to explain this\nnegative result and provide several thoughts on possible improvement.", "published": "2024-01-09 21:09:07", "link": "http://arxiv.org/abs/2401.04821v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI Hallucinations: A Misnomer Worth Clarifying", "abstract": "As large language models continue to advance in Artificial Intelligence (AI),\ntext generation systems have been shown to suffer from a problematic phenomenon\ntermed often as \"hallucination.\" However, with AI's increasing presence across\nvarious domains including medicine, concerns have arisen regarding the use of\nthe term itself. In this study, we conducted a systematic review to identify\npapers defining \"AI hallucination\" across fourteen databases. We present and\nanalyze definitions obtained across all databases, categorize them based on\ntheir applications, and extract key points within each category. Our results\nhighlight a lack of consistency in how the term is used, but also help identify\nseveral alternative terms in the literature. We discuss implications of these\nand call for a more unified effort to bring consistency to an important\ncontemporary AI issue that can affect multiple domains significantly.", "published": "2024-01-09 01:49:41", "link": "http://arxiv.org/abs/2401.06796v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt\n  Learning with Data-Dependent Prior", "abstract": "Recent Vision-Language Pretrained (VLP) models have become the backbone for\nmany downstream tasks, but they are utilized as frozen model without learning.\nPrompt learning is a method to improve the pre-trained VLP model by adding a\nlearnable context vector to the inputs of the text encoder. In a few-shot\nlearning scenario of the downstream task, MLE training can lead the context\nvector to over-fit dominant image features in the training data. This\noverfitting can potentially harm the generalization ability, especially in the\npresence of a distribution shift between the training and test dataset. This\npaper presents a Bayesian-based framework of prompt learning, which could\nalleviate the overfitting issues on few-shot learning application and increase\nthe adaptability of prompts on unseen instances. Specifically, modeling\ndata-dependent prior enhances the adaptability of text features for both seen\nand unseen image features without the trade-off of performance between them.\nBased on the Bayesian framework, we utilize the Wasserstein Gradient Flow in\nthe estimation of our target posterior distribution, which enables our prompt\nto be flexible in capturing the complex modes of image features. We demonstrate\nthe effectiveness of our method on benchmark datasets for several experiments\nby showing statistically significant improvements on performance compared to\nexisting methods. The code is available at https://github.com/youngjae-cho/APP.", "published": "2024-01-09 10:15:59", "link": "http://arxiv.org/abs/2401.06799v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization", "abstract": "Differentially private stochastic gradient descent (DP-SGD) allows models to\nbe trained in a privacy-preserving manner, but has proven difficult to scale to\nthe era of foundation models. We introduce DP-ZO, a private fine-tuning\nframework for large language models by privatizing zeroth order optimization\nmethods. A key insight into the design of our method is that the direction of\nthe gradient in the zeroth-order optimization we use is random and the only\ninformation from training data is the step size, i.e., a scalar. Therefore, we\nonly need to privatize the scalar step size, which is memory-efficient. DP-ZO\nprovides a strong privacy-utility trade-off across different tasks, and model\nsizes that are comparable to DP-SGD in $(\\varepsilon,\\delta)$-DP. Notably,\nDP-ZO possesses significant advantages over DP-SGD in memory efficiency, and\nobtains higher utility in $\\varepsilon$-DP when using the Laplace mechanism.", "published": "2024-01-09 03:53:59", "link": "http://arxiv.org/abs/2401.04343v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "TwinBooster: Synergising Large Language Models with Barlow Twins and\n  Gradient Boosting for Enhanced Molecular Property Prediction", "abstract": "The success of drug discovery and development relies on the precise\nprediction of molecular activities and properties. While in silico molecular\nproperty prediction has shown remarkable potential, its use has been limited so\nfar to assays for which large amounts of data are available. In this study, we\nuse a fine-tuned large language model to integrate biological assays based on\ntheir textual information, coupled with Barlow Twins, a Siamese neural network\nusing a novel self-supervised learning approach. This architecture uses both\nassay information and molecular fingerprints to extract the true molecular\ninformation. TwinBooster enables the prediction of properties of unseen\nbioassays and molecules by providing state-of-the-art zero-shot learning tasks.\nRemarkably, our artificial intelligence pipeline shows excellent performance on\nthe FS-Mol benchmark. This breakthrough demonstrates the application of deep\nlearning to critical property prediction tasks where data is typically scarce.\nBy accelerating the early identification of active molecules in drug discovery\nand development, this method has the potential to help streamline the\nidentification of novel therapeutics.", "published": "2024-01-09 10:36:20", "link": "http://arxiv.org/abs/2401.04478v2", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Rewriting the Code: A Simple Method for Large Language Model Augmented\n  Code Search", "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which\ngenerates exemplar code snippets to augment queries, has emerged as a promising\nstrategy to address the principal challenge of modality misalignment between\ncode snippets and natural language queries, particularly with the demonstrated\ncode generation capabilities of Large Language Models (LLMs). Nevertheless, our\npreliminary investigations indicate that the improvements conferred by such an\nLLM-augmented framework are somewhat constrained. This limitation could\npotentially be ascribed to the fact that the generated codes, albeit\nfunctionally accurate, frequently display a pronounced stylistic deviation from\nthe ground truth code in the codebase. In this paper, we extend the\nfoundational GAR framework and propose a simple yet effective method that\nadditionally Rewrites the Code (ReCo) within the codebase for style\nnormalization. Experimental results demonstrate that ReCo significantly boosts\nretrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),\nand fine-tuned dense (up to 23.6%) retrieval settings in diverse search\nscenarios. To further elucidate the advantages of ReCo and stimulate research\nin code style normalization, we introduce Code Style Similarity, the first\nmetric tailored to quantify stylistic similarities in code. Notably, our\nempirical findings reveal the inadequacy of existing metrics in capturing\nstylistic nuances. The source code and data are available at\n\\url{https://github.com/Alex-HaochenLi/ReCo}.", "published": "2024-01-09 12:12:50", "link": "http://arxiv.org/abs/2401.04514v2", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Evaluating Language Model Agency through Negotiations", "abstract": "We introduce an approach to evaluate language model (LM) agency using\nnegotiation games. This approach better reflects real-world use cases and\naddresses some of the shortcomings of alternative LM benchmarks. Negotiation\ngames enable us to study multi-turn, and cross-model interactions, modulate\ncomplexity, and side-step accidental evaluation data leakage. We use our\napproach to test six widely used and publicly accessible LMs, evaluating\nperformance and alignment in both self-play and cross-play settings. Noteworthy\nfindings include: (i) only closed-source models tested here were able to\ncomplete these tasks; (ii) cooperative bargaining games proved to be most\nchallenging to the models; and (iii) even the most powerful models sometimes\n\"lose\" to weaker opponents", "published": "2024-01-09 13:19:37", "link": "http://arxiv.org/abs/2401.04536v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DebugBench: Evaluating Debugging Capability of Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand four open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models exhibit inferior debugging performance compared to humans,\nopen-source models relatively lower pass rate scores; (2) the complexity of\ndebugging notably fluctuates depending on the bug category; (3) incorporating\nruntime feedback has a clear impact on debugging performance which is not\nalways helpful. As an extension, we also compare LLM debugging and code\ngeneration, revealing a strong correlation between them for closed-source\nmodels. These findings will benefit the development of LLMs in debugging.", "published": "2024-01-09 15:46:38", "link": "http://arxiv.org/abs/2401.04621v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Applying Large Language Models API to Issue Classification Problem", "abstract": "Effective prioritization of issue reports is crucial in software engineering\nto optimize resource allocation and address critical problems promptly.\nHowever, the manual classification of issue reports for prioritization is\nlaborious and lacks scalability. Alternatively, many open source software (OSS)\nprojects employ automated processes for this task, albeit relying on\nsubstantial datasets for adequate training. This research seeks to devise an\nautomated approach that ensures reliability in issue prioritization, even when\ntrained on smaller datasets. Our proposed methodology harnesses the power of\nGenerative Pre-trained Transformers (GPT), recognizing their potential to\nefficiently handle this task. By leveraging the capabilities of such models, we\naim to develop a robust system for prioritizing issue reports accurately,\nmitigating the necessity for extensive training data while maintaining\nreliability. In our research, we have developed a reliable GPT-based approach\nto accurately label and prioritize issue reports with a reduced training\ndataset. By reducing reliance on massive data requirements and focusing on\nfew-shot fine-tuning, our methodology offers a more accessible and efficient\nsolution for issue prioritization in software engineering. Our model predicted\nissue types in individual projects up to 93.2% in precision, 95% in recall, and\n89.3% in F1-score.", "published": "2024-01-09 16:05:47", "link": "http://arxiv.org/abs/2401.04637v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation", "abstract": "We investigate parameter-efficient fine-tuning (PEFT) methods that can\nprovide good accuracy under limited computational and memory budgets in the\ncontext of large language models (LLMs). We present a new PEFT method called\nRobust Adaptation (RoSA) inspired by robust principal component analysis that\njointly trains $\\textit{low-rank}$ and $\\textit{highly-sparse}$ components on\ntop of a set of fixed pretrained weights to efficiently approximate the\nperformance of a full-fine-tuning (FFT) solution. Across a series of\nchallenging generative tasks such as grade-school math and SQL query\ngeneration, which require fine-tuning for good performance, we show that RoSA\noutperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at\nthe same parameter budget, and can even recover the performance of FFT on some\ntasks. We provide system support for RoSA to complement the training algorithm,\nspecifically in the form of sparse GPU kernels which enable memory- and\ncomputationally-efficient training, and show that it is also compatible with\nlow-precision base weights, resulting in the first joint representation\ncombining quantization, low-rank and sparse approximations. Our code is\navailable at https://github.com/IST-DASLab/RoSA.", "published": "2024-01-09 17:09:01", "link": "http://arxiv.org/abs/2401.04679v7", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with\n  Large Language Models", "abstract": "In this paper, we introduce Uni3D-LLM, a unified framework that leverages a\nLarge Language Model (LLM) to integrate tasks of 3D perception, generation, and\nediting within point cloud scenes. This framework empowers users to\neffortlessly generate and modify objects at specified locations within a scene,\nguided by the versatility of natural language descriptions. Uni3D-LLM harnesses\nthe expressive power of natural language to allow for precise command over the\ngeneration and editing of 3D objects, thereby significantly enhancing\noperational flexibility and controllability. By mapping point cloud into the\nunified representation space, Uni3D-LLM achieves cross-application\nfunctionality, enabling the seamless execution of a wide array of tasks,\nranging from the accurate instantiation of 3D objects to the diverse\nrequirements of interactive design. Through a comprehensive suite of rigorous\nexperiments, the efficacy of Uni3D-LLM in the comprehension, generation, and\nediting of point cloud has been validated. Additionally, we have assessed the\nimpact of integrating a point cloud perception module on the generation and\nediting processes, confirming the substantial potential of our approach for\npractical applications.", "published": "2024-01-09 06:20:23", "link": "http://arxiv.org/abs/2402.03327v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RaD-Net: A Repairing and Denoising Network for Speech Signal Improvement", "abstract": "This paper introduces our repairing and denoising network (RaD-Net) for the\nICASSP 2024 Speech Signal Improvement (SSI) Challenge. We extend our previous\nframework based on a two-stage network and propose an upgraded model.\nSpecifically, we replace the repairing network with COM-Net from TEA-PSE. In\naddition, multi-resolution discriminators and multi-band discriminators are\nadopted in the training stage. Finally, we use a three-step training strategy\nto optimize our model. We submit two models with different sets of parameters\nto meet the RTF requirement of the two tracks. According to the official\nresults, the proposed systems rank 2nd in track 1 and 3rd in track 2.", "published": "2024-01-09 07:21:28", "link": "http://arxiv.org/abs/2401.04389v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Class-Incremental Learning for Multi-Label Audio Classification", "abstract": "In this paper, we propose a method for class-incremental learning of\npotentially overlapping sounds for solving a sequence of multi-label audio\nclassification tasks. We design an incremental learner that learns new classes\nindependently of the old classes. To preserve knowledge about the old classes,\nwe propose a cosine similarity-based distillation loss that minimizes\ndiscrepancy in the feature representations of subsequent learners, and use it\nalong with a Kullback-Leibler divergence-based distillation loss that minimizes\ndiscrepancy in their respective outputs. Experiments are performed on a dataset\nwith 50 sound classes, with an initial classification task containing 30 base\nclasses and 4 incremental phases of 5 classes each. After each phase, the\nsystem is tested for multi-label classification with the entire set of classes\nlearned so far. The proposed method obtains an average F1-score of 40.9% over\nthe five phases, ranging from 45.2% in phase 0 on 30 classes, to 36.3% in phase\n4 on 50 classes. Average performance degradation over incremental phases is\nonly 0.7 percentage points from the initial F1-score of 45.2%.", "published": "2024-01-09 09:25:13", "link": "http://arxiv.org/abs/2401.04447v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SonicVisionLM: Playing Sound with Vision Language Models", "abstract": "There has been a growing interest in the task of generating sound for silent\nvideos, primarily because of its practicality in streamlining video\npost-production. However, existing methods for video-sound generation attempt\nto directly create sound from visual representations, which can be challenging\ndue to the difficulty of aligning visual representations with audio\nrepresentations. In this paper, we present SonicVisionLM, a novel framework\naimed at generating a wide range of sound effects by leveraging vision-language\nmodels(VLMs). Instead of generating audio directly from video, we use the\ncapabilities of powerful VLMs. When provided with a silent video, our approach\nfirst identifies events within the video using a VLM to suggest possible sounds\nthat match the video content. This shift in approach transforms the challenging\ntask of aligning image and audio into more well-studied sub-problems of\naligning image-to-text and text-to-audio through the popular diffusion models.\nTo improve the quality of audio recommendations with LLMs, we have collected an\nextensive dataset that maps text descriptions to specific sound effects and\ndeveloped a time-controlled audio adapter. Our approach surpasses current\nstate-of-the-art methods for converting video to audio, enhancing\nsynchronization with the visuals, and improving alignment between audio and\nvideo components. Project page:\nhttps://yusiissy.github.io/SonicVisionLM.github.io/", "published": "2024-01-09 07:30:10", "link": "http://arxiv.org/abs/2401.04394v3", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement", "abstract": "The problem of audio-to-audio (A2A) style transfer involves replacing the\nstyle features of the source audio with those from the target audio while\npreserving the content related attributes of the source audio. In this paper,\nwe propose an efficient approach, termed as Zero-shot Emotion Style Transfer\n(ZEST), that allows the transfer of emotional content present in the given\nsource audio with the one embedded in the target audio while retaining the\nspeaker and speech content from the source. The proposed system builds upon\ndecomposing speech into semantic tokens, speaker representations and emotion\nembeddings. Using these factors, we propose a framework to reconstruct the\npitch contour of the given speech signal and train a decoder that reconstructs\nthe speech signal. The model is trained using a self-supervision based\nreconstruction loss. During conversion, the emotion embedding is alone derived\nfrom the target audio, while rest of the factors are derived from the source\naudio. In our experiments, we show that, even without using parallel training\ndata or labels from the source or target audio, we illustrate zero shot emotion\ntransfer capabilities of the proposed ZEST model using objective and subjective\nquality evaluations.", "published": "2024-01-09 12:10:04", "link": "http://arxiv.org/abs/2401.04511v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HyperGANStrument: Instrument Sound Synthesis and Editing with\n  Pitch-Invariant Hypernetworks", "abstract": "GANStrument, exploiting GANs with a pitch-invariant feature extractor and\ninstance conditioning technique, has shown remarkable capabilities in\nsynthesizing realistic instrument sounds. To further improve the reconstruction\nability and pitch accuracy to enhance the editability of user-provided sound,\nwe propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to\nmodulate the weights of a pre-trained GANStrument generator, given a one-shot\nsound as input. The hypernetwork modulation provides feedback for the generator\nin the reconstruction of the input sound. In addition, we take advantage of an\nadversarial fine-tuning scheme for the hypernetwork to improve the\nreconstruction fidelity and generation diversity of the generator. Experimental\nresults show that the proposed model not only enhances the generation\ncapability of GANStrument but also significantly improves the editability of\nsynthesized sounds. Audio examples are available at the online demo page.", "published": "2024-01-09 13:54:32", "link": "http://arxiv.org/abs/2401.04558v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Masked Audio Generation using a Single Non-Autoregressive Transformer", "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.", "published": "2024-01-09 14:29:39", "link": "http://arxiv.org/abs/2401.04577v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Genre Classification: A Comparative Analysis of CNN and XGBoost\n  Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms", "abstract": "In recent years, various well-designed algorithms have empowered music\nplatforms to provide content based on one's preferences. Music genres are\ndefined through various aspects, including acoustic features and cultural\nconsiderations. Music genre classification works well with content-based\nfiltering, which recommends content based on music similarity to users. Given a\nconsiderable dataset, one premise is automatic annotation using machine\nlearning or deep learning methods that can effectively classify audio files.\nThe effectiveness of systems largely depends on feature and model selection, as\ndifferent architectures and features can facilitate each other and yield\ndifferent results. In this study, we conduct a comparative study investigating\nthe performances of three models: a proposed convolutional neural network\n(CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient\nBoosting (XGBoost) approach on different features: 30-second Mel spectrogram\nand 3-second Mel-frequency cepstral coefficients (MFCCs). The results show that\nthe MFCC XGBoost model outperformed the others. Furthermore, applying data\nsegmentation in the data preprocessing phase can significantly enhance the\nperformance of the CNNs.", "published": "2024-01-09 01:50:31", "link": "http://arxiv.org/abs/2401.04737v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven\n  Holistic 3D Expression and Gesture Generation", "abstract": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D\nExpression and Gesture generation with arbitrary length. While previous works\nfocused on co-speech gesture or expression generation individually, the joint\ngeneration of synchronized expressions and gestures remains barely explored. To\naddress this, our diffusion-based co-speech motion generation transformer\nenables uni-directional information flow from expression to gesture,\nfacilitating improved matching of joint expression-gesture distributions.\nFurthermore, we introduce an outpainting-based sampling strategy for arbitrary\nlong sequence generation in diffusion models, offering flexibility and\ncomputational efficiency. Our method provides a practical solution that\nproduces high-quality synchronized expression and gesture generation driven by\nspeech. Evaluated on two public datasets, our approach achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nAdditionally, a user study confirms the superiority of DiffSHEG over prior\napproaches. By enabling the real-time generation of expressive and synchronized\nmotions, DiffSHEG showcases its potential for various applications in the\ndevelopment of digital humans and embodied agents.", "published": "2024-01-09 11:38:18", "link": "http://arxiv.org/abs/2401.04747v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
