{"title": "Deep learning interpretability for rough volatility", "abstract": "Deep learning methods have become a widespread toolbox for pricing and\ncalibration of financial models. While they often provide new directions and\nresearch results, their `black box' nature also results in a lack of\ninterpretability. We provide a detailed interpretability analysis of these\nmethods in the context of rough volatility - a new class of volatility models\nfor Equity and FX markets. Our work sheds light on the neural network learned\ninverse map between the rough volatility model parameters, seen as mathematical\nmodel inputs and network outputs, and the resulting implied volatility across\nstrikes and maturities, seen as mathematical model outputs and network inputs.\nThis contributes to building a solid framework for a safer use of neural\nnetworks in this context and in quantitative finance more generally.", "published": "2024-11-28 18:42:44", "link": "http://arxiv.org/abs/2411.19317v1", "categories": ["q-fin.CP", "68T07, 91G20, 91G60"], "primary_category": "q-fin.CP"}
{"title": "On the relative performance of some parametric and nonparametric estimators of option prices", "abstract": "We examine the empirical performance of some parametric and nonparametric\nestimators of prices of options with a fixed time to maturity, focusing on\nvariance-gamma and Heston models on one side, and on expansions in Hermite\nfunctions on the other side. The latter class of estimators can be seen as\nperturbations of the classical Black-Scholes model. The comparison between\nparametric and Hermite-based models having the same \"degrees of freedom\" is\nemphasized. The main criterion is the out-of-sample relative pricing error on a\ndataset of historical option prices on the S&P500 index. Prior to the main\nempirical study, the approximation of variance-gamma and Heston densities by\nseries of Hermite functions is studied, providing explicit expressions for the\ncoefficients of the expansion in the former case, and integral expressions\ninvolving the explicit characteristic function in the latter case. Moreover,\nthese approximations are investigated numerically on a few test cases,\nindicating that expansions in Hermite functions with few terms achieve\ncompetitive accuracy in the estimation of Heston densities and the pricing of\n(European) options, but they perform less effectively with variance-gamma\ndensities. On the other hand, the main large-scale empirical study show that\nparsimonious Hermite estimators can even outperform the Heston model in terms\nof pricing errors. These results underscore the trade-offs inherent in model\nselection and calibration, and their empirical fit in practical applications.", "published": "2024-11-28 16:07:43", "link": "http://arxiv.org/abs/2412.00135v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "GRU-PFG: Extract Inter-Stock Correlation from Stock Factors with Graph Neural Network", "abstract": "The complexity of stocks and industries presents challenges for stock\nprediction. Currently, stock prediction models can be divided into two\ncategories. One category, represented by GRU and ALSTM, relies solely on stock\nfactors for prediction, with limited effectiveness. The other category,\nrepresented by HIST and TRA, incorporates not only stock factors but also\nindustry information, industry financial reports, public sentiment, and other\ninputs for prediction. The second category of models can capture correlations\nbetween stocks by introducing additional information, but the extra data is\ndifficult to standardize and generalize. Considering the current state and\nlimitations of these two types of models, this paper proposes the GRU-PFG\n(Project Factors into Graph) model. This model only takes stock factors as\ninput and extracts inter-stock correlations using graph neural networks. It\nachieves prediction results that not only outperform the others models relies\nsolely on stock factors, but also achieve comparable performance to the second\ncategory models. The experimental results show that on the CSI300 dataset, the\nIC of GRU-PFG is 0.134, outperforming HIST's 0.131 and significantly surpassing\nGRU and Transformer, achieving results better than the second category models.\nMoreover as a model that relies solely on stock factors, it has greater\npotential for generalization.", "published": "2024-11-28 08:50:55", "link": "http://arxiv.org/abs/2411.18997v1", "categories": ["q-fin.CP", "cs.AI"], "primary_category": "q-fin.CP"}
{"title": "Measuring Risk of Bias in Biomedical Reports: The RoBBR Benchmark", "abstract": "Systems that answer questions by reviewing the scientific literature are\nbecoming increasingly feasible. To draw reliable conclusions, these systems\nshould take into account the quality of available evidence, placing more weight\non studies that use a valid methodology. We present a benchmark for measuring\nthe methodological strength of biomedical papers, drawing on the risk-of-bias\nframework used for systematic reviews. The four benchmark tasks, drawn from\nmore than 500 papers, cover the analysis of research study methodology,\nfollowed by evaluation of risk of bias in these studies. The benchmark contains\n2000 expert-generated bias annotations, and a human-validated pipeline for\nfine-grained alignment with research paper content. We evaluate a range of\nlarge language models on the benchmark, and find that these models fall\nsignificantly short of expert-level performance. By providing a standardized\ntool for measuring judgments of study quality, the benchmark can help to guide\nsystems that perform large-scale aggregation of scientific data. The dataset is\navailable at https://github.com/RoBBR-Benchmark/RoBBR.", "published": "2024-11-28 00:21:31", "link": "http://arxiv.org/abs/2411.18831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sneaking Syntax into Transformer Language Models with Tree\n  Regularization", "abstract": "While compositional accounts of human language understanding are based on a\nhierarchical tree-like process, neural models like transformers lack a direct\ninductive bias for such tree structures. Introducing syntactic inductive biases\ncould unlock more robust and data-efficient learning in transformer language\nmodels (LMs), but existing methods for incorporating such structure greatly\nrestrict models, either limiting their expressivity or increasing inference\ncomplexity. This work instead aims to softly inject syntactic inductive biases\ninto given transformer circuits, through a structured regularizer. We introduce\nTreeReg, an auxiliary loss function that converts bracketing decisions from\nsilver parses into a set of differentiable orthogonality constraints on vector\nhidden states. TreeReg integrates seamlessly with the standard LM objective,\nrequiring no architectural changes. LMs pre-trained with TreeReg on natural\nlanguage corpora such as WikiText-103 achieve up to 10% lower perplexities on\nout-of-distribution data and up to 9.5 point improvements in syntactic\ngeneralization, requiring less than half the training data to outperform\nstandard LMs. TreeReg still provides gains for pre-trained LLMs: Continued\npre-training of Sheared Llama with TreeReg results in improved syntactic\ngeneralization, and fine-tuning on MultiNLI with TreeReg mitigates degradation\nof performance on adversarial NLI benchmarks by 41.2 points. We release all\ncode to guide future research.", "published": "2024-11-28 03:27:48", "link": "http://arxiv.org/abs/2411.18885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Example Selection in Few-Shot Prompting on Automated Essay\n  Scoring Using GPT Models", "abstract": "This study investigates the impact of example selection on the performance of\nau-tomated essay scoring (AES) using few-shot prompting with GPT models. We\nevaluate the effects of the choice and order of examples in few-shot prompting\non several versions of GPT-3.5 and GPT-4 models. Our experiments involve 119\nprompts with different examples, and we calculate the quadratic weighted kappa\n(QWK) to measure the agreement between GPT and human rater scores. Regres-sion\nanalysis is used to quantitatively assess biases introduced by example\nselec-tion. The results show that the impact of example selection on QWK varies\nacross models, with GPT-3.5 being more influenced by examples than GPT-4. We\nalso find evidence of majority label bias, which is a tendency to favor the\nmajority la-bel among the examples, and recency bias, which is a tendency to\nfavor the label of the most recent example, in GPT-generated essay scores and\nQWK, with these biases being more pronounced in GPT-3.5. Notably, careful\nexample selection enables GPT-3.5 models to outperform some GPT-4 models.\nHowever, among the GPT models, the June 2023 version of GPT-4, which is not the\nlatest model, exhibits the highest stability and performance. Our findings\nprovide insights into the importance of example selection in few-shot prompting\nfor AES, especially in GPT-3.5 models, and highlight the need for individual\nperformance evaluations of each model, even for minor versions.", "published": "2024-11-28 05:24:51", "link": "http://arxiv.org/abs/2411.18924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rephrasing Electronic Health Records for Pretraining Clinical Language\n  Models", "abstract": "Clinical language models are important for many applications in healthcare,\nbut their development depends on access to extensive clinical text for\npretraining. However, obtaining clinical notes from electronic health records\n(EHRs) at scale is challenging due to patient privacy concerns. In this study,\nwe rephrase existing clinical notes using LLMs to generate synthetic\npretraining corpora, drawing inspiration from previous work on rephrasing web\ndata. We examine four popular small-sized LLMs (<10B) to create synthetic\nclinical text to pretrain both decoder-based and encoder-based language models.\nThe method yields better results in language modeling and downstream tasks than\nprevious synthesis approaches without referencing real clinical text. We find\nthat augmenting original clinical notes with synthetic corpora from different\nLLMs improves performances even at a small token budget, showing the potential\nof this method to support pretraining at the institutional level or be scaled\nto synthesize large-scale clinical corpora.", "published": "2024-11-28 06:12:28", "link": "http://arxiv.org/abs/2411.18940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Slot Filling in the Age of LLMs for Dialogue Systems", "abstract": "Zero-shot slot filling is a well-established subtask of Natural Language\nUnderstanding (NLU). However, most existing methods primarily focus on\nsingle-turn text data, overlooking the unique complexities of conversational\ndialogue. Conversational data is highly dynamic, often involving abrupt topic\nshifts, interruptions, and implicit references that make it difficult to\ndirectly apply zero-shot slot filling techniques, even with the remarkable\ncapabilities of large language models (LLMs). This paper addresses these\nchallenges by proposing strategies for automatic data annotation with slot\ninduction and black-box knowledge distillation (KD) from a teacher LLM to a\nsmaller model, outperforming vanilla LLMs on internal datasets by 26% absolute\nincrease in F1 score. Additionally, we introduce an efficient system\narchitecture for call center product settings that surpasses off-the-shelf\nextractive models by 34% relative F1 score, enabling near real-time inference\non dialogue streams with higher accuracy, while preserving low latency.", "published": "2024-11-28 08:02:25", "link": "http://arxiv.org/abs/2411.18980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Talking to oneself in CMC: a study of self replies in Wikipedia talk\n  pages", "abstract": "This study proposes a qualitative analysis of self replies in Wikipedia talk\npages, more precisely when the first two messages of a discussion are written\nby the same user. This specific pattern occurs in more than 10% of threads with\ntwo messages or more and can be explained by a number of reasons. After a first\nexamination of the lexical specificities of second messages, we propose a seven\ncategories typology and use it to annotate two reference samples (English and\nFrench) of 100 threads each. Finally, we analyse and compare the performance of\nhuman annotators (who reach a reasonable global efficiency) and\ninstruction-tuned LLMs (which encounter important difficulties with several\ncategories).", "published": "2024-11-28 09:14:58", "link": "http://arxiv.org/abs/2411.19007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pralekha: An Indic Document Alignment Evaluation Benchmark", "abstract": "Mining parallel document pairs poses a significant challenge because existing\nsentence embedding models often have limited context windows, preventing them\nfrom effectively capturing document-level information. Another overlooked issue\nis the lack of concrete evaluation benchmarks comprising high-quality parallel\ndocument pairs for assessing document-level mining approaches, particularly for\nIndic languages. In this study, we introduce Pralekha, a large-scale benchmark\nfor document-level alignment evaluation. Pralekha includes over 2 million\ndocuments, with a 1:2 ratio of unaligned to aligned pairs, covering 11 Indic\nlanguages and English. Using Pralekha, we evaluate various document-level\nmining approaches across three dimensions: the embedding models, the\ngranularity levels, and the alignment algorithm. To address the challenge of\naligning documents using sentence and chunk-level alignments, we propose a\nnovel scoring method, Document Alignment Coefficient (DAC). DAC demonstrates\nsubstantial improvements over baseline pooling approaches, particularly in\nnoisy scenarios, achieving average gains of 20-30% in precision and 15-20% in\nF1 score. These results highlight DAC's effectiveness in parallel document\nmining for Indic languages.", "published": "2024-11-28 12:17:24", "link": "http://arxiv.org/abs/2411.19096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Logit Lens: Contextual Embeddings for Robust Hallucination\n  Detection & Grounding in VLMs", "abstract": "The rapid development of Large Multimodal Models (LMMs) has significantly\nadvanced multimodal understanding by harnessing the language abilities of Large\nLanguage Models (LLMs) and integrating modality-specific encoders. However,\nLMMs are plagued by hallucinations that limit their reliability and adoption.\nWhile traditional methods to detect and mitigate these hallucinations often\ninvolve costly training or rely heavily on external models, recent approaches\nutilizing internal model features present a promising alternative. In this\npaper, we critically assess the limitations of the state-of-the-art\ntraining-free technique, the logit lens, in handling generalized visual\nhallucinations. We introduce ContextualLens, a refined method that leverages\ncontextual token embeddings from middle layers of LMMs. This approach\nsignificantly improves hallucination detection and grounding across diverse\ncategories, including actions and OCR, while also excelling in tasks requiring\ncontextual understanding, such as spatial relations and attribute comparison.\nOur novel grounding technique yields highly precise bounding boxes,\nfacilitating a transition from Zero-Shot Object Segmentation to Grounded Visual\nQuestion Answering. Our contributions pave the way for more reliable and\ninterpretable multimodal models.", "published": "2024-11-28 14:47:55", "link": "http://arxiv.org/abs/2411.19187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Extensive Evaluation of Factual Consistency in Large Language Models\n  for Data-to-Text Generation", "abstract": "Large Language Models (LLMs) have shown exceptional performance across\nvarious Data-to-Text Generation (DTG) tasks. However, generating factually\nconsistent text in DTG remains challenging for LLMs. Despite this, in-depth\nevaluations of LLM factual consistency for DTG remain missing in the current\nliterature. This paper addresses this gap by providing an extensive evaluation\nof factual consistency in LLMs for DTG. Our evaluation covers five widely used\nDTG datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and five prominent\nLLM families (T5, BART, OPT, BLOOM, and Llama 2). To ensure a thorough\nevaluation of factual consistency, we use four state-of-the-art automatic\nmetrics and include essential human assessments. Our extensive evaluations\nreveals three key findings regarding factual consistency in LLMs for DTG.\nFirst, Llama 2 often excels in generating factually consistent text, although\nsmaller models like T5 and BART can achieve strong factual consistency on\nlarger, lexically less-diverse datasets. Second, the average rate of change\n(AROC) indicates that increasing model size (number of model trainable\nparameters) generally enhances factual consistency of LLMs in DTG. Third, we\nobserve that source-reference divergence (i.e., when the reference text\ndiverges semantically from the source) typically reduces the factual\nconsistency of LLMs in DTG.", "published": "2024-11-28 15:23:12", "link": "http://arxiv.org/abs/2411.19203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How far can bias go? -- Tracing bias from pretraining data to alignment", "abstract": "As LLMs are increasingly integrated into user-facing applications, addressing\nbiases that perpetuate societal inequalities is crucial. While much work has\ngone into measuring or mitigating biases in these models, fewer studies have\ninvestigated their origins. Therefore, this study examines the correlation\nbetween gender-occupation bias in pre-training data and their manifestation in\nLLMs, focusing on the Dolma dataset and the OLMo model. Using zero-shot\nprompting and token co-occurrence analyses, we explore how biases in training\ndata influence model outputs. Our findings reveal that biases present in\npre-training data are amplified in model outputs. The study also examines the\neffects of prompt types, hyperparameters, and instruction-tuning on bias\nexpression, finding instruction-tuning partially alleviating representational\nbias while still maintaining overall stereotypical gender associations, whereas\nhyperparameters and prompting variation have a lesser effect on bias\nexpression. Our research traces bias throughout the LLM development pipeline\nand underscores the importance of mitigating bias at the pretraining stage.", "published": "2024-11-28 16:20:25", "link": "http://arxiv.org/abs/2411.19240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consolidating and Developing Benchmarking Datasets for the Nepali\n  Natural Language Understanding Tasks", "abstract": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects, which pose a\nunique challenge for natural language processing (NLP) evaluation. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of NLP\nmodels. To address this limitation, we introduce eight new datasets, creating a\nnew benchmark, the Nepali Language Understanding Evaluation (NLUE) benchmark,\nwhich covers a total of 12 tasks for evaluating the performance of models\nacross a diverse set of Natural Language Understanding (NLU) tasks. The added\ntasks include single-sentence classification, similarity and paraphrase tasks,\nand Natural Language Inference (NLI) tasks. On evaluating the models using\nadded tasks, we observe that the existing models fall short in handling complex\nNLU tasks effectively. This expanded benchmark sets a new standard for\nevaluating, comparing, and advancing models, contributing significantly to the\nbroader goal of advancing NLP research for low-resource languages.", "published": "2024-11-28 16:32:02", "link": "http://arxiv.org/abs/2411.19244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Information in a Low-resource Setting: Case Study on\n  Bioinformatics Workflows", "abstract": "Bioinformatics workflows are essential for complex biological data analyses\nand are often described in scientific articles with source code in public\nrepositories. Extracting detailed workflow information from articles can\nimprove accessibility and reusability but is hindered by limited annotated\ncorpora. To address this, we framed the problem as a low-resource extraction\ntask and tested four strategies: 1) creating a tailored annotated corpus, 2)\nfew-shot named-entity recognition (NER) with an autoregressive language model,\n3) NER using masked language models with existing and new corpora, and 4)\nintegrating workflow knowledge into NER models. Using BioToFlow, a new corpus\nof 52 articles annotated with 16 entities, a SciBERT-based NER model achieved a\n70.4 F-measure, comparable to inter-annotator agreement. While knowledge\nintegration improved performance for specific entities, it was less effective\nacross the entire information schema. Our results demonstrate that\nhigh-performance information extraction for bioinformatics workflows is\nachievable.", "published": "2024-11-28 18:04:31", "link": "http://arxiv.org/abs/2411.19295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Learning Content Retrieval with Knowledge Injection", "abstract": "With the rise of online education platforms, there is a growing abundance of\neducational content across various domain. It can be difficult to navigate the\nnumerous available resources to find the most suitable training, especially in\ndomains that include many interconnected areas, such as ICT. In this study, we\npropose a domain-specific chatbot application that requires limited resources,\nutilizing versions of the Phi language model to help learners with educational\ncontent. In the proposed method, Phi-2 and Phi-3 models were fine-tuned using\nQLoRA. The data required for fine-tuning was obtained from the Huawei Talent\nPlatform, where courses are available at different levels of expertise in the\nfield of computer science. RAG system was used to support the model, which was\nfine-tuned by 500 Q&A pairs. Additionally, a total of 420 Q&A pairs of content\nwere extracted from different formats such as JSON, PPT, and DOC to create a\nvector database to be used in the RAG system. By using the fine-tuned model and\nRAG approach together, chatbots with different competencies were obtained. The\nquestions and answers asked to the generated chatbots were saved separately and\nevaluated using ROUGE, BERTScore, METEOR, and BLEU metrics. The precision value\nof the Phi-2 model supported by RAG was 0.84 and the F1 score was 0.82. In\naddition to a total of 13 different evaluation metrics in 4 different\ncategories, the answers of each model were compared with the created content\nand the most appropriate method was selected for real-life applications.", "published": "2024-11-28 12:06:14", "link": "http://arxiv.org/abs/2412.00125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAG-V: A Multi-Agent Framework for Synthetic Data Generation and\n  Verification", "abstract": "Extending the capabilities of Large Language Models (LLMs) with functions or\ntools for environment interaction has led to the emergence of the agent\nparadigm. In industry, training an LLM is not always feasible because of the\nscarcity of domain data, legal holds on proprietary customer data, rapidly\nchanging business requirements, and the need to prototype new assistants.\nAgents provide an elegant solution to the above by relying on the zero-shot\nreasoning abilities of the underlying LLM and utilizing tools to explore and\nreason over customer data and respond to user requests. However, there are two\nconcerns here: (I) acquiring large scale customer queries for agent testing is\ntime-consuming, and (II) high reliance on the tool call sequence (or\ntrajectory) followed by the agent to respond to user queries may lead to\nunexpected or incorrect behavior. To address this, we propose MAG-V, a\nmulti-agent framework to first generate a dataset of questions that mimic\ncustomer queries; and second, reverse-engineer alternate questions from the\nresponses for trajectory verification. Initial results indicate that our\nsynthetic data can improve agent performance on actual customer queries.\nFurthermore, our trajectory verification methodology, inspired by distant\nsupervision and using traditional machine learning (ML) models, outperforms a\nGPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4\njudge on our constructed dataset. Overall, our approach is a step towards\nunifying diverse task agents into a cohesive framework for achieving an aligned\nobjective.", "published": "2024-11-28 19:36:11", "link": "http://arxiv.org/abs/2412.04494v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks", "abstract": "Sparse Autoencoders (SAEs) are an interpretability technique aimed at\ndecomposing neural network activations into interpretable units. However, a\nmajor bottleneck for SAE development has been the lack of high-quality\nperformance metrics, with prior work largely relying on unsupervised proxies.\nIn this work, we introduce a family of evaluations based on SHIFT, a downstream\ntask from Marks et al. (Sparse Feature Circuits, 2024) in which spurious cues\nare removed from a classifier by ablating SAE features judged to be\ntask-irrelevant by a human annotator. We adapt SHIFT into an automated metric\nof SAE quality; this involves replacing the human annotator with an LLM.\nAdditionally, we introduce the Targeted Probe Perturbation (TPP) metric that\nquantifies an SAE's ability to disentangle similar concepts, effectively\nscaling SHIFT to a wider range of datasets. We apply both SHIFT and TPP to\nmultiple open-source models, demonstrating that these metrics effectively\ndifferentiate between various SAE training hyperparameters and architectures.", "published": "2024-11-28 03:58:48", "link": "http://arxiv.org/abs/2411.18895v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MATATA: A weakly-supervised MAthematical Tool-Assisted reasoning for\n  Tabular Applications", "abstract": "Mathematical reasoning capabilities are increasing with tool-augmented\nlanguage agents, but methods often rely either on closed-source or large\nmodels, external data, or extensive prompt engineering. This work introduces\nMATATA, a novel cost-effective method to train LLM agents for tabular data\nproblems through reasoning, planning, and tool use. With a progressive\nself-improvement paradigm and an iterative weak supervision, it empowers\n3.8B/8B Small Language Models (SLMs), particularly suited for local hosting and\nsensitive business contexts where data privacy is crucial. By employing a\nflexible and reusable tools across different datasets, it achieves robust\nperformance with effective scalability across shared tasks. Experiments show\nthat MATATA reaches state-of-the-art performances on FinQA and TAT-QA among\nreasoning frameworks based on open-source models. Moreover, MATATA models\ncompete with GPT-4 based frameworks on TabMWP, while being SLMs.", "published": "2024-11-28 05:12:17", "link": "http://arxiv.org/abs/2411.18915v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Devising a Set of Compact and Explainable Spoken Language Feature for\n  Screening Alzheimer's Disease", "abstract": "Alzheimer's disease (AD) has become one of the most significant health\nchallenges in an aging society. The use of spoken language-based AD detection\nmethods has gained prevalence due to their scalability due to their\nscalability. Based on the Cookie Theft picture description task, we devised an\nexplainable and effective feature set that leverages the visual capabilities of\na large language model (LLM) and the Term Frequency-Inverse Document Frequency\n(TF-IDF) model. Our experimental results show that the newly proposed features\nconsistently outperform traditional linguistic features across two different\nclassifiers with high dimension efficiency. Our new features can be well\nexplained and interpreted step by step which enhance the interpretability of\nautomatic AD screening.", "published": "2024-11-28 05:23:22", "link": "http://arxiv.org/abs/2411.18922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EzSQL: An SQL intermediate representation for improving SQL-to-text\n  Generation", "abstract": "The SQL-to-text generation task traditionally uses template base, Seq2Seq,\ntree-to-sequence, and graph-to-sequence models. Recent models take advantage of\npre-trained generative language models for this task in the Seq2Seq framework.\nHowever, treating SQL as a sequence of inputs to the pre-trained models is not\noptimal. In this work, we put forward a new SQL intermediate representation\ncalled EzSQL to align SQL with the natural language text sequence. EzSQL\nsimplifies the SQL queries and brings them closer to natural language text by\nmodifying operators and keywords, which can usually be described in natural\nlanguage. EzSQL also removes the need for set operators. Our proposed\nSQL-to-text generation model uses EzSQL as the input to a pre-trained\ngenerative language model for generating the text descriptions. We demonstrate\nthat our model is an effective state-of-the-art method to generate text\nnarrations from SQL queries on the WikiSQL and Spider datasets. We also show\nthat by generating pretraining data using our SQL-to-text generation model, we\ncan enhance the performance of Text-to-SQL parsers.", "published": "2024-11-28 05:24:46", "link": "http://arxiv.org/abs/2411.18923v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual\n  Semantic Textual Relatedness Task", "abstract": "Cross-lingual semantic textual relatedness task is an important research task\nthat addresses challenges in cross-lingual communication and text\nunderstanding. It helps establish semantic connections between different\nlanguages, crucial for downstream tasks like machine translation, multilingual\ninformation retrieval, and cross-lingual text understanding.Based on extensive\ncomparative experiments, we choose the XLM-R-base as our base model and use\npre-trained sentence representations based on whitening to reduce\nanisotropy.Additionally, for the given training data, we design a delicate data\nfiltering method to alleviate the curse of multilingualism. With our approach,\nwe achieve a 2nd score in Spanish, a 3rd in Indonesian, and multiple entries in\nthe top ten results in the competition's track C. We further do a comprehensive\nanalysis to inspire future research aimed at improving performance on\ncross-lingual tasks.", "published": "2024-11-28 08:40:14", "link": "http://arxiv.org/abs/2411.18990v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey on Automatic Online Hate Speech Detection in Low-Resource\n  Languages", "abstract": "The expanding influence of social media platforms over the past decade has\nimpacted the way people communicate. The level of obscurity provided by social\nmedia and easy accessibility of the internet has facilitated the spread of hate\nspeech. The terms and expressions related to hate speech gets updated with\nchanging times which poses an obstacle to policy-makers and researchers in case\nof hate speech identification. With growing number of individuals using their\nnative languages to communicate with each other, hate speech in these\nlow-resource languages are also growing. Although, there is awareness about the\nEnglish-related approaches, much attention have not been provided to these\nlow-resource languages due to lack of datasets and online available data. This\narticle provides a detailed survey of hate speech detection in low-resource\nlanguages around the world with details of available datasets, features\nutilized and techniques used. This survey further discusses the prevailing\nsurveys, overlapping concepts related to hate speech, research challenges and\nopportunities.", "published": "2024-11-28 09:42:53", "link": "http://arxiv.org/abs/2411.19017v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DIESEL -- Dynamic Inference-Guidance via Evasion of Semantic Embeddings\n  in LLMs", "abstract": "In recent years, large language models (LLMs) have had great success in tasks\nsuch as casual conversation, contributing to significant advancements in\ndomains like virtual assistance. However, they often generate responses that\nare not aligned with human values (e.g., ethical standards, safety), leading to\npotentially unsafe or inappropriate outputs. While several techniques have been\nproposed to address this problem, they come with a cost, requiring\ncomputationally expensive training or dramatically increasing the inference\ntime. In this paper, we present DIESEL, a lightweight inference-guidance\ntechnique that can be seamlessly integrated into any autoregressive LLM to\nsemantically filter undesired concepts from the response. DIESEL can function\neither as a standalone safeguard or as an additional layer of defense,\nenhancing response safety by reranking the LLM's proposed tokens based on their\nsimilarity to predefined negative concepts in the latent space. Our evaluation\ndemonstrates DIESEL's effectiveness on state-of-the-art conversational models,\neven in adversarial jailbreaking scenarios that challenge response safety. We\nalso highlight DIESEL's generalization capabilities, showing that it can be\nused in use cases other than safety, providing general-purpose response\nfiltering.", "published": "2024-11-28 10:33:11", "link": "http://arxiv.org/abs/2411.19038v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Way to Specialist: Closing Loop Between Specialized LLM and Evolving\n  Domain Knowledge Graph", "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\na wide variety of domains. Nonetheless, generalist LLMs continue to fall short\nin reasoning tasks necessitating specialized knowledge. Prior investigations\ninto specialized LLMs focused on domain-specific training, which entails\nsubstantial efforts in domain data acquisition and model parameter fine-tuning.\nTo address these challenges, this paper proposes the Way-to-Specialist (WTS)\nframework, which synergizes retrieval-augmented generation with knowledge\ngraphs (KGs) to enhance the specialized capability of LLMs in the absence of\nspecialized training. In distinction to existing paradigms that merely utilize\nexternal knowledge from general KGs or static domain KGs to prompt LLM for\nenhanced domain-specific reasoning, WTS proposes an innovative\n\"LLM$\\circlearrowright$KG\" paradigm, which achieves bidirectional enhancement\nbetween specialized LLM and domain knowledge graph (DKG). The proposed paradigm\nencompasses two closely coupled components: the DKG-Augmented LLM and the\nLLM-Assisted DKG Evolution. The former retrieves question-relevant domain\nknowledge from DKG and uses it to prompt LLM to enhance the reasoning\ncapability for domain-specific tasks; the latter leverages LLM to generate new\ndomain knowledge from processed tasks and use it to evolve DKG. WTS closes the\nloop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling\ncontinuous improvement in the domain specialization as it progressively answers\nand learns from domain-specific questions. We validate the performance of WTS\non 6 datasets spanning 5 domains. The experimental results show that WTS\nsurpasses the previous SOTA in 4 specialized domains and achieves a maximum\nperformance improvement of 11.3%.", "published": "2024-11-28 11:24:43", "link": "http://arxiv.org/abs/2411.19064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models", "abstract": "In this paper, we introduce an open-source Korean-English vision-language\nmodel (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that\nallows a model learn both linguistic and visual information while preserving\nthe backbone model's knowledge. Our model demonstrates outstanding performance\nin diverse settings requiring bilingual image-text understanding and generation\nabilities compared to models of similar size. VARCO-VISION is also capable of\ngrounding, referring, and OCR, expanding its usage and potential applications\nfor real-world scenarios. In addition to the model, we release five Korean\nevaluation datasets, including four closed-set and one openset benchmarks. We\nanticipate that our milestone will broaden the opportunities for AI researchers\naiming to train VLMs. VARCO-VISION is available at\nhttps://huggingface.co/NCSOFT/VARCO-VISION-14B.", "published": "2024-11-28 12:38:42", "link": "http://arxiv.org/abs/2411.19103v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Integration of Contextual Descriptors in Ontology Alignment for\n  Enrichment of Semantic Correspondence", "abstract": "This paper proposes a novel approach to semantic ontology alignment using\ncontextual descriptors. A formalization was developed that enables the\nintegration of essential and contextual descriptors to create a comprehensive\nknowledge model. The hierarchical structure of the semantic approach and the\nmathematical apparatus for analyzing potential conflicts between concepts,\nparticularly in the example of \"Transparency\" and \"Privacy\" in the context of\nartificial intelligence, are demonstrated. Experimental studies showed a\nsignificant improvement in ontology alignment metrics after the implementation\nof contextual descriptors, especially in the areas of privacy, responsibility,\nand freedom & autonomy. The application of contextual descriptors achieved an\naverage overall improvement of approximately 4.36%. The results indicate the\neffectiveness of the proposed approach for more accurately reflecting the\ncomplexity of knowledge and its contextual dependence.", "published": "2024-11-28 12:59:32", "link": "http://arxiv.org/abs/2411.19113v1", "categories": ["cs.CL", "cs.IR", "I.2.4; I.2.3; K.4.1"], "primary_category": "cs.CL"}
{"title": "NERsocial: Efficient Named Entity Recognition Dataset Construction for\n  Human-Robot Interaction Utilizing RapidNER", "abstract": "Adapting named entity recognition (NER) methods to new domains poses\nsignificant challenges. We introduce RapidNER, a framework designed for the\nrapid deployment of NER systems through efficient dataset construction.\nRapidNER operates through three key steps: (1) extracting domain-specific\nsub-graphs and triples from a general knowledge graph, (2) collecting and\nleveraging texts from various sources to build the NERsocial dataset, which\nfocuses on entities typical in human-robot interaction, and (3) implementing an\nannotation scheme using Elasticsearch (ES) to enhance efficiency. NERsocial,\nvalidated by human annotators, includes six entity types, 153K tokens, and\n99.4K sentences, demonstrating RapidNER's capability to expedite dataset\ncreation.", "published": "2024-11-28 03:24:49", "link": "http://arxiv.org/abs/2412.09634v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "ArEEG_Words: Dataset for Envisioned Speech Recognition using EEG for\n  Arabic Words", "abstract": "Brain-Computer-Interface (BCI) aims to support communication-impaired\npatients by translating neural signals into speech. A notable research topic in\nBCI involves Electroencephalography (EEG) signals that measure the electrical\nactivity in the brain. While significant advancements have been made in BCI EEG\nresearch, a major limitation still exists: the scarcity of publicly available\nEEG datasets for non-English languages, such as Arabic. To address this gap, we\nintroduce in this paper ArEEG_Words dataset, a novel EEG dataset recorded from\n22 participants with mean age of 22 years (5 female, 17 male) using a\n14-channel Emotiv Epoc X device. The participants were asked to be free from\nany effects on their nervous system, such as coffee, alcohol, cigarettes, and\nso 8 hours before recording. They were asked to stay calm in a clam room during\nimagining one of the 16 Arabic Words for 10 seconds. The words include 16\ncommonly used words such as up, down, left, and right. A total of 352 EEG\nrecordings were collected, then each recording was divided into multiple 250ms\nsignals, resulting in a total of 15,360 EEG signals. To the best of our\nknowledge, ArEEG_Words data is the first of its kind in Arabic EEG domain.\nMoreover, it is publicly available for researchers as we hope that will fill\nthe gap in Arabic EEG research.", "published": "2024-11-28 03:31:12", "link": "http://arxiv.org/abs/2411.18888v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large\n  Multimodal Models with Visual Programming Challenges", "abstract": "Recent advancements in large multimodal models (LMMs) have showcased\nimpressive code generation capabilities, primarily evaluated through\nimage-to-code benchmarks. However, these benchmarks are limited to specific\nvisual programming scenarios where the logic reasoning and the multimodal\nunderstanding capacities are split apart. To fill this gap, we propose\nScratchEval, a novel benchmark designed to evaluate the visual programming\nreasoning ability of LMMs. ScratchEval is based on Scratch, a block-based\nvisual programming language widely used in children's programming education. By\nintegrating visual elements and embedded programming logic, ScratchEval\nrequires the model to process both visual information and code structure,\nthereby comprehensively evaluating its programming intent understanding\nability. Our evaluation approach goes beyond the traditional image-to-code\nmapping and focuses on unified logical thinking and problem-solving abilities,\nproviding a more comprehensive and challenging framework for evaluating the\nvisual programming ability of LMMs. ScratchEval not only fills the gap in\nexisting evaluation methods, but also provides new insights for the future\ndevelopment of LMMs in the field of visual programming. Our benchmark can be\naccessed at https://github.com/HKBUNLP/ScratchEval .", "published": "2024-11-28 05:51:45", "link": "http://arxiv.org/abs/2411.18932v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language\n  for Open-Vocabulary Segmentation", "abstract": "Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/.", "published": "2024-11-28 19:00:03", "link": "http://arxiv.org/abs/2411.19331v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image\n  Collections", "abstract": "In the era of foundation models, CLIP has emerged as a powerful tool for\naligning text & visual modalities into a common embedding space. However, the\nalignment objective used to train CLIP often results in subpar visual features\nfor fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at\nextracting rich visual features due to their specialized training paradigm.\nYet, these SSL models require an additional supervised linear probing step,\nwhich relies on fully labeled data which is often expensive and difficult to\nobtain at scale. In this paper, we propose a label-free prompt-tuning method\nthat leverages the rich visual features of self-supervised learning models\n(DINO) and the broad textual knowledge of large language models (LLMs) to\nlargely enhance CLIP-based image classification performance using unlabeled\nimages. Our approach unfolds in three key steps: (1) We generate robust textual\nfeature embeddings that more accurately represent object classes by leveraging\nclass-specific descriptions from LLMs, enabling more effective zero-shot\nclassification compared to CLIP's default name-specific prompts. (2) These\ntextual embeddings are then used to produce pseudo-labels to train an alignment\nmodule that integrates the complementary strengths of LLM description-based\ntextual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's\nvision encoder through DINO-assisted supervision using the trained alignment\nmodule. This three-step process allows us to harness the best of visual &\ntextual foundation models, resulting in a powerful and efficient approach that\nsurpasses state-of-the-art label-free classification methods. Notably, our\nframework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%\nover the state-of-the-art LaFTer across 11 diverse image classification\ndatasets. Our code & models can be found at https://github.com/fazliimam/NoLA.", "published": "2024-11-28 19:48:54", "link": "http://arxiv.org/abs/2411.19346v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack\n  Abilities", "abstract": "The Needle-in-a-haystack (NIAH) test is a general task used to assess\nlanguage models' (LMs') abilities to recall particular information from long\ninput context. This framework however does not provide a means of analyzing\nwhat factors, beyond context length, contribute to LMs' abilities or\ninabilities to separate and recall needles from their haystacks. To provide a\nsystematic means of assessing what features contribute to LMs' NIAH\ncapabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented\nEvaluation of NIAH for LLM's). Our work expands on previous NIAH studies by\nablating NIAH features beyond typical context length including data type, size,\nand patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's\nperformance on DENIAHL, and drops in recall performance when features like item\nsize are increased, and to some degree when data type is changed from numbers\nto letters. This has implications for increasingly large context models,\ndemonstrating factors beyond item-number impact NIAH capabilities.", "published": "2024-11-28 20:14:47", "link": "http://arxiv.org/abs/2411.19360v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Libra: Leveraging Temporal Images for Biomedical Radiology Analysis", "abstract": "Radiology report generation (RRG) requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. While multimodal\nlarge language models (MLLMs) align with pre-trained vision encoders to enhance\nvisual-language understanding, most existing methods rely on single-image\nanalysis or rule-based heuristics to process multiple images, failing to fully\nleverage temporal information in multi-modal medical datasets. In this paper,\nwe introduce Libra, a temporal-aware MLLM tailored for chest X-ray report\ngeneration. Libra combines a radiology-specific image encoder with a novel\nTemporal Alignment Connector (TAC), designed to accurately capture and\nintegrate temporal differences between paired current and prior images.\nExtensive experiments on the MIMIC-CXR dataset demonstrate that Libra\nestablishes a new state-of-the-art benchmark among similarly scaled MLLMs,\nsetting new standards in both clinical relevance and lexical accuracy.", "published": "2024-11-28 21:07:22", "link": "http://arxiv.org/abs/2411.19378v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "I.2.10; J.3; I.5.4"], "primary_category": "cs.CV"}
{"title": "Orthus: Autoregressive Interleaved Image-Text Generation with\n  Modality-Specific Heads", "abstract": "We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.", "published": "2024-11-28 13:00:38", "link": "http://arxiv.org/abs/2412.00127v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sparse Attention Vectors: Generative Multimodal Model Features Are\n  Discriminative Vision-Language Classifiers", "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks such as image captioning or visual\nquestion answering. Despite strong performance, LMMs are not directly suited\nfor foundational discriminative vision-language tasks (i.e., tasks requiring\ndiscrete label predictions) such as image classification and multiple-choice\nVQA. One key challenge in utilizing LMMs for discriminative tasks is the\nextraction of useful features from generative models. To overcome this issue,\nwe propose an approach for finding features in the model's latent space to more\neffectively leverage LMMs for discriminative tasks. Toward this end, we present\nSparse Attention Vectors (SAVs) -- a finetuning-free method that leverages\nsparse attention head activations (fewer than 1\\% of the heads) in LMMs as\nstrong features for VL tasks. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of discriminative tasks. Our experiments also imply\nthat SAVs can scale in performance with additional examples and generalize to\nsimilar tasks, establishing SAVs as both effective and robust multimodal\nfeature representations.", "published": "2024-11-28 18:55:41", "link": "http://arxiv.org/abs/2412.00142v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Using Images to Find Context-Independent Word Representations in Vector\n  Space", "abstract": "Many methods have been proposed to find vector representation for words, but\nmost rely on capturing context from the text to find semantic relationships\nbetween these vectors. We propose a novel method of using dictionary meanings\nand image depictions to find word vectors independent of any context. We use\nauto-encoder on the word images to find meaningful representations and use them\nto calculate the word vectors. We finally evaluate our method on word\nsimilarity, concept categorization and outlier detection tasks. Our method\nperforms comparably to context-based methods while taking much less training\ntime.", "published": "2024-11-28 08:44:10", "link": "http://arxiv.org/abs/2412.03592v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CovidLLM: A Robust Large Language Model with Missing Value Adaptation\n  and Multi-Objective Learning Strategy for Predicting Disease Severity and\n  Clinical Outcomes in COVID-19 Patients", "abstract": "Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused\nmillions of deaths worldwide. Although effective vaccines have been developed\nto mitigate severe symptoms, certain populations, particularly the elderly and\nthose with comorbidities, remain at high risk for severe outcomes and increased\nmortality. Consequently, early identification of the severity and clinical\noutcomes of the disease in these patients is vital to prevent adverse\nprognoses. Although traditional machine learning and deep learning models have\nbeen widely employed in this area, the potential of large language models\n(LLMs) remains largely unexplored. Our research focuses primarily on\nconstructing specialized prompts and adopting multi-objective learning\nstrategies. We started by selecting serological indicators that significantly\ncorrelate with clinical outcomes and disease severity to serve as input data\nfor the model. Blood test samples often contain numerous missing values, and\ntraditional models generally rely on imputation to handle these gaps in the\ndata. In contrast, LLMs offer the advantage of robust semantic understanding.\nBy setting prompts, we can explicitly inform the model when a feature's value\nis missing, without the need for imputation. For the multi-objective learning\nstrategy, the model is designed to first predict disease severity and then\npredict clinical outcomes. Given that LLMs utilize both the input text and the\ngenerated tokens as input for generating the next token, the predicted severity\nis used as a basis for generating the clinical outcome. During the fine-tuning\nof the LLM, the two objectives influence and improve each other. Our\nexperiments were implemented based on the ChatGLM model. The results\ndemonstrate the effectiveness of LLMs in this task, suggesting promising\npotential for further development.", "published": "2024-11-28 11:27:38", "link": "http://arxiv.org/abs/2412.03593v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examining Multimodal Gender and Content Bias in ChatGPT-4o", "abstract": "This study investigates ChatGPT-4o's multimodal content generation,\nhighlighting significant disparities in its treatment of sexual content and\nnudity versus violent and drug-related themes. Detailed analysis reveals that\nChatGPT-4o consistently censors sexual content and nudity, while showing\nleniency towards violence and drug use. Moreover, a pronounced gender bias\nemerges, with female-specific content facing stricter regulation compared to\nmale-specific content. This disparity likely stems from media scrutiny and\npublic backlash over past AI controversies, prompting tech companies to impose\nstringent guidelines on sensitive issues to protect their reputations. Our\nfindings emphasize the urgent need for AI systems to uphold genuine ethical\nstandards and accountability, transcending mere political correctness. This\nresearch contributes to the understanding of biases in AI-driven language and\nmultimodal models, calling for more balanced and ethical content moderation\npractices.", "published": "2024-11-28 13:41:44", "link": "http://arxiv.org/abs/2411.19140v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "stat.OT"], "primary_category": "cs.CY"}
{"title": "AudioSetCaps: An Enriched Audio-Caption Dataset using Automated\n  Generation Pipeline with Large Audio and Language Models", "abstract": "With the emergence of audio-language models, constructing large-scale paired\naudio-language datasets has become essential yet challenging for model\ndevelopment, primarily due to the time-intensive and labour-heavy demands\ninvolved. While large language models (LLMs) have improved the efficiency of\nsynthetic audio caption generation, current approaches struggle to effectively\nextract and incorporate detailed audio information. In this paper, we propose\nan automated pipeline that integrates audio-language models for fine-grained\ncontent extraction, LLMs for synthetic caption generation, and a contrastive\nlanguage-audio pretraining (CLAP) model-based refinement process to improve the\nquality of captions. Specifically, we employ prompt chaining techniques in the\ncontent extraction stage to obtain accurate and fine-grained audio information,\nwhile we use the refinement process to mitigate potential hallucinations in the\ngenerated captions. Leveraging the AudioSet dataset and the proposed approach,\nwe create AudioSetCaps, a dataset comprising 1.9 million audio-caption pairs,\nthe largest audio-caption dataset at the time of writing. The models trained\nwith AudioSetCaps achieve state-of-the-art performance on audio-text retrieval\nwith R@1 scores of 46.3% for text-to-audio and 59.7% for audio-to-text\nretrieval and automated audio captioning with the CIDEr score of 84.8. As our\napproach has shown promising results with AudioSetCaps, we create another\ndataset containing 4.1 million synthetic audio-language pairs based on the\nYoutube-8M and VGGSound datasets. To facilitate research in audio-language\nlearning, we have made our pipeline, datasets with 6 million audio-language\npairs, and pre-trained models publicly available at\nhttps://github.com/JishengBai/AudioSetCaps.", "published": "2024-11-28 06:50:13", "link": "http://arxiv.org/abs/2411.18953v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-shot Voice\n  Conversion", "abstract": "Zero-shot voice conversion (VC) aims to convert the original speaker's timbre\nto any target speaker while keeping the linguistic content. Current mainstream\nzero-shot voice conversion approaches depend on pre-trained recognition models\nto disentangle linguistic content and speaker representation. This results in a\ntimbre residue within the decoupled linguistic content and inadequacies in\nspeaker representation modeling. In this study, we propose CoDiff-VC, an\nend-to-end framework for zero-shot voice conversion that integrates a speech\ncodec and a diffusion model to produce high-fidelity waveforms. Our approach\ninvolves employing a single-codebook codec to separate linguistic content from\nthe source speech. To enhance content disentanglement, we introduce Mix-Style\nlayer normalization (MSLN) to perturb the original timbre. Additionally, we\nincorporate a multi-scale speaker timbre modeling approach to ensure timbre\nconsistency and improve voice detail similarity. To improve speech quality and\nspeaker similarity, we introduce dual classifier-free guidance, providing both\ncontent and timbre guidance during the generation process. Objective and\nsubjective experiments affirm that CoDiff-VC significantly improves speaker\nsimilarity, generating natural and higher-quality speech.", "published": "2024-11-28 05:12:42", "link": "http://arxiv.org/abs/2411.18918v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Voice-based Triage for Type 2 Diabetes using a Conversational Virtual\n  Assistant in the Home Environment", "abstract": "Incorporating cloud technology with Internet of Medical Things for ubiquitous\nhealthcare has seen many successful applications in the last decade with the\nadvent of machine learning and deep learning techniques. One of these\napplications, namely voice-based pathology, has yet to receive notable\nattention from academia and industry. Applying voice analysis to early\ndetection of fatal diseases holds much promise to improve health outcomes and\nquality of life of patients. In this paper, we propose a novel application of\nacoustic machine learning based triaging into commoditised conversational\nvirtual assistant systems to pre-screen for onset of diabetes. Specifically, we\ndeveloped a triaging system which extracts acoustic features from the voices of\nn=24 older adults when they converse with a virtual assistant and predict the\nincidence of Diabetes Mellitus (Type 2) or not. Our triaging system achieved\nhit-rates of 70% and 60% for male and female older adult subjects,\nrespectively. Our proposed triaging uses 7 non-identifiable voice-based\nfeatures and can operate within resource-constrained embedded systems running\nvoice-based virtual assistants. This application demonstrates the feasibility\nof applying voice-based pathology analysis to improve health outcomes of older\nadults within the home environment by early detection of life-changing chronic\nconditions like diabetes.", "published": "2024-11-28 15:23:48", "link": "http://arxiv.org/abs/2411.19204v1", "categories": ["cs.SD", "eess.AS", "F.2.2, I.2.7"], "primary_category": "cs.SD"}
{"title": "Parameter-Efficient Transfer Learning for Music Foundation Models", "abstract": "More music foundation models are recently being released, promising a\ngeneral, mostly task independent encoding of musical information. Common ways\nof adapting music foundation models to downstream tasks are probing and\nfine-tuning. These common transfer learning approaches, however, face\nchallenges. Probing might lead to suboptimal performance because the\npre-trained weights are frozen, while fine-tuning is computationally expensive\nand is prone to overfitting. Our work investigates the use of\nparameter-efficient transfer learning (PETL) for music foundation models which\nintegrates the advantage of probing and fine-tuning. We introduce three types\nof PETL methods: adapter-based methods, prompt-based methods, and\nreparameterization-based methods. These methods train only a small number of\nparameters, and therefore do not require significant computational resources.\nResults show that PETL methods outperform both probing and fine-tuning on music\nauto-tagging. On key detection and tempo estimation, they achieve similar\nresults as fine-tuning with significantly less training cost. However, the\nusefulness of the current generation of foundation model on key and tempo tasks\nis questioned by the similar results achieved by training a small model from\nscratch. Code available at https://github.com/suncerock/peft-music/", "published": "2024-11-28 20:50:40", "link": "http://arxiv.org/abs/2411.19371v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
