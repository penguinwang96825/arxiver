{"title": "Deep learning interpretability for rough volatility", "abstract": "Deep learning methods have become a widespread toolbox for pricing and\ncalibration of financial models. While they often provide new directions and\nresearch results, their `black box' nature also results in a lack of\ninterpretability. We provide a detailed interpretability analysis of these\nmethods in the context of rough volatility - a new class of volatility models\nfor Equity and FX markets. Our work sheds light on the neural network learned\ninverse map between the rough volatility model parameters, seen as mathematical\nmodel inputs and network outputs, and the resulting implied volatility across\nstrikes and maturities, seen as mathematical model outputs and network inputs.\nThis contributes to building a solid framework for a safer use of neural\nnetworks in this context and in quantitative finance more generally.", "published": "2024-11-28 18:42:44", "link": "http://arxiv.org/abs/2411.19317v1", "categories": ["q-fin.CP", "68T07, 91G20, 91G60"], "primary_category": "q-fin.CP"}
{"title": "On the relative performance of some parametric and nonparametric estimators of option prices", "abstract": "We examine the empirical performance of some parametric and nonparametric\nestimators of prices of options with a fixed time to maturity, focusing on\nvariance-gamma and Heston models on one side, and on expansions in Hermite\nfunctions on the other side. The latter class of estimators can be seen as\nperturbations of the classical Black-Scholes model. The comparison between\nparametric and Hermite-based models having the same \"degrees of freedom\" is\nemphasized. The main criterion is the out-of-sample relative pricing error on a\ndataset of historical option prices on the S&P500 index. Prior to the main\nempirical study, the approximation of variance-gamma and Heston densities by\nseries of Hermite functions is studied, providing explicit expressions for the\ncoefficients of the expansion in the former case, and integral expressions\ninvolving the explicit characteristic function in the latter case. Moreover,\nthese approximations are investigated numerically on a few test cases,\nindicating that expansions in Hermite functions with few terms achieve\ncompetitive accuracy in the estimation of Heston densities and the pricing of\n(European) options, but they perform less effectively with variance-gamma\ndensities. On the other hand, the main large-scale empirical study show that\nparsimonious Hermite estimators can even outperform the Heston model in terms\nof pricing errors. These results underscore the trade-offs inherent in model\nselection and calibration, and their empirical fit in practical applications.", "published": "2024-11-28 16:07:43", "link": "http://arxiv.org/abs/2412.00135v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "GRU-PFG: Extract Inter-Stock Correlation from Stock Factors with Graph Neural Network", "abstract": "The complexity of stocks and industries presents challenges for stock\nprediction. Currently, stock prediction models can be divided into two\ncategories. One category, represented by GRU and ALSTM, relies solely on stock\nfactors for prediction, with limited effectiveness. The other category,\nrepresented by HIST and TRA, incorporates not only stock factors but also\nindustry information, industry financial reports, public sentiment, and other\ninputs for prediction. The second category of models can capture correlations\nbetween stocks by introducing additional information, but the extra data is\ndifficult to standardize and generalize. Considering the current state and\nlimitations of these two types of models, this paper proposes the GRU-PFG\n(Project Factors into Graph) model. This model only takes stock factors as\ninput and extracts inter-stock correlations using graph neural networks. It\nachieves prediction results that not only outperform the others models relies\nsolely on stock factors, but also achieve comparable performance to the second\ncategory models. The experimental results show that on the CSI300 dataset, the\nIC of GRU-PFG is 0.134, outperforming HIST's 0.131 and significantly surpassing\nGRU and Transformer, achieving results better than the second category models.\nMoreover as a model that relies solely on stock factors, it has greater\npotential for generalization.", "published": "2024-11-28 08:50:55", "link": "http://arxiv.org/abs/2411.18997v1", "categories": ["q-fin.CP", "cs.AI"], "primary_category": "q-fin.CP"}
