{"title": "MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese\n  Grammatical Error Correction", "abstract": "This paper presents MuCGEC, a multi-reference multi-source evaluation dataset\nfor Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences\ncollected from three Chinese-as-a-Second-Language (CSL) learner sources. Each\nsentence is corrected by three annotators, and their corrections are carefully\nreviewed by a senior annotator, resulting in 2.3 references per sentence. We\nconduct experiments with two mainstream CGEC models, i.e., the\nsequence-to-sequence model and the sequence-to-edit model, both enhanced with\nlarge pretrained language models, achieving competitive benchmark performance\non previous and our datasets. We also discuss CGEC evaluation methodologies,\nincluding the effect of multiple references and using a char-based metric. Our\nannotation guidelines, data, and code are available at\n\\url{https://github.com/HillZhang1999/MuCGEC}.", "published": "2022-04-23 05:20:38", "link": "http://arxiv.org/abs/2204.10994v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LitMind Dictionary: An Open-Source Online Dictionary", "abstract": "Dictionaries can help language learners to learn vocabulary by providing\ndefinitions of words. Since traditional dictionaries present word senses as\ndiscrete items in predefined inventories, they fall short of flexibility, which\nis required in providing specific meanings of words in particular contexts. In\nthis paper, we introduce the LitMind Dictionary\n(https://dictionary.litmind.ink), an open-source online generative dictionary\nthat takes a word and context containing the word as input and automatically\ngenerates a definition as output. Incorporating state-of-the-art definition\ngeneration models, it supports not only Chinese and English, but also\nChinese-English cross-lingual queries. Moreover, it has a user-friendly\nfront-end design that can help users understand the query words quickly and\neasily. All the code and data are available at\nhttps://github.com/blcuicall/litmind-dictionary.", "published": "2022-04-23 15:10:40", "link": "http://arxiv.org/abs/2204.11087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiMulti: a Corpus for Cross-Lingual Summarization", "abstract": "Cross-lingual summarization (CLS) is the task to produce a summary in one\nparticular language for a source document in a different language. We introduce\nWikiMulti - a new dataset for cross-lingual summarization based on Wikipedia\narticles in 15 languages. As a set of baselines for further studies, we\nevaluate the performance of existing cross-lingual abstractive summarization\nmethods on our dataset. We make our dataset publicly available here:\nhttps://github.com/tikhonovpavel/wikimulti", "published": "2022-04-23 16:47:48", "link": "http://arxiv.org/abs/2204.11104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Meaning Representation for Task-Oriented Dialogue Systems", "abstract": "Dialogue meaning representation formulates natural language utterance\nsemantics in their conversational context in an explicit and machine-readable\nform. Previous work typically follows the intent-slot framework, which is easy\nfor annotation yet limited in scalability for complex linguistic expressions. A\nline of works alleviates the representation issue by introducing hierarchical\nstructures but challenging to express complex compositional semantics, such as\nnegation and coreference. We propose Dialogue Meaning Representation (DMR), a\npliable and easily extendable representation for task-oriented dialogue. Our\nrepresentation contains a set of nodes and edges to represent rich\ncompositional semantics. Moreover, we propose an inheritance hierarchy\nmechanism focusing on domain extensibility. Additionally, we annotated\nDMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with\nDMR. We propose two evaluation tasks to evaluate different dialogue models and\na novel coreference resolution model GNNCoref for the graph-based coreference\nresolution task. Experiments show that DMR can be parsed well with pre-trained\nSeq2Seq models, and GNNCoref outperforms the baseline models by a large margin.", "published": "2022-04-23 04:17:55", "link": "http://arxiv.org/abs/2204.10989v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Task Transferability in Large-Scale Multi-Task\n  Learning", "abstract": "Recent work has found that multi-task training with a large number of diverse\ntasks can uniformly improve downstream performance on unseen target tasks. In\ncontrast, literature on task transferability has established that the choice of\nintermediate tasks can heavily affect downstream task performance. In this\nwork, we aim to disentangle the effect of scale and relatedness of tasks in\nmulti-task representation learning. We find that, on average, increasing the\nscale of multi-task learning, in terms of the number of tasks, indeed results\nin better learned representations than smaller multi-task setups. However, if\nthe target tasks are known ahead of time, then training on a smaller set of\nrelated tasks is competitive to the large-scale multi-task training at a\nreduced computational cost.", "published": "2022-04-23 18:11:35", "link": "http://arxiv.org/abs/2204.11117v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Separation Consistency Training for Adaptation of\n  Unsupervised Speech Separation", "abstract": "Recently, supervised speech separation has made great progress. However,\nlimited by the nature of supervised training, most existing separation methods\nrequire ground-truth sources and are trained on synthetic datasets. This\nground-truth reliance is problematic, because the ground-truth signals are\nusually unavailable in real conditions. Moreover, in many industry scenarios,\nthe real acoustic characteristics deviate far from the ones in simulated\ndatasets. Therefore, the performance usually degrades significantly when\napplying the supervised speech separation models to real applications. To\naddress these problems, in this study, we propose a novel separation\nconsistency training, termed SCT, to exploit the real-world unlabeled mixtures\nfor improving cross-domain unsupervised speech separation in an iterative\nmanner, by leveraging upon the complementary information obtained from\nheterogeneous (structurally distinct but behaviorally complementary) models.\nSCT follows a framework using two heterogeneous neural networks (HNNs) to\nproduce high confidence pseudo labels of unlabeled real speech mixtures. These\nlabels are then updated, and used to refine the HNNs to produce more reliable\nconsistent separation results for real mixture pseudo-labeling. To maximally\nutilize the large complementary information between different separation\nnetworks, a cross-knowledge adaptation is further proposed. Together with\nsimulated dataset, those real mixtures with high confidence pseudo labels are\nthen used to update the HNN separation models iteratively. In addition, we find\nthat combing the heterogeneous separation outputs by a simple linear fusion can\nfurther slightly improve the final system performance.", "published": "2022-04-23 09:33:13", "link": "http://arxiv.org/abs/2204.11032v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Self-Supervised Learning-based MOS Prediction Networks", "abstract": "MOS (Mean Opinion Score) is a subjective method used for the evaluation of a\nsystem's quality. Telecommunications (for voice and video), and speech\nsynthesis systems (for generated speech) are a few of the many applications of\nthe method. While MOS tests are widely accepted, they are time-consuming and\ncostly since human input is required. In addition, since the systems and\nsubjects of the tests differ, the results are not really comparable. On the\nother hand, a large number of previous tests allow us to train machine learning\nmodels that are capable of predicting MOS value. By automatically predicting\nMOS values, both the aforementioned issues can be resolved.\n  The present work introduces data-, training- and post-training specific\nimprovements to a previous self-supervised learning-based MOS prediction model.\nWe used a wav2vec 2.0 model pre-trained on LibriSpeech, extended with LSTM and\nnon-linear dense layers. We introduced transfer learning, target data\npreprocessing a two- and three-phase training method with different batch\nformulations, dropout accumulation (for larger batch sizes) and quantization of\nthe predictions.\n  The methods are evaluated using the shared synthetic speech dataset of the\nfirst Voice MOS challenge.", "published": "2022-04-23 09:19:16", "link": "http://arxiv.org/abs/2204.11030v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musical Stylistic Analysis: A Study of Intervallic Transition Graphs via\n  Persistent Homology", "abstract": "Topological data analysis has been recently applied to investigate stylistic\nsignatures and trends in musical compositions. A useful tool in this area is\nPersistent Homology. In this paper, we develop a novel method to represent a\nweighted directed graph as a finite metric space and then use persistent\nhomology to extract useful features. We apply this method to weighted directed\ngraphs obtained from pitch transitions information of a given musical fragment\nand use these techniques to the study of stylistic trends. In particular, we\nare interested in using these tools to make quantitative stylistic comparisons.\nAs a first illustration, we analyze a selection of string quartets by Haydn,\nMozart and Beethoven and discuss possible implications of our results in terms\nof different approaches by these composers to stylistic exploration and\nvariety. We observe that Haydn is stylistically the most conservative, followed\nby Mozart, while Beethoven is the most innovative, expanding and modifying the\nstring quartet as a musical form. Finally we also compare the variability of\ndifferent genres, namely minuets, allegros, prestos and adagios, by a given\ncomposer and conclude that the minuet is the most stable form of the string\nquartet movements.", "published": "2022-04-23 20:33:49", "link": "http://arxiv.org/abs/2204.11139v1", "categories": ["cs.SD", "eess.AS", "math.AT", "62R40, 55N31 (Primary) 05C90, 00A65 (Secondary)"], "primary_category": "cs.SD"}
