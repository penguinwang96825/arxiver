{"title": "Cue Me In: Content-Inducing Approaches to Interactive Story Generation", "abstract": "Automatically generating stories is a challenging problem that requires\nproducing causally related and logical sequences of events about a topic.\nPrevious approaches in this domain have focused largely on one-shot generation,\nwhere a language model outputs a complete story based on limited initial input\nfrom a user. Here, we instead focus on the task of interactive story\ngeneration, where the user provides the model mid-level sentence abstractions\nin the form of cue phrases during the generation process. This provides an\ninterface for human users to guide the story generation. We present two\ncontent-inducing approaches to effectively incorporate this additional\ninformation. Experimental results from both automatic and human evaluations\nshow that these methods produce more topically coherent and personalized\nstories compared to baseline methods.", "published": "2020-10-20 00:36:15", "link": "http://arxiv.org/abs/2010.09935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elaborative Simplification: Content Addition and Explanation Generation\n  in Text Simplification", "abstract": "Much of modern-day text simplification research focuses on sentence-level\nsimplification, transforming original, more complex sentences into simplified\nversions. However, adding content can often be useful when difficult concepts\nand reasoning need to be explained. In this work, we present the first\ndata-driven study of content addition in text simplification, which we call\nelaborative simplification. We introduce a new annotated dataset of 1.3K\ninstances of elaborative simplification in the Newsela corpus, and analyze how\nentities, ideas, and concepts are elaborated through the lens of contextual\nspecificity. We establish baselines for elaboration generation using\nlarge-scale pre-trained language models, and demonstrate that considering\ncontextual specificity during generation can improve performance. Our results\nillustrate the complexities of elaborative simplification, suggesting many\ninteresting directions for future work.", "published": "2020-10-20 05:06:23", "link": "http://arxiv.org/abs/2010.10035v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Factual Completeness and Consistency of Image-to-Text\n  Radiology Report Generation", "abstract": "Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. However, existing report\ngeneration systems, despite achieving high performances on natural language\ngeneration metrics such as CIDEr or BLEU, still suffer from incomplete and\ninconsistent generations. Here we introduce two new simple rewards to encourage\nthe generation of factually complete and consistent radiology reports: one that\nencourages the system to generate radiology domain entities consistent with the\nreference, and one that uses natural language inference to encourage these\nentities to be described in inferentially consistent ways. We combine these\nwith the novel use of an existing semantic equivalence metric (BERTScore). We\nfurther propose a report generation system that optimizes these rewards via\nreinforcement learning. On two open radiology report datasets, our system\nsubstantially improved the F1 score of a clinical information extraction\nperformance by +22.1 (Delta +63.9%). We further show via a human evaluation and\na qualitative analysis that our system leads to generations that are more\nfactually complete and consistent compared to the baselines.", "published": "2020-10-20 05:42:47", "link": "http://arxiv.org/abs/2010.10042v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Commonsense Knowledge into Abstractive Dialogue\n  Summarization via Heterogeneous Graph Networks", "abstract": "Abstractive dialogue summarization is the task of capturing the highlights of\na dialogue and rewriting them into a concise version. In this paper, we present\na novel multi-speaker dialogue summarizer to demonstrate how large-scale\ncommonsense knowledge can facilitate dialogue understanding and summary\ngeneration. In detail, we consider utterance and commonsense knowledge as two\ndifferent types of data and design a Dialogue Heterogeneous Graph Network\n(D-HGN) for modeling both information. Meanwhile, we also add speakers as\nheterogeneous nodes to facilitate information flow. Experimental results on the\nSAMSum dataset show that our model can outperform various methods. We also\nconduct zero-shot setting experiments on the Argumentative Dialogue Summary\nCorpus, the results show that our model can better generalized to the new\ndomain.", "published": "2020-10-20 05:44:55", "link": "http://arxiv.org/abs/2010.10044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Language Modeling for Contextualized Temporal Graph Generation", "abstract": "This paper presents the first study on using large-scale pre-trained language\nmodels for automated generation of an event-level temporal graph for a\ndocument. Despite the huge success of neural pre-training methods in NLP tasks,\nits potential for temporal reasoning over event graphs has not been\nsufficiently explored. Part of the reason is the difficulty in obtaining large\ntraining corpora with human-annotated events and temporal links. We address\nthis challenge by using existing IE/NLP tools to automatically generate a large\nquantity (89,000) of system-produced document-graph pairs, and propose a novel\nformulation of the contextualized graph generation problem as a\nsequence-to-sequence mapping task. These strategies enable us to leverage and\nfine-tune pre-trained language models on the system-induced training data for\nthe graph generation task. Our experiments show that our approach is highly\neffective in generating structurally and semantically valid graphs. Further,\nevaluation on a challenging hand-labeled, out-domain corpus shows that our\nmethod outperforms the closest existing method by a large margin on several\nmetrics. Code and pre-trained models are available at\nhttps://github.com/madaan/temporal-graph-gen.", "published": "2020-10-20 07:08:00", "link": "http://arxiv.org/abs/2010.10077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JUNLP@Dravidian-CodeMix-FIRE2020: Sentiment Classification of Code-Mixed\n  Tweets using Bi-Directional RNN and Language Tags", "abstract": "Sentiment analysis has been an active area of research in the past two\ndecades and recently, with the advent of social media, there has been an\nincreasing demand for sentiment analysis on social media texts. Since the\nsocial media texts are not in one language and are largely code-mixed in\nnature, the traditional sentiment classification models fail to produce\nacceptable results. This paper tries to solve this very research problem and\nuses bi-directional LSTMs along with language tagging, to facilitate sentiment\ntagging of code-mixed Tamil texts that have been extracted from social media.\nThe presented algorithm, when evaluated on the test data, garnered precision,\nrecall, and F1 scores of 0.59, 0.66, and 0.58 respectively.", "published": "2020-10-20 08:10:29", "link": "http://arxiv.org/abs/2010.10111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supertagging-based Parsing with Linear Context-free Rewriting Systems", "abstract": "We present the first supertagging-based parser for LCFRS. It utilizes neural\nclassifiers and tremendously outperforms previous LCFRS-based parsers in both\naccuracy and parsing speed. Moreover, our results keep up with the best\n(general) discontinuous parsers, particularly the scores for discontinuous\nconstitutents are excellent. The heart of our approach is an efficient\nlexicalization procedure which induces a lexical LCFRS from any discontinuous\ntreebank. It is an adaptation of previous work by M\\\"orbitz and Ruprecht\n(2020). We also describe a modification to usual chart-based LCFRS parsing that\naccounts for supertagging and introduce a procedure for the transformation of\nlexical LCFRS derivations into equivalent parse trees of the original treebank.\nOur approach is implemented and evaluated on the English Discontinuous Penn\nTreebank and the German corpora NeGra and Tiger.", "published": "2020-10-20 13:02:42", "link": "http://arxiv.org/abs/2010.10238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-directional Cognitive Thinking Network for Machine Reading\n  Comprehension", "abstract": "We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for\nreading comprehension from the perspective of complementary learning systems\ntheory. It aims to simulate two ways of thinking in the brain to answer\nquestions, including reverse thinking and inertial thinking. To validate the\neffectiveness of our framework, we design a corresponding Bi-directional\nCognitive Thinking Network (BCTN) to encode the passage and generate a question\n(answer) given an answer (question) and decouple the bi-directional knowledge.\nThe model has the ability to reverse reasoning questions which can assist\ninertial thinking to generate more accurate answers. Competitive improvement is\nobserved in DuReader dataset, confirming our hypothesis that bi-directional\nknowledge helps the QA task. The novel framework shows an interesting\nperspective on machine reading comprehension and cognitive science.", "published": "2020-10-20 13:56:30", "link": "http://arxiv.org/abs/2010.10286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Guided Abstractive Text Summarization: a Joint Learning Approach", "abstract": "We introduce a new approach for abstractive text summarization, Topic-Guided\nAbstractive Summarization, which calibrates long-range dependencies from\ntopic-level features with globally salient content. The idea is to incorporate\nneural topic modeling with a Transformer-based sequence-to-sequence (seq2seq)\nmodel in a joint learning framework. This design can learn and preserve the\nglobal semantics of the document, which can provide additional contextual\nguidance for capturing important ideas of the document, thereby enhancing the\ngeneration of summary. We conduct extensive experiments on two datasets and the\nresults show that our proposed model outperforms many extractive and\nabstractive systems in terms of both ROUGE measurements and human evaluation.\nOur code is available at: https://github.com/chz816/tas.", "published": "2020-10-20 14:45:25", "link": "http://arxiv.org/abs/2010.10323v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary\n  Representations From Characters", "abstract": "Due to the compelling improvements brought by BERT, many recent\nrepresentation models adopted the Transformer architecture as their main\nbuilding block, consequently inheriting the wordpiece tokenization system\ndespite it not being intrinsically linked to the notion of Transformers. While\nthis system is thought to achieve a good balance between the flexibility of\ncharacters and the efficiency of full words, using predefined wordpiece\nvocabularies from the general domain is not always suitable, especially when\nbuilding models for specialized domains (e.g., the medical domain). Moreover,\nadopting a wordpiece tokenization shifts the focus from the word level to the\nsubword level, making the models conceptually more complex and arguably less\nconvenient in practice. For these reasons, we propose CharacterBERT, a new\nvariant of BERT that drops the wordpiece system altogether and uses a\nCharacter-CNN module instead to represent entire words by consulting their\ncharacters. We show that this new model improves the performance of BERT on a\nvariety of medical domain tasks while at the same time producing robust,\nword-level and open-vocabulary representations.", "published": "2020-10-20 15:58:53", "link": "http://arxiv.org/abs/2010.10392v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Interactive Knowledge Base Spelling Correction Models for\n  Low-Resource Languages", "abstract": "Spelling normalization for low resource languages is a challenging task\nbecause the patterns are hard to predict and large corpora are usually required\nto collect enough examples. This work shows a comparison of a neural model and\ncharacter language models with varying amounts on target language data. Our\nusage scenario is interactive correction with nearly zero amounts of training\nexamples, improving models as more data is collected, for example within a chat\napp. Such models are designed to be incrementally improved as feedback is given\nfrom users. In this work, we design a knowledge-base and prediction model\nembedded system for spelling correction in low-resource languages. Experimental\nresults on multiple languages show that the model could become effective with a\nsmall amount of data. We perform experiments on both natural and synthetic\ndata, as well as on data from two endangered languages (Ainu and Griko). Last,\nwe built a prototype system that was used for a small case study on Hinglish,\nwhich further demonstrated the suitability of our approach in real world\nscenarios.", "published": "2020-10-20 17:31:07", "link": "http://arxiv.org/abs/2010.10472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Inference with Mixed Effects", "abstract": "There is growing evidence that the prevalence of disagreement in the raw\nannotations used to construct natural language inference datasets makes the\ncommon practice of aggregating those annotations to a single label problematic.\nWe propose a generic method that allows one to skip the aggregation step and\ntrain on the raw annotations directly without subjecting the model to unwanted\nnoise that can arise from annotator response biases. We demonstrate that this\nmethod, which generalizes the notion of a \\textit{mixed effects model} by\nincorporating \\textit{annotator random effects} into any existing neural model,\nimproves performance over models that do not incorporate such effects.", "published": "2020-10-20 17:54:16", "link": "http://arxiv.org/abs/2010.10501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Highlighting: Creating Sub-Sentence Summary Highlights", "abstract": "Amongst the best means to summarize is highlighting. In this paper, we aim to\ngenerate summary highlights to be overlaid on the original documents to make it\neasier for readers to sift through a large amount of text. The method allows\nsummaries to be understood in context to prevent a summarizer from distorting\nthe original meaning, of which abstractive summarizers usually fall short. In\nparticular, we present a new method to produce self-contained highlights that\nare understandable on their own to avoid confusion. Our method combines\ndeterminantal point processes and deep contextualized representations to\nidentify an optimal set of sub-sentence segments that are both important and\nnon-redundant to form summary highlights. To demonstrate the flexibility and\nmodeling power of our method, we conduct extensive experiments on summarization\ndatasets. Our analysis provides evidence that highlighting is a promising\navenue of research towards future summarization.", "published": "2020-10-20 18:57:42", "link": "http://arxiv.org/abs/2010.10566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoMeTS: The Autocomplete for Medical Text Simplification", "abstract": "The goal of text simplification (TS) is to transform difficult text into a\nversion that is easier to understand and more broadly accessible to a wide\nvariety of readers. In some domains, such as healthcare, fully automated\napproaches cannot be used since information must be accurately preserved.\nInstead, semi-automated approaches can be used that assist a human writer in\nsimplifying text faster and at a higher quality. In this paper, we examine the\napplication of autocomplete to text simplification in the medical domain. We\nintroduce a new parallel medical data set consisting of aligned English\nWikipedia with Simple English Wikipedia sentences and examine the application\nof pretrained neural language models (PNLMs) on this dataset. We compare four\nPNLMs(BERT, RoBERTa, XLNet, and GPT-2), and show how the additional context of\nthe sentence to be simplified can be incorporated to achieve better results\n(6.17% absolute improvement over the best individual model). We also introduce\nan ensemble model that combines the four PNLMs and outperforms the best\nindividual model by 2.1%, resulting in an overall word prediction accuracy of\n64.52%.", "published": "2020-10-20 19:20:29", "link": "http://arxiv.org/abs/2010.10573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Media Bias in News Articles using Gaussian Bias Distributions", "abstract": "Media plays an important role in shaping public opinion. Biased media can\ninfluence people in undesirable directions and hence should be unmasked as\nsuch. We observe that featurebased and neural text classification approaches\nwhich rely only on the distribution of low-level lexical information fail to\ndetect media bias. This weakness becomes most noticeable for articles on new\nevents, where words appear in new contexts and hence their \"bias\npredictiveness\" is unclear. In this paper, we therefore study how second-order\ninformation about biased statements in an article helps to improve detection\neffectiveness. In particular, we utilize the probability distributions of the\nfrequency, positions, and sequential order of lexical and informational\nsentence-level bias in a Gaussian Mixture Model. On an existing media bias\ndataset, we find that the frequency and positions of biased statements strongly\nimpact article-level bias, whereas their exact sequential order is secondary.\nUsing a standard model for sentence-level bias detection, we provide empirical\nevidence that article-level bias detectors that use second-order information\nclearly outperform those without.", "published": "2020-10-20 22:20:49", "link": "http://arxiv.org/abs/2010.10649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Political Bias and Unfairness in News Articles at Different\n  Levels of Granularity", "abstract": "Media organizations bear great reponsibility because of their considerable\ninfluence on shaping beliefs and positions of our society. Any form of media\ncan contain overly biased content, e.g., by reporting on political events in a\nselective or incomplete manner. A relevant question hence is whether and how\nsuch form of imbalanced news coverage can be exposed. The research presented in\nthis paper addresses not only the automatic detection of bias but goes one step\nfurther in that it explores how political bias and unfairness are manifested\nlinguistically. In this regard we utilize a new corpus of 6964 news articles\nwith labels derived from adfontesmedia.com and develop a neural model for bias\nassessment. By analyzing this model on article excerpts, we find insightful\nbias patterns at different levels of text granularity, from single words to the\nwhole article discourse.", "published": "2020-10-20 22:25:00", "link": "http://arxiv.org/abs/2010.10652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transition-based Parsing with Stack-Transformers", "abstract": "Modeling the parser state is key to good performance in transition-based\nparsing. Recurrent Neural Networks considerably improved the performance of\ntransition-based systems by modelling the global state, e.g. stack-LSTM\nparsers, or local state modeling of contextualized features, e.g. Bi-LSTM\nparsers. Given the success of Transformer architectures in recent parsing\nsystems, this work explores modifications of the sequence-to-sequence\nTransformer architecture to model either global or local parser states in\ntransition-based parsing. We show that modifications of the cross attention\nmechanism of the Transformer considerably strengthen performance both on\ndependency and Abstract Meaning Representation (AMR) parsing tasks,\nparticularly for smaller models or limited training data.", "published": "2020-10-20 23:20:31", "link": "http://arxiv.org/abs/2010.10669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pushing the Limits of AMR Parsing with Self-Learning", "abstract": "Abstract Meaning Representation (AMR) parsing has experienced a notable\ngrowth in performance in the last two years, due both to the impact of transfer\nlearning and the development of novel architectures specific to AMR. At the\nsame time, self-learning techniques have helped push the performance boundaries\nof other natural language processing applications, such as machine translation\nor question answering. In this paper, we explore different ways in which\ntrained models can be applied to improve AMR parsing performance, including\ngeneration of synthetic text and AMR annotations as well as refinement of\nactions oracle. We show that, without any additional human annotations, these\ntechniques improve an already performant parser and achieve state-of-the-art\nresults on AMR 1.0 and AMR 2.0.", "published": "2020-10-20 23:45:04", "link": "http://arxiv.org/abs/2010.10673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Investigation of Contextualized Number Prediction", "abstract": "We conduct a large scale empirical investigation of contextualized number\nprediction in running text. Specifically, we consider two tasks: (1)masked\nnumber prediction-predicting a missing numerical value within a sentence, and\n(2)numerical anomaly detection-detecting an errorful numeric value within a\nsentence. We experiment with novel combinations of contextual encoders and\noutput distributions over the real number line. Specifically, we introduce a\nsuite of output distribution parameterizations that incorporate latent\nvariables to add expressivity and better fit the natural distribution of\nnumeric values in running text, and combine them with both recurrent and\ntransformer-based encoder architectures. We evaluate these models on two\nnumeric datasets in the financial and scientific domain. Our findings show that\noutput distributions that incorporate discrete latent variables and allow for\nmultiple modes outperform simple flow-based counterparts on all datasets,\nyielding more accurate numerical prediction and anomaly detection. We also show\nthat our models effectively utilize textual con-text and benefit from\ngeneral-purpose unsupervised pretraining.", "published": "2020-10-20 23:12:23", "link": "http://arxiv.org/abs/2011.07961v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Performance of Transfer Learning Model vs. Traditional Neural Network in\n  Low System Resource Environment", "abstract": "Recently, the use of pre-trained model to build neural network based on\ntransfer learning methodology is increasingly popular. These pre-trained models\npresent the benefit of using less computing resources to train model with\nsmaller amount of training data. The rise of state-of-the-art models such as\nBERT, XLNet and GPT boost accuracy and benefit as a base model for transfer\nleanring. However, these models are still too complex and consume many\ncomputing resource to train for transfer learning with low GPU memory. We will\ncompare the performance and cost between lighter transfer learning model and\npurposely built neural network for NLP application of text classification and\nNER model.", "published": "2020-10-20 08:12:56", "link": "http://arxiv.org/abs/2011.07962v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Shape Matters: Robust Machine Translation with Visual Embedding", "abstract": "Neural machine translation has achieved remarkable empirical performance over\nstandard benchmark datasets, yet recent evidence suggests that the models can\nstill fail easily dealing with substandard inputs such as misspelled words, To\novercome this issue, we introduce a new encoding heuristic of the input symbols\nfor character-level NLP models: it encodes the shape of each character through\nthe images depicting the letters when printed. We name this new strategy visual\nembedding and it is expected to improve the robustness of NLP models because\nhumans also process the corpus visually through printed letters, instead of\nmachinery one-hot vectors. Empirically, our method improves models' robustness\nagainst substandard inputs, even in the test scenario where the models are\ntested with the noises that are beyond what is available during the training\nphase.", "published": "2020-10-20 04:08:03", "link": "http://arxiv.org/abs/2010.09997v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Looking for Clues of Language in Multilingual BERT to Improve\n  Cross-lingual Generalization", "abstract": "Token embeddings in multilingual BERT (m-BERT) contain both language and\nsemantic information. We find that the representation of a language can be\nobtained by simply averaging the embeddings of the tokens of the language.\nGiven this language representation, we control the output languages of\nmultilingual BERT by manipulating the token embeddings, thus achieving\nunsupervised token translation. We further propose a computationally cheap but\neffective approach to improve the cross-lingual ability of m-BERT based on this\nobservation.", "published": "2020-10-20 05:41:35", "link": "http://arxiv.org/abs/2010.10041v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fluent and Low-latency Simultaneous Speech-to-Speech Translation with\n  Self-adaptive Training", "abstract": "Simultaneous speech-to-speech translation is widely useful but extremely\nchallenging, since it needs to generate target-language speech concurrently\nwith the source-language speech, with only a few seconds delay. In addition, it\nneeds to continuously translate a stream of sentences, but all recent solutions\nmerely focus on the single-sentence scenario. As a result, current approaches\naccumulate latencies progressively when the speaker talks faster, and introduce\nunnatural pauses when the speaker talks slower. To overcome these issues, we\npropose Self-Adaptive Translation (SAT) which flexibly adjusts the length of\ntranslations to accommodate different source speech rates. At similar levels of\ntranslation quality (as measured by BLEU), our method generates more fluent\ntarget speech (as measured by the naturalness metric MOS) with substantially\nlower latency than the baseline, in both Zh <-> En directions.", "published": "2020-10-20 06:02:15", "link": "http://arxiv.org/abs/2010.10048v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting Procedural Knowledge from Technical Documents", "abstract": "Procedures are an important knowledge component of documents that can be\nleveraged by cognitive assistants for automation, question-answering or driving\na conversation. It is a challenging problem to parse big dense documents like\nproduct manuals, user guides to automatically understand which parts are\ntalking about procedures and subsequently extract them. Most of the existing\nresearch has focused on extracting flows in given procedures or understanding\nthe procedures in order to answer conceptual questions. Identifying and\nextracting multiple procedures automatically from documents of diverse formats\nremains a relatively less addressed problem. In this work, we cover some of\nthis ground by -- 1) Providing insights on how structural and linguistic\nproperties of documents can be grouped to define types of procedures, 2)\nAnalyzing documents to extract the relevant linguistic and structural\nproperties, and 3) Formulating procedure identification as a classification\nproblem that leverages the features of the document derived from the above\nanalysis. We first implemented and deployed unsupervised techniques which were\nused in different use cases. Based on the evaluation in different use cases, we\nfigured out the weaknesses of the unsupervised approach. We then designed an\nimproved version which was supervised. We demonstrate that our technique is\neffective in identifying procedures from big and complex documents alike by\nachieving accuracy of 89%.", "published": "2020-10-20 09:47:52", "link": "http://arxiv.org/abs/2010.10156v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Individual corpora predict fast memory retrieval during reading", "abstract": "The corpus, from which a predictive language model is trained, can be\nconsidered the experience of a semantic system. We recorded everyday reading of\ntwo participants for two months on a tablet, generating individual corpus\nsamples of 300/500K tokens. Then we trained word2vec models from individual\ncorpora and a 70 million-sentence newspaper corpus to obtain individual and\nnorm-based long-term memory structure. To test whether individual corpora can\nmake better predictions for a cognitive task of long-term memory retrieval, we\ngenerated stimulus materials consisting of 134 sentences with uncorrelated\nindividual and norm-based word probabilities. For the subsequent eye tracking\nstudy 1-2 months later, our regression analyses revealed that individual, but\nnot norm-corpus-based word probabilities can account for first-fixation\nduration and first-pass gaze duration. Word length additionally affected gaze\nduration and total viewing duration. The results suggest that corpora\nrepresentative for an individual's longterm memory structure can better explain\nreading performance than a norm corpus, and that recently acquired information\nis lexically accessed rapidly.", "published": "2020-10-20 10:18:20", "link": "http://arxiv.org/abs/2010.10176v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Simulated Chats for Building Dialog Systems: Learning to Generate\n  Conversations from Instructions", "abstract": "Popular dialog datasets such as MultiWOZ are created by providing crowd\nworkers an instruction, expressed in natural language, that describes the task\nto be accomplished. Crowd workers play the role of a user and an agent to\ngenerate dialogs to accomplish tasks involving booking restaurant tables,\ncalling a taxi etc. In this paper, we present a data creation strategy that\nuses the pre-trained language model, GPT2, to simulate the interaction between\ncrowd workers by creating a user bot and an agent bot. We train the simulators\nusing a smaller percentage of actual crowd-generated conversations and their\ncorresponding instructions. We demonstrate that by using the simulated data, we\nachieve significant improvements in low-resource settings on two publicly\navailable datasets - the MultiWOZ dataset and the Persona chat dataset.", "published": "2020-10-20 12:04:19", "link": "http://arxiv.org/abs/2010.10216v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Complete Multilingual Neural Machine Translation", "abstract": "Multilingual Neural Machine Translation (MNMT) models are commonly trained on\na joint set of bilingual corpora which is acutely English-centric (i.e. English\neither as the source or target language). While direct data between two\nlanguages that are non-English is explicitly available at times, its use is not\ncommon. In this paper, we first take a step back and look at the commonly used\nbilingual corpora (WMT), and resurface the existence and importance of implicit\nstructure that existed in it: multi-way alignment across examples (the same\nsentence in more than two languages). We set out to study the use of multi-way\naligned examples to enrich the original English-centric parallel corpora. We\nreintroduce this direct parallel data from multi-way aligned corpora between\nall source and target languages. By doing so, the English-centric graph expands\ninto a complete graph, every language pair being connected. We call MNMT with\nsuch connectivity pattern complete Multilingual Neural Machine Translation\n(cMNMT) and demonstrate its utility and efficacy with a series of experiments\nand analysis. In combination with a novel training data sampling strategy that\nis conditioned on the target language only, cMNMT yields competitive\ntranslation quality for all language pairs. We further study the size effect of\nmulti-way aligned data, its transfer learning capabilities and how it eases\nadding a new language in MNMT. Finally, we stress test cMNMT at scale and\ndemonstrate that we can train a cMNMT model with up to 111*112=12,432 language\npairs that provides competitive translation quality for all language pairs.", "published": "2020-10-20 13:03:48", "link": "http://arxiv.org/abs/2010.10239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Human-Paraphrased References Improve Neural Machine Translation", "abstract": "Automatic evaluation comparing candidate translations to human-generated\nparaphrases of reference translations has recently been proposed by Freitag et\nal. When used in place of original references, the paraphrased versions produce\nmetric scores that correlate better with human judgment. This effect holds for\na variety of different automatic metrics, and tends to favor natural\nformulations over more literal (translationese) ones. In this paper we compare\nthe results of performing end-to-end system development using standard and\nparaphrased references. With state-of-the-art English-German NMT components, we\nshow that tuning to paraphrased references produces a system that is\nsignificantly better according to human judgment, but 5 BLEU points worse when\ntested on standard references. Our work confirms the finding that paraphrased\nreferences yield metric scores that correlate better with human judgment, and\ndemonstrates for the first time that using these scores for system development\ncan lead to significant improvements.", "published": "2020-10-20 13:14:57", "link": "http://arxiv.org/abs/2010.10245v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Classification of Manifestos and COVID-19 Press Briefings using\n  BERT and Convolutional Neural Networks", "abstract": "We build a sentence-level political discourse classifier using existing human\nexpert annotated corpora of political manifestos from the Manifestos Project\n(Volkens et al., 2020a) and applying them to a corpus ofCOVID-19Press Briefings\n(Chatsiou, 2020). We use manually annotated political manifestos as training\ndata to train a local topic ConvolutionalNeural Network (CNN) classifier; then\napply it to the COVID-19PressBriefings Corpus to automatically classify\nsentences in the test corpus.We report on a series of experiments with CNN\ntrained on top of pre-trained embeddings for sentence-level classification\ntasks. We show thatCNN combined with transformers like BERT outperforms CNN\ncombined with other embeddings (Word2Vec, Glove, ELMo) and that it is possible\nto use a pre-trained classifier to conduct automatic classification on\ndifferent political texts without additional training.", "published": "2020-10-20 13:39:58", "link": "http://arxiv.org/abs/2010.10267v2", "categories": ["cs.CL", "cs.LG", "68T07, 91F10", "J.4; J.5"], "primary_category": "cs.CL"}
{"title": "CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for\n  Conversational Recommendation", "abstract": "Growing interests have been attracted in Conversational Recommender Systems\n(CRS), which explore user preference through conversational interactions in\norder to make appropriate recommendation. However, there is still a lack of\nability in existing CRS to (1) traverse multiple reasoning paths over\nbackground knowledge to introduce relevant items and attributes, and (2)\narrange selected entities appropriately under current system intents to control\nresponse generation. To address these issues, we propose CR-Walker in this\npaper, a model that performs tree-structured reasoning on a knowledge graph,\nand generates informative dialog acts to guide language generation. The unique\nscheme of tree-structured reasoning views the traversed entity at each hop as\npart of dialog acts to facilitate language generation, which links how entities\nare selected and expressed. Automatic and human evaluations show that CR-Walker\ncan arrive at more accurate recommendation, and generate more informative and\nengaging responses.", "published": "2020-10-20 14:53:22", "link": "http://arxiv.org/abs/2010.10333v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Lease Contract Review", "abstract": "Extracting entities and other useful information from legal contracts is an\nimportant task whose automation can help legal professionals perform contract\nreviews more efficiently and reduce relevant risks. In this paper, we tackle\nthe problem of detecting two different types of elements that play an important\nrole in a contract review, namely entities and red flags. The latter are terms\nor sentences that indicate that there is some danger or other potentially\nproblematic situation for one or more of the signing parties. We focus on\nsupporting the review of lease agreements, a contract type that has received\nlittle attention in the legal information extraction literature, and we define\nthe types of entities and red flags needed for that task. We release a new\nbenchmark dataset of 179 lease agreement documents that we have manually\nannotated with the entities and red flags they contain, and which can be used\nto train and test relevant extraction algorithms. Finally, we release a new\nlanguage model, called ALeaseBERT, pre-trained on this dataset and fine-tuned\nfor the detection of the aforementioned elements, providing a baseline for\nfurther research", "published": "2020-10-20 15:50:50", "link": "http://arxiv.org/abs/2010.10386v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ConjNLI: Natural Language Inference Over Conjunctive Sentences", "abstract": "Reasoning about conjuncts in conjunctive sentences is important for a deeper\nunderstanding of conjunctions in English and also how their usages and\nsemantics differ from conjunctive and disjunctive boolean logic. Existing NLI\nstress tests do not consider non-boolean usages of conjunctions and use\ntemplates for testing such model knowledge. Hence, we introduce ConjNLI, a\nchallenge stress-test for natural language inference over conjunctive\nsentences, where the premise differs from the hypothesis by conjuncts removed,\nadded, or replaced. These sentences contain single and multiple instances of\ncoordinating conjunctions (\"and\", \"or\", \"but\", \"nor\") with quantifiers,\nnegations, and requiring diverse boolean and non-boolean inferences over\nconjuncts. We find that large-scale pre-trained language models like RoBERTa do\nnot understand conjunctive semantics well and resort to shallow heuristics to\nmake inferences over such sentences. As some initial solutions, we first\npresent an iterative adversarial fine-tuning method that uses synthetically\ncreated training data based on boolean and non-boolean heuristics. We also\npropose a direct model advancement by making RoBERTa aware of predicate\nsemantic roles. While we observe some performance gains, ConjNLI is still\nchallenging for current methods, thus encouraging interesting future work for\nbetter understanding of conjunctions. Our data and code are publicly available\nat: https://github.com/swarnaHub/ConjNLI", "published": "2020-10-20 16:29:13", "link": "http://arxiv.org/abs/2010.10418v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open Question Answering over Tables and Text", "abstract": "In open question answering (QA), the answer to a question is produced by\nretrieving and then analyzing documents that might contain answers to the\nquestion. Most open QA systems have considered only retrieving information from\nunstructured text. Here we consider for the first time open QA over both\ntabular and textual data and present a new large-scale dataset Open\nTable-and-Text Question Answering (OTT-QA) to evaluate performance on this\ntask. Most questions in OTT-QA require multi-hop inference across tabular data\nand unstructured text, and the evidence required to answer a question can be\ndistributed in different ways over these two types of input, making evidence\nretrieval challenging -- our baseline model using an iterative retriever and\nBERT-based reader achieves an exact match score less than 10%. We then propose\ntwo novel techniques to address the challenge of retrieving and aggregating\nevidence for OTT-QA. The first technique is to use \"early fusion\" to group\nmultiple highly relevant tabular and textual units into a fused block, which\nprovides more context for the retriever to search for. The second technique is\nto use a cross-block reader to model the cross-dependency between multiple\nretrieved evidence with global-local sparse attention. Combining these two\ntechniques improves the score significantly, to above 27%.", "published": "2020-10-20 16:48:14", "link": "http://arxiv.org/abs/2010.10439v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Content and Context with Deep Relational Learning", "abstract": "Building models for realistic natural language tasks requires dealing with\nlong texts and accounting for complicated structural dependencies.\nNeural-symbolic representations have emerged as a way to combine the reasoning\ncapabilities of symbolic methods, with the expressiveness of neural networks.\nHowever, most of the existing frameworks for combining neural and symbolic\nrepresentations have been designed for classic relational learning tasks that\nwork over a universe of symbolic entities and relations. In this paper, we\npresent DRaiL, an open-source declarative framework for specifying deep\nrelational models, designed to support a variety of NLP scenarios. Our\nframework supports easy integration with expressive language encoders, and\nprovides an interface to study the interactions between representation,\ninference and learning.", "published": "2020-10-20 17:09:35", "link": "http://arxiv.org/abs/2010.10453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimal Subarchitecture Extraction For BERT", "abstract": "We extract an optimal subset of architectural parameters for the BERT\narchitecture from Devlin et al. (2018) by applying recent breakthroughs in\nalgorithms for neural architecture search. This optimal subset, which we refer\nto as \"Bort\", is demonstrably smaller, having an effective (that is, not\ncounting the embedding layer) size of $5.5\\%$ the original BERT-large\narchitecture, and $16\\%$ of the net size. Bort is also able to be pretrained in\n$288$ GPU hours, which is $1.2\\%$ of the time required to pretrain the\nhighest-performing BERT parametric architectural variant, RoBERTa-large (Liu et\nal., 2019), and about $33\\%$ of that of the world-record, in GPU hours,\nrequired to train BERT-large on the same hardware. It is also $7.9$x faster on\na CPU, as well as being better performing than other compressed variants of the\narchitecture, and some of the non-compressed variants: it obtains performance\nimprovements of between $0.3\\%$ and $31\\%$, absolute, with respect to\nBERT-large, on multiple public natural language understanding (NLU) benchmarks.", "published": "2020-10-20 17:53:01", "link": "http://arxiv.org/abs/2010.10499v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SKATE: A Natural Language Interface for Encoding Structured Knowledge", "abstract": "In Natural Language (NL) applications, there is often a mismatch between what\nthe NL interface is capable of interpreting and what a lay user knows how to\nexpress. This work describes a novel natural language interface that reduces\nthis mismatch by refining natural language input through successive,\nautomatically generated semi-structured templates. In this paper we describe\nhow our approach, called SKATE, uses a neural semantic parser to parse NL input\nand suggest semi-structured templates, which are recursively filled to produce\nfully structured interpretations. We also show how SKATE integrates with a\nneural rule-generation model to interactively suggest and acquire commonsense\nknowledge. We provide a preliminary coverage analysis of SKATE for the task of\nstory understanding, and then describe a current business use-case of the tool\nin a specific domain: COVID-19 policy design.", "published": "2020-10-20 20:13:09", "link": "http://arxiv.org/abs/2010.10597v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "What makes multilingual BERT multilingual?", "abstract": "Recently, multilingual BERT works remarkably well on cross-lingual transfer\ntasks, superior to static non-contextualized word embeddings. In this work, we\nprovide an in-depth experimental study to supplement the existing literature of\ncross-lingual ability. We compare the cross-lingual ability of\nnon-contextualized and contextualized representation model with the same data.\nWe found that datasize and context window size are crucial factors to the\ntransferability.", "published": "2020-10-20 05:41:56", "link": "http://arxiv.org/abs/2010.10938v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Keyphrase Extraction from Microblogs using Human Reading Time", "abstract": "The premise of manual keyphrase annotation is to read the corresponding\ncontent of an annotated object. Intuitively, when we read, more important words\nwill occupy a longer reading time. Hence, by leveraging human reading time, we\ncan find the salient words in the corresponding content. However, previous\nstudies on keyphrase extraction ignore human reading features. In this article,\nwe aim to leverage human reading time to extract keyphrases from microblog\nposts. There are two main tasks in this study. One is to determine how to\nmeasure the time spent by a human on reading a word. We use eye fixation\ndurations extracted from an open source eye-tracking corpus (OSEC). Moreover,\nwe propose strategies to make eye fixation duration more effective on keyphrase\nextraction. The other task is to determine how to integrate human reading time\ninto keyphrase extraction models. We propose two novel neural network models.\nThe first is a model in which the human reading time is used as the ground\ntruth of the attention mechanism. In the second model, we use human reading\ntime as the external feature. Quantitative and qualitative experiments show\nthat our proposed models yield better performance than the baseline models on\ntwo microblog datasets.", "published": "2020-10-20 00:18:44", "link": "http://arxiv.org/abs/2010.09934v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Dialog Systems for Negotiation with Personality Modeling", "abstract": "In this paper, we explore the ability to model and infer personality types of\nopponents, predict their responses, and use this information to adapt a dialog\nagent's high-level strategy in negotiation tasks. Inspired by the idea of\nincorporating a theory of mind (ToM) into machines, we introduce a\nprobabilistic formulation to encapsulate the opponent's personality type during\nboth learning and inference. We test our approach on the CraigslistBargain\ndataset and show that our method using ToM inference achieves a 20% higher\ndialog agreement rate compared to baselines on a mixed population of opponents.\nWe also find that our model displays diverse negotiation behavior with\ndifferent types of opponents.", "published": "2020-10-20 01:46:03", "link": "http://arxiv.org/abs/2010.09954v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SOrT-ing VQA Models : Contrastive Gradient Learning for Improved\n  Consistency", "abstract": "Recent research in Visual Question Answering (VQA) has revealed\nstate-of-the-art models to be inconsistent in their understanding of the world\n-- they answer seemingly difficult questions requiring reasoning correctly but\nget simpler associated sub-questions wrong. These sub-questions pertain to\nlower level visual concepts in the image that models ideally should understand\nto be able to answer the higher level question correctly. To address this, we\nfirst present a gradient-based interpretability approach to determine the\nquestions most strongly correlated with the reasoning question on an image, and\nuse this to evaluate VQA models on their ability to identify the relevant\nsub-questions needed to answer a reasoning question. Next, we propose a\ncontrastive gradient learning based approach called Sub-question Oriented\nTuning (SOrT) which encourages models to rank relevant sub-questions higher\nthan irrelevant questions for an <image, reasoning-question> pair. We show that\nSOrT improves model consistency by upto 6.5% points over existing baselines,\nwhile also improving visual grounding.", "published": "2020-10-20 05:15:48", "link": "http://arxiv.org/abs/2010.10038v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded\n  Dialogues", "abstract": "Video-grounded dialogues are very challenging due to (i) the complexity of\nvideos which contain both spatial and temporal variations, and (ii) the\ncomplexity of user utterances which query different segments and/or different\nobjects in videos over multiple dialogue turns. However, existing approaches to\nvideo-grounded dialogues often focus on superficial temporal-level visual cues,\nbut neglect more fine-grained spatial signals from videos. To address this\ndrawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a\nvision-language neural framework for high-resolution queries in videos based on\ntextual cues. Specifically, our approach not only exploits both spatial and\ntemporal-level information, but also learns dynamic information diffusion\nbetween the two feature spaces through spatial-to-temporal and\ntemporal-to-spatial reasoning. The bidirectional strategy aims to tackle the\nevolving semantics of user queries in the dialogue setting. The retrieved\nvisual cues are used as contextual information to construct relevant responses\nto the users. Our empirical results and comprehensive qualitative analysis show\nthat BiST achieves competitive performance and generates reasonable responses\non a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA\nsetting, and substantially outperform prior approaches on the TGIF-QA\nbenchmark.", "published": "2020-10-20 07:43:00", "link": "http://arxiv.org/abs/2010.10095v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Local Knowledge Powered Conversational Agents", "abstract": "State-of-the-art conversational agents have advanced significantly in\nconjunction with the use of large transformer-based language models. However,\neven with these advancements, conversational agents still lack the ability to\nproduce responses that are informative and coherent with the local context. In\nthis work, we propose a dialog framework that incorporates both local knowledge\nas well as users' past dialogues to generate high quality conversations. We\nintroduce an approach to build a dataset based on Reddit conversations, where\noutbound URL links are widely available in the conversations and the\nhyperlinked documents can be naturally included as local external knowledge.\nUsing our framework and dataset, we demonstrate that incorporating local\nknowledge can largely improve informativeness, coherency and realisticness\nmeasures using human evaluations. In particular, our approach consistently\noutperforms the state-of-the-art conversational model on the Reddit dataset\nacross all three measures. We also find that scaling the size of our models\nfrom 117M to 8.3B parameters yields consistent improvement of validation\nperplexity as well as human evaluated metrics. Our model with 8.3B parameters\ncan generate human-like responses as rated by various human evaluations in a\nsingle-turn dialog setting.", "published": "2020-10-20 09:34:40", "link": "http://arxiv.org/abs/2010.10150v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Replacing Human Audio with Synthetic Audio for On-device Unspoken\n  Punctuation Prediction", "abstract": "We present a novel multi-modal unspoken punctuation prediction system for the\nEnglish language which combines acoustic and text features. We demonstrate for\nthe first time, that by relying exclusively on synthetic data generated using a\nprosody-aware text-to-speech system, we can outperform a model trained with\nexpensive human audio recordings on the unspoken punctuation prediction\nproblem. Our model architecture is well suited for on-device use. This is\nachieved by leveraging hash-based embeddings of automatic speech recognition\ntext output in conjunction with acoustic features as input to a quasi-recurrent\nneural network, keeping the model size small and latency low.", "published": "2020-10-20 11:30:26", "link": "http://arxiv.org/abs/2010.10203v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CoRT: Complementary Rankings from Transformers", "abstract": "Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.", "published": "2020-10-20 13:28:27", "link": "http://arxiv.org/abs/2010.10252v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "68P20", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "Bootleg: Chasing the Tail with Self-Supervised Named Entity\n  Disambiguation", "abstract": "A challenge for named entity disambiguation (NED), the task of mapping\ntextual mentions to entities in a knowledge base, is how to disambiguate\nentities that appear rarely in the training data, termed tail entities. Humans\nuse subtle reasoning patterns based on knowledge of entity facts, relations,\nand types to disambiguate unfamiliar entities. Inspired by these patterns, we\nintroduce Bootleg, a self-supervised NED system that is explicitly grounded in\nreasoning patterns for disambiguation. We define core reasoning patterns for\ndisambiguation, create a learning procedure to encourage the self-supervised\nmodel to learn the patterns, and show how to use weak supervision to enhance\nthe signals in the training data. Encoding the reasoning patterns in a simple\nTransformer architecture, Bootleg meets or exceeds state-of-the-art on three\nNED benchmarks. We further show that the learned representations from Bootleg\nsuccessfully transfer to other non-disambiguation tasks that require\nentity-based knowledge: we set a new state-of-the-art in the popular TACRED\nrelation extraction task by 1.0 F1 points and demonstrate up to 8% performance\nlift in highly optimized production search and assistant tasks at a major\ntechnology company", "published": "2020-10-20 15:17:49", "link": "http://arxiv.org/abs/2010.10363v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual\n  Embeddings Using the Unified Medical Language System Metathesaurus", "abstract": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n  In this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.", "published": "2020-10-20 15:56:31", "link": "http://arxiv.org/abs/2010.10391v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Separation Using Speaker Inventories and Estimated Speech", "abstract": "We propose speaker separation using speaker inventories and estimated speech\n(SSUSIES), a framework leveraging speaker profiles and estimated speech for\nspeaker separation. SSUSIES contains two methods, speaker separation using\nspeaker inventories (SSUSI) and speaker separation using estimated speech\n(SSUES). SSUSI performs speaker separation with the help of speaker inventory.\nBy combining the advantages of permutation invariant training (PIT) and speech\nextraction, SSUSI significantly outperforms conventional approaches. SSUES is a\nwidely applicable technique that can substantially improve speaker separation\nperformance using the output of first-pass separation. We evaluate the models\non both speaker separation and speech recognition metrics.", "published": "2020-10-20 18:15:45", "link": "http://arxiv.org/abs/2010.10556v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey on Deep Learning and Explainability for Automatic Report\n  Generation from Medical Images", "abstract": "Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.", "published": "2020-10-20 18:48:37", "link": "http://arxiv.org/abs/2010.10563v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards End-to-End In-Image Neural Machine Translation", "abstract": "In this paper, we offer a preliminary investigation into the task of in-image\nmachine translation: transforming an image containing text in one language into\nan image containing the same text in another language. We propose an end-to-end\nneural model for this task inspired by recent approaches to neural machine\ntranslation, and demonstrate promising initial results based purely on\npixel-level supervision. We then offer a quantitative and qualitative\nevaluation of our system outputs and discuss some common failure modes.\nFinally, we conclude with directions for future work.", "published": "2020-10-20 22:20:04", "link": "http://arxiv.org/abs/2010.10648v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting convolutional networks trained on textual data", "abstract": "There have been many advances in the artificial intelligence field due to the\nemergence of deep learning. In almost all sub-fields, artificial neural\nnetworks have reached or exceeded human-level performance. However, most of the\nmodels are not interpretable. As a result, it is hard to trust their decisions,\nespecially in life and death scenarios. In recent years, there has been a\nmovement toward creating explainable artificial intelligence, but most work to\ndate has concentrated on image processing models, as it is easier for humans to\nperceive visual patterns. There has been little work in other fields like\nnatural language processing. In this paper, we train a convolutional model on\ntextual data and analyze the global logic of the model by studying its filter\nvalues. In the end, we find the most important words in our corpus to our\nmodels logic and remove the rest (95%). New models trained on just the 5% most\nimportant words can achieve the same performance as the original model while\nreducing training time by more than half. Approaches such as this will help us\nto understand NLP models, explain their decisions according to their word\nchoices, and improve them by finding blind spots and biases.", "published": "2020-10-20 20:12:05", "link": "http://arxiv.org/abs/2010.13585v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phase recovery with Bregman divergences for audio source separation", "abstract": "Time-frequency audio source separation is usually achieved by estimating the\nshort-time Fourier transform (STFT) magnitude of each source, and then applying\na phase recovery algorithm to retrieve time-domain signals. In particular, the\nmultiple input spectrogram inversion (MISI) algorithm has shown good\nperformance in several recent works. This algorithm minimizes a quadratic\nreconstruction error between magnitude spectrograms. However, this loss does\nnot properly account for some perceptual properties of audio, and alternative\ndiscrepancy measures such as beta-divergences have been preferred in many\nsettings. In this paper, we propose to reformulate phase recovery in audio\nsource separation as a minimization problem involving Bregman divergences. To\noptimize the resulting objective, we derive a projected gradient descent\nalgorithm. Experiments conducted on a speech enhancement task show that this\napproach outperforms MISI for several alternative losses, which highlights\ntheir relevance for audio source separation applications.", "published": "2020-10-20 13:30:38", "link": "http://arxiv.org/abs/2010.10255v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic multitrack mixing with a differentiable mixing console of\n  neural audio effects", "abstract": "Applications of deep learning to automatic multitrack mixing are largely\nunexplored. This is partly due to the limited available data, coupled with the\nfact that such data is relatively unstructured and variable. To address these\nchallenges, we propose a domain-inspired model with a strong inductive bias for\nthe mixing task. We achieve this with the application of pre-trained\nsub-networks and weight sharing, as well as with a sum/difference stereo loss\nfunction. The proposed model can be trained with a limited number of examples,\nis permutation invariant with respect to the input ordering, and places no\nlimit on the number of input sources. Furthermore, it produces human-readable\nmixing parameters, allowing users to manually adjust or refine the generated\nmix. Results from a perceptual evaluation involving audio engineers indicate\nthat our approach generates mixes that outperform baseline approaches. To the\nbest of our knowledge, this work demonstrates the first approach in learning\nmultitrack mixing conventions from real-world data at the waveform level,\nwithout knowledge of the underlying mixing parameters.", "published": "2020-10-20 14:04:22", "link": "http://arxiv.org/abs/2010.10291v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution", "abstract": "Keyword Spotting (KWS) plays a vital role in human-computer interaction for\nsmart on-device terminals and service robots. It remains challenging to achieve\nthe trade-off between small footprint and high accuracy for KWS task. In this\npaper, we explore the application of multi-scale temporal modeling to the\nsmall-footprint keyword spotting task. We propose a multi-branch temporal\nconvolution module (MTConv), a CNN block consisting of multiple temporal\nconvolution filters with different kernel sizes, which enriches temporal\nfeature space. Besides, taking advantage of temporal and depthwise convolution,\na temporal efficient neural network (TENet) is designed for KWS system. Based\non the purposed model, we replace standard temporal convolution layers with\nMTConvs that can be trained for better performance. While at the inference\nstage, the MTConv can be equivalently converted to the base convolution\narchitecture, so that no extra parameters and computational costs are added\ncompared to the base model. The results on Google Speech Command Dataset show\nthat one of our models trained with MTConv performs the accuracy of 96.8% with\nonly 100K parameters.", "published": "2020-10-20 02:07:07", "link": "http://arxiv.org/abs/2010.09960v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Effect of Spectrogram Reconstruction on Automatic Music\n  Transcription: An Alternative Approach to Improve Transcription Accuracy", "abstract": "Most of the state-of-the-art automatic music transcription (AMT) models break\ndown the main transcription task into sub-tasks such as onset prediction and\noffset prediction and train them with onset and offset labels. These\npredictions are then concatenated together and used as the input to train\nanother model with the pitch labels to obtain the final transcription. We\nattempt to use only the pitch labels (together with spectrogram reconstruction\nloss) and explore how far this model can go without introducing supervised\nsub-tasks. In this paper, we do not aim at achieving state-of-the-art\ntranscription accuracy, instead, we explore the effect that spectrogram\nreconstruction has on our AMT model. Our proposed model consists of two U-nets:\nthe first U-net transcribes the spectrogram into a posteriorgram, and a second\nU-net transforms the posteriorgram back into a spectrogram. A reconstruction\nloss is applied between the original spectrogram and the reconstructed\nspectrogram to constrain the second U-net to focus only on reconstruction. We\ntrain our model on three different datasets: MAPS, MAESTRO, and MusicNet. Our\nexperiments show that adding the reconstruction loss can generally improve the\nnote-level transcription accuracy when compared to the same model without the\nreconstruction part. Moreover, it can also boost the frame-level precision to\nbe higher than the state-of-the-art models. The feature maps learned by our\nU-net contain gridlike structures (not present in the baseline model) which\nimplies that with the presence of the reconstruction loss, the model is\nprobably trying to count along both the time and frequency axis, resulting in a\nhigher note-level transcription accuracy.", "published": "2020-10-20 02:37:39", "link": "http://arxiv.org/abs/2010.09969v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Power pooling: An adaptive pooling function for weakly labelled sound\n  event detection", "abstract": "Access to large corpora with strongly labelled sound events is expensive and\ndifficult in engineering applications. Much research turns to address the\nproblem of how to detect both the types and the timestamps of sound events with\nweak labels that only specify the types. This task can be treated as a multiple\ninstance learning (MIL) problem, and the key to it is the design of a pooling\nfunction. In this paper, we propose an adaptive power pooling function which\ncan automatically adapt to various sound sources. On two public datasets, the\nproposed power pooling function outperforms the state-of-the-art linear softmax\npooling on both coarsegrained and fine-grained metrics. Notably, it improves\nthe event-based F1 score (which evaluates the detection of event onsets and\noffsets) by 11.4% and 10.2% relative on the two datasets. While this paper\nfocuses on sound event detection applications, the proposed method can be\napplied to MIL tasks in other domains.", "published": "2020-10-20 03:14:14", "link": "http://arxiv.org/abs/2010.09985v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tongji University Undergraduate Team for the VoxCeleb Speaker\n  Recognition Challenge2020", "abstract": "In this report, we discribe the submission of Tongji University undergraduate\nteam to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC)\n2020 at Interspeech 2020. We applied the RSBU-CW module to the ResNet34\nframework to improve the denoising ability of the network and better complete\nthe speaker verification task in a complex environment.We trained two variants\nof ResNet,used score fusion and data-augmentation methods to improve the\nperformance of the model. Our fusion of two selected systems for the CLOSE\ntrack achieves 0.2973 DCF and 4.9700\\% EER on the challenge evaluation set.", "published": "2020-10-20 09:25:40", "link": "http://arxiv.org/abs/2010.10145v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging the structure of musical preference in content-aware music\n  recommendation", "abstract": "State-of-the-art music recommendation systems are based on collaborative\nfiltering, which predicts a user's interest from his listening habits and\nsimilarities with other users' profiles. These approaches are agnostic to the\nsong content, and therefore face the cold-start problem: they cannot recommend\nnovel songs without listening history. To tackle this issue, content-aware\nrecommendation incorporates information about the songs that can be used for\nrecommending new items. Most methods falling in this category exploit either\nuser-annotated tags, acoustic features or deeply-learned features.\nConsequently, these content features do not have a clear musical meaning, thus\nthey are not necessarily relevant from a musical preference perspective. In\nthis work, we propose instead to leverage a model of musical preference which\noriginates from the field of music psychology. From low-level acoustic features\nwe extract three factors (arousal, valence and depth), which have been shown\nappropriate for describing musical taste. Then we integrate those into a\ncollaborative filtering framework for content-aware music recommendation.\nExperiments conducted on large-scale data show that this approach is able to\naddress the cold-start problem, while using a compact and meaningful set of\nmusical features.", "published": "2020-10-20 13:46:08", "link": "http://arxiv.org/abs/2010.10276v2", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Investigating Cross-Domain Losses for Speech Enhancement", "abstract": "Recent years have seen a surge in the number of available frameworks for\nspeech enhancement (SE) and recognition. Whether model-based or constructed via\ndeep learning, these frameworks often rely in isolation on either time-domain\nsignals or time-frequency (TF) representations of speech data. In this study,\nwe investigate the advantages of each set of approaches by separately examining\ntheir impact on speech intelligibility and quality. Furthermore, we combine the\nfragmented benefits of time-domain and TF speech representations by introducing\ntwo new cross-domain SE frameworks. A quantitative comparative analysis against\nrecent model-based and deep learning SE approaches is performed to illustrate\nthe merit of the proposed frameworks.", "published": "2020-10-20 17:28:07", "link": "http://arxiv.org/abs/2010.10468v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech\n  Recognition", "abstract": "We employ a combination of recent developments in semi-supervised learning\nfor automatic speech recognition to obtain state-of-the-art results on\nLibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More\nprecisely, we carry out noisy student training with SpecAugment using giant\nConformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we\nare able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech\ntest/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.", "published": "2020-10-20 17:58:13", "link": "http://arxiv.org/abs/2010.10504v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Transfer for Efficient On-device False Trigger Mitigation", "abstract": "In this paper, we address the task of determining whether a given utterance\nis directed towards a voice-enabled smart-assistant device or not. An\nundirected utterance is termed as a \"false trigger\" and false trigger\nmitigation (FTM) is essential for designing a privacy-centric non-intrusive\nsmart assistant. The directedness of an utterance can be identified by running\nautomatic speech recognition (ASR) on it and determining the user intent by\nanalyzing the ASR transcript. But in case of a false trigger, transcribing the\naudio using ASR itself is strongly undesirable. To alleviate this issue, we\npropose an LSTM-based FTM architecture which determines the user intent from\nacoustic features directly without explicitly generating ASR transcripts from\nthe audio. The proposed models are small footprint and can be run on-device\nwith limited computational resources. During training, the model parameters are\noptimized using a knowledge transfer approach where a more accurate\nself-attention graph neural network model serves as the teacher. Given the\nwhole audio snippets, our approach mitigates 87% of false triggers at 99% true\npositive rate (TPR), and in a streaming audio scenario, the system listens to\nonly 1.69s of the false trigger audio before rejecting it while achieving the\nsame TPR.", "published": "2020-10-20 20:01:44", "link": "http://arxiv.org/abs/2010.10591v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
