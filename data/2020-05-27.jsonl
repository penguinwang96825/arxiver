{"title": "Predict-then-Decide: A Predictive Approach for Wait or Answer Task in\n  Dialogue Systems", "abstract": "Different people have different habits of describing their intents in\nconversations. Some people tend to deliberate their intents in several\nsuccessive utterances, i.e., they use several consistent messages for\nreadability instead of a long sentence to express their question. This creates\na predicament faced by the application of dialogue systems, especially in\nreal-world industry scenarios, in which the dialogue system is unsure whether\nit should answer the query of user immediately or wait for further\nsupplementary input. Motivated by such an interesting predicament, we define a\nnovel Wait-or-Answer task for dialogue systems. We shed light on a new research\ntopic about how the dialogue system can be more intelligent to behave in this\nWait-or-Answer quandary. Further, we propose a predictive approach named\nPredict-then-Decide (PTD) to tackle this Wait-or-Answer task. More\nspecifically, we take advantage of a decision model to help the dialogue system\ndecide whether to wait or answer. The decision of decision model is made with\nthe assistance of two ancillary prediction models: a user prediction and an\nagent prediction. The user prediction model tries to predict what the user\nwould supplement and uses its prediction to persuade the decision model that\nthe user has some information to add, so the dialogue system should wait. The\nagent prediction model tries to predict the answer of the dialogue system and\nconvince the decision model that it is a superior choice to answer the query of\nuser immediately since the input of user has come to an end. We conduct our\nexperiments on two real-life scenarios and three public datasets. Experimental\nresults on five datasets show our proposed PTD approach significantly\noutperforms the existing models in solving this Wait-or-Answer problem.", "published": "2020-05-27 01:48:54", "link": "http://arxiv.org/abs/2005.13119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT-Adapted Datasheets for Datasets: Template and Repository", "abstract": "In this report we are taking the standardized model proposed by Gebru et al.\n(2018) for documenting the popular machine translation datasets of the EuroParl\n(Koehn, 2005) and News-Commentary (Barrault et al., 2019). Within this\ndocumentation process, we have adapted the original datasheet to the particular\ncase of data consumers within the Machine Translation area. We are also\nproposing a repository for collecting the adapted datasheets in this research\narea", "published": "2020-05-27 04:56:08", "link": "http://arxiv.org/abs/2005.13156v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Give Me Convenience and Give Her Death: Who Should Decide What Uses of\n  NLP are Appropriate, and on What Basis?", "abstract": "As part of growing NLP capabilities, coupled with an awareness of the ethical\ndimensions of research, questions have been raised about whether particular\ndatasets and tasks should be deemed off-limits for NLP research. We examine\nthis question with respect to a paper on automatic legal sentencing from EMNLP\n2019 which was a source of some debate, in asking whether the paper should have\nbeen allowed to be published, who should have been charged with making such a\ndecision, and on what basis. We focus in particular on the role of data\nstatements in ethically assessing research, but also discuss the topic of dual\nuse, and examine the outcomes of similar debates in other scientific\ndisciplines.", "published": "2020-05-27 07:31:57", "link": "http://arxiv.org/abs/2005.13213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing a New State-of-the-Art for French Named Entity Recognition", "abstract": "The French TreeBank developed at the University Paris 7 is the main source of\nmorphosyntactic and syntactic annotations for French. However, it does not\ninclude explicit information related to named entities, which are among the\nmost useful information for several natural language processing tasks and\napplications. Moreover, no large-scale French corpus with named entity\nannotations contain referential information, which complement the type and the\nspan of each mention with an indication of the entity it refers to. We have\nmanually annotated the French TreeBank with such information, after an\nautomatic pre-annotation step. We sketch the underlying annotation guidelines\nand we provide a few figures about the resulting annotations.", "published": "2020-05-27 08:44:09", "link": "http://arxiv.org/abs/2005.13236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking, exploring and analyzing recent developments in German-language\n  online press in the face of the coronavirus crisis: cOWIDplus Analysis and\n  cOWIDplus Viewer", "abstract": "The coronavirus pandemic may be the largest crisis the world has had to face\nsince World War II. It does not come as a surprise that it is also having an\nimpact on language as our primary communication tool. We present three\ninter-connected resources that are designed to capture and illustrate these\neffects on a subset of the German language: An RSS corpus of German-language\nnewsfeeds (with freely available untruncated unigram frequency lists), a static\nbut continuously updated HTML page tracking the diversity of the used\nvocabulary and a web application that enables other researchers and the broader\npublic to explore these effects without any or with little knowledge of corpus\nrepresentation/exploration or statistical analyses.", "published": "2020-05-27 12:21:36", "link": "http://arxiv.org/abs/2005.13316v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriched In-Order Linearization for Faster Sequence-to-Sequence\n  Constituent Parsing", "abstract": "Sequence-to-sequence constituent parsing requires a linearization to\nrepresent trees as sequences. Top-down tree linearizations, which can be based\non brackets or shift-reduce actions, have achieved the best accuracy to date.\nIn this paper, we show that these results can be improved by using an in-order\nlinearization instead. Based on this observation, we implement an enriched\nin-order shift-reduce linearization inspired by Vinyals et al. (2015)'s\napproach, achieving the best accuracy to date on the English PTB dataset among\nfully-supervised single-model sequence-to-sequence constituent parsers.\nFinally, we apply deterministic attention mechanisms to match the speed of\nstate-of-the-art transition-based parsers, thus showing that\nsequence-to-sequence models can match them, not only in accuracy, but also in\nspeed.", "published": "2020-05-27 13:01:02", "link": "http://arxiv.org/abs/2005.13334v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Transition-based Semantic Dependency Parsing with Pointer Networks", "abstract": "Transition-based parsers implemented with Pointer Networks have become the\nnew state of the art in dependency parsing, excelling in producing labelled\nsyntactic trees and outperforming graph-based models in this task. In order to\nfurther test the capabilities of these powerful neural networks on a harder NLP\nproblem, we propose a transition system that, thanks to Pointer Networks, can\nstraightforwardly produce labelled directed acyclic graphs and perform semantic\ndependency parsing. In addition, we enhance our approach with deep\ncontextualized word embeddings extracted from BERT. The resulting system not\nonly outperforms all existing transition-based models, but also matches the\nbest fully-supervised accuracy to date on the SemEval 2015 Task 18 English\ndatasets among previous state-of-the-art graph-based parsers.", "published": "2020-05-27 13:18:27", "link": "http://arxiv.org/abs/2005.13344v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Thirty Musts for Meaning Banking", "abstract": "Meaning banking--creating a semantically annotated corpus for the purpose of\nsemantic parsing or generation--is a challenging task. It is quite simple to\ncome up with a complex meaning representation, but it is hard to design a\nsimple meaning representation that captures many nuances of meaning. This paper\nlists some lessons learned in nearly ten years of meaning annotation during the\ndevelopment of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel\nMeaning Bank (Abzianidze et al., 2017). The paper's format is rather\nunconventional: there is no explicit related work, no methodology section, no\nresults, and no discussion (and the current snippet is not an abstract but\nactually an introductory preface). Instead, its structure is inspired by work\nof Traum (2000) and Bender (2013). The list starts with a brief overview of the\nexisting meaning banks (Section 1) and the rest of the items are roughly\ndivided into three groups: corpus collection (Section 2 and 3, annotation\nmethods (Section 4-11), and design of meaning representations (Section 12-30).\nWe hope this overview will give inspiration and guidance in creating improved\nmeaning banks in the future.", "published": "2020-05-27 15:28:12", "link": "http://arxiv.org/abs/2005.13421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Training for Unsupervised Parsing with PRPN", "abstract": "Neural unsupervised parsing (UP) models learn to parse without access to\nsyntactic annotations, while being optimized for another task like language\nmodeling. In this work, we propose self-training for neural UP models: we\nleverage aggregated annotations predicted by copies of our model as supervision\nfor future copies. To be able to use our model's predictions during training,\nwe extend a recent neural UP architecture, the PRPN (Shen et al., 2018a) such\nthat it can be trained in a semi-supervised fashion. We then add examples with\nparses predicted by our model to our unlabeled UP training data. Our\nself-trained model outperforms the PRPN by 8.1% F1 and the previous state of\nthe art by 1.6% F1. In addition, we show that our architecture can also be\nhelpful for semi-supervised parsing in ultra-low-resource settings.", "published": "2020-05-27 16:11:09", "link": "http://arxiv.org/abs/2005.13455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Structure Distillation Pretraining For Bidirectional Encoders", "abstract": "Textual representation learners trained on large amounts of data have\nachieved notable success on downstream tasks; intriguingly, they have also\nperformed well on challenging tests of syntactic competence. Given this\nsuccess, it remains an open question whether scalable learners like BERT can\nbecome fully proficient in the syntax of natural language by virtue of data\nscale alone, or whether they still benefit from more explicit syntactic biases.\nTo answer this question, we introduce a knowledge distillation strategy for\ninjecting syntactic biases into BERT pretraining, by distilling the\nsyntactically informative predictions of a hierarchical---albeit harder to\nscale---syntactic language model. Since BERT models masked words in\nbidirectional context, we propose to distill the approximate marginal\ndistribution over words in context from the syntactic LM. Our approach reduces\nrelative error by 2-21% on a diverse set of structured prediction tasks,\nalthough we obtain mixed results on the GLUE benchmark. Our findings\ndemonstrate the benefits of syntactic biases, even in representation learners\nthat exploit large amounts of data, and contribute to a better understanding of\nwhere syntactic biases are most helpful in benchmarks of natural language\nunderstanding.", "published": "2020-05-27 16:44:01", "link": "http://arxiv.org/abs/2005.13482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Representation Models for Fine-Grained Sentiment Classification", "abstract": "Sentiment classification is a quickly advancing field of study with\napplications in almost any field. While various models and datasets have shown\nhigh accuracy inthe task of binary classification, the task of fine-grained\nsentiment classification is still an area with room for significant\nimprovement. Analyzing the SST-5 dataset,previous work by Munikar et al. (2019)\nshowed that the embedding tool BERT allowed a simple model to achieve\nstate-of-the-art accuracy. Since that paper, several BERT alternatives have\nbeen published, with three primary ones being AlBERT (Lan et al., 2019),\nDistilBERT (Sanh et al. 2019), and RoBERTa (Liu etal. 2019). While these models\nreport some improvement over BERT on the popular benchmarks GLUE, SQuAD, and\nRACE, they have not been applied to the fine-grained classification task. In\nthis paper, we examine whether the improvements hold true when applied to a\nnovel task, by replicating the BERT model from Munikar et al., and swapping the\nembedding layer to the alternative models. Over the experiments, we found that\nAlBERT suffers significantly more accuracy loss than reported on other tasks,\nDistilBERT has accuracy loss similar to their reported loss on other tasks\nwhile being the fastest model to train, and RoBERTa reaches anew\nstate-of-the-art accuracy for prediction on the SST-5 root level (60.2%).", "published": "2020-05-27 20:01:56", "link": "http://arxiv.org/abs/2005.13619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Detection meets Transfer Learning", "abstract": "We can consider Counterfactuals as belonging in the domain of Discourse\nstructure and semantics, A core area in Natural Language Understanding and in\nthis paper, we introduce an approach to resolving counterfactual detection as\nwell as the indexing of the antecedents and consequents of Counterfactual\nstatements. While Transfer learning is already being applied to several NLP\ntasks, It has the characteristics to excel in a novel number of tasks. We show\nthat detecting Counterfactuals is a straightforward Binary Classification Task\nthat can be implemented with minimal adaptation on already existing model\nArchitectures, thanks to a well annotated training data set,and we introduce a\nnew end to end pipeline to process antecedents and consequents as an entity\nrecognition task, thus adapting them into Token Classification.", "published": "2020-05-27 02:02:57", "link": "http://arxiv.org/abs/2005.13125v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chat as Expected: Learning to Manipulate Black-box Neural Dialogue\n  Models", "abstract": "Recently, neural network based dialogue systems have become ubiquitous in our\nincreasingly digitalized society. However, due to their inherent opaqueness,\nsome recently raised concerns about using neural models are starting to be\ntaken seriously. In fact, intentional or unintentional behaviors could lead to\na dialogue system to generate inappropriate responses. Thus, in this paper, we\ninvestigate whether we can learn to craft input sentences that result in a\nblack-box neural dialogue model being manipulated into having its outputs\ncontain target words or match target sentences. We propose a reinforcement\nlearning based model that can generate such desired inputs automatically.\nExtensive experiments on a popular well-trained state-of-the-art neural\ndialogue model show that our method can successfully seek out desired inputs\nthat lead to the target outputs in a considerable portion of cases.\nConsequently, our work reveals the potential of neural dialogue models to be\nmanipulated, which inspires and opens the door towards developing strategies to\ndefend them.", "published": "2020-05-27 05:34:12", "link": "http://arxiv.org/abs/2005.13170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Catching Attention with Automatic Pull Quote Selection", "abstract": "To advance understanding on how to engage readers, we advocate the novel task\nof automatic pull quote selection. Pull quotes are a component of articles\nspecifically designed to catch the attention of readers with spans of text\nselected from the article and given more salient presentation. This task\ndiffers from related tasks such as summarization and clickbait identification\nby several aspects. We establish a spectrum of baseline approaches to the task,\nranging from handcrafted features to a neural mixture-of-experts to cross-task\nmodels. By examining the contributions of individual features and embedding\ndimensions from these models, we uncover unexpected properties of pull quotes\nto help answer the important question of what engages readers. Human evaluation\nalso supports the uniqueness of this task and the suitability of our selection\nmodels. The benefits of exploring this problem further are clear: pull quotes\nincrease enjoyment and readability, shape reader perceptions, and facilitate\nlearning. Code to reproduce this work is available at\nhttps://github.com/tannerbohn/AutomaticPullQuoteSelection.", "published": "2020-05-27 09:59:34", "link": "http://arxiv.org/abs/2005.13263v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards an Open Platform for Legal Information", "abstract": "Recent advances in the area of legal information systems have led to a\nvariety of applications that promise support in processing and accessing legal\ndocuments. Unfortunately, these applications have various limitations, e.g.,\nregarding scope or extensibility. Furthermore, we do not observe a trend\ntowards open access in digital libraries in the legal domain as we observe in\nother domains, e.g., economics of computer science. To improve open access in\nthe legal domain, we present our approach for an open source platform to\ntransparently process and access Legal Open Data. This enables the sustainable\ndevelopment of legal applications by offering a single technology stack.\nMoreover, the approach facilitates the development and deployment of new\ntechnologies. As proof of concept, we implemented six technologies and\ngenerated metadata for more than 250,000 German laws and court decisions. Thus,\nwe can provide users of our platform not only access to legal documents, but\nalso the contained information.", "published": "2020-05-27 13:16:19", "link": "http://arxiv.org/abs/2005.13342v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews", "abstract": "Despite the recent advances in opinion mining for written reviews, few works\nhave tackled the problem on other sources of reviews. In light of this issue,\nwe propose a multi-modal approach for mining fine-grained opinions from video\nreviews that is able to determine the aspects of the item under review that are\nbeing discussed and the sentiment orientation towards them. Our approach works\nat the sentence level without the need for time annotations and uses features\nderived from the audio, video and language transcriptions of its contents. We\nevaluate our approach on two datasets and show that leveraging the video and\naudio modalities consistently provides increased performance over text-only\nbaselines, providing evidence these extra modalities are key in better\nunderstanding video reviews.", "published": "2020-05-27 13:46:11", "link": "http://arxiv.org/abs/2005.13362v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing", "abstract": "One daunting problem for semantic parsing is the scarcity of annotation.\nAiming to reduce nontrivial human labor, we propose a two-stage semantic\nparsing framework, where the first stage utilizes an unsupervised paraphrase\nmodel to convert an unlabeled natural language utterance into the canonical\nutterance. The downstream naive semantic parser accepts the intermediate output\nand returns the target logical form. Furthermore, the entire training process\nis split into two phases: pre-training and cycle learning. Three tailored\nself-supervised tasks are introduced throughout training to activate the\nunsupervised paraphrase model. Experimental results on benchmarks Overnight and\nGeoGranno demonstrate that our framework is effective and compatible with\nsupervised training.", "published": "2020-05-27 16:47:44", "link": "http://arxiv.org/abs/2005.13485v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Temporal Opinion Modelling for Opinion Prediction on Twitter", "abstract": "Opinion prediction on Twitter is challenging due to the transient nature of\ntweet content and neighbourhood context. In this paper, we model users' tweet\nposting behaviour as a temporal point process to jointly predict the posting\ntime and the stance label of the next tweet given a user's historical tweet\nsequence and tweets posted by their neighbours. We design a topic-driven\nattention mechanism to capture the dynamic topic shifts in the neighbourhood\ncontext. Experimental results show that the proposed model predicts both the\nposting time and the stance labels of future tweets more accurately compared to\na number of competitive baselines.", "published": "2020-05-27 16:49:04", "link": "http://arxiv.org/abs/2005.13486v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "In search of isoglosses: continuous and discrete language embeddings in\n  Slavic historical phonology", "abstract": "This paper investigates the ability of neural network architectures to\neffectively learn diachronic phonological generalizations in a multilingual\nsetting. We employ models using three different types of language embedding\n(dense, sigmoid, and straight-through). We find that the Straight-Through model\noutperforms the other two in terms of accuracy, but the Sigmoid model's\nlanguage embeddings show the strongest agreement with the traditional\nsubgrouping of the Slavic languages. We find that the Straight-Through model\nhas learned coherent, semi-interpretable information about sound change, and\noutline directions for future research.", "published": "2020-05-27 18:10:46", "link": "http://arxiv.org/abs/2005.13575v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rationalizing Text Matching: Learning Sparse Alignments via Optimal\n  Transport", "abstract": "Selecting input features of top relevance has become a popular method for\nbuilding self-explaining models. In this work, we extend this selective\nrationalization approach to text matching, where the goal is to jointly select\nand align text pieces, such as tokens or sentences, as a justification for the\ndownstream prediction. Our approach employs optimal transport (OT) to find a\nminimal cost alignment between the inputs. However, directly applying OT often\nproduces dense and therefore uninterpretable alignments. To overcome this\nlimitation, we introduce novel constrained variants of the OT problem that\nresult in highly sparse alignments with controllable sparsity. Our model is\nend-to-end differentiable using the Sinkhorn algorithm for OT and can be\ntrained without any alignment annotations. We evaluate our model on the\nStackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very\nsparse rationale selections with high fidelity while preserving prediction\naccuracy compared to strong attention baseline models.", "published": "2020-05-27 01:20:49", "link": "http://arxiv.org/abs/2005.13111v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The First Shared Task on Discourse Representation Structure Parsing", "abstract": "The paper presents the IWCS 2019 shared task on semantic parsing where the\ngoal is to produce Discourse Representation Structures (DRSs) for English\nsentences. DRSs originate from Discourse Representation Theory and represent\nscoped meaning representations that capture the semantics of negation, modals,\nquantification, and presupposition triggers. Additionally, concepts and\nevent-participants in DRSs are described with WordNet synsets and the thematic\nroles from VerbNet. To measure similarity between two DRSs, they are\nrepresented in a clausal form, i.e. as a set of tuples. Participant systems\nwere expected to produce DRSs in this clausal form. Taking into account the\nrich lexical information, explicit scope marking, a high number of shared\nvariables among clauses, and highly-constrained format of valid DRSs, all these\nmakes the DRS parsing a challenging NLP task. The results of the shared task\ndisplayed improvements over the existing state-of-the-art parser.", "published": "2020-05-27 14:52:21", "link": "http://arxiv.org/abs/2005.13399v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CausaLM: Causal Model Explanation Through Counterfactual Language Models", "abstract": "Understanding predictions made by deep neural networks is notoriously\ndifficult, but also crucial to their dissemination. As all machine learning\nbased methods, they are as good as their training data, and can also capture\nunwanted biases. While there are tools that can help understand whether such\nbiases exist, they do not distinguish between correlation and causation, and\nmight be ill-suited for text-based models and for reasoning about high level\nlanguage concepts. A key problem of estimating the causal effect of a concept\nof interest on a given model is that this estimation requires the generation of\ncounterfactual examples, which is challenging with existing generation\ntechnology. To bridge that gap, we propose CausaLM, a framework for producing\ncausal model explanations using counterfactual language representation models.\nOur approach is based on fine-tuning of deep contextualized embedding models\nwith auxiliary adversarial tasks derived from the causal graph of the problem.\nConcretely, we show that by carefully choosing auxiliary adversarial\npre-training tasks, language representation models such as BERT can effectively\nlearn a counterfactual representation for a given concept of interest, and be\nused to estimate its true causal effect on model performance. A byproduct of\nour method is a language representation model that is unaffected by the tested\nconcept, which can be useful in mitigating unwanted bias ingrained in the data.", "published": "2020-05-27 15:06:35", "link": "http://arxiv.org/abs/2005.13407v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phone Features Improve Speech Translation", "abstract": "End-to-end models for speech translation (ST) more tightly couple speech\nrecognition (ASR) and machine translation (MT) than a traditional cascade of\nseparate ASR and MT models, with simpler model architectures and the potential\nfor reduced error propagation. Their performance is often assumed to be\nsuperior, though in many conditions this is not yet the case. We compare\ncascaded and end-to-end models across high, medium, and low-resource\nconditions, and show that cascades remain stronger baselines. Further, we\nintroduce two methods to incorporate phone features into ST models. We show\nthat these features improve both architectures, closing the gap between\nend-to-end models and cascades, and outperforming previous academic work -- by\nup to 9 BLEU on our low-resource setting.", "published": "2020-05-27 22:05:10", "link": "http://arxiv.org/abs/2005.13681v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ACGAN-based Data Augmentation Integrated with Long-term Scalogram for\n  Acoustic Scene Classification", "abstract": "In acoustic scene classification (ASC), acoustic features play a crucial role\nin the extraction of scene information, which can be stored over different time\nscales. Moreover, the limited size of the dataset may lead to a biased model\nwith a poor performance for records from unseen cities and confusing scene\nclasses. In order to overcome this, we propose a long-term wavelet feature that\nrequires a lower storage capacity and can be classified faster and more\naccurately compared with classic Mel filter bank coefficients (FBank). This\nfeature can be extracted with predefined wavelet scales similar to the FBank.\nFurthermore, a novel data augmentation scheme based on generative adversarial\nneural networks with auxiliary classifiers (ACGANs) is adopted to improve the\ngeneralization of the ASC systems. The scheme, which contains ACGANs and a\nsample filter, extends the database iteratively by splitting the dataset,\ntraining the ACGANs and subsequently filtering samples. Experiments were\nconducted on datasets from the Detection and Classification of Acoustic Scenes\nand Events (DCASE) challenges. The results on the DCASE19 dataset demonstrate\nthe improved performance of the proposed techniques compared with the classic\nFBank classifier. Moreover, the proposed fusion system achieved first place in\nthe DCASE19 competition and surpassed the top accuracies on the DCASE17\ndataset.", "published": "2020-05-27 04:01:03", "link": "http://arxiv.org/abs/2005.13146v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Insertion-Based Modeling for End-to-End Automatic Speech Recognition", "abstract": "End-to-end (E2E) models have gained attention in the research field of\nautomatic speech recognition (ASR). Many E2E models proposed so far assume\nleft-to-right autoregressive generation of an output token sequence except for\nconnectionist temporal classification (CTC) and its variants. However,\nleft-to-right decoding cannot consider the future output context, and it is not\nalways optimal for ASR. One of the non-left-to-right models is known as\nnon-autoregressive Transformer (NAT) and has been intensively investigated in\nthe area of neural machine translation (NMT) research. One NAT model,\nmask-predict, has been applied to ASR but the model needs some heuristics or\nadditional component to estimate the length of the output token sequence. This\npaper proposes to apply another type of NAT called insertion-based models, that\nwere originally proposed for NMT, to ASR tasks. Insertion-based models solve\nthe above mask-predict issues and can generate an arbitrary generation order of\nan output sequence. In addition, we introduce a new formulation of joint\ntraining of the insertion-based models and CTC. This formulation reinforces CTC\nby making it dependent on insertion-based token generation in a\nnon-autoregressive manner. We conducted experiments on three public benchmarks\nand achieved competitive performance to strong autoregressive Transformer with\na similar decoding condition.", "published": "2020-05-27 07:25:31", "link": "http://arxiv.org/abs/2005.13211v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised source localization with deep generative modeling", "abstract": "We propose a semi-supervised localization approach based on deep generative\nmodeling with variational autoencoders (VAEs). Localization in reverberant\nenvironments remains a challenge, which machine learning (ML) has shown promise\nin addressing. Even with large data volumes, the number of labels available for\nsupervised learning in reverberant environments is usually small. We address\nthis issue by performing semi-supervised learning (SSL) with convolutional\nVAEs. The VAE is trained to generate the phase of relative transfer functions\n(RTFs), in parallel with a DOA classifier, on both labeled and unlabeled RTF\nsamples. The VAE-SSL approach is compared with SRP-PHAT and fully-supervised\nCNNs. We find that VAE-SSL can outperform both SRP-PHAT and CNN in\nlabel-limited scenarios.", "published": "2020-05-27 04:59:52", "link": "http://arxiv.org/abs/2005.13163v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Scalable Polyhedral Verification of Recurrent Neural Networks", "abstract": "We present a scalable and precise verifier for recurrent neural networks,\ncalled Prover based on two novel ideas: (i) a method to compute a set of\npolyhedral abstractions for the non-convex and nonlinear recurrent update\nfunctions by combining sampling, optimization, and Fermat's theorem, and (ii) a\ngradient descent based algorithm for abstraction refinement guided by the\ncertification problem that combines multiple abstractions for each neuron.\nUsing Prover, we present the first study of certifying a non-trivial use case\nof recurrent neural networks, namely speech classification. To achieve this, we\nadditionally develop custom abstractions for the non-linear speech\npreprocessing pipeline. Our evaluation shows that Prover successfully verifies\nseveral challenging recurrent models in computer vision, speech, and motion\nsensor data classification beyond the reach of prior work.", "published": "2020-05-27 11:57:01", "link": "http://arxiv.org/abs/2005.13300v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end\n  Approaches towards Data Efficiency and Low Latency", "abstract": "In this paper, we present a new open source toolkit for speech recognition,\nnamed CAT (CTC-CRF based ASR Toolkit). CAT inherits the data-efficiency of the\nhybrid approach and the simplicity of the E2E approach, providing a\nfull-fledged implementation of CTC-CRFs and complete training and testing\nscripts for a number of English and Chinese benchmarks. Experiments show CAT\nobtains state-of-the-art results, which are comparable to the fine-tuned hybrid\nmodels in Kaldi but with a much simpler training pipeline. Compared to existing\nnon-modularized E2E models, CAT performs better on limited-scale datasets,\ndemonstrating its data efficiency. Furthermore, we propose a new method called\ncontextualized soft forgetting, which enables CAT to do streaming ASR without\naccuracy degradation. We hope CAT, especially the CTC-CRF based framework and\nsoftware, will be of broad interest to the community, and can be further\nexplored and improved.", "published": "2020-05-27 12:41:21", "link": "http://arxiv.org/abs/2005.13326v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing\n  Label Features from Multi-Modal Embeddings", "abstract": "In this paper, we propose a novel approach for generalized zero-shot learning\nin a multi-modal setting, where we have novel classes of audio/video during\ntesting that are not seen during training. We use the semantic relatedness of\ntext embeddings as a means for zero-shot learning by aligning audio and video\nembeddings with the corresponding class label text feature space. Our approach\nuses a cross-modal decoder and a composite triplet loss. The cross-modal\ndecoder enforces a constraint that the class label text features can be\nreconstructed from the audio and video embeddings of data points. This helps\nthe audio and video embeddings to move closer to the class label text\nembedding. The composite triplet loss makes use of the audio, video, and text\nembeddings. It helps bring the embeddings from the same class closer and push\naway the embeddings from different classes in a multi-modal setting. This helps\nthe network to perform better on the multi-modal zero-shot learning task.\nImportantly, our multi-modal zero-shot learning approach works even if a\nmodality is missing at test time. We test our approach on the generalized\nzero-shot classification and retrieval tasks and show that our approach\noutperforms other models in the presence of a single modality as well as in the\npresence of multiple modalities. We validate our approach by comparing it with\nprevious approaches and using various ablations.", "published": "2020-05-27 14:58:34", "link": "http://arxiv.org/abs/2005.13402v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Weighted Data Spaces for Correlation-Based Array Imaging in Experimental\n  Aeroacoustics", "abstract": "This article discusses aeroacoustic imaging methods based on correlation\nmeasurements in the frequency domain. Standard methods in this field assume\nthat the estimated correlation matrix is superimposed with additive white\nnoise. In this paper we present a mathematical model for the measurement\nprocess covering arbitrarily correlated noise. The covariance matrix of\ncorrelation data is given in terms of fourth order moments. The aim of this\npaper is to explore the use of such additional information on the measurement\ndata in imaging methods. For this purpose a class of weighted data spaces is\nintroduced, where each data space naturally defines an associated beamforming\nmethod with a corresponding point spread function. This generic class of\nbeamformers contains many well-known methods such as Conventional Beamforming,\n(Robust) Adaptive Beamforming or beamforming with shading. This article\nexamines in particular weightings that depend on the noise (co)variances. In a\ntheoretical analysis we prove that the beamformer, weighted by the full noise\ncovariance matrix, has minimal variance among all beamformers from the\ndescribed class. Application of the (co)variance weighted methods on synthetic\nand experimental data show that the resolution of the results is improved and\nnoise effects are reduced.", "published": "2020-05-27 15:31:21", "link": "http://arxiv.org/abs/2005.13426v3", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Modality Dropout for Improved Performance-driven Talking Faces", "abstract": "We describe our novel deep learning approach for driving animated faces using\nboth acoustic and visual information. In particular, speech-related facial\nmovements are generated using audiovisual information, and non-speech facial\nmovements are generated using only visual information. To ensure that our model\nexploits both modalities during training, batches are generated that contain\naudio-only, video-only, and audiovisual input features. The probability of\ndropping a modality allows control over the degree to which the model exploits\naudio and visual information during training. Our trained model runs in\nreal-time on resource limited hardware (e.g.\\ a smart phone), it is user\nagnostic, and it is not dependent on a potentially error-prone transcription of\nthe speech. We use subjective testing to demonstrate: 1) the improvement of\naudiovisual-driven animation over the equivalent video-only approach, and 2)\nthe improvement in the animation of speech-related facial movements after\nintroducing modality dropout. Before introducing dropout, viewers prefer\naudiovisual-driven animation in 51% of the test sequences compared with only\n18% for video-driven. After introducing dropout viewer preference for\naudiovisual-driven animation increases to 74%, but decreases to 8% for\nvideo-only.", "published": "2020-05-27 19:55:33", "link": "http://arxiv.org/abs/2005.13616v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Comparative Study of Machine Learning Models for Tabular Data Through\n  Challenge of Monitoring Parkinson's Disease Progression Using Voice\n  Recordings", "abstract": "People with Parkinson's disease must be regularly monitored by their\nphysician to observe how the disease is progressing and potentially adjust\ntreatment plans to mitigate the symptoms. Monitoring the progression of the\ndisease through a voice recording captured by the patient at their own home can\nmake the process faster and less stressful. Using a dataset of voice recordings\nof 42 people with early-stage Parkinson's disease over a time span of 6 months,\nwe applied multiple machine learning techniques to find a correlation between\nthe voice recording and the patient's motor UPDRS score. We approached this\nproblem using a multitude of both regression and classification techniques.\nMuch of this paper is dedicated to mapping the voice data to motor UPDRS scores\nusing regression techniques in order to obtain a more precise value for unknown\ninstances. Through this comparative study of variant machine learning methods,\nwe realized some old machine learning methods like trees outperform cutting\nedge deep learning models on numerous tabular datasets.", "published": "2020-05-27 16:09:26", "link": "http://arxiv.org/abs/2005.14257v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Deep Sensory Substitution: Noninvasively Enabling Biological Neural\n  Networks to Receive Input from Artificial Neural Networks", "abstract": "As is expressed in the adage \"a picture is worth a thousand words\", when\nusing spoken language to communicate visual information, brevity can be a\nchallenge. This work describes a novel technique for leveraging machine-learned\nfeature embeddings to sonify visual (and other types of) information into a\nperceptual audio domain, allowing users to perceive this information using only\ntheir aural faculty. The system uses a pretrained image embedding network to\nextract visual features and embed them in a compact subset of Euclidean space\n-- this converts the images into feature vectors whose $L^2$ distances can be\nused as a meaningful measure of similarity. A generative adversarial network\n(GAN) is then used to find a distance preserving map from this metric space of\nfeature vectors into the metric space defined by a target audio dataset\nequipped with either the Euclidean metric or a mel-frequency cepstrum-based\npsychoacoustic distance metric. We demonstrate this technique by sonifying\nimages of faces into human speech-like audio. For both target audio metrics,\nthe GAN successfully found a metric preserving mapping, and in human subject\ntests, users were able to accurately classify audio sonifications of faces.", "published": "2020-05-27 11:41:48", "link": "http://arxiv.org/abs/2005.13291v3", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
