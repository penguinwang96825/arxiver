{"title": "Part of Speech Tagging (POST) of a Low-resource Language using another\n  Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged\n  Persian (Farsi) Corpus)", "abstract": "Tagged corpora play a crucial role in a wide range of Natural Language\nProcessing. The Part of Speech Tagging (POST) is essential in developing tagged\ncorpora. It is time-and-effort-consuming and costly, and therefore, it could be\nmore affordable if it is automated. The Kurdish language currently lacks\npublicly available tagged corpora of proper sizes. Tagging the publicly\navailable Kurdish corpora can leverage the capability of those resources to a\nhigher level than what raw or segmented corpora can provide. Developing\nPOS-tagged lexicons can assist the mentioned task. We use a tagged corpus\n(Bijankhan corpus) in Persian (Farsi) as a close language to Kurdish to develop\na POS-tagged lexicon. This paper presents the approach of leveraging the\nresource of a close language to Kurdish to enrich its resources. A partial\ndataset of the results is publicly available for non-commercial use under CC\nBY-NC-SA 4.0 license at https://kurdishblark.github.io/. We plan to make the\nwhole tagged corpus available after further investigation on the outcome. The\ndataset can help in developing POS-tagged lexicons for other Kurdish dialects\nand automated Kurdish corpora tagging.", "published": "2022-01-30 11:49:43", "link": "http://arxiv.org/abs/2201.12793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognition of Implicit Geographic Movement in Text", "abstract": "Analyzing the geographic movement of humans, animals, and other phenomena is\na growing field of research. This research has benefited urban planning,\nlogistics, animal migration understanding, and much more. Typically, the\nmovement is captured as precise geographic coordinates and time stamps with\nGlobal Positioning Systems (GPS). Although some research uses computational\ntechniques to take advantage of implicit movement in descriptions of route\ndirections, hiking paths, and historical exploration routes, innovation would\naccelerate with a large and diverse corpus. We created a corpus of sentences\nlabeled as describing geographic movement or not and including the type of\nentity moving. Creating this corpus proved difficult without any comparable\ncorpora to start with, high human labeling costs, and since movement can at\ntimes be interpreted differently. To overcome these challenges, we developed an\niterative process employing hand labeling, crowd voting for confirmation, and\nmachine learning to predict more labels. By merging advances in word embeddings\nwith traditional machine learning models and model ensembling, prediction\naccuracy is at an acceptable level to produce a large silver-standard corpus\ndespite the small gold-standard corpus training set. Our corpus will likely\nbenefit computational processing of geography in text and spatial cognition, in\naddition to detection of movement.", "published": "2022-01-30 12:22:55", "link": "http://arxiv.org/abs/2201.12799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Segmentation and Morphological Parsing for Sanskrit", "abstract": "We describe our participation in the Word Segmentation and Morphological\nParsing (WSMP) for Sanskrit hackathon. We approach the word segmentation task\nas a sequence labelling task by predicting edit operations from which\nsegmentations are derived. We approach the morphological analysis task by\npredicting morphological tags and rules that transform inflected words into\ntheir corresponding stems. Also, we propose an end-to-end trainable pipeline\nmodel for joint segmentation and morphological analysis. Our model performed\nbest in the joint segmentation and analysis subtask (80.018 F1 score) and\nperformed second best in the individual subtasks (segmentation: 96.189 F1 score\n/ analysis: 69.180 F1 score).\n  Finally, we analyse errors made by our models and suggest future work and\npossible improvements regarding data and evaluation.", "published": "2022-01-30 14:37:00", "link": "http://arxiv.org/abs/2201.12833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anticipation-Free Training for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SimulMT) speeds up the translation process\nby starting to translate before the source sentence is completely available. It\nis difficult due to limited context and word order difference between\nlanguages. Existing methods increase latency or introduce adaptive read-write\npolicies for SimulMT models to handle local reordering and improve translation\nquality. However, the long-distance reordering would make the SimulMT models\nlearn translation mistakenly. Specifically, the model may be forced to predict\ntarget tokens when the corresponding source tokens have not been read. This\nleads to aggressive anticipation during inference, resulting in the\nhallucination phenomenon. To mitigate this problem, we propose a new framework\nthat decompose the translation process into the monotonic translation step and\nthe reordering step, and we model the latter by the auxiliary sorting network\n(ASN). The ASN rearranges the hidden states to match the order in the target\nlanguage, so that the SimulMT model could learn to translate more reasonably.\nThe entire model is optimized end-to-end and does not rely on external aligners\nor data. During inference, ASN is removed to achieve streaming. Experiments\nshow the proposed framework could outperform previous methods with less\nlatency.", "published": "2022-01-30 16:29:37", "link": "http://arxiv.org/abs/2201.12868v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical cues to subjecthood are redundant in a majority of simple\n  clauses across languages", "abstract": "Grammatical cues are sometimes redundant with word meanings in natural\nlanguage. For instance, English word order rules constrain the word order of a\nsentence like \"The dog chewed the bone\" even though the status of \"dog\" as\nsubject and \"bone\" as object can be inferred from world knowledge and\nplausibility. Quantifying how often this redundancy occurs, and how the level\nof redundancy varies across typologically diverse languages, can shed light on\nthe function and evolution of grammar. To that end, we performed a behavioral\nexperiment in English and Russian and a cross-linguistic computational analysis\nmeasuring the redundancy of grammatical cues in transitive clauses extracted\nfrom corpus text. English and Russian speakers (n=484) were presented with\nsubjects, verbs, and objects (in random order and with morphological markings\nremoved) extracted from naturally occurring sentences and were asked to\nidentify which noun is the subject of the action. Accuracy was high in both\nlanguages (~89% in English, ~87% in Russian). Next, we trained a neural network\nmachine classifier on a similar task: predicting which nominal in a\nsubject-verb-object triad is the subject. Across 30 languages from eight\nlanguage families, performance was consistently high: a median accuracy of 87%,\ncomparable to the accuracy observed in the human experiments. The conclusion is\nthat grammatical cues such as word order are necessary to convey subjecthood\nand objecthood in a minority of naturally occurring transitive clauses;\nnevertheless, they can (a) provide an important source of redundancy and (b)\nare crucial for conveying intended meaning that cannot be inferred from the\nwords alone, including descriptions of human interactions, where roles are\noften reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing\nnon-prototypical meanings (e.g., \"The bone chewed the dog.\").", "published": "2022-01-30 21:01:10", "link": "http://arxiv.org/abs/2201.12911v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Frustratingly Simple Approach for End-to-End Image Captioning", "abstract": "Image Captioning is a fundamental task to join vision and language,\nconcerning about cross-modal understanding and text generation. Recent years\nwitness the emerging attention on image captioning. Most of existing works\nfollow a traditional two-stage training paradigm. Before training the\ncaptioning models, an extra object detector is utilized to recognize the\nobjects in the image at first. However, they require sizeable datasets with\nfine-grained object annotation for training the object detector, which is a\ndaunting task. In addition, the errors of the object detectors are easy to\npropagate to the following captioning models, degenerating models' performance.\nTo alleviate such defects, we propose a frustratingly simple but highly\neffective end-to-end image captioning framework, Visual Conditioned GPT\n(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language\ndecoder (GPT2). Different from the vanilla connection method that directly\ninserts the cross-attention modules into GPT2, we come up with a self-ensemble\ncross-modal fusion mechanism that comprehensively considers both the single-\nand cross-modal knowledge. As a result, we do not need extra object detectors\nfor model training. Experimental results conducted on three popular image\ncaptioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our\nVC-GPT achieves either the best or the second-best performance across all\nevaluation metrics over extensive baseline systems.", "published": "2022-01-30 04:44:54", "link": "http://arxiv.org/abs/2201.12723v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Co-Regularized Adversarial Learning for Multi-Domain Text Classification", "abstract": "Multi-domain text classification (MDTC) aims to leverage all available\nresources from multiple domains to learn a predictive model that can generalize\nwell on these domains. Recently, many MDTC methods adopt adversarial learning,\nshared-private paradigm, and entropy minimization to yield state-of-the-art\nresults. However, these approaches face three issues: (1) Minimizing domain\ndivergence can not fully guarantee the success of domain alignment; (2)\nAligning marginal feature distributions can not fully guarantee the\ndiscriminability of the learned features; (3) Standard entropy minimization may\nmake the predictions on unlabeled data over-confident, deteriorating the\ndiscriminability of the learned features. In order to address the above issues,\nwe propose a co-regularized adversarial learning (CRAL) mechanism for MDTC.\nThis approach constructs two diverse shared latent spaces, performs domain\nalignment in each of them, and punishes the disagreements of these two\nalignments with respect to the predictions on unlabeled data. Moreover, virtual\nadversarial training (VAT) with entropy minimization is incorporated to impose\nconsistency regularization to the CRAL method. Experiments show that our model\noutperforms state-of-the-art methods on two MDTC benchmarks.", "published": "2022-01-30 12:15:41", "link": "http://arxiv.org/abs/2201.12796v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Dataset for Medical Instructional Video Classification and Question\n  Answering", "abstract": "This paper introduces a new challenge and datasets to foster research toward\ndesigning systems that can understand medical videos and provide visual answers\nto natural language questions. We believe medical videos may provide the best\npossible answers to many first aids, medical emergency, and medical education\nquestions. Toward this, we created the MedVidCL and MedVidQA datasets and\nintroduce the tasks of Medical Video Classification (MVC) and Medical Visual\nAnswer Localization (MVAL), two tasks that focus on cross-modal (medical\nlanguage and medical video) understanding. The proposed tasks and datasets have\nthe potential to support the development of sophisticated downstream\napplications that can benefit the public and medical practitioners. Our\ndatasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated\nquestions and answers timestamps from 899 videos for the MVAL task. These\ndatasets have been verified and corrected by medical informatics experts. We\nhave also benchmarked each task with the created MedVidCL and MedVidQA datasets\nand proposed the multimodal learning methods that set competitive baselines for\nfuture research.", "published": "2022-01-30 18:06:31", "link": "http://arxiv.org/abs/2201.12888v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving End-to-End Contextual Speech Recognition with Fine-Grained\n  Contextual Knowledge Selection", "abstract": "Nowadays, most methods in end-to-end contextual speech recognition bias the\nrecognition process towards contextual knowledge. Since all-neural contextual\nbiasing methods rely on phrase-level contextual modeling and attention-based\nrelevance modeling, they may encounter confusion between similar\ncontext-specific phrases, which hurts predictions at the token level. In this\nwork, we focus on mitigating confusion problems with fine-grained contextual\nknowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge\nto reduce the uncertainty of token predictions. Specifically, we first apply\nphrase selection to narrow the range of phrase candidates, and then conduct\ntoken attention on the tokens in the selected phrase candidates. Moreover, we\nre-normalize the attention weights of most relevant phrases in inference to\nobtain more focused phrase-level contextual representations, and inject\nposition information to better discriminate phrases or tokens. On LibriSpeech\nand an in-house 160,000-hour dataset, we explore the proposed methods based on\na controllable all-neural biasing method, collaborative decoding (ColDec). The\nproposed methods provide at most 6.1% relative word error rate reduction on\nLibriSpeech and 16.4% relative character error rate reduction on the in-house\ndataset over ColDec.", "published": "2022-01-30 13:08:16", "link": "http://arxiv.org/abs/2201.12806v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Compositionality as Lexical Symmetry", "abstract": "In tasks like semantic parsing, instruction following, and question\nanswering, standard deep networks fail to generalize compositionally from small\ndatasets. Many existing approaches overcome this limitation with model\narchitectures that enforce a compositional process of sentence interpretation.\nIn this paper, we present a domain-general and model-agnostic formulation of\ncompositionality as a constraint on symmetries of data distributions rather\nthan models. Informally, we prove that whenever a task can be solved by a\ncompositional model, there is a corresponding data augmentation scheme -- a\nprocedure for transforming examples into other well formed examples -- that\nimparts compositional inductive bias on any model trained to solve the same\ntask. We describe a procedure called LEXSYM that discovers these\ntransformations automatically, then applies them to training data for ordinary\nneural sequence models. Unlike existing compositional data augmentation\nprocedures, LEXSYM can be deployed agnostically across text, structured data,\nand even images. It matches or surpasses state-of-the-art, task-specific models\non COGS semantic parsing, SCAN and ALCHEMY instruction following, and\nCLEVR-COGENT visual question answering datasets.", "published": "2022-01-30 21:44:46", "link": "http://arxiv.org/abs/2201.12926v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Literature Review about Idea Mining: The Use of\n  Machine-driven Analytics to Generate Ideas", "abstract": "Idea generation is the core activity of innovation. Digital data sources,\nwhich are sources of innovation, such as patents, publications, social media,\nwebsites, etc., are increasingly growing at unprecedented volume. Manual idea\ngeneration is time-consuming and is affected by the subjectivity of the\nindividuals involved. Therefore, the use machine-driven data analytics\ntechniques to analyze data to generate ideas and support idea generation by\nserving users is useful. The objective of this study is to study state-of\nthe-art machine-driven analytics for idea generation and data sources, hence\nthe result of this study will generally server as a guideline for choosing\ntechniques and data sources. A systematic literature review is conducted to\nidentify relevant scholarly literature from IEEE, Scopus, Web of Science and\nGoogle Scholar. We selected a total of 71 articles and analyzed them\nthematically. The results of this study indicate that idea generation through\nmachine-driven analytics applies text mining, information retrieval (IR),\nartificial intelligence (AI), deep learning, machine learning, statistical\ntechniques, natural language processing (NLP), NLP-based morphological\nanalysis, network analysis, and bibliometric to support idea generation. The\nresults include a list of techniques and procedures in idea generation through\nmachine-driven idea analytics. Additionally, characterization and heuristics\nused in idea generation are summarized. For the future, tools designed to\ngenerate ideas could be explored.", "published": "2022-01-30 21:46:21", "link": "http://arxiv.org/abs/2202.12826v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "HGCN: Harmonic gated compensation network for speech enhancement", "abstract": "Mask processing in the time-frequency (T-F) domain through the neural network\nhas been one of the mainstreams for single-channel speech enhancement. However,\nit is hard for most models to handle the situation when harmonics are partially\nmasked by noise. To tackle this challenge, we propose a harmonic gated\ncompensation network (HGCN). We design a high-resolution harmonic integral\nspectrum to improve the accuracy of harmonic locations prediction. Then we add\nvoice activity detection (VAD) and voiced region detection (VRD) to the\nconvolutional recurrent network (CRN) to filter harmonic locations. Finally,\nthe harmonic gating mechanism is used to guide the compensation model to adjust\nthe coarse results from CRN to obtain the refinedly enhanced results. Our\nexperiments show HGCN achieves substantial gain over a number of advanced\napproaches in the community.", "published": "2022-01-30 09:06:20", "link": "http://arxiv.org/abs/2201.12755v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
