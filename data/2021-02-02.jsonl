{"title": "MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations", "abstract": "We study conversational dialog in which there are many possible responses to\na given history. We present the MultiTalk Dataset, a corpus of over 320,000\nsentences of written conversational dialog that balances a high branching\nfactor (10) with several conversation turns (6) through selective branch\ncontinuation. We make multiple contributions to study dialog generation in the\nhighly branching setting. In order to evaluate a diverse set of generations, we\npropose a simple scoring algorithm, based on bipartite graph matching, to\noptimally incorporate a set of diverse references. We study multiple language\ngeneration tasks at different levels of predictive conversation depth, using\ntextual attributes induced automatically from pretrained classifiers. Our\nculminating task is a challenging theory of mind problem, a controllable\ngeneration task which requires reasoning about the expected reaction of the\nlistener.", "published": "2021-02-02 02:29:40", "link": "http://arxiv.org/abs/2102.01263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Improved Baseline for Sentence-level Relation Extraction", "abstract": "Sentence-level relation extraction (RE) aims at identifying the relationship\nbetween two entities in a sentence. Many efforts have been devoted to this\nproblem, while the best performing methods are still far from perfect. In this\npaper, we revisit two problems that affect the performance of existing RE\nmodels, namely entity representation and noisy or ill-defined labels. Our\nimproved RE baseline, incorporated with entity representations with typed\nmarkers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous\nSOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1%\non the refined Re-TACRED dataset, demonstrating that the pretrained language\nmodels (PLMs) achieve high performance on this task. We release our code to the\ncommunity for future research.", "published": "2021-02-02 07:57:06", "link": "http://arxiv.org/abs/2102.01373v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Demonstrations of the Machine Translation Applications to Historical\n  Documents", "abstract": "We present our demonstration of two machine translation applications to\nhistorical documents. The first task consists in generating a new version of a\nhistorical document, written in the modern version of its original language.\nThe second application is limited to a document's orthography. It adapts the\ndocument's spelling to modern standards in order to achieve an orthography\nconsistency and accounting for the lack of spelling conventions. We followed an\ninteractive, adaptive framework that allows the user to introduce corrections\nto the system's hypothesis. The system reacts to these corrections by\ngenerating a new hypothesis that takes them into account. Once the user is\nsatisfied with the system's hypothesis and validates it, the system adapts its\nmodel following an online learning strategy. This system is implemented\nfollowing a client-server architecture. We developed a website which\ncommunicates with the neural models. All code is open-source and publicly\navailable. The demonstration is hosted at http://demosmt.prhlt.upv.es/mthd/.", "published": "2021-02-02 10:28:31", "link": "http://arxiv.org/abs/2102.01417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAUVE: Measuring the Gap Between Neural Text and Human Text using\n  Divergence Frontiers", "abstract": "As major progress is made in open-ended text generation, measuring how close\nmachine-generated text is to human language remains a critical open problem. We\nintroduce MAUVE, a comparison measure for open-ended text generation, which\ndirectly compares the learnt distribution from a text generation model to the\ndistribution of human-written text using divergence frontiers. MAUVE scales up\nto modern text generation models by computing information divergences in a\nquantized embedding space. Through an extensive empirical study on three\nopen-ended generation tasks, we find that MAUVE identifies known properties of\ngenerated text, scales naturally with model size, and correlates with human\njudgments, with fewer restrictions than existing distributional evaluation\nmetrics.", "published": "2021-02-02 11:59:28", "link": "http://arxiv.org/abs/2102.01454v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clickbait Headline Detection in Indonesian News Sites using Multilingual\n  Bidirectional Encoder Representations from Transformers (M-BERT)", "abstract": "Click counts are related to the amount of money that online advertisers paid\nto news sites. Such business models forced some news sites to employ a dirty\ntrick of click-baiting, i.e., using a hyperbolic and interesting words,\nsometimes unfinished sentence in a headline to purposefully tease the readers.\nSome Indonesian online news sites also joined the party of clickbait, which\nindirectly degrade other established news sites' credibility. A neural network\nwith a pre-trained language model M-BERT that acted as a embedding layer is\nthen combined with a 100 nodes hidden layer and topped with a sigmoid\nclassifier was trained to detect clickbait headlines. With a total of 6632\nheadlines as a training dataset, the classifier performed remarkably well.\nEvaluated with 5-fold cross validation, it has an accuracy score of 0.914,\nf1-score of 0.914, precision score of 0.916, and ROC-AUC of 0.92. The usage of\nmultilingual BERT in Indonesian text classification task was tested and is\npossible to be enhanced further. Future possibilities, societal impact, and\nlimitations of the clickbait detection are discussed.", "published": "2021-02-02 14:13:02", "link": "http://arxiv.org/abs/2102.01497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTC-based Compression for Direct Speech Translation", "abstract": "Previous studies demonstrated that a dynamic phone-informed compression of\nthe input audio is beneficial for speech translation (ST). However, they\nrequired a dedicated model for phone recognition and did not test this solution\nfor direct ST, in which a single model translates the input audio into the\ntarget language without intermediate representations. In this work, we propose\nthe first method able to perform a dynamic compression of the input indirect ST\nmodels. In particular, we exploit the Connectionist Temporal Classification\n(CTC) to compress the input sequence according to its phonetic characteristics.\nOur experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement\nover a strong baseline on two language pairs (English-Italian and\nEnglish-German), contextually reducing the memory footprint by more than 10%.", "published": "2021-02-02 16:09:19", "link": "http://arxiv.org/abs/2102.01578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Multilingual TEDx Corpus for Speech Recognition and Translation", "abstract": "We present the Multilingual TEDx corpus, built to support speech recognition\n(ASR) and speech translation (ST) research across many non-English source\nlanguages. The corpus is a collection of audio recordings from TEDx talks in 8\nsource languages. We segment transcripts into sentences and align them to the\nsource-language audio and target-language translations. The corpus is released\nalong with open-sourced code enabling extension to new talks and languages as\nthey become available. Our corpus creation methodology can be applied to more\nlanguages than previous work, and creates multi-way parallel evaluation sets.\nWe provide baselines in multiple ASR and ST settings, including multilingual\nmodels to improve translation performance for low-resource language pairs.", "published": "2021-02-02 21:16:25", "link": "http://arxiv.org/abs/2102.01757v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Data Augmentation via Example Extrapolation", "abstract": "In many applications of machine learning, certain categories of examples may\nbe underrepresented in the training data, causing systems to underperform on\nsuch \"few-shot\" cases at test time. A common remedy is to perform data\naugmentation, such as by duplicating underrepresented examples, or\nheuristically synthesizing new examples. But these remedies often fail to cover\nthe full diversity and complexity of real examples.\n  We propose a data augmentation approach that performs neural Example\nExtrapolation (Ex2). Given a handful of exemplars sampled from some\ndistribution, Ex2 synthesizes new examples that also belong to the same\ndistribution. The Ex2 model is learned by simulating the example generation\nprocedure on data-rich slices of the data, and it is applied to\nunderrepresented, few-shot slices.\n  We apply Ex2 to a range of language understanding tasks and significantly\nimprove over state-of-the-art methods on multiple few-shot learning benchmarks,\nincluding for relation extraction (FewRel) and intent classification + slot\nfilling (SNIPS).", "published": "2021-02-02 06:20:19", "link": "http://arxiv.org/abs/2102.01335v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Robustness of Neural Semantic Parsers", "abstract": "Semantic parsing maps natural language (NL) utterances into logical forms\n(LFs), which underpins many advanced NLP problems. Semantic parsers gain\nperformance boosts with deep neural networks, but inherit vulnerabilities\nagainst adversarial examples. In this paper, we provide the empirical study on\nthe robustness of semantic parsers in the presence of adversarial attacks.\nFormally, adversaries of semantic parsing are considered to be the perturbed\nutterance-LF pairs, whose utterances have exactly the same meanings as the\noriginal ones. A scalable methodology is proposed to construct robustness test\nsets based on existing benchmark corpora. Our results answered five research\nquestions in measuring the sate-of-the-art parsers' performance on robustness\ntest sets, and evaluating the effect of data augmentation.", "published": "2021-02-02 15:41:28", "link": "http://arxiv.org/abs/2102.01563v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parametrized Quantum Circuits of Synonymous Sentences in Quantum Natural\n  Language Processing", "abstract": "In this paper, we develop a compositional vector-based semantics of positive\ntransitive sentences in quantum natural language processing for a non-English\nlanguage, i.e. Persian, to compare the parametrized quantum circuits of two\nsynonymous sentences in two languages, English and Persian. By considering\ngrammar+meaning of a transitive sentence, we translate DisCoCat diagram via\nZX-calculus into quantum circuit form. Also, we use a bigraph method to rewrite\nDisCoCat diagram and turn into quantum circuit in the semantic side.", "published": "2021-02-02 23:11:41", "link": "http://arxiv.org/abs/2102.02204v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "The impact of external innovation on new drug approvals: A retrospective\n  analysis", "abstract": "Pharmaceutical companies are relying more often on external sources of\ninnovation to boost their discovery research productivity. However, more\nin-depth knowledge about how external innovation may translate to successful\nproduct launches is still required in order to better understand how to best\nleverage the innovation ecosystem. We analyzed the pre-approval publication\nhistories for FDA-approved new molecular entities (NMEs) and new biologic\nentities (NBEs) launched by 13 top research pharma companies during the last\ndecade (2006-2016). We found that academic institutions contributed the\nmajority of pre-approval publications and that publication subject matter is\nclosely aligned with the strengths of the respective innovator. We found this\nto also be true for candidate drugs terminated in Phase 3, but the volume of\nliterature on these molecules is substantially less than for approved drugs.\nThis may suggest that approved drugs are often associated with a more robust\ndataset provided by a large number of institutes. Collectively, the results of\nour analysis support the hypothesis that a collaborative research innovation\nenvironment spanning across academia, industry and government is highly\nconducive to successful drug approvals.", "published": "2021-02-02 02:21:34", "link": "http://arxiv.org/abs/2102.01260v1", "categories": ["cs.CL", "cs.CY", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "The Hitachi-JHU DIHARD III System: Competitive End-to-End Neural\n  Diarization and X-Vector Clustering Systems Combined by DOVER-Lap", "abstract": "This paper provides a detailed description of the Hitachi-JHU system that was\nsubmitted to the Third DIHARD Speech Diarization Challenge. The system outputs\nthe ensemble results of the five subsystems: two x-vector-based subsystems, two\nend-to-end neural diarization-based subsystems, and one hybrid subsystem. We\nrefine each system and all five subsystems become competitive and\ncomplementary. After the DOVER-Lap based system combination, it achieved\ndiarization error rates of 11.58 % and 14.09 % in Track 1 full and core, and\n16.94 % and 20.01 % in Track 2 full and core, respectively. With their results,\nwe won second place in all the tasks of the challenge.", "published": "2021-02-02 07:30:44", "link": "http://arxiv.org/abs/2102.01363v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WeNet: Production oriented Streaming and Non-streaming End-to-End Speech\n  Recognition Toolkit", "abstract": "In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.", "published": "2021-02-02 15:19:41", "link": "http://arxiv.org/abs/2102.01547v5", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SPEAK WITH YOUR HANDS Using Continuous Hand Gestures to control\n  Articulatory Speech Synthesizer", "abstract": "This work presents our advancements in controlling an articulatory speech\nsynthesis engine, \\textit{viz.}, Pink Trombone, with hand gestures. Our\ninterface translates continuous finger movements and wrist flexion into\ncontinuous speech using vocal tract area-function based articulatory speech\nsynthesis. We use Cyberglove II with 18 sensors to capture the kinematic\ninformation of the wrist and the individual fingers, in order to control a\nvirtual tongue. The coordinates and the bending values of the sensors are then\nutilized to fit a spline tongue model that smoothens out the noisy values and\noutliers. Considering the upper palate as fixed and the spline model as the\ndynamically moving lower surface (tongue) of the vocal tract, we compute 1D\narea functional values that are fed to the Pink Trombone, generating continuous\nspeech sounds. Therefore, by learning to manipulate one's wrist and fingers,\none can learn to produce speech sounds just through one's hands, without the\nneed for using the vocal tract.", "published": "2021-02-02 17:49:51", "link": "http://arxiv.org/abs/2102.01640v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The GEM Benchmark: Natural Language Generation, its Evaluation and\n  Metrics", "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG),\nits Evaluation, and Metrics. Measuring progress in NLG relies on a constantly\nevolving ecosystem of automated metrics, datasets, and human evaluation\nstandards. Due to this moving target, new models often still evaluate on\ndivergent anglo-centric corpora with well-established, but flawed, metrics.\nThis disconnect makes it challenging to identify the limitations of current\nmodels and opportunities for progress. Addressing this limitation, GEM provides\nan environment in which models can easily be applied to a wide set of tasks and\nin which evaluation strategies can be tested. Regular updates to the benchmark\nwill help NLG research become more multilingual and evolve the challenge\nalongside models. This paper serves as the description of the data for which we\nare organizing a shared task at our ACL 2021 Workshop and to which we invite\nthe entire NLG community to participate.", "published": "2021-02-02 18:42:05", "link": "http://arxiv.org/abs/2102.01672v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generacion de voces artificiales infantiles en castellano con acento\n  costarricense", "abstract": "This article evaluates a first experience of generating artificial children's\nvoices with a Costa Rican accent, using the technique of statistical parametric\nspeech synthesis based on Hidden Markov Models. The process of recording the\nvoice samples used for learning the models, the fundamentals of the technique\nused and the subjective evaluation of the results through the perception of a\ngroup of people is described. The results show that the intelligibility of the\nresults, evaluated in isolated words, is lower than the voices recorded by the\ngroup of participating children. Similarly, the detection of the age and gender\nof the speaking person is significantly affected in artificial voices, relative\nto recordings of natural voices. These results show the need to obtain larger\namounts of data, in addition to becoming a numerical reference for future\ndevelopments resulting from new data or from processes to improve results in\nthe same technique.", "published": "2021-02-02 02:12:28", "link": "http://arxiv.org/abs/2102.01692v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Autoencoder-based Fuzzy C-Means for Topic Detection", "abstract": "Topic detection is a process for determining topics from a collection of\ntextual data. One of the topic detection methods is a clustering-based method,\nwhich assumes that the centroids are topics. The clustering method has the\nadvantage that it can process data with negative representations. Therefore,\nthe clustering method allows a combination with a broader representation\nlearning method. In this paper, we adopt deep learning for topic detection by\nusing a deep autoencoder and fuzzy c-means called deep autoencoder-based fuzzy\nc-means (DFCM). The encoder of the autoencoder performs a lower-dimensional\nrepresentation learning. Fuzzy c-means groups the lower-dimensional\nrepresentation to identify the centroids. The autoencoder's decoder transforms\nback the centroids into the original representation to be interpreted as the\ntopics. Our simulation shows that DFCM improves the coherence score of\neigenspace-based fuzzy c-means (EFCM) and is comparable to the leading standard\nmethods, i.e., nonnegative matrix factorization (NMF) or latent Dirichlet\nallocation (LDA).", "published": "2021-02-02 07:41:52", "link": "http://arxiv.org/abs/2102.02636v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Internal Language Model Training for Domain-Adaptive End-to-End Speech\n  Recognition", "abstract": "The efficacy of external language model (LM) integration with existing\nend-to-end (E2E) automatic speech recognition (ASR) systems can be improved\nsignificantly using the internal language model estimation (ILME) method. In\nthis method, the internal LM score is subtracted from the score obtained by\ninterpolating the E2E score with the external LM score, during inference. To\nimprove the ILME-based inference, we propose an internal LM training (ILMT)\nmethod to minimize an additional internal LM loss by updating only the E2E\nmodel components that affect the internal LM estimation. ILMT encourages the\nE2E model to form a standalone LM inside its existing components, without\nsacrificing ASR accuracy. After ILMT, the more modular E2E model with matched\ntraining and inference criteria enables a more thorough elimination of the\nsource-domain internal LM, and therefore leads to a more effective integration\nof the target-domain external LM. Experimented with 30K-hour trained recurrent\nneural network transducer and attention-based encoder-decoder models, ILMT with\nILME-based inference achieves up to 31.5% and 11.4% relative word error rate\nreductions from standard E2E training with Shallow Fusion on out-of-domain\nLibriSpeech and in-domain Microsoft production test sets, respectively.", "published": "2021-02-02 08:15:02", "link": "http://arxiv.org/abs/2102.01380v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Inference of the Selective Auditory Attention using Sequential LMMSE\n  Estimation", "abstract": "Attentive listening in a multispeaker environment such as a cocktail party\nrequires suppression of the interfering speakers and the noise around. People\nwith normal hearing perform remarkably well in such situations. Analysis of the\ncortical signals using electroencephalography (EEG) has revealed that the EEG\nsignals track the envelope of the attended speech stronger than that of the\ninterfering speech. This has enabled the development of algorithms that can\ndecode the selective attention of a listener in controlled experimental\nsettings. However, often these algorithms require longer trial duration and\ncomputationally expensive calibration to obtain a reliable inference of\nattention. In this paper, we present a novel framework to decode the attention\nof a listener within trial durations of the order of two seconds. It comprises\nof three modules: 1) Dynamic estimation of the temporal response functions\n(TRF) in every trial using a sequential linear minimum mean squared error\n(LMMSE) estimator, 2) Extract the N1-P2 peak of the estimated TRF that serves\nas a marker related to the attentional state and 3) Obtain a probabilistic\nmeasure of the attentional state using a support vector machine followed by a\nlogistic regression. The efficacy of the proposed decoding framework was\nevaluated using EEG data collected from 27 subjects. The total number of\nelectrodes required to infer the attention was four: One for the signal\nestimation, one for the noise estimation and the other two being the reference\nand the ground electrodes. Our results make further progress towards the\nrealization of neuro-steered hearing aids.", "published": "2021-02-02 20:39:03", "link": "http://arxiv.org/abs/2102.01746v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and\n  Aggregation", "abstract": "Audio tagging is an active research area and has a wide range of\napplications. Since the release of AudioSet, great progress has been made in\nadvancing model performance, which mostly comes from the development of novel\nmodel architectures and attention modules. However, we find that appropriate\ntraining techniques are equally important for building audio tagging models\nwith AudioSet, but have not received the attention they deserve. To fill the\ngap, in this work, we present PSLA, a collection of training techniques that\ncan noticeably boost the model accuracy including ImageNet pretraining,\nbalanced sampling, data augmentation, label enhancement, model aggregation and\ntheir design choices. By training an EfficientNet with these techniques, we\nobtain a single model (with 13.6M parameters) and an ensemble model that\nachieve mean average precision (mAP) scores of 0.444 and 0.474 on AudioSet,\nrespectively, outperforming the previous best system of 0.439 with 81M\nparameters. In addition, our model also achieves a new state-of-the-art mAP of\n0.567 on FSD50K.", "published": "2021-02-02 01:00:38", "link": "http://arxiv.org/abs/2102.01243v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Attention Fusion for Target Speaker Extraction", "abstract": "Target speaker extraction, which aims at extracting a target speaker's voice\nfrom a mixture of voices using audio, visual or locational clues, has received\nmuch interest. Recently an audio-visual target speaker extraction has been\nproposed that extracts target speech by using complementary audio and visual\nclues. Although audio-visual target speaker extraction offers a more stable\nperformance than single modality methods for simulated data, its adaptation\ntowards realistic situations has not been fully explored as well as evaluations\non real recorded mixtures. One of the major issues to handle realistic\nsituations is how to make the system robust to clue corruption because in real\nrecordings both clues may not be equally reliable, e.g. visual clues may be\naffected by occlusions. In this work, we propose a novel attention mechanism\nfor multi-modal fusion and its training methods that enable to effectively\ncapture the reliability of the clues and weight the more reliable ones. Our\nproposals improve signal to distortion ratio (SDR) by 1.0 dB over conventional\nfusion mechanisms on simulated data. Moreover, we also record an audio-visual\ndataset of simultaneous speech with realistic visual clue corruption and show\nthat audio-visual target speaker extraction with our proposals successfully\nwork on real data.", "published": "2021-02-02 05:59:35", "link": "http://arxiv.org/abs/2102.01326v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
