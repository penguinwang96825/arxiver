{"title": "Exploiting Entity BIO Tag Embeddings and Multi-task Learning for\n  Relation Extraction with Imbalanced Data", "abstract": "In practical scenario, relation extraction needs to first identify entity\npairs that have relation and then assign a correct relation class. However, the\nnumber of non-relation entity pairs in context (negative instances) usually far\nexceeds the others (positive instances), which negatively affects a model's\nperformance. To mitigate this problem, we propose a multi-task architecture\nwhich jointly trains a model to perform relation identification with\ncross-entropy loss and relation classification with ranking loss. Meanwhile, we\nobserve that a sentence may have multiple entities and relation mentions, and\nthe patterns in which the entities appear in a sentence may contain useful\nsemantic information that can be utilized to distinguish between positive and\nnegative instances. Thus we further incorporate the embeddings of\ncharacter-wise/word-wise BIO tag from the named entity recognition task into\ncharacter/word embeddings to enrich the input representation. Experiment\nresults show that our proposed approach can significantly improve the\nperformance of a baseline model with more than 10% absolute increase in\nF1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and\nEnglish corpus. Moreover, BIO tag embeddings are particularly effective and can\nbe used to improve other models as well.", "published": "2019-06-21 03:25:30", "link": "http://arxiv.org/abs/1906.08931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Generative Model for Code-Switched Text", "abstract": "Code-switching, the interleaving of two or more languages within a sentence\nor discourse is pervasive in multilingual societies. Accurate language models\nfor code-switched text are critical for NLP tasks. State-of-the-art\ndata-intensive neural language models are difficult to train well from scarce\nlanguage-labeled code-switched text. A potential solution is to use deep\ngenerative models to synthesize large volumes of realistic code-switched text.\nAlthough generative adversarial networks and variational autoencoders can\nsynthesize plausible monolingual text from continuous latent space, they cannot\nadequately address code-switched text, owing to their informal style and\ncomplex interplay between the constituent languages. We introduce VACS, a novel\nvariational autoencoder architecture specifically tailored to code-switching\nphenomena. VACS encodes to and decodes from a two-level hierarchical\nrepresentation, which models syntactic contextual signals in the lower level,\nand language switching signals in the upper layer. Sampling representations\nfrom the prior and decoding them produced well-formed, diverse code-switched\nsentences. Extensive experiments show that using synthetic code-switched text\nwith natural monolingual data results in significant (33.06%) drop in\nperplexity.", "published": "2019-06-21 06:27:17", "link": "http://arxiv.org/abs/1906.08972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias in Natural Language Processing: Literature Review", "abstract": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in\npopularity, it becomes increasingly vital to recognize the role they play in\nshaping societal biases and stereotypes. Although NLP models have shown success\nin modeling various applications, they propagate and may even amplify gender\nbias found in text corpora. While the study of bias in artificial intelligence\nis not new, methods to mitigate gender bias in NLP are relatively nascent. In\nthis paper, we review contemporary studies on recognizing and mitigating gender\nbias in NLP. We discuss gender bias based on four forms of representation bias\nand analyze methods recognizing gender bias. Furthermore, we discuss the\nadvantages and drawbacks of existing gender debiasing methods. Finally, we\ndiscuss future studies for recognizing and mitigating gender bias in NLP.", "published": "2019-06-21 06:39:11", "link": "http://arxiv.org/abs/1906.08976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Adaptation of NMT for Professional Post-editors: A User\n  Study", "abstract": "A common use of machine translation in the industry is providing initial\ntranslation hypotheses, which are later supervised and post-edited by a human\nexpert. During this revision process, new bilingual data are continuously\ngenerated. Machine translation systems can benefit from these new data,\nincrementally updating the underlying models under an online learning paradigm.\nWe conducted a user study on this scenario, for a neural machine translation\nsystem. The experimentation was carried out by professional translators, with a\nvast experience in machine translation post-editing. The results showed a\nreduction in the required amount of human effort needed when post-editing the\noutputs of the system, improvements in the translation quality and a positive\nperception of the adaptive system by the users.", "published": "2019-06-21 08:10:05", "link": "http://arxiv.org/abs/1906.08996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demonstration of a Neural Machine Translation System with Online\n  Learning for Translators", "abstract": "We introduce a demonstration of our system, which implements online learning\nfor neural machine translation in a production environment. These techniques\nallow the system to continuously learn from the corrections provided by the\ntranslators. We implemented an end-to-end platform integrating our machine\ntranslation servers to one of the most common user interfaces for professional\ntranslators: SDL Trados Studio. Our objective was to save post-editing effort\nas the machine is continuously learning from human choices and adapting the\nmodels to a specific domain or user style.", "published": "2019-06-21 08:19:49", "link": "http://arxiv.org/abs/1906.09000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUNI System for the WMT19 Robustness Task", "abstract": "We present our submission to the WMT19 Robustness Task. Our baseline system\nis the Charles University (CUNI) Transformer system trained for the WMT18\nshared task on News Translation. Quantitative results show that the CUNI\nTransformer system is already far more robust to noisy input than the\nLSTM-based baseline provided by the task organizers. We further improved the\nperformance of our model by fine-tuning on the in-domain noisy data without\ninfluencing the translation quality on the news domain.", "published": "2019-06-21 17:11:46", "link": "http://arxiv.org/abs/1906.09246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SurfCon: Synonym Discovery on Privacy-Aware Clinical Data", "abstract": "Unstructured clinical texts contain rich health-related information. To\nbetter utilize the knowledge buried in clinical texts, discovering synonyms for\na medical query term has become an important task. Recent automatic synonym\ndiscovery methods leveraging raw text information have been developed. However,\nto preserve patient privacy and security, it is usually quite difficult to get\naccess to large-scale raw clinical texts. In this paper, we study a new setting\nnamed synonym discovery on privacy-aware clinical data (i.e., medical terms\nextracted from the clinical texts and their aggregated co-occurrence counts,\nwithout raw clinical texts). To solve the problem, we propose a new framework\nSurfCon that leverages two important types of information in the privacy-aware\nclinical data, i.e., the surface form information, and the global context\ninformation for synonym discovery. In particular, the surface form module\nenables us to detect synonyms that look similar while the global context module\nplays a complementary role to discover synonyms that are semantically similar\nbut in different surface forms, and both allow us to deal with the OOV query\nissue (i.e., when the query is not found in the given data). We conduct\nextensive experiments and case studies on publicly available privacy-aware\nclinical data, and show that SurfCon can outperform strong baseline methods by\nlarge margins under various settings.", "published": "2019-06-21 18:23:17", "link": "http://arxiv.org/abs/1906.09285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translating from Natural Language to SPARQL", "abstract": "SPARQL is a highly powerful query language for an ever-growing number of\nLinked Data resources and Knowledge Graphs. Using it requires a certain\nfamiliarity with the entities in the domain to be queried as well as expertise\nin the language's syntax and semantics, none of which average human web users\ncan be assumed to possess. To overcome this limitation, automatically\ntranslating natural language questions to SPARQL queries has been a vibrant\nfield of research. However, to this date, the vast success of deep learning\nmethods has not yet been fully propagated to this research problem. This paper\ncontributes to filling this gap by evaluating the utilization of eight\ndifferent Neural Machine Translation (NMT) models for the task of translating\nfrom natural language to the structured query language SPARQL. While\nhighlighting the importance of high-quantity and high-quality datasets, the\nresults show a dominance of a CNN-based architecture with a BLEU score of up to\n98 and accuracy of up to 94%.", "published": "2019-06-21 19:34:19", "link": "http://arxiv.org/abs/1906.09302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Named Entity Recognition Using Pretrained Embeddings,\n  Attention Mechanism and NCRF", "abstract": "In this paper we tackle multilingual named entity recognition task. We use\nthe BERT Language Model as embeddings with bidirectional recurrent network,\nattention, and NCRF on the top. We apply multilingual BERT only as embedder\nwithout any fine-tuning. We test out model on the dataset of the BSNLP shared\ntask, which consists of texts in Bulgarian, Czech, Polish and Russian\nlanguages.", "published": "2019-06-21 13:03:47", "link": "http://arxiv.org/abs/1906.09978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Be Consistent! Improving Procedural Text Comprehension using Label\n  Consistency", "abstract": "Our goal is procedural text comprehension, namely tracking how the properties\nof entities (e.g., their location) change with time given a procedural text\n(e.g., a paragraph about photosynthesis, a recipe). This task is challenging as\nthe world is changing throughout the text, and despite recent advances, current\nsystems still struggle with this task. Our approach is to leverage the fact\nthat, for many procedural texts, multiple independent descriptions are readily\navailable, and that predictions from them should be consistent (label\nconsistency). We present a new learning framework that leverages label\nconsistency during training, allowing consistency bias to be built into the\nmodel. Evaluation on a standard benchmark dataset for procedural text, ProPara\n(Dalvi et al., 2018), shows that our approach significantly improves prediction\nperformance (F1) over prior state-of-the-art systems.", "published": "2019-06-21 04:29:22", "link": "http://arxiv.org/abs/1906.08942v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multitask Network for Localization and Recognition of Text in Images", "abstract": "We present an end-to-end trainable multi-task network that addresses the\nproblem of lexicon-free text extraction from complex documents. This network\nsimultaneously solves the problems of text localization and text recognition\nand text segments are identified with no post-processing, cropping, or word\ngrouping. A convolutional backbone and Feature Pyramid Network are combined to\nprovide a shared representation that benefits each of three model heads: text\nlocalization, classification, and text recognition. To improve recognition\naccuracy, we describe a dynamic pooling mechanism that retains high-resolution\ninformation across all RoIs. For text recognition, we propose a convolutional\nmechanism with attention which out-performs more common recurrent\narchitectures. Our model is evaluated against benchmark datasets and comparable\nmethods and achieves high performance in challenging regimes of non-traditional\nOCR.", "published": "2019-06-21 17:25:45", "link": "http://arxiv.org/abs/1906.09266v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Identification of Tasks, Datasets, Evaluation Metrics, and Numeric\n  Scores for Scientific Leaderboards Construction", "abstract": "While the fast-paced inception of novel tasks and new datasets helps foster\nactive research in a community towards interesting directions, keeping track of\nthe abundance of research activity in different areas on different datasets is\nlikely to become increasingly difficult. The community could greatly benefit\nfrom an automatic system able to summarize scientific results, e.g., in the\nform of a leaderboard. In this paper we build two datasets and develop a\nframework (TDMS-IE) aimed at automatically extracting task, dataset, metric and\nscore from NLP papers, towards the automatic construction of leaderboards.\nExperiments show that our model outperforms several baselines by a large\nmargin. Our model is a first step towards automatic leaderboard construction,\ne.g., in the NLP domain.", "published": "2019-06-21 20:55:57", "link": "http://arxiv.org/abs/1906.09317v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward a Standard Interface for User-Defined Scheduling in OpenMP", "abstract": "Parallel loops are an important part of OpenMP programs. Efficient scheduling\nof parallel loops can improve performance of the programs. The current OpenMP\nspecification only offers three options for loop scheduling, which are\ninsufficient in certain instances. Given the large number of other possible\nscheduling strategies, it is infeasible to standardize each one. A more viable\napproach is to extend the OpenMP standard to allow for users to define loop\nscheduling strategies. The approach will enable standard-compliant\napplication-specific scheduling. This work analyzes the principal components\nrequired by user-defined scheduling and proposes two competing interfaces as\ncandidates for the OpenMP standard. We conceptually compare the two proposed\ninterfaces with respect to the three host languages of OpenMP, i.e., C, C++,\nand Fortran. These interfaces serve the OpenMP community as a basis for\ndiscussion and prototype implementation for user-defined scheduling.", "published": "2019-06-21 01:47:32", "link": "http://arxiv.org/abs/1906.08911v2", "categories": ["cs.DC", "cs.CL", "cs.PF", "cs.PL"], "primary_category": "cs.DC"}
{"title": "Meta-learning of textual representations", "abstract": "Recent progress in AutoML has lead to state-of-the-art methods (e.g.,\nAutoSKLearn) that can be readily used by non-experts to approach any supervised\nlearning problem. Whereas these methods are quite effective, they are still\nlimited in the sense that they work for tabular (matrix formatted) data only.\nThis paper describes one step forward in trying to automate the design of\nsupervised learning methods in the context of text mining. We introduce a meta\nlearning methodology for automatically obtaining a representation for text\nmining tasks starting from raw text. We report experiments considering 60\ndifferent textual representations and more than 80 text mining datasets\nassociated to a wide variety of tasks. Experimental results show the proposed\nmethodology is a promising solution to obtain highly effective off the shell\ntext classification pipelines.", "published": "2019-06-21 03:39:46", "link": "http://arxiv.org/abs/1906.08934v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Bilingual Word Embeddings Using Lexical Definitions", "abstract": "Bilingual word embeddings, which representlexicons of different languages in\na shared em-bedding space, are essential for supporting se-mantic and knowledge\ntransfers in a variety ofcross-lingual NLP tasks. Existing approachesto\ntraining bilingual word embeddings requireoften require pre-defined seed\nlexicons that areexpensive to obtain, or parallel sentences thatcomprise coarse\nand noisy alignment. In con-trast, we propose BilLex that leverages pub-licly\navailable lexical definitions for bilingualword embedding learning. Without the\nneedof predefined seed lexicons, BilLex comprisesa novel word pairing strategy\nto automati-cally identify and propagate the precise fine-grained word\nalignment from lexical defini-tions. We evaluate BilLex in word-level\nandsentence-level translation tasks, which seek tofind the cross-lingual\ncounterparts of wordsand sentences respectively.BilLex signifi-cantly\noutperforms previous embedding meth-ods on both tasks.", "published": "2019-06-21 04:14:07", "link": "http://arxiv.org/abs/1906.08939v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Survey of Recent Natural Language Interfaces for Databases", "abstract": "Over the last few years natural language interfaces (NLI) for databases have\ngained significant traction both in academia and industry. These systems use\nvery different approaches as described in recent survey papers. However, these\nsystems have not been systematically compared against a set of benchmark\nquestions in order to rigorously evaluate their functionalities and expressive\npower.\n  In this paper, we give an overview over 24 recently developed NLIs for\ndatabases. Each of the systems is evaluated using a curated list of ten sample\nquestions to show their strengths and weaknesses. We categorize the NLIs into\nfour groups based on the methodology they are using: keyword-, pattern-,\nparsing-, and grammar-based NLI. Overall, we learned that keyword-based systems\nare enough to answer simple questions. To solve more complex questions\ninvolving subqueries, the system needs to apply some sort of parsing to\nidentify structural dependencies. Grammar-based systems are overall the most\npowerful ones, but are highly dependent on their manually designed rules. In\naddition to providing a systematic analysis of the major systems, we derive\nlessons learned that are vital for designing NLIs that can answer a wide range\nof user questions.", "published": "2019-06-21 07:49:36", "link": "http://arxiv.org/abs/1906.08990v1", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in\n  End-to-End Models", "abstract": "Contextual automatic speech recognition, i.e., biasing recognition towards a\ngiven context (e.g. user's playlists, or contacts), is challenging in\nend-to-end (E2E) models. Such models maintain a limited number of candidates\nduring beam-search decoding, and have been found to recognize rare named\nentities poorly. The problem is exacerbated when biasing towards proper nouns\nin foreign languages, e.g., geographic location names, which are virtually\nunseen in training and are thus out-of-vocabulary (OOV). While grapheme or\nwordpiece E2E models might have a difficult time spelling OOV words, phonemes\nare more acoustically salient and past work has shown that E2E phoneme models\ncan better predict such words. In this work, we propose an E2E model containing\nboth English wordpieces and phonemes in the modeling space, and perform\ncontextual biasing of foreign words at the phoneme level by mapping\npronunciations of foreign words into similar English phonemes. In experimental\nevaluations, we find that the proposed approach performs 16% better than a\ngrapheme-only biasing model, and 8% better than a wordpiece-only biasing model\non a foreign place name recognition task, with only slight degradation on\nregular English tasks.", "published": "2019-06-21 19:04:39", "link": "http://arxiv.org/abs/1906.09292v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Approximating Interactive Human Evaluation with Self-Play for\n  Open-Domain Dialog Systems", "abstract": "Building an open-domain conversational agent is a challenging problem.\nCurrent evaluation methods, mostly post-hoc judgments of static conversation,\ndo not capture conversation quality in a realistic interactive context. In this\npaper, we investigate interactive human evaluation and provide evidence for its\nnecessity; we then introduce a novel, model-agnostic, and dataset-agnostic\nmethod to approximate it. In particular, we propose a self-play scenario where\nthe dialog system talks to itself and we calculate a combination of proxies\nsuch as sentiment and semantic coherence on the conversation trajectory. We\nshow that this metric is capable of capturing the human-rated quality of a\ndialog model better than any automated metric known to-date, achieving a\nsignificant Pearson correlation (r>.7, p<.05). To investigate the strengths of\nthis novel metric and interactive evaluation in comparison to state-of-the-art\nmetrics and human evaluation of static conversations, we perform extended\nexperiments with a set of models, including several that make novel\nimprovements to recent hierarchical dialog generation architectures through\nsentiment and semantic knowledge distillation on the utterance level. Finally,\nwe open-source the interactive evaluation platform we built and the dataset we\ncollected to allow researchers to efficiently deploy and evaluate dialog\nmodels.", "published": "2019-06-21 20:08:18", "link": "http://arxiv.org/abs/1906.09308v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Study of State Aliasing in Structured Prediction with RNNs", "abstract": "End-to-end reinforcement learning agents learn a state representation and a\npolicy at the same time. Recurrent neural networks (RNNs) have been trained\nsuccessfully as reinforcement learning agents in settings like dialogue that\nrequire structured prediction. In this paper, we investigate the\nrepresentations learned by RNN-based agents when trained with both policy\ngradient and value-based methods. We show through extensive experiments and\nanalysis that, when trained with policy gradient, recurrent neural networks\noften fail to learn a state representation that leads to an optimal policy in\nsettings where the same action should be taken at different states. To explain\nthis failure, we highlight the problem of state aliasing, which entails\nconflating two or more distinct states in the representation space. We\ndemonstrate that state aliasing occurs when several states share the same\noptimal action and the agent is trained via policy gradient. We characterize\nthis phenomenon through experiments on a simple maze setting and a more complex\ntext-based game, and make recommendations for training RNNs with reinforcement\nlearning.", "published": "2019-06-21 20:16:52", "link": "http://arxiv.org/abs/1906.09310v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Graph Star Net for Generalized Multi-Task Learning", "abstract": "In this work, we present graph star net (GraphStar), a novel and unified\ngraph neural net architecture which utilizes message-passing relay and\nattention mechanism for multiple prediction tasks - node classification, graph\nclassification and link prediction. GraphStar addresses many earlier challenges\nfacing graph neural nets and achieves non-local representation without\nincreasing the model depth or bearing heavy computational costs. We also\npropose a new method to tackle topic-specific sentiment analysis based on node\nclassification and text classification as graph classification. Our work shows\nthat 'star nodes' can learn effective graph-data representation and improve on\ncurrent methods for the three tasks. Specifically, for graph classification and\nlink prediction, GraphStar outperforms the current state-of-the-art models by\n2-5% on several key benchmarks.", "published": "2019-06-21 03:05:17", "link": "http://arxiv.org/abs/1906.12330v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Understanding and Classifying Cultural Music Using Melodic Features Case\n  Of Hindustani, Carnatic And Turkish Music", "abstract": "We present a melody based classification of musical styles by exploiting the\npitch and energy based characteristics derived from the audio signal. Three\nprominent musical styles were chosen which have improvisation as integral part\nwith similar melodic principles, theme, and structure of concerts namely,\nHindustani, Carnatic and Turkish music. Listeners of one or more of these\ngenres can discriminate between these based on the melodic contour alone.\nListening tests were carried out using melodic attributes alone, on similar\nmelodic pieces with respect to raga/makam, and removing any instrumentation cue\nto validate our hypothesis that style distinction is evident in the melody. Our\nmethod is based on finding a set of highly discriminatory features, derived\nfrom musicology, to capture distinct characteristics of the melodic contour.\nBehavior in terms of transitions of the pitch contour, the presence of\nmicro-tonal notes and the nature of variations in the vocal energy are\nexploited. The automatically classified style labels are found to correlate\nwell with subjective listening judgments. This was verified by using\nstatistical tests to compare the labels from subjective and objective\njudgments. The melody based features, when combined with timbre based features,\nwere seen to improve the classification performance.", "published": "2019-06-21 02:06:36", "link": "http://arxiv.org/abs/1906.08916v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Polyphonic ADSR Piano Note Transcription", "abstract": "We investigate a late-fusion approach to piano transcription, combined with a\nstrong temporal prior in the form of a handcrafted Hidden Markov Model (HMM).\nThe network architecture under consideration is compact in terms of its number\nof parameters and easy to train with gradient descent. The network outputs are\nfused over time in the final stage to obtain note segmentations, with an HMM\nwhose transition probabilities are chosen based on a model of attack, decay,\nsustain, release (ADSR) envelopes, commonly used for sound synthesis. The note\nsegments are then subject to a final binary decision rule to reject too weak\nnote segment hypotheses. We obtain state-of-the-art results on the MAPS\ndataset, and are able to outperform other approaches by a large margin, when\npredicting complete note regions from onsets to offsets.", "published": "2019-06-21 14:38:36", "link": "http://arxiv.org/abs/1906.09165v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Phoneme and Word Discovery from Multiple Speakers using\n  Double Articulation Analyzer and Neural Network with Parametric Bias", "abstract": "This paper describes a new unsupervised machine learning method for\nsimultaneous phoneme and word discovery from multiple speakers. Human infants\ncan acquire knowledge of phonemes and words from interactions with his/her\nmother as well as with others surrounding him/her. From a computational\nperspective, phoneme and word discovery from multiple speakers is a more\nchallenging problem than that from one speaker because the speech signals from\ndifferent speakers exhibit different acoustic features. This paper proposes an\nunsupervised phoneme and word discovery method that simultaneously uses\nnonparametric Bayesian double articulation analyzer (NPB-DAA) and deep sparse\nautoencoder with parametric bias in hidden layer (DSAE-PBHL). We assume that an\ninfant can recognize and distinguish speakers based on certain other features,\ne.g., visual face recognition. DSAE-PBHL is aimed to be able to subtract\nspeaker-dependent acoustic features and extract speaker-independent features.\nAn experiment demonstrated that DSAE-PBHL can subtract distributed\nrepresentations of acoustic signals, enabling extraction based on the types of\nphonemes rather than on the speakers. Another experiment demonstrated that a\ncombination of NPB-DAA and DSAE-PB outperformed the available methods in\nphoneme and word discovery tasks involving speech signals with Japanese vowel\nsequences from multiple speakers.", "published": "2019-06-21 02:24:55", "link": "http://arxiv.org/abs/1906.11049v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mirage: 2D Source Localization Using Microphone Pair Augmentation with\n  Echoes", "abstract": "It is commonly observed that acoustic echoes hurt performance of sound source\nlocalization (SSL) methods. We introduce the concept of microphone array\naugmentation with echoes (MIRAGE) and show how estimation of early-echo\ncharacteristics can in fact benefit SSL. We propose a learning-based scheme for\necho estimation combined with a physics-based scheme for echo aggregation. In a\nsimple scenario involving 2 microphones close to a reflective surface and one\nsource, we show using simulated data that the proposed approach performs\nsimilarly to a correlation-based method in azimuth estimation while retrieving\nelevation as well from 2 microphones only, an impossible task in anechoic\nsettings.", "published": "2019-06-21 06:19:27", "link": "http://arxiv.org/abs/1906.08968v1", "categories": ["eess.AS", "eess.SP", "physics.class-ph"], "primary_category": "eess.AS"}
{"title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for\n  Acoustic Modeling", "abstract": "This paper presents a method of using autoregressive neural networks for the\nacoustic modeling of singing voice synthesis (SVS). Singing voice differs from\nspeech and it contains more local dynamic movements of acoustic features, e.g.,\nvibratos. Therefore, our method adopts deep autoregressive (DAR) models to\npredict the F0 and spectral features of singing voice in order to better\ndescribe the dependencies among the acoustic features of consecutive frames.\nFor F0 modeling, discretized F0 values are used and the influences of the\nhistory length in DAR are analyzed by experiments. An F0 post-processing\nstrategy is also designed to alleviate the inconsistency between the predicted\nF0 contours and the F0 values determined by music notes. Furthermore, we extend\nthe DAR model to deal with continuous spectral features, and a prenet module\nwith self-attention layers is introduced to process historical frames.\nExperiments on a Chinese singing voice corpus demonstrate that our method using\nDARs can produce F0 contours with vibratos effectively, and can achieve better\nobjective and subjective performance than the conventional method using\nrecurrent neural networks (RNNs).", "published": "2019-06-21 06:40:06", "link": "http://arxiv.org/abs/1906.08977v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Query-based Deep Improvisation", "abstract": "In this paper we explore techniques for generating new music using a\nVariational Autoencoder (VAE) neural network that was trained on a corpus of\nspecific style. Instead of randomly sampling the latent states of the network\nto produce free improvisation, we generate new music by querying the network\nwith musical input in a style different from the training corpus. This allows\nus to produce new musical output with longer-term structure that blends aspects\nof the query to the style of the network. In order to control the level of this\nblending we add a noisy channel between the VAE encoder and decoder using\nbit-allocation algorithm from communication rate-distortion theory. Our\nexperiments provide new insight into relations between the representational and\nstructural information of latent states and the query signal, suggesting their\npossible use for composition purposes.", "published": "2019-06-21 14:09:22", "link": "http://arxiv.org/abs/1906.09155v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "The Shape of RemiXXXes to Come: Audio Texture Synthesis with\n  Time-frequency Scattering", "abstract": "This article explains how to apply time--frequency scattering, a\nconvolutional operator extracting modulations in the time--frequency domain at\ndifferent rates and scales, to the re-synthesis and manipulation of audio\ntextures. After implementing phase retrieval in the scattering network by\ngradient backpropagation, we introduce scale--rate DAFx, a class of audio\ntransformations expressed in the domain of time--frequency scattering\ncoefficients. One example of scale--rate DAFx is chirp rate inversion, which\ncauses each sonic event to be locally reversed in time while leaving the arrow\nof time globally unchanged. Over the past two years, our work has led to the\ncreation of four electroacoustic pieces: ``FAVN''; ``Modulator (Scattering\nTransform)''; ``Experimental Palimpsest''; ``Inspection''; and a remix of\nLorenzo Senni's ``XAllegroX'', released by Warp Records on a vinyl entitled\n``The Shape of RemiXXXes to Come''. The source code to reproduce experiments\nand figures is made freely available at:\nhttps://github.com/lostanlen/scattering.m. A companion website containing demos\nis at: https://lostanlen.com/pubs/dafx2019", "published": "2019-06-21 21:25:55", "link": "http://arxiv.org/abs/1906.09334v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Classical Music Prediction and Composition by means of Variational\n  Autoencoders", "abstract": "This paper proposes a new model for music prediction based on Variational\nAutoencoders (VAEs). In this work, VAEs are used in a novel way in order to\naddress two different problems: music representation into the latent space, and\nusing this representation to make predictions of the future values of the\nmusical piece. This approach was trained with different songs of a classical\ncomposer. As a result, the system can represent the music in the latent space,\nand make accurate predictions. Therefore, the system can be used to compose new\nmusic either from an existing piece or from a random starting point. An\nadditional feature of this system is that a small dataset was used for\ntraining. However, results show that the system is able to return accurate\nrepresentations and predictions in unseen data.", "published": "2019-06-21 15:44:12", "link": "http://arxiv.org/abs/1906.09972v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integration of TensorFlow based Acoustic Model with Kaldi WFST Decoder", "abstract": "While the Kaldi framework provides state-of-the-art components for speech\nrecognition like feature extraction, deep neural network (DNN)-based acoustic\nmodels, and a weighted finite state transducer (WFST)-based decoder, it is\ndifficult to implement a new flexible DNN model. By contrast, a general-purpose\ndeep learning framework, such as TensorFlow, can easily build various types of\nneural network architectures using a tensor-based computation method, but it is\ndifficult to apply them to WFST-based speech recognition. In this study, a\nTensorFlow-based acoustic model is integrated with a WFST-based Kaldi decoder\nto combine the two frameworks. The features and alignments used in Kaldi are\nconverted so they can be trained by the TensorFlow model, and the DNN-based\nacoustic model is then trained. In the integrated Kaldi decoder, the posterior\nprobabilities are calculated by querying the trained TensorFlow model, and a\nbeam search is performed to generate the lattice. The advantages of the\nproposed one-pass decoder include the application of various types of neural\nnetworks to WFST-based speech recognition and WFST-based online decoding using\na TensorFlow-based acoustic model. The TensorFlow based acoustic models trained\nusing the RM, WSJ, and LibriSpeech datasets show the same level of performance\nas the model trained using the Kaldi framework.", "published": "2019-06-21 06:54:35", "link": "http://arxiv.org/abs/1906.11018v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Span Acoustic Modelling using Raw Waveform Signals", "abstract": "Traditional automatic speech recognition (ASR) systems often use an acoustic\nmodel (AM) built on handcrafted acoustic features, such as log Mel-filter bank\n(FBANK) values. Recent studies found that AMs with convolutional neural\nnetworks (CNNs) can directly use the raw waveform signal as input. Given\nsufficient training data, these AMs can yield a competitive word error rate\n(WER) to those built on FBANK features. This paper proposes a novel multi-span\nstructure for acoustic modelling based on the raw waveform with multiple\nstreams of CNN input layers, each processing a different span of the raw\nwaveform signal. Evaluation on both the single channel CHiME4 and AMI data sets\nshow that multi-span AMs give a lower WER than FBANK AMs by an average of about\n5% (relative). Analysis of the trained multi-span model reveals that the CNNs\ncan learn filters that are rather different to the log Mel filters.\nFurthermore, the paper shows that a widely used single span raw waveform AM can\nbe improved by using a smaller CNN kernel size and increased stride to yield\nimproved WERs.", "published": "2019-06-21 06:06:00", "link": "http://arxiv.org/abs/1906.11047v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
