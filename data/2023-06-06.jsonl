{"title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive\n  Learning", "abstract": "It has always been an important yet challenging problem to control language\nmodels to avoid generating texts with undesirable attributes, such as toxic\nlanguage and unnatural repetition. We introduce Click for controllable text\ngeneration, which needs no modification to the model architecture and\nfacilitates out-of-the-box use of trained models. It employs a contrastive loss\non sequence likelihood, which fundamentally decreases the generation\nprobability of negative samples (i.e., generations with undesirable\nattributes). It also adopts a novel likelihood ranking-based strategy to\nconstruct contrastive samples from model generations. On the tasks of language\ndetoxification, sentiment steering, and repetition reduction, we show that\nClick outperforms strong baselines of controllable text generation and\ndemonstrate the superiority of Click's sample construction strategy.", "published": "2023-06-06 01:56:44", "link": "http://arxiv.org/abs/2306.03350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient and Interpretable Compressive Text Summarisation with\n  Unsupervised Dual-Agent Reinforcement Learning", "abstract": "Recently, compressive text summarisation offers a balance between the\nconciseness issue of extractive summarisation and the factual hallucination\nissue of abstractive summarisation. However, most existing compressive\nsummarisation methods are supervised, relying on the expensive effort of\ncreating a new training dataset with corresponding compressive summaries. In\nthis paper, we propose an efficient and interpretable compressive summarisation\nmethod that utilises unsupervised dual-agent reinforcement learning to optimise\na summary's semantic coverage and fluency by simulating human judgment on\nsummarisation quality. Our model consists of an extractor agent and a\ncompressor agent, and both agents have a multi-head attentional pointer-based\nstructure. The extractor agent first chooses salient sentences from a document,\nand then the compressor agent compresses these extracted sentences by selecting\nsalient words to form a summary without using reference summaries to compute\nthe summary reward. To our best knowledge, this is the first work on\nunsupervised compressive summarisation. Experimental results on three widely\nused datasets (e.g., Newsroom, CNN/DM, and XSum) show that our model achieves\npromising performance and a significant improvement on Newsroom in terms of the\nROUGE metric, as well as interpretability of semantic coverage of summarisation\nresults.", "published": "2023-06-06 05:30:49", "link": "http://arxiv.org/abs/2306.03415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Event Extraction via Structural Semantic Matching", "abstract": "Event Extraction (EE) is one of the essential tasks in information\nextraction, which aims to detect event mentions from text and find the\ncorresponding argument roles. The EE task can be abstracted as a process of\nmatching the semantic definitions and argument structures of event types with\nthe target text. This paper encodes the semantic features of event types and\nmakes structural matching with target text. Specifically, Semantic Type\nEmbedding (STE) and Dynamic Structure Encoder (DSE) modules are proposed. Also,\nthe Joint Structural Semantic Matching (JSSM) model is built to jointly perform\nevent detection and argument extraction tasks through a bidirectional attention\nlayer. The experimental results on the ACE2005 dataset indicate that our model\nachieves a significant performance improvement", "published": "2023-06-06 07:42:39", "link": "http://arxiv.org/abs/2306.03469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"A Little is Enough\": Few-Shot Quality Estimation based Corpus Filtering\n  improves Machine Translation", "abstract": "Quality Estimation (QE) is the task of evaluating the quality of a\ntranslation when reference translation is not available. The goal of QE aligns\nwith the task of corpus filtering, where we assign the quality score to the\nsentence pairs present in the pseudo-parallel corpus. We propose a Quality\nEstimation based Filtering approach to extract high-quality parallel data from\nthe pseudo-parallel corpus. To the best of our knowledge, this is a novel\nadaptation of the QE framework to extract quality parallel corpus from the\npseudo-parallel corpus. By training with this filtered corpus, we observe an\nimprovement in the Machine Translation (MT) system's performance by up to 1.8\nBLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language\npairs, over the baseline model. The baseline model is the one that is trained\non the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned\nfrom the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali\ntraining instances, shows an improvement of up to 0.6 BLEU points for\nHindi-Bengali language pair, compared to the baseline model. This demonstrates\nthe promise of transfer learning in the setting under discussion. QE systems\ntypically require in the order of (7K-25K) of training data. Our Hindi-Bengali\nQE is trained on only 500 instances of training that is 1/40th of the normal\nrequirement and achieves comparable performance. All the scripts and datasets\nutilized in this study will be publicly available.", "published": "2023-06-06 08:53:01", "link": "http://arxiv.org/abs/2306.03507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciLit: A Platform for Joint Scientific Literature Discovery,\n  Summarization and Citation Generation", "abstract": "Scientific writing involves retrieving, summarizing, and citing relevant\npapers, which can be time-consuming processes in large and rapidly evolving\nfields. By making these processes inter-operable, natural language processing\n(NLP) provides opportunities for creating end-to-end assistive writing tools.\nWe propose SciLit, a pipeline that automatically recommends relevant papers,\nextracts highlights, and suggests a reference sentence as a citation of a\npaper, taking into consideration the user-provided context and keywords. SciLit\nefficiently recommends papers from large databases of hundreds of millions of\npapers using a two-stage pre-fetching and re-ranking literature search system\nthat flexibly deals with addition and removal of a paper database. We provide a\nconvenient user interface that displays the recommended papers as extractive\nsummaries and that offers abstractively-generated citing sentences which are\naligned with the provided context and which mention the chosen keyword(s). Our\nassistive tool for literature discovery and scientific writing is available at\nhttps://scilit.vercel.app", "published": "2023-06-06 09:34:45", "link": "http://arxiv.org/abs/2306.03535v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUE: An Uncertainty Interpretation Framework for Text Classifiers Built\n  on Pre-Trained Language Models", "abstract": "Text classifiers built on Pre-trained Language Models (PLMs) have achieved\nremarkable progress in various tasks including sentiment analysis, natural\nlanguage inference, and question-answering. However, the occurrence of\nuncertain predictions by these classifiers poses a challenge to their\nreliability when deployed in practical applications. Much effort has been\ndevoted to designing various probes in order to understand what PLMs capture.\nBut few studies have delved into factors influencing PLM-based classifiers'\npredictive uncertainty. In this paper, we propose a novel framework, called\nCUE, which aims to interpret uncertainties inherent in the predictions of\nPLM-based models. In particular, we first map PLM-encoded representations to a\nlatent space via a variational auto-encoder. We then generate text\nrepresentations by perturbing the latent space which causes fluctuation in\npredictive uncertainty. By comparing the difference in predictive uncertainty\nbetween the perturbed and the original text representations, we are able to\nidentify the latent dimensions responsible for uncertainty and subsequently\ntrace back to the input features that contribute to such uncertainty. Our\nextensive experiments on four benchmark datasets encompassing linguistic\nacceptability classification, emotion classification, and natural language\ninference show the feasibility of our proposed framework. Our source code is\navailable at: https://github.com/lijiazheng99/CUE.", "published": "2023-06-06 11:37:46", "link": "http://arxiv.org/abs/2306.03598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting knowledge into language generation: a case study in\n  auto-charting after-visit care instructions from medical dialogue", "abstract": "Factual correctness is often the limiting factor in practical applications of\nnatural language generation in high-stakes domains such as healthcare. An\nessential requirement for maintaining factuality is the ability to deal with\nrare tokens. This paper focuses on rare tokens that appear in both the source\nand the reference sequences, and which, when missed during generation, decrease\nthe factual correctness of the output text. For high-stake domains that are\nalso knowledge-rich, we show how to use knowledge to (a) identify which rare\ntokens that appear in both source and reference are important and (b) uplift\ntheir conditional probability. We introduce the ``utilization rate'' that\nencodes knowledge and serves as a regularizer by maximizing the marginal\nprobability of selected tokens. We present a study in a knowledge-rich domain\nof healthcare, where we tackle the problem of generating after-visit care\ninstructions based on patient-doctor dialogues. We verify that, in our dataset,\nspecific medical concepts with high utilization rates are underestimated by\nconventionally trained sequence-to-sequence models. We observe that correcting\nthis with our approach to knowledge injection reduces the uncertainty of the\nmodel as well as improves factuality and coherence without negatively impacting\nfluency.", "published": "2023-06-06 13:13:27", "link": "http://arxiv.org/abs/2306.03652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Difference of BERT-style and CLIP-style Text Encoders", "abstract": "Masked language modeling (MLM) has been one of the most popular pretraining\nrecipes in natural language processing, e.g., BERT, one of the representative\nmodels. Recently, contrastive language-image pretraining (CLIP) has also\nattracted attention, especially its vision models that achieve excellent\nperformance on a broad range of vision tasks. However, few studies are\ndedicated to studying the text encoders learned by CLIP. In this paper, we\nanalyze the difference between BERT-style and CLIP-style text encoders from\nthree experiments: (i) general text understanding, (ii) vision-centric text\nunderstanding, and (iii) text-to-image generation. Experimental analyses show\nthat although CLIP-style text encoders underperform BERT-style ones for general\ntext understanding tasks, they are equipped with a unique ability, i.e.,\nsynesthesia, for the cross-modal association, which is more similar to the\nsenses of humans.", "published": "2023-06-06 13:41:09", "link": "http://arxiv.org/abs/2306.03678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Linguistic Pressure for Uniform Information Density in Word\n  Order", "abstract": "While natural languages differ widely in both canonical word order and word\norder flexibility, their word orders still follow shared cross-linguistic\nstatistical patterns, often attributed to functional pressures. In the effort\nto identify these pressures, prior work has compared real and counterfactual\nword orders. Yet one functional pressure has been overlooked in such\ninvestigations: the uniform information density (UID) hypothesis, which holds\nthat information should be spread evenly throughout an utterance. Here, we ask\nwhether a pressure for UID may have influenced word order patterns\ncross-linguistically. To this end, we use computational models to test whether\nreal orders lead to greater information uniformity than counterfactual orders.\nIn our empirical study of 10 typologically diverse languages, we find that: (i)\namong SVO languages, real word orders consistently have greater uniformity than\nreverse word orders, and (ii) only linguistically implausible counterfactual\norders consistently exceed the uniformity of real orders. These findings are\ncompatible with a pressure for information uniformity in the development and\nusage of natural languages.", "published": "2023-06-06 14:52:15", "link": "http://arxiv.org/abs/2306.03734v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinRED: A Dataset for Relation Extraction in Financial Domain", "abstract": "Relation extraction models trained on a source domain cannot be applied on a\ndifferent target domain due to the mismatch between relation sets. In the\ncurrent literature, there is no extensive open-source relation extraction\ndataset specific to the finance domain. In this paper, we release FinRED, a\nrelation extraction dataset curated from financial news and earning call\ntranscripts containing relations from the finance domain. FinRED has been\ncreated by mapping Wikidata triplets using distance supervision method. We\nmanually annotate the test data to ensure proper evaluation. We also experiment\nwith various state-of-the-art relation extraction models on this dataset to\ncreate the benchmark. We see a significant drop in their performance on FinRED\ncompared to the general relation extraction datasets which tells that we need\nbetter models for financial relation extraction.", "published": "2023-06-06 14:52:47", "link": "http://arxiv.org/abs/2306.03736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Hybrid Linguistic Features for Turkish Text Readability", "abstract": "This paper presents the first comprehensive study on automatic readability\nassessment of Turkish texts. We combine state-of-the-art neural network models\nwith linguistic features at lexical, morphosyntactic, syntactic and discourse\nlevels to develop an advanced readability tool. We evaluate the effectiveness\nof traditional readability formulas compared to modern automated methods and\nidentify key linguistic features that determine the readability of Turkish\ntexts.", "published": "2023-06-06 15:32:22", "link": "http://arxiv.org/abs/2306.03774v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language\n  Models", "abstract": "Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt \"Let's think step by step\",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}", "published": "2023-06-06 15:43:16", "link": "http://arxiv.org/abs/2306.03799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Key Points to Key Point Hierarchy: Structured and Expressive\n  Opinion Summarization", "abstract": "Key Point Analysis (KPA) has been recently proposed for deriving fine-grained\ninsights from collections of textual comments. KPA extracts the main points in\nthe data as a list of concise sentences or phrases, termed key points, and\nquantifies their prevalence. While key points are more expressive than word\nclouds and key phrases, making sense of a long, flat list of key points, which\noften express related ideas in varying levels of granularity, may still be\nchallenging. To address this limitation of KPA, we introduce the task of\norganizing a given set of key points into a hierarchy, according to their\nspecificity. Such hierarchies may be viewed as a novel type of Textual\nEntailment Graph. We develop ThinkP, a high quality benchmark dataset of key\npoint hierarchies for business and product reviews, obtained by consolidating\nmultiple annotations. We compare different methods for predicting pairwise\nrelations between key points, and for inferring a hierarchy from these pairwise\npredictions. In particular, for the task of computing pairwise key point\nrelations, we achieve significant gains over existing strong baselines by\napplying directional distributional similarity methods to a novel\ndistributional representation of key points, and further boost performance via\nweak supervision.", "published": "2023-06-06 16:45:44", "link": "http://arxiv.org/abs/2306.03853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal interventions expose implicit situation models for commonsense\n  language understanding", "abstract": "Accounts of human language processing have long appealed to implicit\n``situation models'' that enrich comprehension with relevant but unstated world\nknowledge. Here, we apply causal intervention techniques to recent transformer\nmodels to analyze performance on the Winograd Schema Challenge (WSC), where a\nsingle context cue shifts interpretation of an ambiguous pronoun. We identify a\nrelatively small circuit of attention heads that are responsible for\npropagating information from the context word that guides which of the\ncandidate noun phrases the pronoun ultimately attends to. We then compare how\nthis circuit behaves in a closely matched ``syntactic'' control where the\nsituation model is not strictly necessary. These analyses suggest distinct\npathways through which implicit situation models are constructed to guide\npronoun resolution.", "published": "2023-06-06 17:36:43", "link": "http://arxiv.org/abs/2306.03882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns", "abstract": "Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n  Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging\n34.2% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nperformance for neo-pronouns, but only to 64.7% even with 20 shots. We release\nthe full dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/", "published": "2023-06-06 18:27:52", "link": "http://arxiv.org/abs/2306.03950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECQED: Emotion-Cause Quadruple Extraction in Dialogs", "abstract": "The existing emotion-cause pair extraction (ECPE) task, unfortunately,\nignores extracting the emotion type and cause type, while these fine-grained\nmeta-information can be practically useful in real-world applications, i.e.,\nchat robots and empathic dialog generation. Also the current ECPE is limited to\nthe scenario of single text piece, while neglecting the studies at dialog level\nthat should have more realistic values. In this paper, we extend the ECPE task\nwith a broader definition and scenario, presenting a new task, Emotion-Cause\nQuadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause\nutterance pairs and emotion and cause types. We present an ECQED model based on\na structural and semantic heterogeneous graph as well as a parallel grid\ntagging scheme, which advances in effectively incorporating the dialog context\nstructure, meanwhile solving the challenging overlapped quadruple issue. Via\nexperiments we show that introducing the fine-grained emotion and cause\nfeatures evidently helps better dialog generation. Also our proposed ECQED\nsystem shows exceptional superiority over baselines on both the emotion-cause\nquadruple or pair extraction tasks, meanwhile being highly efficient.", "published": "2023-06-06 19:04:30", "link": "http://arxiv.org/abs/2306.03969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named\n  Entity Recognition", "abstract": "Few-shot named entity recognition (NER) exploits limited annotated instances\nto identify named mentions. Effectively transferring the internal or external\nresources thus becomes the key to few-shot NER. While the existing prompt\ntuning methods have shown remarkable few-shot performances, they still fail to\nmake full use of knowledge. In this work, we investigate the integration of\nrich knowledge to prompt tuning for stronger few-shot NER. We propose\nincorporating the deep prompt tuning framework with threefold knowledge (namely\nTKDP), including the internal 1) context knowledge and the external 2) label\nknowledge & 3) sememe knowledge. TKDP encodes the three feature sources and\nincorporates them into the soft prompt embeddings, which are further injected\ninto an existing pre-trained language model to facilitate predictions. On five\nbenchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1\nover the raw deep prompt method, and significantly outperforms 8\nstrong-performing baseline systems in 5-/10-/20-shot settings, showing great\npotential in few-shot NER. Our TKDP can be broadly adapted to other few-shot\ntasks without effort.", "published": "2023-06-06 19:11:59", "link": "http://arxiv.org/abs/2306.03974v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Conversation Discourse for Dialogue Disentanglement", "abstract": "Dialogue disentanglement aims to detach the chronologically ordered\nutterances into several independent sessions. Conversation utterances are\nessentially organized and described by the underlying discourse, and thus\ndialogue disentanglement requires the full understanding and harnessing of the\nintrinsic discourse attribute. In this paper, we propose enhancing dialogue\ndisentanglement by taking full advantage of the dialogue discourse\ncharacteristics. First of all, in feature encoding stage, we construct the\nheterogeneous graph representations to model the various dialogue-specific\ndiscourse structural features, including the static speaker-role structures\n(i.e., speaker-utterance and speaker-mentioning structure) and the dynamic\ncontextual structures (i.e., the utterance-distance and partial-replying\nstructure). We then develop a structure-aware framework to integrate the rich\nstructural features for better modeling the conversational semantic context.\nSecond, in model learning stage, we perform optimization with a hierarchical\nranking loss mechanism, which groups dialogue utterances into different\ndiscourse levels and carries training covering pair-wise and session-wise\nlevels hierarchically. Third, in inference stage, we devise an easy-first\ndecoding algorithm, which performs utterance pairing under the easy-to-hard\nmanner with a global context, breaking the constraint of traditional sequential\ndecoding order. On two benchmark datasets, our overall system achieves new\nstate-of-the-art performances on all evaluations. In-depth analyses further\ndemonstrate the efficacy of each proposed idea and also reveal how our methods\nhelp advance the task. Our work has great potential to facilitate broader\nmulti-party multi-thread dialogue applications.", "published": "2023-06-06 19:17:47", "link": "http://arxiv.org/abs/2306.03975v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "B\u00fcy\u00fck dil modellerinin T\u00fcrk\u00e7e verisetleri ile\n  e\u011fitilmesi ve ince ayarlanmas\u0131", "abstract": "Large language models have advanced enormously, gained vast attraction and\nare having a phase of intensed research. Some of the developed models and\ntraining datasets have been made open-accessible. Hence these may be further\nfine-tuned with some techniques to obtain specialized models for specific\ntasks. When it comes to Turkish language, open-access models do not provide\nsatisfactory coverage. This is also observed over published datasets. In this\nwork, we propose some ideas to mitigate this issue: creating large Turkish\ndatasets, training LLMs with these and fine-tuning pre-trained models with\nTurkish inputs. We report our findings on Turkish-based trainings with the\nproblems encountered along the way. We conclude with outcomes of these\nexperiments and propose ideas for further works.\n  --\n  B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk\nilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir\nd\\\"onemdedirler. Geli\\c{s}tirilen modeller ve e\\u{g}itimde kullanilan\nverisetlerinden bazilari a\\c{c}ik eri\\c{s}imli olarak sunulmaktadir. B\\\"oylece\nince ayarlama teknikleri uygulayarak \\\"ozelle\\c{s}mi\\c{s} g\\\"orevler i\\c{c}in\n\\c{c}ali\\c{s}abilir modeller elde edilmektedir. T\\\"urk\\c{c}e s\\\"oz konusu\noldu\\u{g}unda bu modellerinin kapsayicili\\u{g}i yeterli d\\\"uzeyde de\\u{g}ildir.\nBu durum, yayimlanan verisetlerinde de g\\\"ozlemlenebilir. Bunu a\\c{s}manin\nyollari T\\\"urk\\c{c}e i\\c{c}erikli b\\\"uy\\\"uk verisetlerinin olu\\c{s}turulmasi,\nb\\\"uy\\\"uk dil modellerinin bunlarla e\\u{g}itilmesi ve \\\"onceden\ne\\u{g}itilmi\\c{s} modellerin T\\\"urk\\c{c}e girdilerle ince ayarlanmalari\nolabilir. Bu \\c{c}ali\\c{s}mada a\\c{c}ik eri\\c{s}imli dil modelleri ve\nverisetleri \\\"uzerinde durulmakta ve T\\\"urk\\c{c}e temelli bazi deneyler,\nkar\\c{s}ila\\c{s}ilan sorunlar ve sonu\\c{c}lar irdelenmektedir.", "published": "2023-06-06 19:31:08", "link": "http://arxiv.org/abs/2306.03978v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Finance: From Transformers Back to eXplainable\n  Lexicons (XLex)", "abstract": "Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.", "published": "2023-06-06 20:19:33", "link": "http://arxiv.org/abs/2306.03997v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Reader Engagement in Literary Fiction through Eye\n  Tracking and Linguistic Features", "abstract": "Capturing readers' engagement in fiction is a challenging but important\naspect of narrative understanding. In this study, we collected 23 readers'\nreactions to 2 short stories through eye tracking, sentence-level annotations,\nand an overall engagement scale survey. We analyzed the significance of various\nqualities of the text in predicting how engaging a reader is likely to find it.\nAs enjoyment of fiction is highly contextual, we also investigated individual\ndifferences in our data. Furthering our understanding of what captivates\nreaders in fiction will help better inform models used in creative narrative\ngeneration and collaborative writing tools.", "published": "2023-06-06 22:14:59", "link": "http://arxiv.org/abs/2306.04043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing\n  Pre-Trained Language Models", "abstract": "The increasingly large size of modern pretrained language models not only\nmakes them inherit more human-like biases from the training corpora, but also\nmakes it computationally expensive to mitigate such biases. In this paper, we\ninvestigate recent parameter-efficient methods in combination with\ncounterfactual data augmentation (CDA) for bias mitigation. We conduct\nextensive experiments with prefix tuning, prompt tuning, and adapter tuning on\ndifferent language models and bias types to evaluate their debiasing\nperformance and abilities to preserve the internal knowledge of a pre-trained\nmodel. We find that the parameter-efficient methods (i) are effective in\nmitigating gender bias, where adapter tuning is consistently the most effective\none and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less\neffective when it comes to racial and religious bias, which may be attributed\nto the limitations of CDA, and (iii) can perform similarly to or sometimes\nbetter than full fine-tuning with improved time and memory efficiency, as well\nas maintain the internal knowledge in BERT and GPT-2, evaluated via fact\nretrieval and downstream fine-tuning.", "published": "2023-06-06 23:56:18", "link": "http://arxiv.org/abs/2306.04067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good is the Model in Model-in-the-loop Event Coreference Resolution\n  Annotation?", "abstract": "Annotating cross-document event coreference links is a time-consuming and\ncognitively demanding task that can compromise annotation quality and\nefficiency. To address this, we propose a model-in-the-loop annotation approach\nfor event coreference resolution, where a machine learning model suggests\nlikely corefering event pairs only. We evaluate the effectiveness of this\napproach by first simulating the annotation process and then, using a novel\nannotator-centric Recall-Annotation effort trade-off metric, we compare the\nresults of various underlying models and datasets. We finally present a method\nfor obtaining 97\\% recall while substantially reducing the workload required by\na fully manual annotation process. Code and data can be found at\nhttps://github.com/ahmeshaf/model_in_coref", "published": "2023-06-06 18:06:24", "link": "http://arxiv.org/abs/2306.05434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware\n  Conversational Agents for Engaging Dialogue", "abstract": "This paper presents a method for building a personalized open-domain dialogue\nsystem to address the WWH (WHAT, WHEN, and HOW) problem for natural response\ngeneration in a commercial setting, where personalized dialogue responses are\nheavily interleaved with casual response turns. The proposed approach involves\nweighted dataset blending, negative persona information augmentation methods,\nand the design of personalized conversation datasets to address the challenges\nof WWH in personalized, open-domain dialogue systems. Our work effectively\nbalances dialogue fluency and tendency to ground, while also introducing a\nresponse-type label to improve the controllability and explainability of the\ngrounded responses. The combination of these methods leads to more fluent\nconversations, as evidenced by subjective human evaluations as well as\nobjective evaluations.", "published": "2023-06-06 02:28:38", "link": "http://arxiv.org/abs/2306.03361v3", "categories": ["cs.CL", "cs.AI", "I.2.1, I.2.7"], "primary_category": "cs.CL"}
{"title": "TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision", "abstract": "End-to-end text spotting is a vital computer vision task that aims to\nintegrate scene text detection and recognition into a unified framework.\nTypical methods heavily rely on Region-of-Interest (RoI) operations to extract\nlocal features and complex post-processing steps to produce final predictions.\nTo address these limitations, we propose TextFormer, a query-based end-to-end\ntext spotter with Transformer architecture. Specifically, using query embedding\nper text instance, TextFormer builds upon an image encoder and a text decoder\nto learn a joint semantic understanding for multi-task modeling. It allows for\nmutual training and optimization of classification, segmentation, and\nrecognition branches, resulting in deeper feature sharing without sacrificing\nflexibility or simplicity. Additionally, we design an Adaptive Global\naGgregation (AGG) module to transfer global features into sequential features\nfor reading arbitrarily-shaped texts, which overcomes the sub-optimization\nproblem of RoI operations. Furthermore, potential corpus information is\nutilized from weak annotations to full labels through mixed supervision,\nfurther improving text detection and end-to-end text spotting results.\nExtensive experiments on various bilingual (i.e., English and Chinese)\nbenchmarks demonstrate the superiority of our method. Especially on TDA-ReCTS\ndataset, TextFormer surpasses the state-of-the-art method in terms of 1-NED by\n13.2%.", "published": "2023-06-06 03:37:41", "link": "http://arxiv.org/abs/2306.03377v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TwistList: Resources and Baselines for Tongue Twister Generation", "abstract": "Previous work in phonetically-grounded language generation has mainly focused\non domains such as lyrics and poetry. In this paper, we present work on the\ngeneration of tongue twisters - a form of language that is required to be\nphonetically conditioned to maximise sound overlap, whilst maintaining semantic\nconsistency with an input topic, and still being grammatically correct. We\npresent \\textbf{TwistList}, a large annotated dataset of tongue twisters,\nconsisting of 2.1K+ human-authored examples. We additionally present several\nbenchmark systems (referred to as TwisterMisters) for the proposed task of\ntongue twister generation, including models that both do and do not require\ntraining on in-domain data. We present the results of automatic and human\nevaluation to demonstrate the performance of existing mainstream pre-trained\nmodels in this task with limited (or no) task specific training and data, and\nno explicit phonetic knowledge. We find that the task of tongue twister\ngeneration is challenging for models under these conditions, yet some models\nare still capable of generating acceptable examples of this language type.", "published": "2023-06-06 07:20:51", "link": "http://arxiv.org/abs/2306.03457v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Putting Humans in the Image Captioning Loop", "abstract": "Image Captioning (IC) models can highly benefit from human feedback in the\ntraining process, especially in cases where data is limited. We present\nwork-in-progress on adapting an IC system to integrate human feedback, with the\ngoal to make it easily adaptable to user-specific data. Our approach builds on\na base IC model pre-trained on the MS COCO dataset, which generates captions\nfor unseen images. The user will then be able to offer feedback on the image\nand the generated/predicted caption, which will be augmented to create\nadditional training instances for the adaptation of the model. The additional\ninstances are integrated into the model using step-wise updates, and a sparse\nmemory replay component is used to avoid catastrophic forgetting. We hope that\nthis approach, while leading to improved results, will also result in\ncustomizable IC models.", "published": "2023-06-06 07:50:46", "link": "http://arxiv.org/abs/2306.03476v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SciCap+: A Knowledge Augmented Dataset to Study the Challenges of\n  Scientific Figure Captioning", "abstract": "In scholarly documents, figures provide a straightforward way of\ncommunicating scientific findings to readers. Automating figure caption\ngeneration helps move model understandings of scientific documents beyond text\nand will help authors write informative captions that facilitate communicating\nscientific findings. Unlike previous studies, we reframe scientific figure\ncaptioning as a knowledge-augmented image captioning task that models need to\nutilize knowledge embedded across modalities for caption generation. To this\nend, we extended the large-scale SciCap\ndataset~\\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes\nmention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we\nconduct experiments with the M4C-Captioner (a multimodal transformer-based\nmodel with a pointer network) as a baseline for our study. Our results indicate\nthat mention-paragraphs serves as additional context knowledge, which\nsignificantly boosts the automatic standard image caption evaluation scores\ncompared to the figure-only baselines. Human evaluations further reveal the\nchallenges of generating figure captions that are informative to readers. The\ncode and SciCap+ dataset will be publicly available at\nhttps://github.com/ZhishenYang/scientific_figure_captioning_dataset", "published": "2023-06-06 08:16:16", "link": "http://arxiv.org/abs/2306.03491v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Adaptable and Interactive Image Captioning with Data\n  Augmentation and Episodic Memory", "abstract": "Interactive machine learning (IML) is a beneficial learning paradigm in cases\nof limited data availability, as human feedback is incrementally integrated\ninto the training process. In this paper, we present an IML pipeline for image\ncaptioning which allows us to incrementally adapt a pre-trained image\ncaptioning model to a new data distribution based on user input. In order to\nincorporate user input into the model, we explore the use of a combination of\nsimple data augmentation methods to obtain larger data batches for each newly\nannotated data instance and implement continual learning methods to prevent\ncatastrophic forgetting from repeated updates. For our experiments, we split a\ndomain-specific image captioning dataset, namely VizWiz, into non-overlapping\nparts to simulate an incremental input flow for continually adapting the model\nto new data. We find that, while data augmentation worsens results, even when\nrelatively small amounts of data are available, episodic memory is an effective\nstrategy to retain knowledge from previously seen clusters.", "published": "2023-06-06 08:38:10", "link": "http://arxiv.org/abs/2306.03500v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Take the Hint: Improving Arabic Diacritization with\n  Partially-Diacritized Text", "abstract": "Automatic Arabic diacritization is useful in many applications, ranging from\nreading support for language learners to accurate pronunciation predictor for\ndownstream tasks like speech synthesis. While most of the previous works\nfocused on models that operate on raw non-diacritized text, production systems\ncan gain accuracy by first letting humans partly annotate ambiguous words. In\nthis paper, we propose 2SDiac, a multi-source model that can effectively\nsupport optional diacritics in input to inform all predictions. We also\nintroduce Guided Learning, a training scheme to leverage given diacritics in\ninput with different levels of random masking. We show that the provided hints\nduring test affect more output positions than those annotated. Moreover,\nexperiments on two common benchmarks show that our approach i) greatly\noutperforms the baseline also when evaluated on non-diacritized text; and ii)\nachieves state-of-the-art results while reducing the parameter count by over\n60%.", "published": "2023-06-06 10:18:17", "link": "http://arxiv.org/abs/2306.03557v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language acquisition: do children and language models follow similar\n  learning stages?", "abstract": "During language acquisition, children follow a typical sequence of learning\nstages, whereby they first learn to categorize phonemes before they develop\ntheir lexicon and eventually master increasingly complex syntactic structures.\nHowever, the computational principles that lead to this learning trajectory\nremain largely unknown. To investigate this, we here compare the learning\ntrajectories of deep language models to those of children. Specifically, we\ntest whether, during its training, GPT-2 exhibits stages of language\nacquisition comparable to those observed in children aged between 18 months and\n6 years. For this, we train 48 GPT-2 models from scratch and evaluate their\nsyntactic and semantic abilities at each training step, using 96 probes curated\nfrom the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these\nevaluations with the behavior of 54 children during language production. Our\nanalyses reveal three main findings. First, similarly to children, the language\nmodels tend to learn linguistic skills in a systematic order. Second, this\nlearning scheme is parallel: the language tasks that are learned last improve\nfrom the very first training steps. Third, some - but not all - learning stages\nare shared between children and these language models. Overall, these results\nshed new light on the principles of language acquisition, and highlight\nimportant divergences in how humans and modern algorithms learn to process\nnatural language.", "published": "2023-06-06 11:08:20", "link": "http://arxiv.org/abs/2306.03586v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Quantum-Cognitively Inspired Sentiment Analysis Models", "abstract": "Quantum theory, originally proposed as a physical theory to describe the\nmotions of microscopic particles, has been applied to various non-physics\ndomains involving human cognition and decision-making that are inherently\nuncertain and exhibit certain non-classical, quantum-like characteristics.\nSentiment analysis is a typical example of such domains. In the last few years,\nby leveraging the modeling power of quantum probability (a non-classical\nprobability stemming from quantum mechanics methodology) and deep neural\nnetworks, a range of novel quantum-cognitively inspired models for sentiment\nanalysis have emerged and performed well. This survey presents a timely\noverview of the latest developments in this fascinating cross-disciplinary\narea. We first provide a background of quantum probability and quantum\ncognition at a theoretical level, analyzing their advantages over classical\ntheories in modeling the cognitive aspects of sentiment analysis. Then, recent\nquantum-cognitively inspired models are introduced and discussed in detail,\nfocusing on how they approach the key challenges of the sentiment analysis\ntask. Finally, we discuss the limitations of the current research and highlight\nfuture research directions.", "published": "2023-06-06 11:54:48", "link": "http://arxiv.org/abs/2306.03608v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Convergence and Diversity in the Control Hierarchy", "abstract": "Weir has defined a hierarchy of language classes whose second member\n($\\mathcal{L}_2$) is generated by tree-adjoining grammars (TAG), linear indexed\ngrammars (LIG), combinatory categorial grammars, and head grammars. The\nhierarchy is obtained using the mechanism of control, and $\\mathcal{L}_2$ is\nobtained using a context-free grammar (CFG) whose derivations are controlled by\nanother CFG. We adapt Weir's definition of a controllable CFG to give a\ndefinition of controllable pushdown automata (PDAs). This yields three new\ncharacterizations of $\\mathcal{L}_2$ as the class of languages generated by\nPDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We\nshow that these four formalisms are not only weakly equivalent but equivalent\nin a stricter sense that we call d-weak equivalence. Furthermore, using an even\nstricter notion of equivalence called d-strong equivalence, we make precise the\nintuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an\nembedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this\nfamily, a CFG controlling a PDA, does not correspond to any formalism we know\nof, so we invent one and call it a Pushdown Adjoining Automaton.", "published": "2023-06-06 12:30:29", "link": "http://arxiv.org/abs/2306.03628v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm,\n  Sentiment and Emotion Analysis", "abstract": "Sarcasm, sentiment, and emotion are three typical kinds of spontaneous\naffective responses of humans to external events and they are tightly\nintertwined with each other. Such events may be expressed in multiple\nmodalities (e.g., linguistic, visual and acoustic), e.g., multi-modal\nconversations. Joint analysis of humans' multi-modal sarcasm, sentiment, and\nemotion is an important yet challenging topic, as it is a complex cognitive\nprocess involving both cross-modality interaction and cross-affection\ncorrelation. From the probability theory perspective, cross-affection\ncorrelation also means that the judgments on sarcasm, sentiment, and emotion\nare incompatible. However, this exposed phenomenon cannot be sufficiently\nmodelled by classical probability theory due to its assumption of\ncompatibility. Neither do the existing approaches take it into consideration.\nIn view of the recent success of quantum probability (QP) in modeling human\ncognition, particularly contextual incompatible decision making, we take the\nfirst step towards introducing QP into joint multi-modal sarcasm, sentiment,\nand emotion analysis. Specifically, we propose a QUantum probabIlity driven\nmulti-modal sarcasm, sEntiment and emoTion analysis framework, termed QUIET.\nExtensive experiments on two datasets and the results show that the\neffectiveness and advantages of QUIET in comparison with a wide range of the\nstate-of-the-art baselines. We also show the great potential of QP in\nmulti-affect analysis.", "published": "2023-06-06 13:08:22", "link": "http://arxiv.org/abs/2306.03650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effectiveness of Natural Language Inference for Hate\n  Speech Detection in Languages with Limited Labeled Data", "abstract": "Most research on hate speech detection has focused on English where a\nsizeable amount of labeled training data is available. However, to expand hate\nspeech detection into more languages, approaches that require minimal training\ndata are needed. In this paper, we test whether natural language inference\n(NLI) models which perform well in zero- and few-shot settings can benefit hate\nspeech detection performance in scenarios where only a limited amount of\nlabeled data is available in the target language. Our evaluation on five\nlanguages demonstrates large performance improvements of NLI fine-tuning over\ndirect fine-tuning in the target language. However, the effectiveness of\nprevious work that proposed intermediate fine-tuning on English data is hard to\nmatch. Only in settings where the English training data does not match the test\ndomain, can our customised NLI-formulation outperform intermediate fine-tuning\non English. Based on our extensive experiments, we propose a set of\nrecommendations for hate speech detection in languages where minimal labeled\ntraining data is available.", "published": "2023-06-06 14:40:41", "link": "http://arxiv.org/abs/2306.03722v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Novel Approach To User Agent String Parsing For Vulnerability Analysis\n  Using Mutli-Headed Attention", "abstract": "The increasing reliance on the internet has led to the proliferation of a\ndiverse set of web-browsers and operating systems (OSs) capable of browsing the\nweb. User agent strings (UASs) are a component of web browsing that are\ntransmitted with every Hypertext Transfer Protocol (HTTP) request. They contain\ninformation about the client device and software, which is used by web servers\nfor various purposes such as content negotiation and security. However, due to\nthe proliferation of various browsers and devices, parsing UASs is a\nnon-trivial task due to a lack of standardization of UAS formats. Current\nrules-based approaches are often brittle and can fail when encountering such\nnon-standard formats. In this work, a novel methodology for parsing UASs using\nMulti-Headed Attention Based transformers is proposed. The proposed methodology\nexhibits strong performance in parsing a variety of UASs with differing\nformats. Furthermore, a framework to utilize parsed UASs to estimate the\nvulnerability scores for large sections of publicly visible IT networks or\nregions is also discussed. The methodology present here can also be easily\nextended or deployed for real-time parsing of logs in enterprise settings.", "published": "2023-06-06 14:49:25", "link": "http://arxiv.org/abs/2306.03733v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Iterative Translation Refinement with Large Language Models", "abstract": "We propose iteratively prompting a large language model to self-correct a\ntranslation, with inspiration from their strong language understanding and\ntranslation capability as well as a human-like translation approach.\nInterestingly, multi-turn querying reduces the output's string-based metric\nscores, but neural metrics suggest comparable or improved quality. Human\nevaluations indicate better fluency and naturalness compared to initial\ntranslations and even human references, all while maintaining quality. Ablation\nstudies underscore the importance of anchoring the refinement to the source and\na reasonable seed translation for quality considerations. We also discuss the\nchallenges in evaluation and relation to human performance and translationese.", "published": "2023-06-06 16:51:03", "link": "http://arxiv.org/abs/2306.03856v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correction of Errors in Preference Ratings from Automated Metrics for\n  Text Generation", "abstract": "A major challenge in the field of Text Generation is evaluation: Human\nevaluations are cost-intensive, and automated metrics often display\nconsiderable disagreement with human judgments. In this paper, we propose a\nstatistical model of Text Generation evaluation that accounts for the\nerror-proneness of automated metrics when used to generate preference rankings\nbetween system outputs. We show that existing automated metrics are generally\nover-confident in assigning significant differences between systems in this\nsetting. However, our model enables an efficient combination of human and\nautomated ratings to remedy the error-proneness of the automated metrics. We\nshow that using this combination, we only require about 50% of the human\nannotations typically used in evaluations to arrive at robust and statistically\nsignificant results while yielding the same evaluation outcome as the pure\nhuman evaluation in 95% of cases. We showcase the benefits of approach for\nthree text generation tasks: dialogue systems, machine translation, and text\nsummarization.", "published": "2023-06-06 17:09:29", "link": "http://arxiv.org/abs/2306.03866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CL-UZH at SemEval-2023 Task 10: Sexism Detection through Incremental\n  Fine-Tuning and Multi-Task Learning with Label Descriptions", "abstract": "The widespread popularity of social media has led to an increase in hateful,\nabusive, and sexist language, motivating methods for the automatic detection of\nsuch phenomena. The goal of the SemEval shared task \\textit{Towards Explainable\nDetection of Online Sexism} (EDOS 2023) is to detect sexism in English social\nmedia posts (subtask A), and to categorize such posts into four coarse-grained\nsexism categories (subtask B), and eleven fine-grained subcategories (subtask\nC). In this paper, we present our submitted systems for all three subtasks,\nbased on a multi-task model that has been fine-tuned on a range of related\ntasks and datasets before being fine-tuned on the specific EDOS subtasks. We\nimplement multi-task learning by formulating each task as binary pairwise text\nclassification, where the dataset and label descriptions are given along with\nthe input text. The results show clear improvements over a fine-tuned\nDeBERTa-V3 serving as a baseline leading to $F_1$-scores of 85.9\\% in subtask A\n(rank 13/84), 64.8\\% in subtask B (rank 19/69), and 44.9\\% in subtask C\n(26/63).", "published": "2023-06-06 17:59:49", "link": "http://arxiv.org/abs/2306.03907v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action\n  Prediction", "abstract": "Task-oriented dialogues often require agents to enact complex, multi-step\nprocedures in order to meet user requests. While large language models have\nfound success automating these dialogues in constrained environments, their\nwidespread deployment is limited by the substantial quantities of task-specific\ndata required for training. The following paper presents a data-efficient\nsolution to constructing dialogue systems, leveraging explicit instructions\nderived from agent guidelines, such as company policies or customer service\nmanuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a\nlarge language model with a knowledge retrieval module that pulls documents\noutlining relevant procedures from a predefined set of policies, given a\nuser-agent interaction. To train this system, we introduce a semi-supervised\npre-training scheme that employs dialogue-document matching and action-oriented\nmasked language modeling with partial parameter freezing. We evaluate the\neffectiveness of our approach on prominent task-oriented dialogue datasets,\nAction-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue\ntasks: action state tracking and workflow discovery. Our results demonstrate\nthat procedural knowledge augmentation improves accuracy predicting in- and\nout-of-distribution actions while preserving high performance in settings with\nlow or sparse data.", "published": "2023-06-06 18:42:08", "link": "http://arxiv.org/abs/2306.03959v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Toward More Accurate and Generalizable Evaluation Metrics for\n  Task-Oriented Dialogs", "abstract": "Measurement of interaction quality is a critical task for the improvement of\nspoken dialog systems. Existing approaches to dialog quality estimation either\nfocus on evaluating the quality of individual turns, or collect dialog-level\nquality measurements from end users immediately following an interaction. In\ncontrast to these approaches, we introduce a new dialog-level annotation\nworkflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate\nthe quality of dialogs as a whole, and also label dialogs for attributes such\nas goal completion and user sentiment. In this contribution, we show that: (i)\nwhile dialog quality cannot be completely decomposed into dialog-level\nattributes, there is a strong relationship between some objective dialog\nattributes and judgments of dialog quality; (ii) for the task of dialog-level\nquality estimation, a supervised model trained on dialog-level annotations\noutperforms methods based purely on aggregating turn-level features; and (iii)\nthe proposed evaluation model shows better domain generalization ability\ncompared to the baselines. On the basis of these results, we argue that having\nhigh-quality human-annotated data is an important component of evaluating\ninteraction quality for large industrial-scale voice assistant platforms.", "published": "2023-06-06 19:43:29", "link": "http://arxiv.org/abs/2306.03984v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Triggering Multi-Hop Reasoning for Question Answering in Language Models\n  using Soft Prompts and Random Walks", "abstract": "Despite readily memorizing world knowledge about entities, pre-trained\nlanguage models (LMs) struggle to compose together two or more facts to perform\nmulti-hop reasoning in question-answering tasks. In this work, we propose\ntechniques that improve upon this limitation by relying on random walks over\nstructured knowledge graphs. Specifically, we use soft prompts to guide LMs to\nchain together their encoded knowledge by learning to map multi-hop questions\nto random walk paths that lead to the answer. Applying our methods on two T5\nLMs shows substantial improvements over standard tuning approaches in answering\nquestions that require 2-hop reasoning.", "published": "2023-06-06 20:45:18", "link": "http://arxiv.org/abs/2306.04009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Augmenting Reddit Posts to Determine Wellness Dimensions impacting\n  Mental Health", "abstract": "Amid ongoing health crisis, there is a growing necessity to discern possible\nsigns of Wellness Dimensions (WD) manifested in self-narrated text. As the\ndistribution of WD on social media data is intrinsically imbalanced, we\nexperiment the generative NLP models for data augmentation to enable further\nimprovement in the pre-screening task of classifying WD. To this end, we\npropose a simple yet effective data augmentation approach through prompt-based\nGenerative NLP models, and evaluate the ROUGE scores and syntactic/semantic\nsimilarity among existing interpretations and augmented data. Our approach with\nChatGPT model surpasses all the other methods and achieves improvement over\nbaselines such as Easy-Data Augmentation and Backtranslation. Introducing data\naugmentation to generate more training samples and balanced dataset, results in\nthe improved F-score and the Matthew's Correlation Coefficient for upto 13.11%\nand 15.95%, respectively.", "published": "2023-06-06 23:15:59", "link": "http://arxiv.org/abs/2306.04059v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Detecting Human Rights Violations on Social Media during Russia-Ukraine\n  War", "abstract": "The present-day Russia-Ukraine military conflict has exposed the pivotal role\nof social media in enabling the transparent and unbridled sharing of\ninformation directly from the frontlines. In conflict zones where freedom of\nexpression is constrained and information warfare is pervasive, social media\nhas emerged as an indispensable lifeline. Anonymous social media platforms, as\npublicly available sources for disseminating war-related information, have the\npotential to serve as effective instruments for monitoring and documenting\nHuman Rights Violations (HRV). Our research focuses on the analysis of data\nfrom Telegram, the leading social media platform for reading independent news\nin post-Soviet regions. We gathered a dataset of posts sampled from 95 public\nTelegram channels that cover politics and war news, which we have utilized to\nidentify potential occurrences of HRV. Employing a mBERT-based text classifier,\nwe have conducted an analysis to detect any mentions of HRV in the Telegram\ndata. Our final approach yielded an $F_2$ score of 0.71 for HRV detection,\nrepresenting an improvement of 0.38 over the multilingual BERT base model. We\nrelease two datasets that contains Telegram posts: (1) large corpus with over\n2.3 millions posts and (2) annotated at the sentence-level dataset to indicate\nHRVs. The Telegram posts are in the context of the Russia-Ukraine war. We posit\nthat our findings hold significant implications for NGOs, governments, and\nresearchers by providing a means to detect and document possible human rights\nviolations.", "published": "2023-06-06 12:59:03", "link": "http://arxiv.org/abs/2306.05370v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language\n  Model", "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the \"truthfulness\" of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.", "published": "2023-06-06 01:26:53", "link": "http://arxiv.org/abs/2306.03341v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision,\n  Language, and Graphs", "abstract": "In-Batch contrastive learning is a state-of-the-art self-supervised method\nthat brings semantically-similar instances close while pushing dissimilar\ninstances apart within a mini-batch. Its key to success is the negative sharing\nstrategy, in which every instance serves as a negative for the others within\nthe mini-batch. Recent studies aim to improve performance by sampling hard\nnegatives \\textit{within the current mini-batch}, whose quality is bounded by\nthe mini-batch itself. In this work, we propose to improve contrastive learning\nby sampling mini-batches from the input data. We present\nBatchSampler\\footnote{The code is available at\n\\url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of\nhard-to-distinguish (i.e., hard and true negatives to each other) instances. To\nmake each mini-batch have fewer false negatives, we design the proximity graph\nof randomly-selected instances. To form the mini-batch, we leverage random walk\nwith restart on the proximity graph to help sample hard-to-distinguish\ninstances. BatchSampler is a simple and general technique that can be directly\nplugged into existing contrastive learning models in vision, language, and\ngraphs. Extensive experiments on datasets of three modalities show that\nBatchSampler can consistently improve the performance of powerful contrastive\nmodels, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE\non STS (language), and GraphCL and MVGRL on graph datasets.", "published": "2023-06-06 02:13:27", "link": "http://arxiv.org/abs/2306.03355v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search", "abstract": "Customers interacting with product search engines are increasingly\nformulating information-seeking queries. Frequently Asked Question (FAQ)\nretrieval aims to retrieve common question-answer pairs for a user query with\nquestion intent. Integrating FAQ retrieval in product search can not only\nempower users to make more informed purchase decisions, but also enhance user\nretention through efficient post-purchase support. Determining when an FAQ\nentry can satisfy a user's information need within product search, without\ndisrupting their shopping experience, represents an important challenge. We\npropose an intent-aware FAQ retrieval system consisting of (1) an intent\nclassifier that predicts when a user's information need can be answered by an\nFAQ; (2) a reformulation model that rewrites a query into a natural question.\nOffline evaluation demonstrates that our approach improves Hit@1 by 13% on\nretrieving ground-truth FAQs, while reducing latency by 95% compared to\nbaseline systems. These improvements are further validated by real user\nfeedback, where 71% of displayed FAQs on top of product search results received\nexplicit positive user feedback. Overall, our findings show promising\ndirections for integrating FAQ retrieval into product search at scale.", "published": "2023-06-06 05:18:21", "link": "http://arxiv.org/abs/2306.03411v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Role of Attention in Prompt-tuning", "abstract": "Prompt-tuning is an emerging strategy to adapt large language models (LLM) to\ndownstream tasks by learning a (soft-)prompt parameter from data. Despite its\nsuccess in LLMs, there is limited theoretical understanding of the power of\nprompt-tuning and the role of the attention mechanism in prompting. In this\nwork, we explore prompt-tuning for one-layer attention architectures and study\ncontextual mixture-models where each input token belongs to a context-relevant\nor -irrelevant set. We isolate the role of prompt-tuning through a\nself-contained prompt-attention model. Our contributions are as follows: (1) We\nshow that softmax-prompt-attention is provably more expressive than\nsoftmax-self-attention and linear-prompt-attention under our contextual data\nmodel. (2) We analyze the initial trajectory of gradient descent and show that\nit learns the prompt and prediction head with near-optimal sample complexity\nand demonstrate how prompt can provably attend to sparse context-relevant\ntokens. (3) Assuming a known prompt but an unknown prediction head, we\ncharacterize the exact finite sample performance of prompt-attention which\nreveals the fundamental performance limits and the precise benefit of the\ncontext information. We also provide experiments that verify our theoretical\ninsights on real datasets and demonstrate how prompt-tuning enables the model\nto attend to context-relevant information.", "published": "2023-06-06 06:23:38", "link": "http://arxiv.org/abs/2306.03435v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Large Language Models of Code Fail at Completing Code with Potential\n  Bugs", "abstract": "Large language models of code (Code-LLMs) have recently brought tremendous\nadvances to code completion, a fundamental feature of programming assistance\nand code intelligence. However, most existing works ignore the possible\npresence of bugs in the code context for generation, which are inevitable in\nsoftware development. Therefore, we introduce and study the buggy-code\ncompletion problem, inspired by the realistic scenario of real-time code\nsuggestion where the code context contains potential bugs -- anti-patterns that\ncan become bugs in the completed program. To systematically study the task, we\nintroduce two datasets: one with synthetic bugs derived from semantics-altering\noperator changes (buggy-HumanEval) and one with realistic bugs derived from\nuser submissions to coding problems (buggy-FixEval). We find that the presence\nof potential bugs significantly degrades the generation performance of the\nhigh-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO\non test cases of buggy-HumanEval drop more than 50% given a single potential\nbug in the context. Finally, we investigate several post-hoc methods for\nmitigating the adverse effect of potential bugs and find that there remains a\nsignificant gap in post-mitigation performance.", "published": "2023-06-06 06:35:27", "link": "http://arxiv.org/abs/2306.03438v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Alzheimer Disease Classification through ASR-based Transcriptions:\n  Exploring the Impact of Punctuation and Pauses", "abstract": "Alzheimer's Disease (AD) is the world's leading neurodegenerative disease,\nwhich often results in communication difficulties. Analysing speech can serve\nas a diagnostic tool for identifying the condition. The recent ADReSS challenge\nprovided a dataset for AD classification and highlighted the utility of manual\ntranscriptions. In this study, we used the new state-of-the-art Automatic\nSpeech Recognition (ASR) model Whisper to obtain the transcriptions, which also\ninclude automatic punctuation. The classification models achieved test accuracy\nscores of 0.854 and 0.833 combining the pretrained FastText word embeddings and\nrecurrent neural networks on manual and ASR transcripts respectively.\nAdditionally, we explored the influence of including pause information and\npunctuation in the transcriptions. We found that punctuation only yielded minor\nimprovements in some cases, whereas pause encoding aided AD classification for\nboth manual and ASR transcriptions across all approaches investigated.", "published": "2023-06-06 06:49:41", "link": "http://arxiv.org/abs/2306.03443v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics", "abstract": "Automatic assessment of reading fluency using automatic speech recognition\n(ASR) holds great potential for early detection of reading difficulties and\nsubsequent timely intervention. Precise assessment tools are required,\nespecially for languages other than English. In this study, we evaluate six\nstate-of-the-art ASR-based systems for automatically assessing Dutch oral\nreading accuracy using Kaldi and Whisper. Results show our most successful\nsystem reached substantial agreement with human evaluations (MCC = .63). The\nsame system reached the highest correlation between forced decoding confidence\nscores and word correctness (r = .45). This system's language model (LM)\nconsisted of manual orthographic transcriptions and reading prompts of the test\ndata, which shows that including reading errors in the LM improves assessment\nperformance. We discuss the implications for developing automatic assessment\nsystems and identify possible avenues of future research.", "published": "2023-06-06 06:49:58", "link": "http://arxiv.org/abs/2306.03444v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Natural Language Commanding via Program Synthesis", "abstract": "We present Semantic Interpreter, a natural language-friendly AI system for\nproductivity software such as Microsoft Office that leverages large language\nmodels (LLMs) to execute user intent across application features. While LLMs\nare excellent at understanding user intent expressed as natural language, they\nare not sufficient for fulfilling application-specific user intent that\nrequires more than text-to-text transformations. We therefore introduce the\nOffice Domain Specific Language (ODSL), a concise, high-level language\nspecialized for performing actions in and interacting with entities in Office\napplications. Semantic Interpreter leverages an Analysis-Retrieval prompt\nconstruction method with LLMs for program synthesis, translating natural\nlanguage user utterances to ODSL programs that can be transpiled to application\nAPIs and then executed. We focus our discussion primarily on a research\nexploration for Microsoft PowerPoint.", "published": "2023-06-06 07:28:49", "link": "http://arxiv.org/abs/2306.03460v1", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Applying Standards to Advance Upstream & Downstream Ethics in Large\n  Language Models", "abstract": "This paper explores how AI-owners can develop safeguards for AI-generated\ncontent by drawing from established codes of conduct and ethical standards in\nother content-creation industries. It delves into the current state of ethical\nawareness on Large Language Models (LLMs). By dissecting the mechanism of\ncontent generation by LLMs, four key areas (upstream/downstream and at user\nprompt/answer), where safeguards could be effectively applied, are identified.\nA comparative analysis of these four areas follows and includes an evaluation\nof the existing ethical safeguards in terms of cost, effectiveness, and\nalignment with established industry practices. The paper's key argument is that\nexisting IT-related ethical codes, while adequate for traditional IT\nengineering, are inadequate for the challenges posed by LLM-based content\ngeneration. Drawing from established practices within journalism, we propose\npotential standards for businesses involved in distributing and selling\nLLM-generated content. Finally, potential conflicts of interest between dataset\ncuration at upstream and ethical benchmarking downstream are highlighted to\nunderscore the need for a broader evaluation beyond mere output. This study\nprompts a nuanced conversation around ethical implications in this rapidly\nevolving field of content generation.", "published": "2023-06-06 08:47:42", "link": "http://arxiv.org/abs/2306.03503v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "K.4.1; I.2.0"], "primary_category": "cs.CY"}
{"title": "Financial Numeric Extreme Labelling: A Dataset and Benchmarking for XBRL\n  Tagging", "abstract": "The U.S. Securities and Exchange Commission (SEC) mandates all public\ncompanies to file periodic financial statements that should contain numerals\nannotated with a particular label from a taxonomy. In this paper, we formulate\nthe task of automating the assignment of a label to a particular numeral span\nin a sentence from an extremely large label set. Towards this task, we release\na dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794\nlabels. We benchmark the performance of the FNXL dataset by formulating the\ntask as (a) a sequence labelling problem and (b) a pipeline with span\nextraction followed by Extreme Classification. Although the two approaches\nperform comparably, the pipeline solution provides a slight edge for the least\nfrequent labels.", "published": "2023-06-06 14:41:30", "link": "http://arxiv.org/abs/2306.03723v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "LEACE: Perfect linear concept erasure in closed form", "abstract": "Concept erasure aims to remove specified features from an embedding. It can\nimprove fairness (e.g. preventing a classifier from using gender or race) and\ninterpretability (e.g. removing a concept to observe changes in model\nbehavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form\nmethod which provably prevents all linear classifiers from detecting a concept\nwhile changing the embedding as little as possible, as measured by a broad\nclass of norms. We apply LEACE to large language models with a novel procedure\ncalled \"concept scrubbing,\" which erases target concept information from every\nlayer in the network. We demonstrate our method on two tasks: measuring the\nreliance of language models on part-of-speech information, and reducing gender\nbias in BERT embeddings. Code is available at\nhttps://github.com/EleutherAI/concept-erasure.", "published": "2023-06-06 16:07:24", "link": "http://arxiv.org/abs/2306.03819v4", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Deductive Verification of Chain-of-Thought Reasoning", "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated\nerrors, thereby limiting models' ability to solve complex reasoning tasks.\nInspired by how humans engage in careful and meticulous deductive logical\nreasoning processes to solve tasks, we seek to enable language models to\nperform explicit and rigorous deductive reasoning, and also ensure the\ntrustworthiness of their reasoning process through self-verification. However,\ndirectly verifying the validity of an entire deductive reasoning process is\nchallenging, even with advanced models like ChatGPT. In light of this, we\npropose to decompose a reasoning verification process into a series of\nstep-by-step subprocesses, each only receiving their necessary context and\npremises. To facilitate this procedure, we propose Natural Program, a natural\nlanguage-based deductive reasoning format. Our approach enables models to\ngenerate precise reasoning steps where subsequent steps are more rigorously\ngrounded on prior steps. It also empowers language models to carry out\nreasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly\nenhance the rigor and trustfulness of generated reasoning steps. Along this\nprocess, we also improve the answer correctness on complex reasoning tasks.\nCode will be released at https://github.com/lz1oceani/verify_cot.", "published": "2023-06-06 17:18:56", "link": "http://arxiv.org/abs/2306.03872v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory", "abstract": "Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .", "published": "2023-06-06 17:58:24", "link": "http://arxiv.org/abs/2306.03901v2", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Utterance Classification with Logical Neural Network: Explainable AI for\n  Mental Disorder Diagnosis", "abstract": "In response to the global challenge of mental health problems, we proposes a\nLogical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis\nof mental disorders. Due to the lack of effective therapy coverage for mental\ndisorders, there is a need for an AI solution that can assist therapists with\nthe diagnosis. However, current Neural Network models lack explainability and\nmay not be trusted by therapists. The LNN is a Recurrent Neural Network\narchitecture that combines the learning capabilities of neural networks with\nthe reasoning capabilities of classical logic-based AI. The proposed system\nuses input predicates from clinical interviews to output a mental disorder\nclass, and different predicate pruning techniques are used to achieve\nscalability and higher scores. In addition, we provide an insight extraction\nmethod to aid therapists with their diagnosis. The proposed system addresses\nthe lack of explainability of current Neural Network models and provides a more\ntrustworthy solution for mental disorder diagnosis.", "published": "2023-06-06 17:58:44", "link": "http://arxiv.org/abs/2306.03902v1", "categories": ["cs.CL", "cs.AI", "cs.LO", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Turning large language models into cognitive models", "abstract": "Large language models are powerful systems that excel at many tasks, ranging\nfrom translation to mathematical reasoning. Yet, at the same time, these models\noften show unhuman-like characteristics. In the present paper, we address this\ngap and ask whether large language models can be turned into cognitive models.\nWe find that -- after finetuning them on data from psychological experiments --\nthese models offer accurate representations of human behavior, even\noutperforming traditional cognitive models in two decision-making domains. In\naddition, we show that their representations contain the information necessary\nto model behavior on the level of individual subjects. Finally, we demonstrate\nthat finetuning on multiple tasks enables large language models to predict\nhuman behavior in a previously unseen task. Taken together, these results\nsuggest that large, pre-trained models can be adapted to become generalist\ncognitive models, thereby opening up new research directions that could\ntransform cognitive psychology and the behavioral sciences as a whole.", "published": "2023-06-06 18:00:01", "link": "http://arxiv.org/abs/2306.03917v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recognition of Handwritten Japanese Characters Using Ensemble of\n  Convolutional Neural Networks", "abstract": "The Japanese writing system is complex, with three character types of\nHiragana, Katakana, and Kanji. Kanji consists of thousands of unique\ncharacters, further adding to the complexity of character identification and\nliterature understanding. Being able to translate handwritten Japanese\ncharacters into digital text is useful for data analysis, translation, learning\nand cultural preservation. In this study, a machine learning approach to\nanalyzing and recognizing handwritten Japanese characters (Kanji) is proposed.\nThe study used an ensemble of three convolutional neural networks (CNNs) for\nrecognizing handwritten Kanji characters and utilized four datasets of MNIST,\nK-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the\nKuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results\nindicate feasibility of using proposed CNN-ensemble architecture for\nrecognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4%\nclassification accuracy on MNIST, K-MNIS, K49, and K-Kanji datasets\nrespectively.", "published": "2023-06-06 18:30:51", "link": "http://arxiv.org/abs/2306.03954v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual\n  Navigation in Noisy Environments", "abstract": "Audio-visual navigation of an agent towards locating an audio goal is a\nchallenging task especially when the audio is sporadic or the environment is\nnoisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual\nEmbodied Navigation framework in which the agent may interact with a\nhuman/oracle for solving the task of navigating to an audio goal. Specifically,\nCAVEN is modeled as a budget-aware partially observable semi-Markov decision\nprocess that implicitly learns the uncertainty in the audio-based navigation\npolicy to decide when and how the agent may interact with the oracle. Our CAVEN\nagent can engage in fully-bidirectional natural language conversations by\nproducing relevant questions and interpret free-form, potentially noisy\nresponses from the oracle based on the audio-visual context. To enable such a\ncapability, CAVEN is equipped with: (i) a trajectory forecasting network that\nis grounded in audio-visual cues to produce a potential trajectory to the\nestimated goal, and (ii) a natural language based question generation and\nreasoning network to pose an interactive question to the oracle or interpret\nthe oracle's response to produce navigation instructions. To train the\ninteractive modules, we present a large scale dataset: AVN-Instruct, based on\nthe Landmark-RxR dataset. To substantiate the usefulness of conversations, we\npresent experiments on the benchmark audio-goal task using the SoundSpaces\nsimulator under various noisy settings. Our results reveal that our\nfully-conversational approach leads to nearly an order-of-magnitude improvement\nin success rate, especially in localizing new sound sources and against methods\nthat only use uni-directional interaction.", "published": "2023-06-06 22:32:49", "link": "http://arxiv.org/abs/2306.04047v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMZip: Lossless Text Compression using Large Language Models", "abstract": "We provide new estimates of an asymptotic upper bound on the entropy of\nEnglish using the large language model LLaMA-7B as a predictor for the next\ntoken given a window of past tokens. This estimate is significantly smaller\nthan currently available estimates in \\cite{cover1978convergent},\n\\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless\ncompression of English text which combines the prediction from the large\nlanguage model with a lossless compression scheme. Preliminary results from\nlimited experiments suggest that our scheme outperforms state-of-the-art text\ncompression schemes such as BSC, ZPAQ, and paq8h.", "published": "2023-06-06 22:42:00", "link": "http://arxiv.org/abs/2306.04050v2", "categories": ["cs.IT", "cs.CL", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Towards End-to-end Speech-to-text Summarization", "abstract": "Speech-to-text (S2T) summarization is a time-saving technique for filtering\nand keeping up with the broadcast news uploaded online on a daily basis. The\nrise of large language models from deep learning with impressive text\ngeneration capabilities has placed the research focus on summarization systems\nthat produce paraphrased compact versions of the document content, also known\nas abstractive summaries. End-to-end (E2E) modelling of S2T abstractive\nsummarization is a promising approach that offers the possibility of generating\nrich latent representations that leverage non-verbal and acoustic information,\nas opposed to the use of only linguistic information from automatically\ngenerated transcripts in cascade systems. However, the few literature on E2E\nmodelling of this task fails on exploring different domains, namely broadcast\nnews, which is challenging domain where large and diversified volumes of data\nare presented to the user every day. We model S2T summarization both with a\ncascade and an E2E system for a corpus of broadcast news in French. Our novel\nE2E model leverages external data by resorting to transfer learning from a\npre-trained T2T summarizer. Experiments show that both our cascade and E2E\nabstractive summarizers are stronger than an extractive baseline. However, the\nperformance of the E2E model still lies behind the cascade one, which is object\nof an extensive analysis that includes future directions to close that gap.", "published": "2023-06-06 15:22:16", "link": "http://arxiv.org/abs/2306.05432v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Fairness and Robustness in End-to-End Speech Recognition\n  through unsupervised clustering", "abstract": "The challenge of fairness arises when Automatic Speech Recognition (ASR)\nsystems do not perform equally well for all sub-groups of the population. In\nthe past few years there have been many improvements in overall speech\nrecognition quality, but without any particular focus on advancing Equality and\nEquity for all user groups for whom systems do not perform well. ASR fairness\nis therefore also a robustness issue. Meanwhile, data privacy also takes\npriority in production systems. In this paper, we present a privacy preserving\napproach to improve fairness and robustness of end-to-end ASR without using\nmetadata, zip codes, or even speaker or utterance embeddings directly in\ntraining. We extract utterance level embeddings using a speaker ID model\ntrained on a public dataset, which we then use in an unsupervised fashion to\ncreate acoustic clusters. We use cluster IDs instead of speaker utterance\nembeddings as extra features during model training, which shows improvements\nfor all demographic groups and in particular for different accents.", "published": "2023-06-06 21:13:08", "link": "http://arxiv.org/abs/2306.06083v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A generative framework for conversational laughter: Its 'language model'\n  and laughter sound synthesis", "abstract": "As the phonetic and acoustic manifestations of laughter in conversation are\nhighly diverse, laughter synthesis should be capable of accommodating such\ndiversity while maintaining high controllability. This paper proposes a\ngenerative model of laughter in conversation that can produce a wide variety of\nlaughter by utilizing the emotion dimension as a conversational context. The\nmodel comprises two parts: the laughter \"phones generator,\" which generates\nvarious, but realistic, combinations of laughter components for a given speaker\nID and emotional state, and the laughter \"sound synthesizer,\" which receives\nthe laughter phone sequence and produces acoustic features that reflect the\nspeaker's individuality and emotional state. The results of a listening\nexperiment indicated that conditioning both the phones generator and the sound\nsynthesizer on emotion dimensions resulted in the most effective control of the\nperceived emotion in synthesized laughter.", "published": "2023-06-06 07:35:24", "link": "http://arxiv.org/abs/2306.03465v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Phase perturbation improves channel robustness for speech spoofing\n  countermeasures", "abstract": "In this paper, we aim to address the problem of channel robustness in speech\ncountermeasure (CM) systems, which are used to distinguish synthetic speech\nfrom human natural speech. On the basis of two hypotheses, we suggest an\napproach for perturbing phase information during the training of time-domain CM\nsystems. Communication networks often employ lossy compression codec that\nencodes only magnitude information, therefore heavily altering phase\ninformation. Also, state-of-the-art CM systems rely on phase information to\nidentify spoofed speech. Thus, we believe the information loss in the phase\ndomain induced by lossy compression codec degrades the performance of the\nunseen channel. We first establish the dependence of time-domain CM systems on\nphase information by perturbing phase in evaluation, showing strong\ndegradation. Then, we demonstrated that perturbing phase during training leads\nto a significant performance improvement, whereas perturbing magnitude leads to\nfurther degradation.", "published": "2023-06-06 04:06:20", "link": "http://arxiv.org/abs/2306.03389v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Experimenting with Additive Margins for Contrastive Self-Supervised\n  Speaker Verification", "abstract": "Most state-of-the-art self-supervised speaker verification systems rely on a\ncontrastive-based objective function to learn speaker representations from\nunlabeled speech data. We explore different ways to improve the performance of\nthese methods by: (1) revisiting how positive and negative pairs are sampled\nthrough a \"symmetric\" formulation of the contrastive loss; (2) introducing\nmargins similar to AM-Softmax and AAM-Softmax that have been widely adopted in\nthe supervised setting. We demonstrate the effectiveness of the symmetric\ncontrastive loss which provides more supervision for the self-supervised task.\nMoreover, we show that Additive Margin and Additive Angular Margin allow\nreducing the overall number of false negatives and false positives by improving\nspeaker separability. Finally, by combining both techniques and training a\nlarger model we achieve 7.50% EER and 0.5804 minDCF on the VoxCeleb1 test set,\nwhich outperforms other contrastive self supervised methods on speaker\nverification.", "published": "2023-06-06 13:26:43", "link": "http://arxiv.org/abs/2306.03664v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis", "abstract": "We are interested in a novel task, namely low-resource text-to-talking\navatar. Given only a few-minute-long talking person video with the audio track\nas the training data and arbitrary texts as the driving input, we aim to\nsynthesize high-quality talking portrait videos corresponding to the input\ntext. This task has broad application prospects in the digital human industry\nbut has not been technically achieved yet due to two challenges: (1) It is\nchallenging to mimic the timbre from out-of-domain audio for a traditional\nmulti-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and\nlip-synchronized talking avatars with limited training data. In this paper, we\nintroduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a\ngeneric zero-shot multi-speaker TTS model that well disentangles the text\ncontent, timbre, and prosody; and (2) embraces recent advances in neural\nrendering to achieve realistic audio-driven talking face video generation. With\nthese designs, our method overcomes the aforementioned two challenges and\nachieves to generate identity-preserving speech and realistic talking person\nvideo. Experiments demonstrate that our method could synthesize realistic,\nidentity-preserving, and audio-visual synchronized talking avatar videos.", "published": "2023-06-06 08:50:13", "link": "http://arxiv.org/abs/2306.03504v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive\n  Bias", "abstract": "Scaling text-to-speech to a large and wild dataset has been proven to be\nhighly effective in achieving timbre and speech style generalization,\nparticularly in zero-shot TTS. However, previous works usually encode speech\ninto latent using audio codec and use autoregressive language models or\ndiffusion models to generate it, which ignores the intrinsic nature of speech\nand may lead to inferior or uncontrollable results. We argue that speech can be\ndecomposed into several attributes (e.g., content, timbre, prosody, and phase)\nand each of them should be modeled using a module with appropriate inductive\nbiases. From this perspective, we carefully design a novel and large zero-shot\nTTS system called Mega-TTS, which is trained with large-scale wild data and\nmodels different attributes in different ways: 1) Instead of using latent\nencoded by audio codec as the intermediate feature, we still choose spectrogram\nas it separates the phase and other attributes very well. Phase can be\nappropriately constructed by the GAN-based vocoder and does not need to be\nmodeled by the language model. 2) We model the timbre using global vectors\nsince timbre is a global attribute that changes slowly over time. 3) We further\nuse a VQGAN-based acoustic model to generate the spectrogram and a latent code\nlanguage model to fit the distribution of prosody, since prosody changes\nquickly over time in a sentence, and language models can capture both local and\nlong-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K\nhours of speech and evaluate its performance on unseen speakers. Experimental\nresults demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on\nzero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior\nnaturalness, robustness, and speaker similarity due to the proper inductive\nbias of each module. Audio samples are available at\nhttps://mega-tts.github.io/demo-page.", "published": "2023-06-06 08:54:49", "link": "http://arxiv.org/abs/2306.03509v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dance Generation by Sound Symbolic Words", "abstract": "This study introduces a novel approach to generate dance motions using\nonomatopoeia as input, with the aim of enhancing creativity and diversity in\ndance generation. Unlike text and music, onomatopoeia conveys rhythm and\nmeaning through abstract word expressions without constraints on expression and\nwithout need for specialized knowledge. We adapt the AI Choreographer framework\nand employ the Sakamoto system, a feature extraction method for onomatopoeia\nfocusing on phonemes and syllables. Additionally, we present a new dataset of\n40 onomatopoeia-dance motion pairs collected through a user survey. Our results\ndemonstrate that the proposed method enables more intuitive dance generation\nand can create dance motions using sound-symbolic words from a variety of\nlanguages, including those without onomatopoeia. This highlights the potential\nfor diverse dance creation across different languages and cultures, accessible\nto a wider audience. Qualitative samples from our model can be found at:\nhttps://sites.google.com/view/onomatopoeia-dance/home/.", "published": "2023-06-06 13:00:47", "link": "http://arxiv.org/abs/2306.03646v1", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Emotion-Conditioned Melody Harmonization with Hierarchical Variational\n  Autoencoder", "abstract": "Existing melody harmonization models have made great progress in improving\nthe quality of generated harmonies, but most of them ignored the emotions\nbeneath the music. Meanwhile, the variability of harmonies generated by\nprevious methods is insufficient. To solve these problems, we propose a novel\nLSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the\ninfluence of emotional conditions on melody harmonization, while improving the\nquality of generated harmonies and capturing the abundant variability of chord\nprogressions. Specifically, LHVAE incorporates latent variables and emotional\nconditions at different levels (piece- and bar-level) to model the global and\nlocal music properties. Additionally, we introduce an attention-based melody\ncontext vector at each step to better learn the correspondence between melodies\nand harmonies. Objective experimental results show that our proposed model\noutperforms other LSTM-based models. Through subjective evaluation, we conclude\nthat only altering the types of chords hardly changes the overall emotion of\nthe music. The qualitative analysis demonstrates the ability of our model to\ngenerate variable harmonies.", "published": "2023-06-06 14:28:57", "link": "http://arxiv.org/abs/2306.03718v4", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RescueSpeech: A German Corpus for Speech Recognition in Search and\n  Rescue Domain", "abstract": "Despite the recent advancements in speech recognition, there are still\ndifficulties in accurately transcribing conversational and emotional speech in\nnoisy and reverberant acoustic environments. This poses a particular challenge\nin the search and rescue (SAR) domain, where transcribing conversations among\nrescue team members is crucial to support real-time decision-making. The\nscarcity of speech data and associated background noise in SAR scenarios make\nit difficult to deploy robust speech recognition systems. To address this\nissue, we have created and made publicly available a German speech dataset\ncalled RescueSpeech. This dataset includes real speech recordings from\nsimulated rescue exercises. Additionally, we have released competitive training\nrecipes and pre-trained models. Our study highlights that the performance\nattained by state-of-the-art methods in this challenging scenario is still far\nfrom reaching an acceptable level.", "published": "2023-06-06 23:04:22", "link": "http://arxiv.org/abs/2306.04054v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Systematic Analysis of Music Representations from BERT", "abstract": "There have been numerous attempts to represent raw data as numerical vectors\nthat effectively capture semantic and contextual information. However, in the\nfield of symbolic music, previous works have attempted to validate their music\nembeddings by observing the performance improvement of various fine-tuning\ntasks. In this work, we directly analyze embeddings from BERT and BERT with\ncontrastive learning trained on bar-level MIDI, inspecting their musical\ninformation that can be obtained from MIDI events. We observe that the\nembeddings exhibit distinct characteristics of information depending on the\ncontrastive objectives and the choice of layers. Our code is available at\nhttps://github.com/sjhan91/MusicBERT.", "published": "2023-06-06 13:26:55", "link": "http://arxiv.org/abs/2306.04628v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modulation Classification Through Deep Learning Using Resolution\n  Transformed Spectrograms", "abstract": "Modulation classification is an essential step of signal processing and has\nbeen regularly applied in the field of tele-communication. Since variations of\nfrequency with respect to time remains a vital distinction among radio signals\nhaving different modulation formats, these variations can be used for feature\nextraction by converting 1-D radio signals into frequency domain. In this\npaper, we propose a scheme for Automatic Modulation Classification (AMC) using\nmodern architectures of Convolutional Neural Networks (CNN), through generating\nspectrum images of eleven different modulation types. Additionally, we perform\nresolution transformation of spectrograms that results up to 99.61% of\ncomputational load reduction and 8x faster conversion from the received I/Q\ndata. This proposed AMC is implemented on CPU and GPU, to recognize digital as\nwell as analogue signal modulation schemes on signals. The performance is\nevaluated on existing CNN models including SqueezeNet, Resnet-50,\nInceptionResnet-V2, Inception-V3, VGG-16 and Densenet-201. Best results of\n91.2% are achieved in presence of AWGN and other noise impairments in the\nsignals, stating that the transformed spectrogram-based AMC has good\nclassification accuracy as the spectral features are highly discriminant, and\nCNN based models have capability to extract these high-dimensional features.\nThe spectrograms were created under different SNRs ranging from 5 to 30db with\na step size of 5db to observe the experimental results at various SNR levels.\nThe proposed methodology is efficient to be applied in wireless communication\nnetworks for real-time applications.", "published": "2023-06-06 16:14:15", "link": "http://arxiv.org/abs/2306.04655v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
