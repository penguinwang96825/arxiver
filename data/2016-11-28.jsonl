{"title": "Exploiting Unlabeled Data for Neural Grammatical Error Detection", "abstract": "Identifying and correcting grammatical errors in the text written by\nnon-native writers has received increasing attention in recent years. Although\na number of annotated corpora have been established to facilitate data-driven\ngrammatical error detection and correction approaches, they are still limited\nin terms of quantity and coverage because human annotation is labor-intensive,\ntime-consuming, and expensive. In this work, we propose to utilize unlabeled\ndata to train neural network based grammatical error detection models. The\nbasic idea is to cast error detection as a binary classification problem and\nderive positive and negative training examples from unlabeled data. We\nintroduce an attention-based neural network to capture long-distance\ndependencies that influence the word being detected. Experiments show that the\nproposed approach significantly outperforms SVMs and convolutional networks\nwith fixed-size context window.", "published": "2016-11-28 05:32:35", "link": "http://arxiv.org/abs/1611.08987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a cardiovascular disease risk factor annotated corpus of\n  Chinese electronic medical records", "abstract": "Cardiovascular disease (CVD) has become the leading cause of death in China,\nand most of the cases can be prevented by controlling risk factors. The goal of\nthis study was to build a corpus of CVD risk factor annotations based on\nChinese electronic medical records (CEMRs). This corpus is intended to be used\nto develop a risk factor information extraction system that, in turn, can be\napplied as a foundation for the further study of the progress of risk factors\nand CVD. We designed a light annotation task to capture CVD risk factors with\nindicators, temporal attributes and assertions that were explicitly or\nimplicitly displayed in the records. The task included: 1) preparing data; 2)\ncreating guidelines for capturing annotations (these were created with the help\nof clinicians); 3) proposing an annotation method including building the\nguidelines draft, training the annotators and updating the guidelines, and\ncorpus construction. Then, a risk factor annotated corpus based on\nde-identified discharge summaries and progress notes from 600 patients was\ndeveloped. Built with the help of clinicians, this corpus has an\ninter-annotator agreement (IAA) F1-measure of 0.968, indicating a high\nreliability. To the best of our knowledge, this is the first annotated corpus\nconcerning CVD risk factors in CEMRs and the guidelines for capturing CVD risk\nfactor annotations from CEMRs were proposed. The obtained document-level\nannotations can be applied in future studies to monitor risk factors and CVD\nover the long term.", "published": "2016-11-28 08:20:54", "link": "http://arxiv.org/abs/1611.09020v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "We use reinforcement learning to learn tree-structured neural networks for\ncomputing representations of natural language sentences. In contrast with prior\nwork on tree-structured models in which the trees are either provided as input\nor predicted using supervision from explicit treebank annotations, the tree\nstructures in this work are optimized to improve performance on a downstream\ntask. Experiments demonstrate the benefit of learning task-specific composition\norders, outperforming both sequential encoders and recursive encoders based on\ntreebank annotations. We analyze the induced trees and show that while they\ndiscover some linguistically intuitive structures (e.g., noun phrases, simple\nverb phrases), they are different than conventional English syntactic\nstructures.", "published": "2016-11-28 12:57:07", "link": "http://arxiv.org/abs/1611.09100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-End Architecture for Keyword Spotting and Voice Activity\n  Detection", "abstract": "We propose a single neural network architecture for two tasks: on-line\nkeyword spotting and voice activity detection. We develop novel inference\nalgorithms for an end-to-end Recurrent Neural Network trained with the\nConnectionist Temporal Classification loss function which allow our model to\nachieve high accuracy on both keyword spotting and voice activity detection\nwithout retraining. In contrast to prior voice activity detection models, our\narchitecture does not require aligned training data and uses the same\nparameters as the keyword spotting model. This allows us to deploy a high\nquality voice activity detector with no additional memory or maintenance\nrequirements.", "published": "2016-11-28 22:03:22", "link": "http://arxiv.org/abs/1611.09405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Copying and Restricted Generation for Paraphrase", "abstract": "Many natural language generation tasks, such as abstractive summarization and\ntext simplification, are paraphrase-orientated. In these tasks, copying and\nrewriting are two main writing modes. Most previous sequence-to-sequence\n(Seq2Seq) models use a single decoder and neglect this fact. In this paper, we\ndevelop a novel Seq2Seq model to fuse a copying decoder and a restricted\ngenerative decoder. The copying decoder finds the position to be copied based\non a typical attention model. The generative decoder produces words limited in\nthe source-specific vocabulary. To combine the two decoders and determine the\nfinal output, we develop a predictor to predict the mode of copying or\nrewriting. This predictor can be guided by the actual writing mode in the\ntraining data. We conduct extensive experiments on two different paraphrase\ndatasets. The result shows that our model outperforms the state-of-the-art\napproaches in terms of both informativeness and language quality.", "published": "2016-11-28 16:49:37", "link": "http://arxiv.org/abs/1611.09235v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Multi-Document Summarization via Text Classification", "abstract": "Developed so far, multi-document summarization has reached its bottleneck due\nto the lack of sufficient training data and diverse categories of documents.\nText classification just makes up for these deficiencies. In this paper, we\npropose a novel summarization system called TCSum, which leverages plentiful\ntext classification data to improve the performance of multi-document\nsummarization. TCSum projects documents onto distributed representations which\nact as a bridge between text classification and summarization. It also utilizes\nthe classification results to produce summaries of different styles. Extensive\nexperiments on DUC generic multi-document summarization datasets show that,\nTCSum can achieve the state-of-the-art performance without using any\nhand-crafted features and has the capability to catch the variations of summary\nstyles with respect to different text categories.", "published": "2016-11-28 16:53:06", "link": "http://arxiv.org/abs/1611.09238v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "abstract": "We introduce a large scale MAchine Reading COmprehension dataset, which we\nname MS MARCO. The dataset comprises of 1,010,916 anonymized\nquestions---sampled from Bing's search query logs---each with a human generated\nanswer and 182,669 completely human rewritten generated answers. In addition,\nthe dataset contains 8,841,823 passages---extracted from 3,563,535 web\ndocuments retrieved by Bing---that provide the information necessary for\ncurating the natural language answers. A question in the MS MARCO dataset may\nhave multiple answers or no answers at all. Using this dataset, we propose\nthree different tasks with varying levels of difficulty: (i) predict if a\nquestion is answerable given a set of context passages, and extract and\nsynthesize the answer as a human would (ii) generate a well-formed answer (if\npossible) based on the context passages that can be understood with the\nquestion and passage context, and finally (iii) rank a set of retrieved\npassages given a question. The size of the dataset and the fact that the\nquestions are derived from real user search queries distinguishes MS MARCO from\nother well-known publicly available datasets for machine reading comprehension\nand question-answering. We believe that the scale and the real-world nature of\nthis dataset makes it attractive for benchmarking machine reading comprehension\nand question-answering models.", "published": "2016-11-28 18:14:11", "link": "http://arxiv.org/abs/1611.09268v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning a Natural Language Interface with Neural Programmer", "abstract": "Learning a natural language interface for database tables is a challenging\ntask that involves deep language understanding and multi-step reasoning. The\ntask is often approached by mapping natural language queries to logical forms\nor programs that provide the desired response when executed on the database. To\nour knowledge, this paper presents the first weakly supervised, end-to-end\nneural network model to induce such programs on a real-world dataset. We\nenhance the objective function of Neural Programmer, a neural network with\nbuilt-in discrete operations, and apply it on WikiTableQuestions, a natural\nlanguage question-answering dataset. The model is trained end-to-end with weak\nsupervision of question-answer pairs, and does not require domain-specific\ngrammars, rules, or annotations that are key elements in previous approaches to\nprogram induction. The main experimental result in this paper is that a single\nNeural Programmer model achieves 34.2% accuracy using only 10,000 examples with\nweak supervision. An ensemble of 15 models, with a trivial combination\ntechnique, achieves 37.7% accuracy, which is competitive to the current\nstate-of-the-art accuracy of 37.1% obtained by a traditional natural language\nsemantic parser.", "published": "2016-11-28 00:54:34", "link": "http://arxiv.org/abs/1611.08945v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Analyzing Features for the Detection of Happy Endings in German Novels", "abstract": "With regard to a computational representation of literary plot, this paper\nlooks at the use of sentiment analysis for happy ending detection in German\nnovels. Its focus lies on the investigation of previously proposed sentiment\nfeatures in order to gain insight about the relevance of specific features on\nthe one hand and the implications of their performance on the other hand.\nTherefore, we study various partitionings of novels, considering the highly\nvariable concept of \"ending\". We also show that our approach, even though still\nrather simple, can potentially lead to substantial findings relevant to\nliterary studies.", "published": "2016-11-28 08:56:04", "link": "http://arxiv.org/abs/1611.09028v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech", "abstract": "Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop.", "published": "2016-11-28 15:51:25", "link": "http://arxiv.org/abs/1611.09207v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech\n  Recognition", "abstract": "In computer vision pixelwise dense prediction is the task of predicting a\nlabel for each pixel in the image. Convolutional neural networks achieve good\nperformance on this task, while being computationally efficient. In this paper\nwe carry these ideas over to the problem of assigning a sequence of labels to a\nset of speech frames, a task commonly known as framewise classification. We\nshow that dense prediction view of framewise classification offers several\nadvantages and insights, including computational efficiency and the ability to\napply batch normalization. When doing dense prediction we pay specific\nattention to strided pooling in time and introduce an asymmetric dilated\nconvolution, called time-dilated convolution, that allows for efficient and\nelegant implementation of pooling in time. We show results using time-dilated\nconvolutions in a very deep VGG-style CNN with batch normalization on the Hub5\nSwitchboard-2000 benchmark task. With a big n-gram language model, we achieve\n7.7% WER which is the best single model single-pass performance reported so\nfar.", "published": "2016-11-28 19:09:58", "link": "http://arxiv.org/abs/1611.09288v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability", "abstract": "There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities.", "published": "2016-11-28 23:48:41", "link": "http://arxiv.org/abs/1611.09434v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
