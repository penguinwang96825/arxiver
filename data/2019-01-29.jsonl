{"title": "An Arabic Dependency Treebank in the Travel Domain", "abstract": "In this paper we present a dependency treebank of travel domain sentences in\nModern Standard Arabic. The text comes from a translation of the English\nequivalent sentences in the Basic Traveling Expressions Corpus. The treebank\ndependency representation is in the style of the Columbia Arabic Treebank. The\npaper motivates the effort and discusses the construction process and\nguidelines. We also present parsing results and discuss the effect of domain\nand genre difference on parsing.", "published": "2019-01-29 09:24:15", "link": "http://arxiv.org/abs/1901.10188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Generate: Neural Generation of Complex Sentences", "abstract": "We propose a task to generate a complex sentence from a simple sentence in\norder to amplify various kinds of responses in the database. We first divide a\ncomplex sentence into a main clause and a subordinate clause to learn a\ngenerator model of modifiers, and then use the model to generate a modifier\nclause to create a complex sentence from a simple sentence. We present an\nautomatic evaluation metric to estimate the quality of the models and show that\na pipeline model outperforms an end-to-end model.", "published": "2019-01-29 10:00:54", "link": "http://arxiv.org/abs/1901.10196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language\nand images. It determines the importance of context elements by comparing each\nelement to the current time step. In this paper, we show that a very\nlightweight convolution can perform competitively to the best reported\nself-attention results. Next, we introduce dynamic convolutions which are\nsimpler and more efficient than self-attention. We predict separate convolution\nkernels based solely on the current time-step in order to determine the\nimportance of context elements. The number of operations required by this\napproach scales linearly in the input length, whereas self-attention is\nquadratic. Experiments on large-scale machine translation, language modeling\nand abstractive summarization show that dynamic convolutions improve over\nstrong self-attention models. On the WMT'14 English-German test set dynamic\nconvolutions achieve a new state of the art of 29.7 BLEU.", "published": "2019-01-29 18:01:35", "link": "http://arxiv.org/abs/1901.10430v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Training Required: Exploring Random Encoders for Sentence\n  Classification", "abstract": "We explore various methods for computing sentence representations from\npre-trained word embeddings without any training, i.e., using nothing but\nrandom parameterizations. Our aim is to put sentence embeddings on more solid\nfooting by 1) looking at how much modern sentence embeddings gain over random\nmethods---as it turns out, surprisingly little; and by 2) providing the field\nwith more appropriate baselines going forward---which are, as it turns out,\nquite strong. We also make important observations about proper experimental\nprotocol for sentence classification evaluation, together with recommendations\nfor future research.", "published": "2019-01-29 18:44:01", "link": "http://arxiv.org/abs/1901.10444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Dependency Parsing from Scratch", "abstract": "This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We\nintroduce a complete neural pipeline system that takes raw text as input, and\nperforms all tasks required by the shared task, ranging from tokenization and\nsentence segmentation, to POS tagging and dependency parsing. Our single system\nsubmission achieved very competitive performance on big treebanks. Moreover,\nafter fixing an unfortunate bug, our corrected system would have placed the\n2nd, 1st, and 3rd on the official evaluation metrics LAS,MLAS, and BLEX, and\nwould have outperformed all submission systems on low-resource treebank\ncategories on all metrics by a large margin. We further show the effectiveness\nof different model components through extensive ablation studies.", "published": "2019-01-29 18:58:29", "link": "http://arxiv.org/abs/1901.10457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structuring an unordered text document", "abstract": "Segmenting an unordered text document into different sections is a very\nuseful task in many text processing applications like multiple document\nsummarization, question answering, etc. This paper proposes structuring of an\nunordered text document based on the keywords in the document. We test our\napproach on Wikipedia documents using both statistical and predictive methods\nsuch as the TextRank algorithm and Google's USE (Universal Sentence Encoder).\nFrom our experimental results, we show that the proposed model can effectively\nstructure an unordered document into sections.", "published": "2019-01-29 06:53:21", "link": "http://arxiv.org/abs/1901.10133v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Revised JNLPBA Corpus: A Revised Version of Biomedical NER Corpus for\n  Relation Extraction Task", "abstract": "The advancement of biomedical named entity recognition (BNER) and biomedical\nrelation extraction (BRE) researches promotes the development of text mining in\nbiological domains. As a cornerstone of BRE, robust BNER system is required to\nidentify the mentioned NEs in plain texts for further relation extraction\nstage. However, the current BNER corpora, which play important roles in these\ntasks, paid less attention to achieve the criteria for BRE task. In this study,\nwe present Revised JNLPBA corpus, the revision of JNLPBA corpus, to broaden the\napplicability of a NER corpus from BNER to BRE task. We preserve the original\nentity types including protein, DNA, RNA, cell line and cell type while all the\nabstracts in JNLPBA corpus are manually curated by domain experts again basis\non the new annotation guideline focusing on the specific NEs instead of general\nterms. Simultaneously, several imperfection issues in JNLPBA are pointed out\nand made up in the new corpus. To compare the adaptability of different NER\nsystems in Revised JNLPBA and JNLPBA corpora, the F1-measure was measured in\nthree open sources NER systems including BANNER, Gimli and NERSuite. In the\nsame circumstance, all the systems perform average 10% better in Revised JNLPBA\nthan in JNLPBA. Moreover, the cross-validation test is carried out which we\ntrain the NER systems on JNLPBA/Revised JNLPBA corpora and access the\nperformance in both protein-protein interaction extraction (PPIE) and\nbiomedical event extraction (BEE) corpora to confirm that the newly refined\nRevised JNLPBA is a competent NER corpus in biomedical relation application.\nThe revised JNLPBA corpus is freely available at\niasl-btm.iis.sinica.edu.tw/BNER/Content/Revised_JNLPBA.zip.", "published": "2019-01-29 11:12:58", "link": "http://arxiv.org/abs/1901.10219v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Guidelines for creating man-machine multimodal interfaces", "abstract": "Understanding details of human multimodal interaction can elucidate many\naspects of the type of information processing machines must perform to interact\nwith humans. This article gives an overview of recent findings from Linguistics\nregarding the organization of conversation in turns, adjacent pairs,\n(dis)preferred responses, (self)repairs, etc. Besides, we describe how multiple\nmodalities of signs interfere with each other modifying meanings. Then, we\npropose an abstract algorithm that describes how a machine can implement a\ndouble-feedback system that can reproduces a human-like face-to-face\ninteraction by processing various signs, such as verbal, prosodic, facial\nexpressions, gestures, etc. Multimodal face-to-face interactions enrich the\nexchange of information between agents, mainly because these agents are active\nall the time by emitting and interpreting signs simultaneously. This article is\nnot about an untested new computational model. Instead, it translates findings\nfrom Linguistics as guidelines for designs of multimodal man-machine\ninterfaces. An algorithm is presented. Brought from Linguistics, it is a\ndescription pointing out how human face-to-face interactions work. The\nlinguistic findings reported here are the first steps towards the integration\nof multimodal communication. Some developers involved on interface designs\ncarry on working on isolated models for interpreting text, grammar, gestures\nand facial expressions, neglecting the interwoven between these signs. In\ncontrast, for linguists working on the state-of-the-art multimodal integration,\nthe interpretation of separated modalities leads to an incomplete\ninterpretation, if not to a miscomprehension of information. The algorithm\nproposed herein intends to guide man-machine interface designers who want to\nintegrate multimodal components on face-to-face interactions as close as\npossible to those performed between humans.", "published": "2019-01-29 17:23:32", "link": "http://arxiv.org/abs/1901.10408v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Impact of Training Dataset Size on Neural Answer Selection Models", "abstract": "It is held as a truism that deep neural networks require large datasets to\ntrain effective models. However, large datasets, especially with high-quality\nlabels, can be expensive to obtain. This study sets out to investigate (i) how\nlarge a dataset must be to train well-performing models, and (ii) what impact\ncan be shown from fractional changes to the dataset size. A practical method to\ninvestigate these questions is to train a collection of deep neural answer\nselection models using fractional subsets of varying sizes of an initial\ndataset. We observe that dataset size has a conspicuous lack of effect on the\ntraining of some of these models, bringing the underlying algorithms into\nquestion.", "published": "2019-01-29 19:00:21", "link": "http://arxiv.org/abs/1901.10496v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Glyce: Glyph-vectors for Chinese Character Representations", "abstract": "It is intuitive that NLP tasks for logographic languages like Chinese should\nbenefit from the use of the glyph information in those languages. However, due\nto the lack of rich pictographic evidence in glyphs and the weak generalization\nability of standard computer vision models on character data, an effective way\nto utilize the glyph information remains to be found. In this paper, we address\nthis gap by presenting Glyce, the glyph-vectors for Chinese character\nrepresentations. We make three major innovations: (1) We use historical Chinese\nscripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to\nenrich the pictographic evidence in characters; (2) We design CNN structures\n(called tianzege-CNN) tailored to Chinese character image processing; and (3)\nWe use image-classification as an auxiliary task in a multi-task learning setup\nto increase the model's ability to generalize. We show that glyph-based models\nare able to consistently outperform word/char ID-based models in a wide range\nof Chinese NLP tasks. We are able to set new state-of-the-art results for a\nvariety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair\nclassification, single sentence classification tasks, dependency parsing, and\nsemantic role labeling. For example, the proposed model achieves an F1 score of\n80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost\nperfect accuracy of 99.8\\% on the Fudan corpus for text classification. Code\nfound at https://github.com/ShannonAI/glyce.", "published": "2019-01-29 06:15:36", "link": "http://arxiv.org/abs/1901.10125v5", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "TiFi: Taxonomy Induction for Fictional Domains [Extended version]", "abstract": "Taxonomies are important building blocks of structured knowledge bases, and\ntheir construction from text sources and Wikipedia has received much attention.\nIn this paper we focus on the construction of taxonomies for fictional domains,\nusing noisy category systems from fan wikis or text extraction as input. Such\nfictional domains are archetypes of entity universes that are poorly covered by\nWikipedia, such as also enterprise-specific knowledge bases or highly\nspecialized verticals. Our fiction-targeted approach, called TiFi, consists of\nthree phases: (i) category cleaning, by identifying candidate categories that\ntruly represent classes in the domain of interest, (ii) edge cleaning, by\nselecting subcategory relationships that correspond to class subsumption, and\n(iii) top-level construction, by mapping classes onto a subset of high-level\nWordNet categories. A comprehensive evaluation shows that TiFi is able to\nconstruct taxonomies for a diverse range of fictional domains such as Lord of\nthe Rings, The Simpsons or Greek Mythology with very high precision and that it\noutperforms state-of-the-art baselines for taxonomy induction by a substantial\nmargin.", "published": "2019-01-29 13:07:13", "link": "http://arxiv.org/abs/1901.10263v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Implicit Diversity in Image Summarization", "abstract": "Studies have shown that the people depicted in image search results tend to\nbe of majority groups with respect to socially salient attributes. This skew\ngoes beyond that which already exists in the world - e.g., Kay et al. showed\nthat although 28% of CEOs in US are women, only 10% of the top 100 results for\nCEO in Google Image Search are women. Most existing approaches to correct for\nthis kind of bias assume that the images of people include socially salient\nattribute labels. However, such labels are often unknown. Further, using\nautomated techniques to infer these labels may often not be possible within\nacceptable accuracy ranges, and may not be desirable due to the additional\nbiases this process could incur. We develop a novel approach that takes as\ninput a visibly diverse control set of images and uses this set to select a set\nof images of people in response to a query. The goal is to have a resulting set\nthat is more visibly diverse in a manner that emulates the diversity depicted\nin the control set. Importantly, this approach does not require images to be\nlabelled at any point; effectively, it gives a way to implicitly diversify the\nset of images selected. We provide two variants of our approach: the first is a\nmodification of the MMR algorithm to incorporate the diversity scores, and\nsecond is a more efficient variant that does not consider within-list\nredundancy. We evaluate these approaches empirically on two datasets 1) a new\ndataset containing top Google image results for 96 occupations, for which we\nevaluate gender and skin-tone diversity with respect to occupations and 2) the\nCelebA dataset for which we evaluate gender diversity with respect to facial\nfeatures. Our approaches produce image sets that significantly improve the\nvisible diversity of the results, compared to current Google search and other\ndiverse image summarization algorithms, at a minimal cost to accuracy.", "published": "2019-01-29 13:13:16", "link": "http://arxiv.org/abs/1901.10265v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Latent Normalizing Flows for Discrete Sequences", "abstract": "Normalizing flows are a powerful class of generative models for continuous\nrandom variables, showing both strong model flexibility and the potential for\nnon-autoregressive generation. These benefits are also desired when modeling\ndiscrete random variables such as text, but directly applying normalizing flows\nto discrete sequences poses significant additional challenges. We propose a\nVAE-based generative model which jointly learns a normalizing flow-based\ndistribution in the latent space and a stochastic mapping to an observed\ndiscrete space. In this setting, we find that it is crucial for the flow-based\ndistribution to be highly multimodal. To capture this property, we propose\nseveral normalizing flow architectures to maximize model flexibility.\nExperiments consider common discrete sequence tasks of character-level language\nmodeling and polyphonic music generation. Our results indicate that an\nautoregressive flow-based model can match the performance of a comparable\nautoregressive baseline, and a non-autoregressive flow-based model can improve\ngeneration speed with a penalty to performance.", "published": "2019-01-29 21:05:46", "link": "http://arxiv.org/abs/1901.10548v4", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Applying Visual Domain Style Transfer and Texture Synthesis Techniques\n  to Audio - Insights and Challenges", "abstract": "Style transfer is a technique for combining two images based on the\nactivations and feature statistics in a deep learning neural network\narchitecture. This paper studies the analogous task in the audio domain and\ntakes a critical look at the problems that arise when adapting the original\nvision-based framework to handle spectrogram representations. We conclude that\nCNN architectures with features based on 2D representations and convolutions\nare better suited for visual images than for time-frequency representations of\naudio. Despite the awkward fit, experiments show that the Gram matrix\ndetermined \"style\" for audio is more closely aligned with timbral signatures\nwithout temporal structure whereas network layer activity determining audio\n\"content\" seems to capture more of the pitch and rhythmic structures. We shed\ninsight on several reasons for the domain differences with illustrative\nexamples. We motivate the use of several types of one-dimensional CNNs that\ngenerate results that are better aligned with intuitive notions of audio\ntexture than those based on existing architectures built for images. These\nideas also prompt an exploration of audio texture synthesis with architectural\nvariants for extensions to infinite textures, multi-textures, parametric\ncontrol of receptive fields and the constant-Q transform as an alternative\nfrequency scaling for the spectrogram.", "published": "2019-01-29 11:59:47", "link": "http://arxiv.org/abs/1901.10240v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
