{"title": "Long-term decomposition of robust pricing kernels under G-expectation", "abstract": "This study develops a BSDE method for the long-term decomposition of pricing\nkernels under the G-expectation framework. We establish the existence,\nuniqueness, and regularity of solutions to three types of quadratic G-BSDEs:\nfinite-horizon G-BSDEs, infinite-horizon G-BSDEs, and ergodic G-BSDEs.\nMoreover, we explore the Feynman--Kac formula associated with these three types\nof quadratic G-BSDEs. Using these results, a pricing kernel is uniquely\ndecomposed into four components: an exponential discounting component, a\ntransitory component, a symmetric G-martingale, and a decreasing component that\ncaptures the volatility uncertainty of the G-Brownian motion. Furthermore,\nthese components are represented through a solution to a PDE. This study\nextends previous findings obtained under a single fixed probability framework\nto the G-expectation context.", "published": "2024-08-31 19:46:40", "link": "http://arxiv.org/abs/2409.00535v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "State-Space Dynamic Functional Regression for Multicurve Fixed Income Spread Analysis and Stress Testing", "abstract": "The Nelson-Siegel model is widely used in fixed income markets to produce\nyield curve dynamics. The multiple time-dependent parameter model conveniently\naddresses the level, slope, and curvature dynamics of the yield curves. In this\nstudy, we present a novel state-space functional regression model that\nincorporates a dynamic Nelson-Siegel model and functional regression\nformulations applied to multi-economy setting. This framework offers distinct\nadvantages in explaining the relative spreads in yields between a reference\neconomy and a response economy. To address the inherent challenges of model\ncalibration, a kernel principal component analysis is employed to transform the\nrepresentation of functional regression into a finite-dimensional, tractable\nestimation problem. A comprehensive empirical analysis is conducted to assess\nthe efficacy of the functional regression approach, including an in-sample\nperformance comparison with the dynamic Nelson-Siegel model. We conducted the\nstress testing analysis of yield curves term-structure within a dual economy\nframework. The bond ladder portfolio was examined through a case study focused\non spread modelling using historical data for US Treasury and UK bonds.", "published": "2024-08-31 04:30:05", "link": "http://arxiv.org/abs/2409.00348v2", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "YA-TA: Towards Personalized Question-Answering Teaching Assistants using\n  Instructor-Student Dual Retrieval-augmented Knowledge Fusion", "abstract": "Engagement between instructors and students plays a crucial role in enhancing\nstudents'academic performance. However, instructors often struggle to provide\ntimely and personalized support in large classes. To address this challenge, we\npropose a novel Virtual Teaching Assistant (VTA) named YA-TA, designed to offer\nresponses to students that are grounded in lectures and are easy to understand.\nTo facilitate YA-TA, we introduce the Dual Retrieval-augmented Knowledge Fusion\n(DRAKE) framework, which incorporates dual retrieval of instructor and student\nknowledge and knowledge fusion for tailored response generation. Experiments\nconducted in real-world classroom settings demonstrate that the DRAKE framework\nexcels in aligning responses with knowledge retrieved from both instructor and\nstudent sides. Furthermore, we offer additional extensions of YA-TA, such as a\nQ&A board and self-practice tools to enhance the overall learning experience.\nOur video is publicly available.", "published": "2024-08-31 05:37:51", "link": "http://arxiv.org/abs/2409.00355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Information Extraction using Large Language Models", "abstract": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability.", "published": "2024-08-31 07:10:16", "link": "http://arxiv.org/abs/2409.00369v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "With Good MT There is No Need For End-to-End: A Case for\n  Translate-then-Summarize Cross-lingual Summarization", "abstract": "Recent work has suggested that end-to-end system designs for cross-lingual\nsummarization are competitive solutions that perform on par or even better than\ntraditional pipelined designs. A closer look at the evidence reveals that this\nintuition is based on the results of only a handful of languages or using\nunderpowered pipeline baselines. In this work, we compare these two paradigms\nfor cross-lingual summarization on 39 source languages into English and show\nthat a simple \\textit{translate-then-summarize} pipeline design consistently\noutperforms even an end-to-end system with access to enormous amounts of\nparallel data. For languages where our pipeline model does not perform well, we\nshow that system performance is highly correlated with publicly distributed\nBLEU scores, allowing practitioners to establish the feasibility of a language\npair a priori. Contrary to recent publication trends, our result suggests that\nthe combination of individual progress of monolingual summarization and\ntranslation tasks offers better performance than an end-to-end system,\nsuggesting that end-to-end designs should be considered with care.", "published": "2024-08-31 10:44:16", "link": "http://arxiv.org/abs/2409.00414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large\n  Language Models", "abstract": "Large language models (LLMs) face significant challenges in handling\nlong-context tasks because of their limited effective context window size\nduring pretraining, which restricts their ability to generalize over extended\nsequences. Meanwhile, extending the context window in LLMs through\npost-pretraining is highly resource-intensive. To address this, we introduce\nLongRecipe, an efficient training strategy for extending the context window of\nLLMs, including impactful token analysis, position index transformation, and\ntraining optimization strategies. It simulates long-sequence inputs while\nmaintaining training efficiency and significantly improves the model's\nunderstanding of long-range dependencies. Experiments on three types of LLMs\nshow that LongRecipe can utilize long sequences while requiring only 30% of the\ntarget context window size, and reduces computational training resource over\n85% compared to full sequence training. Furthermore, LongRecipe also preserves\nthe original LLM's capabilities in general tasks. Ultimately, we can extend the\neffective context window of open-source LLMs from 8k to 128k, achieving\nperformance close to GPT-4 with just one day of dedicated training using a\nsingle GPU with 80G memory. Our code is released at\nhttps://github.com/zhiyuanhubj/LongRecipe.", "published": "2024-08-31 17:19:30", "link": "http://arxiv.org/abs/2409.00509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Prediction to Application: Language Model-based Code Knowledge\n  Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with\n  Pedagogical Prompting for Comprehensive Programming Education", "abstract": "Knowledge Tracing (KT) is a critical component in online learning, but\ntraditional approaches face limitations in interpretability and cross-domain\nadaptability. This paper introduces Language Model-based Code Knowledge Tracing\n(CodeLKT), an innovative application of Language model-based Knowledge Tracing\n(LKT) to programming education. CodeLKT leverages pre-trained language models\nto process learning data, demonstrating superior performance over existing KT\nand Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task\nAdaptive Pre-Training (TAPT), showing enhanced performance in the coding domain\nand investigating cross-domain transfer between mathematics and coding.\nAdditionally, we present an theoretically-informed integrated system combining\nCodeLKT with large language models to generate personalized, in-depth feedback\nto support students' programming learning. This work advances the field of Code\nKnowledge Tracing by expanding the knowledge base with language model-based\napproach and offering practical implications for programming education through\ndata-informed feedback.", "published": "2024-08-31 01:36:38", "link": "http://arxiv.org/abs/2409.00323v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph\n  Construction", "abstract": "Recently, there has been an increasing interest in the construction of\ngeneral-domain and domain-specific causal knowledge graphs. Such knowledge\ngraphs enable reasoning for causal analysis and event prediction, and so have a\nrange of applications across different domains. While great progress has been\nmade toward automated construction of causal knowledge graphs, the evaluation\nof such solutions has either focused on low-level tasks (e.g., cause-effect\nphrase extraction) or on ad hoc evaluation data and small manual evaluations.\nIn this paper, we present a corpus, task, and evaluation framework for causal\nknowledge graph construction. Our corpus consists of Wikipedia articles for a\ncollection of event-related concepts in Wikidata. The task is to extract causal\nrelations between event concepts from the corpus. The evaluation is performed\nin part using existing causal relations in Wikidata to measure recall, and in\npart using Large Language Models to avoid the need for manual or crowd-sourced\nevaluation. We evaluate a pipeline for causal knowledge graph construction that\nrelies on neural models for question answering and concept linking, and show\nhow the corpus and the evaluation framework allow us to effectively find the\nright model for each task. The corpus and the evaluation framework are publicly\navailable.", "published": "2024-08-31 02:21:39", "link": "http://arxiv.org/abs/2409.00331v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chatting Up Attachment: Using LLMs to Predict Adult Bonds", "abstract": "Obtaining data in the medical field is challenging, making the adoption of AI\ntechnology within the space slow and high-risk. We evaluate whether we can\novercome this obstacle with synthetic data generated by large language models\n(LLMs). In particular, we use GPT-4 and Claude 3 Opus to create agents that\nsimulate adults with varying profiles, childhood memories, and attachment\nstyles. These agents participate in simulated Adult Attachment Interviews\n(AAI), and we use their responses to train models for predicting their\nunderlying attachment styles. We evaluate our models using a transcript dataset\nfrom 9 humans who underwent the same interview protocol, analyzed and labeled\nby mental health professionals. Our findings indicate that training the models\nusing only synthetic data achieves performance comparable to training the\nmodels on human data. Additionally, while the raw embeddings from synthetic\nanswers occupy a distinct space compared to those from real human responses,\nthe introduction of unlabeled human data and a simple standardization allows\nfor a closer alignment of these representations. This adjustment is supported\nby qualitative analyses and is reflected in the enhanced predictive accuracy of\nthe standardized embeddings.", "published": "2024-08-31 04:29:19", "link": "http://arxiv.org/abs/2409.00347v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Does Alignment Tuning Really Break LLMs' Internal Confidence?", "abstract": "Large Language Models (LLMs) have shown remarkable progress, but their\nreal-world application necessitates reliable calibration. This study conducts a\ncomprehensive analysis of calibration degradation of LLMs across four\ndimensions: models, calibration metrics, tasks, and confidence extraction\nmethods. Initial analysis showed that the relationship between alignment and\ncalibration is not always a trade-off, but under stricter analysis conditions,\nwe found the alignment process consistently harms calibration. This highlights\nthe need for (1) a careful approach when measuring model confidences and\ncalibration errors and (2) future research into algorithms that can help LLMs\nto achieve both instruction-following and calibration without sacrificing\neither.", "published": "2024-08-31 05:12:36", "link": "http://arxiv.org/abs/2409.00352v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting the Target Word of Game-playing Conversations using a\n  Low-Rank Dialect Adapter for Decoder Models", "abstract": "Dialect adapters that improve the performance of LLMs for NLU tasks on\ncertain sociolects/dialects/national varieties ('dialects' for the sake of\nbrevity) have been reported for encoder models. In this paper, we extend the\nidea of dialect adapters to decoder models in our architecture called LoRDD.\nUsing MD-3, a publicly available dataset of word game-playing conversations\nbetween dialectal speakers, our task is Target Word Prediction (TWP) from a\nmasked conversation. LoRDD combines task adapters and dialect adapters where\nthe latter employ contrastive learning on pseudo-parallel conversations from\nMD-3. Our experiments on Indian English and Nigerian English conversations with\ntwo models (Mistral and Gemma) demonstrate that LoRDD outperforms four\nbaselines on TWP. Additionally, it significantly reduces the performance gap\nwith American English, narrowing it to 12% and 5.8% for word similarity, and\n25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is\nin its promise for dialect adaptation of decoder models using TWP, a simplified\nversion of the commonly used next-word prediction task.", "published": "2024-08-31 05:53:39", "link": "http://arxiv.org/abs/2409.00358v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Backdoor Detection Evaluation for Language Models", "abstract": "Backdoor attacks, in which a model behaves maliciously when given an\nattacker-specified trigger, pose a major security risk for practitioners who\ndepend on publicly released language models. Backdoor detection methods aim to\ndetect whether a released model contains a backdoor, so that practitioners can\navoid such vulnerabilities. While existing backdoor detection methods have high\naccuracy in detecting backdoored models on standard benchmarks, it is unclear\nwhether they can robustly identify backdoors in the wild. In this paper, we\nexamine the robustness of backdoor detectors by manipulating different factors\nduring backdoor planting. We find that the success of existing methods highly\ndepends on how intensely the model is trained on poisoned data during backdoor\nplanting. Specifically, backdoors planted with either more aggressive or more\nconservative training are significantly more difficult to detect than the\ndefault ones. Our results highlight a lack of robustness of existing backdoor\ndetectors and the limitations in current benchmark construction.", "published": "2024-08-31 09:19:39", "link": "http://arxiv.org/abs/2409.00399v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Context Length for Open-Domain Dialog Generation", "abstract": "Transformer-based open-domain dialog models have become increasingly popular\nin recent years. These models typically represent context as a concatenation of\na dialog history. However, there is no criterion to decide how many utterances\nshould be kept adequate in a context. We try to figure out how the choice of\ncontext length affects the model. We experiment on three questions from coarse\nto fine: (i) Does longer context help model training? (ii) Is it necessary to\nchange the training context length when dealing with dialogs of different\ncontext lengths? (iii) Do different dialog samples have the same preference for\ncontext length? Our experimental results show that context length, an often\noverlooked setting, deserves attention when implementing Transformer-based\ndialog models.", "published": "2024-08-31 00:56:36", "link": "http://arxiv.org/abs/2409.00315v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effectiveness of Large Language Models in Representing\n  and Understanding Movement Trajectories", "abstract": "This research focuses on assessing the ability of AI foundation models in\nrepresenting the trajectories of movements. We utilize one of the large\nlanguage models (LLMs) (i.e., GPT-J) to encode the string format of\ntrajectories and then evaluate the effectiveness of the LLM-based\nrepresentation for trajectory data analysis. The experiments demonstrate that\nwhile the LLM-based embeddings can preserve certain trajectory distance metrics\n(i.e., the correlation coefficients exceed 0.74 between the Cosine distance\nderived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping\ndistances on raw trajectories), challenges remain in restoring numeric values\nand retrieving spatial neighbors in movement trajectory analytics. In addition,\nthe LLMs can understand the spatiotemporal dependency contained in trajectories\nand have good accuracy in location prediction tasks. This research highlights\nthe need for improvement in terms of capturing the nuances and complexities of\nthe underlying geospatial data and integrating domain knowledge to support\nvarious GeoAI applications using LLMs.", "published": "2024-08-31 02:57:25", "link": "http://arxiv.org/abs/2409.00335v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; E.2"], "primary_category": "cs.CL"}
{"title": "Statistics of punctuation in experimental literature -- the remarkable\n  case of \"Finnegans Wake\" by James Joyce", "abstract": "As the recent studies indicate, the structure imposed onto written texts by\nthe presence of punctuation develops patterns which reveal certain\ncharacteristics of universality. In particular, based on a large collection of\nclassic literary works, it has been evidenced that the distances between\nconsecutive punctuation marks, measured in terms of the number of words, obey\nthe discrete Weibull distribution - a discrete variant of a distribution often\nused in survival analysis. The present work extends the analysis of punctuation\nusage patterns to more experimental pieces of world literature. It turns out\nthat the compliance of the the distances between punctuation marks with the\ndiscrete Weibull distribution typically applies here as well. However, some of\nthe works by James Joyce are distinct in this regard - in the sense that the\ntails of the relevant distributions are significantly thicker and,\nconsequently, the corresponding hazard functions are decreasing functions not\nobserved in typical literary texts in prose. \"Finnegans Wake\" - the same one to\nwhich science owes the word \"quarks\" for the most fundamental constituents of\nmatter - is particularly striking in this context. At the same time, in all the\nstudied texts, the sentence lengths - representing the distances between\nsentence-ending punctuation marks - reveal more freedom and are not constrained\nby the discrete Weibull distribution. This freedom in some cases translates\ninto long-range nonlinear correlations, which manifest themselves in\nmultifractality. Again, a text particularly spectacular in terms of\nmultifractality is \"Finnegans Wake\".", "published": "2024-08-31 15:30:51", "link": "http://arxiv.org/abs/2409.00483v1", "categories": ["physics.soc-ph", "cs.CL", "stat.AP"], "primary_category": "physics.soc-ph"}
{"title": "Post-OCR Text Correction for Bulgarian Historical Documents", "abstract": "The digitization of historical documents is crucial for preserving the\ncultural heritage of the society. An important step in this process is\nconverting scanned images to text using Optical Character Recognition (OCR),\nwhich can enable further search, information extraction, etc. Unfortunately,\nthis is a hard problem as standard OCR tools are not tailored to deal with\nhistorical orthography as well as with challenging layouts. Thus, it is\nstandard to apply an additional text correction step on the OCR output when\ndealing with such documents. In this work, we focus on Bulgarian, and we create\nthe first benchmark dataset for evaluating the OCR text correction for\nhistorical Bulgarian documents written in the first standardized Bulgarian\northography: the Drinov orthography from the 19th century. We further develop a\nmethod for automatically generating synthetic data in this orthography, as well\nas in the subsequent Ivanchev orthography, by leveraging vast amounts of\ncontemporary literature Bulgarian texts. We then use state-of-the-art LLMs and\nencoder-decoder framework which we augment with diagonal attention loss and\ncopy and coverage mechanisms to improve the post-OCR text correction. The\nproposed method reduces the errors introduced during recognition and improves\nthe quality of the documents by 25\\%, which is an increase of 16\\% compared to\nthe state-of-the-art on the ICDAR 2019 Bulgarian dataset. We release our data\nand code at \\url{https://github.com/angelbeshirov/post-ocr-text-correction}.}", "published": "2024-08-31 19:27:46", "link": "http://arxiv.org/abs/2409.00527v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical\n  Vision-Language Zero-Shot Tasks?", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have\nsignificantly enhanced zero-shot medical vision tasks such as image\nclassification by leveraging large-scale medical image-text pair pre-training.\nHowever, the performance of these tasks can be heavily influenced by the\nvariability in textual prompts describing the categories, necessitating\nrobustness in MedVLP models to diverse prompt styles. Yet, this sensitivity\nremains underexplored. In this work, we are the first to systematically assess\nthe sensitivity of three widely-used MedVLP methods to a variety of prompts\nacross 15 different diseases. To achieve this, we designed six unique prompt\nstyles to mirror real clinical scenarios, which were subsequently ranked by\ninterpretability. Our findings indicate that all MedVLP models evaluated show\nunstable performance across different prompt styles, suggesting a lack of\nrobustness. Additionally, the models' performance varied with increasing prompt\ninterpretability, revealing difficulties in comprehending complex medical\nconcepts. This study underscores the need for further development in MedVLP\nmethodologies to enhance their robustness to diverse zero-shot prompts.", "published": "2024-08-31 20:43:06", "link": "http://arxiv.org/abs/2409.00543v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Large Language Models-Enabled Digital Twins for Precision Medicine in\n  Rare Gynecological Tumors", "abstract": "Rare gynecological tumors (RGTs) present major clinical challenges due to\ntheir low incidence and heterogeneity. The lack of clear guidelines leads to\nsuboptimal management and poor prognosis. Molecular tumor boards accelerate\naccess to effective therapies by tailoring treatment based on biomarkers,\nbeyond cancer type. Unstructured data that requires manual curation hinders\nefficient use of biomarker profiling for therapy matching. This study explores\nthe use of large language models (LLMs) to construct digital twins for\nprecision medicine in RGTs.\n  Our proof-of-concept digital twin system integrates clinical and biomarker\ndata from institutional and published cases (n=21) and literature-derived data\n(n=655 publications with n=404,265 patients) to create tailored treatment plans\nfor metastatic uterine carcinosarcoma, identifying options potentially missed\nby traditional, single-source analysis. LLM-enabled digital twins efficiently\nmodel individual patient trajectories. Shifting to a biology-based rather than\norgan-based tumor definition enables personalized care that could advance RGT\nmanagement and thus enhance patient outcomes.", "published": "2024-08-31 21:14:09", "link": "http://arxiv.org/abs/2409.00544v1", "categories": ["cs.CL", "cs.AI", "q-bio.QM", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Testing and Evaluation of Large Language Models: Correctness,\n  Non-Toxicity, and Fairness", "abstract": "Large language models (LLMs), such as ChatGPT, have rapidly penetrated into\npeople's work and daily lives over the past few years, due to their\nextraordinary conversational skills and intelligence. ChatGPT has become the\nfastest-growing software in terms of user numbers in human history and become\nan important foundational model for the next generation of artificial\nintelligence applications. However, the generations of LLMs are not entirely\nreliable, often producing content with factual errors, biases, and toxicity.\nGiven their vast number of users and wide range of application scenarios, these\nunreliable responses can lead to many serious negative impacts. This thesis\nintroduces the exploratory works in the field of language model reliability\nduring the PhD study, focusing on the correctness, non-toxicity, and fairness\nof LLMs from both software testing and natural language processing\nperspectives. First, to measure the correctness of LLMs, we introduce two\ntesting frameworks, FactChecker and LogicAsker, to evaluate factual knowledge\nand logical reasoning accuracy, respectively. Second, for the non-toxicity of\nLLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the\nfairness of LLMs, we introduce two evaluation frameworks, BiasAsker and\nXCulturalBench, to measure the social bias and cultural bias of LLMs,\nrespectively.", "published": "2024-08-31 22:21:04", "link": "http://arxiv.org/abs/2409.00551v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Learning to Ask: When LLM Agents Meet Unclear Instruction", "abstract": "Equipped with the capability to call functions, modern large language models\n(LLMs) can leverage external tools for addressing a range of tasks unattainable\nthrough language skills alone. However, the effective execution of these tools\nrelies heavily not just on the advanced capabilities of LLMs but also on\nprecise user instructions, which often cannot be ensured in the real world. To\nevaluate the performance of LLMs tool-use under imperfect instructions, we\nmeticulously examine the real-world instructions queried from users, analyze\nthe error patterns, and build a challenging tool-use benchmark called Noisy\nToolBench (NoisyToolBench). We find that due to the next-token prediction\ntraining objective, LLMs tend to arbitrarily generate the missed argument,\nwhich may lead to hallucinations and risks. To address this issue, we propose a\nnovel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to\nusers whenever they encounter obstacles due to unclear instructions. Moreover,\nto reduce the manual labor involved in user-LLM interaction and assess LLMs\nperformance in tool utilization from both accuracy and efficiency perspectives,\nwe design an automated evaluation tool named ToolEvaluator. Our experiments\ndemonstrate that the AwN significantly outperforms existing frameworks for tool\nlearning in the NoisyToolBench. We will release all related code and datasets\nto support future research.", "published": "2024-08-31 23:06:12", "link": "http://arxiv.org/abs/2409.00557v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TSO: Self-Training with Scaled Preference Optimization", "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences\nremains an ongoing research challenge. Recently, offline approaches such as\nDirect Preference Optimization (DPO) have gained prominence as attractive\noptions due to offering effective improvement in simple, efficient, and stable\nwithout interactions with reward models. However, these offline preference\noptimization methods highly rely on the quality of pairwise preference samples.\nMeanwhile, numerous iterative methods require additional training of reward\nmodels to select positive and negative samples from the model's own generated\nresponses for preference learning. Furthermore, as LLMs' capabilities advance,\nit is quite challenging to continuously construct high-quality positive and\nnegative preference instances from the model's outputs due to the lack of\ndiversity. To tackle these challenges, we propose TSO, or Self-Training with\nScaled Preference Optimization, a framework for preference optimization that\nconducts self-training preference learning without training an additional\nreward model. TSO enhances the diversity of responses by constructing a model\nmatrix and incorporating human preference responses. Furthermore, TSO\nintroduces corrections for model preference errors through human and AI\nfeedback. Finally, TSO adopts iterative and dual clip reward strategies to\nupdate the reference model and its responses, adaptively adjusting preference\ndata and balancing the optimization process. Experimental results demonstrate\nthat TSO outperforms existing mainstream methods on various alignment\nevaluation benchmarks, providing practical insight into preference data\nconstruction and model training strategies in the alignment domain.", "published": "2024-08-31 05:37:01", "link": "http://arxiv.org/abs/2409.02118v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CoRA: Optimizing Low-Rank Adaptation with Common Subspace of Large\n  Language Models", "abstract": "In fine-tuning large language models (LLMs), conserving computational\nresources while maintaining effectiveness and improving outcomes within the\nsame computational constraints is crucial. The Low-Rank Adaptation (LoRA)\nstrategy balances efficiency and performance in fine-tuning large models by\nreducing the number of trainable parameters and computational costs. However,\ncurrent advancements in LoRA might be focused on its fine-tuning methodologies,\nwith not as much exploration as might be expected into further compression of\nLoRA. Since most of LoRA's parameters might still be superfluous, this may lead\nto unnecessary wastage of computational resources. In this paper, we propose\n\\textbf{CoRA}: leveraging shared knowledge to optimize LoRA training by\nsubstituting its matrix $B$ with a common subspace from large models. Our\ntwo-fold method includes (1) Freezing the substitute matrix $B$ to halve\nparameters while training matrix $A$ for specific tasks and (2) Using the\nsubstitute matrix $B$ as an enhanced initial state for the original matrix $B$,\nachieving improved results with the same parameters. Our experiments show that\nthe first approach achieves the same efficacy as the original LoRA fine-tuning\nwhile being more efficient than halving parameters. At the same time, the\nsecond approach has some improvements compared to LoRA's original fine-tuning\nperformance. They generally attest to the effectiveness of our work.", "published": "2024-08-31 12:48:27", "link": "http://arxiv.org/abs/2409.02119v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HSF: Defending against Jailbreak Attacks with Hidden State Filtering", "abstract": "With the growing deployment of LLMs in daily applications like chatbots and\ncontent generation, efforts to ensure outputs align with human values and avoid\nharmful content have intensified. However, increasingly sophisticated jailbreak\nattacks threaten this alignment, aiming to induce unsafe outputs. Current\ndefense efforts either focus on prompt rewriting or detection, which are\nlimited in effectiveness due to the various design of jailbreak prompts, or on\noutput control and detection, which are computationally expensive as they\nrequire LLM inference. Therefore, designing a pre-inference defense method that\nresists diverse jailbreak prompts is crucial for preventing LLM jailbreak\nattacks. We observe that jailbreak attacks, safe queries, and harmful queries\nexhibit different clustering patterns within the LLM's hidden state\nrepresentation space. This suggests that by leveraging the LLM's hidden state\nrepresentational capabilities, we can analyze the LLM's forthcoming behavior\nand proactively intervene for defense. In this paper, we propose a jailbreak\nattack defense strategy based on a Hidden State Filter (HSF), a lossless\narchitectural defense mechanism that enables the model to preemptively identify\nand reject adversarial inputs before the inference process begins. We activate\nits defensive potential through an additional plugin module, effectively\nframing the defense task as a classification problem. Experimental results on\ntwo benchmark datasets, utilizing three different LLMs, show that HSF\nsignificantly enhances resilience against six cutting-edge jailbreak attacks.\nIt significantly reduces the success rate of jailbreak attacks while minimally\nimpacting responses to benign user queries, with negligible inference overhead,\nand outperforming defense baselines.Our code and data are available at\nhttps://anonymous.4open.science/r/Hidden-State-Filtering-8652/", "published": "2024-08-31 06:50:07", "link": "http://arxiv.org/abs/2409.03788v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Progressive Residual Extraction based Pre-training for Speech\n  Representation Learning", "abstract": "Self-supervised learning (SSL) has garnered significant attention in speech\nprocessing, excelling in linguistic tasks such as speech recognition. However,\njointly improving the performance of pre-trained models on various downstream\ntasks, each requiring different speech information, poses significant\nchallenges. To this purpose, we propose a progressive residual extraction based\nself-supervised learning method, named ProgRE. Specifically, we introduce two\nlightweight and specialized task modules into an encoder-style SSL backbone to\nenhance its ability to extract pitch variation and speaker information from\nspeech. Furthermore, to prevent the interference of reinforced pitch variation\nand speaker information with irrelevant content information learning, we\nresidually remove the information extracted by these two modules from the main\nbranch. The main branch is then trained using HuBERT's speech masking\nprediction to ensure the performance of the Transformer's deep-layer features\non content tasks. In this way, we can progressively extract pitch variation,\nspeaker, and content representations from the input speech. Finally, we can\ncombine multiple representations with diverse speech information using\ndifferent layer weights to obtain task-specific representations for various\ndownstream tasks. Experimental results indicate that our proposed method\nachieves joint performance improvements on various tasks, such as speaker\nidentification, speech recognition, emotion recognition, speech enhancement,\nand voice conversion, compared to excellent SSL methods such as wav2vec2.0,\nHuBERT, and WavLM.", "published": "2024-08-31 08:33:13", "link": "http://arxiv.org/abs/2409.00387v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module", "abstract": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks.", "published": "2024-08-31 15:26:57", "link": "http://arxiv.org/abs/2409.00481v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Real-Time Platform for Portable and Scalable Active Noise Mitigation\n  for Construction Machinery", "abstract": "This paper introduces a novel portable and scalable Active Noise Mitigation\n(PSANM) system designed to reduce low-frequency noise from construction\nmachinery. The PSANM system consists of portable units with autonomous\ncapabilities, optimized for stable performance within a specific power range.\nAn adaptive control algorithm with a variable penalty factor prevents the\nadaptive filter from over-driving the anti-noise actuators, avoiding non-linear\noperation and instability. This feature ensures the PSANM system can\nautonomously control noise at its source, allowing for continuous operation\nwithout human intervention. Additionally, the system includes a web server for\nremote management and is equipped with weather-resistant sensors and actuators,\nenhancing its usability in outdoor conditions. Laboratory and in-situ\nexperiments demonstrate the PSANM system's effectiveness in reducing\nconstruction-related low-frequency noise on a global scale. To further expand\nthe noise reduction zone, additional PSANM units can be strategically\npositioned in front of noise sources, enhancing the system's scalability.The\nPSANM system also provides a valuable prototyping platform for developing\nadaptive algorithms prior to deployment. Unlike many studies that rely solely\non simulation results under ideal conditions, this paper offers a holistic\nevaluation of the effectiveness of applying active noise control techniques\ndirectly at the noise source, demonstrating realistic and perceptible noise\nreduction. This work supports sustainable urban development by offering\ninnovative noise management solutions for the construction industry,\ncontributing to a quieter and more livable urban environment.", "published": "2024-08-31 06:14:23", "link": "http://arxiv.org/abs/2409.10534v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Contrastive Augmentation: An Unsupervised Learning Approach for Keyword\n  Spotting in Speech Technology", "abstract": "This paper addresses the persistent challenge in Keyword Spotting (KWS), a\nfundamental component in speech technology, regarding the acquisition of\nsubstantial labeled data for training. Given the difficulty in obtaining large\nquantities of positive samples and the laborious process of collecting new\ntarget samples when the keyword changes, we introduce a novel approach\ncombining unsupervised contrastive learning and a unique augmentation-based\ntechnique. Our method allows the neural network to train on unlabeled data\nsets, potentially improving performance in downstream tasks with limited\nlabeled data sets. We also propose that similar high-level feature\nrepresentations should be employed for speech utterances with the same keyword\ndespite variations in speed or volume. To achieve this, we present a speech\naugmentation-based unsupervised learning method that utilizes the similarity\nbetween the bottleneck layer feature and the audio reconstructing information\nfor auxiliary training. Furthermore, we propose a compressed convolutional\narchitecture to address potential redundancy and non-informative information in\nKWS tasks, enabling the model to simultaneously learn local features and focus\non long-term information. This method achieves strong performance on the Google\nSpeech Commands V2 Dataset. Inspired by recent advancements in sign spotting\nand spoken term detection, our method underlines the potential of our\ncontrastive learning approach in KWS and the advantages of Query-by-Example\nSpoken Term Detection strategies. The presented CAB-KWS provide new\nperspectives in the field of KWS, demonstrating effective ways to reduce data\ncollection efforts and increase the system's robustness.", "published": "2024-08-31 05:40:37", "link": "http://arxiv.org/abs/2409.00356v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Density Adaptive Attention-based Speech Network: Enhancing Feature\n  Understanding for Mental Health Disorders", "abstract": "Speech-based depression detection poses significant challenges for automated\ndetection due to its unique manifestation across individuals and data scarcity.\nAddressing these challenges, we introduce DAAMAudioCNNLSTM and\nDAAMAudioTransformer, two parameter efficient and explainable models for audio\nfeature extraction and depression detection. DAAMAudioCNNLSTM features a novel\nCNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),\nfocusing dynamically on informative speech segments. DAAMAudioTransformer,\nleveraging a transformer encoder in place of the CNN-LSTM architecture,\nincorporates the same DAAM module for enhanced attention and interpretability.\nThese approaches not only enhance detection robustness and interpretability but\nalso achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro\nscore of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the\nDAIC-WOZ dataset, without reliance on supplementary information such as vowel\npositions and speaker information during training/validation as in previous\napproaches. Both models' significant explainability and efficiency in\nleveraging speech signals for depression detection represent a leap towards\nmore reliable, clinically useful diagnostic tools, promising advancements in\nspeech and mental health care. To foster further research in this domain, we\nmake our code publicly available.", "published": "2024-08-31 08:50:28", "link": "http://arxiv.org/abs/2409.00391v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-label Zero-Shot Audio Classification with Temporal Attention", "abstract": "Zero-shot learning models are capable of classifying new classes by\ntransferring knowledge from the seen classes using auxiliary information. While\nmost of the existing zero-shot learning methods focused on single-label\nclassification tasks, the present study introduces a method to perform\nmulti-label zero-shot audio classification. To address the challenge of\nclassifying multi-label sounds while generalizing to unseen classes, we adapt\ntemporal attention. The temporal attention mechanism assigns importance weights\nto different audio segments based on their acoustic and semantic compatibility,\nthus enabling the model to capture the varying dominance of different sound\nclasses within an audio sample by focusing on the segments most relevant for\neach class. This leads to more accurate multi-label zero-shot classification\nthan methods employing temporally aggregated acoustic features without\nweighting, which treat all audio segments equally. We evaluate our approach on\na subset of AudioSet against a zero-shot model using uniformly aggregated\nacoustic features, a zero-rule baseline, and the proposed method in the\nsupervised scenario. Our results show that temporal attention enhances the\nzero-shot audio classification performance in multi-label scenario.", "published": "2024-08-31 09:49:41", "link": "http://arxiv.org/abs/2409.00408v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Digit Recognition using Multimodal Spiking Neural Networks", "abstract": "Spiking neural networks (SNNs) are the third generation of neural networks\nthat are biologically inspired to process data in a fashion that emulates the\nexchange of signals in the brain. Within the Computer Vision community SNNs\nhave garnered significant attention due in large part to the availability of\nevent-based sensors that produce a spatially resolved spike train in response\nto changes in scene radiance. SNNs are used to process event-based data due to\ntheir neuromorphic nature. The proposed work examines the neuromorphic\nadvantage of fusing multiple sensory inputs in classification tasks.\nSpecifically we study the performance of a SNN in digit classification by\npassing in a visual modality branch (Neuromorphic-MNIST [N-MNIST]) and an\nauditory modality branch (Spiking Heidelberg Digits [SHD]) from datasets that\nwere created using event-based sensors to generate a series of time-dependent\nevents. It is observed that multi-modal SNNs outperform unimodal visual and\nunimodal auditory SNNs. Furthermore, it is observed that the process of sensory\nfusion is insensitive to the depth at which the visual and auditory branches\nare combined. This work achieves a 98.43% accuracy on the combined N-MNIST and\nSHD dataset using a multimodal SNN that concatenates the visual and auditory\nbranches at a late depth.", "published": "2024-08-31 22:27:40", "link": "http://arxiv.org/abs/2409.00552v1", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparative Analysis of Modality Fusion Approaches for Audio-Visual\n  Person Identification and Verification", "abstract": "Multimodal learning involves integrating information from various modalities\nto enhance learning and comprehension. We compare three modality fusion\nstrategies in person identification and verification by processing two\nmodalities: voice and face. In this paper, a one-dimensional convolutional\nneural network is employed for x-vector extraction from voice, while the\npre-trained VGGFace2 network and transfer learning are utilized for face\nmodality. In addition, gammatonegram is used as speech representation in\nengagement with the Darknet19 pre-trained network. The proposed systems are\nevaluated using the K-fold cross-validation technique on the 118 speakers of\nthe test set of the VoxCeleb2 dataset. The comparative evaluations are done for\nsingle-modality and three proposed multimodal strategies in equal situations.\nResults demonstrate that the feature fusion strategy of gammatonegram and\nfacial features achieves the highest performance, with an accuracy of 98.37% in\nthe person identification task. However, concatenating facial features with the\nx-vector reaches 0.62% for EER in verification tasks.", "published": "2024-08-31 23:22:30", "link": "http://arxiv.org/abs/2409.00562v2", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Co-Speech Gesture Representations in Dialogue through\n  Contrastive Learning: An Intrinsic Evaluation", "abstract": "In face-to-face dialogues, the form-meaning relationship of co-speech\ngestures varies depending on contextual factors such as what the gestures refer\nto and the individual characteristics of speakers. These factors make co-speech\ngesture representation learning challenging. How can we learn meaningful\ngestures representations considering gestures' variability and relationship\nwith speech? This paper tackles this challenge by employing self-supervised\ncontrastive learning techniques to learn gesture representations from skeletal\nand speech information. We propose an approach that includes both unimodal and\nmultimodal pre-training to ground gesture representations in co-occurring\nspeech. For training, we utilize a face-to-face dialogue dataset rich with\nrepresentational iconic gestures. We conduct thorough intrinsic evaluations of\nthe learned representations through comparison with human-annotated pairwise\ngesture similarity. Moreover, we perform a diagnostic probing analysis to\nassess the possibility of recovering interpretable gesture features from the\nlearned representations. Our results show a significant positive correlation\nwith human-annotated gesture similarity and reveal that the similarity between\nthe learned representations is consistent with well-motivated patterns related\nto the dynamics of dialogue interaction. Moreover, our findings demonstrate\nthat several features concerning the form of gestures can be recovered from the\nlatent representations. Overall, this study shows that multimodal contrastive\nlearning is a promising approach for learning gesture representations, which\nopens the door to using such representations in larger-scale gesture analysis\nstudies.", "published": "2024-08-31 08:53:18", "link": "http://arxiv.org/abs/2409.10535v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS", "I.4"], "primary_category": "cs.CV"}
{"title": "Multi-scale Multi-instance Visual Sound Localization and Segmentation", "abstract": "Visual sound localization is a typical and challenging problem that predicts\nthe location of objects corresponding to the sound source in a video. Previous\nmethods mainly used the audio-visual association between global audio and\none-scale visual features to localize sounding objects in each image. Despite\ntheir promising performance, they omitted multi-scale visual features of the\ncorresponding image, and they cannot learn discriminative regions compared to\nground truths. To address this issue, we propose a novel multi-scale\nmulti-instance visual sound localization framework, namely M2VSL, that can\ndirectly learn multi-scale semantic features associated with sound sources from\nthe input image to localize sounding objects. Specifically, our M2VSL leverages\nlearnable multi-scale visual features to align audio-visual representations at\nmulti-level locations of the corresponding image. We also introduce a novel\nmulti-scale multi-instance transformer to dynamically aggregate multi-scale\ncross-modal representations for visual sound localization. We conduct extensive\nexperiments on VGGSound-Instruments, VGG-Sound Sources, and AVSBench\nbenchmarks. The results demonstrate that the proposed M2VSL can achieve\nstate-of-the-art performance on sounding object localization and segmentation.", "published": "2024-08-31 15:43:22", "link": "http://arxiv.org/abs/2409.00486v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
