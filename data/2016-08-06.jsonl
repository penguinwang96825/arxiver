{"title": "Desiderata for Vector-Space Word Representations", "abstract": "A plethora of vector-space representations for words is currently available,\nwhich is growing. These consist of fixed-length vectors containing real values,\nwhich represent a word. The result is a representation upon which the power of\nmany conventional information processing and data mining techniques can be\nbrought to bear, as long as the representations are designed with some\nforethought and fit certain constraints. This paper details desiderata for the\ndesign of vector space representations of words.", "published": "2016-08-06 10:47:05", "link": "http://arxiv.org/abs/1608.02094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken\n  Language Understanding", "abstract": "This paper investigates the framework of encoder-decoder with attention for\nsequence labelling based spoken language understanding. We introduce\nBidirectional Long Short Term Memory - Long Short Term Memory networks\n(BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep\nlearning. In the sequence labelling task, the input and output sequences are\naligned word by word, while the attention mechanism cannot provide the exact\nalignment. To address this limitation, we propose a novel focus mechanism for\nencoder-decoder framework. Experiments on the standard ATIS dataset showed that\nBLSTM-LSTM with focus mechanism defined the new state-of-the-art by\noutperforming standard BLSTM and attention based encoder-decoder. Further\nexperiments also show that the proposed model is more robust to speech\nrecognition errors.", "published": "2016-08-06 11:41:05", "link": "http://arxiv.org/abs/1608.02097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment", "abstract": "We introduce HyperLex - a dataset and evaluation resource that quantifies the\nextent of of the semantic category membership, that is, type-of relation also\nknown as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616\nconcept pairs. Cognitive psychology research has established that typicality\nand category/class membership are computed in human semantic memory as a\ngradual rather than binary relation. Nevertheless, most NLP research, and\nexisting large-scale invetories of concept category membership (WordNet,\nDBPedia, etc.) treat category membership and LE as binary. To address this, we\nasked hundreds of native English speakers to indicate typicality and strength\nof category membership between a diverse range of concept pairs on a\ncrowdsourcing platform. Our results confirm that category membership and LE are\nindeed more gradual than binary. We then compare these human judgements with\nthe predictions of automatic systems, which reveals a huge gap between human\nperformance and state-of-the-art LE, distributional and representation learning\nmodels, and substantial differences between the models themselves. We discuss a\npathway for improving semantic models to overcome this discrepancy, and\nindicate future application areas for improved graded LE systems.", "published": "2016-08-06 15:29:34", "link": "http://arxiv.org/abs/1608.02117v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Knowledge from Text to Predict Disease Onset", "abstract": "In many domains such as medicine, training data is in short supply. In such\ncases, external knowledge is often helpful in building predictive models. We\npropose a novel method to incorporate publicly available domain expertise to\nbuild accurate models. Specifically, we use word2vec models trained on a\ndomain-specific corpus to estimate the relevance of each feature's text\ndescription to the prediction problem. We use these relevance estimates to\nrescale the features, causing more important features to experience weaker\nregularization.\n  We apply our method to predict the onset of five chronic diseases in the next\nfive years in two genders and two age groups. Our rescaling approach improves\nthe accuracy of the model, particularly when there are few positive examples.\nFurthermore, our method selects 60% fewer features, easing interpretation by\nphysicians. Our method is applicable to other domains where feature and outcome\ndescriptions are available.", "published": "2016-08-06 06:24:59", "link": "http://arxiv.org/abs/1608.02071v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OCR of historical printings with an application to building diachronic\n  corpora: A case study using the RIDGES herbal corpus", "abstract": "This article describes the results of a case study that applies Neural\nNetwork-based Optical Character Recognition (OCR) to scanned images of books\nprinted between 1487 and 1870 by training the OCR engine OCRopus\n[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].\nTraining specific OCR models was possible because the necessary *ground truth*\nis available as error-corrected diplomatic transcriptions. The OCR results have\nbeen evaluated for accuracy against the ground truth of unseen test sets.\nCharacter and word accuracies (percentage of correctly recognized items) for\nthe resulting machine-readable texts of individual documents range from 94% to\nmore than 99% (character level) and from 76% to 97% (word level). This includes\nthe earliest printed books, which were thought to be inaccessible by OCR\nmethods until recently. Furthermore, OCR models trained on one part of the\ncorpus consisting of books with different printing dates and different typesets\n*(mixed models)* have been tested for their predictive power on the books from\nthe other part containing yet other fonts, mostly yielding character accuracies\nwell above 90%. It therefore seems possible to construct generalized models\ntrained on a range of fonts that can be applied to a wide variety of historical\nprintings still giving good results. A moderate postcorrection effort of some\npages will then enable the training of individual models with even better\naccuracies. Using this method, diachronic corpora including early printings can\nbe constructed much faster and cheaper than by manual transcription. The OCR\nmethods reported here open up the possibility of transforming our printed\ntextual cultural heritage into electronic text by largely automatic means,\nwhich is a prerequisite for the mass conversion of scanned books.", "published": "2016-08-06 20:51:53", "link": "http://arxiv.org/abs/1608.02153v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Bi-directional Attention with Agreement for Dependency Parsing", "abstract": "We develop a novel bi-directional attention model for dependency parsing,\nwhich learns to agree on headword predictions from the forward and backward\nparsing directions. The parsing procedure for each direction is formulated as\nsequentially querying the memory component that stores continuous headword\nembeddings. The proposed parser makes use of {\\it soft} headword embeddings,\nallowing the model to implicitly capture high-order parsing history without\ndramatically increasing the computational complexity. We conduct experiments on\nEnglish, Chinese, and 12 other languages from the CoNLL 2006 shared task,\nshowing that the proposed model achieves state-of-the-art unlabeled attachment\nscores on 6 languages.", "published": "2016-08-06 07:16:31", "link": "http://arxiv.org/abs/1608.02076v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
