{"title": "Transfer Learning for Context-Aware Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) is a key component of task-oriented\ndialogue systems. SLU parses natural language user utterances into semantic\nframes. Previous work has shown that incorporating context information\nsignificantly improves SLU performance for multi-turn dialogues. However,\ncollecting a large-scale human-labeled multi-turn dialogue corpus for the\ntarget domains is complex and costly. To reduce dependency on the collection\nand annotation effort, we propose a Context Encoding Language Transformer\n(CELT) model facilitating exploiting various context information for SLU. We\nexplore different transfer learning approaches to reduce dependency on data\ncollection and annotation. In addition to unsupervised pre-training using\nlarge-scale general purpose unlabeled corpora, such as Wikipedia, we explore\nunsupervised and supervised adaptive training approaches for transfer learning\nto benefit from other in-domain and out-of-domain dialogue corpora.\nExperimental results demonstrate that the proposed model with the proposed\ntransfer learning approaches achieves significant improvement on the SLU\nperformance over state-of-the-art models on two large-scale single-turn\ndialogue benchmarks and one large-scale multi-turn dialogue benchmark.", "published": "2020-03-03 02:56:36", "link": "http://arxiv.org/abs/2003.01305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Candidate Generation for Low-resource Cross-lingual Entity\n  Linking", "abstract": "Cross-lingual entity linking (XEL) is the task of finding referents in a\ntarget-language knowledge base (KB) for mentions extracted from source-language\ntexts. The first step of (X)EL is candidate generation, which retrieves a list\nof plausible candidate entities from the target-language KB for each mention.\nApproaches based on resources from Wikipedia have proven successful in the\nrealm of relatively high-resource languages (HRL), but these do not extend well\nto low-resource languages (LRL) with few, if any, Wikipedia pages. Recently,\ntransfer learning methods have been shown to reduce the demand for resources in\nthe LRL by utilizing resources in closely-related languages, but the\nperformance still lags far behind their high-resource counterparts. In this\npaper, we first assess the problems faced by current entity candidate\ngeneration methods for low-resource XEL, then propose three improvements that\n(1) reduce the disconnect between entity mentions and KB entries, and (2)\nimprove the robustness of the model to low-resource scenarios. The methods are\nsimple, but effective: we experiment with our approach on seven XEL datasets\nand find that they yield an average gain of 16.9% in Top-30 gold candidate\nrecall, compared to state-of-the-art baselines. Our improved model also yields\nan average gain of 7.9% in in-KB accuracy of end-to-end XEL.", "published": "2020-03-03 05:32:09", "link": "http://arxiv.org/abs/2003.01343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language\n  Model", "abstract": "In this paper, we introduce the Chinese corpus from CLUE organization,\nCLUECorpus2020, a large-scale corpus that can be used directly for\nself-supervised learning such as pre-training of a language model, or language\ngeneration. It has 100G raw corpus with 35 billion Chinese characters, which is\nretrieved from Common Crawl. To better understand this corpus, we conduct\nlanguage understanding experiments on both small and large scale, and results\nshow that the models trained on this corpus can achieve excellent performance\non Chinese. We release a new Chinese vocabulary with a size of 8K, which is\nonly one-third of the vocabulary size used in Chinese Bert released by Google.\nIt saves computational cost and memory while works as good as original\nvocabulary. We also release both large and tiny versions of the pre-trained\nmodel on this corpus. The former achieves the state-of-the-art result, and the\nlatter retains most precision while accelerating training and prediction speed\nfor eight times compared to Bert-base. To facilitate future work on\nself-supervised learning on Chinese, we release our dataset, new vocabulary,\ncodes, and pre-trained models on Github.", "published": "2020-03-03 06:39:27", "link": "http://arxiv.org/abs/2003.01355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Embeddings Based On Self-Attention", "abstract": "Creating meta-embeddings for better performance in language modelling has\nreceived attention lately, and methods based on concatenation or merely\ncalculating the arithmetic mean of more than one separately trained embeddings\nto perform meta-embeddings have shown to be beneficial. In this paper, we\ndevise a new meta-embedding model based on the self-attention mechanism, namely\nthe Duo. With less than 0.4M parameters, the Duo mechanism achieves\nstate-of-the-art accuracy in text classification tasks such as 20NG.\nAdditionally, we propose a new meta-embedding sequece-to-sequence model for\nmachine translation, which to the best of our knowledge, is the first machine\ntranslation model based on more than one word-embedding. Furthermore, it has\nturned out that our model outperform the Transformer not only in terms of\nachieving a better result, but also a faster convergence on recognized\nbenchmarks, such as the WMT 2014 English-to-French translation task.", "published": "2020-03-03 07:34:24", "link": "http://arxiv.org/abs/2003.01371v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seshat: A tool for managing and verifying annotation campaigns of audio\n  data", "abstract": "We introduce Seshat, a new, simple and open-source software to efficiently\nmanage annotations of speech corpora. The Seshat software allows users to\neasily customise and manage annotations of large audio corpora while ensuring\ncompliance with the formatting and naming conventions of the annotated output\nfiles. In addition, it includes procedures for checking the content of\nannotations following specific rules that can be implemented in personalised\nparsers. Finally, we propose a double-annotation mode, for which Seshat\ncomputes automatically an associated inter-annotator agreement with the\n$\\gamma$ measure taking into account the categorisation and segmentation\ndiscrepancies.", "published": "2020-03-03 12:11:12", "link": "http://arxiv.org/abs/2003.01472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Generative-Retrieval Transformers for Dialogue Domain Adaptation", "abstract": "Domain adaptation has recently become a key problem in dialogue systems\nresearch. Deep learning, while being the preferred technique for modeling such\nsystems, works best given massive training data. However, in the real-world\nscenario, such resources aren't available for every new domain, so the ability\nto train with a few dialogue examples can be considered essential. Pre-training\non large data sources and adapting to the target data has become the standard\nmethod for few-shot problems within the deep learning framework. In this paper,\nwe present the winning entry at the fast domain adaptation task of DSTC8, a\nhybrid generative-retrieval model based on GPT-2 fine-tuned to the multi-domain\nMetaLWOz dataset. Robust and diverse in response generation, our model uses\nretrieval logic as a fallback, being SoTA on MetaLWOz in human evaluation (>4%\nimprovement over the 2nd place system) and attaining competitive generalization\nperformance in adaptation to the unseen MultiWOZ dataset.", "published": "2020-03-03 18:07:42", "link": "http://arxiv.org/abs/2003.01680v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks\n  with Hyperdimensional Computing enabled Embedding of n-gram Statistics", "abstract": "Recent advances in Deep Learning have led to a significant performance\nincrease on several NLP tasks, however, the models become more and more\ncomputationally demanding. Therefore, this paper tackles the domain of\ncomputationally efficient algorithms for NLP tasks. In particular, it\ninvestigates distributed representations of n-gram statistics of texts. The\nrepresentations are formed using hyperdimensional computing enabled embedding.\nThese representations then serve as features, which are used as input to\nstandard classifiers. We investigate the applicability of the embedding on one\nlarge and three small standard datasets for classification tasks using nine\nclassifiers. The embedding achieved on par F1 scores while decreasing the time\nand memory requirements by several times compared to the conventional n-gram\nstatistics, e.g., for one of the classifiers on a small dataset, the memory\nreduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84\ntimes, respectively. For many classifiers on the large dataset, memory\nreduction was ca. 100 times and train and test speed-ups were over 100 times.\nImportantly, the usage of distributed representations formed via\nhyperdimensional computing allows dissecting strict dependency between the\ndimensionality of the representation and n-gram size, thus, opening a room for\ntradeoffs.", "published": "2020-03-03 22:44:10", "link": "http://arxiv.org/abs/2003.01821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Neural Networks for Noetic End-to-End Response Selection", "abstract": "The noetic end-to-end response selection challenge as one track in the 7th\nDialog System Technology Challenges (DSTC7) aims to push the state of the art\nof utterance classification for real world goal-oriented dialog systems, for\nwhich participants need to select the correct next utterances from a set of\ncandidates for the multi-turn context. This paper presents our systems that are\nranked top 1 on both datasets under this challenge, one focused and small\n(Advising) and the other more diverse and large (Ubuntu). Previous\nstate-of-the-art models use hierarchy-based (utterance-level and token-level)\nneural networks to explicitly model the interactions among different turns'\nutterances for context modeling. In this paper, we investigate a sequential\nmatching model based only on chain sequence for multi-turn response selection.\nOur results demonstrate that the potentials of sequential matching approaches\nhave not yet been fully exploited in the past for multi-turn response\nselection. In addition to ranking top 1 in the challenge, the proposed model\noutperforms all previous models, including state-of-the-art hierarchy-based\nmodels, on two large-scale public multi-turn response selection benchmark\ndatasets.", "published": "2020-03-03 04:36:33", "link": "http://arxiv.org/abs/2003.02126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Context Enhanced Multi-Domain Dialogue System for\n  Multi-domain Task Completion", "abstract": "Task 1 of the DSTC8-track1 challenge aims to develop an end-to-end\nmulti-domain dialogue system to accomplish complex users' goals under tourist\ninformation desk settings. This paper describes our submitted solution,\nHierarchical Context Enhanced Dialogue System (HCEDS), for this task. The main\nmotivation of our system is to comprehensively explore the potential of\nhierarchical context for sufficiently understanding complex dialogues. More\nspecifically, we apply BERT to capture token-level information and employ the\nattention mechanism to capture sentence-level information. The results listed\nin the leaderboard show that our system achieves first place in automatic\nevaluation and the second place in human evaluation.", "published": "2020-03-03 05:10:13", "link": "http://arxiv.org/abs/2003.01338v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Benchmark Performance of Machine And Deep Learning Based Methodologies\n  for Urdu Text Document Classification", "abstract": "In order to provide benchmark performance for Urdu text document\nclassification, the contribution of this paper is manifold. First, it pro-vides\na publicly available benchmark dataset manually tagged against 6 classes.\nSecond, it investigates the performance impact of traditional machine learning\nbased Urdu text document classification methodologies by embedding 10\nfilter-based feature selection algorithms which have been widely used for other\nlanguages. Third, for the very first time, it as-sesses the performance of\nvarious deep learning based methodologies for Urdu text document\nclassification. In this regard, for experimentation, we adapt 10 deep learning\nclassification methodologies which have pro-duced best performance figures for\nEnglish text classification. Fourth, it also investigates the performance\nimpact of transfer learning by utiliz-ing Bidirectional Encoder Representations\nfrom Transformers approach for Urdu language. Fifth, it evaluates the integrity\nof a hybrid approach which combines traditional machine learning based feature\nengineering and deep learning based automated feature engineering. Experimental\nresults show that feature selection approach named as Normalised Dif-ference\nMeasure along with Support Vector Machine outshines state-of-the-art\nperformance on two closed source benchmark datasets CLE Urdu Digest 1000k, and\nCLE Urdu Digest 1Million with a significant margin of 32%, and 13%\nrespectively. Across all three datasets, Normalised Differ-ence Measure\noutperforms other filter based feature selection algorithms as it significantly\nuplifts the performance of all adopted machine learning, deep learning, and\nhybrid approaches. The source code and presented dataset are available at\nGithub repository.", "published": "2020-03-03 05:49:55", "link": "http://arxiv.org/abs/2003.01345v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discover Your Social Identity from What You Tweet: a Content Based\n  Approach", "abstract": "An identity denotes the role an individual or a group plays in highly\ndifferentiated contemporary societies. In this paper, our goal is to classify\nTwitter users based on their role identities. We first collect a coarse-grained\npublic figure dataset automatically, then manually label a more fine-grained\nidentity dataset. We propose a hierarchical self-attention neural network for\nTwitter user role identity classification. Our experiments demonstrate that the\nproposed model significantly outperforms multiple baselines. We further propose\na transfer learning scheme that improves our model's performance by a large\nmargin. Such transfer learning also greatly reduces the need for a large amount\nof human labeled data.", "published": "2020-03-03 21:13:12", "link": "http://arxiv.org/abs/2003.01797v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "GenNet : Reading Comprehension with Multiple Choice Questions using\n  Generation and Selection model", "abstract": "Multiple-choice machine reading comprehension is difficult task as its\nrequired machines to select the correct option from a set of candidate or\npossible options using the given passage and question.Reading Comprehension\nwith Multiple Choice Questions task,required a human (or machine) to read a\ngiven passage, question pair and select the best one option from n given\noptions. There are two different ways to select the correct answer from the\ngiven passage. Either by selecting the best match answer to by eliminating the\nworst match answer. Here we proposed GenNet model, a neural network-based\nmodel. In this model first we will generate the answer of the question from the\npassage and then will matched the generated answer with given answer, the best\nmatched option will be our answer. For answer generation we used S-net (Tan et\nal., 2017) model trained on SQuAD and to evaluate our model we used Large-scale\nRACE (ReAding Comprehension Dataset From Examinations) (Lai et al.,2017).", "published": "2020-03-03 20:35:36", "link": "http://arxiv.org/abs/2003.04360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the influence Brexit had on Financial Markets, in\n  particular the GBP/EUR exchange rate", "abstract": "On 23rd June 2016, 51.9% of British voters voted to leave the European Union,\ntriggering a process and events that have led to the United Kingdom leaving the\nEU, an event that has become known as 'Brexit'. In this piece of research, we\ninvestigate the effects of this entire process on the currency markets,\nspecifically the GBP/EUR exchange rate. Financial markets are known to be\nsensitive to news articles and media, and the aim of this research is to\nevaluate the magnitude of impact of relevant events, as well as whether the\nimpact was positive or negative for the GBP.", "published": "2020-03-03 22:05:37", "link": "http://arxiv.org/abs/2003.05895v1", "categories": ["q-fin.ST", "cs.CL"], "primary_category": "q-fin.ST"}
{"title": "Med7: a transferable clinical natural language processing model for\n  electronic health records", "abstract": "The field of clinical natural language processing has been advanced\nsignificantly since the introduction of deep learning models. The\nself-supervised representation learning and the transfer learning paradigm\nbecame the methods of choice in many natural language processing application,\nin particular in the settings with the dearth of high quality manually\nannotated data. Electronic health record systems are ubiquitous and the\nmajority of patients' data are now being collected electronically and in\nparticular in the form of free text. Identification of medical concepts and\ninformation extraction is a challenging task, yet important ingredient for\nparsing unstructured data into structured and tabulated format for downstream\nanalytical tasks. In this work we introduced a named-entity recognition model\nfor clinical natural language processing. The model is trained to recognise\nseven categories: drug names, route, frequency, dosage, strength, form,\nduration. The model was first self-supervisedly pre-trained by predicting the\nnext word, using a collection of 2 million free-text patients' records from\nMIMIC-III corpora and then fine-tuned on the named-entity recognition task. The\nmodel achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893)\nacross all seven categories. Additionally, we evaluated the transferability of\nthe developed model using the data from the Intensive Care Unit in the US to\nsecondary care mental health records (CRIS) in the UK. A direct application of\nthe trained NER model to CRIS data resulted in reduced performance of F1=0.762,\nhowever after fine-tuning on a small sample from CRIS, the model achieved a\nreasonable performance of F1=0.944. This demonstrated that despite a close\nsimilarity between the data sets and the NER tasks, it is essential to\nfine-tune on the target domain data in order to achieve more accurate results.", "published": "2020-03-03 00:55:43", "link": "http://arxiv.org/abs/2003.01271v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controllable Time-Delay Transformer for Real-Time Punctuation Prediction\n  and Disfluency Detection", "abstract": "With the increased applications of automatic speech recognition (ASR) in\nrecent years, it is essential to automatically insert punctuation marks and\nremove disfluencies in transcripts, to improve the readability of the\ntranscripts as well as the performance of subsequent applications, such as\nmachine translation, dialogue systems, and so forth. In this paper, we propose\na Controllable Time-delay Transformer (CT-Transformer) model that jointly\ncompletes the punctuation prediction and disfluency detection tasks in real\ntime. The CT-Transformer model facilitates freezing partial outputs with\ncontrollable time delay to fulfill the real-time constraints in partial\ndecoding required by subsequent applications. We further propose a fast\ndecoding strategy to minimize latency while maintaining competitive\nperformance. Experimental results on the IWSLT2011 benchmark dataset and an\nin-house Chinese annotated dataset demonstrate that the proposed approach\noutperforms the previous state-of-the-art models on F-scores and achieves a\ncompetitive inference speed.", "published": "2020-03-03 03:17:29", "link": "http://arxiv.org/abs/2003.01309v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding the Prediction Mechanism of Sentiments by XAI\n  Visualization", "abstract": "People often rely on online reviews to make purchase decisions. The present\nwork aimed to gain an understanding of a machine learning model's prediction\nmechanism by visualizing the effect of sentiments extracted from online hotel\nreviews with explainable AI (XAI) methodology. Study 1 used the extracted\nsentiments as features to predict the review ratings by five machine learning\nalgorithms (knn, CART decision trees, support vector machines, random forests,\ngradient boosting machines) and identified random forests as best algorithm.\nStudy 2 analyzed the random forests model by feature importance and revealed\nthe sentiments joy, disgust, positive and negative as the most predictive\nfeatures. Furthermore, the visualization of additive variable attributions and\ntheir prediction distribution showed correct prediction in direction and effect\nsize for the 5-star rating but partially wrong direction and insufficient\neffect size for the 1-star rating. These prediction details were corroborated\nby a what-if analysis for the four top features. In conclusion, the prediction\nmechanism of a machine learning model can be uncovered by visualization of\nparticular observations. Comparing instances of contrasting ground truth values\ncan draw a differential picture of the prediction mechanism and inform\ndecisions for model improvement.", "published": "2020-03-03 10:25:50", "link": "http://arxiv.org/abs/2003.01425v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "XGPT: Cross-modal Generative Pre-Training for Image Captioning", "abstract": "While many BERT-based cross-modal pre-trained models produce excellent\nresults on downstream understanding tasks like image-text retrieval and VQA,\nthey cannot be applied to generation tasks directly. In this paper, we propose\nXGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning\nthat is designed to pre-train text-to-image caption generators through three\nnovel generation tasks, including Image-conditioned Masked Language Modeling\n(IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned\nImage Feature Generation (TIFG). As a result, the pre-trained XGPT can be\nfine-tuned without any task-specific architecture modifications to create\nstate-of-the-art models for image captioning. Experiments show that XGPT\nobtains new state-of-the-art results on the benchmark datasets, including COCO\nCaptions and Flickr30k Captions. We also use XGPT to generate new image\ncaptions as data augmentation for the image retrieval task and achieve\nsignificant improvement on all recall metrics.", "published": "2020-03-03 12:13:06", "link": "http://arxiv.org/abs/2003.01473v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning with Auxiliary Speaker Identification for\n  Conversational Emotion Recognition", "abstract": "Conversational emotion recognition (CER) has attracted increasing interests\nin the natural language processing (NLP) community. Different from the vanilla\nemotion recognition, effective speaker-sensitive utterance representation is\none major challenge for CER. In this paper, we exploit speaker identification\n(SI) as an auxiliary task to enhance the utterance representation in\nconversations. By this method, we can learn better speaker-aware contextual\nrepresentations from the additional SI corpus. Experiments on two benchmark\ndatasets demonstrate that the proposed architecture is highly effective for\nCER, obtaining new state-of-the-art results on two datasets.", "published": "2020-03-03 12:25:03", "link": "http://arxiv.org/abs/2003.01478v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Uyghur ASR systems with decoders using morpheme-based language\n  models", "abstract": "Uyghur is a minority language, and its resources for Automatic Speech\nRecognition (ASR) research are always insufficient. THUYG-20 is currently the\nonly open-sourced dataset of Uyghur speeches. State-of-the-art results of its\nclean and noiseless speech test task haven't been updated since the first\nrelease, which shows a big gap in the development of ASR between mainstream\nlanguages and Uyghur. In this paper, we try to bridge the gap by ultimately\noptimizing the ASR systems, and by developing a morpheme-based decoder,\nMLDG-Decoder (Morpheme Lattice Dynamically Generating Decoder for Uyghur\nDNN-HMM systems), which has long been missing. We have open-sourced the\ndecoder. The MLDG-Decoder employs an algorithm, named as \"on-the-fly\ncomposition with FEBABOS\", to allow the back-off states and transitions to play\nthe role of a relay station in on-the-fly composition. The algorithm empowers\nthe dynamically generated graph to constrain the morpheme sequences in the\nlattices as effectively as the static and fully composed graph does when a\n4-Gram morpheme-based Language Model (LM) is used. We have trained deeper and\nwider neural network acoustic models, and experimented with three kinds of\ndecoding schemes. The experimental results show that the decoding based on the\nstatic and fully composed graph reduces state-of-the-art Word Error Rate (WER)\non the clean and noiseless speech test task in THUYG-20 to 14.24%. The\nMLDG-Decoder reduces the WER to 14.54% while keeping the memory consumption\nreasonable. Based on the open-sourced MLDG-Decoder, readers can easily\nreproduce the experimental results in this paper.", "published": "2020-03-03 14:11:45", "link": "http://arxiv.org/abs/2003.01509v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Real-time Mispronunciation Detection in Kids' Speech", "abstract": "Modern mispronunciation detection and diagnosis systems have seen significant\ngains in accuracy due to the introduction of deep learning. However, these\nsystems have not been evaluated for the ability to be run in real-time, an\nimportant factor in applications that provide rapid feedback. In particular,\nthe state-of-the-art uses bi-directional recurrent networks, where a\nuni-directional network may be more appropriate. Teacher-student learning is a\nnatural approach to use to improve a uni-directional model, but when using a\nCTC objective, this is limited by poor alignment of outputs to evidence. We\naddress this limitation by trying two loss terms for improving the alignments\nof our models. One loss is an \"alignment loss\" term that encourages outputs\nonly when features do not resemble silence. The other loss term uses a\nuni-directional model as teacher model to align the bi-directional model. Our\nproposed model uses these aligned bi-directional models as teacher models.\nExperiments on the CSLU kids' corpus show that these changes decrease the\nlatency of the outputs, and improve the detection rates, with a trade-off\nbetween these goals.", "published": "2020-03-03 19:58:43", "link": "http://arxiv.org/abs/2003.01765v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phonetic Feedback for Speech Enhancement With and Without Parallel\n  Speech Data", "abstract": "While deep learning systems have gained significant ground in speech\nenhancement research, these systems have yet to make use of the full potential\nof deep learning systems to provide high-level feedback. In particular,\nphonetic feedback is rare in speech enhancement research even though it\nincludes valuable top-down information. We use the technique of mimic loss to\nprovide phonetic feedback to an off-the-shelf enhancement system, and find\ngains in objective intelligibility scores on CHiME-4 data. This technique takes\na frozen acoustic model trained on clean speech to provide valuable feedback to\nthe enhancement model, even in the case where no parallel speech data is\navailable. Our work is one of the first to show intelligibility improvement for\nneural enhancement systems without parallel speech data, and we show phonetic\nfeedback can improve a state-of-the-art neural enhancement system trained with\nparallel speech data.", "published": "2020-03-03 20:06:24", "link": "http://arxiv.org/abs/2003.01769v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Untangling in Invariant Speech Recognition", "abstract": "Encouraged by the success of deep neural networks on a variety of visual\ntasks, much theoretical and experimental work has been aimed at understanding\nand interpreting how vision networks operate. Meanwhile, deep neural networks\nhave also achieved impressive performance in audio processing applications,\nboth as sub-components of larger systems and as complete end-to-end systems by\nthemselves. Despite their empirical successes, comparatively little is\nunderstood about how these audio models accomplish these tasks. In this work,\nwe employ a recently developed statistical mechanical theory that connects\ngeometric properties of network representations and the separability of classes\nto probe how information is untangled within neural networks trained to\nrecognize speech. We observe that speaker-specific nuisance variations are\ndiscarded by the network's hierarchy, whereas task-relevant properties such as\nwords and phonemes are untangled in later layers. Higher level concepts such as\nparts-of-speech and context dependence also emerge in the later layers of the\nnetwork. Finally, we find that the deep representations carry out significant\ntemporal untangling by efficiently extracting task-relevant features at each\ntime step of the computation. Taken together, these findings shed light on how\ndeep auditory models process time dependent input signals to achieve invariant\nspeech recognition, and show how different concepts emerge through the layers\nof the network.", "published": "2020-03-03 20:48:43", "link": "http://arxiv.org/abs/2003.01787v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "The Effect of Silence Feature in Dimensional Speech Emotion Recognition", "abstract": "Silence is a part of human-to-human communication, which can be a clue for\nhuman emotion perception. For automatic emotion recognition by a computer, it\nis not clear whether silence is useful to determine human emotion within a\nspeech. This paper presents an investigation of the effect of using silence\nfeature in dimensional emotion recognition. Since the silence feature is\nextracted per utterance, we grouped the silence feature with high statistical\nfunctions from a set of acoustic features. The result reveals that the silence\nfeatures affect the arousal dimension more than other emotion dimensions. The\nproper choice of a threshold factor in the calculation of silence feature\nimproved the performance of dimensional speech emotion recognition performance,\nin terms of a concordance correlation coefficient. On the other side, improper\nchoice of that factor leads to a decrease in performance by using the same\narchitecture.", "published": "2020-03-03 01:17:07", "link": "http://arxiv.org/abs/2003.01277v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Interpretable Representation Learning for Singing Voice\n  Separation", "abstract": "In this work, we present a method for learning interpretable music signal\nrepresentations directly from waveform signals. Our method can be trained using\nunsupervised objectives and relies on the denoising auto-encoder model that\nuses a simple sinusoidal model as decoding functions to reconstruct the singing\nvoice. To demonstrate the benefits of our method, we employ the obtained\nrepresentations to the task of informed singing voice separation via binary\nmasking, and measure the obtained separation quality by means of\nscale-invariant signal to distortion ratio. Our findings suggest that our\nmethod is capable of learning meaningful representations for singing voice\nseparation, while preserving conveniences of the the short-time Fourier\ntransform like non-negativity, smoothness, and reconstruction subject to\ntime-frequency masking, that are desired in audio and music source separation.", "published": "2020-03-03 15:02:13", "link": "http://arxiv.org/abs/2003.01567v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SELD-TCN: Sound Event Localization & Detection via Temporal\n  Convolutional Networks", "abstract": "The understanding of the surrounding environment plays a critical role in\nautonomous robotic systems, such as self-driving cars. Extensive research has\nbeen carried out concerning visual perception. Yet, to obtain a more complete\nperception of the environment, autonomous systems of the future should also\ntake acoustic information into account. Recent sound event localization and\ndetection (SELD) frameworks utilize convolutional recurrent neural networks\n(CRNNs). However, considering the recurrent nature of CRNNs, it becomes\nchallenging to implement them efficiently on embedded hardware. Not only are\ntheir computations strenuous to parallelize, but they also require high memory\nbandwidth and large memory buffers. In this work, we develop a more robust and\nhardware-friendly novel architecture based on a temporal convolutional\nnetwork(TCN). The proposed framework (SELD-TCN) outperforms the\nstate-of-the-art SELDnet performance on four different datasets. Moreover,\nSELD-TCN achieves 4x faster training time per epoch and 40x faster inference\ntime on an ordinary graphics processing unit (GPU).", "published": "2020-03-03 15:48:57", "link": "http://arxiv.org/abs/2003.01609v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
