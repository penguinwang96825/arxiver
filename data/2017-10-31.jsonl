{"title": "Improving Social Media Text Summarization by Learning Sentence Weight\n  Distribution", "abstract": "Recently, encoder-decoder models are widely used in social media text\nsummarization. However, these models sometimes select noise words in irrelevant\nsentences as part of a summary by error, thus declining the performance. In\norder to inhibit irrelevant sentences and focus on key information, we propose\nan effective approach by learning sentence weight distribution. In our model,\nwe build a multi-layer perceptron to predict sentence weights. During training,\nwe use the ROUGE score as an alternative to the estimated sentence weight, and\ntry to minimize the gap between estimated weights and predicted weights. In\nthis way, we encourage our model to focus on the key sentences, which have high\nrelevance with the summary. Experimental results show that our approach\noutperforms baselines on a large-scale social media corpus.", "published": "2017-10-31 05:43:52", "link": "http://arxiv.org/abs/1710.11332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow Discourse Parsing with Maximum Entropy Model", "abstract": "In recent years, more research has been devoted to studying the subtask of\nthe complete shallow discourse parsing, such as indentifying discourse\nconnective and arguments of connective. There is a need to design a full\ndiscourse parser to pull these subtasks together. So we develop a discourse\nparser turning the free text into discourse relations. The parser includes\nconnective identifier, arguments identifier, sense classifier and non-explicit\nidentifier, which connects with each other in pipeline. Each component applies\nthe maximum entropy model with abundant lexical and syntax features extracted\nfrom the Penn Discourse Tree-bank. The head-based representation of the PDTB is\nadopted in the arguments identifier, which turns the problem of indentifying\nthe arguments of discourse connective into finding the head and end of the\narguments. In the non-explicit identifier, the contextual type features like\nwords which have high frequency and can reflect the discourse relation are\nintroduced to improve the performance of non-explicit identifier. Compared with\nother methods, experimental results achieve the considerable performance.", "published": "2017-10-31 05:47:38", "link": "http://arxiv.org/abs/1710.11334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sequential Matching Framework for Multi-turn Response Selection in\n  Retrieval-based Chatbots", "abstract": "We study the problem of response selection for multi-turn conversation in\nretrieval-based chatbots. The task requires matching a response candidate with\na conversation context, whose challenges include how to recognize important\nparts of the context, and how to model the relationships among utterances in\nthe context. Existing matching methods may lose important information in\ncontexts as we can interpret them with a unified framework in which contexts\nare transformed to fixed-length vectors without any interaction with responses\nbefore matching. The analysis motivates us to propose a new matching framework\nthat can sufficiently carry the important information in contexts to matching\nand model the relationships among utterances at the same time. The new\nframework, which we call a sequential matching framework (SMF), lets each\nutterance in a context interacts with a response candidate at the first step\nand transforms the pair to a matching vector. The matching vectors are then\naccumulated following the order of the utterances in the context with a\nrecurrent neural network (RNN) which models the relationships among the\nutterances. The context-response matching is finally calculated with the hidden\nstates of the RNN. Under SMF, we propose a sequential convolutional network and\nsequential attention network and conduct experiments on two public data sets to\ntest their performance. Experimental results show that both models can\nsignificantly outperform the state-of-the-art matching methods. We also show\nthat the models are interpretable with visualizations that provide us insights\non how they capture and leverage the important information in contexts for\nmatching.", "published": "2017-10-31 06:29:11", "link": "http://arxiv.org/abs/1710.11344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar Induction for Minimalist Grammars using Variational Bayesian\n  Inference : A Technical Report", "abstract": "The following technical report presents a formal approach to probabilistic\nminimalist grammar parameter estimation. We describe a formalization of a\nminimalist grammar. We then present an algorithm for the application of\nvariational Bayesian inference to this formalization.", "published": "2017-10-31 07:09:14", "link": "http://arxiv.org/abs/1710.11350v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing Dialogic Arguments from Social Media", "abstract": "Online argumentative dialog is a rich source of information on popular\nbeliefs and opinions that could be useful to companies as well as governmental\nor public policy agencies. Compact, easy to read, summaries of these dialogues\nwould thus be highly valuable. A priori, it is not even clear what form such a\nsummary should take. Previous work on summarization has primarily focused on\nsummarizing written texts, where the notion of an abstract of the text is well\ndefined. We collect gold standard training data consisting of five human\nsummaries for each of 161 dialogues on the topics of Gay Marriage, Gun Control\nand Abortion. We present several different computational models aimed at\nidentifying segments of the dialogues whose content should be used for the\nsummary, using linguistic features and Word2vec features with both SVMs and\nBidirectional LSTMs. We show that we can identify the most important arguments\nby using the dialog context with a best F-measure of 0.74 for gun control, 0.71\nfor gay marriage, and 0.67 for abortion.", "published": "2017-10-31 20:24:07", "link": "http://arxiv.org/abs/1711.00092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A generalized parsing framework for Abstract Grammars", "abstract": "This technical report presents a general framework for parsing a variety of\ngrammar formalisms. We develop a grammar formalism, called an Abstract Grammar,\nwhich is general enough to represent grammars at many levels of the hierarchy,\nincluding Context Free Grammars, Minimalist Grammars, and Generalized\nContext-free Grammars. We then develop a single parsing framework which is\ncapable of parsing grammars which are at least up to GCFGs on the hierarchy.\nOur parsing framework exposes a grammar interface, so that it can parse any\nparticular grammar formalism that can be reduced to an Abstract Grammar.", "published": "2017-10-31 02:23:15", "link": "http://arxiv.org/abs/1710.11301v3", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Machine Translation Using Monolingual Corpora Only", "abstract": "Machine translation has recently achieved impressive performance thanks to\nrecent advances in deep learning and the availability of large-scale parallel\ncorpora. There have been numerous attempts to extend these successes to\nlow-resource language pairs, yet requiring tens of thousands of parallel\nsentences. In this work, we take this research direction to the extreme and\ninvestigate whether it is possible to learn to translate even without any\nparallel data. We propose a model that takes sentences from monolingual corpora\nin two different languages and maps them into the same latent space. By\nlearning to reconstruct in both languages from this shared feature space, the\nmodel effectively learns to translate without using any labeled data. We\ndemonstrate our model on two widely used datasets and two language pairs,\nreporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French\ndatasets, without using even a single parallel sentence at training time.", "published": "2017-10-31 18:31:11", "link": "http://arxiv.org/abs/1711.00043v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DCN+: Mixed Objective and Deep Residual Coattention for Question\n  Answering", "abstract": "Traditional models for question answering optimize using cross entropy loss,\nwhich encourages exact answers at the cost of penalizing nearby or overlapping\nanswers that are sometimes equally accurate. We propose a mixed objective that\ncombines cross entropy loss with self-critical policy learning. The objective\nuses rewards derived from word overlap to solve the misalignment between\nevaluation metric and optimization objective. In addition to the mixed\nobjective, we improve dynamic coattention networks (DCN) with a deep residual\ncoattention encoder that is inspired by recent work in deep self-attention and\nresidual networks. Our proposals improve model performance across question\ntypes and input lengths, especially for long questions that requires the\nability to capture long-term dependencies. On the Stanford Question Answering\nDataset, our model achieves state-of-the-art results with 75.1% exact match\naccuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy\nand 86.0% F1.", "published": "2017-10-31 20:53:42", "link": "http://arxiv.org/abs/1711.00106v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue\n  Policy Learning", "abstract": "This paper presents a new method --- adversarial advantage actor-critic\n(Adversarial A2C), which significantly improves the efficiency of dialogue\npolicy learning in task-completion dialogue systems. Inspired by generative\nadversarial networks (GAN), we train a discriminator to differentiate\nresponses/actions generated by dialogue agents from responses/actions by\nexperts. Then, we incorporate the discriminator as another critic into the\nadvantage actor-critic (A2C) framework, to encourage the dialogue agent to\nexplore state-action within the regions where the agent takes actions similar\nto those of the experts. Experimental results in a movie-ticket booking domain\nshow that the proposed Adversarial A2C can accelerate policy exploration\nefficiently.", "published": "2017-10-31 00:25:03", "link": "http://arxiv.org/abs/1710.11277v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Natural Adversarial Examples", "abstract": "Due to their complex nature, it is hard to characterize the ways in which\nmachine learning models can misbehave or be exploited when deployed. Recent\nwork on adversarial examples, i.e. inputs with minor perturbations that result\nin substantially different model predictions, is helpful in evaluating the\nrobustness of these models by exposing the adversarial scenarios where they\nfail. However, these malicious perturbations are often unnatural, not\nsemantically meaningful, and not applicable to complicated domains such as\nlanguage. In this paper, we propose a framework to generate natural and legible\nadversarial examples that lie on the data manifold, by searching in semantic\nspace of dense and continuous data representation, utilizing the recent\nadvances in generative adversarial networks. We present generated adversaries\nto demonstrate the potential of the proposed approach for black-box classifiers\nfor a wide range of applications such as image classification, textual\nentailment, and machine translation. We include experiments to show that the\ngenerated adversaries are natural, legible to humans, and useful in evaluating\nand analyzing black-box classifiers.", "published": "2017-10-31 06:22:26", "link": "http://arxiv.org/abs/1710.11342v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding", "abstract": "In this paper we argue that crime drama exemplified in television programs\nsuch as CSI:Crime Scene Investigation is an ideal testbed for approximating\nreal-world natural language understanding and the complex inferences associated\nwith it. We propose to treat crime drama as a new inference task, capitalizing\non the fact that each episode poses the same basic question (i.e., who\ncommitted the crime) and naturally provides the answer when the perpetrator is\nrevealed. We develop a new dataset based on CSI episodes, formalize perpetrator\nidentification as a sequence labeling problem, and develop an LSTM-based model\nwhich learns from multi-modal data. Experimental results show that an\nincremental inference strategy is key to making accurate guesses as well as\nlearning from representations fusing textual, visual, and acoustic input.", "published": "2017-10-31 17:27:44", "link": "http://arxiv.org/abs/1710.11601v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generalization without systematicity: On the compositional skills of\n  sequence-to-sequence recurrent networks", "abstract": "Humans can understand and produce new utterances effortlessly, thanks to\ntheir compositional skills. Once a person learns the meaning of a new verb\n\"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing\nand dax.\" In this paper, we introduce the SCAN domain, consisting of a set of\nsimple compositional navigation commands paired with the corresponding action\nsequences. We then test the zero-shot generalization capabilities of a variety\nof recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence\nmethods. We find that RNNs can make successful zero-shot generalizations when\nthe differences between training and test commands are small, so that they can\napply \"mix-and-match\" strategies to solve the task. However, when\ngeneralization requires systematic compositional skills (as in the \"dax\"\nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept\nexperiment in neural machine translation, suggesting that lack of systematicity\nmight be partially responsible for neural networks' notorious training data\nthirst.", "published": "2017-10-31 01:50:02", "link": "http://arxiv.org/abs/1711.00350v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical\n  Properties of Feature Extractors", "abstract": "A F0 and voicing status estimation algorithm for high quality speech\nanalysis/synthesis is proposed. This problem is approached from a different\nperspective that models the behavior of feature extractors under noise, instead\nof directly modeling speech signals. Under time-frequency locality assumptions,\nthe joint distribution of extracted features and target F0 can be characterized\nby training a bank of Gaussian mixture models (GMM) on artificial data\ngenerated from Monte-Carlo simulations. The trained GMMs can then be used to\ngenerate a set of conditional distributions on the predicted F0, which are then\ncombined and post-processed by Viterbi algorithm to give a final F0 trajectory.\nEvaluation on CSTR and CMU Arctic speech databases shows that the proposed\nmethod, trained on fully synthetic data, achieves lower gross error rates than\nstate-of-the-art methods.", "published": "2017-10-31 04:07:30", "link": "http://arxiv.org/abs/1710.11317v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Polyphonic Music Generation with Sequence Generative Adversarial\n  Networks", "abstract": "We propose an application of sequence generative adversarial networks\n(SeqGAN), which are generative adversarial networks for discrete sequence\ngeneration, for creating polyphonic musical sequences. Instead of a monophonic\nmelody generation suggested in the original work, we present an efficient\nrepresentation of a polyphony MIDI file that simultaneously captures chords and\nmelodies with dynamic timings. The proposed method condenses duration, octaves,\nand keys of both melodies and chords into a single word vector representation,\nand recurrent neural networks learn to predict distributions of sequences from\nthe embedded musical word space. We experiment with the original method and the\nleast squares method to the discriminator, which is known to stabilize the\ntraining of GANs. The network can create sequences that are musically coherent\nand shows an improved quantitative and qualitative measures. We also report\nthat careful optimization of reinforcement learning signals of the model is\ncrucial for general application of the model.", "published": "2017-10-31 11:57:00", "link": "http://arxiv.org/abs/1710.11418v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio style transfer", "abstract": "'Style transfer' among images has recently emerged as a very active research\ntopic, fuelled by the power of convolution neural networks (CNNs), and has\nbecome fast a very popular technology in social media. This paper investigates\nthe analogous problem in the audio domain: How to transfer the style of a\nreference audio signal to a target audio content? We propose a flexible\nframework for the task, which uses a sound texture model to extract statistics\ncharacterizing the reference audio style, followed by an optimization-based\naudio texture synthesis to modify the target content. In contrast to mainstream\noptimization-based visual transfer method, the proposed process is initialized\nby the target content instead of random noise and the optimized loss is only\nabout texture, not structure. These differences proved key for audio style\ntransfer in our experiments. In order to extract features of interest, we\ninvestigate different architectures, whether pre-trained on other tasks, as\ndone in image style transfer, or engineered based on the human auditory system.\nExperimental results on different types of audio signal confirm the potential\nof the proposed approach.", "published": "2017-10-31 09:19:28", "link": "http://arxiv.org/abs/1710.11385v3", "categories": ["cs.SD", "eess.AS", "physics.class-ph"], "primary_category": "cs.SD"}
{"title": "SVSGAN: Singing Voice Separation via Generative Adversarial Network", "abstract": "Separating two sources from an audio mixture is an important task with many\napplications. It is a challenging problem since only one signal channel is\navailable for analysis. In this paper, we propose a novel framework for singing\nvoice separation using the generative adversarial network (GAN) with a\ntime-frequency masking function. The mixture spectra is considered to be a\ndistribution and is mapped to the clean spectra which is also considered a\ndistribtution. The approximation of distributions between mixture spectra and\nclean spectra is performed during the adversarial training process. In contrast\nwith current deep learning approaches for source separation, the parameters of\nthe proposed framework are first initialized in a supervised setting and then\noptimized by the training procedure of GAN in an unsupervised setting.\nExperimental results on three datasets (MIR-1K, iKala and DSD100) show that\nperformance can be improved by the proposed framework consisting of\nconventional networks.", "published": "2017-10-31 12:19:23", "link": "http://arxiv.org/abs/1710.11428v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistical Speech Enhancement Based on Probabilistic Integration of\n  Variational Autoencoder and Non-Negative Matrix Factorization", "abstract": "This paper presents a statistical method of single-channel speech enhancement\nthat uses a variational autoencoder (VAE) as a prior distribution on clean\nspeech. A standard approach to speech enhancement is to train a deep neural\nnetwork (DNN) to take noisy speech as input and output clean speech. Although\nthis supervised approach requires a very large amount of pair data for\ntraining, it is not robust against unknown environments. Another approach is to\nuse non-negative matrix factorization (NMF) based on basis spectra trained on\nclean speech in advance and those adapted to noise on the fly. This\nsemi-supervised approach, however, causes considerable signal distortion in\nenhanced speech due to the unrealistic assumption that speech spectrograms are\nlinear combinations of the basis spectra. Replacing the poor linear generative\nmodel of clean speech in NMF with a VAE---a powerful nonlinear deep generative\nmodel---trained on clean speech, we formulate a unified probabilistic\ngenerative model of noisy speech. Given noisy speech as observed data, we can\nsample clean speech from its posterior distribution. The proposed method\noutperformed the conventional DNN-based method in unseen noisy environments.", "published": "2017-10-31 12:52:09", "link": "http://arxiv.org/abs/1710.11439v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Melody Generation for Pop Music via Word Representation of Musical\n  Properties", "abstract": "Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors.\nRepresentation of multivariate property of notes has been one of the primary\nchallenges. It is also difficult to remain in the permissible spectrum of\nmusical variety, outside of which would be perceived as a plain random play\nwithout auditory pleasantness. Observing the conventional structure of pop\nmusic poses further challenges. In this paper, we propose to represent each\nnote and its properties as a unique `word,' thus lessening the prospect of\nmisalignments between the properties, as well as reducing the complexity of\nlearning. We also enforce regularization policies on the range of notes, thus\nencouraging the generated melody to stay close to what humans would find easy\nto follow. Furthermore, we generate melody conditioned on song part\ninformation, thus replicating the overall structure of a full song.\nExperimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.", "published": "2017-10-31 16:04:23", "link": "http://arxiv.org/abs/1710.11549v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "User Environment Detection with Acoustic Sensors Embedded on Mobile\n  Devices for the Recognition of Activities of Daily Living", "abstract": "The detection of the environment where user is located, is of extreme use for\nthe identification of Activities of Daily Living (ADL). ADL can be identified\nby use of the sensors available in many off-the-shelf mobile devices, including\nmagnetic and motion, and the environment can be also identified using acoustic\nsensors. The study presented in this paper is divided in two parts: firstly, we\ndiscuss the recognition of the environment using acoustic sensors (i.e.,\nmicrophone), and secondly, we fuse this information with motion and magnetic\nsensors (i.e., motion and magnetic sensors) for the recognition of standing\nactivities of daily living. The recognition of the environments and the ADL are\nperformed using pattern recognition techniques, in order to develop a system\nthat includes data acquisition, data processing, data fusion, and artificial\nintelligence methods. The artificial intelligence methods explored in this\nstudy are composed by different types of Artificial Neural Networks (ANN),\ncomparing the different types of ANN and selecting the best methods to\nimplement in the different stages of the system developed. Conclusions point to\nthe use of Deep Neural Networks (DNN) with normalized data for the\nidentification of ADL with 85.89% of accuracy, the use of Feedforward neural\nnetworks with non-normalized data for the identification of the environments\nwith 86.50% of accuracy, and the use of DNN with normalized data for the\nidentification of standing activities with 100% of accuracy.", "published": "2017-10-31 22:00:25", "link": "http://arxiv.org/abs/1711.00124v1", "categories": ["cs.SD", "eess.AS", "physics.data-an"], "primary_category": "cs.SD"}
{"title": "Full-info Training for Deep Speaker Feature Learning", "abstract": "In recent studies, it has shown that speaker patterns can be learned from\nvery short speech segments (e.g., 0.3 seconds) by a carefully designed\nconvolutional & time-delay deep neural network (CT-DNN) model. By enforcing the\nmodel to discriminate the speakers in the training data, frame-level speaker\nfeatures can be derived from the last hidden layer. In spite of its good\nperformance, a potential problem of the present model is that it involves a\nparametric classifier, i.e., the last affine layer, which may consume some\ndiscriminative knowledge, thus leading to `information leak' for the feature\nlearning. This paper presents a full-info training approach that discards the\nparametric classifier and enforces all the discriminative knowledge learned by\nthe feature net. Our experiments on the Fisher database demonstrate that this\nnew training scheme can produce more coherent features, leading to consistent\nand notable performance improvement on the speaker verification task.", "published": "2017-10-31 12:44:23", "link": "http://arxiv.org/abs/1711.00366v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
