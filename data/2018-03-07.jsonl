{"title": "Translating Questions into Answers using DBPedia n-triples", "abstract": "In this paper we present a question answering system using a neural network\nto interpret questions learned from the DBpedia repository. We train a\nsequence-to-sequence neural network model with n-triples extracted from the\nDBpedia Infobox Properties. Since these properties do not represent the natural\nlanguage, we further used question-answer dialogues from movie subtitles.\nAlthough the automatic evaluation shows a low overlap of the generated answers\ncompared to the gold standard set, a manual inspection of the showed promising\noutcomes from the experiment for further work.", "published": "2018-03-07 23:29:31", "link": "http://arxiv.org/abs/1803.02914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Action Sequences from Texts Based on Deep Reinforcement\n  Learning", "abstract": "Extracting action sequences from natural language texts is challenging, as it\nrequires commonsense inferences based on world knowledge. Although there has\nbeen work on extracting action scripts, instructions, navigation actions, etc.,\nthey require that either the set of candidate actions be provided in advance,\nor that action descriptions are restricted to a specific form, e.g.,\ndescription templates. In this paper, we aim to extract action sequences from\ntexts in free natural language, i.e., without any restricted templates,\nprovided the candidate set of actions is unknown. We propose to extract action\nsequences from texts based on the deep reinforcement learning framework.\nSpecifically, we view \"selecting\" or \"eliminating\" words from texts as\n\"actions\", and the texts associated with actions as \"states\". We then build\nQ-networks to learn the policy of extracting actions and extract plans from the\nlabeled texts. We demonstrate the effectiveness of our approach on several\ndatasets with comparison to state-of-the-art approaches, including online\nexperiments interacting with humans.", "published": "2018-03-07 13:13:16", "link": "http://arxiv.org/abs/1803.02632v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generating Contradictory, Neutral, and Entailing Sentences", "abstract": "Learning distributed sentence representations remains an interesting problem\nin the field of Natural Language Processing (NLP). We want to learn a model\nthat approximates the conditional latent space over the representations of a\nlogical antecedent of the given statement. In our paper, we propose an approach\nto generating sentences, conditioned on an input sentence and a logical\ninference label. We do this by modeling the different possibilities for the\noutput sentence as a distribution over the latent representation, which we\ntrain using an adversarial objective. We evaluate the model using two\nstate-of-the-art models for the Recognizing Textual Entailment (RTE) task, and\nmeasure the BLEU scores against the actual sentences as a probe for the\ndiversity of sentences produced by our model. The experiment results show that,\ngiven our framework, we have clear ways to improve the quality and diversity of\ngenerated sentences.", "published": "2018-03-07 15:18:03", "link": "http://arxiv.org/abs/1803.02710v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards the Creation of a Large Corpus of Synthetically-Identified\n  Clinical Notes", "abstract": "Clinical notes often describe the most important aspects of a patient's\nphysiology and are therefore critical to medical research. However, these notes\nare typically inaccessible to researchers without prior removal of sensitive\nprotected health information (PHI), a natural language processing (NLP) task\nreferred to as deidentification. Tools to automatically de-identify clinical\nnotes are needed but are difficult to create without access to those very same\nnotes containing PHI. This work presents a first step toward creating a large\nsynthetically-identified corpus of clinical notes and corresponding PHI\nannotations in order to facilitate the development de-identification tools.\nFurther, one such tool is evaluated against this corpus in order to understand\nthe advantages and shortcomings of this approach.", "published": "2018-03-07 15:51:11", "link": "http://arxiv.org/abs/1803.02728v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Automating Reading Comprehension by Generating Question and Answer Pairs", "abstract": "Neural network-based methods represent the state-of-the-art in question\ngeneration from text. Existing work focuses on generating only questions from\ntext without concerning itself with answer generation. Moreover, our analysis\nshows that handling rare words and generating the most appropriate question\ngiven a candidate answer are still challenges facing existing approaches. We\npresent a novel two-stage process to generate question-answer pairs from the\ntext. For the first stage, we present alternatives for encoding the span of the\npivotal answer in the sentence using Pointer Networks. In our second stage, we\nemploy sequence to sequence models for question generation, enhanced with rich\nlinguistic features. Finally, global attention and answer encoding are used for\ngenerating the question most relevant to the answer. We motivate and\nlinguistically analyze the role of each component in our framework and consider\ncompositions of these. This analysis is supported by extensive experimental\nevaluations. Using standard evaluation metrics as well as human evaluations,\nour experimental results validate the significant improvement in the quality of\nquestions generated by our framework over the state-of-the-art. The technique\npresented here represents another step towards more automated reading\ncomprehension assessment. We also present a live system \\footnote{Demo of the\nsystem is available at\n\\url{https://www.cse.iitb.ac.in/~vishwajeet/autoqg.html}.} to demonstrate the\neffectiveness of our approach.", "published": "2018-03-07 07:55:11", "link": "http://arxiv.org/abs/1803.03664v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The emergent algebraic structure of RNNs and embeddings in NLP", "abstract": "We examine the algebraic and geometric properties of a uni-directional GRU\nand word embeddings trained end-to-end on a text classification task. A\nhyperparameter search over word embedding dimension, GRU hidden dimension, and\na linear combination of the GRU outputs is performed. We conclude that words\nnaturally embed themselves in a Lie group and that RNNs form a nonlinear\nrepresentation of the group. Appealing to these results, we propose a novel\nclass of recurrent-like neural networks and a word embedding scheme.", "published": "2018-03-07 19:06:08", "link": "http://arxiv.org/abs/1803.02839v1", "categories": ["cs.CL", "cs.AI", "stat.ML", "97R40"], "primary_category": "cs.CL"}
{"title": "An efficient framework for learning sentence representations", "abstract": "In this work we propose a simple and efficient framework for learning\nsentence representations from unlabelled data. Drawing inspiration from the\ndistributional hypothesis and recent work on learning sentence representations,\nwe reformulate the problem of predicting the context in which a sentence\nappears as a classification problem. Given a sentence and its context, a\nclassifier distinguishes context sentences from other contrastive sentences\nbased on their vector representations. This allows us to efficiently learn\ndifferent types of encoding functions, and we show that the model learns\nhigh-quality sentence representations. We demonstrate that our sentence\nrepresentations outperform state-of-the-art unsupervised and supervised\nrepresentation learning methods on several downstream NLP tasks that involve\nunderstanding sentence semantics while achieving an order of magnitude speedup\nin training time.", "published": "2018-03-07 22:02:10", "link": "http://arxiv.org/abs/1803.02893v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Domain Invariant Features by Unsupervised Learning for Robust\n  Automatic Speech Recognition", "abstract": "The performance of automatic speech recognition (ASR) systems can be\nsignificantly compromised by previously unseen conditions, which is typically\ndue to a mismatch between training and testing distributions. In this paper, we\naddress robustness by studying domain invariant features, such that domain\ninformation becomes transparent to ASR systems, resolving the mismatch problem.\nSpecifically, we investigate a recent model, called the Factorized Hierarchical\nVariational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and\nsegment-level attributes into different latent variables without supervision.\nWe argue that the set of latent variables that contain segment-level\ninformation is our desired domain invariant feature for ASR. Experiments are\nconducted on Aurora-4 and CHiME-4, which demonstrate 41% and 27% absolute word\nerror rate reductions respectively on mismatched domains.", "published": "2018-03-07 07:30:36", "link": "http://arxiv.org/abs/1803.02551v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
